id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/hail-is/hail/issues/5505:1782,Availability,Error,Error,1782,"rver will be able to run code.; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.802 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /etc/jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.803 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /usr/local/etc/jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.804 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /opt/conda/etc/jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.804 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /root/.jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [W 19:59:04.904 NotebookApp] Error loading server extension jupyter_spark; Mar 01 19:59:04 dk-m python[5149]: Traceback (most recent call last):; Mar 01 19:59:04 dk-m python[5149]: File ""/opt/conda/lib/python3.6/site-packages/notebook/notebookapp.py"", line 1575, in init_server_extensions; Mar 01 19:59:04 dk-m python[5149]: func(self); Mar 01 19:59:04 dk-m python[5149]: File ""/opt/conda/lib/python3.6/site-packages/jupyter_spark/__init__.py"", line 30, in load_jupyter_server_extension; Mar 01 19:59:04 dk-m python[5149]: from .handlers import SparkHandler; Mar 01 19:59:04 dk-m python[5149]: File ""/opt/conda/lib/python3.6/site-packages/jupyter_spark/handlers.py"", line 8, in <module>; Mar 01 19:59:04 dk-m python[5149]: class SparkHandler(IPythonHandler):; Mar 01 19:59:04 dk-m python[5149]: File ""/opt/conda/lib/python3.6/site-packages/jupyter_spark/handlers.py"", line 13, in SparkHandler; Mar 01 19:59:04 dk-m python[5149]: @tornado.web.asynchronous; Mar 01 19:59:04 dk-m python[5149]: AttributeError: module 'tornado.web' has no attribute 'asynchronous'; ```. It appears that Jupyter starts even though on",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5505
https://github.com/hail-is/hail/issues/5505:975,Deployability,configurat,configuration,975,"d to hail version. JAR and ZIP:; ```; gs://hail-common/builds/0.2/jars/hail-0.2-3b1cb0772301-Spark-2.2.0.jar; gs://hail-common/builds/0.2/python/hail-0.2-3b1cb0772301.zip; ```. In Google Chrome we see 404s for; ```; GET http://localhost:8123/spark/api/v1/applications; ```; which happened repeatedly if you try to evaluate a cell. On the leader node of the spark cluster, `journalctl -u jupyter` shows:; ```; -- Logs begin at Fri 2019-03-01 19:54:49 UTC, end at Fri 2019-03-01 20:11:51 UTC. --; Mar 01 19:59:03 dk-m systemd[1]: Started Jupyter Notebook.; Mar 01 19:59:04 dk-m python[5149]: [I 19:59:04.630 NotebookApp] Writing notebook server cookie secret to /root/.local/share/jupyter/runtime/notebook_cookie_secret; Mar 01 19:59:04 dk-m python[5149]: [W 19:59:04.796 NotebookApp] All authentication is disabled. Anyone who can connect to this server will be able to run code.; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.802 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /etc/jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.803 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /usr/local/etc/jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.804 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /opt/conda/etc/jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.804 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /root/.jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [W 19:59:04.904 NotebookApp] Error loading server extension jupyter_spark; Mar 01 19:59:04 dk-m python[5149]: Traceback (most recent call last):; Mar 01 19:59:04 dk-m python[5149]: File ""/opt/conda/lib/python3.6/site-packages/notebook/notebookapp.py"", line 1575, i",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5505
https://github.com/hail-is/hail/issues/5505:1175,Deployability,configurat,configuration,1175," for; ```; GET http://localhost:8123/spark/api/v1/applications; ```; which happened repeatedly if you try to evaluate a cell. On the leader node of the spark cluster, `journalctl -u jupyter` shows:; ```; -- Logs begin at Fri 2019-03-01 19:54:49 UTC, end at Fri 2019-03-01 20:11:51 UTC. --; Mar 01 19:59:03 dk-m systemd[1]: Started Jupyter Notebook.; Mar 01 19:59:04 dk-m python[5149]: [I 19:59:04.630 NotebookApp] Writing notebook server cookie secret to /root/.local/share/jupyter/runtime/notebook_cookie_secret; Mar 01 19:59:04 dk-m python[5149]: [W 19:59:04.796 NotebookApp] All authentication is disabled. Anyone who can connect to this server will be able to run code.; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.802 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /etc/jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.803 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /usr/local/etc/jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.804 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /opt/conda/etc/jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.804 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /root/.jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [W 19:59:04.904 NotebookApp] Error loading server extension jupyter_spark; Mar 01 19:59:04 dk-m python[5149]: Traceback (most recent call last):; Mar 01 19:59:04 dk-m python[5149]: File ""/opt/conda/lib/python3.6/site-packages/notebook/notebookapp.py"", line 1575, in init_server_extensions; Mar 01 19:59:04 dk-m python[5149]: func(self); Mar 01 19:59:04 dk-m python[5149]: File ""/opt/conda/lib/python3.6/site-packages/jupyter_spark/__init__.py"", line 30, in load_jupyter",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5505
https://github.com/hail-is/hail/issues/5505:1385,Deployability,configurat,configuration,1385,"s begin at Fri 2019-03-01 19:54:49 UTC, end at Fri 2019-03-01 20:11:51 UTC. --; Mar 01 19:59:03 dk-m systemd[1]: Started Jupyter Notebook.; Mar 01 19:59:04 dk-m python[5149]: [I 19:59:04.630 NotebookApp] Writing notebook server cookie secret to /root/.local/share/jupyter/runtime/notebook_cookie_secret; Mar 01 19:59:04 dk-m python[5149]: [W 19:59:04.796 NotebookApp] All authentication is disabled. Anyone who can connect to this server will be able to run code.; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.802 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /etc/jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.803 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /usr/local/etc/jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.804 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /opt/conda/etc/jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.804 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /root/.jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [W 19:59:04.904 NotebookApp] Error loading server extension jupyter_spark; Mar 01 19:59:04 dk-m python[5149]: Traceback (most recent call last):; Mar 01 19:59:04 dk-m python[5149]: File ""/opt/conda/lib/python3.6/site-packages/notebook/notebookapp.py"", line 1575, in init_server_extensions; Mar 01 19:59:04 dk-m python[5149]: func(self); Mar 01 19:59:04 dk-m python[5149]: File ""/opt/conda/lib/python3.6/site-packages/jupyter_spark/__init__.py"", line 30, in load_jupyter_server_extension; Mar 01 19:59:04 dk-m python[5149]: from .handlers import SparkHandler; Mar 01 19:59:04 dk-m python[5149]: File ""/opt/conda/lib/python3.6/site-packages/jupyter_spark/handlers.py"", line 8, in <",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5505
https://github.com/hail-is/hail/issues/5505:1595,Deployability,configurat,configuration,1595," NotebookApp] Writing notebook server cookie secret to /root/.local/share/jupyter/runtime/notebook_cookie_secret; Mar 01 19:59:04 dk-m python[5149]: [W 19:59:04.796 NotebookApp] All authentication is disabled. Anyone who can connect to this server will be able to run code.; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.802 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /etc/jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.803 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /usr/local/etc/jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.804 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /opt/conda/etc/jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.804 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /root/.jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [W 19:59:04.904 NotebookApp] Error loading server extension jupyter_spark; Mar 01 19:59:04 dk-m python[5149]: Traceback (most recent call last):; Mar 01 19:59:04 dk-m python[5149]: File ""/opt/conda/lib/python3.6/site-packages/notebook/notebookapp.py"", line 1575, in init_server_extensions; Mar 01 19:59:04 dk-m python[5149]: func(self); Mar 01 19:59:04 dk-m python[5149]: File ""/opt/conda/lib/python3.6/site-packages/jupyter_spark/__init__.py"", line 30, in load_jupyter_server_extension; Mar 01 19:59:04 dk-m python[5149]: from .handlers import SparkHandler; Mar 01 19:59:04 dk-m python[5149]: File ""/opt/conda/lib/python3.6/site-packages/jupyter_spark/handlers.py"", line 8, in <module>; Mar 01 19:59:04 dk-m python[5149]: class SparkHandler(IPythonHandler):; Mar 01 19:59:04 dk-m python[5149]: File ""/opt/conda/lib/python3.6/site-packages/jupyter_spark/handlers.py"", l",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5505
https://github.com/hail-is/hail/issues/5505:975,Modifiability,config,configuration,975,"d to hail version. JAR and ZIP:; ```; gs://hail-common/builds/0.2/jars/hail-0.2-3b1cb0772301-Spark-2.2.0.jar; gs://hail-common/builds/0.2/python/hail-0.2-3b1cb0772301.zip; ```. In Google Chrome we see 404s for; ```; GET http://localhost:8123/spark/api/v1/applications; ```; which happened repeatedly if you try to evaluate a cell. On the leader node of the spark cluster, `journalctl -u jupyter` shows:; ```; -- Logs begin at Fri 2019-03-01 19:54:49 UTC, end at Fri 2019-03-01 20:11:51 UTC. --; Mar 01 19:59:03 dk-m systemd[1]: Started Jupyter Notebook.; Mar 01 19:59:04 dk-m python[5149]: [I 19:59:04.630 NotebookApp] Writing notebook server cookie secret to /root/.local/share/jupyter/runtime/notebook_cookie_secret; Mar 01 19:59:04 dk-m python[5149]: [W 19:59:04.796 NotebookApp] All authentication is disabled. Anyone who can connect to this server will be able to run code.; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.802 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /etc/jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.803 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /usr/local/etc/jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.804 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /opt/conda/etc/jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.804 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /root/.jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [W 19:59:04.904 NotebookApp] Error loading server extension jupyter_spark; Mar 01 19:59:04 dk-m python[5149]: Traceback (most recent call last):; Mar 01 19:59:04 dk-m python[5149]: File ""/opt/conda/lib/python3.6/site-packages/notebook/notebookapp.py"", line 1575, i",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5505
https://github.com/hail-is/hail/issues/5505:1175,Modifiability,config,configuration,1175," for; ```; GET http://localhost:8123/spark/api/v1/applications; ```; which happened repeatedly if you try to evaluate a cell. On the leader node of the spark cluster, `journalctl -u jupyter` shows:; ```; -- Logs begin at Fri 2019-03-01 19:54:49 UTC, end at Fri 2019-03-01 20:11:51 UTC. --; Mar 01 19:59:03 dk-m systemd[1]: Started Jupyter Notebook.; Mar 01 19:59:04 dk-m python[5149]: [I 19:59:04.630 NotebookApp] Writing notebook server cookie secret to /root/.local/share/jupyter/runtime/notebook_cookie_secret; Mar 01 19:59:04 dk-m python[5149]: [W 19:59:04.796 NotebookApp] All authentication is disabled. Anyone who can connect to this server will be able to run code.; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.802 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /etc/jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.803 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /usr/local/etc/jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.804 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /opt/conda/etc/jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.804 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /root/.jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [W 19:59:04.904 NotebookApp] Error loading server extension jupyter_spark; Mar 01 19:59:04 dk-m python[5149]: Traceback (most recent call last):; Mar 01 19:59:04 dk-m python[5149]: File ""/opt/conda/lib/python3.6/site-packages/notebook/notebookapp.py"", line 1575, in init_server_extensions; Mar 01 19:59:04 dk-m python[5149]: func(self); Mar 01 19:59:04 dk-m python[5149]: File ""/opt/conda/lib/python3.6/site-packages/jupyter_spark/__init__.py"", line 30, in load_jupyter",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5505
https://github.com/hail-is/hail/issues/5505:1385,Modifiability,config,configuration,1385,"s begin at Fri 2019-03-01 19:54:49 UTC, end at Fri 2019-03-01 20:11:51 UTC. --; Mar 01 19:59:03 dk-m systemd[1]: Started Jupyter Notebook.; Mar 01 19:59:04 dk-m python[5149]: [I 19:59:04.630 NotebookApp] Writing notebook server cookie secret to /root/.local/share/jupyter/runtime/notebook_cookie_secret; Mar 01 19:59:04 dk-m python[5149]: [W 19:59:04.796 NotebookApp] All authentication is disabled. Anyone who can connect to this server will be able to run code.; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.802 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /etc/jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.803 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /usr/local/etc/jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.804 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /opt/conda/etc/jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.804 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /root/.jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [W 19:59:04.904 NotebookApp] Error loading server extension jupyter_spark; Mar 01 19:59:04 dk-m python[5149]: Traceback (most recent call last):; Mar 01 19:59:04 dk-m python[5149]: File ""/opt/conda/lib/python3.6/site-packages/notebook/notebookapp.py"", line 1575, in init_server_extensions; Mar 01 19:59:04 dk-m python[5149]: func(self); Mar 01 19:59:04 dk-m python[5149]: File ""/opt/conda/lib/python3.6/site-packages/jupyter_spark/__init__.py"", line 30, in load_jupyter_server_extension; Mar 01 19:59:04 dk-m python[5149]: from .handlers import SparkHandler; Mar 01 19:59:04 dk-m python[5149]: File ""/opt/conda/lib/python3.6/site-packages/jupyter_spark/handlers.py"", line 8, in <",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5505
https://github.com/hail-is/hail/issues/5505:1595,Modifiability,config,configuration,1595," NotebookApp] Writing notebook server cookie secret to /root/.local/share/jupyter/runtime/notebook_cookie_secret; Mar 01 19:59:04 dk-m python[5149]: [W 19:59:04.796 NotebookApp] All authentication is disabled. Anyone who can connect to this server will be able to run code.; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.802 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /etc/jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.803 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /usr/local/etc/jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.804 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /opt/conda/etc/jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.804 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /root/.jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [W 19:59:04.904 NotebookApp] Error loading server extension jupyter_spark; Mar 01 19:59:04 dk-m python[5149]: Traceback (most recent call last):; Mar 01 19:59:04 dk-m python[5149]: File ""/opt/conda/lib/python3.6/site-packages/notebook/notebookapp.py"", line 1575, in init_server_extensions; Mar 01 19:59:04 dk-m python[5149]: func(self); Mar 01 19:59:04 dk-m python[5149]: File ""/opt/conda/lib/python3.6/site-packages/jupyter_spark/__init__.py"", line 30, in load_jupyter_server_extension; Mar 01 19:59:04 dk-m python[5149]: from .handlers import SparkHandler; Mar 01 19:59:04 dk-m python[5149]: File ""/opt/conda/lib/python3.6/site-packages/jupyter_spark/handlers.py"", line 8, in <module>; Mar 01 19:59:04 dk-m python[5149]: class SparkHandler(IPythonHandler):; Mar 01 19:59:04 dk-m python[5149]: File ""/opt/conda/lib/python3.6/site-packages/jupyter_spark/handlers.py"", l",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5505
https://github.com/hail-is/hail/issues/5505:2873,Modifiability,plugin,plugins,2873,"9:04 dk-m python[5149]: /etc/jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.803 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /usr/local/etc/jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.804 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /opt/conda/etc/jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.804 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /root/.jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [W 19:59:04.904 NotebookApp] Error loading server extension jupyter_spark; Mar 01 19:59:04 dk-m python[5149]: Traceback (most recent call last):; Mar 01 19:59:04 dk-m python[5149]: File ""/opt/conda/lib/python3.6/site-packages/notebook/notebookapp.py"", line 1575, in init_server_extensions; Mar 01 19:59:04 dk-m python[5149]: func(self); Mar 01 19:59:04 dk-m python[5149]: File ""/opt/conda/lib/python3.6/site-packages/jupyter_spark/__init__.py"", line 30, in load_jupyter_server_extension; Mar 01 19:59:04 dk-m python[5149]: from .handlers import SparkHandler; Mar 01 19:59:04 dk-m python[5149]: File ""/opt/conda/lib/python3.6/site-packages/jupyter_spark/handlers.py"", line 8, in <module>; Mar 01 19:59:04 dk-m python[5149]: class SparkHandler(IPythonHandler):; Mar 01 19:59:04 dk-m python[5149]: File ""/opt/conda/lib/python3.6/site-packages/jupyter_spark/handlers.py"", line 13, in SparkHandler; Mar 01 19:59:04 dk-m python[5149]: @tornado.web.asynchronous; Mar 01 19:59:04 dk-m python[5149]: AttributeError: module 'tornado.web' has no attribute 'asynchronous'; ```. It appears that Jupyter starts even though one of its plugins fail. This sucks, since Jupyter doesn't actually seem to work if jupyter_spark fails. This happens even if the evaluated cell contains only `3 + 3`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5505
https://github.com/hail-is/hail/issues/5505:1788,Performance,load,loading,1788,"rver will be able to run code.; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.802 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /etc/jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.803 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /usr/local/etc/jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.804 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /opt/conda/etc/jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.804 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /root/.jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [W 19:59:04.904 NotebookApp] Error loading server extension jupyter_spark; Mar 01 19:59:04 dk-m python[5149]: Traceback (most recent call last):; Mar 01 19:59:04 dk-m python[5149]: File ""/opt/conda/lib/python3.6/site-packages/notebook/notebookapp.py"", line 1575, in init_server_extensions; Mar 01 19:59:04 dk-m python[5149]: func(self); Mar 01 19:59:04 dk-m python[5149]: File ""/opt/conda/lib/python3.6/site-packages/jupyter_spark/__init__.py"", line 30, in load_jupyter_server_extension; Mar 01 19:59:04 dk-m python[5149]: from .handlers import SparkHandler; Mar 01 19:59:04 dk-m python[5149]: File ""/opt/conda/lib/python3.6/site-packages/jupyter_spark/handlers.py"", line 8, in <module>; Mar 01 19:59:04 dk-m python[5149]: class SparkHandler(IPythonHandler):; Mar 01 19:59:04 dk-m python[5149]: File ""/opt/conda/lib/python3.6/site-packages/jupyter_spark/handlers.py"", line 13, in SparkHandler; Mar 01 19:59:04 dk-m python[5149]: @tornado.web.asynchronous; Mar 01 19:59:04 dk-m python[5149]: AttributeError: module 'tornado.web' has no attribute 'asynchronous'; ```. It appears that Jupyter starts even though on",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5505
https://github.com/hail-is/hail/issues/5505:803,Security,authenticat,authentication,803,"Appears unrelated to hail version. JAR and ZIP:; ```; gs://hail-common/builds/0.2/jars/hail-0.2-3b1cb0772301-Spark-2.2.0.jar; gs://hail-common/builds/0.2/python/hail-0.2-3b1cb0772301.zip; ```. In Google Chrome we see 404s for; ```; GET http://localhost:8123/spark/api/v1/applications; ```; which happened repeatedly if you try to evaluate a cell. On the leader node of the spark cluster, `journalctl -u jupyter` shows:; ```; -- Logs begin at Fri 2019-03-01 19:54:49 UTC, end at Fri 2019-03-01 20:11:51 UTC. --; Mar 01 19:59:03 dk-m systemd[1]: Started Jupyter Notebook.; Mar 01 19:59:04 dk-m python[5149]: [I 19:59:04.630 NotebookApp] Writing notebook server cookie secret to /root/.local/share/jupyter/runtime/notebook_cookie_secret; Mar 01 19:59:04 dk-m python[5149]: [W 19:59:04.796 NotebookApp] All authentication is disabled. Anyone who can connect to this server will be able to run code.; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.802 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /etc/jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.803 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /usr/local/etc/jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.804 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /opt/conda/etc/jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.804 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /root/.jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [W 19:59:04.904 NotebookApp] Error loading server extension jupyter_spark; Mar 01 19:59:04 dk-m python[5149]: Traceback (most recent call last):; Mar 01 19:59:04 dk-m python[5149]: File ""/opt/conda/lib/python3.6/site-packages/notebook/notebookapp.p",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5505
https://github.com/hail-is/hail/issues/5505:428,Testability,Log,Logs,428,"Appears unrelated to hail version. JAR and ZIP:; ```; gs://hail-common/builds/0.2/jars/hail-0.2-3b1cb0772301-Spark-2.2.0.jar; gs://hail-common/builds/0.2/python/hail-0.2-3b1cb0772301.zip; ```. In Google Chrome we see 404s for; ```; GET http://localhost:8123/spark/api/v1/applications; ```; which happened repeatedly if you try to evaluate a cell. On the leader node of the spark cluster, `journalctl -u jupyter` shows:; ```; -- Logs begin at Fri 2019-03-01 19:54:49 UTC, end at Fri 2019-03-01 20:11:51 UTC. --; Mar 01 19:59:03 dk-m systemd[1]: Started Jupyter Notebook.; Mar 01 19:59:04 dk-m python[5149]: [I 19:59:04.630 NotebookApp] Writing notebook server cookie secret to /root/.local/share/jupyter/runtime/notebook_cookie_secret; Mar 01 19:59:04 dk-m python[5149]: [W 19:59:04.796 NotebookApp] All authentication is disabled. Anyone who can connect to this server will be able to run code.; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.802 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /etc/jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.803 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /usr/local/etc/jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.804 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /opt/conda/etc/jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.804 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /root/.jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [W 19:59:04.904 NotebookApp] Error loading server extension jupyter_spark; Mar 01 19:59:04 dk-m python[5149]: Traceback (most recent call last):; Mar 01 19:59:04 dk-m python[5149]: File ""/opt/conda/lib/python3.6/site-packages/notebook/notebookapp.p",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5505
https://github.com/hail-is/hail/issues/5506:17,Usability,simpl,simplify,17,There is room to simplify rules,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5506
https://github.com/hail-is/hail/pull/5507:440,Deployability,pipeline,pipeline,440,"There was some real bad capture/broadcast issues in VCFsReader. I made the following changes:. - import_vcfs requires the signature of all files to be the same,; - compute the type once from the first file,; - verify the types agree when parallelizing over files computing the partitions,; - always broadcast the header lines (which can be large). This reduced the DAGScheduler RDD broadcast by about 4x (6MB => 1.4MB) on a simple 10-input pipeline of import_vcfs/transform_one/combine/write_multi.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5507
https://github.com/hail-is/hail/pull/5507:353,Energy Efficiency,reduce,reduced,353,"There was some real bad capture/broadcast issues in VCFsReader. I made the following changes:. - import_vcfs requires the signature of all files to be the same,; - compute the type once from the first file,; - verify the types agree when parallelizing over files computing the partitions,; - always broadcast the header lines (which can be large). This reduced the DAGScheduler RDD broadcast by about 4x (6MB => 1.4MB) on a simple 10-input pipeline of import_vcfs/transform_one/combine/write_multi.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5507
https://github.com/hail-is/hail/pull/5507:424,Usability,simpl,simple,424,"There was some real bad capture/broadcast issues in VCFsReader. I made the following changes:. - import_vcfs requires the signature of all files to be the same,; - compute the type once from the first file,; - verify the types agree when parallelizing over files computing the partitions,; - always broadcast the header lines (which can be large). This reduced the DAGScheduler RDD broadcast by about 4x (6MB => 1.4MB) on a simple 10-input pipeline of import_vcfs/transform_one/combine/write_multi.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5507
https://github.com/hail-is/hail/pull/5508:69,Testability,test,tests,69,I did not rebuild the shared libraries. I do not understand why; the tests did not fail. I will look into that separately. This; should unblock TJ.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5508
https://github.com/hail-is/hail/pull/5509:552,Availability,down,down,552,"Stacked on: https://github.com/hail-is/hail/pull/5507. Drops one broadcast from my test dataset from 1.4MB => 300KB (5x). I think that corresponds to the parallelize for writeSplitSpecs, which is now constant (won't scale according to the number of inputs). The RDD actually doing the writing, the OriginUnionRDD, still scales linearly. I think that's inevitable unless we do the LightweightContextRVDDistributedArray thing I mentioned on Zulip since we necessarily allocate at least one RDD per input. It might still be possible to push the constants down. The point of this change is to avoid capturing the OriginUnionRDD partitions inside the map step. I did this essentially by turning OriginUnionRDD into a union with ""mapPartitionsWithOriginIndex"". I think it might be wroth trying to re-run it after this goes in. Between this one and the last one, there are some pretty big memory/broadcast savings here.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5509
https://github.com/hail-is/hail/pull/5509:466,Energy Efficiency,allocate,allocate,466,"Stacked on: https://github.com/hail-is/hail/pull/5507. Drops one broadcast from my test dataset from 1.4MB => 300KB (5x). I think that corresponds to the parallelize for writeSplitSpecs, which is now constant (won't scale according to the number of inputs). The RDD actually doing the writing, the OriginUnionRDD, still scales linearly. I think that's inevitable unless we do the LightweightContextRVDDistributedArray thing I mentioned on Zulip since we necessarily allocate at least one RDD per input. It might still be possible to push the constants down. The point of this change is to avoid capturing the OriginUnionRDD partitions inside the map step. I did this essentially by turning OriginUnionRDD into a union with ""mapPartitionsWithOriginIndex"". I think it might be wroth trying to re-run it after this goes in. Between this one and the last one, there are some pretty big memory/broadcast savings here.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5509
https://github.com/hail-is/hail/pull/5509:589,Safety,avoid,avoid,589,"Stacked on: https://github.com/hail-is/hail/pull/5507. Drops one broadcast from my test dataset from 1.4MB => 300KB (5x). I think that corresponds to the parallelize for writeSplitSpecs, which is now constant (won't scale according to the number of inputs). The RDD actually doing the writing, the OriginUnionRDD, still scales linearly. I think that's inevitable unless we do the LightweightContextRVDDistributedArray thing I mentioned on Zulip since we necessarily allocate at least one RDD per input. It might still be possible to push the constants down. The point of this change is to avoid capturing the OriginUnionRDD partitions inside the map step. I did this essentially by turning OriginUnionRDD into a union with ""mapPartitionsWithOriginIndex"". I think it might be wroth trying to re-run it after this goes in. Between this one and the last one, there are some pretty big memory/broadcast savings here.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5509
https://github.com/hail-is/hail/pull/5509:83,Testability,test,test,83,"Stacked on: https://github.com/hail-is/hail/pull/5507. Drops one broadcast from my test dataset from 1.4MB => 300KB (5x). I think that corresponds to the parallelize for writeSplitSpecs, which is now constant (won't scale according to the number of inputs). The RDD actually doing the writing, the OriginUnionRDD, still scales linearly. I think that's inevitable unless we do the LightweightContextRVDDistributedArray thing I mentioned on Zulip since we necessarily allocate at least one RDD per input. It might still be possible to push the constants down. The point of this change is to avoid capturing the OriginUnionRDD partitions inside the map step. I did this essentially by turning OriginUnionRDD into a union with ""mapPartitionsWithOriginIndex"". I think it might be wroth trying to re-run it after this goes in. Between this one and the last one, there are some pretty big memory/broadcast savings here.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5509
https://github.com/hail-is/hail/pull/5511:0,Deployability,Update,Updated,0,Updated incorrect doc and added test to verify that if you try to export a sparse block it writes a file of zeros. resolves #5500,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5511
https://github.com/hail-is/hail/pull/5511:32,Testability,test,test,32,Updated incorrect doc and added test to verify that if you try to export a sparse block it writes a file of zeros. resolves #5500,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5511
https://github.com/hail-is/hail/pull/5512:188,Deployability,configurat,configuration,188,Stacked on: https://github.com/hail-is/hail/pull/5509. Broadcast once and reuse the same broadcast where. Never serialize (except via broadcast). Same pattern as RVDPartitioner and Hadoop configuration.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5512
https://github.com/hail-is/hail/pull/5512:188,Modifiability,config,configuration,188,Stacked on: https://github.com/hail-is/hail/pull/5509. Broadcast once and reuse the same broadcast where. Never serialize (except via broadcast). Same pattern as RVDPartitioner and Hadoop configuration.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5512
https://github.com/hail-is/hail/pull/5516:135,Performance,load,load,135,- Added convenience method to export each block of a BlockMatrix into its own file using `export_rectangles`; - Added static method to load rectangle files into a NumPy NDArray,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5516
https://github.com/hail-is/hail/pull/5518:90,Deployability,deploy,deployed,90,I've already applied this change to the cluster since k8s-config.yaml isn't actually auto-deployed yet.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5518
https://github.com/hail-is/hail/pull/5518:58,Modifiability,config,config,58,I've already applied this change to the cluster since k8s-config.yaml isn't actually auto-deployed yet.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5518
https://github.com/hail-is/hail/issues/5519:82,Safety,timeout,timeout,82,"We should probably make batch faster, but a quicker fix is to figure out what the timeout is set to 5 seconds and raise it. The default batch client timeout is 60 seconds. We should probably not send all the logs in response to list_jobs.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5519
https://github.com/hail-is/hail/issues/5519:149,Safety,timeout,timeout,149,"We should probably make batch faster, but a quicker fix is to figure out what the timeout is set to 5 seconds and raise it. The default batch client timeout is 60 seconds. We should probably not send all the logs in response to list_jobs.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5519
https://github.com/hail-is/hail/issues/5519:208,Testability,log,logs,208,"We should probably make batch faster, but a quicker fix is to figure out what the timeout is set to 5 seconds and raise it. The default batch client timeout is 60 seconds. We should probably not send all the logs in response to list_jobs.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5519
https://github.com/hail-is/hail/pull/5522:343,Integrability,Depend,Dependencies,343,- Implemented support for blockmatrix types in the service backend; - All BlockMatrix tests except those for `sparsify` methods and `svd` (which uses `sparsify`) succeed locally on the service backend; - Removed test decorators for tests that succeeded locally; - Added a single test out of the `BlockMatrix` suite to `test-apiserver.sh`. ### Dependencies on BlockMatrix; - `Statgen` doesn't seem to have any `BlockMatrix`-based Java dependencies; - `linear-mixed-model` uses `svd` and `RowMatrix`,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5522
https://github.com/hail-is/hail/pull/5522:434,Integrability,depend,dependencies,434,- Implemented support for blockmatrix types in the service backend; - All BlockMatrix tests except those for `sparsify` methods and `svd` (which uses `sparsify`) succeed locally on the service backend; - Removed test decorators for tests that succeeded locally; - Added a single test out of the `BlockMatrix` suite to `test-apiserver.sh`. ### Dependencies on BlockMatrix; - `Statgen` doesn't seem to have any `BlockMatrix`-based Java dependencies; - `linear-mixed-model` uses `svd` and `RowMatrix`,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5522
https://github.com/hail-is/hail/pull/5522:86,Testability,test,tests,86,- Implemented support for blockmatrix types in the service backend; - All BlockMatrix tests except those for `sparsify` methods and `svd` (which uses `sparsify`) succeed locally on the service backend; - Removed test decorators for tests that succeeded locally; - Added a single test out of the `BlockMatrix` suite to `test-apiserver.sh`. ### Dependencies on BlockMatrix; - `Statgen` doesn't seem to have any `BlockMatrix`-based Java dependencies; - `linear-mixed-model` uses `svd` and `RowMatrix`,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5522
https://github.com/hail-is/hail/pull/5522:212,Testability,test,test,212,- Implemented support for blockmatrix types in the service backend; - All BlockMatrix tests except those for `sparsify` methods and `svd` (which uses `sparsify`) succeed locally on the service backend; - Removed test decorators for tests that succeeded locally; - Added a single test out of the `BlockMatrix` suite to `test-apiserver.sh`. ### Dependencies on BlockMatrix; - `Statgen` doesn't seem to have any `BlockMatrix`-based Java dependencies; - `linear-mixed-model` uses `svd` and `RowMatrix`,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5522
https://github.com/hail-is/hail/pull/5522:232,Testability,test,tests,232,- Implemented support for blockmatrix types in the service backend; - All BlockMatrix tests except those for `sparsify` methods and `svd` (which uses `sparsify`) succeed locally on the service backend; - Removed test decorators for tests that succeeded locally; - Added a single test out of the `BlockMatrix` suite to `test-apiserver.sh`. ### Dependencies on BlockMatrix; - `Statgen` doesn't seem to have any `BlockMatrix`-based Java dependencies; - `linear-mixed-model` uses `svd` and `RowMatrix`,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5522
https://github.com/hail-is/hail/pull/5522:279,Testability,test,test,279,- Implemented support for blockmatrix types in the service backend; - All BlockMatrix tests except those for `sparsify` methods and `svd` (which uses `sparsify`) succeed locally on the service backend; - Removed test decorators for tests that succeeded locally; - Added a single test out of the `BlockMatrix` suite to `test-apiserver.sh`. ### Dependencies on BlockMatrix; - `Statgen` doesn't seem to have any `BlockMatrix`-based Java dependencies; - `linear-mixed-model` uses `svd` and `RowMatrix`,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5522
https://github.com/hail-is/hail/pull/5522:319,Testability,test,test-apiserver,319,- Implemented support for blockmatrix types in the service backend; - All BlockMatrix tests except those for `sparsify` methods and `svd` (which uses `sparsify`) succeed locally on the service backend; - Removed test decorators for tests that succeeded locally; - Added a single test out of the `BlockMatrix` suite to `test-apiserver.sh`. ### Dependencies on BlockMatrix; - `Statgen` doesn't seem to have any `BlockMatrix`-based Java dependencies; - `linear-mixed-model` uses `svd` and `RowMatrix`,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5522
https://github.com/hail-is/hail/pull/5527:25,Availability,down,down,25,"This astronomically cuts down on IR duplication. The following is the IR for one invocation to TableMultiWayZipJoin in the old pipeline, this would be replicated for every vcf imported:; ```; (CastMatrixToTable ""__entries"" ""__cols""; (MatrixMapEntries; (MatrixMapRows; (MatrixMapEntries; (MatrixMapRows; (MatrixMapEntries; (MatrixLiteral); (InsertFields; (SelectFields (AD DP GQ GT MIN_DP PGT PID PL SB); (Ref g)); None; (END; (GetField END; (GetField info; (Ref va)))); (BaseQRankSum; (GetField BaseQRankSum; (GetField info; (Ref va)))); (ClippingRankSum; (GetField ClippingRankSum; (GetField info; (Ref va)))); (MQ; (GetField MQ; (GetField info; (Ref va)))); (MQRankSum; (GetField MQRankSum; (GetField info; (Ref va)))); (ReadPosRankSum; (GetField ReadPosRankSum; (GetField info; (Ref va)))); (LGT; (GetField GT; (Ref g))); (LAD; (If; (ApplyComparisonOp EQ; (ApplyIR indexArray; (GetField alleles; (Ref va)); (I32 -1)); (Str ""<NON_REF>"")); (ApplyIR `[:*]`; (GetField AD; (Ref g)); (I32 -1)); (GetField AD; (Ref g)))); (LPL; (If; (ApplyComparisonOp EQ; (ApplyIR indexArray; (GetField alleles; (Ref va)); (I32 -1)); (Str ""<NON_REF>"")); (If; (ApplyComparisonOp GT; (ArrayLen; (GetField alleles; (Ref va))); (I32 2)); (ApplyIR `[:*]`; (GetField PL; (Ref g)); (ApplyUnaryPrimOp Negate; (ArrayLen; (GetField alleles; (Ref va))))); (NA Array[Int32])); (If; (ApplyComparisonOp GT; (ArrayLen; (GetField alleles; (Ref va))); (I32 1)); (GetField PL; (Ref g)); (NA Array[Int32])))); (LPGT; (GetField PGT; (Ref g))); (RGQ; (If; (ApplyComparisonOp EQ; (ApplyIR indexArray; (GetField alleles; (Ref va)); (I32 -1)); (Str ""<NON_REF>"")); (ApplyIR indexArray; (GetField PL; (Ref g)); (Apply unphasedDiploidGtIndex; (Apply Call; (I32 0); (ApplyBinaryPrimOp Subtract; (ArrayLen; (GetField alleles; (Ref va))); (I32 1)); (False)))); (NA Int32))))); (InsertFields; (SelectFields (locus alleles rsid qual filters info); (Ref va)); None; (alleles; (If; (ApplyComparisonOp EQ; (ApplyIR indexArray; (GetField alleles; (Ref va))",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5527
https://github.com/hail-is/hail/pull/5527:127,Deployability,pipeline,pipeline,127," The following is the IR for one invocation to TableMultiWayZipJoin in the old pipeline, this would be replicated for every vcf imported:; ```; (CastMatrixToTable ""__entries"" ""__cols""; (MatrixMapEntries; (MatrixMapRows; (MatrixMapEntries; (MatrixMapRows; (MatrixMapEntries; (MatrixLiteral); (InsertFields; (SelectFields (AD DP GQ GT MIN_DP PGT PID PL SB); (Ref g)); None; (END; (GetField END; (GetField info; (Ref va)))); (BaseQRankSum; (GetField BaseQRankSum; (GetField info; (Ref va)))); (ClippingRankSum; (GetField ClippingRankSum; (GetField info; (Ref va)))); (MQ; (GetField MQ; (GetField info; (Ref va)))); (MQRankSum; (GetField MQRankSum; (GetField info; (Ref va)))); (ReadPosRankSum; (GetField ReadPosRankSum; (GetField info; (Ref va)))); (LGT; (GetField GT; (Ref g))); (LAD; (If; (ApplyComparisonOp EQ; (ApplyIR indexArray; (GetField alleles; (Ref va)); (I32 -1)); (Str ""<NON_REF>"")); (ApplyIR `[:*]`; (GetField AD; (Ref g)); (I32 -1)); (GetField AD; (Ref g)))); (LPL; (If; (ApplyComparisonOp EQ; (ApplyIR indexArray; (GetField alleles; (Ref va)); (I32 -1)); (Str ""<NON_REF>"")); (If; (ApplyComparisonOp GT; (ArrayLen; (GetField alleles; (Ref va))); (I32 2)); (ApplyIR `[:*]`; (GetField PL; (Ref g)); (ApplyUnaryPrimOp Negate; (ArrayLen; (GetField alleles; (Ref va))))); (NA Array[Int32])); (If; (ApplyComparisonOp GT; (ArrayLen; (GetField alleles; (Ref va))); (I32 1)); (GetField PL; (Ref g)); (NA Array[Int32])))); (LPGT; (GetField PGT; (Ref g))); (RGQ; (If; (ApplyComparisonOp EQ; (ApplyIR indexArray; (GetField alleles; (Ref va)); (I32 -1)); (Str ""<NON_REF>"")); (ApplyIR indexArray; (GetField PL; (Ref g)); (Apply unphasedDiploidGtIndex; (Apply Call; (I32 0); (ApplyBinaryPrimOp Subtract; (ArrayLen; (GetField alleles; (Ref va))); (I32 1)); (False)))); (NA Int32))))); (InsertFields; (SelectFields (locus alleles rsid qual filters info); (Ref va)); None; (alleles; (If; (ApplyComparisonOp EQ; (ApplyIR indexArray; (GetField alleles; (Ref va))",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5527
https://github.com/hail-is/hail/pull/5532:18,Testability,test,tests,18,All the matrix IR tests were in a method in `TableIRTests`. Moved them out into a separate class to stay consistent.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5532
https://github.com/hail-is/hail/pull/5538:173,Deployability,update,updates,173,"Stacks on #5526. Once that commit goes in, the only changes will be to the /wait route, which will now issue a Kubernetes watcher, `notebook-state.html` to support JS state updates (and some minor changes to the organization of the notebook reporting UI). This checks whether the route may be reached both for jobs that are Ready (at page refresh), and those that have just transitioned into ready state (as alerted by the websocket route). cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5538
https://github.com/hail-is/hail/pull/5538:81,Integrability,rout,route,81,"Stacks on #5526. Once that commit goes in, the only changes will be to the /wait route, which will now issue a Kubernetes watcher, `notebook-state.html` to support JS state updates (and some minor changes to the organization of the notebook reporting UI). This checks whether the route may be reached both for jobs that are Ready (at page refresh), and those that have just transitioned into ready state (as alerted by the websocket route). cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5538
https://github.com/hail-is/hail/pull/5538:280,Integrability,rout,route,280,"Stacks on #5526. Once that commit goes in, the only changes will be to the /wait route, which will now issue a Kubernetes watcher, `notebook-state.html` to support JS state updates (and some minor changes to the organization of the notebook reporting UI). This checks whether the route may be reached both for jobs that are Ready (at page refresh), and those that have just transitioned into ready state (as alerted by the websocket route). cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5538
https://github.com/hail-is/hail/pull/5538:433,Integrability,rout,route,433,"Stacks on #5526. Once that commit goes in, the only changes will be to the /wait route, which will now issue a Kubernetes watcher, `notebook-state.html` to support JS state updates (and some minor changes to the organization of the notebook reporting UI). This checks whether the route may be reached both for jobs that are Ready (at page refresh), and those that have just transitioned into ready state (as alerted by the websocket route). cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5538
https://github.com/hail-is/hail/pull/5540:171,Deployability,update,updates,171,"Stacks on #5526. Once that commit goes in, the only changes will be to the /wait route, which will now issue a Kubernetes watcher, notebook-state.html to support JS state updates (and some minor changes to the organization of the notebook reporting UI). Relevant commit: ; 4f4e2b6f875e33da5787f665de1400f0f00a3623. This checks whether the route may be reached both for jobs that are Ready (at page refresh), and those that have just transitioned into ready state (as alerted by the websocket route). Next update will generalize the UI to N notebooks, to handle the case that internet-synchrony issues cause 2 non-deleted notebooks to be generated. This still needs a bit of work; when reach call fails due to 502, it continues issuing 502's, requiring a refresh. First attempts to cache bust, client side and nginx-side failed. cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5540
https://github.com/hail-is/hail/pull/5540:505,Deployability,update,update,505,"Stacks on #5526. Once that commit goes in, the only changes will be to the /wait route, which will now issue a Kubernetes watcher, notebook-state.html to support JS state updates (and some minor changes to the organization of the notebook reporting UI). Relevant commit: ; 4f4e2b6f875e33da5787f665de1400f0f00a3623. This checks whether the route may be reached both for jobs that are Ready (at page refresh), and those that have just transitioned into ready state (as alerted by the websocket route). Next update will generalize the UI to N notebooks, to handle the case that internet-synchrony issues cause 2 non-deleted notebooks to be generated. This still needs a bit of work; when reach call fails due to 502, it continues issuing 502's, requiring a refresh. First attempts to cache bust, client side and nginx-side failed. cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5540
https://github.com/hail-is/hail/pull/5540:81,Integrability,rout,route,81,"Stacks on #5526. Once that commit goes in, the only changes will be to the /wait route, which will now issue a Kubernetes watcher, notebook-state.html to support JS state updates (and some minor changes to the organization of the notebook reporting UI). Relevant commit: ; 4f4e2b6f875e33da5787f665de1400f0f00a3623. This checks whether the route may be reached both for jobs that are Ready (at page refresh), and those that have just transitioned into ready state (as alerted by the websocket route). Next update will generalize the UI to N notebooks, to handle the case that internet-synchrony issues cause 2 non-deleted notebooks to be generated. This still needs a bit of work; when reach call fails due to 502, it continues issuing 502's, requiring a refresh. First attempts to cache bust, client side and nginx-side failed. cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5540
https://github.com/hail-is/hail/pull/5540:339,Integrability,rout,route,339,"Stacks on #5526. Once that commit goes in, the only changes will be to the /wait route, which will now issue a Kubernetes watcher, notebook-state.html to support JS state updates (and some minor changes to the organization of the notebook reporting UI). Relevant commit: ; 4f4e2b6f875e33da5787f665de1400f0f00a3623. This checks whether the route may be reached both for jobs that are Ready (at page refresh), and those that have just transitioned into ready state (as alerted by the websocket route). Next update will generalize the UI to N notebooks, to handle the case that internet-synchrony issues cause 2 non-deleted notebooks to be generated. This still needs a bit of work; when reach call fails due to 502, it continues issuing 502's, requiring a refresh. First attempts to cache bust, client side and nginx-side failed. cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5540
https://github.com/hail-is/hail/pull/5540:492,Integrability,rout,route,492,"Stacks on #5526. Once that commit goes in, the only changes will be to the /wait route, which will now issue a Kubernetes watcher, notebook-state.html to support JS state updates (and some minor changes to the organization of the notebook reporting UI). Relevant commit: ; 4f4e2b6f875e33da5787f665de1400f0f00a3623. This checks whether the route may be reached both for jobs that are Ready (at page refresh), and those that have just transitioned into ready state (as alerted by the websocket route). Next update will generalize the UI to N notebooks, to handle the case that internet-synchrony issues cause 2 non-deleted notebooks to be generated. This still needs a bit of work; when reach call fails due to 502, it continues issuing 502's, requiring a refresh. First attempts to cache bust, client side and nginx-side failed. cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5540
https://github.com/hail-is/hail/pull/5540:781,Performance,cache,cache,781,"Stacks on #5526. Once that commit goes in, the only changes will be to the /wait route, which will now issue a Kubernetes watcher, notebook-state.html to support JS state updates (and some minor changes to the organization of the notebook reporting UI). Relevant commit: ; 4f4e2b6f875e33da5787f665de1400f0f00a3623. This checks whether the route may be reached both for jobs that are Ready (at page refresh), and those that have just transitioned into ready state (as alerted by the websocket route). Next update will generalize the UI to N notebooks, to handle the case that internet-synchrony issues cause 2 non-deleted notebooks to be generated. This still needs a bit of work; when reach call fails due to 502, it continues issuing 502's, requiring a refresh. First attempts to cache bust, client side and nginx-side failed. cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5540
https://github.com/hail-is/hail/pull/5548:151,Availability,error,error,151,We weren't checking for a key even though the docs said the table needed to be keyed. This led to undefined behavior for `distinct`. Not sure what the error mode for `collect_by_key` was.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5548
https://github.com/hail-is/hail/pull/5549:221,Testability,assert,assert,221,"stacks on #5540. Will re-assign when notebook2-no-svc is merged. This is needed because, unwittingly due to networking issues, or just having 2 open sessions, two notebooks could be created. If so, this would trigger the assert len(notebooks) == 1 and prevent the user from using the service again. Adresses: https://github.com/hail-is/hail/issues/5487. Relevant commits:; https://github.com/hail-is/hail/pull/5549/commits/901b1e16daeda9209424319ebbeea2b451862876. Once that goes in, a few small changes to notebook.py to store multiple notebooks in a session (and pass the notebooks array to templates), and then loop over the array in notebook-state.html (which retains all of the same per-notebook ui). . I also removed the explicit reachability check on refresh for {% if ready %}. I believe when you .watch(list_namespaced_pods), the first request gets up-to-date status, so no real need to separately check reachability on refresh (we could back port this to #5540 if needed). <img width=""607"" alt=""screen shot 2019-03-06 at 3 58 32 pm"" src=""https://user-images.githubusercontent.com/5543229/53919928-b28d8780-4028-11e9-8665-4d2a7c10c8cf.png"">. cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5549
https://github.com/hail-is/hail/pull/5556:119,Safety,safe,safe,119,"Tim, like we discussed, thanks for the suggestion. No login for workshop users, we skip auth0 altogether. Not quite as safe, but probably not a big deal provided https, a long-enough password. . I also removed the image picker, because it's I think more implementation detail than workshop users need (and it's not particularly styled). and uses ibg2019. cc @danking @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5556
https://github.com/hail-is/hail/pull/5556:183,Security,password,password,183,"Tim, like we discussed, thanks for the suggestion. No login for workshop users, we skip auth0 altogether. Not quite as safe, but probably not a big deal provided https, a long-enough password. . I also removed the image picker, because it's I think more implementation detail than workshop users need (and it's not particularly styled). and uses ibg2019. cc @danking @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5556
https://github.com/hail-is/hail/pull/5556:54,Testability,log,login,54,"Tim, like we discussed, thanks for the suggestion. No login for workshop users, we skip auth0 altogether. Not quite as safe, but probably not a big deal provided https, a long-enough password. . I also removed the image picker, because it's I think more implementation detail than workshop users need (and it's not particularly styled). and uses ibg2019. cc @danking @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5556
https://github.com/hail-is/hail/issues/5559:693,Availability,avail,available,693,"I think we need to figure out how to get cblas on the broad cluster. ```; # use UGER; # ish -l os=RedHat7; # use Anaconda3; # use Java-1.8; # use OpenBLAS; # source activate hail; # ipython; In [1]: import hail as hl . In [2]: mt = hl.balding_nichols_model(3, 100, 100) ; Initializing Spark and Hail with default parameters...; using hail jar at /home/unix/dking/.conda/envs/hail/lib/python3.7/site-packages/hail/hail-all-spark.jar; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Running on Apache Spark version 2.2.3; SparkUI available at http://10.200.100.39:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.11-cf54f08305d1; LOGGING: writing to /home/unix/dking/hail-20190307-1908-0.2.11-cf54f08305d1.log; 2019-03-07 19:08:30 Hail: INFO: balding_nichols_model: generating genotypes for 3 populations, 100 samples, and 100 variants...; ^[[A; In [3]: t = hl.linear_regression_rows(x=mt.GT.n_alt_alleles(), y=mt.pop, covariates=[1]) ; [Stage 0:============================================> (6 + 1) / 8]2019-03-07 19:08:39 Hail: INFO: Coerced sorted dataset; 2019-03-07 19:08:40 Hail: INFO: linear_regression_rows: running on 100 samples for 1 response variable y,; with input variable x, and 1 additional covariate...; /broad/software/free/Linux/redhat_7_x86_64/pkgs/jdk1.8.0_181/bin/java: symbol lookup error: /tmp/jniloader1327638724610654731netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemm; ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/home/unix/dking/.conda/envs/hail/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1159, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty. During handling of the above exception, another exception",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5559
https://github.com/hail-is/hail/issues/5559:1513,Availability,error,error,1513,"evel to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Running on Apache Spark version 2.2.3; SparkUI available at http://10.200.100.39:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.11-cf54f08305d1; LOGGING: writing to /home/unix/dking/hail-20190307-1908-0.2.11-cf54f08305d1.log; 2019-03-07 19:08:30 Hail: INFO: balding_nichols_model: generating genotypes for 3 populations, 100 samples, and 100 variants...; ^[[A; In [3]: t = hl.linear_regression_rows(x=mt.GT.n_alt_alleles(), y=mt.pop, covariates=[1]) ; [Stage 0:============================================> (6 + 1) / 8]2019-03-07 19:08:39 Hail: INFO: Coerced sorted dataset; 2019-03-07 19:08:40 Hail: INFO: linear_regression_rows: running on 100 samples for 1 response variable y,; with input variable x, and 1 additional covariate...; /broad/software/free/Linux/redhat_7_x86_64/pkgs/jdk1.8.0_181/bin/java: symbol lookup error: /tmp/jniloader1327638724610654731netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemm; ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/home/unix/dking/.conda/envs/hail/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1159, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/home/unix/dking/.conda/envs/hail/lib/python3.7/site-packages/py4j/java_gateway.py"", line 985, in send_command; response = connection.send_command(command); File ""/home/unix/dking/.conda/envs/hail/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1164, in send_command; ""Error while receiving"", e, proto.ERROR_ON_RECEIVE); py4j.protocol.Py4JNetworkError: Error while receiving; ---------------------------------------------------------------------------; Py4JError Traceback",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5559
https://github.com/hail-is/hail/issues/5559:1622,Availability,ERROR,ERROR,1622,"ogLevel(newLevel).; Running on Apache Spark version 2.2.3; SparkUI available at http://10.200.100.39:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.11-cf54f08305d1; LOGGING: writing to /home/unix/dking/hail-20190307-1908-0.2.11-cf54f08305d1.log; 2019-03-07 19:08:30 Hail: INFO: balding_nichols_model: generating genotypes for 3 populations, 100 samples, and 100 variants...; ^[[A; In [3]: t = hl.linear_regression_rows(x=mt.GT.n_alt_alleles(), y=mt.pop, covariates=[1]) ; [Stage 0:============================================> (6 + 1) / 8]2019-03-07 19:08:39 Hail: INFO: Coerced sorted dataset; 2019-03-07 19:08:40 Hail: INFO: linear_regression_rows: running on 100 samples for 1 response variable y,; with input variable x, and 1 additional covariate...; /broad/software/free/Linux/redhat_7_x86_64/pkgs/jdk1.8.0_181/bin/java: symbol lookup error: /tmp/jniloader1327638724610654731netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemm; ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/home/unix/dking/.conda/envs/hail/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1159, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/home/unix/dking/.conda/envs/hail/lib/python3.7/site-packages/py4j/java_gateway.py"", line 985, in send_command; response = connection.send_command(command); File ""/home/unix/dking/.conda/envs/hail/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1164, in send_command; ""Error while receiving"", e, proto.ERROR_ON_RECEIVE); py4j.protocol.Py4JNetworkError: Error while receiving; ---------------------------------------------------------------------------; Py4JError Traceback (most recent call last); <ipython-input-3-9b1033d0ac55> in <module>; ----> 1 t = hl.linear",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5559
https://github.com/hail-is/hail/issues/5559:2332,Availability,Error,Error,2332,"on 100 samples for 1 response variable y,; with input variable x, and 1 additional covariate...; /broad/software/free/Linux/redhat_7_x86_64/pkgs/jdk1.8.0_181/bin/java: symbol lookup error: /tmp/jniloader1327638724610654731netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemm; ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/home/unix/dking/.conda/envs/hail/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1159, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/home/unix/dking/.conda/envs/hail/lib/python3.7/site-packages/py4j/java_gateway.py"", line 985, in send_command; response = connection.send_command(command); File ""/home/unix/dking/.conda/envs/hail/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1164, in send_command; ""Error while receiving"", e, proto.ERROR_ON_RECEIVE); py4j.protocol.Py4JNetworkError: Error while receiving; ---------------------------------------------------------------------------; Py4JError Traceback (most recent call last); <ipython-input-3-9b1033d0ac55> in <module>; ----> 1 t = hl.linear_regression_rows(x=mt.GT.n_alt_alleles(), y=mt.pop, covariates=[1]). <decorator-gen-1332> in linear_regression_rows(y, x, covariates, block_size, pass_through). ~/.conda/envs/hail/lib/python3.7/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 559 def wrapper(__original_func, *args, **kwargs):; 560 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 561 return __original_func(*args_, **kwargs_); 562 ; 563 return wrapper. ~/.conda/envs/hail/lib/python3.7/site-packages/hail/methods/statgen.py in linear_regression_rows(y, x, covariates, block_size, pass_through); 434 ht_result = ht_result.annotate(**{f: ht_result[f][0] for f ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5559
https://github.com/hail-is/hail/issues/5559:2416,Availability,Error,Error,2416,"kup error: /tmp/jniloader1327638724610654731netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemm; ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/home/unix/dking/.conda/envs/hail/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1159, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/home/unix/dking/.conda/envs/hail/lib/python3.7/site-packages/py4j/java_gateway.py"", line 985, in send_command; response = connection.send_command(command); File ""/home/unix/dking/.conda/envs/hail/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1164, in send_command; ""Error while receiving"", e, proto.ERROR_ON_RECEIVE); py4j.protocol.Py4JNetworkError: Error while receiving; ---------------------------------------------------------------------------; Py4JError Traceback (most recent call last); <ipython-input-3-9b1033d0ac55> in <module>; ----> 1 t = hl.linear_regression_rows(x=mt.GT.n_alt_alleles(), y=mt.pop, covariates=[1]). <decorator-gen-1332> in linear_regression_rows(y, x, covariates, block_size, pass_through). ~/.conda/envs/hail/lib/python3.7/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 559 def wrapper(__original_func, *args, **kwargs):; 560 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 561 return __original_func(*args_, **kwargs_); 562 ; 563 return wrapper. ~/.conda/envs/hail/lib/python3.7/site-packages/hail/methods/statgen.py in linear_regression_rows(y, x, covariates, block_size, pass_through); 434 ht_result = ht_result.annotate(**{f: ht_result[f][0] for f in fields}); 435 ; --> 436 return ht_result.persist(); 437 ; 438 . <decorator-gen-918> in persist(self, storage_level). ~/.conda/envs/hail/lib/python3.7/site-packages/hail/typec",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5559
https://github.com/hail-is/hail/issues/5559:5012,Availability,error,error,5012,"level); 1677 Persisted table.; 1678 """"""; -> 1679 return Env.backend().persist_table(self, storage_level); 1680 ; 1681 def unpersist(self) -> 'Table':. ~/.conda/envs/hail/lib/python3.7/site-packages/hail/backend/backend.py in persist_table(self, t, storage_level); 107 ; 108 def persist_table(self, t, storage_level):; --> 109 return Table._from_java(self._to_java_ir(t._tir).pyPersist(storage_level)); 110 ; 111 def unpersist_table(self, t):. ~/.conda/envs/hail/lib/python3.7/site-packages/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. ~/.conda/envs/hail/lib/python3.7/site-packages/hail/utils/java.py in deco(*args, **kwargs); 213 import pyspark; 214 try:; --> 215 return f(*args, **kwargs); 216 except py4j.protocol.Py4JJavaError as e:; 217 s = e.java_exception.toString(). ~/.conda/envs/hail/lib/python3.7/site-packages/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name); 334 raise Py4JError(; 335 ""An error occurred while calling {0}{1}{2}"".; --> 336 format(target_id, ""."", name)); 337 else:; 338 type = answer[1]. Py4JError: An error occurred while calling o51.pyPersist; ```. ```; (hail) -bash:uger-r7-c001:~ 1037 $ find /lib /lib64/ -regex '.*blas.*' -exec ls -al \{\} \;; lrwxrwxrwx. 1 root root 16 Dec 13 2017 /lib64/libblas.so.3.4 -> libblas.so.3.4.2; lrwxrwxrwx. 1 root root 16 Dec 13 2017 /lib64/libblas.so.3 -> libblas.so.3.4.2; lrwxrwxrwx. 1 root root 16 Dec 13 2017 /lib64/libblas.so -> libblas.so.3.4.2; -rwxr-xr-x. 1 root root 364808 Mar 2 2017 /lib64/libblas.so.3.4.2; ```. ```; (hail) -bash:uger-r7-c001:~ 1034 $ objdump -T /lib64/liblapack.so.3 | grep dgemm; 0000000000000000 DF *UND*	0000000000000000 dgemm_; (hail) -bash:uger-r7-c001:~ 1035 $ objdump -T /lib64/libblas.so.3 | grep dgemm; 0000000000018850 g DF .text	0000000000000935 Base dgemm_; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5559
https://github.com/hail-is/hail/issues/5559:5140,Availability,error,error,5140,"level); 1677 Persisted table.; 1678 """"""; -> 1679 return Env.backend().persist_table(self, storage_level); 1680 ; 1681 def unpersist(self) -> 'Table':. ~/.conda/envs/hail/lib/python3.7/site-packages/hail/backend/backend.py in persist_table(self, t, storage_level); 107 ; 108 def persist_table(self, t, storage_level):; --> 109 return Table._from_java(self._to_java_ir(t._tir).pyPersist(storage_level)); 110 ; 111 def unpersist_table(self, t):. ~/.conda/envs/hail/lib/python3.7/site-packages/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. ~/.conda/envs/hail/lib/python3.7/site-packages/hail/utils/java.py in deco(*args, **kwargs); 213 import pyspark; 214 try:; --> 215 return f(*args, **kwargs); 216 except py4j.protocol.Py4JJavaError as e:; 217 s = e.java_exception.toString(). ~/.conda/envs/hail/lib/python3.7/site-packages/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name); 334 raise Py4JError(; 335 ""An error occurred while calling {0}{1}{2}"".; --> 336 format(target_id, ""."", name)); 337 else:; 338 type = answer[1]. Py4JError: An error occurred while calling o51.pyPersist; ```. ```; (hail) -bash:uger-r7-c001:~ 1037 $ find /lib /lib64/ -regex '.*blas.*' -exec ls -al \{\} \;; lrwxrwxrwx. 1 root root 16 Dec 13 2017 /lib64/libblas.so.3.4 -> libblas.so.3.4.2; lrwxrwxrwx. 1 root root 16 Dec 13 2017 /lib64/libblas.so.3 -> libblas.so.3.4.2; lrwxrwxrwx. 1 root root 16 Dec 13 2017 /lib64/libblas.so -> libblas.so.3.4.2; -rwxr-xr-x. 1 root root 364808 Mar 2 2017 /lib64/libblas.so.3.4.2; ```. ```; (hail) -bash:uger-r7-c001:~ 1034 $ objdump -T /lib64/liblapack.so.3 | grep dgemm; 0000000000000000 DF *UND*	0000000000000000 dgemm_; (hail) -bash:uger-r7-c001:~ 1035 $ objdump -T /lib64/libblas.so.3 | grep dgemm; 0000000000018850 g DF .text	0000000000000935 Base dgemm_; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5559
https://github.com/hail-is/hail/issues/5559:1885,Integrability,protocol,protocol,1885,"908-0.2.11-cf54f08305d1.log; 2019-03-07 19:08:30 Hail: INFO: balding_nichols_model: generating genotypes for 3 populations, 100 samples, and 100 variants...; ^[[A; In [3]: t = hl.linear_regression_rows(x=mt.GT.n_alt_alleles(), y=mt.pop, covariates=[1]) ; [Stage 0:============================================> (6 + 1) / 8]2019-03-07 19:08:39 Hail: INFO: Coerced sorted dataset; 2019-03-07 19:08:40 Hail: INFO: linear_regression_rows: running on 100 samples for 1 response variable y,; with input variable x, and 1 additional covariate...; /broad/software/free/Linux/redhat_7_x86_64/pkgs/jdk1.8.0_181/bin/java: symbol lookup error: /tmp/jniloader1327638724610654731netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemm; ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/home/unix/dking/.conda/envs/hail/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1159, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/home/unix/dking/.conda/envs/hail/lib/python3.7/site-packages/py4j/java_gateway.py"", line 985, in send_command; response = connection.send_command(command); File ""/home/unix/dking/.conda/envs/hail/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1164, in send_command; ""Error while receiving"", e, proto.ERROR_ON_RECEIVE); py4j.protocol.Py4JNetworkError: Error while receiving; ---------------------------------------------------------------------------; Py4JError Traceback (most recent call last); <ipython-input-3-9b1033d0ac55> in <module>; ----> 1 t = hl.linear_regression_rows(x=mt.GT.n_alt_alleles(), y=mt.pop, covariates=[1]). <decorator-gen-1332> in linear_regression_rows(y, x, covariates, block_size, pass_through). ~/.conda/envs/hail/lib/python3.7/site-packages/hail/typecheck/check.py in wrapper(__original_func, *arg",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5559
https://github.com/hail-is/hail/issues/5559:2389,Integrability,protocol,protocol,2389," x, and 1 additional covariate...; /broad/software/free/Linux/redhat_7_x86_64/pkgs/jdk1.8.0_181/bin/java: symbol lookup error: /tmp/jniloader1327638724610654731netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemm; ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/home/unix/dking/.conda/envs/hail/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1159, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/home/unix/dking/.conda/envs/hail/lib/python3.7/site-packages/py4j/java_gateway.py"", line 985, in send_command; response = connection.send_command(command); File ""/home/unix/dking/.conda/envs/hail/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1164, in send_command; ""Error while receiving"", e, proto.ERROR_ON_RECEIVE); py4j.protocol.Py4JNetworkError: Error while receiving; ---------------------------------------------------------------------------; Py4JError Traceback (most recent call last); <ipython-input-3-9b1033d0ac55> in <module>; ----> 1 t = hl.linear_regression_rows(x=mt.GT.n_alt_alleles(), y=mt.pop, covariates=[1]). <decorator-gen-1332> in linear_regression_rows(y, x, covariates, block_size, pass_through). ~/.conda/envs/hail/lib/python3.7/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 559 def wrapper(__original_func, *args, **kwargs):; 560 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 561 return __original_func(*args_, **kwargs_); 562 ; 563 return wrapper. ~/.conda/envs/hail/lib/python3.7/site-packages/hail/methods/statgen.py in linear_regression_rows(y, x, covariates, block_size, pass_through); 434 ht_result = ht_result.annotate(**{f: ht_result[f][0] for f in fields}); 435 ; --> 436 return ht_result.persist(); 437 ; 4",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5559
https://github.com/hail-is/hail/issues/5559:2861,Integrability,wrap,wrapper,2861,"r exception occurred:. Traceback (most recent call last):; File ""/home/unix/dking/.conda/envs/hail/lib/python3.7/site-packages/py4j/java_gateway.py"", line 985, in send_command; response = connection.send_command(command); File ""/home/unix/dking/.conda/envs/hail/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1164, in send_command; ""Error while receiving"", e, proto.ERROR_ON_RECEIVE); py4j.protocol.Py4JNetworkError: Error while receiving; ---------------------------------------------------------------------------; Py4JError Traceback (most recent call last); <ipython-input-3-9b1033d0ac55> in <module>; ----> 1 t = hl.linear_regression_rows(x=mt.GT.n_alt_alleles(), y=mt.pop, covariates=[1]). <decorator-gen-1332> in linear_regression_rows(y, x, covariates, block_size, pass_through). ~/.conda/envs/hail/lib/python3.7/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 559 def wrapper(__original_func, *args, **kwargs):; 560 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 561 return __original_func(*args_, **kwargs_); 562 ; 563 return wrapper. ~/.conda/envs/hail/lib/python3.7/site-packages/hail/methods/statgen.py in linear_regression_rows(y, x, covariates, block_size, pass_through); 434 ht_result = ht_result.annotate(**{f: ht_result[f][0] for f in fields}); 435 ; --> 436 return ht_result.persist(); 437 ; 438 . <decorator-gen-918> in persist(self, storage_level). ~/.conda/envs/hail/lib/python3.7/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 559 def wrapper(__original_func, *args, **kwargs):; 560 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 561 return __original_func(*args_, **kwargs_); 562 ; 563 return wrapper. ~/.conda/envs/hail/lib/python3.7/site-packages/hail/table.py in persist(self, storage_level); 1677 Persisted table.; 1678 """"""; -> 1679 return Env.backend().persist_table(self, storage_level); 1680 ; ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5559
https://github.com/hail-is/hail/issues/5559:2912,Integrability,wrap,wrapper,2912,"r exception occurred:. Traceback (most recent call last):; File ""/home/unix/dking/.conda/envs/hail/lib/python3.7/site-packages/py4j/java_gateway.py"", line 985, in send_command; response = connection.send_command(command); File ""/home/unix/dking/.conda/envs/hail/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1164, in send_command; ""Error while receiving"", e, proto.ERROR_ON_RECEIVE); py4j.protocol.Py4JNetworkError: Error while receiving; ---------------------------------------------------------------------------; Py4JError Traceback (most recent call last); <ipython-input-3-9b1033d0ac55> in <module>; ----> 1 t = hl.linear_regression_rows(x=mt.GT.n_alt_alleles(), y=mt.pop, covariates=[1]). <decorator-gen-1332> in linear_regression_rows(y, x, covariates, block_size, pass_through). ~/.conda/envs/hail/lib/python3.7/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 559 def wrapper(__original_func, *args, **kwargs):; 560 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 561 return __original_func(*args_, **kwargs_); 562 ; 563 return wrapper. ~/.conda/envs/hail/lib/python3.7/site-packages/hail/methods/statgen.py in linear_regression_rows(y, x, covariates, block_size, pass_through); 434 ht_result = ht_result.annotate(**{f: ht_result[f][0] for f in fields}); 435 ; --> 436 return ht_result.persist(); 437 ; 438 . <decorator-gen-918> in persist(self, storage_level). ~/.conda/envs/hail/lib/python3.7/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 559 def wrapper(__original_func, *args, **kwargs):; 560 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 561 return __original_func(*args_, **kwargs_); 562 ; 563 return wrapper. ~/.conda/envs/hail/lib/python3.7/site-packages/hail/table.py in persist(self, storage_level); 1677 Persisted table.; 1678 """"""; -> 1679 return Env.backend().persist_table(self, storage_level); 1680 ; ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5559
https://github.com/hail-is/hail/issues/5559:3118,Integrability,wrap,wrapper,3118,"r exception occurred:. Traceback (most recent call last):; File ""/home/unix/dking/.conda/envs/hail/lib/python3.7/site-packages/py4j/java_gateway.py"", line 985, in send_command; response = connection.send_command(command); File ""/home/unix/dking/.conda/envs/hail/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1164, in send_command; ""Error while receiving"", e, proto.ERROR_ON_RECEIVE); py4j.protocol.Py4JNetworkError: Error while receiving; ---------------------------------------------------------------------------; Py4JError Traceback (most recent call last); <ipython-input-3-9b1033d0ac55> in <module>; ----> 1 t = hl.linear_regression_rows(x=mt.GT.n_alt_alleles(), y=mt.pop, covariates=[1]). <decorator-gen-1332> in linear_regression_rows(y, x, covariates, block_size, pass_through). ~/.conda/envs/hail/lib/python3.7/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 559 def wrapper(__original_func, *args, **kwargs):; 560 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 561 return __original_func(*args_, **kwargs_); 562 ; 563 return wrapper. ~/.conda/envs/hail/lib/python3.7/site-packages/hail/methods/statgen.py in linear_regression_rows(y, x, covariates, block_size, pass_through); 434 ht_result = ht_result.annotate(**{f: ht_result[f][0] for f in fields}); 435 ; --> 436 return ht_result.persist(); 437 ; 438 . <decorator-gen-918> in persist(self, storage_level). ~/.conda/envs/hail/lib/python3.7/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 559 def wrapper(__original_func, *args, **kwargs):; 560 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 561 return __original_func(*args_, **kwargs_); 562 ; 563 return wrapper. ~/.conda/envs/hail/lib/python3.7/site-packages/hail/table.py in persist(self, storage_level); 1677 Persisted table.; 1678 """"""; -> 1679 return Env.backend().persist_table(self, storage_level); 1680 ; ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5559
https://github.com/hail-is/hail/issues/5559:3526,Integrability,wrap,wrapper,3526,"t_alleles(), y=mt.pop, covariates=[1]). <decorator-gen-1332> in linear_regression_rows(y, x, covariates, block_size, pass_through). ~/.conda/envs/hail/lib/python3.7/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 559 def wrapper(__original_func, *args, **kwargs):; 560 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 561 return __original_func(*args_, **kwargs_); 562 ; 563 return wrapper. ~/.conda/envs/hail/lib/python3.7/site-packages/hail/methods/statgen.py in linear_regression_rows(y, x, covariates, block_size, pass_through); 434 ht_result = ht_result.annotate(**{f: ht_result[f][0] for f in fields}); 435 ; --> 436 return ht_result.persist(); 437 ; 438 . <decorator-gen-918> in persist(self, storage_level). ~/.conda/envs/hail/lib/python3.7/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 559 def wrapper(__original_func, *args, **kwargs):; 560 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 561 return __original_func(*args_, **kwargs_); 562 ; 563 return wrapper. ~/.conda/envs/hail/lib/python3.7/site-packages/hail/table.py in persist(self, storage_level); 1677 Persisted table.; 1678 """"""; -> 1679 return Env.backend().persist_table(self, storage_level); 1680 ; 1681 def unpersist(self) -> 'Table':. ~/.conda/envs/hail/lib/python3.7/site-packages/hail/backend/backend.py in persist_table(self, t, storage_level); 107 ; 108 def persist_table(self, t, storage_level):; --> 109 return Table._from_java(self._to_java_ir(t._tir).pyPersist(storage_level)); 110 ; 111 def unpersist_table(self, t):. ~/.conda/envs/hail/lib/python3.7/site-packages/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. ~/.conda/envs/hail/lib/python3.7/site-pa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5559
https://github.com/hail-is/hail/issues/5559:3577,Integrability,wrap,wrapper,3577,"t_alleles(), y=mt.pop, covariates=[1]). <decorator-gen-1332> in linear_regression_rows(y, x, covariates, block_size, pass_through). ~/.conda/envs/hail/lib/python3.7/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 559 def wrapper(__original_func, *args, **kwargs):; 560 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 561 return __original_func(*args_, **kwargs_); 562 ; 563 return wrapper. ~/.conda/envs/hail/lib/python3.7/site-packages/hail/methods/statgen.py in linear_regression_rows(y, x, covariates, block_size, pass_through); 434 ht_result = ht_result.annotate(**{f: ht_result[f][0] for f in fields}); 435 ; --> 436 return ht_result.persist(); 437 ; 438 . <decorator-gen-918> in persist(self, storage_level). ~/.conda/envs/hail/lib/python3.7/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 559 def wrapper(__original_func, *args, **kwargs):; 560 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 561 return __original_func(*args_, **kwargs_); 562 ; 563 return wrapper. ~/.conda/envs/hail/lib/python3.7/site-packages/hail/table.py in persist(self, storage_level); 1677 Persisted table.; 1678 """"""; -> 1679 return Env.backend().persist_table(self, storage_level); 1680 ; 1681 def unpersist(self) -> 'Table':. ~/.conda/envs/hail/lib/python3.7/site-packages/hail/backend/backend.py in persist_table(self, t, storage_level); 107 ; 108 def persist_table(self, t, storage_level):; --> 109 return Table._from_java(self._to_java_ir(t._tir).pyPersist(storage_level)); 110 ; 111 def unpersist_table(self, t):. ~/.conda/envs/hail/lib/python3.7/site-packages/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. ~/.conda/envs/hail/lib/python3.7/site-pa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5559
https://github.com/hail-is/hail/issues/5559:3783,Integrability,wrap,wrapper,3783,"t_alleles(), y=mt.pop, covariates=[1]). <decorator-gen-1332> in linear_regression_rows(y, x, covariates, block_size, pass_through). ~/.conda/envs/hail/lib/python3.7/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 559 def wrapper(__original_func, *args, **kwargs):; 560 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 561 return __original_func(*args_, **kwargs_); 562 ; 563 return wrapper. ~/.conda/envs/hail/lib/python3.7/site-packages/hail/methods/statgen.py in linear_regression_rows(y, x, covariates, block_size, pass_through); 434 ht_result = ht_result.annotate(**{f: ht_result[f][0] for f in fields}); 435 ; --> 436 return ht_result.persist(); 437 ; 438 . <decorator-gen-918> in persist(self, storage_level). ~/.conda/envs/hail/lib/python3.7/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 559 def wrapper(__original_func, *args, **kwargs):; 560 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 561 return __original_func(*args_, **kwargs_); 562 ; 563 return wrapper. ~/.conda/envs/hail/lib/python3.7/site-packages/hail/table.py in persist(self, storage_level); 1677 Persisted table.; 1678 """"""; -> 1679 return Env.backend().persist_table(self, storage_level); 1680 ; 1681 def unpersist(self) -> 'Table':. ~/.conda/envs/hail/lib/python3.7/site-packages/hail/backend/backend.py in persist_table(self, t, storage_level); 107 ; 108 def persist_table(self, t, storage_level):; --> 109 return Table._from_java(self._to_java_ir(t._tir).pyPersist(storage_level)); 110 ; 111 def unpersist_table(self, t):. ~/.conda/envs/hail/lib/python3.7/site-packages/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. ~/.conda/envs/hail/lib/python3.7/site-pa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5559
https://github.com/hail-is/hail/issues/5559:4789,Integrability,protocol,protocol,4789,"/.conda/envs/hail/lib/python3.7/site-packages/hail/table.py in persist(self, storage_level); 1677 Persisted table.; 1678 """"""; -> 1679 return Env.backend().persist_table(self, storage_level); 1680 ; 1681 def unpersist(self) -> 'Table':. ~/.conda/envs/hail/lib/python3.7/site-packages/hail/backend/backend.py in persist_table(self, t, storage_level); 107 ; 108 def persist_table(self, t, storage_level):; --> 109 return Table._from_java(self._to_java_ir(t._tir).pyPersist(storage_level)); 110 ; 111 def unpersist_table(self, t):. ~/.conda/envs/hail/lib/python3.7/site-packages/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. ~/.conda/envs/hail/lib/python3.7/site-packages/hail/utils/java.py in deco(*args, **kwargs); 213 import pyspark; 214 try:; --> 215 return f(*args, **kwargs); 216 except py4j.protocol.Py4JJavaError as e:; 217 s = e.java_exception.toString(). ~/.conda/envs/hail/lib/python3.7/site-packages/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name); 334 raise Py4JError(; 335 ""An error occurred while calling {0}{1}{2}"".; --> 336 format(target_id, ""."", name)); 337 else:; 338 type = answer[1]. Py4JError: An error occurred while calling o51.pyPersist; ```. ```; (hail) -bash:uger-r7-c001:~ 1037 $ find /lib /lib64/ -regex '.*blas.*' -exec ls -al \{\} \;; lrwxrwxrwx. 1 root root 16 Dec 13 2017 /lib64/libblas.so.3.4 -> libblas.so.3.4.2; lrwxrwxrwx. 1 root root 16 Dec 13 2017 /lib64/libblas.so.3 -> libblas.so.3.4.2; lrwxrwxrwx. 1 root root 16 Dec 13 2017 /lib64/libblas.so -> libblas.so.3.4.2; -rwxr-xr-x. 1 root root 364808 Mar 2 2017 /lib64/libblas.so.3.4.2; ```. ```; (hail) -bash:uger-r7-c001:~ 1034 $ objdump -T /lib64/liblapack.so.3 | grep dgemm; 0000000000000000 DF *UND*	0000000000000000 dgemm_; (hail) -bash:uger-r7-c001:~ 1035 $ objdump -T /lib64/libb",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5559
https://github.com/hail-is/hail/issues/5559:4908,Integrability,protocol,protocol,4908,"level); 1677 Persisted table.; 1678 """"""; -> 1679 return Env.backend().persist_table(self, storage_level); 1680 ; 1681 def unpersist(self) -> 'Table':. ~/.conda/envs/hail/lib/python3.7/site-packages/hail/backend/backend.py in persist_table(self, t, storage_level); 107 ; 108 def persist_table(self, t, storage_level):; --> 109 return Table._from_java(self._to_java_ir(t._tir).pyPersist(storage_level)); 110 ; 111 def unpersist_table(self, t):. ~/.conda/envs/hail/lib/python3.7/site-packages/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. ~/.conda/envs/hail/lib/python3.7/site-packages/hail/utils/java.py in deco(*args, **kwargs); 213 import pyspark; 214 try:; --> 215 return f(*args, **kwargs); 216 except py4j.protocol.Py4JJavaError as e:; 217 s = e.java_exception.toString(). ~/.conda/envs/hail/lib/python3.7/site-packages/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name); 334 raise Py4JError(; 335 ""An error occurred while calling {0}{1}{2}"".; --> 336 format(target_id, ""."", name)); 337 else:; 338 type = answer[1]. Py4JError: An error occurred while calling o51.pyPersist; ```. ```; (hail) -bash:uger-r7-c001:~ 1037 $ find /lib /lib64/ -regex '.*blas.*' -exec ls -al \{\} \;; lrwxrwxrwx. 1 root root 16 Dec 13 2017 /lib64/libblas.so.3.4 -> libblas.so.3.4.2; lrwxrwxrwx. 1 root root 16 Dec 13 2017 /lib64/libblas.so.3 -> libblas.so.3.4.2; lrwxrwxrwx. 1 root root 16 Dec 13 2017 /lib64/libblas.so -> libblas.so.3.4.2; -rwxr-xr-x. 1 root root 364808 Mar 2 2017 /lib64/libblas.so.3.4.2; ```. ```; (hail) -bash:uger-r7-c001:~ 1034 $ objdump -T /lib64/liblapack.so.3 | grep dgemm; 0000000000000000 DF *UND*	0000000000000000 dgemm_; (hail) -bash:uger-r7-c001:~ 1035 $ objdump -T /lib64/libblas.so.3 | grep dgemm; 0000000000018850 g DF .text	0000000000000935 Base dgemm_; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5559
https://github.com/hail-is/hail/issues/5559:1361,Modifiability,variab,variable,1361,"Initializing Spark and Hail with default parameters...; using hail jar at /home/unix/dking/.conda/envs/hail/lib/python3.7/site-packages/hail/hail-all-spark.jar; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Running on Apache Spark version 2.2.3; SparkUI available at http://10.200.100.39:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.11-cf54f08305d1; LOGGING: writing to /home/unix/dking/hail-20190307-1908-0.2.11-cf54f08305d1.log; 2019-03-07 19:08:30 Hail: INFO: balding_nichols_model: generating genotypes for 3 populations, 100 samples, and 100 variants...; ^[[A; In [3]: t = hl.linear_regression_rows(x=mt.GT.n_alt_alleles(), y=mt.pop, covariates=[1]) ; [Stage 0:============================================> (6 + 1) / 8]2019-03-07 19:08:39 Hail: INFO: Coerced sorted dataset; 2019-03-07 19:08:40 Hail: INFO: linear_regression_rows: running on 100 samples for 1 response variable y,; with input variable x, and 1 additional covariate...; /broad/software/free/Linux/redhat_7_x86_64/pkgs/jdk1.8.0_181/bin/java: symbol lookup error: /tmp/jniloader1327638724610654731netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemm; ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/home/unix/dking/.conda/envs/hail/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1159, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/home/unix/dking/.conda/envs/hail/lib/python3.7/site-packages/py4j/java_gateway.py"", line 985, in send_command; response = connection.send_command(command); File ""/home/unix/dking/.conda/envs/hail/lib/python3.7/site-pa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5559
https://github.com/hail-is/hail/issues/5559:1385,Modifiability,variab,variable,1385,"Initializing Spark and Hail with default parameters...; using hail jar at /home/unix/dking/.conda/envs/hail/lib/python3.7/site-packages/hail/hail-all-spark.jar; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Running on Apache Spark version 2.2.3; SparkUI available at http://10.200.100.39:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.11-cf54f08305d1; LOGGING: writing to /home/unix/dking/hail-20190307-1908-0.2.11-cf54f08305d1.log; 2019-03-07 19:08:30 Hail: INFO: balding_nichols_model: generating genotypes for 3 populations, 100 samples, and 100 variants...; ^[[A; In [3]: t = hl.linear_regression_rows(x=mt.GT.n_alt_alleles(), y=mt.pop, covariates=[1]) ; [Stage 0:============================================> (6 + 1) / 8]2019-03-07 19:08:39 Hail: INFO: Coerced sorted dataset; 2019-03-07 19:08:40 Hail: INFO: linear_regression_rows: running on 100 samples for 1 response variable y,; with input variable x, and 1 additional covariate...; /broad/software/free/Linux/redhat_7_x86_64/pkgs/jdk1.8.0_181/bin/java: symbol lookup error: /tmp/jniloader1327638724610654731netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemm; ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/home/unix/dking/.conda/envs/hail/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1159, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/home/unix/dking/.conda/envs/hail/lib/python3.7/site-packages/py4j/java_gateway.py"", line 985, in send_command; response = connection.send_command(command); File ""/home/unix/dking/.conda/envs/hail/lib/python3.7/site-pa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5559
https://github.com/hail-is/hail/issues/5559:530,Testability,log,log,530,"I think we need to figure out how to get cblas on the broad cluster. ```; # use UGER; # ish -l os=RedHat7; # use Anaconda3; # use Java-1.8; # use OpenBLAS; # source activate hail; # ipython; In [1]: import hail as hl . In [2]: mt = hl.balding_nichols_model(3, 100, 100) ; Initializing Spark and Hail with default parameters...; using hail jar at /home/unix/dking/.conda/envs/hail/lib/python3.7/site-packages/hail/hail-all-spark.jar; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Running on Apache Spark version 2.2.3; SparkUI available at http://10.200.100.39:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.11-cf54f08305d1; LOGGING: writing to /home/unix/dking/hail-20190307-1908-0.2.11-cf54f08305d1.log; 2019-03-07 19:08:30 Hail: INFO: balding_nichols_model: generating genotypes for 3 populations, 100 samples, and 100 variants...; ^[[A; In [3]: t = hl.linear_regression_rows(x=mt.GT.n_alt_alleles(), y=mt.pop, covariates=[1]) ; [Stage 0:============================================> (6 + 1) / 8]2019-03-07 19:08:39 Hail: INFO: Coerced sorted dataset; 2019-03-07 19:08:40 Hail: INFO: linear_regression_rows: running on 100 samples for 1 response variable y,; with input variable x, and 1 additional covariate...; /broad/software/free/Linux/redhat_7_x86_64/pkgs/jdk1.8.0_181/bin/java: symbol lookup error: /tmp/jniloader1327638724610654731netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemm; ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/home/unix/dking/.conda/envs/hail/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1159, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty. During handling of the above exception, another exception",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5559
https://github.com/hail-is/hail/issues/5559:562,Testability,log,logging,562,"I think we need to figure out how to get cblas on the broad cluster. ```; # use UGER; # ish -l os=RedHat7; # use Anaconda3; # use Java-1.8; # use OpenBLAS; # source activate hail; # ipython; In [1]: import hail as hl . In [2]: mt = hl.balding_nichols_model(3, 100, 100) ; Initializing Spark and Hail with default parameters...; using hail jar at /home/unix/dking/.conda/envs/hail/lib/python3.7/site-packages/hail/hail-all-spark.jar; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Running on Apache Spark version 2.2.3; SparkUI available at http://10.200.100.39:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.11-cf54f08305d1; LOGGING: writing to /home/unix/dking/hail-20190307-1908-0.2.11-cf54f08305d1.log; 2019-03-07 19:08:30 Hail: INFO: balding_nichols_model: generating genotypes for 3 populations, 100 samples, and 100 variants...; ^[[A; In [3]: t = hl.linear_regression_rows(x=mt.GT.n_alt_alleles(), y=mt.pop, covariates=[1]) ; [Stage 0:============================================> (6 + 1) / 8]2019-03-07 19:08:39 Hail: INFO: Coerced sorted dataset; 2019-03-07 19:08:40 Hail: INFO: linear_regression_rows: running on 100 samples for 1 response variable y,; with input variable x, and 1 additional covariate...; /broad/software/free/Linux/redhat_7_x86_64/pkgs/jdk1.8.0_181/bin/java: symbol lookup error: /tmp/jniloader1327638724610654731netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemm; ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/home/unix/dking/.conda/envs/hail/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1159, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty. During handling of the above exception, another exception",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5559
https://github.com/hail-is/hail/issues/5559:837,Testability,LOG,LOGGING,837,"I think we need to figure out how to get cblas on the broad cluster. ```; # use UGER; # ish -l os=RedHat7; # use Anaconda3; # use Java-1.8; # use OpenBLAS; # source activate hail; # ipython; In [1]: import hail as hl . In [2]: mt = hl.balding_nichols_model(3, 100, 100) ; Initializing Spark and Hail with default parameters...; using hail jar at /home/unix/dking/.conda/envs/hail/lib/python3.7/site-packages/hail/hail-all-spark.jar; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Running on Apache Spark version 2.2.3; SparkUI available at http://10.200.100.39:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.11-cf54f08305d1; LOGGING: writing to /home/unix/dking/hail-20190307-1908-0.2.11-cf54f08305d1.log; 2019-03-07 19:08:30 Hail: INFO: balding_nichols_model: generating genotypes for 3 populations, 100 samples, and 100 variants...; ^[[A; In [3]: t = hl.linear_regression_rows(x=mt.GT.n_alt_alleles(), y=mt.pop, covariates=[1]) ; [Stage 0:============================================> (6 + 1) / 8]2019-03-07 19:08:39 Hail: INFO: Coerced sorted dataset; 2019-03-07 19:08:40 Hail: INFO: linear_regression_rows: running on 100 samples for 1 response variable y,; with input variable x, and 1 additional covariate...; /broad/software/free/Linux/redhat_7_x86_64/pkgs/jdk1.8.0_181/bin/java: symbol lookup error: /tmp/jniloader1327638724610654731netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemm; ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/home/unix/dking/.conda/envs/hail/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1159, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty. During handling of the above exception, another exception",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5559
https://github.com/hail-is/hail/issues/5559:913,Testability,log,log,913,"I think we need to figure out how to get cblas on the broad cluster. ```; # use UGER; # ish -l os=RedHat7; # use Anaconda3; # use Java-1.8; # use OpenBLAS; # source activate hail; # ipython; In [1]: import hail as hl . In [2]: mt = hl.balding_nichols_model(3, 100, 100) ; Initializing Spark and Hail with default parameters...; using hail jar at /home/unix/dking/.conda/envs/hail/lib/python3.7/site-packages/hail/hail-all-spark.jar; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Running on Apache Spark version 2.2.3; SparkUI available at http://10.200.100.39:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.11-cf54f08305d1; LOGGING: writing to /home/unix/dking/hail-20190307-1908-0.2.11-cf54f08305d1.log; 2019-03-07 19:08:30 Hail: INFO: balding_nichols_model: generating genotypes for 3 populations, 100 samples, and 100 variants...; ^[[A; In [3]: t = hl.linear_regression_rows(x=mt.GT.n_alt_alleles(), y=mt.pop, covariates=[1]) ; [Stage 0:============================================> (6 + 1) / 8]2019-03-07 19:08:39 Hail: INFO: Coerced sorted dataset; 2019-03-07 19:08:40 Hail: INFO: linear_regression_rows: running on 100 samples for 1 response variable y,; with input variable x, and 1 additional covariate...; /broad/software/free/Linux/redhat_7_x86_64/pkgs/jdk1.8.0_181/bin/java: symbol lookup error: /tmp/jniloader1327638724610654731netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemm; ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/home/unix/dking/.conda/envs/hail/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1159, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty. During handling of the above exception, another exception",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5559
https://github.com/hail-is/hail/issues/5564:1001,Availability,down,down,1001,"I believe we are encountering this known Kryo limitation: https://github.com/EsotericSoftware/kryo/issues/497, https://github.com/EsotericSoftware/kryo/issues/382 (also see related GATK issue: https://github.com/broadinstitute/gatk/issues/1524). Danfeng saw the referenced stack trace when trying to broadcast the variants for Plink (see: [LoadPlink.scala:202](https://github.com/hail-is/hail/blob/master/hail/src/main/scala/is/hail/io/plink/LoadPlink.scala#L202). She was running a import_plink, count. The details in EsotericSoftware/kryo#382 indicate that a bad interaction between the data and a hash function can cause this integer map to exceed its size limitations at a load factor of 5%. Even a 20x increase in footprint puts us at 400 million. Each element of that array has 6 entries, so we're at 1.2 billion. That definitely feels like the danger zone. Maybe there's more variants than Danfeng expects, maybe there's more overhead than we've accounted for. The GATK folks have been chasing down the fix. Kryo [released 4.0.0](https://github.com/EsotericSoftware/kryo/issues/431) which should fix this issue. Spark [upgraded to Kryo 4.0.0](https://github.com/apache/spark/pull/22179) on September 8th of 2018. (resolving [Spark-20389](https://issues.apache.org/jira/browse/SPARK-20389)). This change made it to 2.4.0, but it was not back ported to other versions of Spark. GATK [references a temporary fix via JVM options](; https://github.com/broadinstitute/gatk/issues/1524#issuecomment-189368808), which apparently forces the JVM to use an alternative hash function with better behavior in this specific case:; ```; spark.executor.extraJavaOptions -XX:hashCode=0; spark.driver.extraJavaOptions -XX:hashCode=0; ```; A [generally interesting blog post on Java's hashCode](https://srvaroa.github.io/jvm/java/openjdk/biased-locking/2017/01/30/hashCode.html), which I haven't fully read, claims that the JVM previously defaulted to a PRNG draw for an object's hash code. In JDK 8 it uses some ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5564
https://github.com/hail-is/hail/issues/5564:1021,Deployability,release,released,1021,"tering this known Kryo limitation: https://github.com/EsotericSoftware/kryo/issues/497, https://github.com/EsotericSoftware/kryo/issues/382 (also see related GATK issue: https://github.com/broadinstitute/gatk/issues/1524). Danfeng saw the referenced stack trace when trying to broadcast the variants for Plink (see: [LoadPlink.scala:202](https://github.com/hail-is/hail/blob/master/hail/src/main/scala/is/hail/io/plink/LoadPlink.scala#L202). She was running a import_plink, count. The details in EsotericSoftware/kryo#382 indicate that a bad interaction between the data and a hash function can cause this integer map to exceed its size limitations at a load factor of 5%. Even a 20x increase in footprint puts us at 400 million. Each element of that array has 6 entries, so we're at 1.2 billion. That definitely feels like the danger zone. Maybe there's more variants than Danfeng expects, maybe there's more overhead than we've accounted for. The GATK folks have been chasing down the fix. Kryo [released 4.0.0](https://github.com/EsotericSoftware/kryo/issues/431) which should fix this issue. Spark [upgraded to Kryo 4.0.0](https://github.com/apache/spark/pull/22179) on September 8th of 2018. (resolving [Spark-20389](https://issues.apache.org/jira/browse/SPARK-20389)). This change made it to 2.4.0, but it was not back ported to other versions of Spark. GATK [references a temporary fix via JVM options](; https://github.com/broadinstitute/gatk/issues/1524#issuecomment-189368808), which apparently forces the JVM to use an alternative hash function with better behavior in this specific case:; ```; spark.executor.extraJavaOptions -XX:hashCode=0; spark.driver.extraJavaOptions -XX:hashCode=0; ```; A [generally interesting blog post on Java's hashCode](https://srvaroa.github.io/jvm/java/openjdk/biased-locking/2017/01/30/hashCode.html), which I haven't fully read, claims that the JVM previously defaulted to a PRNG draw for an object's hash code. In JDK 8 it uses some function of the curren",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5564
https://github.com/hail-is/hail/issues/5564:1126,Deployability,upgrade,upgraded,1126,"sotericSoftware/kryo/issues/382 (also see related GATK issue: https://github.com/broadinstitute/gatk/issues/1524). Danfeng saw the referenced stack trace when trying to broadcast the variants for Plink (see: [LoadPlink.scala:202](https://github.com/hail-is/hail/blob/master/hail/src/main/scala/is/hail/io/plink/LoadPlink.scala#L202). She was running a import_plink, count. The details in EsotericSoftware/kryo#382 indicate that a bad interaction between the data and a hash function can cause this integer map to exceed its size limitations at a load factor of 5%. Even a 20x increase in footprint puts us at 400 million. Each element of that array has 6 entries, so we're at 1.2 billion. That definitely feels like the danger zone. Maybe there's more variants than Danfeng expects, maybe there's more overhead than we've accounted for. The GATK folks have been chasing down the fix. Kryo [released 4.0.0](https://github.com/EsotericSoftware/kryo/issues/431) which should fix this issue. Spark [upgraded to Kryo 4.0.0](https://github.com/apache/spark/pull/22179) on September 8th of 2018. (resolving [Spark-20389](https://issues.apache.org/jira/browse/SPARK-20389)). This change made it to 2.4.0, but it was not back ported to other versions of Spark. GATK [references a temporary fix via JVM options](; https://github.com/broadinstitute/gatk/issues/1524#issuecomment-189368808), which apparently forces the JVM to use an alternative hash function with better behavior in this specific case:; ```; spark.executor.extraJavaOptions -XX:hashCode=0; spark.driver.extraJavaOptions -XX:hashCode=0; ```; A [generally interesting blog post on Java's hashCode](https://srvaroa.github.io/jvm/java/openjdk/biased-locking/2017/01/30/hashCode.html), which I haven't fully read, claims that the JVM previously defaulted to a PRNG draw for an object's hash code. In JDK 8 it uses some function of the current thread state. It appears this old strategy is preserved as JVM hashCode parameter value 0 and is less likel",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5564
https://github.com/hail-is/hail/issues/5564:340,Performance,Load,LoadPlink,340,"I believe we are encountering this known Kryo limitation: https://github.com/EsotericSoftware/kryo/issues/497, https://github.com/EsotericSoftware/kryo/issues/382 (also see related GATK issue: https://github.com/broadinstitute/gatk/issues/1524). Danfeng saw the referenced stack trace when trying to broadcast the variants for Plink (see: [LoadPlink.scala:202](https://github.com/hail-is/hail/blob/master/hail/src/main/scala/is/hail/io/plink/LoadPlink.scala#L202). She was running a import_plink, count. The details in EsotericSoftware/kryo#382 indicate that a bad interaction between the data and a hash function can cause this integer map to exceed its size limitations at a load factor of 5%. Even a 20x increase in footprint puts us at 400 million. Each element of that array has 6 entries, so we're at 1.2 billion. That definitely feels like the danger zone. Maybe there's more variants than Danfeng expects, maybe there's more overhead than we've accounted for. The GATK folks have been chasing down the fix. Kryo [released 4.0.0](https://github.com/EsotericSoftware/kryo/issues/431) which should fix this issue. Spark [upgraded to Kryo 4.0.0](https://github.com/apache/spark/pull/22179) on September 8th of 2018. (resolving [Spark-20389](https://issues.apache.org/jira/browse/SPARK-20389)). This change made it to 2.4.0, but it was not back ported to other versions of Spark. GATK [references a temporary fix via JVM options](; https://github.com/broadinstitute/gatk/issues/1524#issuecomment-189368808), which apparently forces the JVM to use an alternative hash function with better behavior in this specific case:; ```; spark.executor.extraJavaOptions -XX:hashCode=0; spark.driver.extraJavaOptions -XX:hashCode=0; ```; A [generally interesting blog post on Java's hashCode](https://srvaroa.github.io/jvm/java/openjdk/biased-locking/2017/01/30/hashCode.html), which I haven't fully read, claims that the JVM previously defaulted to a PRNG draw for an object's hash code. In JDK 8 it uses some ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5564
https://github.com/hail-is/hail/issues/5564:442,Performance,Load,LoadPlink,442,"I believe we are encountering this known Kryo limitation: https://github.com/EsotericSoftware/kryo/issues/497, https://github.com/EsotericSoftware/kryo/issues/382 (also see related GATK issue: https://github.com/broadinstitute/gatk/issues/1524). Danfeng saw the referenced stack trace when trying to broadcast the variants for Plink (see: [LoadPlink.scala:202](https://github.com/hail-is/hail/blob/master/hail/src/main/scala/is/hail/io/plink/LoadPlink.scala#L202). She was running a import_plink, count. The details in EsotericSoftware/kryo#382 indicate that a bad interaction between the data and a hash function can cause this integer map to exceed its size limitations at a load factor of 5%. Even a 20x increase in footprint puts us at 400 million. Each element of that array has 6 entries, so we're at 1.2 billion. That definitely feels like the danger zone. Maybe there's more variants than Danfeng expects, maybe there's more overhead than we've accounted for. The GATK folks have been chasing down the fix. Kryo [released 4.0.0](https://github.com/EsotericSoftware/kryo/issues/431) which should fix this issue. Spark [upgraded to Kryo 4.0.0](https://github.com/apache/spark/pull/22179) on September 8th of 2018. (resolving [Spark-20389](https://issues.apache.org/jira/browse/SPARK-20389)). This change made it to 2.4.0, but it was not back ported to other versions of Spark. GATK [references a temporary fix via JVM options](; https://github.com/broadinstitute/gatk/issues/1524#issuecomment-189368808), which apparently forces the JVM to use an alternative hash function with better behavior in this specific case:; ```; spark.executor.extraJavaOptions -XX:hashCode=0; spark.driver.extraJavaOptions -XX:hashCode=0; ```; A [generally interesting blog post on Java's hashCode](https://srvaroa.github.io/jvm/java/openjdk/biased-locking/2017/01/30/hashCode.html), which I haven't fully read, claims that the JVM previously defaulted to a PRNG draw for an object's hash code. In JDK 8 it uses some ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5564
https://github.com/hail-is/hail/issues/5564:677,Performance,load,load,677,"I believe we are encountering this known Kryo limitation: https://github.com/EsotericSoftware/kryo/issues/497, https://github.com/EsotericSoftware/kryo/issues/382 (also see related GATK issue: https://github.com/broadinstitute/gatk/issues/1524). Danfeng saw the referenced stack trace when trying to broadcast the variants for Plink (see: [LoadPlink.scala:202](https://github.com/hail-is/hail/blob/master/hail/src/main/scala/is/hail/io/plink/LoadPlink.scala#L202). She was running a import_plink, count. The details in EsotericSoftware/kryo#382 indicate that a bad interaction between the data and a hash function can cause this integer map to exceed its size limitations at a load factor of 5%. Even a 20x increase in footprint puts us at 400 million. Each element of that array has 6 entries, so we're at 1.2 billion. That definitely feels like the danger zone. Maybe there's more variants than Danfeng expects, maybe there's more overhead than we've accounted for. The GATK folks have been chasing down the fix. Kryo [released 4.0.0](https://github.com/EsotericSoftware/kryo/issues/431) which should fix this issue. Spark [upgraded to Kryo 4.0.0](https://github.com/apache/spark/pull/22179) on September 8th of 2018. (resolving [Spark-20389](https://issues.apache.org/jira/browse/SPARK-20389)). This change made it to 2.4.0, but it was not back ported to other versions of Spark. GATK [references a temporary fix via JVM options](; https://github.com/broadinstitute/gatk/issues/1524#issuecomment-189368808), which apparently forces the JVM to use an alternative hash function with better behavior in this specific case:; ```; spark.executor.extraJavaOptions -XX:hashCode=0; spark.driver.extraJavaOptions -XX:hashCode=0; ```; A [generally interesting blog post on Java's hashCode](https://srvaroa.github.io/jvm/java/openjdk/biased-locking/2017/01/30/hashCode.html), which I haven't fully read, claims that the JVM previously defaulted to a PRNG draw for an object's hash code. In JDK 8 it uses some ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5564
https://github.com/hail-is/hail/issues/5564:600,Security,hash,hash,600,"I believe we are encountering this known Kryo limitation: https://github.com/EsotericSoftware/kryo/issues/497, https://github.com/EsotericSoftware/kryo/issues/382 (also see related GATK issue: https://github.com/broadinstitute/gatk/issues/1524). Danfeng saw the referenced stack trace when trying to broadcast the variants for Plink (see: [LoadPlink.scala:202](https://github.com/hail-is/hail/blob/master/hail/src/main/scala/is/hail/io/plink/LoadPlink.scala#L202). She was running a import_plink, count. The details in EsotericSoftware/kryo#382 indicate that a bad interaction between the data and a hash function can cause this integer map to exceed its size limitations at a load factor of 5%. Even a 20x increase in footprint puts us at 400 million. Each element of that array has 6 entries, so we're at 1.2 billion. That definitely feels like the danger zone. Maybe there's more variants than Danfeng expects, maybe there's more overhead than we've accounted for. The GATK folks have been chasing down the fix. Kryo [released 4.0.0](https://github.com/EsotericSoftware/kryo/issues/431) which should fix this issue. Spark [upgraded to Kryo 4.0.0](https://github.com/apache/spark/pull/22179) on September 8th of 2018. (resolving [Spark-20389](https://issues.apache.org/jira/browse/SPARK-20389)). This change made it to 2.4.0, but it was not back ported to other versions of Spark. GATK [references a temporary fix via JVM options](; https://github.com/broadinstitute/gatk/issues/1524#issuecomment-189368808), which apparently forces the JVM to use an alternative hash function with better behavior in this specific case:; ```; spark.executor.extraJavaOptions -XX:hashCode=0; spark.driver.extraJavaOptions -XX:hashCode=0; ```; A [generally interesting blog post on Java's hashCode](https://srvaroa.github.io/jvm/java/openjdk/biased-locking/2017/01/30/hashCode.html), which I haven't fully read, claims that the JVM previously defaulted to a PRNG draw for an object's hash code. In JDK 8 it uses some ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5564
https://github.com/hail-is/hail/issues/5564:1565,Security,hash,hash,1565,"82 indicate that a bad interaction between the data and a hash function can cause this integer map to exceed its size limitations at a load factor of 5%. Even a 20x increase in footprint puts us at 400 million. Each element of that array has 6 entries, so we're at 1.2 billion. That definitely feels like the danger zone. Maybe there's more variants than Danfeng expects, maybe there's more overhead than we've accounted for. The GATK folks have been chasing down the fix. Kryo [released 4.0.0](https://github.com/EsotericSoftware/kryo/issues/431) which should fix this issue. Spark [upgraded to Kryo 4.0.0](https://github.com/apache/spark/pull/22179) on September 8th of 2018. (resolving [Spark-20389](https://issues.apache.org/jira/browse/SPARK-20389)). This change made it to 2.4.0, but it was not back ported to other versions of Spark. GATK [references a temporary fix via JVM options](; https://github.com/broadinstitute/gatk/issues/1524#issuecomment-189368808), which apparently forces the JVM to use an alternative hash function with better behavior in this specific case:; ```; spark.executor.extraJavaOptions -XX:hashCode=0; spark.driver.extraJavaOptions -XX:hashCode=0; ```; A [generally interesting blog post on Java's hashCode](https://srvaroa.github.io/jvm/java/openjdk/biased-locking/2017/01/30/hashCode.html), which I haven't fully read, claims that the JVM previously defaulted to a PRNG draw for an object's hash code. In JDK 8 it uses some function of the current thread state. It appears this old strategy is preserved as JVM hashCode parameter value 0 and is less likely to trigger the bad behavior in Kryo. This `-XX:hashCode` option is undocumented [1](https://docs.oracle.com/javase/8/docs/technotes/tools/windows/java.html), [2](https://www.oracle.com/technetwork/java/javase/tech/vmoptions-jsp-140102.html) 🤷‍♀️. Another suggested Kryo option is to disable reference tracking. This would cause duplicate objects in the object graph to be serialized twice:; ```java; Kryo kryo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5564
https://github.com/hail-is/hail/issues/5564:1665,Security,hash,hashCode,1665,"ata and a hash function can cause this integer map to exceed its size limitations at a load factor of 5%. Even a 20x increase in footprint puts us at 400 million. Each element of that array has 6 entries, so we're at 1.2 billion. That definitely feels like the danger zone. Maybe there's more variants than Danfeng expects, maybe there's more overhead than we've accounted for. The GATK folks have been chasing down the fix. Kryo [released 4.0.0](https://github.com/EsotericSoftware/kryo/issues/431) which should fix this issue. Spark [upgraded to Kryo 4.0.0](https://github.com/apache/spark/pull/22179) on September 8th of 2018. (resolving [Spark-20389](https://issues.apache.org/jira/browse/SPARK-20389)). This change made it to 2.4.0, but it was not back ported to other versions of Spark. GATK [references a temporary fix via JVM options](; https://github.com/broadinstitute/gatk/issues/1524#issuecomment-189368808), which apparently forces the JVM to use an alternative hash function with better behavior in this specific case:; ```; spark.executor.extraJavaOptions -XX:hashCode=0; spark.driver.extraJavaOptions -XX:hashCode=0; ```; A [generally interesting blog post on Java's hashCode](https://srvaroa.github.io/jvm/java/openjdk/biased-locking/2017/01/30/hashCode.html), which I haven't fully read, claims that the JVM previously defaulted to a PRNG draw for an object's hash code. In JDK 8 it uses some function of the current thread state. It appears this old strategy is preserved as JVM hashCode parameter value 0 and is less likely to trigger the bad behavior in Kryo. This `-XX:hashCode` option is undocumented [1](https://docs.oracle.com/javase/8/docs/technotes/tools/windows/java.html), [2](https://www.oracle.com/technetwork/java/javase/tech/vmoptions-jsp-140102.html) 🤷‍♀️. Another suggested Kryo option is to disable reference tracking. This would cause duplicate objects in the object graph to be serialized twice:; ```java; Kryo kryo = new Kryo();; kryo.setReferences(false);; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5564
https://github.com/hail-is/hail/issues/5564:1711,Security,hash,hashCode,1711,"ata and a hash function can cause this integer map to exceed its size limitations at a load factor of 5%. Even a 20x increase in footprint puts us at 400 million. Each element of that array has 6 entries, so we're at 1.2 billion. That definitely feels like the danger zone. Maybe there's more variants than Danfeng expects, maybe there's more overhead than we've accounted for. The GATK folks have been chasing down the fix. Kryo [released 4.0.0](https://github.com/EsotericSoftware/kryo/issues/431) which should fix this issue. Spark [upgraded to Kryo 4.0.0](https://github.com/apache/spark/pull/22179) on September 8th of 2018. (resolving [Spark-20389](https://issues.apache.org/jira/browse/SPARK-20389)). This change made it to 2.4.0, but it was not back ported to other versions of Spark. GATK [references a temporary fix via JVM options](; https://github.com/broadinstitute/gatk/issues/1524#issuecomment-189368808), which apparently forces the JVM to use an alternative hash function with better behavior in this specific case:; ```; spark.executor.extraJavaOptions -XX:hashCode=0; spark.driver.extraJavaOptions -XX:hashCode=0; ```; A [generally interesting blog post on Java's hashCode](https://srvaroa.github.io/jvm/java/openjdk/biased-locking/2017/01/30/hashCode.html), which I haven't fully read, claims that the JVM previously defaulted to a PRNG draw for an object's hash code. In JDK 8 it uses some function of the current thread state. It appears this old strategy is preserved as JVM hashCode parameter value 0 and is less likely to trigger the bad behavior in Kryo. This `-XX:hashCode` option is undocumented [1](https://docs.oracle.com/javase/8/docs/technotes/tools/windows/java.html), [2](https://www.oracle.com/technetwork/java/javase/tech/vmoptions-jsp-140102.html) 🤷‍♀️. Another suggested Kryo option is to disable reference tracking. This would cause duplicate objects in the object graph to be serialized twice:; ```java; Kryo kryo = new Kryo();; kryo.setReferences(false);; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5564
https://github.com/hail-is/hail/issues/5564:1773,Security,hash,hashCode,1773,"ata and a hash function can cause this integer map to exceed its size limitations at a load factor of 5%. Even a 20x increase in footprint puts us at 400 million. Each element of that array has 6 entries, so we're at 1.2 billion. That definitely feels like the danger zone. Maybe there's more variants than Danfeng expects, maybe there's more overhead than we've accounted for. The GATK folks have been chasing down the fix. Kryo [released 4.0.0](https://github.com/EsotericSoftware/kryo/issues/431) which should fix this issue. Spark [upgraded to Kryo 4.0.0](https://github.com/apache/spark/pull/22179) on September 8th of 2018. (resolving [Spark-20389](https://issues.apache.org/jira/browse/SPARK-20389)). This change made it to 2.4.0, but it was not back ported to other versions of Spark. GATK [references a temporary fix via JVM options](; https://github.com/broadinstitute/gatk/issues/1524#issuecomment-189368808), which apparently forces the JVM to use an alternative hash function with better behavior in this specific case:; ```; spark.executor.extraJavaOptions -XX:hashCode=0; spark.driver.extraJavaOptions -XX:hashCode=0; ```; A [generally interesting blog post on Java's hashCode](https://srvaroa.github.io/jvm/java/openjdk/biased-locking/2017/01/30/hashCode.html), which I haven't fully read, claims that the JVM previously defaulted to a PRNG draw for an object's hash code. In JDK 8 it uses some function of the current thread state. It appears this old strategy is preserved as JVM hashCode parameter value 0 and is less likely to trigger the bad behavior in Kryo. This `-XX:hashCode` option is undocumented [1](https://docs.oracle.com/javase/8/docs/technotes/tools/windows/java.html), [2](https://www.oracle.com/technetwork/java/javase/tech/vmoptions-jsp-140102.html) 🤷‍♀️. Another suggested Kryo option is to disable reference tracking. This would cause duplicate objects in the object graph to be serialized twice:; ```java; Kryo kryo = new Kryo();; kryo.setReferences(false);; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5564
https://github.com/hail-is/hail/issues/5564:1852,Security,hash,hashCode,1852,"ata and a hash function can cause this integer map to exceed its size limitations at a load factor of 5%. Even a 20x increase in footprint puts us at 400 million. Each element of that array has 6 entries, so we're at 1.2 billion. That definitely feels like the danger zone. Maybe there's more variants than Danfeng expects, maybe there's more overhead than we've accounted for. The GATK folks have been chasing down the fix. Kryo [released 4.0.0](https://github.com/EsotericSoftware/kryo/issues/431) which should fix this issue. Spark [upgraded to Kryo 4.0.0](https://github.com/apache/spark/pull/22179) on September 8th of 2018. (resolving [Spark-20389](https://issues.apache.org/jira/browse/SPARK-20389)). This change made it to 2.4.0, but it was not back ported to other versions of Spark. GATK [references a temporary fix via JVM options](; https://github.com/broadinstitute/gatk/issues/1524#issuecomment-189368808), which apparently forces the JVM to use an alternative hash function with better behavior in this specific case:; ```; spark.executor.extraJavaOptions -XX:hashCode=0; spark.driver.extraJavaOptions -XX:hashCode=0; ```; A [generally interesting blog post on Java's hashCode](https://srvaroa.github.io/jvm/java/openjdk/biased-locking/2017/01/30/hashCode.html), which I haven't fully read, claims that the JVM previously defaulted to a PRNG draw for an object's hash code. In JDK 8 it uses some function of the current thread state. It appears this old strategy is preserved as JVM hashCode parameter value 0 and is less likely to trigger the bad behavior in Kryo. This `-XX:hashCode` option is undocumented [1](https://docs.oracle.com/javase/8/docs/technotes/tools/windows/java.html), [2](https://www.oracle.com/technetwork/java/javase/tech/vmoptions-jsp-140102.html) 🤷‍♀️. Another suggested Kryo option is to disable reference tracking. This would cause duplicate objects in the object graph to be serialized twice:; ```java; Kryo kryo = new Kryo();; kryo.setReferences(false);; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5564
https://github.com/hail-is/hail/issues/5564:1968,Security,hash,hash,1968,"ata and a hash function can cause this integer map to exceed its size limitations at a load factor of 5%. Even a 20x increase in footprint puts us at 400 million. Each element of that array has 6 entries, so we're at 1.2 billion. That definitely feels like the danger zone. Maybe there's more variants than Danfeng expects, maybe there's more overhead than we've accounted for. The GATK folks have been chasing down the fix. Kryo [released 4.0.0](https://github.com/EsotericSoftware/kryo/issues/431) which should fix this issue. Spark [upgraded to Kryo 4.0.0](https://github.com/apache/spark/pull/22179) on September 8th of 2018. (resolving [Spark-20389](https://issues.apache.org/jira/browse/SPARK-20389)). This change made it to 2.4.0, but it was not back ported to other versions of Spark. GATK [references a temporary fix via JVM options](; https://github.com/broadinstitute/gatk/issues/1524#issuecomment-189368808), which apparently forces the JVM to use an alternative hash function with better behavior in this specific case:; ```; spark.executor.extraJavaOptions -XX:hashCode=0; spark.driver.extraJavaOptions -XX:hashCode=0; ```; A [generally interesting blog post on Java's hashCode](https://srvaroa.github.io/jvm/java/openjdk/biased-locking/2017/01/30/hashCode.html), which I haven't fully read, claims that the JVM previously defaulted to a PRNG draw for an object's hash code. In JDK 8 it uses some function of the current thread state. It appears this old strategy is preserved as JVM hashCode parameter value 0 and is less likely to trigger the bad behavior in Kryo. This `-XX:hashCode` option is undocumented [1](https://docs.oracle.com/javase/8/docs/technotes/tools/windows/java.html), [2](https://www.oracle.com/technetwork/java/javase/tech/vmoptions-jsp-140102.html) 🤷‍♀️. Another suggested Kryo option is to disable reference tracking. This would cause duplicate objects in the object graph to be serialized twice:; ```java; Kryo kryo = new Kryo();; kryo.setReferences(false);; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5564
https://github.com/hail-is/hail/issues/5564:2088,Security,hash,hashCode,2088,"ata and a hash function can cause this integer map to exceed its size limitations at a load factor of 5%. Even a 20x increase in footprint puts us at 400 million. Each element of that array has 6 entries, so we're at 1.2 billion. That definitely feels like the danger zone. Maybe there's more variants than Danfeng expects, maybe there's more overhead than we've accounted for. The GATK folks have been chasing down the fix. Kryo [released 4.0.0](https://github.com/EsotericSoftware/kryo/issues/431) which should fix this issue. Spark [upgraded to Kryo 4.0.0](https://github.com/apache/spark/pull/22179) on September 8th of 2018. (resolving [Spark-20389](https://issues.apache.org/jira/browse/SPARK-20389)). This change made it to 2.4.0, but it was not back ported to other versions of Spark. GATK [references a temporary fix via JVM options](; https://github.com/broadinstitute/gatk/issues/1524#issuecomment-189368808), which apparently forces the JVM to use an alternative hash function with better behavior in this specific case:; ```; spark.executor.extraJavaOptions -XX:hashCode=0; spark.driver.extraJavaOptions -XX:hashCode=0; ```; A [generally interesting blog post on Java's hashCode](https://srvaroa.github.io/jvm/java/openjdk/biased-locking/2017/01/30/hashCode.html), which I haven't fully read, claims that the JVM previously defaulted to a PRNG draw for an object's hash code. In JDK 8 it uses some function of the current thread state. It appears this old strategy is preserved as JVM hashCode parameter value 0 and is less likely to trigger the bad behavior in Kryo. This `-XX:hashCode` option is undocumented [1](https://docs.oracle.com/javase/8/docs/technotes/tools/windows/java.html), [2](https://www.oracle.com/technetwork/java/javase/tech/vmoptions-jsp-140102.html) 🤷‍♀️. Another suggested Kryo option is to disable reference tracking. This would cause duplicate objects in the object graph to be serialized twice:; ```java; Kryo kryo = new Kryo();; kryo.setReferences(false);; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5564
https://github.com/hail-is/hail/issues/5564:2181,Security,hash,hashCode,2181,"ata and a hash function can cause this integer map to exceed its size limitations at a load factor of 5%. Even a 20x increase in footprint puts us at 400 million. Each element of that array has 6 entries, so we're at 1.2 billion. That definitely feels like the danger zone. Maybe there's more variants than Danfeng expects, maybe there's more overhead than we've accounted for. The GATK folks have been chasing down the fix. Kryo [released 4.0.0](https://github.com/EsotericSoftware/kryo/issues/431) which should fix this issue. Spark [upgraded to Kryo 4.0.0](https://github.com/apache/spark/pull/22179) on September 8th of 2018. (resolving [Spark-20389](https://issues.apache.org/jira/browse/SPARK-20389)). This change made it to 2.4.0, but it was not back ported to other versions of Spark. GATK [references a temporary fix via JVM options](; https://github.com/broadinstitute/gatk/issues/1524#issuecomment-189368808), which apparently forces the JVM to use an alternative hash function with better behavior in this specific case:; ```; spark.executor.extraJavaOptions -XX:hashCode=0; spark.driver.extraJavaOptions -XX:hashCode=0; ```; A [generally interesting blog post on Java's hashCode](https://srvaroa.github.io/jvm/java/openjdk/biased-locking/2017/01/30/hashCode.html), which I haven't fully read, claims that the JVM previously defaulted to a PRNG draw for an object's hash code. In JDK 8 it uses some function of the current thread state. It appears this old strategy is preserved as JVM hashCode parameter value 0 and is less likely to trigger the bad behavior in Kryo. This `-XX:hashCode` option is undocumented [1](https://docs.oracle.com/javase/8/docs/technotes/tools/windows/java.html), [2](https://www.oracle.com/technetwork/java/javase/tech/vmoptions-jsp-140102.html) 🤷‍♀️. Another suggested Kryo option is to disable reference tracking. This would cause duplicate objects in the object graph to be serialized twice:; ```java; Kryo kryo = new Kryo();; kryo.setReferences(false);; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5564
https://github.com/hail-is/hail/issues/5564:2201,Usability,undo,undocumented,2201,"ata and a hash function can cause this integer map to exceed its size limitations at a load factor of 5%. Even a 20x increase in footprint puts us at 400 million. Each element of that array has 6 entries, so we're at 1.2 billion. That definitely feels like the danger zone. Maybe there's more variants than Danfeng expects, maybe there's more overhead than we've accounted for. The GATK folks have been chasing down the fix. Kryo [released 4.0.0](https://github.com/EsotericSoftware/kryo/issues/431) which should fix this issue. Spark [upgraded to Kryo 4.0.0](https://github.com/apache/spark/pull/22179) on September 8th of 2018. (resolving [Spark-20389](https://issues.apache.org/jira/browse/SPARK-20389)). This change made it to 2.4.0, but it was not back ported to other versions of Spark. GATK [references a temporary fix via JVM options](; https://github.com/broadinstitute/gatk/issues/1524#issuecomment-189368808), which apparently forces the JVM to use an alternative hash function with better behavior in this specific case:; ```; spark.executor.extraJavaOptions -XX:hashCode=0; spark.driver.extraJavaOptions -XX:hashCode=0; ```; A [generally interesting blog post on Java's hashCode](https://srvaroa.github.io/jvm/java/openjdk/biased-locking/2017/01/30/hashCode.html), which I haven't fully read, claims that the JVM previously defaulted to a PRNG draw for an object's hash code. In JDK 8 it uses some function of the current thread state. It appears this old strategy is preserved as JVM hashCode parameter value 0 and is less likely to trigger the bad behavior in Kryo. This `-XX:hashCode` option is undocumented [1](https://docs.oracle.com/javase/8/docs/technotes/tools/windows/java.html), [2](https://www.oracle.com/technetwork/java/javase/tech/vmoptions-jsp-140102.html) 🤷‍♀️. Another suggested Kryo option is to disable reference tracking. This would cause duplicate objects in the object graph to be serialized twice:; ```java; Kryo kryo = new Kryo();; kryo.setReferences(false);; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5564
https://github.com/hail-is/hail/issues/5566:457,Safety,timeout,timeout,457,"See for example, this PR test https://storage.googleapis.com/hail-ci-0-1/ci/28153fd91e1ab73e64144620ade2d1ca271f4d5a/8074a6697bbeb0dc0c4d71b27d8313ff83d2398e/job.log . I believe this is causing https://github.com/hail-is/hail/issues/5519. If you look carefully at that log, you'll see that read_timeout in `connectionpool.py` is 5. It is set by:. ```; read_timeout = timeout_obj.read_timeout; ```. timeout_obj, *should* be created by requests with the same timeout value for read and connect which I am confident (see api.py) is set to 60. Somebody somewhere in this (honestly very confusing) pile of calls is erasing our setting. I am worried there's something that modifies the timeout to have `total = True` which means it subtracts the time spent connecting from the read timeout.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5566
https://github.com/hail-is/hail/issues/5566:680,Safety,timeout,timeout,680,"See for example, this PR test https://storage.googleapis.com/hail-ci-0-1/ci/28153fd91e1ab73e64144620ade2d1ca271f4d5a/8074a6697bbeb0dc0c4d71b27d8313ff83d2398e/job.log . I believe this is causing https://github.com/hail-is/hail/issues/5519. If you look carefully at that log, you'll see that read_timeout in `connectionpool.py` is 5. It is set by:. ```; read_timeout = timeout_obj.read_timeout; ```. timeout_obj, *should* be created by requests with the same timeout value for read and connect which I am confident (see api.py) is set to 60. Somebody somewhere in this (honestly very confusing) pile of calls is erasing our setting. I am worried there's something that modifies the timeout to have `total = True` which means it subtracts the time spent connecting from the read timeout.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5566
https://github.com/hail-is/hail/issues/5566:776,Safety,timeout,timeout,776,"See for example, this PR test https://storage.googleapis.com/hail-ci-0-1/ci/28153fd91e1ab73e64144620ade2d1ca271f4d5a/8074a6697bbeb0dc0c4d71b27d8313ff83d2398e/job.log . I believe this is causing https://github.com/hail-is/hail/issues/5519. If you look carefully at that log, you'll see that read_timeout in `connectionpool.py` is 5. It is set by:. ```; read_timeout = timeout_obj.read_timeout; ```. timeout_obj, *should* be created by requests with the same timeout value for read and connect which I am confident (see api.py) is set to 60. Somebody somewhere in this (honestly very confusing) pile of calls is erasing our setting. I am worried there's something that modifies the timeout to have `total = True` which means it subtracts the time spent connecting from the read timeout.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5566
https://github.com/hail-is/hail/issues/5566:25,Testability,test,test,25,"See for example, this PR test https://storage.googleapis.com/hail-ci-0-1/ci/28153fd91e1ab73e64144620ade2d1ca271f4d5a/8074a6697bbeb0dc0c4d71b27d8313ff83d2398e/job.log . I believe this is causing https://github.com/hail-is/hail/issues/5519. If you look carefully at that log, you'll see that read_timeout in `connectionpool.py` is 5. It is set by:. ```; read_timeout = timeout_obj.read_timeout; ```. timeout_obj, *should* be created by requests with the same timeout value for read and connect which I am confident (see api.py) is set to 60. Somebody somewhere in this (honestly very confusing) pile of calls is erasing our setting. I am worried there's something that modifies the timeout to have `total = True` which means it subtracts the time spent connecting from the read timeout.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5566
https://github.com/hail-is/hail/issues/5566:162,Testability,log,log,162,"See for example, this PR test https://storage.googleapis.com/hail-ci-0-1/ci/28153fd91e1ab73e64144620ade2d1ca271f4d5a/8074a6697bbeb0dc0c4d71b27d8313ff83d2398e/job.log . I believe this is causing https://github.com/hail-is/hail/issues/5519. If you look carefully at that log, you'll see that read_timeout in `connectionpool.py` is 5. It is set by:. ```; read_timeout = timeout_obj.read_timeout; ```. timeout_obj, *should* be created by requests with the same timeout value for read and connect which I am confident (see api.py) is set to 60. Somebody somewhere in this (honestly very confusing) pile of calls is erasing our setting. I am worried there's something that modifies the timeout to have `total = True` which means it subtracts the time spent connecting from the read timeout.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5566
https://github.com/hail-is/hail/issues/5566:269,Testability,log,log,269,"See for example, this PR test https://storage.googleapis.com/hail-ci-0-1/ci/28153fd91e1ab73e64144620ade2d1ca271f4d5a/8074a6697bbeb0dc0c4d71b27d8313ff83d2398e/job.log . I believe this is causing https://github.com/hail-is/hail/issues/5519. If you look carefully at that log, you'll see that read_timeout in `connectionpool.py` is 5. It is set by:. ```; read_timeout = timeout_obj.read_timeout; ```. timeout_obj, *should* be created by requests with the same timeout value for read and connect which I am confident (see api.py) is set to 60. Somebody somewhere in this (honestly very confusing) pile of calls is erasing our setting. I am worried there's something that modifies the timeout to have `total = True` which means it subtracts the time spent connecting from the read timeout.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5566
https://github.com/hail-is/hail/pull/5568:291,Safety,avoid,avoid,291,"- methods to go from `BlockMatrix` to `Table` and `MatrixTable` in a row-major representation. The table has fields for row index and array of entries for a row, and the matrix table has a row index, col index and the value at those indexes as the entry. This operation goes through disk to avoid a shuffle.; - method to go from a `Table` to a `MatrixTable` with a similar representation, where selected fields of the same type from the `Table` become column fields in the matrix table, with their values making the fields of the matrix table. Motivation for these conversions can be found at the issue below.; Resolves #5504. Big thanks to @patrick-schultz and @jigold for taking the time to teach me about region-based mem management!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5568
https://github.com/hail-is/hail/pull/5569:14,Integrability,wrap,wrap,14,"These scripts wrap `gcloud` and `kubectl` to provide one line commands to create and remove k8s secrets, with a standard name scheme, for GCP service accounts.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5569
https://github.com/hail-is/hail/pull/5570:61,Integrability,wrap,wrap,61,"This only exposes the raw results to python. I still need to wrap that in some nice convenience functions like approximate_quantile, but I didn't want to let this PR get any longer.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5570
https://github.com/hail-is/hail/pull/5570:10,Security,expose,exposes,10,"This only exposes the raw results to python. I still need to wrap that in some nice convenience functions like approximate_quantile, but I didn't want to let this PR get any longer.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5570
https://github.com/hail-is/hail/pull/5572:265,Testability,test,testing,265,"@cseed There are a lot of things in here that I didn't know exactly what to do, so I just got something working. You can't have async __init__ methods. Hence the work-around with `create`. Not sure what the SQL schema we want is. How to get the database set up for testing. Get it working on cloud...",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5572
https://github.com/hail-is/hail/pull/5574:344,Deployability,Pipeline,Pipelines,344,# Batch Inter-Job File Dependencies. The important addition of this PR is a `copy_service_account_name` field on batch jobs that permits the client to authorize with some GCP credentials stored in a k8s secret. The copy pods and the main pod only share the `/io` folder (k8s forbids mounting a volume as `/`). This interface is pretty onerous. Pipelines will be updated to use this interface. Pipelines is becoming the easy-to-use interface to batch.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5574
https://github.com/hail-is/hail/pull/5574:362,Deployability,update,updated,362,# Batch Inter-Job File Dependencies. The important addition of this PR is a `copy_service_account_name` field on batch jobs that permits the client to authorize with some GCP credentials stored in a k8s secret. The copy pods and the main pod only share the `/io` folder (k8s forbids mounting a volume as `/`). This interface is pretty onerous. Pipelines will be updated to use this interface. Pipelines is becoming the easy-to-use interface to batch.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5574
https://github.com/hail-is/hail/pull/5574:393,Deployability,Pipeline,Pipelines,393,# Batch Inter-Job File Dependencies. The important addition of this PR is a `copy_service_account_name` field on batch jobs that permits the client to authorize with some GCP credentials stored in a k8s secret. The copy pods and the main pod only share the `/io` folder (k8s forbids mounting a volume as `/`). This interface is pretty onerous. Pipelines will be updated to use this interface. Pipelines is becoming the easy-to-use interface to batch.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5574
https://github.com/hail-is/hail/pull/5574:23,Integrability,Depend,Dependencies,23,# Batch Inter-Job File Dependencies. The important addition of this PR is a `copy_service_account_name` field on batch jobs that permits the client to authorize with some GCP credentials stored in a k8s secret. The copy pods and the main pod only share the `/io` folder (k8s forbids mounting a volume as `/`). This interface is pretty onerous. Pipelines will be updated to use this interface. Pipelines is becoming the easy-to-use interface to batch.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5574
https://github.com/hail-is/hail/pull/5574:315,Integrability,interface,interface,315,# Batch Inter-Job File Dependencies. The important addition of this PR is a `copy_service_account_name` field on batch jobs that permits the client to authorize with some GCP credentials stored in a k8s secret. The copy pods and the main pod only share the `/io` folder (k8s forbids mounting a volume as `/`). This interface is pretty onerous. Pipelines will be updated to use this interface. Pipelines is becoming the easy-to-use interface to batch.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5574
https://github.com/hail-is/hail/pull/5574:382,Integrability,interface,interface,382,# Batch Inter-Job File Dependencies. The important addition of this PR is a `copy_service_account_name` field on batch jobs that permits the client to authorize with some GCP credentials stored in a k8s secret. The copy pods and the main pod only share the `/io` folder (k8s forbids mounting a volume as `/`). This interface is pretty onerous. Pipelines will be updated to use this interface. Pipelines is becoming the easy-to-use interface to batch.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5574
https://github.com/hail-is/hail/pull/5574:431,Integrability,interface,interface,431,# Batch Inter-Job File Dependencies. The important addition of this PR is a `copy_service_account_name` field on batch jobs that permits the client to authorize with some GCP credentials stored in a k8s secret. The copy pods and the main pod only share the `/io` folder (k8s forbids mounting a volume as `/`). This interface is pretty onerous. Pipelines will be updated to use this interface. Pipelines is becoming the easy-to-use interface to batch.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5574
https://github.com/hail-is/hail/pull/5574:151,Security,authoriz,authorize,151,# Batch Inter-Job File Dependencies. The important addition of this PR is a `copy_service_account_name` field on batch jobs that permits the client to authorize with some GCP credentials stored in a k8s secret. The copy pods and the main pod only share the `/io` folder (k8s forbids mounting a volume as `/`). This interface is pretty onerous. Pipelines will be updated to use this interface. Pipelines is becoming the easy-to-use interface to batch.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5574
https://github.com/hail-is/hail/issues/5575:1751,Performance,load,loaded,1751,"nsequence_terms); worst_csq = hl.literal(CSQ_ORDER).find(lambda x: all_csq_terms.contains(x)); return hl.struct(worst_csq=worst_csq, protein_coding=protein_coding, lof=lof, no_lof_flags=no_lof_flags). protein_coding = ht.vep.transcript_consequences.filter(lambda x: x.biotype == 'protein_coding'); return ht.annotate(**hl.case(missing_false=True); .when(hl.len(protein_coding) > 0, get_worst_csq(protein_coding, True)); .when(hl.len(ht.vep.transcript_consequences) > 0, get_worst_csq(ht.vep.transcript_consequences, False)); .when(hl.len(ht.vep.regulatory_feature_consequences) > 0, get_worst_csq(ht.vep.regulatory_feature_consequences, False)); .when(hl.len(ht.vep.motif_feature_consequences) > 0, get_worst_csq(ht.vep.motif_feature_consequences, False)); .default(get_worst_csq(ht.vep.intergenic_consequences, False))); ```; When the `csq_list = hl.cond(hl.is_defined(lof), csq_list.filter(lambda x: x.lof == lof), csq_list)` line triggers, this seems to fail to `find` the consequences entirely:; ```; all_csq_terms = csq_list.flatmap(lambda x: x.consequence_terms); all_csq_terms.show(); 2019-03-09 17:48:20 Hail: INFO: interval filter loaded 1 of 9997 partitions; +---------------+------------+-------------------------------+; | locus | alleles | <expr> |; +---------------+------------+-------------------------------+; | locus<GRCh37> | array<str> | array<str> |; +---------------+------------+-------------------------------+; | 1:55509603 | [""C"",""T""] | [""stop_gained"",""stop_gained""] |; +---------------+------------+-------------------------------+; worst_csq = hl.literal(CSQ_ORDER).find(lambda x: all_csq_terms.contains(x)); worst_csq.show(); 2019-03-09 17:48:32 Hail: INFO: interval filter loaded 1 of 9997 partitions; +---------------+------------+--------+; | locus | alleles | <expr> |; +---------------+------------+--------+; | locus<GRCh37> | array<str> | str |; +---------------+------------+--------+; | 1:55509603 | [""C"",""T""] | NA |; +---------------+------------+--------+; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5575
https://github.com/hail-is/hail/issues/5575:2314,Performance,load,loaded,2314,"nsequence_terms); worst_csq = hl.literal(CSQ_ORDER).find(lambda x: all_csq_terms.contains(x)); return hl.struct(worst_csq=worst_csq, protein_coding=protein_coding, lof=lof, no_lof_flags=no_lof_flags). protein_coding = ht.vep.transcript_consequences.filter(lambda x: x.biotype == 'protein_coding'); return ht.annotate(**hl.case(missing_false=True); .when(hl.len(protein_coding) > 0, get_worst_csq(protein_coding, True)); .when(hl.len(ht.vep.transcript_consequences) > 0, get_worst_csq(ht.vep.transcript_consequences, False)); .when(hl.len(ht.vep.regulatory_feature_consequences) > 0, get_worst_csq(ht.vep.regulatory_feature_consequences, False)); .when(hl.len(ht.vep.motif_feature_consequences) > 0, get_worst_csq(ht.vep.motif_feature_consequences, False)); .default(get_worst_csq(ht.vep.intergenic_consequences, False))); ```; When the `csq_list = hl.cond(hl.is_defined(lof), csq_list.filter(lambda x: x.lof == lof), csq_list)` line triggers, this seems to fail to `find` the consequences entirely:; ```; all_csq_terms = csq_list.flatmap(lambda x: x.consequence_terms); all_csq_terms.show(); 2019-03-09 17:48:20 Hail: INFO: interval filter loaded 1 of 9997 partitions; +---------------+------------+-------------------------------+; | locus | alleles | <expr> |; +---------------+------------+-------------------------------+; | locus<GRCh37> | array<str> | array<str> |; +---------------+------------+-------------------------------+; | 1:55509603 | [""C"",""T""] | [""stop_gained"",""stop_gained""] |; +---------------+------------+-------------------------------+; worst_csq = hl.literal(CSQ_ORDER).find(lambda x: all_csq_terms.contains(x)); worst_csq.show(); 2019-03-09 17:48:32 Hail: INFO: interval filter loaded 1 of 9997 partitions; +---------------+------------+--------+; | locus | alleles | <expr> |; +---------------+------------+--------+; | locus<GRCh37> | array<str> | str |; +---------------+------------+--------+; | 1:55509603 | [""C"",""T""] | NA |; +---------------+------------+--------+; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5575
https://github.com/hail-is/hail/pull/5576:37,Deployability,install,installation,37,Also add my extremely specific conda installation location to `loadconda`,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5576
https://github.com/hail-is/hail/pull/5576:63,Performance,load,loadconda,63,Also add my extremely specific conda installation location to `loadconda`,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5576
https://github.com/hail-is/hail/pull/5578:99,Modifiability,variab,variables,99,@cseed @tpoterba I accidentally undid the caching behavior in the tests by using the name of input variables from their name in the environment instead of just using a default set of variable names. Should be much faster now.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5578
https://github.com/hail-is/hail/pull/5578:183,Modifiability,variab,variable,183,@cseed @tpoterba I accidentally undid the caching behavior in the tests by using the name of input variables from their name in the environment instead of just using a default set of variable names. Should be much faster now.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5578
https://github.com/hail-is/hail/pull/5578:66,Testability,test,tests,66,@cseed @tpoterba I accidentally undid the caching behavior in the tests by using the name of input variables from their name in the environment instead of just using a default set of variable names. Should be much faster now.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5578
https://github.com/hail-is/hail/pull/5579:46,Testability,test,tests,46,There were only effectively two non-duplicate tests as far as I can tell. I moved them into IRSuite.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5579
https://github.com/hail-is/hail/pull/5582:55,Availability,down,down,55,"This should take the time it takes to run IRSuite back down to ~7 minutes. Still not great, but better than 12.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5582
https://github.com/hail-is/hail/issues/5583:533,Availability,error,error,533,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; 0.2; ### What you did:; Export hail table to elasticsearch; >>> hl.utils.get_movie_lens('data/'); >>> users = hl.read_table('data/users.ht'); >>> hl.export_elasticsearch(users, 'XX.XXX.XXX.XXX', XXXX, 'data', 'variant', 200,config=N; one, verbose=True). ### What went wrong (all error messages here, including the full java stack trace):. Error:; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""</usr/local/lib/python3.6/site-packages/decorator.py:decorator-gen-1118>"", line 2, in export_elasticsearch; File ""/opt/hail-on-EMR/src/hail/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 561, in wrapper; File ""/opt/hail-on-EMR/src/hail/hail/build/distributions/hail-python.zip/hail/methods/impex.py"", line 2104, in export_elasticsearch; File ""/opt/hail-on-EMR/src/hail/hail/build/distributions/hail-python.zip/hail/table.py"", line 101, in __getattr__; AttributeError: Table instance has no attribute '_jt'",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5583
https://github.com/hail-is/hail/issues/5583:593,Availability,Error,Error,593,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; 0.2; ### What you did:; Export hail table to elasticsearch; >>> hl.utils.get_movie_lens('data/'); >>> users = hl.read_table('data/users.ht'); >>> hl.export_elasticsearch(users, 'XX.XXX.XXX.XXX', XXXX, 'data', 'variant', 200,config=N; one, verbose=True). ### What went wrong (all error messages here, including the full java stack trace):. Error:; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""</usr/local/lib/python3.6/site-packages/decorator.py:decorator-gen-1118>"", line 2, in export_elasticsearch; File ""/opt/hail-on-EMR/src/hail/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 561, in wrapper; File ""/opt/hail-on-EMR/src/hail/hail/build/distributions/hail-python.zip/hail/methods/impex.py"", line 2104, in export_elasticsearch; File ""/opt/hail-on-EMR/src/hail/hail/build/distributions/hail-python.zip/hail/table.py"", line 101, in __getattr__; AttributeError: Table instance has no attribute '_jt'",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5583
https://github.com/hail-is/hail/issues/5583:539,Integrability,message,messages,539,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; 0.2; ### What you did:; Export hail table to elasticsearch; >>> hl.utils.get_movie_lens('data/'); >>> users = hl.read_table('data/users.ht'); >>> hl.export_elasticsearch(users, 'XX.XXX.XXX.XXX', XXXX, 'data', 'variant', 200,config=N; one, verbose=True). ### What went wrong (all error messages here, including the full java stack trace):. Error:; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""</usr/local/lib/python3.6/site-packages/decorator.py:decorator-gen-1118>"", line 2, in export_elasticsearch; File ""/opt/hail-on-EMR/src/hail/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 561, in wrapper; File ""/opt/hail-on-EMR/src/hail/hail/build/distributions/hail-python.zip/hail/methods/impex.py"", line 2104, in export_elasticsearch; File ""/opt/hail-on-EMR/src/hail/hail/build/distributions/hail-python.zip/hail/table.py"", line 101, in __getattr__; AttributeError: Table instance has no attribute '_jt'",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5583
https://github.com/hail-is/hail/issues/5583:900,Integrability,wrap,wrapper,900,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; 0.2; ### What you did:; Export hail table to elasticsearch; >>> hl.utils.get_movie_lens('data/'); >>> users = hl.read_table('data/users.ht'); >>> hl.export_elasticsearch(users, 'XX.XXX.XXX.XXX', XXXX, 'data', 'variant', 200,config=N; one, verbose=True). ### What went wrong (all error messages here, including the full java stack trace):. Error:; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""</usr/local/lib/python3.6/site-packages/decorator.py:decorator-gen-1118>"", line 2, in export_elasticsearch; File ""/opt/hail-on-EMR/src/hail/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 561, in wrapper; File ""/opt/hail-on-EMR/src/hail/hail/build/distributions/hail-python.zip/hail/methods/impex.py"", line 2104, in export_elasticsearch; File ""/opt/hail-on-EMR/src/hail/hail/build/distributions/hail-python.zip/hail/table.py"", line 101, in __getattr__; AttributeError: Table instance has no attribute '_jt'",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5583
https://github.com/hail-is/hail/issues/5583:478,Modifiability,config,config,478,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; 0.2; ### What you did:; Export hail table to elasticsearch; >>> hl.utils.get_movie_lens('data/'); >>> users = hl.read_table('data/users.ht'); >>> hl.export_elasticsearch(users, 'XX.XXX.XXX.XXX', XXXX, 'data', 'variant', 200,config=N; one, verbose=True). ### What went wrong (all error messages here, including the full java stack trace):. Error:; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""</usr/local/lib/python3.6/site-packages/decorator.py:decorator-gen-1118>"", line 2, in export_elasticsearch; File ""/opt/hail-on-EMR/src/hail/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 561, in wrapper; File ""/opt/hail-on-EMR/src/hail/hail/build/distributions/hail-python.zip/hail/methods/impex.py"", line 2104, in export_elasticsearch; File ""/opt/hail-on-EMR/src/hail/hail/build/distributions/hail-python.zip/hail/table.py"", line 101, in __getattr__; AttributeError: Table instance has no attribute '_jt'",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5583
https://github.com/hail-is/hail/pull/5588:34,Testability,assert,assertEvalsTo,34,"- Added an implicit parameter to `assertEvalsTo` to specify the execution strategies you want to ensure support the IR instead of silently failing; - For every test suite that uses `assertEvalsTo`, declared the set of execution strategies that work for a majority of the tests in that suite, overriding individual test methods when necessary; - This gives pretty easy visibility as to the progress of every test block. cc: @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5588
https://github.com/hail-is/hail/pull/5588:160,Testability,test,test,160,"- Added an implicit parameter to `assertEvalsTo` to specify the execution strategies you want to ensure support the IR instead of silently failing; - For every test suite that uses `assertEvalsTo`, declared the set of execution strategies that work for a majority of the tests in that suite, overriding individual test methods when necessary; - This gives pretty easy visibility as to the progress of every test block. cc: @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5588
https://github.com/hail-is/hail/pull/5588:182,Testability,assert,assertEvalsTo,182,"- Added an implicit parameter to `assertEvalsTo` to specify the execution strategies you want to ensure support the IR instead of silently failing; - For every test suite that uses `assertEvalsTo`, declared the set of execution strategies that work for a majority of the tests in that suite, overriding individual test methods when necessary; - This gives pretty easy visibility as to the progress of every test block. cc: @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5588
https://github.com/hail-is/hail/pull/5588:271,Testability,test,tests,271,"- Added an implicit parameter to `assertEvalsTo` to specify the execution strategies you want to ensure support the IR instead of silently failing; - For every test suite that uses `assertEvalsTo`, declared the set of execution strategies that work for a majority of the tests in that suite, overriding individual test methods when necessary; - This gives pretty easy visibility as to the progress of every test block. cc: @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5588
https://github.com/hail-is/hail/pull/5588:314,Testability,test,test,314,"- Added an implicit parameter to `assertEvalsTo` to specify the execution strategies you want to ensure support the IR instead of silently failing; - For every test suite that uses `assertEvalsTo`, declared the set of execution strategies that work for a majority of the tests in that suite, overriding individual test methods when necessary; - This gives pretty easy visibility as to the progress of every test block. cc: @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5588
https://github.com/hail-is/hail/pull/5588:407,Testability,test,test,407,"- Added an implicit parameter to `assertEvalsTo` to specify the execution strategies you want to ensure support the IR instead of silently failing; - For every test suite that uses `assertEvalsTo`, declared the set of execution strategies that work for a majority of the tests in that suite, overriding individual test methods when necessary; - This gives pretty easy visibility as to the progress of every test block. cc: @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5588
https://github.com/hail-is/hail/issues/5589:465,Safety,avoid,avoided,465,"`BlockMatrix.from_entry_expr` is not in the style of our other conversion methods (e.g. (`un`)`localize_entries`, `entries`). We should deprecate it (does python have a way to officially do this? I'd like users to get warnings at least) and replace with `mt.GT.n_alt_alleles().to_block_matrix()`. Obviously this only works if the expression is row & column indexed. @tpoterba thoughts on this? I seem to really hate the `BlockMatrix.from_entry_expr` bit, but we've avoided putting relational methods on expressions.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5589
https://github.com/hail-is/hail/issues/5591:307,Integrability,protocol,protocol,307,"`nbconvert` uses Tornado 6, which has a breaking change. Effectively same issue as https://github.com/hail-is/hail/issues/5505. ```; [E 20:00:16.156 NotebookApp] Uncaught exception GET /instance/b7290cf977194ecebc18854019a0dea3/notebooks/Untitled.ipynb?kernel_name=python3 (10.32.19.179); HTTPServerRequest(protocol='http', host='notebook2.hail.is', method='GET', uri='/instance/b7290cf977194ecebc18854019a0dea3/notebooks/Untitled.ipynb?kernel_name=python3', version='HTTP/1.1', remote_ip='10.32.19.179'); Traceback (most recent call last):; File ""/opt/conda/envs/hail/lib/python3.6/site-packages/tornado/web.py"", line 1697, in _execute; result = method(*self.path_args, **self.path_kwargs); File ""/opt/conda/envs/hail/lib/python3.6/site-packages/tornado/web.py"", line 3174, in wrapper; return method(self, *args, **kwargs); File ""/opt/conda/envs/hail/lib/python3.6/site-packages/notebook/notebook/handlers.py"", line 59, in get; get_custom_frontend_exporters=get_custom_frontend_exporters; File ""/opt/conda/envs/hail/lib/python3.6/site-packages/notebook/base/handlers.py"", line 519, in render_template; return template.render(**ns); File ""/opt/conda/envs/hail/lib/python3.6/site-packages/jinja2/asyncsupport.py"", line 76, in render; return original_render(self, *args, **kwargs); File ""/opt/conda/envs/hail/lib/python3.6/site-packages/jinja2/environment.py"", line 1008, in render; return self.environment.handle_exception(exc_info, True); File ""/opt/conda/envs/hail/lib/python3.6/site-packages/jinja2/environment.py"", line 780, in handle_exception; reraise(exc_type, exc_value, tb); File ""/opt/conda/envs/hail/lib/python3.6/site-packages/jinja2/_compat.py"", line 37, in reraise; raise value.with_traceback(tb); File ""/opt/conda/envs/hail/lib/python3.6/site-packages/notebook/templates/notebook.html"", line 1, in top-level template code; {% extends ""page.html"" %}; File ""/opt/conda/envs/hail/lib/python3.6/site-packages/notebook/templates/page.html"", line 154, in top-level template code; {% block head",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5591
https://github.com/hail-is/hail/issues/5591:778,Integrability,wrap,wrapper,778,"`nbconvert` uses Tornado 6, which has a breaking change. Effectively same issue as https://github.com/hail-is/hail/issues/5505. ```; [E 20:00:16.156 NotebookApp] Uncaught exception GET /instance/b7290cf977194ecebc18854019a0dea3/notebooks/Untitled.ipynb?kernel_name=python3 (10.32.19.179); HTTPServerRequest(protocol='http', host='notebook2.hail.is', method='GET', uri='/instance/b7290cf977194ecebc18854019a0dea3/notebooks/Untitled.ipynb?kernel_name=python3', version='HTTP/1.1', remote_ip='10.32.19.179'); Traceback (most recent call last):; File ""/opt/conda/envs/hail/lib/python3.6/site-packages/tornado/web.py"", line 1697, in _execute; result = method(*self.path_args, **self.path_kwargs); File ""/opt/conda/envs/hail/lib/python3.6/site-packages/tornado/web.py"", line 3174, in wrapper; return method(self, *args, **kwargs); File ""/opt/conda/envs/hail/lib/python3.6/site-packages/notebook/notebook/handlers.py"", line 59, in get; get_custom_frontend_exporters=get_custom_frontend_exporters; File ""/opt/conda/envs/hail/lib/python3.6/site-packages/notebook/base/handlers.py"", line 519, in render_template; return template.render(**ns); File ""/opt/conda/envs/hail/lib/python3.6/site-packages/jinja2/asyncsupport.py"", line 76, in render; return original_render(self, *args, **kwargs); File ""/opt/conda/envs/hail/lib/python3.6/site-packages/jinja2/environment.py"", line 1008, in render; return self.environment.handle_exception(exc_info, True); File ""/opt/conda/envs/hail/lib/python3.6/site-packages/jinja2/environment.py"", line 780, in handle_exception; reraise(exc_type, exc_value, tb); File ""/opt/conda/envs/hail/lib/python3.6/site-packages/jinja2/_compat.py"", line 37, in reraise; raise value.with_traceback(tb); File ""/opt/conda/envs/hail/lib/python3.6/site-packages/notebook/templates/notebook.html"", line 1, in top-level template code; {% extends ""page.html"" %}; File ""/opt/conda/envs/hail/lib/python3.6/site-packages/notebook/templates/page.html"", line 154, in top-level template code; {% block head",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5591
https://github.com/hail-is/hail/issues/5591:1840,Modifiability,extend,extends,1840," File ""/opt/conda/envs/hail/lib/python3.6/site-packages/notebook/notebook/handlers.py"", line 59, in get; get_custom_frontend_exporters=get_custom_frontend_exporters; File ""/opt/conda/envs/hail/lib/python3.6/site-packages/notebook/base/handlers.py"", line 519, in render_template; return template.render(**ns); File ""/opt/conda/envs/hail/lib/python3.6/site-packages/jinja2/asyncsupport.py"", line 76, in render; return original_render(self, *args, **kwargs); File ""/opt/conda/envs/hail/lib/python3.6/site-packages/jinja2/environment.py"", line 1008, in render; return self.environment.handle_exception(exc_info, True); File ""/opt/conda/envs/hail/lib/python3.6/site-packages/jinja2/environment.py"", line 780, in handle_exception; reraise(exc_type, exc_value, tb); File ""/opt/conda/envs/hail/lib/python3.6/site-packages/jinja2/_compat.py"", line 37, in reraise; raise value.with_traceback(tb); File ""/opt/conda/envs/hail/lib/python3.6/site-packages/notebook/templates/notebook.html"", line 1, in top-level template code; {% extends ""page.html"" %}; File ""/opt/conda/envs/hail/lib/python3.6/site-packages/notebook/templates/page.html"", line 154, in top-level template code; {% block header %}; File ""/opt/conda/envs/hail/lib/python3.6/site-packages/notebook/templates/notebook.html"", line 120, in block ""header""; {% for exporter in get_custom_frontend_exporters() %}; File ""/opt/conda/envs/hail/lib/python3.6/site-packages/notebook/notebook/handlers.py"", line 19, in get_custom_frontend_exporters; from nbconvert.exporters.base import get_export_names, get_exporter; File ""/opt/conda/envs/hail/lib/python3.6/site-packages/nbconvert/__init__.py"", line 7, in <module>; from . import postprocessors; File ""/opt/conda/envs/hail/lib/python3.6/site-packages/nbconvert/postprocessors/__init__.py"", line 5, in <module>; from .serve import ServePostProcessor; File ""/opt/conda/envs/hail/lib/python3.6/site-packages/nbconvert/postprocessors/serve.py"", line 19, in <module>; class ProxyHandler(web.RequestHandler):; File ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5591
https://github.com/hail-is/hail/pull/5593:251,Deployability,deploy,deployment,251,"This adds apiserver to CI, and modifies notebook to use the hail-jupyter image that the former defines. To ensure that it's using the appropriate image, notebook2's Makefile has been modified to read the [""hail-jupyter-image"" label that the apiserver deployment defines](https://github.com/hail-is/hail/blob/master/apiserver/deployment.yaml.in#L74); - the Makefile tests that we get a non-empty string from the kubectl command used to read this value before proceeding. Stacks on https://github.com/hail-is/hail/pull/5592, without which notebook2 wouldn't work when also defaulting to hail-jupyter. I used the ""dependencies"" property in `projects.yaml` as well as presenting the proper order, but that doesn't look like it's actually used yet. Still, it seems like a good idea, so if that is planned still I would like notebook2 to take advantage. ```; ..//env-setup.sh:for project in $(cat projects.yaml | grep '^- project: ' | sed 's/^- project: //'); ..//env-setup.sh-do; ..//env-setup.sh- if [[ -e $project/environment.yml ]]; ..//env-setup.sh- then; ..//env-setup.sh- ${CONDA_BINARY} env create -f $project/environment.yml || ${CONDA_BINARY} env update -f $project/environment.yml; ..//env-setup.sh- fi; ..//env-setup.sh-done; ```. ```; ..//hail-ci-build.sh:PROJECTS=$(cat projects.yaml | grep '^- project: ' | sed 's/^- project: //'); ..//hail-ci-build.sh-; ..//hail-ci-build.sh-for project in $PROJECTS; do; ..//hail-ci-build.sh- if [[ -e $project/hail-ci-build.sh ]]; then; ..//hail-ci-build.sh- CHANGED=$(python3 project-changed.py target/$TARGET_BRANCH $project); ..//hail-ci-build.sh- if [[ $CHANGED != no ]]; then; ..//hail-ci-build.sh- (cd $project && /bin/bash hail-ci-build.sh); ..//hail-ci-build.sh- fi; ..//hail-ci-build.sh- fi; ..//hail-ci-build.sh-done; ```. cc @cseed, @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5593
https://github.com/hail-is/hail/pull/5593:325,Deployability,deploy,deployment,325,"This adds apiserver to CI, and modifies notebook to use the hail-jupyter image that the former defines. To ensure that it's using the appropriate image, notebook2's Makefile has been modified to read the [""hail-jupyter-image"" label that the apiserver deployment defines](https://github.com/hail-is/hail/blob/master/apiserver/deployment.yaml.in#L74); - the Makefile tests that we get a non-empty string from the kubectl command used to read this value before proceeding. Stacks on https://github.com/hail-is/hail/pull/5592, without which notebook2 wouldn't work when also defaulting to hail-jupyter. I used the ""dependencies"" property in `projects.yaml` as well as presenting the proper order, but that doesn't look like it's actually used yet. Still, it seems like a good idea, so if that is planned still I would like notebook2 to take advantage. ```; ..//env-setup.sh:for project in $(cat projects.yaml | grep '^- project: ' | sed 's/^- project: //'); ..//env-setup.sh-do; ..//env-setup.sh- if [[ -e $project/environment.yml ]]; ..//env-setup.sh- then; ..//env-setup.sh- ${CONDA_BINARY} env create -f $project/environment.yml || ${CONDA_BINARY} env update -f $project/environment.yml; ..//env-setup.sh- fi; ..//env-setup.sh-done; ```. ```; ..//hail-ci-build.sh:PROJECTS=$(cat projects.yaml | grep '^- project: ' | sed 's/^- project: //'); ..//hail-ci-build.sh-; ..//hail-ci-build.sh-for project in $PROJECTS; do; ..//hail-ci-build.sh- if [[ -e $project/hail-ci-build.sh ]]; then; ..//hail-ci-build.sh- CHANGED=$(python3 project-changed.py target/$TARGET_BRANCH $project); ..//hail-ci-build.sh- if [[ $CHANGED != no ]]; then; ..//hail-ci-build.sh- (cd $project && /bin/bash hail-ci-build.sh); ..//hail-ci-build.sh- fi; ..//hail-ci-build.sh- fi; ..//hail-ci-build.sh-done; ```. cc @cseed, @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5593
https://github.com/hail-is/hail/pull/5593:1151,Deployability,update,update,1151,"This adds apiserver to CI, and modifies notebook to use the hail-jupyter image that the former defines. To ensure that it's using the appropriate image, notebook2's Makefile has been modified to read the [""hail-jupyter-image"" label that the apiserver deployment defines](https://github.com/hail-is/hail/blob/master/apiserver/deployment.yaml.in#L74); - the Makefile tests that we get a non-empty string from the kubectl command used to read this value before proceeding. Stacks on https://github.com/hail-is/hail/pull/5592, without which notebook2 wouldn't work when also defaulting to hail-jupyter. I used the ""dependencies"" property in `projects.yaml` as well as presenting the proper order, but that doesn't look like it's actually used yet. Still, it seems like a good idea, so if that is planned still I would like notebook2 to take advantage. ```; ..//env-setup.sh:for project in $(cat projects.yaml | grep '^- project: ' | sed 's/^- project: //'); ..//env-setup.sh-do; ..//env-setup.sh- if [[ -e $project/environment.yml ]]; ..//env-setup.sh- then; ..//env-setup.sh- ${CONDA_BINARY} env create -f $project/environment.yml || ${CONDA_BINARY} env update -f $project/environment.yml; ..//env-setup.sh- fi; ..//env-setup.sh-done; ```. ```; ..//hail-ci-build.sh:PROJECTS=$(cat projects.yaml | grep '^- project: ' | sed 's/^- project: //'); ..//hail-ci-build.sh-; ..//hail-ci-build.sh-for project in $PROJECTS; do; ..//hail-ci-build.sh- if [[ -e $project/hail-ci-build.sh ]]; then; ..//hail-ci-build.sh- CHANGED=$(python3 project-changed.py target/$TARGET_BRANCH $project); ..//hail-ci-build.sh- if [[ $CHANGED != no ]]; then; ..//hail-ci-build.sh- (cd $project && /bin/bash hail-ci-build.sh); ..//hail-ci-build.sh- fi; ..//hail-ci-build.sh- fi; ..//hail-ci-build.sh-done; ```. cc @cseed, @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5593
https://github.com/hail-is/hail/pull/5593:611,Integrability,depend,dependencies,611,"This adds apiserver to CI, and modifies notebook to use the hail-jupyter image that the former defines. To ensure that it's using the appropriate image, notebook2's Makefile has been modified to read the [""hail-jupyter-image"" label that the apiserver deployment defines](https://github.com/hail-is/hail/blob/master/apiserver/deployment.yaml.in#L74); - the Makefile tests that we get a non-empty string from the kubectl command used to read this value before proceeding. Stacks on https://github.com/hail-is/hail/pull/5592, without which notebook2 wouldn't work when also defaulting to hail-jupyter. I used the ""dependencies"" property in `projects.yaml` as well as presenting the proper order, but that doesn't look like it's actually used yet. Still, it seems like a good idea, so if that is planned still I would like notebook2 to take advantage. ```; ..//env-setup.sh:for project in $(cat projects.yaml | grep '^- project: ' | sed 's/^- project: //'); ..//env-setup.sh-do; ..//env-setup.sh- if [[ -e $project/environment.yml ]]; ..//env-setup.sh- then; ..//env-setup.sh- ${CONDA_BINARY} env create -f $project/environment.yml || ${CONDA_BINARY} env update -f $project/environment.yml; ..//env-setup.sh- fi; ..//env-setup.sh-done; ```. ```; ..//hail-ci-build.sh:PROJECTS=$(cat projects.yaml | grep '^- project: ' | sed 's/^- project: //'); ..//hail-ci-build.sh-; ..//hail-ci-build.sh-for project in $PROJECTS; do; ..//hail-ci-build.sh- if [[ -e $project/hail-ci-build.sh ]]; then; ..//hail-ci-build.sh- CHANGED=$(python3 project-changed.py target/$TARGET_BRANCH $project); ..//hail-ci-build.sh- if [[ $CHANGED != no ]]; then; ..//hail-ci-build.sh- (cd $project && /bin/bash hail-ci-build.sh); ..//hail-ci-build.sh- fi; ..//hail-ci-build.sh- fi; ..//hail-ci-build.sh-done; ```. cc @cseed, @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5593
https://github.com/hail-is/hail/pull/5593:365,Testability,test,tests,365,"This adds apiserver to CI, and modifies notebook to use the hail-jupyter image that the former defines. To ensure that it's using the appropriate image, notebook2's Makefile has been modified to read the [""hail-jupyter-image"" label that the apiserver deployment defines](https://github.com/hail-is/hail/blob/master/apiserver/deployment.yaml.in#L74); - the Makefile tests that we get a non-empty string from the kubectl command used to read this value before proceeding. Stacks on https://github.com/hail-is/hail/pull/5592, without which notebook2 wouldn't work when also defaulting to hail-jupyter. I used the ""dependencies"" property in `projects.yaml` as well as presenting the proper order, but that doesn't look like it's actually used yet. Still, it seems like a good idea, so if that is planned still I would like notebook2 to take advantage. ```; ..//env-setup.sh:for project in $(cat projects.yaml | grep '^- project: ' | sed 's/^- project: //'); ..//env-setup.sh-do; ..//env-setup.sh- if [[ -e $project/environment.yml ]]; ..//env-setup.sh- then; ..//env-setup.sh- ${CONDA_BINARY} env create -f $project/environment.yml || ${CONDA_BINARY} env update -f $project/environment.yml; ..//env-setup.sh- fi; ..//env-setup.sh-done; ```. ```; ..//hail-ci-build.sh:PROJECTS=$(cat projects.yaml | grep '^- project: ' | sed 's/^- project: //'); ..//hail-ci-build.sh-; ..//hail-ci-build.sh-for project in $PROJECTS; do; ..//hail-ci-build.sh- if [[ -e $project/hail-ci-build.sh ]]; then; ..//hail-ci-build.sh- CHANGED=$(python3 project-changed.py target/$TARGET_BRANCH $project); ..//hail-ci-build.sh- if [[ $CHANGED != no ]]; then; ..//hail-ci-build.sh- (cd $project && /bin/bash hail-ci-build.sh); ..//hail-ci-build.sh- fi; ..//hail-ci-build.sh- fi; ..//hail-ci-build.sh-done; ```. cc @cseed, @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5593
https://github.com/hail-is/hail/pull/5596:313,Performance,perform,performance,313,"Saves memory on the master but has an unfortunate side effect of; drastically multiplying the number of times a tabix file is read, as it; is read once per partition per vcf rather than once per vcf. This change places tabix reading in a more critical path of the gVCF; merger, I would appreciate a more detailed performance audit of that; code, in addition to looking over this change. From my measurements it looks like we pay a 20-30 second cost per partition of 100 gVCFs. cc: @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5596
https://github.com/hail-is/hail/pull/5596:325,Security,audit,audit,325,"Saves memory on the master but has an unfortunate side effect of; drastically multiplying the number of times a tabix file is read, as it; is read once per partition per vcf rather than once per vcf. This change places tabix reading in a more critical path of the gVCF; merger, I would appreciate a more detailed performance audit of that; code, in addition to looking over this change. From my measurements it looks like we pay a 20-30 second cost per partition of 100 gVCFs. cc: @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5596
https://github.com/hail-is/hail/pull/5598:108,Performance,perform,performance,108,First step in fixing: https://github.com/hail-is/hail/issues/5358. @chrisvittal FYI this should improve the performance of multiple aggregations across samples in MatrixTable.anntoate_rows.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5598
https://github.com/hail-is/hail/pull/5601:47,Availability,avail,available,47,"Both of these support a number of features not available in hail.plot:; - interactive legend (click to hide/show elements); - labelling with continuous expressions; - labelling using multiple expressions (will display a dropdown selection widget); - specifying color schemes; - hovering on points displays their coordinates, labels and additional source fields; - legend is also displayed outside of the plotting space to be unobtrusive",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5601
https://github.com/hail-is/hail/pull/5601:141,Deployability,continuous,continuous,141,"Both of these support a number of features not available in hail.plot:; - interactive legend (click to hide/show elements); - labelling with continuous expressions; - labelling using multiple expressions (will display a dropdown selection widget); - specifying color schemes; - hovering on points displays their coordinates, labels and additional source fields; - legend is also displayed outside of the plotting space to be unobtrusive",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5601
https://github.com/hail-is/hail/issues/5603:0,Performance,load,loading,0,"loading a file that has 12 partitions. when setting `hl.init(min_block_size=0)` and then `hl.import_table(..., min_partitions=100)`, now getting only 3 partitions.... ☹️",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5603
https://github.com/hail-is/hail/pull/5604:74,Testability,test,tests,74,"This slows the exponential growth in sleeps dramatically. Enough that the tests don't sometimes wait 30 seconds to learn that a batch is finished. On my laptop, the tests just finished in less than three minutes.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5604
https://github.com/hail-is/hail/pull/5604:165,Testability,test,tests,165,"This slows the exponential growth in sleeps dramatically. Enough that the tests don't sometimes wait 30 seconds to learn that a batch is finished. On my laptop, the tests just finished in less than three minutes.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5604
https://github.com/hail-is/hail/pull/5604:115,Usability,learn,learn,115,"This slows the exponential growth in sleeps dramatically. Enough that the tests don't sometimes wait 30 seconds to learn that a batch is finished. On my laptop, the tests just finished in less than three minutes.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5604
https://github.com/hail-is/hail/pull/5605:88,Modifiability,config,configurable,88,"cc: @akotlar @cseed . I whipped this up today. It modifies the batch api to accept user-configurable cookies and headers. The `BatchClient` looks for a token in `~/.hail/batch/token` or at a user-provided path. There's a whitelist of authorized users. If you can talk directly to batch, you can spoof the `User` header. This makes testing easy. For this approach to be secure, we need to ensure:; - only `gateway` may send HTTP requests to `batch`; - `gateway` does not forward any ""secure"" headers (currently, just `User`)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5605
https://github.com/hail-is/hail/pull/5605:234,Security,authoriz,authorized,234,"cc: @akotlar @cseed . I whipped this up today. It modifies the batch api to accept user-configurable cookies and headers. The `BatchClient` looks for a token in `~/.hail/batch/token` or at a user-provided path. There's a whitelist of authorized users. If you can talk directly to batch, you can spoof the `User` header. This makes testing easy. For this approach to be secure, we need to ensure:; - only `gateway` may send HTTP requests to `batch`; - `gateway` does not forward any ""secure"" headers (currently, just `User`)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5605
https://github.com/hail-is/hail/pull/5605:369,Security,secur,secure,369,"cc: @akotlar @cseed . I whipped this up today. It modifies the batch api to accept user-configurable cookies and headers. The `BatchClient` looks for a token in `~/.hail/batch/token` or at a user-provided path. There's a whitelist of authorized users. If you can talk directly to batch, you can spoof the `User` header. This makes testing easy. For this approach to be secure, we need to ensure:; - only `gateway` may send HTTP requests to `batch`; - `gateway` does not forward any ""secure"" headers (currently, just `User`)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5605
https://github.com/hail-is/hail/pull/5605:483,Security,secur,secure,483,"cc: @akotlar @cseed . I whipped this up today. It modifies the batch api to accept user-configurable cookies and headers. The `BatchClient` looks for a token in `~/.hail/batch/token` or at a user-provided path. There's a whitelist of authorized users. If you can talk directly to batch, you can spoof the `User` header. This makes testing easy. For this approach to be secure, we need to ensure:; - only `gateway` may send HTTP requests to `batch`; - `gateway` does not forward any ""secure"" headers (currently, just `User`)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5605
https://github.com/hail-is/hail/pull/5605:331,Testability,test,testing,331,"cc: @akotlar @cseed . I whipped this up today. It modifies the batch api to accept user-configurable cookies and headers. The `BatchClient` looks for a token in `~/.hail/batch/token` or at a user-provided path. There's a whitelist of authorized users. If you can talk directly to batch, you can spoof the `User` header. This makes testing easy. For this approach to be secure, we need to ensure:; - only `gateway` may send HTTP requests to `batch`; - `gateway` does not forward any ""secure"" headers (currently, just `User`)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5605
https://github.com/hail-is/hail/pull/5606:85,Performance,perform,performance,85,"emitPackEncoder now supports requested type. I see a small (few percent) increase in performance in import_vcf/write on a gVCF file. Still, getting rid of a RVB is always a good change.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5606
https://github.com/hail-is/hail/pull/5607:106,Deployability,update,updated,106,"This might fix the bug you're seeing. Capacity (which should have been tracked separately) wasn't getting updated correctly in copyFrom, so if you did two copyFrom's, the second one would truncate the data.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5607
https://github.com/hail-is/hail/pull/5608:38,Testability,test,testing,38,ldscsim is a simulation framework for testing LD score regression,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5608
https://github.com/hail-is/hail/pull/5609:32,Modifiability,inherit,inherit,32,"Mostly I wanted ArrayEmitter to inherit from EmitTriplet since it is a type of EmitTriplet, just deforested when allowed. Also ArrayEmitter -> EmitStream, which I feel like is a better name for what it's actually doing.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5609
https://github.com/hail-is/hail/pull/5610:564,Modifiability,extend,extend,564,"Defines a TStream/PStream type stub. I've omitted some number of things that other types need to define, as the purpose of the stream type is going to be to ensure that we're never fully instantiating collections where we shouldn't be, e.g. all the rows in a table partition. To that end, I've omitted definitions for ordering since I don't forsee a need for ordering on the entire stream (as opposed to on the element, or a subset thereof), as well as generators for annotations, etc. It basically otherwise mimics the PArray/TArray definitions, but I've made it extend Type/PType directly since most of the extra methods on containers seem irrelevant to streams, having mostly to do with e.g. length and loading specific elements. cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5610
https://github.com/hail-is/hail/pull/5610:706,Performance,load,loading,706,"Defines a TStream/PStream type stub. I've omitted some number of things that other types need to define, as the purpose of the stream type is going to be to ensure that we're never fully instantiating collections where we shouldn't be, e.g. all the rows in a table partition. To that end, I've omitted definitions for ordering since I don't forsee a need for ordering on the entire stream (as opposed to on the element, or a subset thereof), as well as generators for annotations, etc. It basically otherwise mimics the PArray/TArray definitions, but I've made it extend Type/PType directly since most of the extra methods on containers seem irrelevant to streams, having mostly to do with e.g. length and loading specific elements. cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5610
https://github.com/hail-is/hail/pull/5610:31,Testability,stub,stub,31,"Defines a TStream/PStream type stub. I've omitted some number of things that other types need to define, as the purpose of the stream type is going to be to ensure that we're never fully instantiating collections where we shouldn't be, e.g. all the rows in a table partition. To that end, I've omitted definitions for ordering since I don't forsee a need for ordering on the entire stream (as opposed to on the element, or a subset thereof), as well as generators for annotations, etc. It basically otherwise mimics the PArray/TArray definitions, but I've made it extend Type/PType directly since most of the extra methods on containers seem irrelevant to streams, having mostly to do with e.g. length and loading specific elements. cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5610
https://github.com/hail-is/hail/pull/5613:16,Availability,down,down,16,"This should cut down on expensive py4j calls in import_vcfs, and the; infrastructure should also help us with complex java <=> python; interchanges, by passing the releavant information via primitive types; or strings in all cases. Brief summary:. - Add vectorIrs map to HailContext to store lists of irs that may need; be referenced later. - Switch import_vcfs to returning a json string with the id of the IR; array that is now stored in the vectorIrs map, along with the size of; that array and its type, suitable to passed to tmatrix._from_json. - Add JIRVectorReference to python, which is the deserialized version of; the returned json. - Add JavaMatrixVectorRef, which represents a single IR contained in; some stored IR array.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5613
https://github.com/hail-is/hail/pull/5615:249,Availability,down,downloading,249,- Only batch for now -- will add for pipeline and ci later; - This should fail until the kubernetes secret is added; - Requires a password `CLOUD_SQL_PASSWORD` to run the tests locally; (not sure what the best way to distribute this is); - Requires downloading the `cloud_sql_proxy` binary to run the tests locally,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5615
https://github.com/hail-is/hail/pull/5615:37,Deployability,pipeline,pipeline,37,- Only batch for now -- will add for pipeline and ci later; - This should fail until the kubernetes secret is added; - Requires a password `CLOUD_SQL_PASSWORD` to run the tests locally; (not sure what the best way to distribute this is); - Requires downloading the `cloud_sql_proxy` binary to run the tests locally,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5615
https://github.com/hail-is/hail/pull/5615:130,Security,password,password,130,- Only batch for now -- will add for pipeline and ci later; - This should fail until the kubernetes secret is added; - Requires a password `CLOUD_SQL_PASSWORD` to run the tests locally; (not sure what the best way to distribute this is); - Requires downloading the `cloud_sql_proxy` binary to run the tests locally,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5615
https://github.com/hail-is/hail/pull/5615:171,Testability,test,tests,171,- Only batch for now -- will add for pipeline and ci later; - This should fail until the kubernetes secret is added; - Requires a password `CLOUD_SQL_PASSWORD` to run the tests locally; (not sure what the best way to distribute this is); - Requires downloading the `cloud_sql_proxy` binary to run the tests locally,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5615
https://github.com/hail-is/hail/pull/5615:301,Testability,test,tests,301,- Only batch for now -- will add for pipeline and ci later; - This should fail until the kubernetes secret is added; - Requires a password `CLOUD_SQL_PASSWORD` to run the tests locally; (not sure what the best way to distribute this is); - Requires downloading the `cloud_sql_proxy` binary to run the tests locally,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5615
https://github.com/hail-is/hail/pull/5616:72,Testability,test,testing,72,Making one change with all of the SQL packages needed so I can speed up testing of other branches.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5616
https://github.com/hail-is/hail/pull/5618:720,Deployability,update,update,720,"Edit: Ready for a look, besides google sa key secret creation, because I'm not completely sure what the use case is, and whether it should be a namespaced secret. Since speaking with Cotton, I've moved to using our cloud mysql instance to track user resources, to ensure that a single user id results in a single resource. We can use auth0, but that would add complexity, and would really only make sense in the context of notebook (or whatever we end up managing users) I think: while auth0 allows you to add custom claims, I believe you need to first get the user's access token (via authentication), then call (server side, no user input needed) the /management api endpoint to check the existence of the claims, and update if they do not exist. So this requires user interaction. Would need to confirm this, if proven true, we will eventually be able to circumvent this by connecting our own database to their service ([they allow this](https://auth0.com/docs/connections/database/custom-db)). Still separately tracking mapping between user id and our resources feels relatively natural, and simpler. . Right now you could supply any identifier for the user_id, as long as its globally unique. I think using the auth0 id makes the most sense, since that is a guaranteed-unique id. I will need to provide you a way to get those ids if you want to use this outside of notebook2. edit: I opted not to separate user table from resources the user owns, because I expect one row per user, so not denormalized. Also, still needs some tests written (mysql related). cc @jigold, @cseed, @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5618
https://github.com/hail-is/hail/pull/5618:568,Security,access,access,568,"Edit: Ready for a look, besides google sa key secret creation, because I'm not completely sure what the use case is, and whether it should be a namespaced secret. Since speaking with Cotton, I've moved to using our cloud mysql instance to track user resources, to ensure that a single user id results in a single resource. We can use auth0, but that would add complexity, and would really only make sense in the context of notebook (or whatever we end up managing users) I think: while auth0 allows you to add custom claims, I believe you need to first get the user's access token (via authentication), then call (server side, no user input needed) the /management api endpoint to check the existence of the claims, and update if they do not exist. So this requires user interaction. Would need to confirm this, if proven true, we will eventually be able to circumvent this by connecting our own database to their service ([they allow this](https://auth0.com/docs/connections/database/custom-db)). Still separately tracking mapping between user id and our resources feels relatively natural, and simpler. . Right now you could supply any identifier for the user_id, as long as its globally unique. I think using the auth0 id makes the most sense, since that is a guaranteed-unique id. I will need to provide you a way to get those ids if you want to use this outside of notebook2. edit: I opted not to separate user table from resources the user owns, because I expect one row per user, so not denormalized. Also, still needs some tests written (mysql related). cc @jigold, @cseed, @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5618
https://github.com/hail-is/hail/pull/5618:586,Security,authenticat,authentication,586,"Edit: Ready for a look, besides google sa key secret creation, because I'm not completely sure what the use case is, and whether it should be a namespaced secret. Since speaking with Cotton, I've moved to using our cloud mysql instance to track user resources, to ensure that a single user id results in a single resource. We can use auth0, but that would add complexity, and would really only make sense in the context of notebook (or whatever we end up managing users) I think: while auth0 allows you to add custom claims, I believe you need to first get the user's access token (via authentication), then call (server side, no user input needed) the /management api endpoint to check the existence of the claims, and update if they do not exist. So this requires user interaction. Would need to confirm this, if proven true, we will eventually be able to circumvent this by connecting our own database to their service ([they allow this](https://auth0.com/docs/connections/database/custom-db)). Still separately tracking mapping between user id and our resources feels relatively natural, and simpler. . Right now you could supply any identifier for the user_id, as long as its globally unique. I think using the auth0 id makes the most sense, since that is a guaranteed-unique id. I will need to provide you a way to get those ids if you want to use this outside of notebook2. edit: I opted not to separate user table from resources the user owns, because I expect one row per user, so not denormalized. Also, still needs some tests written (mysql related). cc @jigold, @cseed, @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5618
https://github.com/hail-is/hail/pull/5618:1531,Testability,test,tests,1531,"Edit: Ready for a look, besides google sa key secret creation, because I'm not completely sure what the use case is, and whether it should be a namespaced secret. Since speaking with Cotton, I've moved to using our cloud mysql instance to track user resources, to ensure that a single user id results in a single resource. We can use auth0, but that would add complexity, and would really only make sense in the context of notebook (or whatever we end up managing users) I think: while auth0 allows you to add custom claims, I believe you need to first get the user's access token (via authentication), then call (server side, no user input needed) the /management api endpoint to check the existence of the claims, and update if they do not exist. So this requires user interaction. Would need to confirm this, if proven true, we will eventually be able to circumvent this by connecting our own database to their service ([they allow this](https://auth0.com/docs/connections/database/custom-db)). Still separately tracking mapping between user id and our resources feels relatively natural, and simpler. . Right now you could supply any identifier for the user_id, as long as its globally unique. I think using the auth0 id makes the most sense, since that is a guaranteed-unique id. I will need to provide you a way to get those ids if you want to use this outside of notebook2. edit: I opted not to separate user table from resources the user owns, because I expect one row per user, so not denormalized. Also, still needs some tests written (mysql related). cc @jigold, @cseed, @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5618
https://github.com/hail-is/hail/pull/5618:1096,Usability,simpl,simpler,1096,"Edit: Ready for a look, besides google sa key secret creation, because I'm not completely sure what the use case is, and whether it should be a namespaced secret. Since speaking with Cotton, I've moved to using our cloud mysql instance to track user resources, to ensure that a single user id results in a single resource. We can use auth0, but that would add complexity, and would really only make sense in the context of notebook (or whatever we end up managing users) I think: while auth0 allows you to add custom claims, I believe you need to first get the user's access token (via authentication), then call (server side, no user input needed) the /management api endpoint to check the existence of the claims, and update if they do not exist. So this requires user interaction. Would need to confirm this, if proven true, we will eventually be able to circumvent this by connecting our own database to their service ([they allow this](https://auth0.com/docs/connections/database/custom-db)). Still separately tracking mapping between user id and our resources feels relatively natural, and simpler. . Right now you could supply any identifier for the user_id, as long as its globally unique. I think using the auth0 id makes the most sense, since that is a guaranteed-unique id. I will need to provide you a way to get those ids if you want to use this outside of notebook2. edit: I opted not to separate user table from resources the user owns, because I expect one row per user, so not denormalized. Also, still needs some tests written (mysql related). cc @jigold, @cseed, @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5618
https://github.com/hail-is/hail/pull/5620:0,Deployability,Deploy,Deploy,0,Deploy is currently failing because deploy script tries to deploy in current namespace (batch-pods) and doesn't have permission.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5620
https://github.com/hail-is/hail/pull/5620:36,Deployability,deploy,deploy,36,Deploy is currently failing because deploy script tries to deploy in current namespace (batch-pods) and doesn't have permission.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5620
https://github.com/hail-is/hail/pull/5620:59,Deployability,deploy,deploy,59,Deploy is currently failing because deploy script tries to deploy in current namespace (batch-pods) and doesn't have permission.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5620
https://github.com/hail-is/hail/pull/5623:204,Deployability,deploy,deploys,204,"FYI @danking @jigold @akotlar This might be contentious. The wisdom is to make docker images small, but I think this will greatly improve our docker pull/build/push times. Watching some recent builds and deploys, it seems insane, the time we spend and the number of images we build with (mostly) duplicate contents. The goal here is to build a common base image for all containers in our cluster. I built on Ubuntu instead of Debian because it has Python 3.6 by default. It contains useful stuff I often find myself installing when debugging inside the cluster (htop, emacs, others should feel free to add tools they like), python3 and pip packages needed by any service (no conda, thank you very much), jvm and the google-cloud-sdk with kubectl. I also build a spark-base (built on base) with spark and the google hadoop connector. Currently, I just use the base in scorecard. If we like this, I'll convert the other services (and pr-builder, which is hopefully not long for this world) on top of this. I will also make image fetcher fetch the base image as well as the notebook images. The current scorecard image is ~900MB. The new base image is ~2GB, and scorecard adds an additional 15KB to that.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5623
https://github.com/hail-is/hail/pull/5623:516,Deployability,install,installing,516,"FYI @danking @jigold @akotlar This might be contentious. The wisdom is to make docker images small, but I think this will greatly improve our docker pull/build/push times. Watching some recent builds and deploys, it seems insane, the time we spend and the number of images we build with (mostly) duplicate contents. The goal here is to build a common base image for all containers in our cluster. I built on Ubuntu instead of Debian because it has Python 3.6 by default. It contains useful stuff I often find myself installing when debugging inside the cluster (htop, emacs, others should feel free to add tools they like), python3 and pip packages needed by any service (no conda, thank you very much), jvm and the google-cloud-sdk with kubectl. I also build a spark-base (built on base) with spark and the google hadoop connector. Currently, I just use the base in scorecard. If we like this, I'll convert the other services (and pr-builder, which is hopefully not long for this world) on top of this. I will also make image fetcher fetch the base image as well as the notebook images. The current scorecard image is ~900MB. The new base image is ~2GB, and scorecard adds an additional 15KB to that.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5623
https://github.com/hail-is/hail/pull/5624:138,Performance,concurren,concurrent,138,"Switch apiserver from flask to aiohttp. Mostly boilerplate, except calling into the JVM is blocking. Therefore, I execute JVM calls via a concurrent ThreadPoolExecutor with (a somewhat randomly selected) 16 threads. py4j is thread safe and executes each request on the server (Java) side in a separate thread: https://github.com/bartdag/py4j/blob/master/py4j-python/src/py4j/java_gateway.py#L898",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5624
https://github.com/hail-is/hail/pull/5624:231,Safety,safe,safe,231,"Switch apiserver from flask to aiohttp. Mostly boilerplate, except calling into the JVM is blocking. Therefore, I execute JVM calls via a concurrent ThreadPoolExecutor with (a somewhat randomly selected) 16 threads. py4j is thread safe and executes each request on the server (Java) side in a separate thread: https://github.com/bartdag/py4j/blob/master/py4j-python/src/py4j/java_gateway.py#L898",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5624
https://github.com/hail-is/hail/issues/5625:134,Integrability,rout,route,134,"Just want to track where aiodns is used, mostly because I don't understand dns resolution during client request issuance, acceptance (route registration/resolution), and want to. I know that asyncio provides the ability to lookup ip and domain name (dns lookup, reverse). But does this need to happen when I issue a normal request (in which I don't want to return the domain->ip mapping)?. Relates to #5616, #5624. cc @jigold, @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5625
https://github.com/hail-is/hail/pull/5627:214,Testability,log,logic,214,"- Implemented an NDArray as simply a struct with enough information to properly index but no knowledge of the data type, so no templating required.; - Convert child IRs from regions to `std::vector` so the NDArray logic is simpler and isn't coupled to regions. Maybe it doesn't need to be decoupled?; - Currently passing by value in the code-gen for simplicity and since the values are pretty small, but would appreciate feedback on memory-management strategies for non-region values like NDArray.; - Tests extract singular values since we can't translate these arrays back into Java.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5627
https://github.com/hail-is/hail/pull/5627:501,Testability,Test,Tests,501,"- Implemented an NDArray as simply a struct with enough information to properly index but no knowledge of the data type, so no templating required.; - Convert child IRs from regions to `std::vector` so the NDArray logic is simpler and isn't coupled to regions. Maybe it doesn't need to be decoupled?; - Currently passing by value in the code-gen for simplicity and since the values are pretty small, but would appreciate feedback on memory-management strategies for non-region values like NDArray.; - Tests extract singular values since we can't translate these arrays back into Java.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5627
https://github.com/hail-is/hail/pull/5627:28,Usability,simpl,simply,28,"- Implemented an NDArray as simply a struct with enough information to properly index but no knowledge of the data type, so no templating required.; - Convert child IRs from regions to `std::vector` so the NDArray logic is simpler and isn't coupled to regions. Maybe it doesn't need to be decoupled?; - Currently passing by value in the code-gen for simplicity and since the values are pretty small, but would appreciate feedback on memory-management strategies for non-region values like NDArray.; - Tests extract singular values since we can't translate these arrays back into Java.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5627
https://github.com/hail-is/hail/pull/5627:223,Usability,simpl,simpler,223,"- Implemented an NDArray as simply a struct with enough information to properly index but no knowledge of the data type, so no templating required.; - Convert child IRs from regions to `std::vector` so the NDArray logic is simpler and isn't coupled to regions. Maybe it doesn't need to be decoupled?; - Currently passing by value in the code-gen for simplicity and since the values are pretty small, but would appreciate feedback on memory-management strategies for non-region values like NDArray.; - Tests extract singular values since we can't translate these arrays back into Java.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5627
https://github.com/hail-is/hail/pull/5627:350,Usability,simpl,simplicity,350,"- Implemented an NDArray as simply a struct with enough information to properly index but no knowledge of the data type, so no templating required.; - Convert child IRs from regions to `std::vector` so the NDArray logic is simpler and isn't coupled to regions. Maybe it doesn't need to be decoupled?; - Currently passing by value in the code-gen for simplicity and since the values are pretty small, but would appreciate feedback on memory-management strategies for non-region values like NDArray.; - Tests extract singular values since we can't translate these arrays back into Java.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5627
https://github.com/hail-is/hail/pull/5627:421,Usability,feedback,feedback,421,"- Implemented an NDArray as simply a struct with enough information to properly index but no knowledge of the data type, so no templating required.; - Convert child IRs from regions to `std::vector` so the NDArray logic is simpler and isn't coupled to regions. Maybe it doesn't need to be decoupled?; - Currently passing by value in the code-gen for simplicity and since the values are pretty small, but would appreciate feedback on memory-management strategies for non-region values like NDArray.; - Tests extract singular values since we can't translate these arrays back into Java.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5627
https://github.com/hail-is/hail/issues/5630:2206,Integrability,Wrap,WrappedArray,2206,testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:767); 	at org.testng.TestRunner.run(TestRunner.java:617); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); 	at org.testng.SuiteRunner.run(SuiteRunner.java:254); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:123); Caused by: java.lang.ArrayIndexOutOfBoundsException: 3; 	at scala.collection.mutable.WrappedArray$ofRef.apply(WrappedArray.scala:127); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33$$anonfun$apply$19.getKey$1(OrderingSuite.scala:345); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33$$anonfun$apply$19.apply(OrderingSuite.scala:347); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33$$anonfun$apply$19.apply(OrderingSuite.scala:318); 	at is.hail.utils.package$.using(package.scala:613); 	at is.hail.annotations.Region$.scoped(Region.scala:13); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33.apply(OrderingSuite.scala:318); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33.apply(OrderingSuite.scala:314); 	at is.hail.check.GenProp1$$anonfun$apply$1$$anonfun$1.apply$mcZ$sp(Prop.scala:28); 	at is.hail.check.GenProp1$$anonfun$apply$1$$anonfun$1.apply(Prop.scala:28); 	at is.hail.check.GenProp1$$anonfun$apply$1$$anonfun$1.apply(Prop.scala:28); 	at scala.util.Try$.apply(Try.scala:192); 	at is.hail.check.GenProp1$$anonfun$apply$1.apply$mcVI$sp(Prop.scala:28); 	... 27 more; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5630
https://github.com/hail-is/hail/issues/5630:2231,Integrability,Wrap,WrappedArray,2231,testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:767); 	at org.testng.TestRunner.run(TestRunner.java:617); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); 	at org.testng.SuiteRunner.run(SuiteRunner.java:254); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:123); Caused by: java.lang.ArrayIndexOutOfBoundsException: 3; 	at scala.collection.mutable.WrappedArray$ofRef.apply(WrappedArray.scala:127); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33$$anonfun$apply$19.getKey$1(OrderingSuite.scala:345); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33$$anonfun$apply$19.apply(OrderingSuite.scala:347); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33$$anonfun$apply$19.apply(OrderingSuite.scala:318); 	at is.hail.utils.package$.using(package.scala:613); 	at is.hail.annotations.Region$.scoped(Region.scala:13); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33.apply(OrderingSuite.scala:318); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33.apply(OrderingSuite.scala:314); 	at is.hail.check.GenProp1$$anonfun$apply$1$$anonfun$1.apply$mcZ$sp(Prop.scala:28); 	at is.hail.check.GenProp1$$anonfun$apply$1$$anonfun$1.apply(Prop.scala:28); 	at is.hail.check.GenProp1$$anonfun$apply$1$$anonfun$1.apply(Prop.scala:28); 	at scala.util.Try$.apply(Try.scala:192); 	at is.hail.check.GenProp1$$anonfun$apply$1.apply$mcVI$sp(Prop.scala:28); 	... 27 more; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5630
https://github.com/hail-is/hail/issues/5630:23,Testability,test,tests,23,"crank up the number of tests in Prop.scala to replicate, or use this one:. ```; > ARG_0: (dict<call, float64>,Map(3 -> 1.4756407611933507E308, 0 -> 46.25904804767313, 1 -> 54.68552327526001),12). java.lang.AssertionError: java.lang.ArrayIndexOutOfBoundsException: 3. 	at is.hail.check.GenProp1$$anonfun$apply$1.apply$mcVI$sp(Prop.scala:38); 	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160); 	at is.hail.check.GenProp1.apply(Prop.scala:26); 	at is.hail.check.Prop.check(Prop.scala:19); 	at is.hail.expr.ir.OrderingSuite.testBinarySearchOnDict(OrderingSuite.scala:357); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:767); 	at org.testng.TestRunner.run(TestRunner.java:617); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); 	at org.testng.SuiteRunner.run(SuiteRunner.java:254); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5630
https://github.com/hail-is/hail/issues/5630:206,Testability,Assert,AssertionError,206,"crank up the number of tests in Prop.scala to replicate, or use this one:. ```; > ARG_0: (dict<call, float64>,Map(3 -> 1.4756407611933507E308, 0 -> 46.25904804767313, 1 -> 54.68552327526001),12). java.lang.AssertionError: java.lang.ArrayIndexOutOfBoundsException: 3. 	at is.hail.check.GenProp1$$anonfun$apply$1.apply$mcVI$sp(Prop.scala:38); 	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160); 	at is.hail.check.GenProp1.apply(Prop.scala:26); 	at is.hail.check.Prop.check(Prop.scala:19); 	at is.hail.expr.ir.OrderingSuite.testBinarySearchOnDict(OrderingSuite.scala:357); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:767); 	at org.testng.TestRunner.run(TestRunner.java:617); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); 	at org.testng.SuiteRunner.run(SuiteRunner.java:254); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5630
https://github.com/hail-is/hail/issues/5630:539,Testability,test,testBinarySearchOnDict,539,"crank up the number of tests in Prop.scala to replicate, or use this one:. ```; > ARG_0: (dict<call, float64>,Map(3 -> 1.4756407611933507E308, 0 -> 46.25904804767313, 1 -> 54.68552327526001),12). java.lang.AssertionError: java.lang.ArrayIndexOutOfBoundsException: 3. 	at is.hail.check.GenProp1$$anonfun$apply$1.apply$mcVI$sp(Prop.scala:38); 	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160); 	at is.hail.check.GenProp1.apply(Prop.scala:26); 	at is.hail.check.Prop.check(Prop.scala:19); 	at is.hail.expr.ir.OrderingSuite.testBinarySearchOnDict(OrderingSuite.scala:357); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:767); 	at org.testng.TestRunner.run(TestRunner.java:617); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); 	at org.testng.SuiteRunner.run(SuiteRunner.java:254); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5630
https://github.com/hail-is/hail/issues/5630:889,Testability,test,testng,889,"crank up the number of tests in Prop.scala to replicate, or use this one:. ```; > ARG_0: (dict<call, float64>,Map(3 -> 1.4756407611933507E308, 0 -> 46.25904804767313, 1 -> 54.68552327526001),12). java.lang.AssertionError: java.lang.ArrayIndexOutOfBoundsException: 3. 	at is.hail.check.GenProp1$$anonfun$apply$1.apply$mcVI$sp(Prop.scala:38); 	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160); 	at is.hail.check.GenProp1.apply(Prop.scala:26); 	at is.hail.check.Prop.check(Prop.scala:19); 	at is.hail.expr.ir.OrderingSuite.testBinarySearchOnDict(OrderingSuite.scala:357); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:767); 	at org.testng.TestRunner.run(TestRunner.java:617); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); 	at org.testng.SuiteRunner.run(SuiteRunner.java:254); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5630
https://github.com/hail-is/hail/issues/5630:982,Testability,test,testng,982,"crank up the number of tests in Prop.scala to replicate, or use this one:. ```; > ARG_0: (dict<call, float64>,Map(3 -> 1.4756407611933507E308, 0 -> 46.25904804767313, 1 -> 54.68552327526001),12). java.lang.AssertionError: java.lang.ArrayIndexOutOfBoundsException: 3. 	at is.hail.check.GenProp1$$anonfun$apply$1.apply$mcVI$sp(Prop.scala:38); 	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160); 	at is.hail.check.GenProp1.apply(Prop.scala:26); 	at is.hail.check.Prop.check(Prop.scala:19); 	at is.hail.expr.ir.OrderingSuite.testBinarySearchOnDict(OrderingSuite.scala:357); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:767); 	at org.testng.TestRunner.run(TestRunner.java:617); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); 	at org.testng.SuiteRunner.run(SuiteRunner.java:254); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5630
https://github.com/hail-is/hail/issues/5630:1046,Testability,test,testng,1046,"licate, or use this one:. ```; > ARG_0: (dict<call, float64>,Map(3 -> 1.4756407611933507E308, 0 -> 46.25904804767313, 1 -> 54.68552327526001),12). java.lang.AssertionError: java.lang.ArrayIndexOutOfBoundsException: 3. 	at is.hail.check.GenProp1$$anonfun$apply$1.apply$mcVI$sp(Prop.scala:38); 	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160); 	at is.hail.check.GenProp1.apply(Prop.scala:26); 	at is.hail.check.Prop.check(Prop.scala:19); 	at is.hail.expr.ir.OrderingSuite.testBinarySearchOnDict(OrderingSuite.scala:357); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:767); 	at org.testng.TestRunner.run(TestRunner.java:617); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); 	at org.testng.SuiteRunner.run(SuiteRunner.java:254); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5630
https://github.com/hail-is/hail/issues/5630:1114,Testability,test,testng,1114,"> 1.4756407611933507E308, 0 -> 46.25904804767313, 1 -> 54.68552327526001),12). java.lang.AssertionError: java.lang.ArrayIndexOutOfBoundsException: 3. 	at is.hail.check.GenProp1$$anonfun$apply$1.apply$mcVI$sp(Prop.scala:38); 	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160); 	at is.hail.check.GenProp1.apply(Prop.scala:26); 	at is.hail.check.Prop.check(Prop.scala:19); 	at is.hail.expr.ir.OrderingSuite.testBinarySearchOnDict(OrderingSuite.scala:357); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:767); 	at org.testng.TestRunner.run(TestRunner.java:617); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); 	at org.testng.SuiteRunner.run(SuiteRunner.java:254); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:123",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5630
https://github.com/hail-is/hail/issues/5630:1184,Testability,test,testng,1184,"01),12). java.lang.AssertionError: java.lang.ArrayIndexOutOfBoundsException: 3. 	at is.hail.check.GenProp1$$anonfun$apply$1.apply$mcVI$sp(Prop.scala:38); 	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160); 	at is.hail.check.GenProp1.apply(Prop.scala:26); 	at is.hail.check.Prop.check(Prop.scala:19); 	at is.hail.expr.ir.OrderingSuite.testBinarySearchOnDict(OrderingSuite.scala:357); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:767); 	at org.testng.TestRunner.run(TestRunner.java:617); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); 	at org.testng.SuiteRunner.run(SuiteRunner.java:254); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:123); Caused by: java.lang.ArrayIndexOutOfBoundsException: 3; 	at scala.c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5630
https://github.com/hail-is/hail/issues/5630:1200,Testability,Test,TestMethodWorker,1200,sertionError: java.lang.ArrayIndexOutOfBoundsException: 3. 	at is.hail.check.GenProp1$$anonfun$apply$1.apply$mcVI$sp(Prop.scala:38); 	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160); 	at is.hail.check.GenProp1.apply(Prop.scala:26); 	at is.hail.check.Prop.check(Prop.scala:19); 	at is.hail.expr.ir.OrderingSuite.testBinarySearchOnDict(OrderingSuite.scala:357); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:767); 	at org.testng.TestRunner.run(TestRunner.java:617); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); 	at org.testng.SuiteRunner.run(SuiteRunner.java:254); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:123); Caused by: java.lang.ArrayIndexOutOfBoundsException: 3; 	at scala.collection.mutable.Wra,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5630
https://github.com/hail-is/hail/issues/5630:1235,Testability,Test,TestMethodWorker,1235,rayIndexOutOfBoundsException: 3. 	at is.hail.check.GenProp1$$anonfun$apply$1.apply$mcVI$sp(Prop.scala:38); 	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160); 	at is.hail.check.GenProp1.apply(Prop.scala:26); 	at is.hail.check.Prop.check(Prop.scala:19); 	at is.hail.expr.ir.OrderingSuite.testBinarySearchOnDict(OrderingSuite.scala:357); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:767); 	at org.testng.TestRunner.run(TestRunner.java:617); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); 	at org.testng.SuiteRunner.run(SuiteRunner.java:254); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:123); Caused by: java.lang.ArrayIndexOutOfBoundsException: 3; 	at scala.collection.mutable.WrappedArray$ofRef.apply(Wrap,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5630
https://github.com/hail-is/hail/issues/5630:1271,Testability,test,testng,1271,hail.check.GenProp1$$anonfun$apply$1.apply$mcVI$sp(Prop.scala:38); 	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160); 	at is.hail.check.GenProp1.apply(Prop.scala:26); 	at is.hail.check.Prop.check(Prop.scala:19); 	at is.hail.expr.ir.OrderingSuite.testBinarySearchOnDict(OrderingSuite.scala:357); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:767); 	at org.testng.TestRunner.run(TestRunner.java:617); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); 	at org.testng.SuiteRunner.run(SuiteRunner.java:254); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:123); Caused by: java.lang.ArrayIndexOutOfBoundsException: 3; 	at scala.collection.mutable.WrappedArray$ofRef.apply(WrappedArray.scala:127); 	at is.hail.expr.ir,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5630
https://github.com/hail-is/hail/issues/5630:1287,Testability,Test,TestMethodWorker,1287,anonfun$apply$1.apply$mcVI$sp(Prop.scala:38); 	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160); 	at is.hail.check.GenProp1.apply(Prop.scala:26); 	at is.hail.check.Prop.check(Prop.scala:19); 	at is.hail.expr.ir.OrderingSuite.testBinarySearchOnDict(OrderingSuite.scala:357); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:767); 	at org.testng.TestRunner.run(TestRunner.java:617); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); 	at org.testng.SuiteRunner.run(SuiteRunner.java:254); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:123); Caused by: java.lang.ArrayIndexOutOfBoundsException: 3; 	at scala.collection.mutable.WrappedArray$ofRef.apply(WrappedArray.scala:127); 	at is.hail.expr.ir.OrderingSuite$$anonf,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5630
https://github.com/hail-is/hail/issues/5630:1308,Testability,Test,TestMethodWorker,1308,ly$mcVI$sp(Prop.scala:38); 	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160); 	at is.hail.check.GenProp1.apply(Prop.scala:26); 	at is.hail.check.Prop.check(Prop.scala:19); 	at is.hail.expr.ir.OrderingSuite.testBinarySearchOnDict(OrderingSuite.scala:357); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:767); 	at org.testng.TestRunner.run(TestRunner.java:617); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); 	at org.testng.SuiteRunner.run(SuiteRunner.java:254); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:123); Caused by: java.lang.ArrayIndexOutOfBoundsException: 3; 	at scala.collection.mutable.WrappedArray$ofRef.apply(WrappedArray.scala:127); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33$$anonfun$appl,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5630
https://github.com/hail-is/hail/issues/5630:1344,Testability,test,testng,1344,ala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160); 	at is.hail.check.GenProp1.apply(Prop.scala:26); 	at is.hail.check.Prop.check(Prop.scala:19); 	at is.hail.expr.ir.OrderingSuite.testBinarySearchOnDict(OrderingSuite.scala:357); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:767); 	at org.testng.TestRunner.run(TestRunner.java:617); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); 	at org.testng.SuiteRunner.run(SuiteRunner.java:254); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:123); Caused by: java.lang.ArrayIndexOutOfBoundsException: 3; 	at scala.collection.mutable.WrappedArray$ofRef.apply(WrappedArray.scala:127); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33$$anonfun$apply$19.getKey$1(OrderingSuite.scala,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5630
https://github.com/hail-is/hail/issues/5630:1351,Testability,Test,TestRunner,1351,ction.immutable.Range.foreach$mVc$sp(Range.scala:160); 	at is.hail.check.GenProp1.apply(Prop.scala:26); 	at is.hail.check.Prop.check(Prop.scala:19); 	at is.hail.expr.ir.OrderingSuite.testBinarySearchOnDict(OrderingSuite.scala:357); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:767); 	at org.testng.TestRunner.run(TestRunner.java:617); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); 	at org.testng.SuiteRunner.run(SuiteRunner.java:254); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:123); Caused by: java.lang.ArrayIndexOutOfBoundsException: 3; 	at scala.collection.mutable.WrappedArray$ofRef.apply(WrappedArray.scala:127); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33$$anonfun$apply$19.getKey$1(OrderingSuite.scala:345); 	a,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5630
https://github.com/hail-is/hail/issues/5630:1373,Testability,Test,TestRunner,1373,ange.foreach$mVc$sp(Range.scala:160); 	at is.hail.check.GenProp1.apply(Prop.scala:26); 	at is.hail.check.Prop.check(Prop.scala:19); 	at is.hail.expr.ir.OrderingSuite.testBinarySearchOnDict(OrderingSuite.scala:357); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:767); 	at org.testng.TestRunner.run(TestRunner.java:617); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); 	at org.testng.SuiteRunner.run(SuiteRunner.java:254); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:123); Caused by: java.lang.ArrayIndexOutOfBoundsException: 3; 	at scala.collection.mutable.WrappedArray$ofRef.apply(WrappedArray.scala:127); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33$$anonfun$apply$19.getKey$1(OrderingSuite.scala:345); 	at is.hail.expr.i,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5630
https://github.com/hail-is/hail/issues/5630:1403,Testability,test,testng,1403,60); 	at is.hail.check.GenProp1.apply(Prop.scala:26); 	at is.hail.check.Prop.check(Prop.scala:19); 	at is.hail.expr.ir.OrderingSuite.testBinarySearchOnDict(OrderingSuite.scala:357); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:767); 	at org.testng.TestRunner.run(TestRunner.java:617); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); 	at org.testng.SuiteRunner.run(SuiteRunner.java:254); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:123); Caused by: java.lang.ArrayIndexOutOfBoundsException: 3; 	at scala.collection.mutable.WrappedArray$ofRef.apply(WrappedArray.scala:127); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33$$anonfun$apply$19.getKey$1(OrderingSuite.scala:345); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33$$anonf,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5630
https://github.com/hail-is/hail/issues/5630:1410,Testability,Test,TestRunner,1410,is.hail.check.GenProp1.apply(Prop.scala:26); 	at is.hail.check.Prop.check(Prop.scala:19); 	at is.hail.expr.ir.OrderingSuite.testBinarySearchOnDict(OrderingSuite.scala:357); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:767); 	at org.testng.TestRunner.run(TestRunner.java:617); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); 	at org.testng.SuiteRunner.run(SuiteRunner.java:254); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:123); Caused by: java.lang.ArrayIndexOutOfBoundsException: 3; 	at scala.collection.mutable.WrappedArray$ofRef.apply(WrappedArray.scala:127); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33$$anonfun$apply$19.getKey$1(OrderingSuite.scala:345); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33$$anonfun$apply$,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5630
https://github.com/hail-is/hail/issues/5630:1425,Testability,Test,TestRunner,1425,.GenProp1.apply(Prop.scala:26); 	at is.hail.check.Prop.check(Prop.scala:19); 	at is.hail.expr.ir.OrderingSuite.testBinarySearchOnDict(OrderingSuite.scala:357); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:767); 	at org.testng.TestRunner.run(TestRunner.java:617); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); 	at org.testng.SuiteRunner.run(SuiteRunner.java:254); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:123); Caused by: java.lang.ArrayIndexOutOfBoundsException: 3; 	at scala.collection.mutable.WrappedArray$ofRef.apply(WrappedArray.scala:127); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33$$anonfun$apply$19.getKey$1(OrderingSuite.scala:345); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33$$anonfun$apply$19.apply(Orde,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5630
https://github.com/hail-is/hail/issues/5630:1455,Testability,test,testng,1455,; 	at is.hail.check.Prop.check(Prop.scala:19); 	at is.hail.expr.ir.OrderingSuite.testBinarySearchOnDict(OrderingSuite.scala:357); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:767); 	at org.testng.TestRunner.run(TestRunner.java:617); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); 	at org.testng.SuiteRunner.run(SuiteRunner.java:254); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:123); Caused by: java.lang.ArrayIndexOutOfBoundsException: 3; 	at scala.collection.mutable.WrappedArray$ofRef.apply(WrappedArray.scala:127); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33$$anonfun$apply$19.getKey$1(OrderingSuite.scala:345); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33$$anonfun$apply$19.apply(OrderingSuite.scala:347); 	at is.h,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5630
https://github.com/hail-is/hail/issues/5630:1513,Testability,test,testng,1513,.expr.ir.OrderingSuite.testBinarySearchOnDict(OrderingSuite.scala:357); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:767); 	at org.testng.TestRunner.run(TestRunner.java:617); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); 	at org.testng.SuiteRunner.run(SuiteRunner.java:254); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:123); Caused by: java.lang.ArrayIndexOutOfBoundsException: 3; 	at scala.collection.mutable.WrappedArray$ofRef.apply(WrappedArray.scala:127); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33$$anonfun$apply$19.getKey$1(OrderingSuite.scala:345); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33$$anonfun$apply$19.apply(OrderingSuite.scala:347); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33$$anonfun$apply$19.ap,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5630
https://github.com/hail-is/hail/issues/5630:1579,Testability,test,testng,1579,357); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:767); 	at org.testng.TestRunner.run(TestRunner.java:617); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); 	at org.testng.SuiteRunner.run(SuiteRunner.java:254); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:123); Caused by: java.lang.ArrayIndexOutOfBoundsException: 3; 	at scala.collection.mutable.WrappedArray$ofRef.apply(WrappedArray.scala:127); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33$$anonfun$apply$19.getKey$1(OrderingSuite.scala:345); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33$$anonfun$apply$19.apply(OrderingSuite.scala:347); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33$$anonfun$apply$19.apply(OrderingSuite.scala:318); 	at is.hail.utils.package$.using(pac,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5630
https://github.com/hail-is/hail/issues/5630:1640,Testability,test,testng,1640, Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:767); 	at org.testng.TestRunner.run(TestRunner.java:617); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); 	at org.testng.SuiteRunner.run(SuiteRunner.java:254); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:123); Caused by: java.lang.ArrayIndexOutOfBoundsException: 3; 	at scala.collection.mutable.WrappedArray$ofRef.apply(WrappedArray.scala:127); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33$$anonfun$apply$19.getKey$1(OrderingSuite.scala:345); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33$$anonfun$apply$19.apply(OrderingSuite.scala:347); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33$$anonfun$apply$19.apply(OrderingSuite.scala:318); 	at is.hail.utils.package$.using(package.scala:613); 	at is.hail.annotations.Region$.scoped(Regio,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5630
https://github.com/hail-is/hail/issues/5630:1694,Testability,test,testng,1694,oke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:767); 	at org.testng.TestRunner.run(TestRunner.java:617); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); 	at org.testng.SuiteRunner.run(SuiteRunner.java:254); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:123); Caused by: java.lang.ArrayIndexOutOfBoundsException: 3; 	at scala.collection.mutable.WrappedArray$ofRef.apply(WrappedArray.scala:127); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33$$anonfun$apply$19.getKey$1(OrderingSuite.scala:345); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33$$anonfun$apply$19.apply(OrderingSuite.scala:347); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33$$anonfun$apply$19.apply(OrderingSuite.scala:318); 	at is.hail.utils.package$.using(package.scala:613); 	at is.hail.annotations.Region$.scoped(Region.scala:13); 	at is.hail.expr.ir.OrderingSuite$$anonfu,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5630
https://github.com/hail-is/hail/issues/5630:1764,Testability,test,testng,1764,dAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:767); 	at org.testng.TestRunner.run(TestRunner.java:617); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); 	at org.testng.SuiteRunner.run(SuiteRunner.java:254); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:123); Caused by: java.lang.ArrayIndexOutOfBoundsException: 3; 	at scala.collection.mutable.WrappedArray$ofRef.apply(WrappedArray.scala:127); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33$$anonfun$apply$19.getKey$1(OrderingSuite.scala:345); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33$$anonfun$apply$19.apply(OrderingSuite.scala:347); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33$$anonfun$apply$19.apply(OrderingSuite.scala:318); 	at is.hail.utils.package$.using(package.scala:613); 	at is.hail.annotations.Region$.scoped(Region.scala:13); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33.apply(OrderingSuite.scala:318); 	at is.hail.expr.ir.OrderingSuite,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5630
https://github.com/hail-is/hail/issues/5630:1829,Testability,test,testng,1829,ava.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:767); 	at org.testng.TestRunner.run(TestRunner.java:617); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); 	at org.testng.SuiteRunner.run(SuiteRunner.java:254); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:123); Caused by: java.lang.ArrayIndexOutOfBoundsException: 3; 	at scala.collection.mutable.WrappedArray$ofRef.apply(WrappedArray.scala:127); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33$$anonfun$apply$19.getKey$1(OrderingSuite.scala:345); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33$$anonfun$apply$19.apply(OrderingSuite.scala:347); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33$$anonfun$apply$19.apply(OrderingSuite.scala:318); 	at is.hail.utils.package$.using(package.scala:613); 	at is.hail.annotations.Region$.scoped(Region.scala:13); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33.apply(OrderingSuite.scala:318); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33.apply(OrderingSuite.scala:314); 	at is.hail.check.Ge,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5630
https://github.com/hail-is/hail/issues/5630:1836,Testability,Test,TestNG,1836,g.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:767); 	at org.testng.TestRunner.run(TestRunner.java:617); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); 	at org.testng.SuiteRunner.run(SuiteRunner.java:254); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:123); Caused by: java.lang.ArrayIndexOutOfBoundsException: 3; 	at scala.collection.mutable.WrappedArray$ofRef.apply(WrappedArray.scala:127); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33$$anonfun$apply$19.getKey$1(OrderingSuite.scala:345); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33$$anonfun$apply$19.apply(OrderingSuite.scala:347); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33$$anonfun$apply$19.apply(OrderingSuite.scala:318); 	at is.hail.utils.package$.using(package.scala:613); 	at is.hail.annotations.Region$.scoped(Region.scala:13); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33.apply(OrderingSuite.scala:318); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33.apply(OrderingSuite.scala:314); 	at is.hail.check.GenProp1$,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5630
https://github.com/hail-is/hail/issues/5630:1865,Testability,Test,TestNG,1865,nvoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:767); 	at org.testng.TestRunner.run(TestRunner.java:617); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); 	at org.testng.SuiteRunner.run(SuiteRunner.java:254); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:123); Caused by: java.lang.ArrayIndexOutOfBoundsException: 3; 	at scala.collection.mutable.WrappedArray$ofRef.apply(WrappedArray.scala:127); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33$$anonfun$apply$19.getKey$1(OrderingSuite.scala:345); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33$$anonfun$apply$19.apply(OrderingSuite.scala:347); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33$$anonfun$apply$19.apply(OrderingSuite.scala:318); 	at is.hail.utils.package$.using(package.scala:613); 	at is.hail.annotations.Region$.scoped(Region.scala:13); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33.apply(OrderingSuite.scala:318); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33.apply(OrderingSuite.scala:314); 	at is.hail.check.GenProp1$$anonfun$apply$1$$,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5630
https://github.com/hail-is/hail/issues/5630:1892,Testability,test,testng,1892,.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:767); 	at org.testng.TestRunner.run(TestRunner.java:617); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); 	at org.testng.SuiteRunner.run(SuiteRunner.java:254); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:123); Caused by: java.lang.ArrayIndexOutOfBoundsException: 3; 	at scala.collection.mutable.WrappedArray$ofRef.apply(WrappedArray.scala:127); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33$$anonfun$apply$19.getKey$1(OrderingSuite.scala:345); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33$$anonfun$apply$19.apply(OrderingSuite.scala:347); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33$$anonfun$apply$19.apply(OrderingSuite.scala:318); 	at is.hail.utils.package$.using(package.scala:613); 	at is.hail.annotations.Region$.scoped(Region.scala:13); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33.apply(OrderingSuite.scala:318); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33.apply(OrderingSuite.scala:314); 	at is.hail.check.GenProp1$$anonfun$apply$1$$anonfun$1.apply$mcZ$sp(Prop.scala:28);,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5630
https://github.com/hail-is/hail/issues/5630:1899,Testability,Test,TestNG,1899,al.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:767); 	at org.testng.TestRunner.run(TestRunner.java:617); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); 	at org.testng.SuiteRunner.run(SuiteRunner.java:254); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:123); Caused by: java.lang.ArrayIndexOutOfBoundsException: 3; 	at scala.collection.mutable.WrappedArray$ofRef.apply(WrappedArray.scala:127); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33$$anonfun$apply$19.getKey$1(OrderingSuite.scala:345); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33$$anonfun$apply$19.apply(OrderingSuite.scala:347); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33$$anonfun$apply$19.apply(OrderingSuite.scala:318); 	at is.hail.utils.package$.using(package.scala:613); 	at is.hail.annotations.Region$.scoped(Region.scala:13); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33.apply(OrderingSuite.scala:318); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33.apply(OrderingSuite.scala:314); 	at is.hail.check.GenProp1$$anonfun$apply$1$$anonfun$1.apply$mcZ$sp(Prop.scala:28); 	at is,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5630
https://github.com/hail-is/hail/issues/5630:1923,Testability,Test,TestNG,1923,ionHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:767); 	at org.testng.TestRunner.run(TestRunner.java:617); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); 	at org.testng.SuiteRunner.run(SuiteRunner.java:254); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:123); Caused by: java.lang.ArrayIndexOutOfBoundsException: 3; 	at scala.collection.mutable.WrappedArray$ofRef.apply(WrappedArray.scala:127); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33$$anonfun$apply$19.getKey$1(OrderingSuite.scala:345); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33$$anonfun$apply$19.apply(OrderingSuite.scala:347); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33$$anonfun$apply$19.apply(OrderingSuite.scala:318); 	at is.hail.utils.package$.using(package.scala:613); 	at is.hail.annotations.Region$.scoped(Region.scala:13); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33.apply(OrderingSuite.scala:318); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33.apply(OrderingSuite.scala:314); 	at is.hail.check.GenProp1$$anonfun$apply$1$$anonfun$1.apply$mcZ$sp(Prop.scala:28); 	at is.hail.check.Gen,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5630
https://github.com/hail-is/hail/issues/5630:1950,Testability,test,testng,1950,tionHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:767); 	at org.testng.TestRunner.run(TestRunner.java:617); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); 	at org.testng.SuiteRunner.run(SuiteRunner.java:254); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:123); Caused by: java.lang.ArrayIndexOutOfBoundsException: 3; 	at scala.collection.mutable.WrappedArray$ofRef.apply(WrappedArray.scala:127); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33$$anonfun$apply$19.getKey$1(OrderingSuite.scala:345); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33$$anonfun$apply$19.apply(OrderingSuite.scala:347); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33$$anonfun$apply$19.apply(OrderingSuite.scala:318); 	at is.hail.utils.package$.using(package.scala:613); 	at is.hail.annotations.Region$.scoped(Region.scala:13); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33.apply(OrderingSuite.scala:318); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33.apply(OrderingSuite.scala:314); 	at is.hail.check.GenProp1$$anonfun$apply$1$$anonfun$1.apply$mcZ$sp(Prop.scala:28); 	at is.hail.check.GenProp1$$anonfun$apply$1$$anonfun$1.ap,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5630
https://github.com/hail-is/hail/issues/5630:1957,Testability,Test,TestNG,1957,per.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:767); 	at org.testng.TestRunner.run(TestRunner.java:617); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); 	at org.testng.SuiteRunner.run(SuiteRunner.java:254); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:123); Caused by: java.lang.ArrayIndexOutOfBoundsException: 3; 	at scala.collection.mutable.WrappedArray$ofRef.apply(WrappedArray.scala:127); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33$$anonfun$apply$19.getKey$1(OrderingSuite.scala:345); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33$$anonfun$apply$19.apply(OrderingSuite.scala:347); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33$$anonfun$apply$19.apply(OrderingSuite.scala:318); 	at is.hail.utils.package$.using(package.scala:613); 	at is.hail.annotations.Region$.scoped(Region.scala:13); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33.apply(OrderingSuite.scala:318); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33.apply(OrderingSuite.scala:314); 	at is.hail.check.GenProp1$$anonfun$apply$1$$anonfun$1.apply$mcZ$sp(Prop.scala:28); 	at is.hail.check.GenProp1$$anonfun$apply$1$$anonfun$1.apply(Pro,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5630
https://github.com/hail-is/hail/issues/5630:1968,Testability,Test,TestNG,1968,85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:767); 	at org.testng.TestRunner.run(TestRunner.java:617); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); 	at org.testng.SuiteRunner.run(SuiteRunner.java:254); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:123); Caused by: java.lang.ArrayIndexOutOfBoundsException: 3; 	at scala.collection.mutable.WrappedArray$ofRef.apply(WrappedArray.scala:127); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33$$anonfun$apply$19.getKey$1(OrderingSuite.scala:345); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33$$anonfun$apply$19.apply(OrderingSuite.scala:347); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33$$anonfun$apply$19.apply(OrderingSuite.scala:318); 	at is.hail.utils.package$.using(package.scala:613); 	at is.hail.annotations.Region$.scoped(Region.scala:13); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33.apply(OrderingSuite.scala:318); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33.apply(OrderingSuite.scala:314); 	at is.hail.check.GenProp1$$anonfun$apply$1$$anonfun$1.apply$mcZ$sp(Prop.scala:28); 	at is.hail.check.GenProp1$$anonfun$apply$1$$anonfun$1.apply(Prop.scala:2,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5630
https://github.com/hail-is/hail/issues/5630:1995,Testability,test,testng,1995,Invoker.invokeMethod(Invoker.java:696); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:767); 	at org.testng.TestRunner.run(TestRunner.java:617); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); 	at org.testng.SuiteRunner.run(SuiteRunner.java:254); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:123); Caused by: java.lang.ArrayIndexOutOfBoundsException: 3; 	at scala.collection.mutable.WrappedArray$ofRef.apply(WrappedArray.scala:127); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33$$anonfun$apply$19.getKey$1(OrderingSuite.scala:345); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33$$anonfun$apply$19.apply(OrderingSuite.scala:347); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33$$anonfun$apply$19.apply(OrderingSuite.scala:318); 	at is.hail.utils.package$.using(package.scala:613); 	at is.hail.annotations.Region$.scoped(Region.scala:13); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33.apply(OrderingSuite.scala:318); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33.apply(OrderingSuite.scala:314); 	at is.hail.check.GenProp1$$anonfun$apply$1$$anonfun$1.apply$mcZ$sp(Prop.scala:28); 	at is.hail.check.GenProp1$$anonfun$apply$1$$anonfun$1.apply(Prop.scala:28); 	at is.hail.check.GenProp,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5630
https://github.com/hail-is/hail/issues/5630:2058,Testability,test,testng,2058,.Invoker.invokeTestMethod(Invoker.java:882); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:767); 	at org.testng.TestRunner.run(TestRunner.java:617); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); 	at org.testng.SuiteRunner.run(SuiteRunner.java:254); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:123); Caused by: java.lang.ArrayIndexOutOfBoundsException: 3; 	at scala.collection.mutable.WrappedArray$ofRef.apply(WrappedArray.scala:127); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33$$anonfun$apply$19.getKey$1(OrderingSuite.scala:345); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33$$anonfun$apply$19.apply(OrderingSuite.scala:347); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33$$anonfun$apply$19.apply(OrderingSuite.scala:318); 	at is.hail.utils.package$.using(package.scala:613); 	at is.hail.annotations.Region$.scoped(Region.scala:13); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33.apply(OrderingSuite.scala:318); 	at is.hail.expr.ir.OrderingSuite$$anonfun$33.apply(OrderingSuite.scala:314); 	at is.hail.check.GenProp1$$anonfun$apply$1$$anonfun$1.apply$mcZ$sp(Prop.scala:28); 	at is.hail.check.GenProp1$$anonfun$apply$1$$anonfun$1.apply(Prop.scala:28); 	at is.hail.check.GenProp1$$anonfun$apply$1$$anonfun$1.apply(Prop.scala:28); 	at scala.u,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5630
https://github.com/hail-is/hail/pull/5632:22,Availability,down,down,22,Movie lens hosting is down. We can re-enable when it is back up.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5632
https://github.com/hail-is/hail/pull/5633:243,Safety,avoid,avoid,243,"cc @cseed, @jigold, @danking . Adds a `users.user_data` table reader. Upon login the user's `user_data` entry is read, stored in a `user` cookie, as a hmac/sha256-signed jwt. The secret key is the `'/notebook-secrets/secret-key'`. In order to avoid duplicating user data storage, I use the `user` cookie in place of `session['user']`, and to handle this wrote a decorator to decode the token and store it in Flask.g (for the duration of the request). The claims included:. ```python; {; 'id': [int],; 'auth0_id': [str],; 'name': [str],; 'email': [str],; 'picture': [str],; 'ksa_name': [str],; 'gsa_name': [str],; 'bucket_name': [str],; }; ```. An example cookie; ```python; { ; 'auth0_id': 'google-oauth2|000000000000000',; 'bucket_name': 'user-f2khk67pq8a9pc38wnbjigarg',; 'email': 'akotlar@broadinstitute.org',; 'gsa_name': 'projects/hail-vdc/serviceAccounts/user-f2khk67pq8a9pc38wnbjigarg@hail-vdc.iam.gserviceaccount.com',; 'id': 1,; 'ksa_name': 'user-4c4pr',; 'name': 'Alex Kotlar',; 'picture': 'https://lh4.googleusercontent.com/-QIkmrfGTN1M/AAAAAAAAAAI/AAAAAAAAAAA/ACHi3reUAvw1lwovp2ozAIThEN72j-qzeQ/mo/photo.jpg'; }; ```. Here 'id' means `users.user_data.id`. To parse the cookie in your applications:; ```python; SECRET_KEY = read_string('/notebook-secrets/secret-key'). def jwt_decode(token):; if token is None:; return None. try:; payload = jwt.decode(token, SECRET_KEY, algorithms=['HS256']); except jwt.exceptions.InvalidTokenError as e:; log.exception(e); payload = None. return payload. def jwt_encode(payload):; return jwt.encode(payload, SECRET_KEY, algorithm='HS256'). def get_domain(host):; parts = host.split('.'); p_len = len(parts). return f""{parts[p_len - 2]}.{parts[p_len - 1]}""; ```. And to use this (in Flask...aiohttp will be similar): `user = jwt_decode(request.cookies.get('user'))`. The cookie is scoped to hail.is (or whatever the lowest level domain happens to be if you're locally testing). I think this is a mostly straightforward implementation, but happy to take fe",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5633
https://github.com/hail-is/hail/pull/5633:75,Testability,log,login,75,"cc @cseed, @jigold, @danking . Adds a `users.user_data` table reader. Upon login the user's `user_data` entry is read, stored in a `user` cookie, as a hmac/sha256-signed jwt. The secret key is the `'/notebook-secrets/secret-key'`. In order to avoid duplicating user data storage, I use the `user` cookie in place of `session['user']`, and to handle this wrote a decorator to decode the token and store it in Flask.g (for the duration of the request). The claims included:. ```python; {; 'id': [int],; 'auth0_id': [str],; 'name': [str],; 'email': [str],; 'picture': [str],; 'ksa_name': [str],; 'gsa_name': [str],; 'bucket_name': [str],; }; ```. An example cookie; ```python; { ; 'auth0_id': 'google-oauth2|000000000000000',; 'bucket_name': 'user-f2khk67pq8a9pc38wnbjigarg',; 'email': 'akotlar@broadinstitute.org',; 'gsa_name': 'projects/hail-vdc/serviceAccounts/user-f2khk67pq8a9pc38wnbjigarg@hail-vdc.iam.gserviceaccount.com',; 'id': 1,; 'ksa_name': 'user-4c4pr',; 'name': 'Alex Kotlar',; 'picture': 'https://lh4.googleusercontent.com/-QIkmrfGTN1M/AAAAAAAAAAI/AAAAAAAAAAA/ACHi3reUAvw1lwovp2ozAIThEN72j-qzeQ/mo/photo.jpg'; }; ```. Here 'id' means `users.user_data.id`. To parse the cookie in your applications:; ```python; SECRET_KEY = read_string('/notebook-secrets/secret-key'). def jwt_decode(token):; if token is None:; return None. try:; payload = jwt.decode(token, SECRET_KEY, algorithms=['HS256']); except jwt.exceptions.InvalidTokenError as e:; log.exception(e); payload = None. return payload. def jwt_encode(payload):; return jwt.encode(payload, SECRET_KEY, algorithm='HS256'). def get_domain(host):; parts = host.split('.'); p_len = len(parts). return f""{parts[p_len - 2]}.{parts[p_len - 1]}""; ```. And to use this (in Flask...aiohttp will be similar): `user = jwt_decode(request.cookies.get('user'))`. The cookie is scoped to hail.is (or whatever the lowest level domain happens to be if you're locally testing). I think this is a mostly straightforward implementation, but happy to take fe",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5633
https://github.com/hail-is/hail/pull/5633:1452,Testability,log,log,1452,"d, @jigold, @danking . Adds a `users.user_data` table reader. Upon login the user's `user_data` entry is read, stored in a `user` cookie, as a hmac/sha256-signed jwt. The secret key is the `'/notebook-secrets/secret-key'`. In order to avoid duplicating user data storage, I use the `user` cookie in place of `session['user']`, and to handle this wrote a decorator to decode the token and store it in Flask.g (for the duration of the request). The claims included:. ```python; {; 'id': [int],; 'auth0_id': [str],; 'name': [str],; 'email': [str],; 'picture': [str],; 'ksa_name': [str],; 'gsa_name': [str],; 'bucket_name': [str],; }; ```. An example cookie; ```python; { ; 'auth0_id': 'google-oauth2|000000000000000',; 'bucket_name': 'user-f2khk67pq8a9pc38wnbjigarg',; 'email': 'akotlar@broadinstitute.org',; 'gsa_name': 'projects/hail-vdc/serviceAccounts/user-f2khk67pq8a9pc38wnbjigarg@hail-vdc.iam.gserviceaccount.com',; 'id': 1,; 'ksa_name': 'user-4c4pr',; 'name': 'Alex Kotlar',; 'picture': 'https://lh4.googleusercontent.com/-QIkmrfGTN1M/AAAAAAAAAAI/AAAAAAAAAAA/ACHi3reUAvw1lwovp2ozAIThEN72j-qzeQ/mo/photo.jpg'; }; ```. Here 'id' means `users.user_data.id`. To parse the cookie in your applications:; ```python; SECRET_KEY = read_string('/notebook-secrets/secret-key'). def jwt_decode(token):; if token is None:; return None. try:; payload = jwt.decode(token, SECRET_KEY, algorithms=['HS256']); except jwt.exceptions.InvalidTokenError as e:; log.exception(e); payload = None. return payload. def jwt_encode(payload):; return jwt.encode(payload, SECRET_KEY, algorithm='HS256'). def get_domain(host):; parts = host.split('.'); p_len = len(parts). return f""{parts[p_len - 2]}.{parts[p_len - 1]}""; ```. And to use this (in Flask...aiohttp will be similar): `user = jwt_decode(request.cookies.get('user'))`. The cookie is scoped to hail.is (or whatever the lowest level domain happens to be if you're locally testing). I think this is a mostly straightforward implementation, but happy to take feedback.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5633
https://github.com/hail-is/hail/pull/5633:1914,Testability,test,testing,1914,"d, @jigold, @danking . Adds a `users.user_data` table reader. Upon login the user's `user_data` entry is read, stored in a `user` cookie, as a hmac/sha256-signed jwt. The secret key is the `'/notebook-secrets/secret-key'`. In order to avoid duplicating user data storage, I use the `user` cookie in place of `session['user']`, and to handle this wrote a decorator to decode the token and store it in Flask.g (for the duration of the request). The claims included:. ```python; {; 'id': [int],; 'auth0_id': [str],; 'name': [str],; 'email': [str],; 'picture': [str],; 'ksa_name': [str],; 'gsa_name': [str],; 'bucket_name': [str],; }; ```. An example cookie; ```python; { ; 'auth0_id': 'google-oauth2|000000000000000',; 'bucket_name': 'user-f2khk67pq8a9pc38wnbjigarg',; 'email': 'akotlar@broadinstitute.org',; 'gsa_name': 'projects/hail-vdc/serviceAccounts/user-f2khk67pq8a9pc38wnbjigarg@hail-vdc.iam.gserviceaccount.com',; 'id': 1,; 'ksa_name': 'user-4c4pr',; 'name': 'Alex Kotlar',; 'picture': 'https://lh4.googleusercontent.com/-QIkmrfGTN1M/AAAAAAAAAAI/AAAAAAAAAAA/ACHi3reUAvw1lwovp2ozAIThEN72j-qzeQ/mo/photo.jpg'; }; ```. Here 'id' means `users.user_data.id`. To parse the cookie in your applications:; ```python; SECRET_KEY = read_string('/notebook-secrets/secret-key'). def jwt_decode(token):; if token is None:; return None. try:; payload = jwt.decode(token, SECRET_KEY, algorithms=['HS256']); except jwt.exceptions.InvalidTokenError as e:; log.exception(e); payload = None. return payload. def jwt_encode(payload):; return jwt.encode(payload, SECRET_KEY, algorithm='HS256'). def get_domain(host):; parts = host.split('.'); p_len = len(parts). return f""{parts[p_len - 2]}.{parts[p_len - 1]}""; ```. And to use this (in Flask...aiohttp will be similar): `user = jwt_decode(request.cookies.get('user'))`. The cookie is scoped to hail.is (or whatever the lowest level domain happens to be if you're locally testing). I think this is a mostly straightforward implementation, but happy to take feedback.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5633
https://github.com/hail-is/hail/pull/5633:1999,Usability,feedback,feedback,1999,"d, @jigold, @danking . Adds a `users.user_data` table reader. Upon login the user's `user_data` entry is read, stored in a `user` cookie, as a hmac/sha256-signed jwt. The secret key is the `'/notebook-secrets/secret-key'`. In order to avoid duplicating user data storage, I use the `user` cookie in place of `session['user']`, and to handle this wrote a decorator to decode the token and store it in Flask.g (for the duration of the request). The claims included:. ```python; {; 'id': [int],; 'auth0_id': [str],; 'name': [str],; 'email': [str],; 'picture': [str],; 'ksa_name': [str],; 'gsa_name': [str],; 'bucket_name': [str],; }; ```. An example cookie; ```python; { ; 'auth0_id': 'google-oauth2|000000000000000',; 'bucket_name': 'user-f2khk67pq8a9pc38wnbjigarg',; 'email': 'akotlar@broadinstitute.org',; 'gsa_name': 'projects/hail-vdc/serviceAccounts/user-f2khk67pq8a9pc38wnbjigarg@hail-vdc.iam.gserviceaccount.com',; 'id': 1,; 'ksa_name': 'user-4c4pr',; 'name': 'Alex Kotlar',; 'picture': 'https://lh4.googleusercontent.com/-QIkmrfGTN1M/AAAAAAAAAAI/AAAAAAAAAAA/ACHi3reUAvw1lwovp2ozAIThEN72j-qzeQ/mo/photo.jpg'; }; ```. Here 'id' means `users.user_data.id`. To parse the cookie in your applications:; ```python; SECRET_KEY = read_string('/notebook-secrets/secret-key'). def jwt_decode(token):; if token is None:; return None. try:; payload = jwt.decode(token, SECRET_KEY, algorithms=['HS256']); except jwt.exceptions.InvalidTokenError as e:; log.exception(e); payload = None. return payload. def jwt_encode(payload):; return jwt.encode(payload, SECRET_KEY, algorithm='HS256'). def get_domain(host):; parts = host.split('.'); p_len = len(parts). return f""{parts[p_len - 2]}.{parts[p_len - 1]}""; ```. And to use this (in Flask...aiohttp will be similar): `user = jwt_decode(request.cookies.get('user'))`. The cookie is scoped to hail.is (or whatever the lowest level domain happens to be if you're locally testing). I think this is a mostly straightforward implementation, but happy to take feedback.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5633
https://github.com/hail-is/hail/pull/5637:96,Integrability,wrap,wrapper,96,"This is currently an IR that does nothing except take an IR of a container type and changes the wrapper type to TStream. In this PR, I've defined it in the usual (Scala) IR framework and in Emit (currently equivalent to ToArray, but that will change in the c++ emitter as the stream infrastructure goes in). It's otherwise not used yet (and will essentially continue to be a no-op in the JVM emitter), so I haven't written tests other than the parser test. I intend to use this mostly to enforce non-instantiation of arrays in the c++ emitter, so that we don't inadvertently try to create e.g. an array of all the rows in a partition.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5637
https://github.com/hail-is/hail/pull/5637:423,Testability,test,tests,423,"This is currently an IR that does nothing except take an IR of a container type and changes the wrapper type to TStream. In this PR, I've defined it in the usual (Scala) IR framework and in Emit (currently equivalent to ToArray, but that will change in the c++ emitter as the stream infrastructure goes in). It's otherwise not used yet (and will essentially continue to be a no-op in the JVM emitter), so I haven't written tests other than the parser test. I intend to use this mostly to enforce non-instantiation of arrays in the c++ emitter, so that we don't inadvertently try to create e.g. an array of all the rows in a partition.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5637
https://github.com/hail-is/hail/pull/5637:451,Testability,test,test,451,"This is currently an IR that does nothing except take an IR of a container type and changes the wrapper type to TStream. In this PR, I've defined it in the usual (Scala) IR framework and in Emit (currently equivalent to ToArray, but that will change in the c++ emitter as the stream infrastructure goes in). It's otherwise not used yet (and will essentially continue to be a no-op in the JVM emitter), so I haven't written tests other than the parser test. I intend to use this mostly to enforce non-instantiation of arrays in the c++ emitter, so that we don't inadvertently try to create e.g. an array of all the rows in a partition.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5637
https://github.com/hail-is/hail/pull/5638:108,Deployability,update,updated,108,I think this resolves a number of our issues actually wherein CI keeps hearing about jobs that it's already updated its internal state about.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5638
https://github.com/hail-is/hail/pull/5639:36,Testability,test,test,36,There's no reason for these tiny CI test jobs to use 3.7 cpus.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5639
https://github.com/hail-is/hail/pull/5640:18,Integrability,depend,dependencies,18,I need transitive dependencies for the forthcoming hailjwt library. fyi @akotlar,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5640
https://github.com/hail-is/hail/pull/5641:85,Deployability,install,install,85,cc: @akotlar @jigold @cseed . Is everyone OK with this? You can run `python setup.py install` to install this project. I want to use this in the cookies batch PR.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5641
https://github.com/hail-is/hail/pull/5641:97,Deployability,install,install,97,cc: @akotlar @jigold @cseed . Is everyone OK with this? You can run `python setup.py install` to install this project. I want to use this in the cookies batch PR.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5641
https://github.com/hail-is/hail/issues/5643:451,Availability,error,error,451,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; 0.2; ### What you did:; users = hl.read_table('data/users.ht'); hl.export_elasticsearch(users, 'XX.XXX.XXX.XXX', 32565, 'users', 'movies', 200,config=None, verbose=True ). ### What went wrong (all error messages here, including the full java stack trace):; Gotten this error even though the elasticsearch IP and port number 32565 is correct. The IP mentioned in the error 192.168.185.157:9200 was not found anywhere in our EMR or elasticsearch cluster. ; >>> hl.export_elasticsearch(users, 'XX.XXX.XXX.XXX', 32565, 'users', 'movies', 200,config=None, verbose=True ); Config Map(es.nodes -> XX.XXX.XXX.XXX, es.port -> 32565, es.batch.size.entries -> 200, es.index.auto.create -> true); [Stage 0:> (0 + 32) / 65]Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""</usr/local/lib/python3.6/site-packages/decorator.py:decorator-gen-1122>"", line 2, in export_elasticsearch; File ""/opt/hail-on-EMR/src/hail/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 561, in wrapper; File ""/opt/hail-on-EMR/src/hail/hail/build/distributions/hail-python.zip/hail/methods/impex.py"", line 2106, in export_elasticsearch; File ""/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/opt/hail-on-EMR/src/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 227, in deco; hail.utils.java.FatalError: EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[192.168.185.157:9200, 192.168.81.209:9200]] . Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 4 times, most recent failure: Lost task 1.3 in stage 0.0 (TID 73, ip-172-31-10-234.ap",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5643
https://github.com/hail-is/hail/issues/5643:523,Availability,error,error,523,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; 0.2; ### What you did:; users = hl.read_table('data/users.ht'); hl.export_elasticsearch(users, 'XX.XXX.XXX.XXX', 32565, 'users', 'movies', 200,config=None, verbose=True ). ### What went wrong (all error messages here, including the full java stack trace):; Gotten this error even though the elasticsearch IP and port number 32565 is correct. The IP mentioned in the error 192.168.185.157:9200 was not found anywhere in our EMR or elasticsearch cluster. ; >>> hl.export_elasticsearch(users, 'XX.XXX.XXX.XXX', 32565, 'users', 'movies', 200,config=None, verbose=True ); Config Map(es.nodes -> XX.XXX.XXX.XXX, es.port -> 32565, es.batch.size.entries -> 200, es.index.auto.create -> true); [Stage 0:> (0 + 32) / 65]Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""</usr/local/lib/python3.6/site-packages/decorator.py:decorator-gen-1122>"", line 2, in export_elasticsearch; File ""/opt/hail-on-EMR/src/hail/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 561, in wrapper; File ""/opt/hail-on-EMR/src/hail/hail/build/distributions/hail-python.zip/hail/methods/impex.py"", line 2106, in export_elasticsearch; File ""/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/opt/hail-on-EMR/src/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 227, in deco; hail.utils.java.FatalError: EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[192.168.185.157:9200, 192.168.81.209:9200]] . Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 4 times, most recent failure: Lost task 1.3 in stage 0.0 (TID 73, ip-172-31-10-234.ap",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5643
https://github.com/hail-is/hail/issues/5643:620,Availability,error,error,620,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; 0.2; ### What you did:; users = hl.read_table('data/users.ht'); hl.export_elasticsearch(users, 'XX.XXX.XXX.XXX', 32565, 'users', 'movies', 200,config=None, verbose=True ). ### What went wrong (all error messages here, including the full java stack trace):; Gotten this error even though the elasticsearch IP and port number 32565 is correct. The IP mentioned in the error 192.168.185.157:9200 was not found anywhere in our EMR or elasticsearch cluster. ; >>> hl.export_elasticsearch(users, 'XX.XXX.XXX.XXX', 32565, 'users', 'movies', 200,config=None, verbose=True ); Config Map(es.nodes -> XX.XXX.XXX.XXX, es.port -> 32565, es.batch.size.entries -> 200, es.index.auto.create -> true); [Stage 0:> (0 + 32) / 65]Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""</usr/local/lib/python3.6/site-packages/decorator.py:decorator-gen-1122>"", line 2, in export_elasticsearch; File ""/opt/hail-on-EMR/src/hail/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 561, in wrapper; File ""/opt/hail-on-EMR/src/hail/hail/build/distributions/hail-python.zip/hail/methods/impex.py"", line 2106, in export_elasticsearch; File ""/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/opt/hail-on-EMR/src/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 227, in deco; hail.utils.java.FatalError: EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[192.168.185.157:9200, 192.168.81.209:9200]] . Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 4 times, most recent failure: Lost task 1.3 in stage 0.0 (TID 73, ip-172-31-10-234.ap",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5643
https://github.com/hail-is/hail/issues/5643:1686,Availability,error,error,1686,"---. ### Hail version:; 0.2; ### What you did:; users = hl.read_table('data/users.ht'); hl.export_elasticsearch(users, 'XX.XXX.XXX.XXX', 32565, 'users', 'movies', 200,config=None, verbose=True ). ### What went wrong (all error messages here, including the full java stack trace):; Gotten this error even though the elasticsearch IP and port number 32565 is correct. The IP mentioned in the error 192.168.185.157:9200 was not found anywhere in our EMR or elasticsearch cluster. ; >>> hl.export_elasticsearch(users, 'XX.XXX.XXX.XXX', 32565, 'users', 'movies', 200,config=None, verbose=True ); Config Map(es.nodes -> XX.XXX.XXX.XXX, es.port -> 32565, es.batch.size.entries -> 200, es.index.auto.create -> true); [Stage 0:> (0 + 32) / 65]Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""</usr/local/lib/python3.6/site-packages/decorator.py:decorator-gen-1122>"", line 2, in export_elasticsearch; File ""/opt/hail-on-EMR/src/hail/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 561, in wrapper; File ""/opt/hail-on-EMR/src/hail/hail/build/distributions/hail-python.zip/hail/methods/impex.py"", line 2106, in export_elasticsearch; File ""/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/opt/hail-on-EMR/src/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 227, in deco; hail.utils.java.FatalError: EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[192.168.185.157:9200, 192.168.81.209:9200]] . Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 4 times, most recent failure: Lost task 1.3 in stage 0.0 (TID 73, ip-172-31-10-234.ap-southeast-1.compute.internal, executor 3): org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[192.168.185.157:9200, 192.168.81.209:9200]]",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5643
https://github.com/hail-is/hail/issues/5643:1880,Availability,failure,failure,1880,"---. ### Hail version:; 0.2; ### What you did:; users = hl.read_table('data/users.ht'); hl.export_elasticsearch(users, 'XX.XXX.XXX.XXX', 32565, 'users', 'movies', 200,config=None, verbose=True ). ### What went wrong (all error messages here, including the full java stack trace):; Gotten this error even though the elasticsearch IP and port number 32565 is correct. The IP mentioned in the error 192.168.185.157:9200 was not found anywhere in our EMR or elasticsearch cluster. ; >>> hl.export_elasticsearch(users, 'XX.XXX.XXX.XXX', 32565, 'users', 'movies', 200,config=None, verbose=True ); Config Map(es.nodes -> XX.XXX.XXX.XXX, es.port -> 32565, es.batch.size.entries -> 200, es.index.auto.create -> true); [Stage 0:> (0 + 32) / 65]Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""</usr/local/lib/python3.6/site-packages/decorator.py:decorator-gen-1122>"", line 2, in export_elasticsearch; File ""/opt/hail-on-EMR/src/hail/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 561, in wrapper; File ""/opt/hail-on-EMR/src/hail/hail/build/distributions/hail-python.zip/hail/methods/impex.py"", line 2106, in export_elasticsearch; File ""/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/opt/hail-on-EMR/src/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 227, in deco; hail.utils.java.FatalError: EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[192.168.185.157:9200, 192.168.81.209:9200]] . Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 4 times, most recent failure: Lost task 1.3 in stage 0.0 (TID 73, ip-172-31-10-234.ap-southeast-1.compute.internal, executor 3): org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[192.168.185.157:9200, 192.168.81.209:9200]]",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5643
https://github.com/hail-is/hail/issues/5643:1937,Availability,failure,failure,1937,"---. ### Hail version:; 0.2; ### What you did:; users = hl.read_table('data/users.ht'); hl.export_elasticsearch(users, 'XX.XXX.XXX.XXX', 32565, 'users', 'movies', 200,config=None, verbose=True ). ### What went wrong (all error messages here, including the full java stack trace):; Gotten this error even though the elasticsearch IP and port number 32565 is correct. The IP mentioned in the error 192.168.185.157:9200 was not found anywhere in our EMR or elasticsearch cluster. ; >>> hl.export_elasticsearch(users, 'XX.XXX.XXX.XXX', 32565, 'users', 'movies', 200,config=None, verbose=True ); Config Map(es.nodes -> XX.XXX.XXX.XXX, es.port -> 32565, es.batch.size.entries -> 200, es.index.auto.create -> true); [Stage 0:> (0 + 32) / 65]Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""</usr/local/lib/python3.6/site-packages/decorator.py:decorator-gen-1122>"", line 2, in export_elasticsearch; File ""/opt/hail-on-EMR/src/hail/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 561, in wrapper; File ""/opt/hail-on-EMR/src/hail/hail/build/distributions/hail-python.zip/hail/methods/impex.py"", line 2106, in export_elasticsearch; File ""/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/opt/hail-on-EMR/src/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 227, in deco; hail.utils.java.FatalError: EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[192.168.185.157:9200, 192.168.81.209:9200]] . Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 4 times, most recent failure: Lost task 1.3 in stage 0.0 (TID 73, ip-172-31-10-234.ap-southeast-1.compute.internal, executor 3): org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[192.168.185.157:9200, 192.168.81.209:9200]]",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5643
https://github.com/hail-is/hail/issues/5643:2116,Availability,error,error,2116,"---. ### Hail version:; 0.2; ### What you did:; users = hl.read_table('data/users.ht'); hl.export_elasticsearch(users, 'XX.XXX.XXX.XXX', 32565, 'users', 'movies', 200,config=None, verbose=True ). ### What went wrong (all error messages here, including the full java stack trace):; Gotten this error even though the elasticsearch IP and port number 32565 is correct. The IP mentioned in the error 192.168.185.157:9200 was not found anywhere in our EMR or elasticsearch cluster. ; >>> hl.export_elasticsearch(users, 'XX.XXX.XXX.XXX', 32565, 'users', 'movies', 200,config=None, verbose=True ); Config Map(es.nodes -> XX.XXX.XXX.XXX, es.port -> 32565, es.batch.size.entries -> 200, es.index.auto.create -> true); [Stage 0:> (0 + 32) / 65]Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""</usr/local/lib/python3.6/site-packages/decorator.py:decorator-gen-1122>"", line 2, in export_elasticsearch; File ""/opt/hail-on-EMR/src/hail/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 561, in wrapper; File ""/opt/hail-on-EMR/src/hail/hail/build/distributions/hail-python.zip/hail/methods/impex.py"", line 2106, in export_elasticsearch; File ""/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/opt/hail-on-EMR/src/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 227, in deco; hail.utils.java.FatalError: EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[192.168.185.157:9200, 192.168.81.209:9200]] . Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 4 times, most recent failure: Lost task 1.3 in stage 0.0 (TID 73, ip-172-31-10-234.ap-southeast-1.compute.internal, executor 3): org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[192.168.185.157:9200, 192.168.81.209:9200]]",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5643
https://github.com/hail-is/hail/issues/5643:457,Integrability,message,messages,457,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; 0.2; ### What you did:; users = hl.read_table('data/users.ht'); hl.export_elasticsearch(users, 'XX.XXX.XXX.XXX', 32565, 'users', 'movies', 200,config=None, verbose=True ). ### What went wrong (all error messages here, including the full java stack trace):; Gotten this error even though the elasticsearch IP and port number 32565 is correct. The IP mentioned in the error 192.168.185.157:9200 was not found anywhere in our EMR or elasticsearch cluster. ; >>> hl.export_elasticsearch(users, 'XX.XXX.XXX.XXX', 32565, 'users', 'movies', 200,config=None, verbose=True ); Config Map(es.nodes -> XX.XXX.XXX.XXX, es.port -> 32565, es.batch.size.entries -> 200, es.index.auto.create -> true); [Stage 0:> (0 + 32) / 65]Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""</usr/local/lib/python3.6/site-packages/decorator.py:decorator-gen-1122>"", line 2, in export_elasticsearch; File ""/opt/hail-on-EMR/src/hail/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 561, in wrapper; File ""/opt/hail-on-EMR/src/hail/hail/build/distributions/hail-python.zip/hail/methods/impex.py"", line 2106, in export_elasticsearch; File ""/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/opt/hail-on-EMR/src/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 227, in deco; hail.utils.java.FatalError: EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[192.168.185.157:9200, 192.168.81.209:9200]] . Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 4 times, most recent failure: Lost task 1.3 in stage 0.0 (TID 73, ip-172-31-10-234.ap",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5643
https://github.com/hail-is/hail/issues/5643:1263,Integrability,wrap,wrapper,1263,"---. ### Hail version:; 0.2; ### What you did:; users = hl.read_table('data/users.ht'); hl.export_elasticsearch(users, 'XX.XXX.XXX.XXX', 32565, 'users', 'movies', 200,config=None, verbose=True ). ### What went wrong (all error messages here, including the full java stack trace):; Gotten this error even though the elasticsearch IP and port number 32565 is correct. The IP mentioned in the error 192.168.185.157:9200 was not found anywhere in our EMR or elasticsearch cluster. ; >>> hl.export_elasticsearch(users, 'XX.XXX.XXX.XXX', 32565, 'users', 'movies', 200,config=None, verbose=True ); Config Map(es.nodes -> XX.XXX.XXX.XXX, es.port -> 32565, es.batch.size.entries -> 200, es.index.auto.create -> true); [Stage 0:> (0 + 32) / 65]Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""</usr/local/lib/python3.6/site-packages/decorator.py:decorator-gen-1122>"", line 2, in export_elasticsearch; File ""/opt/hail-on-EMR/src/hail/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 561, in wrapper; File ""/opt/hail-on-EMR/src/hail/hail/build/distributions/hail-python.zip/hail/methods/impex.py"", line 2106, in export_elasticsearch; File ""/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/opt/hail-on-EMR/src/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 227, in deco; hail.utils.java.FatalError: EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[192.168.185.157:9200, 192.168.81.209:9200]] . Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 4 times, most recent failure: Lost task 1.3 in stage 0.0 (TID 73, ip-172-31-10-234.ap-southeast-1.compute.internal, executor 3): org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[192.168.185.157:9200, 192.168.81.209:9200]]",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5643
https://github.com/hail-is/hail/issues/5643:397,Modifiability,config,config,397,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; 0.2; ### What you did:; users = hl.read_table('data/users.ht'); hl.export_elasticsearch(users, 'XX.XXX.XXX.XXX', 32565, 'users', 'movies', 200,config=None, verbose=True ). ### What went wrong (all error messages here, including the full java stack trace):; Gotten this error even though the elasticsearch IP and port number 32565 is correct. The IP mentioned in the error 192.168.185.157:9200 was not found anywhere in our EMR or elasticsearch cluster. ; >>> hl.export_elasticsearch(users, 'XX.XXX.XXX.XXX', 32565, 'users', 'movies', 200,config=None, verbose=True ); Config Map(es.nodes -> XX.XXX.XXX.XXX, es.port -> 32565, es.batch.size.entries -> 200, es.index.auto.create -> true); [Stage 0:> (0 + 32) / 65]Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""</usr/local/lib/python3.6/site-packages/decorator.py:decorator-gen-1122>"", line 2, in export_elasticsearch; File ""/opt/hail-on-EMR/src/hail/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 561, in wrapper; File ""/opt/hail-on-EMR/src/hail/hail/build/distributions/hail-python.zip/hail/methods/impex.py"", line 2106, in export_elasticsearch; File ""/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/opt/hail-on-EMR/src/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 227, in deco; hail.utils.java.FatalError: EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[192.168.185.157:9200, 192.168.81.209:9200]] . Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 4 times, most recent failure: Lost task 1.3 in stage 0.0 (TID 73, ip-172-31-10-234.ap",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5643
https://github.com/hail-is/hail/issues/5643:792,Modifiability,config,config,792,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; 0.2; ### What you did:; users = hl.read_table('data/users.ht'); hl.export_elasticsearch(users, 'XX.XXX.XXX.XXX', 32565, 'users', 'movies', 200,config=None, verbose=True ). ### What went wrong (all error messages here, including the full java stack trace):; Gotten this error even though the elasticsearch IP and port number 32565 is correct. The IP mentioned in the error 192.168.185.157:9200 was not found anywhere in our EMR or elasticsearch cluster. ; >>> hl.export_elasticsearch(users, 'XX.XXX.XXX.XXX', 32565, 'users', 'movies', 200,config=None, verbose=True ); Config Map(es.nodes -> XX.XXX.XXX.XXX, es.port -> 32565, es.batch.size.entries -> 200, es.index.auto.create -> true); [Stage 0:> (0 + 32) / 65]Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""</usr/local/lib/python3.6/site-packages/decorator.py:decorator-gen-1122>"", line 2, in export_elasticsearch; File ""/opt/hail-on-EMR/src/hail/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 561, in wrapper; File ""/opt/hail-on-EMR/src/hail/hail/build/distributions/hail-python.zip/hail/methods/impex.py"", line 2106, in export_elasticsearch; File ""/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/opt/hail-on-EMR/src/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 227, in deco; hail.utils.java.FatalError: EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[192.168.185.157:9200, 192.168.81.209:9200]] . Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 4 times, most recent failure: Lost task 1.3 in stage 0.0 (TID 73, ip-172-31-10-234.ap",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5643
https://github.com/hail-is/hail/issues/5643:821,Modifiability,Config,Config,821,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; 0.2; ### What you did:; users = hl.read_table('data/users.ht'); hl.export_elasticsearch(users, 'XX.XXX.XXX.XXX', 32565, 'users', 'movies', 200,config=None, verbose=True ). ### What went wrong (all error messages here, including the full java stack trace):; Gotten this error even though the elasticsearch IP and port number 32565 is correct. The IP mentioned in the error 192.168.185.157:9200 was not found anywhere in our EMR or elasticsearch cluster. ; >>> hl.export_elasticsearch(users, 'XX.XXX.XXX.XXX', 32565, 'users', 'movies', 200,config=None, verbose=True ); Config Map(es.nodes -> XX.XXX.XXX.XXX, es.port -> 32565, es.batch.size.entries -> 200, es.index.auto.create -> true); [Stage 0:> (0 + 32) / 65]Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""</usr/local/lib/python3.6/site-packages/decorator.py:decorator-gen-1122>"", line 2, in export_elasticsearch; File ""/opt/hail-on-EMR/src/hail/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 561, in wrapper; File ""/opt/hail-on-EMR/src/hail/hail/build/distributions/hail-python.zip/hail/methods/impex.py"", line 2106, in export_elasticsearch; File ""/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/opt/hail-on-EMR/src/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 227, in deco; hail.utils.java.FatalError: EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[192.168.185.157:9200, 192.168.81.209:9200]] . Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 4 times, most recent failure: Lost task 1.3 in stage 0.0 (TID 73, ip-172-31-10-234.ap",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5643
https://github.com/hail-is/hail/issues/5643:1859,Safety,abort,aborted,1859,"---. ### Hail version:; 0.2; ### What you did:; users = hl.read_table('data/users.ht'); hl.export_elasticsearch(users, 'XX.XXX.XXX.XXX', 32565, 'users', 'movies', 200,config=None, verbose=True ). ### What went wrong (all error messages here, including the full java stack trace):; Gotten this error even though the elasticsearch IP and port number 32565 is correct. The IP mentioned in the error 192.168.185.157:9200 was not found anywhere in our EMR or elasticsearch cluster. ; >>> hl.export_elasticsearch(users, 'XX.XXX.XXX.XXX', 32565, 'users', 'movies', 200,config=None, verbose=True ); Config Map(es.nodes -> XX.XXX.XXX.XXX, es.port -> 32565, es.batch.size.entries -> 200, es.index.auto.create -> true); [Stage 0:> (0 + 32) / 65]Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""</usr/local/lib/python3.6/site-packages/decorator.py:decorator-gen-1122>"", line 2, in export_elasticsearch; File ""/opt/hail-on-EMR/src/hail/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 561, in wrapper; File ""/opt/hail-on-EMR/src/hail/hail/build/distributions/hail-python.zip/hail/methods/impex.py"", line 2106, in export_elasticsearch; File ""/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/opt/hail-on-EMR/src/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 227, in deco; hail.utils.java.FatalError: EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[192.168.185.157:9200, 192.168.81.209:9200]] . Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 4 times, most recent failure: Lost task 1.3 in stage 0.0 (TID 73, ip-172-31-10-234.ap-southeast-1.compute.internal, executor 3): org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[192.168.185.157:9200, 192.168.81.209:9200]]",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5643
https://github.com/hail-is/hail/pull/5644:0,Deployability,Deploy,Deploy,0,Deploy is still failing because batch build is failing. The async stuff had new requirements that didn't install on alpine. I migrated it to base. I verified the build by hand.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5644
https://github.com/hail-is/hail/pull/5644:105,Deployability,install,install,105,Deploy is still failing because batch build is failing. The async stuff had new requirements that didn't install on alpine. I migrated it to base. I verified the build by hand.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5644
https://github.com/hail-is/hail/pull/5651:135,Deployability,update,updated,135,Binary search changed to return the size of the keys if the given key was greater than all existing keys. The randomized tests weren't updated and would fail intermittently in this case. Added in a check for this case and asserted that the given key is in fact greater than the entire keyset. Resolves #5630,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5651
https://github.com/hail-is/hail/pull/5651:121,Testability,test,tests,121,Binary search changed to return the size of the keys if the given key was greater than all existing keys. The randomized tests weren't updated and would fail intermittently in this case. Added in a check for this case and asserted that the given key is in fact greater than the entire keyset. Resolves #5630,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5651
https://github.com/hail-is/hail/pull/5651:222,Testability,assert,asserted,222,Binary search changed to return the size of the keys if the given key was greater than all existing keys. The randomized tests weren't updated and would fail intermittently in this case. Added in a check for this case and asserted that the given key is in fact greater than the entire keyset. Resolves #5630,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5651
https://github.com/hail-is/hail/pull/5652:300,Testability,test,test,300,"- Added `NDArrayExpression` in Python, with only `__getitem__`; - Added experimental constructor `hl.experimental.ndarray` that constructs an ndarray from a numpy ndarray. Next need to add the ability to construct from an arbitrary `Expression`.; - Added `run_with_cxx_compile` decorator that runs a test with cxx code gen. `FoldConstants` assumes that all `IR`s are interpretable, which `MakeNDArray` and `NDArrayRef` are not. Added `Interpretable` to guard against trying to interpret these IR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5652
https://github.com/hail-is/hail/pull/5654:77,Testability,log,logs,77,"The ubuntu 18.04 docker default is ascii, which was causing batch to barf on logs that contained special characters.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5654
https://github.com/hail-is/hail/pull/5655:33,Deployability,deploy,deploy,33,"Shit, one more fix needed to get deploy back on track. Missed in this change: https://github.com/hail-is/hail/pull/5644",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5655
https://github.com/hail-is/hail/issues/5657:340,Availability,error,error,340,"We have some problem with running nirvana in hail and we use the sample file of GWAS Tutorial. Code: ; hl.utils.get_1kg('data/'); vcfVds = hl.import_vcf('data/1kg.vcf.bgz', min_partitions=8); vds = hl.nirvana(vcfVds,'data/nirvana.properties'). Version:; Running on Apache Spark version 2.2.1; Hail version: 0.2.10-ceb85fc87544. We got this error massage:; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""</usr/local/lib/python3.6/dist-packages/decorator.py:decorator-gen-1226>"", line 2, in nirvana; File ""/seqslab/hail/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 561, in wrapper; File ""/seqslab/hail/hail/build/distributions/hail-python.zip/hail/methods/qc.py"", line 860, in nirvana; File ""/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/seqslab/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 227, in deco; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.methods.Nirvana$.annotate(Nirvana.scala:361); at is.hail.methods.Nirvana$.apply(Nirvana.scala:487); at is.hail.methods.Nirvana.apply(Nirvana.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.10-ceb85fc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5657
https://github.com/hail-is/hail/issues/5657:2008,Availability,Error,Error,2008,":; java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.methods.Nirvana$.annotate(Nirvana.scala:361); at is.hail.methods.Nirvana$.apply(Nirvana.scala:487); at is.hail.methods.Nirvana.apply(Nirvana.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.10-ceb85fc87544; Error summary: AssertionError: assertion failed. We also check the key of input file:; ; >>> vcfVds.describe(); ----------------------------------------; Global fields:; None; ----------------------------------------; Column fields:; 's': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'rsid': str; 'qual': float64; 'filters': set<str>; 'info': struct {; AC: array<int32>,; AF: array<float64>,; AN: int32,; BaseQRankSum: float64,; ClippingRankSum: float64,; DP: int32,; DS: bool,; FS: float64,; HaplotypeScore: float64,; InbreedingCoeff: float64,; MLEAC: array<int32>,; MLEAF: array<float64>,; MQ: float64,; MQ0: int32,; MQRankSum: float64,; QD: float64,; ReadPosRankSum: float64,; set: str; }; ----------------------------------------; Entry fields:; 'GT': call; 'AD': array<int32>; 'DP': int32; 'GQ': int32; 'PL': array<int32>; ----------------------------------------; Column key: ['s']; Row key: ['locus', 'alleles']; ----------------------------------",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5657
https://github.com/hail-is/hail/issues/5657:630,Integrability,wrap,wrapper,630,"We have some problem with running nirvana in hail and we use the sample file of GWAS Tutorial. Code: ; hl.utils.get_1kg('data/'); vcfVds = hl.import_vcf('data/1kg.vcf.bgz', min_partitions=8); vds = hl.nirvana(vcfVds,'data/nirvana.properties'). Version:; Running on Apache Spark version 2.2.1; Hail version: 0.2.10-ceb85fc87544. We got this error massage:; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""</usr/local/lib/python3.6/dist-packages/decorator.py:decorator-gen-1226>"", line 2, in nirvana; File ""/seqslab/hail/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 561, in wrapper; File ""/seqslab/hail/hail/build/distributions/hail-python.zip/hail/methods/qc.py"", line 860, in nirvana; File ""/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/seqslab/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 227, in deco; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.methods.Nirvana$.annotate(Nirvana.scala:361); at is.hail.methods.Nirvana$.apply(Nirvana.scala:487); at is.hail.methods.Nirvana.apply(Nirvana.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.10-ceb85fc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5657
https://github.com/hail-is/hail/issues/5657:973,Testability,Assert,AssertionError,973,"We have some problem with running nirvana in hail and we use the sample file of GWAS Tutorial. Code: ; hl.utils.get_1kg('data/'); vcfVds = hl.import_vcf('data/1kg.vcf.bgz', min_partitions=8); vds = hl.nirvana(vcfVds,'data/nirvana.properties'). Version:; Running on Apache Spark version 2.2.1; Hail version: 0.2.10-ceb85fc87544. We got this error massage:; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""</usr/local/lib/python3.6/dist-packages/decorator.py:decorator-gen-1226>"", line 2, in nirvana; File ""/seqslab/hail/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 561, in wrapper; File ""/seqslab/hail/hail/build/distributions/hail-python.zip/hail/methods/qc.py"", line 860, in nirvana; File ""/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/seqslab/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 227, in deco; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.methods.Nirvana$.annotate(Nirvana.scala:361); at is.hail.methods.Nirvana$.apply(Nirvana.scala:487); at is.hail.methods.Nirvana.apply(Nirvana.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.10-ceb85fc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5657
https://github.com/hail-is/hail/issues/5657:989,Testability,assert,assertion,989,"We have some problem with running nirvana in hail and we use the sample file of GWAS Tutorial. Code: ; hl.utils.get_1kg('data/'); vcfVds = hl.import_vcf('data/1kg.vcf.bgz', min_partitions=8); vds = hl.nirvana(vcfVds,'data/nirvana.properties'). Version:; Running on Apache Spark version 2.2.1; Hail version: 0.2.10-ceb85fc87544. We got this error massage:; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""</usr/local/lib/python3.6/dist-packages/decorator.py:decorator-gen-1226>"", line 2, in nirvana; File ""/seqslab/hail/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 561, in wrapper; File ""/seqslab/hail/hail/build/distributions/hail-python.zip/hail/methods/qc.py"", line 860, in nirvana; File ""/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/seqslab/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 227, in deco; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.methods.Nirvana$.annotate(Nirvana.scala:361); at is.hail.methods.Nirvana$.apply(Nirvana.scala:487); at is.hail.methods.Nirvana.apply(Nirvana.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.10-ceb85fc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5657
https://github.com/hail-is/hail/issues/5657:1036,Testability,Assert,AssertionError,1036,"use the sample file of GWAS Tutorial. Code: ; hl.utils.get_1kg('data/'); vcfVds = hl.import_vcf('data/1kg.vcf.bgz', min_partitions=8); vds = hl.nirvana(vcfVds,'data/nirvana.properties'). Version:; Running on Apache Spark version 2.2.1; Hail version: 0.2.10-ceb85fc87544. We got this error massage:; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""</usr/local/lib/python3.6/dist-packages/decorator.py:decorator-gen-1226>"", line 2, in nirvana; File ""/seqslab/hail/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 561, in wrapper; File ""/seqslab/hail/hail/build/distributions/hail-python.zip/hail/methods/qc.py"", line 860, in nirvana; File ""/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/seqslab/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 227, in deco; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.methods.Nirvana$.annotate(Nirvana.scala:361); at is.hail.methods.Nirvana$.apply(Nirvana.scala:487); at is.hail.methods.Nirvana.apply(Nirvana.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.10-ceb85fc87544; Error summary: AssertionError: assertion failed. W",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5657
https://github.com/hail-is/hail/issues/5657:1052,Testability,assert,assertion,1052,"use the sample file of GWAS Tutorial. Code: ; hl.utils.get_1kg('data/'); vcfVds = hl.import_vcf('data/1kg.vcf.bgz', min_partitions=8); vds = hl.nirvana(vcfVds,'data/nirvana.properties'). Version:; Running on Apache Spark version 2.2.1; Hail version: 0.2.10-ceb85fc87544. We got this error massage:; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""</usr/local/lib/python3.6/dist-packages/decorator.py:decorator-gen-1226>"", line 2, in nirvana; File ""/seqslab/hail/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 561, in wrapper; File ""/seqslab/hail/hail/build/distributions/hail-python.zip/hail/methods/qc.py"", line 860, in nirvana; File ""/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/seqslab/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 227, in deco; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.methods.Nirvana$.annotate(Nirvana.scala:361); at is.hail.methods.Nirvana$.apply(Nirvana.scala:487); at is.hail.methods.Nirvana.apply(Nirvana.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.10-ceb85fc87544; Error summary: AssertionError: assertion failed. W",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5657
https://github.com/hail-is/hail/issues/5657:1087,Testability,assert,assert,1087," Code: ; hl.utils.get_1kg('data/'); vcfVds = hl.import_vcf('data/1kg.vcf.bgz', min_partitions=8); vds = hl.nirvana(vcfVds,'data/nirvana.properties'). Version:; Running on Apache Spark version 2.2.1; Hail version: 0.2.10-ceb85fc87544. We got this error massage:; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""</usr/local/lib/python3.6/dist-packages/decorator.py:decorator-gen-1226>"", line 2, in nirvana; File ""/seqslab/hail/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 561, in wrapper; File ""/seqslab/hail/hail/build/distributions/hail-python.zip/hail/methods/qc.py"", line 860, in nirvana; File ""/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/seqslab/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 227, in deco; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.methods.Nirvana$.annotate(Nirvana.scala:361); at is.hail.methods.Nirvana$.apply(Nirvana.scala:487); at is.hail.methods.Nirvana.apply(Nirvana.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.10-ceb85fc87544; Error summary: AssertionError: assertion failed. We also check the key of input file:;",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5657
https://github.com/hail-is/hail/issues/5657:2023,Testability,Assert,AssertionError,2023,":; java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.methods.Nirvana$.annotate(Nirvana.scala:361); at is.hail.methods.Nirvana$.apply(Nirvana.scala:487); at is.hail.methods.Nirvana.apply(Nirvana.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.10-ceb85fc87544; Error summary: AssertionError: assertion failed. We also check the key of input file:; ; >>> vcfVds.describe(); ----------------------------------------; Global fields:; None; ----------------------------------------; Column fields:; 's': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'rsid': str; 'qual': float64; 'filters': set<str>; 'info': struct {; AC: array<int32>,; AF: array<float64>,; AN: int32,; BaseQRankSum: float64,; ClippingRankSum: float64,; DP: int32,; DS: bool,; FS: float64,; HaplotypeScore: float64,; InbreedingCoeff: float64,; MLEAC: array<int32>,; MLEAF: array<float64>,; MQ: float64,; MQ0: int32,; MQRankSum: float64,; QD: float64,; ReadPosRankSum: float64,; set: str; }; ----------------------------------------; Entry fields:; 'GT': call; 'AD': array<int32>; 'DP': int32; 'GQ': int32; 'PL': array<int32>; ----------------------------------------; Column key: ['s']; Row key: ['locus', 'alleles']; ----------------------------------",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5657
https://github.com/hail-is/hail/issues/5657:2039,Testability,assert,assertion,2039,":; java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.methods.Nirvana$.annotate(Nirvana.scala:361); at is.hail.methods.Nirvana$.apply(Nirvana.scala:487); at is.hail.methods.Nirvana.apply(Nirvana.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.10-ceb85fc87544; Error summary: AssertionError: assertion failed. We also check the key of input file:; ; >>> vcfVds.describe(); ----------------------------------------; Global fields:; None; ----------------------------------------; Column fields:; 's': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'rsid': str; 'qual': float64; 'filters': set<str>; 'info': struct {; AC: array<int32>,; AF: array<float64>,; AN: int32,; BaseQRankSum: float64,; ClippingRankSum: float64,; DP: int32,; DS: bool,; FS: float64,; HaplotypeScore: float64,; InbreedingCoeff: float64,; MLEAC: array<int32>,; MLEAF: array<float64>,; MQ: float64,; MQ0: int32,; MQRankSum: float64,; QD: float64,; ReadPosRankSum: float64,; set: str; }; ----------------------------------------; Entry fields:; 'GT': call; 'AD': array<int32>; 'DP': int32; 'GQ': int32; 'PL': array<int32>; ----------------------------------------; Column key: ['s']; Row key: ['locus', 'alleles']; ----------------------------------",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5657
https://github.com/hail-is/hail/issues/5659:415,Availability,error,error,415,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: 0.2.11. ### What you did: ./gradlew -Dspark.version=2.2.1 -Dspark.version=2.2.1 -Dpy4j.version=0.10.4 -Dbreeze.version=0.13.1 shadowJar. ### What went wrong (all error messages here, including the full java stack trace):; :compileJava UP-TO-DATE; :generateBuildInfo; :nativeLib; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux testutils/unit-tests.cpp -MG -M -MF build/testutils/unit-tests.d -MT build/testutils/unit-tests.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux test.cpp -MG -M -MF build/test.d -MT build/test.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux ibs.cpp -MG -M -MF build/ibs.d -MT build/ibs.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux davies.cpp -MG -M -MF build/davies.d -MT build/davies.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux cache-tests.cpp -MG -M -MF build/cache-tests.d -MT build/cache-tests.o; g++ -march=s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5659
https://github.com/hail-is/hail/issues/5659:7545,Availability,error,error,7545,"d/Decoder.d -MT build/Decoder.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux ApproximateQuantiles_test.cpp -MG -M -MF build/ApproximateQuantiles_test.d -MT build/ApproximateQuantiles_test.o; g++ -o build/NativeBoot.o -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux -MD -MF build/NativeBoot.d -MT build/NativeBoot.o -c NativeBoot.cpp; g++ -fvisibility=default -rdynamic -shared -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux build/NativeBoot.o -o lib/linux-x86-64/libboot.so; g++ -o build/davies.o -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux -MD -MF build/davies.d -MT build/davies.o -c davies.cpp; davies.cpp: In member function ‘real DaviesAlgo::qf(real*, real*, int*, int, real, real, int, real, real*, int*)’:; davies.cpp:241:8: error: variable ‘qfval’ might be clobbered by ‘longjmp’ or ‘vfork’ [-Werror=clobbered]; real qfval;; ^; cc1plus: all warnings being treated as errors; make: *** [build/davies.o] Error 1; Makefile:63: recipe for target 'build/davies.o' failed; :nativeLib FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':nativeLib'.; > Process 'command 'make'' finished with non-zero exit value 2. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5659
https://github.com/hail-is/hail/issues/5659:7688,Availability,error,errors,7688,"d/Decoder.d -MT build/Decoder.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux ApproximateQuantiles_test.cpp -MG -M -MF build/ApproximateQuantiles_test.d -MT build/ApproximateQuantiles_test.o; g++ -o build/NativeBoot.o -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux -MD -MF build/NativeBoot.d -MT build/NativeBoot.o -c NativeBoot.cpp; g++ -fvisibility=default -rdynamic -shared -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux build/NativeBoot.o -o lib/linux-x86-64/libboot.so; g++ -o build/davies.o -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux -MD -MF build/davies.d -MT build/davies.o -c davies.cpp; davies.cpp: In member function ‘real DaviesAlgo::qf(real*, real*, int*, int, real, real, int, real, real*, int*)’:; davies.cpp:241:8: error: variable ‘qfval’ might be clobbered by ‘longjmp’ or ‘vfork’ [-Werror=clobbered]; real qfval;; ^; cc1plus: all warnings being treated as errors; make: *** [build/davies.o] Error 1; Makefile:63: recipe for target 'build/davies.o' failed; :nativeLib FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':nativeLib'.; > Process 'command 'make'' finished with non-zero exit value 2. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5659
https://github.com/hail-is/hail/issues/5659:7723,Availability,Error,Error,7723,"d/Decoder.d -MT build/Decoder.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux ApproximateQuantiles_test.cpp -MG -M -MF build/ApproximateQuantiles_test.d -MT build/ApproximateQuantiles_test.o; g++ -o build/NativeBoot.o -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux -MD -MF build/NativeBoot.d -MT build/NativeBoot.o -c NativeBoot.cpp; g++ -fvisibility=default -rdynamic -shared -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux build/NativeBoot.o -o lib/linux-x86-64/libboot.so; g++ -o build/davies.o -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux -MD -MF build/davies.d -MT build/davies.o -c davies.cpp; davies.cpp: In member function ‘real DaviesAlgo::qf(real*, real*, int*, int, real, real, int, real, real*, int*)’:; davies.cpp:241:8: error: variable ‘qfval’ might be clobbered by ‘longjmp’ or ‘vfork’ [-Werror=clobbered]; real qfval;; ^; cc1plus: all warnings being treated as errors; make: *** [build/davies.o] Error 1; Makefile:63: recipe for target 'build/davies.o' failed; :nativeLib FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':nativeLib'.; > Process 'command 'make'' finished with non-zero exit value 2. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5659
https://github.com/hail-is/hail/issues/5659:7807,Availability,FAILURE,FAILURE,7807,"d/Decoder.d -MT build/Decoder.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux ApproximateQuantiles_test.cpp -MG -M -MF build/ApproximateQuantiles_test.d -MT build/ApproximateQuantiles_test.o; g++ -o build/NativeBoot.o -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux -MD -MF build/NativeBoot.d -MT build/NativeBoot.o -c NativeBoot.cpp; g++ -fvisibility=default -rdynamic -shared -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux build/NativeBoot.o -o lib/linux-x86-64/libboot.so; g++ -o build/davies.o -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux -MD -MF build/davies.d -MT build/davies.o -c davies.cpp; davies.cpp: In member function ‘real DaviesAlgo::qf(real*, real*, int*, int, real, real, int, real, real*, int*)’:; davies.cpp:241:8: error: variable ‘qfval’ might be clobbered by ‘longjmp’ or ‘vfork’ [-Werror=clobbered]; real qfval;; ^; cc1plus: all warnings being treated as errors; make: *** [build/davies.o] Error 1; Makefile:63: recipe for target 'build/davies.o' failed; :nativeLib FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':nativeLib'.; > Process 'command 'make'' finished with non-zero exit value 2. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5659
https://github.com/hail-is/hail/issues/5659:421,Integrability,message,messages,421,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: 0.2.11. ### What you did: ./gradlew -Dspark.version=2.2.1 -Dspark.version=2.2.1 -Dpy4j.version=0.10.4 -Dbreeze.version=0.13.1 shadowJar. ### What went wrong (all error messages here, including the full java stack trace):; :compileJava UP-TO-DATE; :generateBuildInfo; :nativeLib; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux testutils/unit-tests.cpp -MG -M -MF build/testutils/unit-tests.d -MT build/testutils/unit-tests.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux test.cpp -MG -M -MF build/test.d -MT build/test.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux ibs.cpp -MG -M -MF build/ibs.d -MT build/ibs.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux davies.cpp -MG -M -MF build/davies.d -MT build/davies.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux cache-tests.cpp -MG -M -MF build/cache-tests.d -MT build/cache-tests.o; g++ -march=s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5659
https://github.com/hail-is/hail/issues/5659:7552,Modifiability,variab,variable,7552,"d/Decoder.d -MT build/Decoder.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux ApproximateQuantiles_test.cpp -MG -M -MF build/ApproximateQuantiles_test.d -MT build/ApproximateQuantiles_test.o; g++ -o build/NativeBoot.o -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux -MD -MF build/NativeBoot.d -MT build/NativeBoot.o -c NativeBoot.cpp; g++ -fvisibility=default -rdynamic -shared -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux build/NativeBoot.o -o lib/linux-x86-64/libboot.so; g++ -o build/davies.o -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux -MD -MF build/davies.d -MT build/davies.o -c davies.cpp; davies.cpp: In member function ‘real DaviesAlgo::qf(real*, real*, int*, int, real, real, int, real, real*, int*)’:; davies.cpp:241:8: error: variable ‘qfval’ might be clobbered by ‘longjmp’ or ‘vfork’ [-Werror=clobbered]; real qfval;; ^; cc1plus: all warnings being treated as errors; make: *** [build/davies.o] Error 1; Makefile:63: recipe for target 'build/davies.o' failed; :nativeLib FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':nativeLib'.; > Process 'command 'make'' finished with non-zero exit value 2. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5659
https://github.com/hail-is/hail/issues/5659:1917,Performance,cache,cache-tests,1917,h=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux test.cpp -MG -M -MF build/test.d -MT build/test.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux ibs.cpp -MG -M -MF build/ibs.d -MT build/ibs.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux davies.cpp -MG -M -MF build/davies.d -MT build/davies.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux cache-tests.cpp -MG -M -MF build/cache-tests.d -MT build/cache-tests.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux Upcalls.cpp -MG -M -MF build/Upcalls.d -MT build/Upcalls.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux Region_test.cpp -MG -M -MF build/Region_test.d -MT build/Region_test.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux Region.cpp -MG -M -MF build/Region.d -MT build/Region.o; g++ -march=,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5659
https://github.com/hail-is/hail/issues/5659:1950,Performance,cache,cache-tests,1950,-fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux test.cpp -MG -M -MF build/test.d -MT build/test.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux ibs.cpp -MG -M -MF build/ibs.d -MT build/ibs.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux davies.cpp -MG -M -MF build/davies.d -MT build/davies.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux cache-tests.cpp -MG -M -MF build/cache-tests.d -MT build/cache-tests.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux Upcalls.cpp -MG -M -MF build/Upcalls.d -MT build/Upcalls.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux Region_test.cpp -MG -M -MF build/Region_test.d -MT build/Region_test.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux Region.cpp -MG -M -MF build/Region.d -MT build/Region.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -f,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5659
https://github.com/hail-is/hail/issues/5659:1974,Performance,cache,cache-tests,1974,ources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux test.cpp -MG -M -MF build/test.d -MT build/test.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux ibs.cpp -MG -M -MF build/ibs.d -MT build/ibs.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux davies.cpp -MG -M -MF build/davies.d -MT build/davies.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux cache-tests.cpp -MG -M -MF build/cache-tests.d -MT build/cache-tests.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux Upcalls.cpp -MG -M -MF build/Upcalls.d -MT build/Upcalls.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux Region_test.cpp -MG -M -MF build/Region_test.d -MT build/Region_test.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux Region.cpp -MG -M -MF build/Region.d -MT build/Region.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../reso,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5659
https://github.com/hail-is/hail/issues/5659:758,Testability,test,testutils,758,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: 0.2.11. ### What you did: ./gradlew -Dspark.version=2.2.1 -Dspark.version=2.2.1 -Dpy4j.version=0.10.4 -Dbreeze.version=0.13.1 shadowJar. ### What went wrong (all error messages here, including the full java stack trace):; :compileJava UP-TO-DATE; :generateBuildInfo; :nativeLib; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux testutils/unit-tests.cpp -MG -M -MF build/testutils/unit-tests.d -MT build/testutils/unit-tests.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux test.cpp -MG -M -MF build/test.d -MT build/test.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux ibs.cpp -MG -M -MF build/ibs.d -MT build/ibs.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux davies.cpp -MG -M -MF build/davies.d -MT build/davies.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux cache-tests.cpp -MG -M -MF build/cache-tests.d -MT build/cache-tests.o; g++ -march=s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5659
https://github.com/hail-is/hail/issues/5659:773,Testability,test,tests,773,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: 0.2.11. ### What you did: ./gradlew -Dspark.version=2.2.1 -Dspark.version=2.2.1 -Dpy4j.version=0.10.4 -Dbreeze.version=0.13.1 shadowJar. ### What went wrong (all error messages here, including the full java stack trace):; :compileJava UP-TO-DATE; :generateBuildInfo; :nativeLib; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux testutils/unit-tests.cpp -MG -M -MF build/testutils/unit-tests.d -MT build/testutils/unit-tests.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux test.cpp -MG -M -MF build/test.d -MT build/test.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux ibs.cpp -MG -M -MF build/ibs.d -MT build/ibs.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux davies.cpp -MG -M -MF build/davies.d -MT build/davies.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux cache-tests.cpp -MG -M -MF build/cache-tests.d -MT build/cache-tests.o; g++ -march=s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5659
https://github.com/hail-is/hail/issues/5659:800,Testability,test,testutils,800,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: 0.2.11. ### What you did: ./gradlew -Dspark.version=2.2.1 -Dspark.version=2.2.1 -Dpy4j.version=0.10.4 -Dbreeze.version=0.13.1 shadowJar. ### What went wrong (all error messages here, including the full java stack trace):; :compileJava UP-TO-DATE; :generateBuildInfo; :nativeLib; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux testutils/unit-tests.cpp -MG -M -MF build/testutils/unit-tests.d -MT build/testutils/unit-tests.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux test.cpp -MG -M -MF build/test.d -MT build/test.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux ibs.cpp -MG -M -MF build/ibs.d -MT build/ibs.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux davies.cpp -MG -M -MF build/davies.d -MT build/davies.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux cache-tests.cpp -MG -M -MF build/cache-tests.d -MT build/cache-tests.o; g++ -march=s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5659
https://github.com/hail-is/hail/issues/5659:815,Testability,test,tests,815,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: 0.2.11. ### What you did: ./gradlew -Dspark.version=2.2.1 -Dspark.version=2.2.1 -Dpy4j.version=0.10.4 -Dbreeze.version=0.13.1 shadowJar. ### What went wrong (all error messages here, including the full java stack trace):; :compileJava UP-TO-DATE; :generateBuildInfo; :nativeLib; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux testutils/unit-tests.cpp -MG -M -MF build/testutils/unit-tests.d -MT build/testutils/unit-tests.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux test.cpp -MG -M -MF build/test.d -MT build/test.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux ibs.cpp -MG -M -MF build/ibs.d -MT build/ibs.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux davies.cpp -MG -M -MF build/davies.d -MT build/davies.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux cache-tests.cpp -MG -M -MF build/cache-tests.d -MT build/cache-tests.o; g++ -march=s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5659
https://github.com/hail-is/hail/issues/5659:833,Testability,test,testutils,833,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: 0.2.11. ### What you did: ./gradlew -Dspark.version=2.2.1 -Dspark.version=2.2.1 -Dpy4j.version=0.10.4 -Dbreeze.version=0.13.1 shadowJar. ### What went wrong (all error messages here, including the full java stack trace):; :compileJava UP-TO-DATE; :generateBuildInfo; :nativeLib; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux testutils/unit-tests.cpp -MG -M -MF build/testutils/unit-tests.d -MT build/testutils/unit-tests.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux test.cpp -MG -M -MF build/test.d -MT build/test.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux ibs.cpp -MG -M -MF build/ibs.d -MT build/ibs.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux davies.cpp -MG -M -MF build/davies.d -MT build/davies.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux cache-tests.cpp -MG -M -MF build/cache-tests.d -MT build/cache-tests.o; g++ -march=s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5659
https://github.com/hail-is/hail/issues/5659:848,Testability,test,tests,848,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: 0.2.11. ### What you did: ./gradlew -Dspark.version=2.2.1 -Dspark.version=2.2.1 -Dpy4j.version=0.10.4 -Dbreeze.version=0.13.1 shadowJar. ### What went wrong (all error messages here, including the full java stack trace):; :compileJava UP-TO-DATE; :generateBuildInfo; :nativeLib; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux testutils/unit-tests.cpp -MG -M -MF build/testutils/unit-tests.d -MT build/testutils/unit-tests.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux test.cpp -MG -M -MF build/test.d -MT build/test.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux ibs.cpp -MG -M -MF build/ibs.d -MT build/ibs.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux davies.cpp -MG -M -MF build/davies.d -MT build/davies.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux cache-tests.cpp -MG -M -MF build/cache-tests.d -MT build/cache-tests.o; g++ -march=s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5659
https://github.com/hail-is/hail/issues/5659:1083,Testability,test,test,1083,"information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: 0.2.11. ### What you did: ./gradlew -Dspark.version=2.2.1 -Dspark.version=2.2.1 -Dpy4j.version=0.10.4 -Dbreeze.version=0.13.1 shadowJar. ### What went wrong (all error messages here, including the full java stack trace):; :compileJava UP-TO-DATE; :generateBuildInfo; :nativeLib; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux testutils/unit-tests.cpp -MG -M -MF build/testutils/unit-tests.d -MT build/testutils/unit-tests.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux test.cpp -MG -M -MF build/test.d -MT build/test.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux ibs.cpp -MG -M -MF build/ibs.d -MT build/ibs.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux davies.cpp -MG -M -MF build/davies.d -MT build/davies.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux cache-tests.cpp -MG -M -MF build/cache-tests.d -MT build/cache-tests.o; g++ -march=sandybridge -O3 -std=c++14 -I",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5659
https://github.com/hail-is/hail/issues/5659:1109,Testability,test,test,1109,"scussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: 0.2.11. ### What you did: ./gradlew -Dspark.version=2.2.1 -Dspark.version=2.2.1 -Dpy4j.version=0.10.4 -Dbreeze.version=0.13.1 shadowJar. ### What went wrong (all error messages here, including the full java stack trace):; :compileJava UP-TO-DATE; :generateBuildInfo; :nativeLib; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux testutils/unit-tests.cpp -MG -M -MF build/testutils/unit-tests.d -MT build/testutils/unit-tests.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux test.cpp -MG -M -MF build/test.d -MT build/test.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux ibs.cpp -MG -M -MF build/ibs.d -MT build/ibs.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux davies.cpp -MG -M -MF build/davies.d -MT build/davies.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux cache-tests.cpp -MG -M -MF build/cache-tests.d -MT build/cache-tests.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5659
https://github.com/hail-is/hail/issues/5659:1126,Testability,test,test,1126,"//discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: 0.2.11. ### What you did: ./gradlew -Dspark.version=2.2.1 -Dspark.version=2.2.1 -Dpy4j.version=0.10.4 -Dbreeze.version=0.13.1 shadowJar. ### What went wrong (all error messages here, including the full java stack trace):; :compileJava UP-TO-DATE; :generateBuildInfo; :nativeLib; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux testutils/unit-tests.cpp -MG -M -MF build/testutils/unit-tests.d -MT build/testutils/unit-tests.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux test.cpp -MG -M -MF build/test.d -MT build/test.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux ibs.cpp -MG -M -MF build/ibs.d -MT build/ibs.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux davies.cpp -MG -M -MF build/davies.d -MT build/davies.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux cache-tests.cpp -MG -M -MF build/cache-tests.d -MT build/cache-tests.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5659
https://github.com/hail-is/hail/issues/5659:1923,Testability,test,tests,1923,h=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux test.cpp -MG -M -MF build/test.d -MT build/test.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux ibs.cpp -MG -M -MF build/ibs.d -MT build/ibs.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux davies.cpp -MG -M -MF build/davies.d -MT build/davies.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux cache-tests.cpp -MG -M -MF build/cache-tests.d -MT build/cache-tests.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux Upcalls.cpp -MG -M -MF build/Upcalls.d -MT build/Upcalls.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux Region_test.cpp -MG -M -MF build/Region_test.d -MT build/Region_test.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux Region.cpp -MG -M -MF build/Region.d -MT build/Region.o; g++ -march=,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5659
https://github.com/hail-is/hail/issues/5659:1956,Testability,test,tests,1956,-fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux test.cpp -MG -M -MF build/test.d -MT build/test.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux ibs.cpp -MG -M -MF build/ibs.d -MT build/ibs.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux davies.cpp -MG -M -MF build/davies.d -MT build/davies.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux cache-tests.cpp -MG -M -MF build/cache-tests.d -MT build/cache-tests.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux Upcalls.cpp -MG -M -MF build/Upcalls.d -MT build/Upcalls.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux Region_test.cpp -MG -M -MF build/Region_test.d -MT build/Region_test.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux Region.cpp -MG -M -MF build/Region.d -MT build/Region.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -f,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5659
https://github.com/hail-is/hail/issues/5659:1980,Testability,test,tests,1980,ources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux test.cpp -MG -M -MF build/test.d -MT build/test.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux ibs.cpp -MG -M -MF build/ibs.d -MT build/ibs.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux davies.cpp -MG -M -MF build/davies.d -MT build/davies.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux cache-tests.cpp -MG -M -MF build/cache-tests.d -MT build/cache-tests.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux Upcalls.cpp -MG -M -MF build/Upcalls.d -MT build/Upcalls.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux Region_test.cpp -MG -M -MF build/Region_test.d -MT build/Region_test.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux Region.cpp -MG -M -MF build/Region.d -MT build/Region.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../reso,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5659
https://github.com/hail-is/hail/issues/5659:5505,Testability,Log,Logging,5505,db -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux NativeLongFunc.cpp -MG -M -MF build/NativeLongFunc.d -MT build/NativeLongFunc.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux NativeCodeSuite.cpp -MG -M -MF build/NativeCodeSuite.d -MT build/NativeCodeSuite.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux NativeBoot.cpp -MG -M -MF build/NativeBoot.d -MT build/NativeBoot.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux Logging.cpp -MG -M -MF build/Logging.d -MT build/Logging.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux Encoder.cpp -MG -M -MF build/Encoder.d -MT build/Encoder.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux Decoder.cpp -MG -M -MF build/Decoder.d -MT build/Decoder.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux ApproximateQuantiles_test.cpp -MG -M -MF build/ApproximateQuantiles_test.d -MT build/Appro,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5659
https://github.com/hail-is/hail/issues/5659:5534,Testability,Log,Logging,5534,k-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux NativeLongFunc.cpp -MG -M -MF build/NativeLongFunc.d -MT build/NativeLongFunc.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux NativeCodeSuite.cpp -MG -M -MF build/NativeCodeSuite.d -MT build/NativeCodeSuite.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux NativeBoot.cpp -MG -M -MF build/NativeBoot.d -MT build/NativeBoot.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux Logging.cpp -MG -M -MF build/Logging.d -MT build/Logging.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux Encoder.cpp -MG -M -MF build/Encoder.d -MT build/Encoder.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux Decoder.cpp -MG -M -MF build/Decoder.d -MT build/Decoder.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux ApproximateQuantiles_test.cpp -MG -M -MF build/ApproximateQuantiles_test.d -MT build/ApproximateQuantiles_test.o; g++ -o build/NativeBoot.o -march=sandybridge -O3 -s,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5659
https://github.com/hail-is/hail/issues/5659:5554,Testability,Log,Logging,5554,b/jvm/java-8-openjdk-amd64/include/linux NativeLongFunc.cpp -MG -M -MF build/NativeLongFunc.d -MT build/NativeLongFunc.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux NativeCodeSuite.cpp -MG -M -MF build/NativeCodeSuite.d -MT build/NativeCodeSuite.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux NativeBoot.cpp -MG -M -MF build/NativeBoot.d -MT build/NativeBoot.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux Logging.cpp -MG -M -MF build/Logging.d -MT build/Logging.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux Encoder.cpp -MG -M -MF build/Encoder.d -MT build/Encoder.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux Decoder.cpp -MG -M -MF build/Decoder.d -MT build/Decoder.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux ApproximateQuantiles_test.cpp -MG -M -MF build/ApproximateQuantiles_test.d -MT build/ApproximateQuantiles_test.o; g++ -o build/NativeBoot.o -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5659
https://github.com/hail-is/hail/issues/5659:8080,Testability,log,log,8080,"d/Decoder.d -MT build/Decoder.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux ApproximateQuantiles_test.cpp -MG -M -MF build/ApproximateQuantiles_test.d -MT build/ApproximateQuantiles_test.o; g++ -o build/NativeBoot.o -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux -MD -MF build/NativeBoot.d -MT build/NativeBoot.o -c NativeBoot.cpp; g++ -fvisibility=default -rdynamic -shared -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux build/NativeBoot.o -o lib/linux-x86-64/libboot.so; g++ -o build/davies.o -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux -MD -MF build/davies.d -MT build/davies.o -c davies.cpp; davies.cpp: In member function ‘real DaviesAlgo::qf(real*, real*, int*, int, real, real, int, real, real*, int*)’:; davies.cpp:241:8: error: variable ‘qfval’ might be clobbered by ‘longjmp’ or ‘vfork’ [-Werror=clobbered]; real qfval;; ^; cc1plus: all warnings being treated as errors; make: *** [build/davies.o] Error 1; Makefile:63: recipe for target 'build/davies.o' failed; :nativeLib FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':nativeLib'.; > Process 'command 'make'' finished with non-zero exit value 2. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5659
https://github.com/hail-is/hail/issues/5663:14,Deployability,integrat,integrate,14,`show` should integrate with the jupyter table stuff the same way that pandas does.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5663
https://github.com/hail-is/hail/issues/5663:14,Integrability,integrat,integrate,14,`show` should integrate with the jupyter table stuff the same way that pandas does.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5663
https://github.com/hail-is/hail/issues/5665:69,Deployability,deploy,deploy,69,"In particular, batch leaves its pods around when it gets killed by a deploy.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5665
https://github.com/hail-is/hail/pull/5666:404,Safety,Detect,Detecting,404,"Take a look at the docs for [`IPython.display.display`](https://ipython.readthedocs.io/en/stable/api/generated/IPython.display.html#IPython.display.display). I preserve the user's ability to specify a custom handler. The handler is no longer given a string but an object that has a sensible `__str__` and `__repr__`. Moreover, this object has a `_repr_html_` which Jupyter uses to display an HTML table. Detecting what frontend is being run is done by `IPython.display.display`. I use the `_Show` shim class to avoid having tables themselves print as HTML. I also check for terminal size and use that to pick `n` and `width`. Should `_hl_repr` live here?. Resolves #5663, #2847",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5666
https://github.com/hail-is/hail/pull/5666:511,Safety,avoid,avoid,511,"Take a look at the docs for [`IPython.display.display`](https://ipython.readthedocs.io/en/stable/api/generated/IPython.display.html#IPython.display.display). I preserve the user's ability to specify a custom handler. The handler is no longer given a string but an object that has a sensible `__str__` and `__repr__`. Moreover, this object has a `_repr_html_` which Jupyter uses to display an HTML table. Detecting what frontend is being run is done by `IPython.display.display`. I use the `_Show` shim class to avoid having tables themselves print as HTML. I also check for terminal size and use that to pick `n` and `width`. Should `_hl_repr` live here?. Resolves #5663, #2847",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5666
https://github.com/hail-is/hail/pull/5667:19,Modifiability,variab,variable,19,"If we're binding a variable and using it in an array expression, and then transforming that array expression, we should be able to deforest the entire thing.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5667
https://github.com/hail-is/hail/issues/5673:227,Availability,error,error,227,"`import_bgen` fails because there are no reference genomes on worker nodes. `import_bgen` needs to read the index file. Reading the index file means parsing a type. Parsing a locus type means looking up a reference genome. The error message comes from line 588 in `ReferenceGenome.scala` by way of line 70 of `IndexReader.scala`:; ```scala; val keyType = IRParser.parseType(metadata.keyType); ```. The root cause seems to be #5512, in which we [stop loading the genomes from resources](https://github.com/hail-is/hail/pull/5512/files#diff-16c24a9c4265932816e9e88806f5a2abL527).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5673
https://github.com/hail-is/hail/issues/5673:233,Integrability,message,message,233,"`import_bgen` fails because there are no reference genomes on worker nodes. `import_bgen` needs to read the index file. Reading the index file means parsing a type. Parsing a locus type means looking up a reference genome. The error message comes from line 588 in `ReferenceGenome.scala` by way of line 70 of `IndexReader.scala`:; ```scala; val keyType = IRParser.parseType(metadata.keyType); ```. The root cause seems to be #5512, in which we [stop loading the genomes from resources](https://github.com/hail-is/hail/pull/5512/files#diff-16c24a9c4265932816e9e88806f5a2abL527).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5673
https://github.com/hail-is/hail/issues/5673:450,Performance,load,loading,450,"`import_bgen` fails because there are no reference genomes on worker nodes. `import_bgen` needs to read the index file. Reading the index file means parsing a type. Parsing a locus type means looking up a reference genome. The error message comes from line 588 in `ReferenceGenome.scala` by way of line 70 of `IndexReader.scala`:; ```scala; val keyType = IRParser.parseType(metadata.keyType); ```. The root cause seems to be #5512, in which we [stop loading the genomes from resources](https://github.com/hail-is/hail/pull/5512/files#diff-16c24a9c4265932816e9e88806f5a2abL527).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5673
https://github.com/hail-is/hail/pull/5674:27,Usability,simpl,simple,27,Resolves #5673 in the most simple way. cc @tpoterba you may have thoughts on this.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5674
https://github.com/hail-is/hail/pull/5686:407,Safety,sanity check,sanity check,407,`BlockMatrix.to_numpy` writes separate block files if the matrix size is too big to export in a single file on the leader. This caused a bug on the cluster because the workers were writing to their local filesystems and not Hadoop. This now actually writes and reads with a temp Hadoop path. Added an underscore parameter to the `to_numpy` method to force writing the block matrix out in blocks and added a sanity check in the cluster tests.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5686
https://github.com/hail-is/hail/pull/5686:435,Testability,test,tests,435,`BlockMatrix.to_numpy` writes separate block files if the matrix size is too big to export in a single file on the leader. This caused a bug on the cluster because the workers were writing to their local filesystems and not Hadoop. This now actually writes and reads with a temp Hadoop path. Added an underscore parameter to the `to_numpy` method to force writing the block matrix out in blocks and added a sanity check in the cluster tests.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5686
https://github.com/hail-is/hail/issues/5700:355,Availability,error,error,355,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: 0.2.11-fea176012ee0. ### What you did:. ```; hl.eval({'a':2, None: 1}); ```. ### What went wrong (all error messages here, including the full java stack trace):. ```; ---------------------------------------------------------------------------; ExpressionException Traceback (most recent call last); /home/hail/hail.zip/hail/expr/expressions/expression_typecheck.py in check(self, x, caller, param); 73 try:; ---> 74 return self.coerce(to_expr(x)); 75 except ExpressionException as e:. /home/hail/hail.zip/hail/expr/expressions/base_expression.py in to_expr(e, dtype); 99 if not dtype:; --> 100 dtype = impute_type(e); 101 x = _to_expr(e, dtype). /home/hail/hail.zip/hail/expr/expressions/base_expression.py in impute_type(x); 73 raise ExpressionException(""Cannot impute type of empty dict. Use 'hl.empty_dict' to create an empty dict.""); ---> 74 kts = {impute_type(element) for element in x.keys()}; 75 vts = {impute_type(element) for element in x.values()}. /home/hail/hail.zip/hail/expr/expressions/base_expression.py in <setcomp>(.0); 73 raise ExpressionException(""Cannot impute type of empty dict. Use 'hl.empty_dict' to create an empty dict.""); ---> 74 kts = {impute_type(element) for element in x.keys()}; 75 vts = {impute_type(element) for element in x.values()}. /home/hail/hail.zip/hail/expr/expressions/base_expression.py in impute_type(x); 85 elif x is None:; ---> 86 raise ExpressionException(""Hail cannot impute the type of 'None'""); 87 elif isinstance(x, (hl.expr.builders.CaseBuilder, hl.expr.builders.SwitchBuilder)):. ExpressionException: Hail cannot impute the type of 'None'. The above exception was the direct cause of the following exception:. TypecheckFailure Traceback (most recent call last); /home/hail/hail.zip/hail/typech",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5700
https://github.com/hail-is/hail/issues/5700:361,Integrability,message,messages,361,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: 0.2.11-fea176012ee0. ### What you did:. ```; hl.eval({'a':2, None: 1}); ```. ### What went wrong (all error messages here, including the full java stack trace):. ```; ---------------------------------------------------------------------------; ExpressionException Traceback (most recent call last); /home/hail/hail.zip/hail/expr/expressions/expression_typecheck.py in check(self, x, caller, param); 73 try:; ---> 74 return self.coerce(to_expr(x)); 75 except ExpressionException as e:. /home/hail/hail.zip/hail/expr/expressions/base_expression.py in to_expr(e, dtype); 99 if not dtype:; --> 100 dtype = impute_type(e); 101 x = _to_expr(e, dtype). /home/hail/hail.zip/hail/expr/expressions/base_expression.py in impute_type(x); 73 raise ExpressionException(""Cannot impute type of empty dict. Use 'hl.empty_dict' to create an empty dict.""); ---> 74 kts = {impute_type(element) for element in x.keys()}; 75 vts = {impute_type(element) for element in x.values()}. /home/hail/hail.zip/hail/expr/expressions/base_expression.py in <setcomp>(.0); 73 raise ExpressionException(""Cannot impute type of empty dict. Use 'hl.empty_dict' to create an empty dict.""); ---> 74 kts = {impute_type(element) for element in x.keys()}; 75 vts = {impute_type(element) for element in x.values()}. /home/hail/hail.zip/hail/expr/expressions/base_expression.py in impute_type(x); 85 elif x is None:; ---> 86 raise ExpressionException(""Hail cannot impute the type of 'None'""); 87 elif isinstance(x, (hl.expr.builders.CaseBuilder, hl.expr.builders.SwitchBuilder)):. ExpressionException: Hail cannot impute the type of 'None'. The above exception was the direct cause of the following exception:. TypecheckFailure Traceback (most recent call last); /home/hail/hail.zip/hail/typech",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5700
https://github.com/hail-is/hail/issues/5700:2702,Integrability,wrap,wrapper,2702,"y dict. Use 'hl.empty_dict' to create an empty dict.""); ---> 74 kts = {impute_type(element) for element in x.keys()}; 75 vts = {impute_type(element) for element in x.values()}. /home/hail/hail.zip/hail/expr/expressions/base_expression.py in impute_type(x); 85 elif x is None:; ---> 86 raise ExpressionException(""Hail cannot impute the type of 'None'""); 87 elif isinstance(x, (hl.expr.builders.CaseBuilder, hl.expr.builders.SwitchBuilder)):. ExpressionException: Hail cannot impute the type of 'None'. The above exception was the direct cause of the following exception:. TypecheckFailure Traceback (most recent call last); /home/hail/hail.zip/hail/typecheck/check.py in check_all(f, args, kwargs, checks, is_method); 487 arg = args[i]; --> 488 args_.append(checker.check(arg, name, arg_name)); 489 # passed as keyword. /home/hail/hail.zip/hail/expr/expressions/expression_typecheck.py in check(self, x, caller, param); 75 except ExpressionException as e:; ---> 76 raise TypecheckFailure from e; 77 . TypecheckFailure: . The above exception was the direct cause of the following exception:. TypeError Traceback (most recent call last); <ipython-input-76-5be38e2a21ef> in <module>; ----> 1 hl.eval({'a':2, None: 1}). </opt/conda/lib/python3.6/site-packages/decorator.py:decorator-gen-583> in eval(expression). /home/hail/hail.zip/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 558 @decorator; 559 def wrapper(__original_func, *args, **kwargs):; --> 560 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); 561 return __original_func(*args_, **kwargs_); 562 . /home/hail/hail.zip/hail/typecheck/check.py in check_all(f, args, kwargs, checks, is_method); 512 expected=checker.expects(),; 513 found=checker.format(arg); --> 514 )) from e; 515 elif param.kind == param.VAR_POSITIONAL:; 516 # consume the rest of the positional arguments. TypeError: eval: parameter 'expression': expected expression of type any, found dict: {'a': 2, None: 1}; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5700
https://github.com/hail-is/hail/issues/5700:2769,Integrability,wrap,wrapper,2769,"y dict. Use 'hl.empty_dict' to create an empty dict.""); ---> 74 kts = {impute_type(element) for element in x.keys()}; 75 vts = {impute_type(element) for element in x.values()}. /home/hail/hail.zip/hail/expr/expressions/base_expression.py in impute_type(x); 85 elif x is None:; ---> 86 raise ExpressionException(""Hail cannot impute the type of 'None'""); 87 elif isinstance(x, (hl.expr.builders.CaseBuilder, hl.expr.builders.SwitchBuilder)):. ExpressionException: Hail cannot impute the type of 'None'. The above exception was the direct cause of the following exception:. TypecheckFailure Traceback (most recent call last); /home/hail/hail.zip/hail/typecheck/check.py in check_all(f, args, kwargs, checks, is_method); 487 arg = args[i]; --> 488 args_.append(checker.check(arg, name, arg_name)); 489 # passed as keyword. /home/hail/hail.zip/hail/expr/expressions/expression_typecheck.py in check(self, x, caller, param); 75 except ExpressionException as e:; ---> 76 raise TypecheckFailure from e; 77 . TypecheckFailure: . The above exception was the direct cause of the following exception:. TypeError Traceback (most recent call last); <ipython-input-76-5be38e2a21ef> in <module>; ----> 1 hl.eval({'a':2, None: 1}). </opt/conda/lib/python3.6/site-packages/decorator.py:decorator-gen-583> in eval(expression). /home/hail/hail.zip/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 558 @decorator; 559 def wrapper(__original_func, *args, **kwargs):; --> 560 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); 561 return __original_func(*args_, **kwargs_); 562 . /home/hail/hail.zip/hail/typecheck/check.py in check_all(f, args, kwargs, checks, is_method); 512 expected=checker.expects(),; 513 found=checker.format(arg); --> 514 )) from e; 515 elif param.kind == param.VAR_POSITIONAL:; 516 # consume the rest of the positional arguments. TypeError: eval: parameter 'expression': expected expression of type any, found dict: {'a': 2, None: 1}; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5700
https://github.com/hail-is/hail/issues/5707:103,Usability,clear,clear,103,see: https://hail.zulipchat.com/#narrow/stream/123010-Hail-0.2E2.20support/topic/Shuffle.20issues. Not clear exactly what's wrong but switching from the cartesian join syntax to the dict join syntax resolves the issue.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5707
https://github.com/hail-is/hail/pull/5709:23,Modifiability,refactor,refactoring,23,the uri got dropped in refactoring on PR that just went in #5686. Reverted the change and added a test to cluster sanity check.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5709
https://github.com/hail-is/hail/pull/5709:114,Safety,sanity check,sanity check,114,the uri got dropped in refactoring on PR that just went in #5686. Reverted the change and added a test to cluster sanity check.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5709
https://github.com/hail-is/hail/pull/5709:98,Testability,test,test,98,the uri got dropped in refactoring on PR that just went in #5686. Reverted the change and added a test to cluster sanity check.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5709
https://github.com/hail-is/hail/issues/5715:19,Safety,Safe,SafeRow,19,"Instead of using a SafeRow for globals, use a region that is owned by the caller (CompileAndEvaluate). . This will be easier after Matrix lowering is done.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5715
https://github.com/hail-is/hail/issues/5718:247,Availability,Error,Error,247,"```; # cat foo; 7	75216143	75216143	C/T	+; # python <<EOF; import hail as hl; hl.import_matrix_table(; 'foo',; row_fields={f'f{i}': hl.tstr for i in range(5)},; no_header=True).count(); EOF; ```; produces:; ```; Hail version: 0.2.11-cf54f08305d1; Error summary: HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; ```; I expect it to say something like ""yo dawg, you forgot to have entries, maybe you actually want import_table"". full output:; ```; Initializing Spark and Hail with default parameters...; using hail jar at /Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/hail/hail-all-spark.jar; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; 19/03/27 18:27:22 WARN Utils: Your hostname, wmb16-359 resolves to a loopback address: 127.0.0.1; using 10.1.1.163 instead (on interface en0); 19/03/27 18:27:22 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address; Running on Apache Spark version 2.2.3; SparkUI available at http://10.1.1.163:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.11-cf54f08305d1; LOGGING: writing to /Users/dking/hail-20190327-1827-0.2.11-cf54f08305d1.log; Traceback (most recent call last):; File ""<stdin>"", line 4, in <module>; File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/hail/matrixtable.py"", line 2371, in count; return (self.count_rows(), self.count_cols()); File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/hail/matrixtable.py"", line 2331, in count_rows; TableCount(MatrixRowsTable(self._mir))); File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/hail/backend/backend.py"", line 94, in execute; self._to_java_ir(ir))); File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/py4j/java_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/issues/5718:277,Availability,Error,Error,277,"```; # cat foo; 7	75216143	75216143	C/T	+; # python <<EOF; import hail as hl; hl.import_matrix_table(; 'foo',; row_fields={f'f{i}': hl.tstr for i in range(5)},; no_header=True).count(); EOF; ```; produces:; ```; Hail version: 0.2.11-cf54f08305d1; Error summary: HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; ```; I expect it to say something like ""yo dawg, you forgot to have entries, maybe you actually want import_table"". full output:; ```; Initializing Spark and Hail with default parameters...; using hail jar at /Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/hail/hail-all-spark.jar; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; 19/03/27 18:27:22 WARN Utils: Your hostname, wmb16-359 resolves to a loopback address: 127.0.0.1; using 10.1.1.163 instead (on interface en0); 19/03/27 18:27:22 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address; Running on Apache Spark version 2.2.3; SparkUI available at http://10.1.1.163:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.11-cf54f08305d1; LOGGING: writing to /Users/dking/hail-20190327-1827-0.2.11-cf54f08305d1.log; Traceback (most recent call last):; File ""<stdin>"", line 4, in <module>; File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/hail/matrixtable.py"", line 2371, in count; return (self.count_rows(), self.count_cols()); File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/hail/matrixtable.py"", line 2331, in count_rows; TableCount(MatrixRowsTable(self._mir))); File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/hail/backend/backend.py"", line 94, in execute; self._to_java_ir(ir))); File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/py4j/java_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/issues/5718:1182,Availability,avail,available,1182,"F; ```; produces:; ```; Hail version: 0.2.11-cf54f08305d1; Error summary: HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; ```; I expect it to say something like ""yo dawg, you forgot to have entries, maybe you actually want import_table"". full output:; ```; Initializing Spark and Hail with default parameters...; using hail jar at /Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/hail/hail-all-spark.jar; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; 19/03/27 18:27:22 WARN Utils: Your hostname, wmb16-359 resolves to a loopback address: 127.0.0.1; using 10.1.1.163 instead (on interface en0); 19/03/27 18:27:22 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address; Running on Apache Spark version 2.2.3; SparkUI available at http://10.1.1.163:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.11-cf54f08305d1; LOGGING: writing to /Users/dking/hail-20190327-1827-0.2.11-cf54f08305d1.log; Traceback (most recent call last):; File ""<stdin>"", line 4, in <module>; File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/hail/matrixtable.py"", line 2371, in count; return (self.count_rows(), self.count_cols()); File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/hail/matrixtable.py"", line 2331, in count_rows; TableCount(MatrixRowsTable(self._mir))); File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/hail/backend/backend.py"", line 94, in execute; self._to_java_ir(ir))); File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/py4j/java_gateway.py"", line 1257, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/hail/utils/java.py"", line ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/issues/5718:2203,Availability,Error,Error,2203,"3:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.11-cf54f08305d1; LOGGING: writing to /Users/dking/hail-20190327-1827-0.2.11-cf54f08305d1.log; Traceback (most recent call last):; File ""<stdin>"", line 4, in <module>; File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/hail/matrixtable.py"", line 2371, in count; return (self.count_rows(), self.count_cols()); File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/hail/matrixtable.py"", line 2331, in count_rows; TableCount(MatrixRowsTable(self._mir))); File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/hail/backend/backend.py"", line 94, in execute; self._to_java_ir(ir))); File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/py4j/java_gateway.py"", line 1257, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/hail/utils/java.py"", line 227, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost, executor driver): is.hail.utils.HailException: foo: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; offending line: 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:20); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.utils.Context.wrapException(Context.scala:19); 	at is.hail.utils.WithContext.wrap(Context.scala:43); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/issues/5718:2322,Availability,Error,Error,2322,"Users/dking/hail-20190327-1827-0.2.11-cf54f08305d1.log; Traceback (most recent call last):; File ""<stdin>"", line 4, in <module>; File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/hail/matrixtable.py"", line 2371, in count; return (self.count_rows(), self.count_cols()); File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/hail/matrixtable.py"", line 2331, in count_rows; TableCount(MatrixRowsTable(self._mir))); File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/hail/backend/backend.py"", line 94, in execute; self._to_java_ir(ir))); File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/py4j/java_gateway.py"", line 1257, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/hail/utils/java.py"", line 227, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost, executor driver): is.hail.utils.HailException: foo: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; offending line: 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:20); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.utils.Context.wrapException(Context.scala:19); 	at is.hail.utils.WithContext.wrap(Context.scala:43); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/issues/5718:2517,Availability,failure,failure,2517,"l/lib/python3.6/site-packages/hail/matrixtable.py"", line 2371, in count; return (self.count_rows(), self.count_cols()); File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/hail/matrixtable.py"", line 2331, in count_rows; TableCount(MatrixRowsTable(self._mir))); File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/hail/backend/backend.py"", line 94, in execute; self._to_java_ir(ir))); File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/py4j/java_gateway.py"", line 1257, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/hail/utils/java.py"", line 227, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost, executor driver): is.hail.utils.HailException: foo: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; offending line: 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:20); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.utils.Context.wrapException(Context.scala:19); 	at is.hail.utils.WithContext.wrap(Context.scala:43); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/issues/5718:2574,Availability,failure,failure,2574,"71, in count; return (self.count_rows(), self.count_cols()); File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/hail/matrixtable.py"", line 2331, in count_rows; TableCount(MatrixRowsTable(self._mir))); File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/hail/backend/backend.py"", line 94, in execute; self._to_java_ir(ir))); File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/py4j/java_gateway.py"", line 1257, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/hail/utils/java.py"", line 227, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost, executor driver): is.hail.utils.HailException: foo: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; offending line: 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:20); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.utils.Context.wrapException(Context.scala:19); 	at is.hail.utils.WithContext.wrap(Context.scala:43); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:575); 	at ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/issues/5718:2681,Availability,Error,Error,2681,"ixtable.py"", line 2331, in count_rows; TableCount(MatrixRowsTable(self._mir))); File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/hail/backend/backend.py"", line 94, in execute; self._to_java_ir(ir))); File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/py4j/java_gateway.py"", line 1257, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/hail/utils/java.py"", line 227, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost, executor driver): is.hail.utils.HailException: foo: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; offending line: 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:20); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.utils.Context.wrapException(Context.scala:19); 	at is.hail.utils.WithContext.wrap(Context.scala:43); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:575); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:573); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.app",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/issues/5718:2860,Availability,Error,ErrorHandling,2860,"end.py"", line 94, in execute; self._to_java_ir(ir))); File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/py4j/java_gateway.py"", line 1257, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/hail/utils/java.py"", line 227, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost, executor driver): is.hail.utils.HailException: foo: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; offending line: 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:20); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.utils.Context.wrapException(Context.scala:19); 	at is.hail.utils.WithContext.wrap(Context.scala:43); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:575); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:573); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.Conte",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/issues/5718:2886,Availability,Error,ErrorHandling,2886," execute; self._to_java_ir(ir))); File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/py4j/java_gateway.py"", line 1257, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/hail/utils/java.py"", line 227, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost, executor driver): is.hail.utils.HailException: foo: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; offending line: 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:20); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.utils.Context.wrapException(Context.scala:19); 	at is.hail.utils.WithContext.wrap(Context.scala:43); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:575); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:573); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/issues/5718:5365,Availability,Error,Error,5365,ollection.AbstractIterator.foldLeft(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.fold(TraversableOnce.scala:212); 	at scala.collection.AbstractIterator.fold(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	... 30 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1575); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1563); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1562); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collect,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/issues/5718:5501,Availability,Error,ErrorHandling,5501,nce.scala:212); 	at scala.collection.AbstractIterator.fold(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	... 30 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1575); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1563); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1562); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGSched,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/issues/5718:5527,Availability,Error,ErrorHandling,5527,scala.collection.AbstractIterator.fold(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	... 30 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1575); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1563); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1562); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1562); 	a,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/issues/5718:10014,Availability,Error,Error,10014,expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:39); 	at is.hail.expr.ir.CompileAndEvaluate$.evaluateToJSON(CompileAndEvaluate.scala:14); 	at is.hail.expr.ir.CompileAndEvaluate.evaluateToJSON(CompileAndEvaluate.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:745). is.hail.utils.HailException: foo: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; offending line: 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:20); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.utils.Context.wrapException(Context.scala:19); 	at is.hail.utils.WithContext.wrap(Context.scala:43); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:575); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:573); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.app,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/issues/5718:10193,Availability,Error,ErrorHandling,10193,r.ir.CompileAndEvaluate.evaluateToJSON(CompileAndEvaluate.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:745). is.hail.utils.HailException: foo: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; offending line: 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:20); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.utils.Context.wrapException(Context.scala:19); 	at is.hail.utils.WithContext.wrap(Context.scala:43); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:575); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:573); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.Conte,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/issues/5718:10219,Availability,Error,ErrorHandling,10219,ate.evaluateToJSON(CompileAndEvaluate.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:745). is.hail.utils.HailException: foo: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; offending line: 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:20); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.utils.Context.wrapException(Context.scala:19); 	at is.hail.utils.WithContext.wrap(Context.scala:43); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:575); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:573); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/issues/5718:12687,Availability,Error,Error,12687,	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.fold(TraversableOnce.scala:212); 	at scala.collection.AbstractIterator.fold(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.rvd.RVD$$anonfun$count$2.ap,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/issues/5718:12823,Availability,Error,ErrorHandling,12823,raversableOnce.scala:212); 	at scala.collection.AbstractIterator.fold(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:575); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:573); 	at is.hail.sparkextras.ContextRDD$$ano,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/issues/5718:12849,Availability,Error,ErrorHandling,12849,:212); 	at scala.collection.AbstractIterator.fold(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:575); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:573); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/issues/5718:15529,Availability,Error,Error,15529,is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:575); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:573); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); 	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.fold(TraversableOnce.scala:212); 	at scala.collection.AbstractIterator.fold(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Hail version: 0.2.11-cf54f08305d1; Error summary: HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/issues/5718:15559,Availability,Error,Error,15559,is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:575); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:573); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); 	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.fold(TraversableOnce.scala:212); 	at scala.collection.AbstractIterator.fold(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Hail version: 0.2.11-cf54f08305d1; Error summary: HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/issues/5718:4929,Energy Efficiency,schedul,scheduler,4929,); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); 	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.fold(TraversableOnce.scala:212); 	at scala.collection.AbstractIterator.fold(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	... 30 mor,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/issues/5718:5001,Energy Efficiency,schedul,scheduler,5001,ply(ContextRDD.scala:192); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); 	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.fold(TraversableOnce.scala:212); 	at scala.collection.AbstractIterator.fold(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	... 30 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$a,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/issues/5718:5978,Energy Efficiency,schedul,scheduler,5978, org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	... 30 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1575); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1563); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1562); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1562); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1790); 	at org.apa,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/issues/5718:6018,Energy Efficiency,schedul,scheduler,6018,rg.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	... 30 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1575); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1563); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1562); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1562); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1790); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/issues/5718:6117,Energy Efficiency,schedul,scheduler,6117,til.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	... 30 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1575); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1563); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1562); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1562); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1790); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1745); 	at org.apache.spark.scheduler.DAGSchedulerEventProces,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/issues/5718:6215,Energy Efficiency,schedul,scheduler,6215,t.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	... 30 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1575); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1563); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1562); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1562); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1790); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1745); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1734); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoo,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/issues/5718:6469,Energy Efficiency,schedul,scheduler,6469,3	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	... 30 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1575); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1563); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1562); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1562); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1790); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1745); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1734); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:619); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); 	at org.apache.spark.rdd.RDD$$a,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/issues/5718:6550,Energy Efficiency,schedul,scheduler,6550,is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	... 30 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1575); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1563); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1562); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1562); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1790); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1745); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1734); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:619); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1097); 	at org.apache.spark.rdd.RDDOperationScope$.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/issues/5718:6656,Energy Efficiency,schedul,scheduler,6656,:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	... 30 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1575); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1563); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1562); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1562); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1790); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1745); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1734); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:619); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1097); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperation,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/issues/5718:6806,Energy Efficiency,schedul,scheduler,6806,fun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	... 30 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1575); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1563); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1562); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1562); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1790); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1745); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1734); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:619); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1097); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.fold(RDD.scala:1091); 	at is.hail.rvd.RVD.count(,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/issues/5718:6895,Energy Efficiency,schedul,scheduler,6895,rap(Context.scala:41); 	... 30 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1575); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1563); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1562); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1562); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1790); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1745); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1734); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:619); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1097); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.fold(RDD.scala:1091); 	at is.hail.rvd.RVD.count(RVD.scala:580); 	at is.hail.expr.ir.Interpret$$anonfun$apply$1.apply$mcJ$sp(Interpret.sca,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/issues/5718:6993,Energy Efficiency,schedul,scheduler,6993,er.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1575); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1563); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1562); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1562); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1790); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1745); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1734); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:619); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1097); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.fold(RDD.scala:1091); 	at is.hail.rvd.RVD.count(RVD.scala:580); 	at is.hail.expr.ir.Interpret$$anonfun$apply$1.apply$mcJ$sp(Interpret.scala:756); 	at is.hail.expr.ir.Interpret$$anonfun$apply$1.apply(Interpret.scala:756); 	at is.hail.ex,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/issues/5718:7089,Energy Efficiency,schedul,scheduler,7089,; 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1563); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1562); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1562); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1790); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1745); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1734); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:619); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1097); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.fold(RDD.scala:1091); 	at is.hail.rvd.RVD.count(RVD.scala:580); 	at is.hail.expr.ir.Interpret$$anonfun$apply$1.apply$mcJ$sp(Interpret.scala:756); 	at is.hail.expr.ir.Interpret$$anonfun$apply$1.apply(Interpret.scala:756); 	at is.hail.expr.ir.Interpret$$anonfun$apply$1.apply(Interpret.scala:756); 	at scala.Option.getOrElse(Option.s,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/issues/5718:7254,Energy Efficiency,schedul,scheduler,7254,.apply(DAGScheduler.scala:1562); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1562); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1790); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1745); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1734); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:619); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1097); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.fold(RDD.scala:1091); 	at is.hail.rvd.RVD.count(RVD.scala:580); 	at is.hail.expr.ir.Interpret$$anonfun$apply$1.apply$mcJ$sp(Interpret.scala:756); 	at is.hail.expr.ir.Interpret$$anonfun$apply$1.apply(Interpret.scala:756); 	at is.hail.expr.ir.Interpret$$anonfun$apply$1.apply(Interpret.scala:756); 	at scala.Option.getOrElse(Option.scala:121); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:756); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:86); 	at is.hail.expr.ir.Interpret$.apply(,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/issues/5718:12262,Energy Efficiency,schedul,scheduler,12262,); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); 	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.fold(TraversableOnce.scala:212); 	at scala.collection.AbstractIterator.fold(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	at is.hail.io.LoadMat,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/issues/5718:12334,Energy Efficiency,schedul,scheduler,12334,ply(ContextRDD.scala:192); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); 	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.fold(TraversableOnce.scala:212); 	at scala.collection.AbstractIterator.fold(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hai,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/issues/5718:15098,Energy Efficiency,schedul,scheduler,15098,is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:575); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:573); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); 	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.fold(TraversableOnce.scala:212); 	at scala.collection.AbstractIterator.fold(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Hail version: 0.2.11-cf54f08305d1; Error summary: HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/issues/5718:15170,Energy Efficiency,schedul,scheduler,15170,is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:575); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:573); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); 	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.fold(TraversableOnce.scala:212); 	at scala.collection.AbstractIterator.fold(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Hail version: 0.2.11-cf54f08305d1; Error summary: HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/issues/5718:1030,Integrability,interface,interface,1030,"rix_table(; 'foo',; row_fields={f'f{i}': hl.tstr for i in range(5)},; no_header=True).count(); EOF; ```; produces:; ```; Hail version: 0.2.11-cf54f08305d1; Error summary: HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; ```; I expect it to say something like ""yo dawg, you forgot to have entries, maybe you actually want import_table"". full output:; ```; Initializing Spark and Hail with default parameters...; using hail jar at /Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/hail/hail-all-spark.jar; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; 19/03/27 18:27:22 WARN Utils: Your hostname, wmb16-359 resolves to a loopback address: 127.0.0.1; using 10.1.1.163 instead (on interface en0); 19/03/27 18:27:22 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address; Running on Apache Spark version 2.2.3; SparkUI available at http://10.1.1.163:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.11-cf54f08305d1; LOGGING: writing to /Users/dking/hail-20190327-1827-0.2.11-cf54f08305d1.log; Traceback (most recent call last):; File ""<stdin>"", line 4, in <module>; File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/hail/matrixtable.py"", line 2371, in count; return (self.count_rows(), self.count_cols()); File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/hail/matrixtable.py"", line 2331, in count_rows; TableCount(MatrixRowsTable(self._mir))); File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/hail/backend/backend.py"", line 94, in execute; self._to_java_ir(ir))); File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/py4j/java_gateway.py"", line 1257, in __call__; answer, self.gateway_client, self.target_id, self.name",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/issues/5718:2989,Integrability,wrap,wrapException,2989,"_gateway.py"", line 1257, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/hail/utils/java.py"", line 227, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost, executor driver): is.hail.utils.HailException: foo: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; offending line: 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:20); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.utils.Context.wrapException(Context.scala:19); 	at is.hail.utils.WithContext.wrap(Context.scala:43); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:575); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:573); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$appl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/issues/5718:3052,Integrability,wrap,wrap,3052,"_client, self.target_id, self.name); File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/hail/utils/java.py"", line 227, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost, executor driver): is.hail.utils.HailException: foo: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; offending line: 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:20); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.utils.Context.wrapException(Context.scala:19); 	at is.hail.utils.WithContext.wrap(Context.scala:43); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:575); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:573); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at scala.collection.Itera",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/issues/5718:5899,Integrability,wrap,wrap,5899,); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	... 30 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1575); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1563); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1562); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1562); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.D,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/issues/5718:10322,Integrability,wrap,wrapException,10322, 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:745). is.hail.utils.HailException: foo: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; offending line: 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:20); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.utils.Context.wrapException(Context.scala:19); 	at is.hail.utils.WithContext.wrap(Context.scala:43); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:575); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:573); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$appl,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/issues/5718:10385,Integrability,wrap,wrap,10385,hodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:745). is.hail.utils.HailException: foo: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; offending line: 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:20); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.utils.Context.wrapException(Context.scala:19); 	at is.hail.utils.WithContext.wrap(Context.scala:43); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:575); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:573); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at scala.collection.Itera,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/issues/5718:13221,Integrability,wrap,wrap,13221,.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:575); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:573); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at scala.collection.Itera,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/issues/5718:3091,Performance,Load,LoadMatrix,3091,"ng/anaconda2/envs/hail/lib/python3.6/site-packages/hail/utils/java.py"", line 227, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost, executor driver): is.hail.utils.HailException: foo: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; offending line: 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:20); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.utils.Context.wrapException(Context.scala:19); 	at is.hail.utils.WithContext.wrap(Context.scala:43); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:575); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:573); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/issues/5718:3137,Performance,Load,LoadMatrix,3137,"ython3.6/site-packages/hail/utils/java.py"", line 227, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost, executor driver): is.hail.utils.HailException: foo: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; offending line: 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:20); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.utils.Context.wrapException(Context.scala:19); 	at is.hail.utils.WithContext.wrap(Context.scala:43); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:575); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:573); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/issues/5718:3175,Performance,Load,LoadMatrix,3175," deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost, executor driver): is.hail.utils.HailException: foo: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; offending line: 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:20); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.utils.Context.wrapException(Context.scala:19); 	at is.hail.utils.WithContext.wrap(Context.scala:43); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:575); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:573); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Itera",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/issues/5718:3221,Performance,Load,LoadMatrix,3221," (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost, executor driver): is.hail.utils.HailException: foo: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; offending line: 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:20); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.utils.Context.wrapException(Context.scala:19); 	at is.hail.utils.WithContext.wrap(Context.scala:43); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:575); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:573); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$class.foreach(Iterator.sc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/issues/5718:5126,Performance,concurren,concurrent,5126,$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); 	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.fold(TraversableOnce.scala:212); 	at scala.collection.AbstractIterator.fold(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	... 30 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1575); 	at org.apache.spark.scheduler.DAGSc,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/issues/5718:5211,Performance,concurren,concurrent,5211,rator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); 	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.fold(TraversableOnce.scala:212); 	at scala.collection.AbstractIterator.fold(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	... 30 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1575); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1563); 	at org.apache.spark.sc,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/issues/5718:5618,Performance,Load,LoadMatrixParser,5618,fun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	... 30 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1575); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1563); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1562); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1562); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scal,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/issues/5718:5645,Performance,Load,LoadMatrix,5645,$20.apply(RDD.scala:1095); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	... 30 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1575); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1563); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1562); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1562); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at org.apa,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/issues/5718:5682,Performance,Load,LoadMatrix,5682,fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	... 30 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1575); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1563); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1562); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1562); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.a,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/issues/5718:5745,Performance,Load,LoadMatrix,5745,95); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	... 30 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1575); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1563); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1562); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1562); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at sca,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/issues/5718:5783,Performance,Load,LoadMatrix,5783,ntext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	... 30 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1575); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1563); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1562); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1562); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.schedu,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/issues/5718:5846,Performance,Load,LoadMatrix,5846,rk.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	... 30 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1575); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1563); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1562); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1562); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/issues/5718:10424,Performance,Load,LoadMatrix,10424,ethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:745). is.hail.utils.HailException: foo: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; offending line: 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:20); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.utils.Context.wrapException(Context.scala:19); 	at is.hail.utils.WithContext.wrap(Context.scala:43); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:575); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:573); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/issues/5718:10470,Performance,Load,LoadMatrix,10470,egatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:745). is.hail.utils.HailException: foo: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; offending line: 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:20); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.utils.Context.wrapException(Context.scala:19); 	at is.hail.utils.WithContext.wrap(Context.scala:43); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:575); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:573); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/issues/5718:10508,Performance,Load,LoadMatrix,10508,t.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:745). is.hail.utils.HailException: foo: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; offending line: 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:20); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.utils.Context.wrapException(Context.scala:19); 	at is.hail.utils.WithContext.wrap(Context.scala:43); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:575); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:573); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Itera,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/issues/5718:10554,Performance,Load,LoadMatrix,10554,498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:745). is.hail.utils.HailException: foo: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; offending line: 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:20); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.utils.Context.wrapException(Context.scala:19); 	at is.hail.utils.WithContext.wrap(Context.scala:43); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:575); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:573); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$class.foreach(Iterator.sc,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/issues/5718:12459,Performance,concurren,concurrent,12459,$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); 	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.fold(TraversableOnce.scala:212); 	at scala.collection.AbstractIterator.fold(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/issues/5718:12544,Performance,concurren,concurrent,12544,rator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); 	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.fold(TraversableOnce.scala:212); 	at scala.collection.AbstractIterator.fold(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at sca,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/issues/5718:12940,Performance,Load,LoadMatrixParser,12940,d.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:575); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:573); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$cma,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/issues/5718:12967,Performance,Load,LoadMatrix,12967,$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:575); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:573); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anon,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/issues/5718:13004,Performance,Load,LoadMatrix,13004,D$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:575); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:573); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/issues/5718:13067,Performance,Load,LoadMatrix,13067,DD.scala:1095); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:575); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:573); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$a,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/issues/5718:13105,Performance,Load,LoadMatrix,13105,ply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:575); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:573); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at is.hail.sparkextras.Cont,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/issues/5718:13168,Performance,Load,LoadMatrix,13168,.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:575); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:573); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/issues/5718:13260,Performance,Load,LoadMatrix,13260,sk.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:575); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:573); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/issues/5718:13306,Performance,Load,LoadMatrix,13306,87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:575); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:573); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/issues/5718:13344,Performance,Load,LoadMatrix,13344,109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:575); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:573); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Itera,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/issues/5718:13390,Performance,Load,LoadMatrix,13390,xecutor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:575); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:573); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$class.foreach(Iterator.sc,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/issues/5718:15295,Performance,concurren,concurrent,15295,is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:575); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:573); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); 	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.fold(TraversableOnce.scala:212); 	at scala.collection.AbstractIterator.fold(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Hail version: 0.2.11-cf54f08305d1; Error summary: HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/issues/5718:15380,Performance,concurren,concurrent,15380,is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:575); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:573); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); 	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.fold(TraversableOnce.scala:212); 	at scala.collection.AbstractIterator.fold(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Hail version: 0.2.11-cf54f08305d1; Error summary: HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/issues/5718:2496,Safety,abort,aborted,2496,"l/lib/python3.6/site-packages/hail/matrixtable.py"", line 2371, in count; return (self.count_rows(), self.count_cols()); File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/hail/matrixtable.py"", line 2331, in count_rows; TableCount(MatrixRowsTable(self._mir))); File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/hail/backend/backend.py"", line 94, in execute; self._to_java_ir(ir))); File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/py4j/java_gateway.py"", line 1257, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/hail/utils/java.py"", line 227, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost, executor driver): is.hail.utils.HailException: foo: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; offending line: 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:20); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.utils.Context.wrapException(Context.scala:19); 	at is.hail.utils.WithContext.wrap(Context.scala:43); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/issues/5718:6149,Safety,abort,abortStage,6149,oolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	... 30 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1575); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1563); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1562); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1562); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1790); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1745); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSche,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/issues/5718:6247,Safety,abort,abortStage,6247,orker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	... 30 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1575); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1563); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1562); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1562); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1790); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1745); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1734); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.ap,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/issues/5718:6492,Safety,abort,abortStage,6492,orHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	... 30 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1575); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1563); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1562); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1562); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1790); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1745); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1734); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:619); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/issues/5718:787,Testability,log,log,787,"```; # cat foo; 7	75216143	75216143	C/T	+; # python <<EOF; import hail as hl; hl.import_matrix_table(; 'foo',; row_fields={f'f{i}': hl.tstr for i in range(5)},; no_header=True).count(); EOF; ```; produces:; ```; Hail version: 0.2.11-cf54f08305d1; Error summary: HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; ```; I expect it to say something like ""yo dawg, you forgot to have entries, maybe you actually want import_table"". full output:; ```; Initializing Spark and Hail with default parameters...; using hail jar at /Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/hail/hail-all-spark.jar; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; 19/03/27 18:27:22 WARN Utils: Your hostname, wmb16-359 resolves to a loopback address: 127.0.0.1; using 10.1.1.163 instead (on interface en0); 19/03/27 18:27:22 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address; Running on Apache Spark version 2.2.3; SparkUI available at http://10.1.1.163:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.11-cf54f08305d1; LOGGING: writing to /Users/dking/hail-20190327-1827-0.2.11-cf54f08305d1.log; Traceback (most recent call last):; File ""<stdin>"", line 4, in <module>; File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/hail/matrixtable.py"", line 2371, in count; return (self.count_rows(), self.count_cols()); File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/hail/matrixtable.py"", line 2331, in count_rows; TableCount(MatrixRowsTable(self._mir))); File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/hail/backend/backend.py"", line 94, in execute; self._to_java_ir(ir))); File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/py4j/java_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/issues/5718:819,Testability,log,logging,819,"```; # cat foo; 7	75216143	75216143	C/T	+; # python <<EOF; import hail as hl; hl.import_matrix_table(; 'foo',; row_fields={f'f{i}': hl.tstr for i in range(5)},; no_header=True).count(); EOF; ```; produces:; ```; Hail version: 0.2.11-cf54f08305d1; Error summary: HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; ```; I expect it to say something like ""yo dawg, you forgot to have entries, maybe you actually want import_table"". full output:; ```; Initializing Spark and Hail with default parameters...; using hail jar at /Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/hail/hail-all-spark.jar; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; 19/03/27 18:27:22 WARN Utils: Your hostname, wmb16-359 resolves to a loopback address: 127.0.0.1; using 10.1.1.163 instead (on interface en0); 19/03/27 18:27:22 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address; Running on Apache Spark version 2.2.3; SparkUI available at http://10.1.1.163:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.11-cf54f08305d1; LOGGING: writing to /Users/dking/hail-20190327-1827-0.2.11-cf54f08305d1.log; Traceback (most recent call last):; File ""<stdin>"", line 4, in <module>; File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/hail/matrixtable.py"", line 2371, in count; return (self.count_rows(), self.count_cols()); File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/hail/matrixtable.py"", line 2331, in count_rows; TableCount(MatrixRowsTable(self._mir))); File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/hail/backend/backend.py"", line 94, in execute; self._to_java_ir(ir))); File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/py4j/java_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/issues/5718:1323,Testability,LOG,LOGGING,1323," found.; File: foo; Line:; 7	75216143	75216143	C/T	+; ```; I expect it to say something like ""yo dawg, you forgot to have entries, maybe you actually want import_table"". full output:; ```; Initializing Spark and Hail with default parameters...; using hail jar at /Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/hail/hail-all-spark.jar; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; 19/03/27 18:27:22 WARN Utils: Your hostname, wmb16-359 resolves to a loopback address: 127.0.0.1; using 10.1.1.163 instead (on interface en0); 19/03/27 18:27:22 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address; Running on Apache Spark version 2.2.3; SparkUI available at http://10.1.1.163:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.11-cf54f08305d1; LOGGING: writing to /Users/dking/hail-20190327-1827-0.2.11-cf54f08305d1.log; Traceback (most recent call last):; File ""<stdin>"", line 4, in <module>; File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/hail/matrixtable.py"", line 2371, in count; return (self.count_rows(), self.count_cols()); File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/hail/matrixtable.py"", line 2331, in count_rows; TableCount(MatrixRowsTable(self._mir))); File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/hail/backend/backend.py"", line 94, in execute; self._to_java_ir(ir))); File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/py4j/java_gateway.py"", line 1257, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/hail/utils/java.py"", line 227, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: Error parsing row fi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/issues/5718:1395,Testability,log,log,1395,"ave entries, maybe you actually want import_table"". full output:; ```; Initializing Spark and Hail with default parameters...; using hail jar at /Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/hail/hail-all-spark.jar; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; 19/03/27 18:27:22 WARN Utils: Your hostname, wmb16-359 resolves to a loopback address: 127.0.0.1; using 10.1.1.163 instead (on interface en0); 19/03/27 18:27:22 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address; Running on Apache Spark version 2.2.3; SparkUI available at http://10.1.1.163:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.11-cf54f08305d1; LOGGING: writing to /Users/dking/hail-20190327-1827-0.2.11-cf54f08305d1.log; Traceback (most recent call last):; File ""<stdin>"", line 4, in <module>; File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/hail/matrixtable.py"", line 2371, in count; return (self.count_rows(), self.count_cols()); File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/hail/matrixtable.py"", line 2331, in count_rows; TableCount(MatrixRowsTable(self._mir))); File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/hail/backend/backend.py"", line 94, in execute; self._to_java_ir(ir))); File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/py4j/java_gateway.py"", line 1257, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/hail/utils/java.py"", line 227, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+. Java stack trace:; o",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5718
https://github.com/hail-is/hail/pull/5726:120,Security,expose,expose,120,"Default mysql port is 3306, should probably stick with that for default CLOUD_SQL_PORT, especially since google doesn't expose way to override the default",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5726
https://github.com/hail-is/hail/pull/5732:32,Integrability,wrap,wrapping,32,* `approx_quantiles` aggregator wrapping `approx_cdf`; * cdf and pdf plots; * Allow the histogram plot to take the result of the `approx_cdf` aggregator; * Add interactivity to the histogram and pdf plots,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5732
https://github.com/hail-is/hail/pull/5736:282,Deployability,deploy,deploy,282,Spark worker port (on spark-worker) 9000; Spark driver port (on apiserver) 9001; Block manager port (on apiserver and spark-worker) 9002. The apiserver was hanging trying to connect to the master (and therefore notebook2 notebooks trying to connect to it) without this. Tested hand deploy and it fixed it.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5736
https://github.com/hail-is/hail/pull/5736:270,Testability,Test,Tested,270,Spark worker port (on spark-worker) 9000; Spark driver port (on apiserver) 9001; Block manager port (on apiserver and spark-worker) 9002. The apiserver was hanging trying to connect to the master (and therefore notebook2 notebooks trying to connect to it) without this. Tested hand deploy and it fixed it.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5736
https://github.com/hail-is/hail/pull/5737:160,Modifiability,variab,variable,160,"- Register arithmetic functions for NDArray w/ NDArray, value w/ NDArray and NDArray w/ value both in Scala and Python; - Add natural number and natural number variable to both Scala and Python and make NDArray's `nDim` a `NatBase`. This allows specifying function signatures that work on NDArrays of any (but matching) dimensionality.; - Add `NDArrayNumericExpression` and arithmetic methods on it. ### Notes; - In Scala, I directly register the same numeric operations for arrays on ndarrays for reuse and so they don't diverge. However, the IRs generated for `%` and `**` are not supported in C++ yet. I removed the front-end methods for them on `NDArrayNumericExpression` but left the rest of the infrastructure in.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5737
https://github.com/hail-is/hail/pull/5738:598,Availability,error,errors,598,"FYI @cseed @danking: I found a bad bug in the Kubernetes deserializer that we use to deserialize the pod spec from the request from the client. Any variable with a compound name like `generate_name` or `cluster_name` was being set to None regardless of what the input was. This is because their code has a mapping from the Python style with underscores to camel case and it was using the camel case to look up the attributes instead of the underscore names. @akotlar was going to make a PR to correct it in their repo. For now, I'm including the correct code in our repo. Without it, I was getting errors trying to deserialize pod templates with metadata from the MySQL database back to Python. The other horrible behavior I found with the deserializer is if you pass `None` to the deserialize function, you get a dictionary with all attributes set to `None` instead of just `None`. Something to be aware of or we should fix it in this PR to return None instead. @akotlar: I tried to make everything just adhere to Python3, but can you make sure I did the conversions correctly?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5738
https://github.com/hail-is/hail/pull/5738:148,Modifiability,variab,variable,148,"FYI @cseed @danking: I found a bad bug in the Kubernetes deserializer that we use to deserialize the pod spec from the request from the client. Any variable with a compound name like `generate_name` or `cluster_name` was being set to None regardless of what the input was. This is because their code has a mapping from the Python style with underscores to camel case and it was using the camel case to look up the attributes instead of the underscore names. @akotlar was going to make a PR to correct it in their repo. For now, I'm including the correct code in our repo. Without it, I was getting errors trying to deserialize pod templates with metadata from the MySQL database back to Python. The other horrible behavior I found with the deserializer is if you pass `None` to the deserialize function, you get a dictionary with all attributes set to `None` instead of just `None`. Something to be aware of or we should fix it in this PR to return None instead. @akotlar: I tried to make everything just adhere to Python3, but can you make sure I did the conversions correctly?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5738
https://github.com/hail-is/hail/pull/5740:69,Deployability,update,updated,69,"The CountMentions implementation was totally wrong -- it hadn't been updated to use the binding environment. I think most of the usages of Mentions were technically correct as implemented, but this is much safer.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5740
https://github.com/hail-is/hail/pull/5740:206,Safety,safe,safer,206,"The CountMentions implementation was totally wrong -- it hadn't been updated to use the binding environment. I think most of the usages of Mentions were technically correct as implemented, but this is much safer.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5740
https://github.com/hail-is/hail/pull/5741:0,Deployability,Update,Updates,0,Updates notebook2 to use pymysql and hail base image. cc @cseed,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5741
https://github.com/hail-is/hail/issues/5744:84,Availability,error,error,84,"Running on Apache Spark version 2.3.0; Hail version 0.2.12-9409c0635781. The follow error occurs when reading a matrix table. This code worked with Hail v2.8. ```; Traceback (most recent call last):; File ""/restricted/projectnb/ukbiobank/ad/analysis/ad.v1/ad_parent_linreg_all_races_one_over_60.py"", line 70, in <module>; mt=hl.read_matrix_table(mt_fn); File ""<decorator-gen-1219>"", line 2, in read_matrix_table; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 561, in wrapper; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/methods/impex.py"", line 1704, in read_matrix_table; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/matrixtable.py"", line 558, in __init__; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/ir/base_ir.py"", line 158, in typ; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/ir/matrix_ir.py"", line 40, in _compute_type; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/backend/backend.py"", line 104, in matrix_type; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/backend/backend.py"", line 87, in _to_java_ir; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/ir/base_ir.py"", line 163, in parse; File ""/share/pkg/spark/2.3.0/install/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py"", line 1160, in __call__; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 227, in deco; hail.utils.java.FatalError: MappingException: Parsed JSON values do not match with class constructor; args=; arg types=; constructor=public is.hail.variant.AbstractMatrixTableSpec(). Java stack trace:; org.json4s.package$MappingException: Parsed JSON values do not match with class con",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5744
https://github.com/hail-is/hail/issues/5744:5911,Availability,Error,Error,5911,ilder$$anonfun$result$6.apply(Extraction.scala:514); at org.json4s.Extraction$ClassInstanceBuilder$$anonfun$result$6.apply(Extraction.scala:512); at org.json4s.Extraction$.org$json4s$Extraction$$customOrElse(Extraction.scala:524); at org.json4s.Extraction$ClassInstanceBuilder.result(Extraction.scala:512); at org.json4s.Extraction$.extract(Extraction.scala:351); at org.json4s.Extraction$.extract(Extraction.scala:42); at org.json4s.ExtractableJsonAstNode.extract(ExtractableJsonAstNode.scala:21); at org.json4s.jackson.Serialization$.read(Serialization.scala:50); at is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:1094); at is.hail.expr.ir.IRParser$.matrix_ir(Parser.scala:1030); at is.hail.expr.ir.IRParser$$anonfun$parse_matrix_ir$2.apply(Parser.scala:1246); at is.hail.expr.ir.IRParser$$anonfun$parse_matrix_ir$2.apply(Parser.scala:1246); at is.hail.expr.ir.IRParser$.parse(Parser.scala:1230); at is.hail.expr.ir.IRParser$.parse_matrix_ir(Parser.scala:1246); at is.hail.expr.ir.IRParser$.parse_matrix_ir(Parser.scala:1245); at is.hail.expr.ir.IRParser.parse_matrix_ir(Parser.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:282); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:745). Hail version: 0.2.12-9409c0635781; Error summary: MappingException: Parsed JSON values do not match with class constructor; args=; arg types=; constructor=public is.hail.variant.AbstractMatrixTableSpec(). ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5744
https://github.com/hail-is/hail/issues/5744:444,Deployability,install,install,444,"Running on Apache Spark version 2.3.0; Hail version 0.2.12-9409c0635781. The follow error occurs when reading a matrix table. This code worked with Hail v2.8. ```; Traceback (most recent call last):; File ""/restricted/projectnb/ukbiobank/ad/analysis/ad.v1/ad_parent_linreg_all_races_one_over_60.py"", line 70, in <module>; mt=hl.read_matrix_table(mt_fn); File ""<decorator-gen-1219>"", line 2, in read_matrix_table; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 561, in wrapper; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/methods/impex.py"", line 1704, in read_matrix_table; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/matrixtable.py"", line 558, in __init__; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/ir/base_ir.py"", line 158, in typ; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/ir/matrix_ir.py"", line 40, in _compute_type; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/backend/backend.py"", line 104, in matrix_type; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/backend/backend.py"", line 87, in _to_java_ir; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/ir/base_ir.py"", line 163, in parse; File ""/share/pkg/spark/2.3.0/install/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py"", line 1160, in __call__; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 227, in deco; hail.utils.java.FatalError: MappingException: Parsed JSON values do not match with class constructor; args=; arg types=; constructor=public is.hail.variant.AbstractMatrixTableSpec(). Java stack trace:; org.json4s.package$MappingException: Parsed JSON values do not match with class con",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5744
https://github.com/hail-is/hail/issues/5744:577,Deployability,install,install,577,"Running on Apache Spark version 2.3.0; Hail version 0.2.12-9409c0635781. The follow error occurs when reading a matrix table. This code worked with Hail v2.8. ```; Traceback (most recent call last):; File ""/restricted/projectnb/ukbiobank/ad/analysis/ad.v1/ad_parent_linreg_all_races_one_over_60.py"", line 70, in <module>; mt=hl.read_matrix_table(mt_fn); File ""<decorator-gen-1219>"", line 2, in read_matrix_table; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 561, in wrapper; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/methods/impex.py"", line 1704, in read_matrix_table; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/matrixtable.py"", line 558, in __init__; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/ir/base_ir.py"", line 158, in typ; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/ir/matrix_ir.py"", line 40, in _compute_type; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/backend/backend.py"", line 104, in matrix_type; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/backend/backend.py"", line 87, in _to_java_ir; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/ir/base_ir.py"", line 163, in parse; File ""/share/pkg/spark/2.3.0/install/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py"", line 1160, in __call__; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 227, in deco; hail.utils.java.FatalError: MappingException: Parsed JSON values do not match with class constructor; args=; arg types=; constructor=public is.hail.variant.AbstractMatrixTableSpec(). Java stack trace:; org.json4s.package$MappingException: Parsed JSON values do not match with class con",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5744
https://github.com/hail-is/hail/issues/5744:719,Deployability,install,install,719,"Running on Apache Spark version 2.3.0; Hail version 0.2.12-9409c0635781. The follow error occurs when reading a matrix table. This code worked with Hail v2.8. ```; Traceback (most recent call last):; File ""/restricted/projectnb/ukbiobank/ad/analysis/ad.v1/ad_parent_linreg_all_races_one_over_60.py"", line 70, in <module>; mt=hl.read_matrix_table(mt_fn); File ""<decorator-gen-1219>"", line 2, in read_matrix_table; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 561, in wrapper; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/methods/impex.py"", line 1704, in read_matrix_table; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/matrixtable.py"", line 558, in __init__; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/ir/base_ir.py"", line 158, in typ; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/ir/matrix_ir.py"", line 40, in _compute_type; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/backend/backend.py"", line 104, in matrix_type; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/backend/backend.py"", line 87, in _to_java_ir; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/ir/base_ir.py"", line 163, in parse; File ""/share/pkg/spark/2.3.0/install/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py"", line 1160, in __call__; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 227, in deco; hail.utils.java.FatalError: MappingException: Parsed JSON values do not match with class constructor; args=; arg types=; constructor=public is.hail.variant.AbstractMatrixTableSpec(). Java stack trace:; org.json4s.package$MappingException: Parsed JSON values do not match with class con",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5744
https://github.com/hail-is/hail/issues/5744:849,Deployability,install,install,849,"Running on Apache Spark version 2.3.0; Hail version 0.2.12-9409c0635781. The follow error occurs when reading a matrix table. This code worked with Hail v2.8. ```; Traceback (most recent call last):; File ""/restricted/projectnb/ukbiobank/ad/analysis/ad.v1/ad_parent_linreg_all_races_one_over_60.py"", line 70, in <module>; mt=hl.read_matrix_table(mt_fn); File ""<decorator-gen-1219>"", line 2, in read_matrix_table; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 561, in wrapper; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/methods/impex.py"", line 1704, in read_matrix_table; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/matrixtable.py"", line 558, in __init__; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/ir/base_ir.py"", line 158, in typ; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/ir/matrix_ir.py"", line 40, in _compute_type; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/backend/backend.py"", line 104, in matrix_type; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/backend/backend.py"", line 87, in _to_java_ir; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/ir/base_ir.py"", line 163, in parse; File ""/share/pkg/spark/2.3.0/install/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py"", line 1160, in __call__; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 227, in deco; hail.utils.java.FatalError: MappingException: Parsed JSON values do not match with class constructor; args=; arg types=; constructor=public is.hail.variant.AbstractMatrixTableSpec(). Java stack trace:; org.json4s.package$MappingException: Parsed JSON values do not match with class con",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5744
https://github.com/hail-is/hail/issues/5744:973,Deployability,install,install,973,"Running on Apache Spark version 2.3.0; Hail version 0.2.12-9409c0635781. The follow error occurs when reading a matrix table. This code worked with Hail v2.8. ```; Traceback (most recent call last):; File ""/restricted/projectnb/ukbiobank/ad/analysis/ad.v1/ad_parent_linreg_all_races_one_over_60.py"", line 70, in <module>; mt=hl.read_matrix_table(mt_fn); File ""<decorator-gen-1219>"", line 2, in read_matrix_table; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 561, in wrapper; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/methods/impex.py"", line 1704, in read_matrix_table; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/matrixtable.py"", line 558, in __init__; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/ir/base_ir.py"", line 158, in typ; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/ir/matrix_ir.py"", line 40, in _compute_type; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/backend/backend.py"", line 104, in matrix_type; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/backend/backend.py"", line 87, in _to_java_ir; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/ir/base_ir.py"", line 163, in parse; File ""/share/pkg/spark/2.3.0/install/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py"", line 1160, in __call__; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 227, in deco; hail.utils.java.FatalError: MappingException: Parsed JSON values do not match with class constructor; args=; arg types=; constructor=public is.hail.variant.AbstractMatrixTableSpec(). Java stack trace:; org.json4s.package$MappingException: Parsed JSON values do not match with class con",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5744
https://github.com/hail-is/hail/issues/5744:1108,Deployability,install,install,1108,"code worked with Hail v2.8. ```; Traceback (most recent call last):; File ""/restricted/projectnb/ukbiobank/ad/analysis/ad.v1/ad_parent_linreg_all_races_one_over_60.py"", line 70, in <module>; mt=hl.read_matrix_table(mt_fn); File ""<decorator-gen-1219>"", line 2, in read_matrix_table; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 561, in wrapper; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/methods/impex.py"", line 1704, in read_matrix_table; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/matrixtable.py"", line 558, in __init__; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/ir/base_ir.py"", line 158, in typ; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/ir/matrix_ir.py"", line 40, in _compute_type; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/backend/backend.py"", line 104, in matrix_type; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/backend/backend.py"", line 87, in _to_java_ir; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/ir/base_ir.py"", line 163, in parse; File ""/share/pkg/spark/2.3.0/install/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py"", line 1160, in __call__; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 227, in deco; hail.utils.java.FatalError: MappingException: Parsed JSON values do not match with class constructor; args=; arg types=; constructor=public is.hail.variant.AbstractMatrixTableSpec(). Java stack trace:; org.json4s.package$MappingException: Parsed JSON values do not match with class constructor; args=; arg types=; constructor=public is.hail.variant.AbstractMatrixTableSpec(); at org.json4s.Extraction$ClassInstanceBu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5744
https://github.com/hail-is/hail/issues/5744:1245,Deployability,install,install,1245,"nreg_all_races_one_over_60.py"", line 70, in <module>; mt=hl.read_matrix_table(mt_fn); File ""<decorator-gen-1219>"", line 2, in read_matrix_table; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 561, in wrapper; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/methods/impex.py"", line 1704, in read_matrix_table; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/matrixtable.py"", line 558, in __init__; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/ir/base_ir.py"", line 158, in typ; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/ir/matrix_ir.py"", line 40, in _compute_type; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/backend/backend.py"", line 104, in matrix_type; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/backend/backend.py"", line 87, in _to_java_ir; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/ir/base_ir.py"", line 163, in parse; File ""/share/pkg/spark/2.3.0/install/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py"", line 1160, in __call__; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 227, in deco; hail.utils.java.FatalError: MappingException: Parsed JSON values do not match with class constructor; args=; arg types=; constructor=public is.hail.variant.AbstractMatrixTableSpec(). Java stack trace:; org.json4s.package$MappingException: Parsed JSON values do not match with class constructor; args=; arg types=; constructor=public is.hail.variant.AbstractMatrixTableSpec(); at org.json4s.Extraction$ClassInstanceBuilder.org$json4s$Extraction$ClassInstanceBuilder$$instantiate(Extraction.scala:495); at org.json4s.Extraction$ClassInstanceBuilder$$anonf",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5744
https://github.com/hail-is/hail/issues/5744:1381,Deployability,install,install,1381,"x_table; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 561, in wrapper; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/methods/impex.py"", line 1704, in read_matrix_table; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/matrixtable.py"", line 558, in __init__; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/ir/base_ir.py"", line 158, in typ; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/ir/matrix_ir.py"", line 40, in _compute_type; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/backend/backend.py"", line 104, in matrix_type; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/backend/backend.py"", line 87, in _to_java_ir; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/ir/base_ir.py"", line 163, in parse; File ""/share/pkg/spark/2.3.0/install/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py"", line 1160, in __call__; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 227, in deco; hail.utils.java.FatalError: MappingException: Parsed JSON values do not match with class constructor; args=; arg types=; constructor=public is.hail.variant.AbstractMatrixTableSpec(). Java stack trace:; org.json4s.package$MappingException: Parsed JSON values do not match with class constructor; args=; arg types=; constructor=public is.hail.variant.AbstractMatrixTableSpec(); at org.json4s.Extraction$ClassInstanceBuilder.org$json4s$Extraction$ClassInstanceBuilder$$instantiate(Extraction.scala:495); at org.json4s.Extraction$ClassInstanceBuilder$$anonfun$result$6.apply(Extraction.scala:515); at org.json4s.Extraction$ClassInstanceBuilder$$anonfun$result$6.apply(Extraction.scala:512); at",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5744
https://github.com/hail-is/hail/issues/5744:1505,Deployability,install,install,1505,"k.py"", line 561, in wrapper; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/methods/impex.py"", line 1704, in read_matrix_table; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/matrixtable.py"", line 558, in __init__; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/ir/base_ir.py"", line 158, in typ; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/ir/matrix_ir.py"", line 40, in _compute_type; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/backend/backend.py"", line 104, in matrix_type; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/backend/backend.py"", line 87, in _to_java_ir; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/ir/base_ir.py"", line 163, in parse; File ""/share/pkg/spark/2.3.0/install/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py"", line 1160, in __call__; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 227, in deco; hail.utils.java.FatalError: MappingException: Parsed JSON values do not match with class constructor; args=; arg types=; constructor=public is.hail.variant.AbstractMatrixTableSpec(). Java stack trace:; org.json4s.package$MappingException: Parsed JSON values do not match with class constructor; args=; arg types=; constructor=public is.hail.variant.AbstractMatrixTableSpec(); at org.json4s.Extraction$ClassInstanceBuilder.org$json4s$Extraction$ClassInstanceBuilder$$instantiate(Extraction.scala:495); at org.json4s.Extraction$ClassInstanceBuilder$$anonfun$result$6.apply(Extraction.scala:515); at org.json4s.Extraction$ClassInstanceBuilder$$anonfun$result$6.apply(Extraction.scala:512); at org.json4s.Extraction$.org$json4s$Extraction$$customOrElse(Extraction.scala:524); at org.json4s.Extraction$Clas",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5744
https://github.com/hail-is/hail/issues/5744:1622,Deployability,install,install,1622,"mpex.py"", line 1704, in read_matrix_table; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/matrixtable.py"", line 558, in __init__; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/ir/base_ir.py"", line 158, in typ; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/ir/matrix_ir.py"", line 40, in _compute_type; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/backend/backend.py"", line 104, in matrix_type; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/backend/backend.py"", line 87, in _to_java_ir; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/ir/base_ir.py"", line 163, in parse; File ""/share/pkg/spark/2.3.0/install/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py"", line 1160, in __call__; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 227, in deco; hail.utils.java.FatalError: MappingException: Parsed JSON values do not match with class constructor; args=; arg types=; constructor=public is.hail.variant.AbstractMatrixTableSpec(). Java stack trace:; org.json4s.package$MappingException: Parsed JSON values do not match with class constructor; args=; arg types=; constructor=public is.hail.variant.AbstractMatrixTableSpec(); at org.json4s.Extraction$ClassInstanceBuilder.org$json4s$Extraction$ClassInstanceBuilder$$instantiate(Extraction.scala:495); at org.json4s.Extraction$ClassInstanceBuilder$$anonfun$result$6.apply(Extraction.scala:515); at org.json4s.Extraction$ClassInstanceBuilder$$anonfun$result$6.apply(Extraction.scala:512); at org.json4s.Extraction$.org$json4s$Extraction$$customOrElse(Extraction.scala:524); at org.json4s.Extraction$ClassInstanceBuilder.result(Extraction.scala:512); at org.json4s.Extraction$.extract(Extraction.scala:351); at org.json4s.Extraction$",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5744
https://github.com/hail-is/hail/issues/5744:537,Integrability,wrap,wrapper,537,"Running on Apache Spark version 2.3.0; Hail version 0.2.12-9409c0635781. The follow error occurs when reading a matrix table. This code worked with Hail v2.8. ```; Traceback (most recent call last):; File ""/restricted/projectnb/ukbiobank/ad/analysis/ad.v1/ad_parent_linreg_all_races_one_over_60.py"", line 70, in <module>; mt=hl.read_matrix_table(mt_fn); File ""<decorator-gen-1219>"", line 2, in read_matrix_table; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 561, in wrapper; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/methods/impex.py"", line 1704, in read_matrix_table; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/matrixtable.py"", line 558, in __init__; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/ir/base_ir.py"", line 158, in typ; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/ir/matrix_ir.py"", line 40, in _compute_type; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/backend/backend.py"", line 104, in matrix_type; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/backend/backend.py"", line 87, in _to_java_ir; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/ir/base_ir.py"", line 163, in parse; File ""/share/pkg/spark/2.3.0/install/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py"", line 1160, in __call__; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 227, in deco; hail.utils.java.FatalError: MappingException: Parsed JSON values do not match with class constructor; args=; arg types=; constructor=public is.hail.variant.AbstractMatrixTableSpec(). Java stack trace:; org.json4s.package$MappingException: Parsed JSON values do not match with class con",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5744
https://github.com/hail-is/hail/pull/5746:251,Availability,error,error,251,"This fixes the notebook2 deployment permission issue that was resulting in CrashLoopBackoff (no permissions for the Table class to `read_namespaced_secret('get-users', 'default')`). Already tested, works (notebook2 back up). It also fixes an apparent error in the master branch RoleBinding. This diff looks slightly weird. I fixed the existing notebook Roles/RoleBindings by deleting the `create-services` `Role` and `notebook-create-services` `RoleBinding`, and then fixing the broken `notebook-create-servivces-and-pods` `RoleBiding`, by correctly updating the `roleRef` to read `create-services-and-pods`. When notebook1 totally goes away, we can probably remove the ""services"" permission. Before:; ```yaml; ---; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: create-services; rules:; - apiGroups: [""""]; resources: [""services""]; verbs: [""*""]; ---; kind: RoleBinding; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: notebook-create-services; subjects:; - kind: ServiceAccount; name: notebook; namespace: default; roleRef:; kind: Role; name: create-services; apiGroup: """"; ---; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: create-services-and-pods; rules:; - apiGroups: [""""]; resources: [""services""]; verbs: [""*""]; - apiGroups: [""""]; resources: [""pods""]; verbs: [""*""]; ---; kind: RoleBinding; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: notebook-create-services-and-pods; subjects:; - kind: ServiceAccount; name: notebook; namespace: default; roleRef:; kind: Role; name: create-services #this was causing the error, and of course the create-services role is superseded by the the create-services-and-pods role; apiGroup: """"; ---; ```. After:; ```yaml; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: create-services-and-pods; rules:; - apiGroups: [""""]; resources: [""services""]; verbs: [""*""]; - apiGrou",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5746
https://github.com/hail-is/hail/pull/5746:1666,Availability,error,error,1666,"ok1 totally goes away, we can probably remove the ""services"" permission. Before:; ```yaml; ---; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: create-services; rules:; - apiGroups: [""""]; resources: [""services""]; verbs: [""*""]; ---; kind: RoleBinding; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: notebook-create-services; subjects:; - kind: ServiceAccount; name: notebook; namespace: default; roleRef:; kind: Role; name: create-services; apiGroup: """"; ---; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: create-services-and-pods; rules:; - apiGroups: [""""]; resources: [""services""]; verbs: [""*""]; - apiGroups: [""""]; resources: [""pods""]; verbs: [""*""]; ---; kind: RoleBinding; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: notebook-create-services-and-pods; subjects:; - kind: ServiceAccount; name: notebook; namespace: default; roleRef:; kind: Role; name: create-services #this was causing the error, and of course the create-services role is superseded by the the create-services-and-pods role; apiGroup: """"; ---; ```. After:; ```yaml; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: create-services-and-pods; rules:; - apiGroups: [""""]; resources: [""services""]; verbs: [""*""]; - apiGroups: [""""]; resources: [""pods""]; verbs: [""*""]; ---; kind: RoleBinding; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: notebook-create-services-and-pods; subjects:; - kind: ServiceAccount; name: notebook; namespace: default; roleRef:; kind: Role; name: create-services-and-pods; apiGroup: """"; ---; ```. ### Results of test runs. Before:. ```sh; kubectl apply -f k8s-config.yaml; ERROR: (gcloud.compute.addresses.describe) Could not fetch resource:; - Required 'compute.addresses.get' permission for 'projects/hail-vdc-staging/regions/us-central1/addresses/site'. namespace/batch-pods unchang",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5746
https://github.com/hail-is/hail/pull/5746:2409,Availability,ERROR,ERROR,2409,"; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: notebook-create-services-and-pods; subjects:; - kind: ServiceAccount; name: notebook; namespace: default; roleRef:; kind: Role; name: create-services #this was causing the error, and of course the create-services role is superseded by the the create-services-and-pods role; apiGroup: """"; ---; ```. After:; ```yaml; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: create-services-and-pods; rules:; - apiGroups: [""""]; resources: [""services""]; verbs: [""*""]; - apiGroups: [""""]; resources: [""pods""]; verbs: [""*""]; ---; kind: RoleBinding; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: notebook-create-services-and-pods; subjects:; - kind: ServiceAccount; name: notebook; namespace: default; roleRef:; kind: Role; name: create-services-and-pods; apiGroup: """"; ---; ```. ### Results of test runs. Before:. ```sh; kubectl apply -f k8s-config.yaml; ERROR: (gcloud.compute.addresses.describe) Could not fetch resource:; - Required 'compute.addresses.get' permission for 'projects/hail-vdc-staging/regions/us-central1/addresses/site'. namespace/batch-pods unchanged; ...; The RoleBinding ""notebook-create-services-and-pods"" is invalid: roleRef: Invalid value: rbac.RoleRef{APIGroup:""rbac.authorization.k8s.io"", Kind:""Role"", Name:""create-services""}: cannot change roleRef; make: *** [k8s-config] Error 1; ```. After:; ```sh; ERROR: (gcloud.compute.addresses.describe) Could not fetch resource:; - Required 'compute.addresses.get' permission for 'projects/hail-vdc-staging/regions/us-central1/addresses/site'. ...; role.rbac.authorization.k8s.io/create-services-and-pods unchanged; rolebinding.rbac.authorization.k8s.io/notebook-create-services-and-pods configured; role.rbac.authorization.k8s.io/read-get-user-secret unchanged; rolebinding.rbac.authorization.k8s.io/notebook-read-get-users-secret configured; ```. I think the error just reflects my not havi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5746
https://github.com/hail-is/hail/pull/5746:2853,Availability,Error,Error,2853,"ebook-create-services-and-pods; subjects:; - kind: ServiceAccount; name: notebook; namespace: default; roleRef:; kind: Role; name: create-services #this was causing the error, and of course the create-services role is superseded by the the create-services-and-pods role; apiGroup: """"; ---; ```. After:; ```yaml; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: create-services-and-pods; rules:; - apiGroups: [""""]; resources: [""services""]; verbs: [""*""]; - apiGroups: [""""]; resources: [""pods""]; verbs: [""*""]; ---; kind: RoleBinding; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: notebook-create-services-and-pods; subjects:; - kind: ServiceAccount; name: notebook; namespace: default; roleRef:; kind: Role; name: create-services-and-pods; apiGroup: """"; ---; ```. ### Results of test runs. Before:. ```sh; kubectl apply -f k8s-config.yaml; ERROR: (gcloud.compute.addresses.describe) Could not fetch resource:; - Required 'compute.addresses.get' permission for 'projects/hail-vdc-staging/regions/us-central1/addresses/site'. namespace/batch-pods unchanged; ...; The RoleBinding ""notebook-create-services-and-pods"" is invalid: roleRef: Invalid value: rbac.RoleRef{APIGroup:""rbac.authorization.k8s.io"", Kind:""Role"", Name:""create-services""}: cannot change roleRef; make: *** [k8s-config] Error 1; ```. After:; ```sh; ERROR: (gcloud.compute.addresses.describe) Could not fetch resource:; - Required 'compute.addresses.get' permission for 'projects/hail-vdc-staging/regions/us-central1/addresses/site'. ...; role.rbac.authorization.k8s.io/create-services-and-pods unchanged; rolebinding.rbac.authorization.k8s.io/notebook-create-services-and-pods configured; role.rbac.authorization.k8s.io/read-get-user-secret unchanged; rolebinding.rbac.authorization.k8s.io/notebook-read-get-users-secret configured; ```. I think the error just reflects my not having hail-vdc-staging permissions, that is unaffected by this PR. cc @cseed, @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5746
https://github.com/hail-is/hail/pull/5746:2882,Availability,ERROR,ERROR,2882,"ebook-create-services-and-pods; subjects:; - kind: ServiceAccount; name: notebook; namespace: default; roleRef:; kind: Role; name: create-services #this was causing the error, and of course the create-services role is superseded by the the create-services-and-pods role; apiGroup: """"; ---; ```. After:; ```yaml; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: create-services-and-pods; rules:; - apiGroups: [""""]; resources: [""services""]; verbs: [""*""]; - apiGroups: [""""]; resources: [""pods""]; verbs: [""*""]; ---; kind: RoleBinding; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: notebook-create-services-and-pods; subjects:; - kind: ServiceAccount; name: notebook; namespace: default; roleRef:; kind: Role; name: create-services-and-pods; apiGroup: """"; ---; ```. ### Results of test runs. Before:. ```sh; kubectl apply -f k8s-config.yaml; ERROR: (gcloud.compute.addresses.describe) Could not fetch resource:; - Required 'compute.addresses.get' permission for 'projects/hail-vdc-staging/regions/us-central1/addresses/site'. namespace/batch-pods unchanged; ...; The RoleBinding ""notebook-create-services-and-pods"" is invalid: roleRef: Invalid value: rbac.RoleRef{APIGroup:""rbac.authorization.k8s.io"", Kind:""Role"", Name:""create-services""}: cannot change roleRef; make: *** [k8s-config] Error 1; ```. After:; ```sh; ERROR: (gcloud.compute.addresses.describe) Could not fetch resource:; - Required 'compute.addresses.get' permission for 'projects/hail-vdc-staging/regions/us-central1/addresses/site'. ...; role.rbac.authorization.k8s.io/create-services-and-pods unchanged; rolebinding.rbac.authorization.k8s.io/notebook-create-services-and-pods configured; role.rbac.authorization.k8s.io/read-get-user-secret unchanged; rolebinding.rbac.authorization.k8s.io/notebook-read-get-users-secret configured; ```. I think the error just reflects my not having hail-vdc-staging permissions, that is unaffected by this PR. cc @cseed, @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5746
https://github.com/hail-is/hail/pull/5746:3383,Availability,error,error,3383,"ebook-create-services-and-pods; subjects:; - kind: ServiceAccount; name: notebook; namespace: default; roleRef:; kind: Role; name: create-services #this was causing the error, and of course the create-services role is superseded by the the create-services-and-pods role; apiGroup: """"; ---; ```. After:; ```yaml; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: create-services-and-pods; rules:; - apiGroups: [""""]; resources: [""services""]; verbs: [""*""]; - apiGroups: [""""]; resources: [""pods""]; verbs: [""*""]; ---; kind: RoleBinding; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: notebook-create-services-and-pods; subjects:; - kind: ServiceAccount; name: notebook; namespace: default; roleRef:; kind: Role; name: create-services-and-pods; apiGroup: """"; ---; ```. ### Results of test runs. Before:. ```sh; kubectl apply -f k8s-config.yaml; ERROR: (gcloud.compute.addresses.describe) Could not fetch resource:; - Required 'compute.addresses.get' permission for 'projects/hail-vdc-staging/regions/us-central1/addresses/site'. namespace/batch-pods unchanged; ...; The RoleBinding ""notebook-create-services-and-pods"" is invalid: roleRef: Invalid value: rbac.RoleRef{APIGroup:""rbac.authorization.k8s.io"", Kind:""Role"", Name:""create-services""}: cannot change roleRef; make: *** [k8s-config] Error 1; ```. After:; ```sh; ERROR: (gcloud.compute.addresses.describe) Could not fetch resource:; - Required 'compute.addresses.get' permission for 'projects/hail-vdc-staging/regions/us-central1/addresses/site'. ...; role.rbac.authorization.k8s.io/create-services-and-pods unchanged; rolebinding.rbac.authorization.k8s.io/notebook-create-services-and-pods configured; role.rbac.authorization.k8s.io/read-get-user-secret unchanged; rolebinding.rbac.authorization.k8s.io/notebook-read-get-users-secret configured; ```. I think the error just reflects my not having hail-vdc-staging permissions, that is unaffected by this PR. cc @cseed, @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5746
https://github.com/hail-is/hail/pull/5746:25,Deployability,deploy,deployment,25,"This fixes the notebook2 deployment permission issue that was resulting in CrashLoopBackoff (no permissions for the Table class to `read_namespaced_secret('get-users', 'default')`). Already tested, works (notebook2 back up). It also fixes an apparent error in the master branch RoleBinding. This diff looks slightly weird. I fixed the existing notebook Roles/RoleBindings by deleting the `create-services` `Role` and `notebook-create-services` `RoleBinding`, and then fixing the broken `notebook-create-servivces-and-pods` `RoleBiding`, by correctly updating the `roleRef` to read `create-services-and-pods`. When notebook1 totally goes away, we can probably remove the ""services"" permission. Before:; ```yaml; ---; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: create-services; rules:; - apiGroups: [""""]; resources: [""services""]; verbs: [""*""]; ---; kind: RoleBinding; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: notebook-create-services; subjects:; - kind: ServiceAccount; name: notebook; namespace: default; roleRef:; kind: Role; name: create-services; apiGroup: """"; ---; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: create-services-and-pods; rules:; - apiGroups: [""""]; resources: [""services""]; verbs: [""*""]; - apiGroups: [""""]; resources: [""pods""]; verbs: [""*""]; ---; kind: RoleBinding; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: notebook-create-services-and-pods; subjects:; - kind: ServiceAccount; name: notebook; namespace: default; roleRef:; kind: Role; name: create-services #this was causing the error, and of course the create-services role is superseded by the the create-services-and-pods role; apiGroup: """"; ---; ```. After:; ```yaml; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: create-services-and-pods; rules:; - apiGroups: [""""]; resources: [""services""]; verbs: [""*""]; - apiGrou",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5746
https://github.com/hail-is/hail/pull/5746:2396,Modifiability,config,config,2396,"""*""]; ---; kind: RoleBinding; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: notebook-create-services-and-pods; subjects:; - kind: ServiceAccount; name: notebook; namespace: default; roleRef:; kind: Role; name: create-services #this was causing the error, and of course the create-services role is superseded by the the create-services-and-pods role; apiGroup: """"; ---; ```. After:; ```yaml; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: create-services-and-pods; rules:; - apiGroups: [""""]; resources: [""services""]; verbs: [""*""]; - apiGroups: [""""]; resources: [""pods""]; verbs: [""*""]; ---; kind: RoleBinding; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: notebook-create-services-and-pods; subjects:; - kind: ServiceAccount; name: notebook; namespace: default; roleRef:; kind: Role; name: create-services-and-pods; apiGroup: """"; ---; ```. ### Results of test runs. Before:. ```sh; kubectl apply -f k8s-config.yaml; ERROR: (gcloud.compute.addresses.describe) Could not fetch resource:; - Required 'compute.addresses.get' permission for 'projects/hail-vdc-staging/regions/us-central1/addresses/site'. namespace/batch-pods unchanged; ...; The RoleBinding ""notebook-create-services-and-pods"" is invalid: roleRef: Invalid value: rbac.RoleRef{APIGroup:""rbac.authorization.k8s.io"", Kind:""Role"", Name:""create-services""}: cannot change roleRef; make: *** [k8s-config] Error 1; ```. After:; ```sh; ERROR: (gcloud.compute.addresses.describe) Could not fetch resource:; - Required 'compute.addresses.get' permission for 'projects/hail-vdc-staging/regions/us-central1/addresses/site'. ...; role.rbac.authorization.k8s.io/create-services-and-pods unchanged; rolebinding.rbac.authorization.k8s.io/notebook-create-services-and-pods configured; role.rbac.authorization.k8s.io/read-get-user-secret unchanged; rolebinding.rbac.authorization.k8s.io/notebook-read-get-users-secret configured; ```. I think the er",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5746
https://github.com/hail-is/hail/pull/5746:2845,Modifiability,config,config,2845,"ebook-create-services-and-pods; subjects:; - kind: ServiceAccount; name: notebook; namespace: default; roleRef:; kind: Role; name: create-services #this was causing the error, and of course the create-services role is superseded by the the create-services-and-pods role; apiGroup: """"; ---; ```. After:; ```yaml; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: create-services-and-pods; rules:; - apiGroups: [""""]; resources: [""services""]; verbs: [""*""]; - apiGroups: [""""]; resources: [""pods""]; verbs: [""*""]; ---; kind: RoleBinding; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: notebook-create-services-and-pods; subjects:; - kind: ServiceAccount; name: notebook; namespace: default; roleRef:; kind: Role; name: create-services-and-pods; apiGroup: """"; ---; ```. ### Results of test runs. Before:. ```sh; kubectl apply -f k8s-config.yaml; ERROR: (gcloud.compute.addresses.describe) Could not fetch resource:; - Required 'compute.addresses.get' permission for 'projects/hail-vdc-staging/regions/us-central1/addresses/site'. namespace/batch-pods unchanged; ...; The RoleBinding ""notebook-create-services-and-pods"" is invalid: roleRef: Invalid value: rbac.RoleRef{APIGroup:""rbac.authorization.k8s.io"", Kind:""Role"", Name:""create-services""}: cannot change roleRef; make: *** [k8s-config] Error 1; ```. After:; ```sh; ERROR: (gcloud.compute.addresses.describe) Could not fetch resource:; - Required 'compute.addresses.get' permission for 'projects/hail-vdc-staging/regions/us-central1/addresses/site'. ...; role.rbac.authorization.k8s.io/create-services-and-pods unchanged; rolebinding.rbac.authorization.k8s.io/notebook-create-services-and-pods configured; role.rbac.authorization.k8s.io/read-get-user-secret unchanged; rolebinding.rbac.authorization.k8s.io/notebook-read-get-users-secret configured; ```. I think the error just reflects my not having hail-vdc-staging permissions, that is unaffected by this PR. cc @cseed, @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5746
https://github.com/hail-is/hail/pull/5746:3210,Modifiability,config,configured,3210,"ebook-create-services-and-pods; subjects:; - kind: ServiceAccount; name: notebook; namespace: default; roleRef:; kind: Role; name: create-services #this was causing the error, and of course the create-services role is superseded by the the create-services-and-pods role; apiGroup: """"; ---; ```. After:; ```yaml; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: create-services-and-pods; rules:; - apiGroups: [""""]; resources: [""services""]; verbs: [""*""]; - apiGroups: [""""]; resources: [""pods""]; verbs: [""*""]; ---; kind: RoleBinding; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: notebook-create-services-and-pods; subjects:; - kind: ServiceAccount; name: notebook; namespace: default; roleRef:; kind: Role; name: create-services-and-pods; apiGroup: """"; ---; ```. ### Results of test runs. Before:. ```sh; kubectl apply -f k8s-config.yaml; ERROR: (gcloud.compute.addresses.describe) Could not fetch resource:; - Required 'compute.addresses.get' permission for 'projects/hail-vdc-staging/regions/us-central1/addresses/site'. namespace/batch-pods unchanged; ...; The RoleBinding ""notebook-create-services-and-pods"" is invalid: roleRef: Invalid value: rbac.RoleRef{APIGroup:""rbac.authorization.k8s.io"", Kind:""Role"", Name:""create-services""}: cannot change roleRef; make: *** [k8s-config] Error 1; ```. After:; ```sh; ERROR: (gcloud.compute.addresses.describe) Could not fetch resource:; - Required 'compute.addresses.get' permission for 'projects/hail-vdc-staging/regions/us-central1/addresses/site'. ...; role.rbac.authorization.k8s.io/create-services-and-pods unchanged; rolebinding.rbac.authorization.k8s.io/notebook-create-services-and-pods configured; role.rbac.authorization.k8s.io/read-get-user-secret unchanged; rolebinding.rbac.authorization.k8s.io/notebook-read-get-users-secret configured; ```. I think the error just reflects my not having hail-vdc-staging permissions, that is unaffected by this PR. cc @cseed, @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5746
https://github.com/hail-is/hail/pull/5746:3354,Modifiability,config,configured,3354,"ebook-create-services-and-pods; subjects:; - kind: ServiceAccount; name: notebook; namespace: default; roleRef:; kind: Role; name: create-services #this was causing the error, and of course the create-services role is superseded by the the create-services-and-pods role; apiGroup: """"; ---; ```. After:; ```yaml; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: create-services-and-pods; rules:; - apiGroups: [""""]; resources: [""services""]; verbs: [""*""]; - apiGroups: [""""]; resources: [""pods""]; verbs: [""*""]; ---; kind: RoleBinding; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: notebook-create-services-and-pods; subjects:; - kind: ServiceAccount; name: notebook; namespace: default; roleRef:; kind: Role; name: create-services-and-pods; apiGroup: """"; ---; ```. ### Results of test runs. Before:. ```sh; kubectl apply -f k8s-config.yaml; ERROR: (gcloud.compute.addresses.describe) Could not fetch resource:; - Required 'compute.addresses.get' permission for 'projects/hail-vdc-staging/regions/us-central1/addresses/site'. namespace/batch-pods unchanged; ...; The RoleBinding ""notebook-create-services-and-pods"" is invalid: roleRef: Invalid value: rbac.RoleRef{APIGroup:""rbac.authorization.k8s.io"", Kind:""Role"", Name:""create-services""}: cannot change roleRef; make: *** [k8s-config] Error 1; ```. After:; ```sh; ERROR: (gcloud.compute.addresses.describe) Could not fetch resource:; - Required 'compute.addresses.get' permission for 'projects/hail-vdc-staging/regions/us-central1/addresses/site'. ...; role.rbac.authorization.k8s.io/create-services-and-pods unchanged; rolebinding.rbac.authorization.k8s.io/notebook-create-services-and-pods configured; role.rbac.authorization.k8s.io/read-get-user-secret unchanged; rolebinding.rbac.authorization.k8s.io/notebook-read-get-users-secret configured; ```. I think the error just reflects my not having hail-vdc-staging permissions, that is unaffected by this PR. cc @cseed, @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5746
https://github.com/hail-is/hail/pull/5746:745,Security,authoriz,authorization,745,"This fixes the notebook2 deployment permission issue that was resulting in CrashLoopBackoff (no permissions for the Table class to `read_namespaced_secret('get-users', 'default')`). Already tested, works (notebook2 back up). It also fixes an apparent error in the master branch RoleBinding. This diff looks slightly weird. I fixed the existing notebook Roles/RoleBindings by deleting the `create-services` `Role` and `notebook-create-services` `RoleBinding`, and then fixing the broken `notebook-create-servivces-and-pods` `RoleBiding`, by correctly updating the `roleRef` to read `create-services-and-pods`. When notebook1 totally goes away, we can probably remove the ""services"" permission. Before:; ```yaml; ---; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: create-services; rules:; - apiGroups: [""""]; resources: [""services""]; verbs: [""*""]; ---; kind: RoleBinding; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: notebook-create-services; subjects:; - kind: ServiceAccount; name: notebook; namespace: default; roleRef:; kind: Role; name: create-services; apiGroup: """"; ---; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: create-services-and-pods; rules:; - apiGroups: [""""]; resources: [""services""]; verbs: [""*""]; - apiGroups: [""""]; resources: [""pods""]; verbs: [""*""]; ---; kind: RoleBinding; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: notebook-create-services-and-pods; subjects:; - kind: ServiceAccount; name: notebook; namespace: default; roleRef:; kind: Role; name: create-services #this was causing the error, and of course the create-services role is superseded by the the create-services-and-pods role; apiGroup: """"; ---; ```. After:; ```yaml; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: create-services-and-pods; rules:; - apiGroups: [""""]; resources: [""services""]; verbs: [""*""]; - apiGrou",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5746
https://github.com/hail-is/hail/pull/5746:931,Security,authoriz,authorization,931,"This fixes the notebook2 deployment permission issue that was resulting in CrashLoopBackoff (no permissions for the Table class to `read_namespaced_secret('get-users', 'default')`). Already tested, works (notebook2 back up). It also fixes an apparent error in the master branch RoleBinding. This diff looks slightly weird. I fixed the existing notebook Roles/RoleBindings by deleting the `create-services` `Role` and `notebook-create-services` `RoleBinding`, and then fixing the broken `notebook-create-servivces-and-pods` `RoleBiding`, by correctly updating the `roleRef` to read `create-services-and-pods`. When notebook1 totally goes away, we can probably remove the ""services"" permission. Before:; ```yaml; ---; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: create-services; rules:; - apiGroups: [""""]; resources: [""services""]; verbs: [""*""]; ---; kind: RoleBinding; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: notebook-create-services; subjects:; - kind: ServiceAccount; name: notebook; namespace: default; roleRef:; kind: Role; name: create-services; apiGroup: """"; ---; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: create-services-and-pods; rules:; - apiGroups: [""""]; resources: [""services""]; verbs: [""*""]; - apiGroups: [""""]; resources: [""pods""]; verbs: [""*""]; ---; kind: RoleBinding; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: notebook-create-services-and-pods; subjects:; - kind: ServiceAccount; name: notebook; namespace: default; roleRef:; kind: Role; name: create-services #this was causing the error, and of course the create-services role is superseded by the the create-services-and-pods role; apiGroup: """"; ---; ```. After:; ```yaml; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: create-services-and-pods; rules:; - apiGroups: [""""]; resources: [""services""]; verbs: [""*""]; - apiGrou",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5746
https://github.com/hail-is/hail/pull/5746:1183,Security,authoriz,authorization,1183,"tested, works (notebook2 back up). It also fixes an apparent error in the master branch RoleBinding. This diff looks slightly weird. I fixed the existing notebook Roles/RoleBindings by deleting the `create-services` `Role` and `notebook-create-services` `RoleBinding`, and then fixing the broken `notebook-create-servivces-and-pods` `RoleBiding`, by correctly updating the `roleRef` to read `create-services-and-pods`. When notebook1 totally goes away, we can probably remove the ""services"" permission. Before:; ```yaml; ---; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: create-services; rules:; - apiGroups: [""""]; resources: [""services""]; verbs: [""*""]; ---; kind: RoleBinding; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: notebook-create-services; subjects:; - kind: ServiceAccount; name: notebook; namespace: default; roleRef:; kind: Role; name: create-services; apiGroup: """"; ---; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: create-services-and-pods; rules:; - apiGroups: [""""]; resources: [""services""]; verbs: [""*""]; - apiGroups: [""""]; resources: [""pods""]; verbs: [""*""]; ---; kind: RoleBinding; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: notebook-create-services-and-pods; subjects:; - kind: ServiceAccount; name: notebook; namespace: default; roleRef:; kind: Role; name: create-services #this was causing the error, and of course the create-services role is superseded by the the create-services-and-pods role; apiGroup: """"; ---; ```. After:; ```yaml; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: create-services-and-pods; rules:; - apiGroups: [""""]; resources: [""services""]; verbs: [""*""]; - apiGroups: [""""]; resources: [""pods""]; verbs: [""*""]; ---; kind: RoleBinding; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: notebook-create-services-and-pods; subjec",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5746
https://github.com/hail-is/hail/pull/5746:1432,Security,authoriz,authorization,1432,"ces` `RoleBinding`, and then fixing the broken `notebook-create-servivces-and-pods` `RoleBiding`, by correctly updating the `roleRef` to read `create-services-and-pods`. When notebook1 totally goes away, we can probably remove the ""services"" permission. Before:; ```yaml; ---; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: create-services; rules:; - apiGroups: [""""]; resources: [""services""]; verbs: [""*""]; ---; kind: RoleBinding; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: notebook-create-services; subjects:; - kind: ServiceAccount; name: notebook; namespace: default; roleRef:; kind: Role; name: create-services; apiGroup: """"; ---; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: create-services-and-pods; rules:; - apiGroups: [""""]; resources: [""services""]; verbs: [""*""]; - apiGroups: [""""]; resources: [""pods""]; verbs: [""*""]; ---; kind: RoleBinding; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: notebook-create-services-and-pods; subjects:; - kind: ServiceAccount; name: notebook; namespace: default; roleRef:; kind: Role; name: create-services #this was causing the error, and of course the create-services role is superseded by the the create-services-and-pods role; apiGroup: """"; ---; ```. After:; ```yaml; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: create-services-and-pods; rules:; - apiGroups: [""""]; resources: [""services""]; verbs: [""*""]; - apiGroups: [""""]; resources: [""pods""]; verbs: [""*""]; ---; kind: RoleBinding; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: notebook-create-services-and-pods; subjects:; - kind: ServiceAccount; name: notebook; namespace: default; roleRef:; kind: Role; name: create-services-and-pods; apiGroup: """"; ---; ```. ### Results of test runs. Before:. ```sh; kubectl apply -f k8s-config.yaml; ERROR: (gcloud.compute.address",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5746
https://github.com/hail-is/hail/pull/5746:1838,Security,authoriz,authorization,1838,"[""""]; resources: [""services""]; verbs: [""*""]; ---; kind: RoleBinding; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: notebook-create-services; subjects:; - kind: ServiceAccount; name: notebook; namespace: default; roleRef:; kind: Role; name: create-services; apiGroup: """"; ---; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: create-services-and-pods; rules:; - apiGroups: [""""]; resources: [""services""]; verbs: [""*""]; - apiGroups: [""""]; resources: [""pods""]; verbs: [""*""]; ---; kind: RoleBinding; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: notebook-create-services-and-pods; subjects:; - kind: ServiceAccount; name: notebook; namespace: default; roleRef:; kind: Role; name: create-services #this was causing the error, and of course the create-services role is superseded by the the create-services-and-pods role; apiGroup: """"; ---; ```. After:; ```yaml; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: create-services-and-pods; rules:; - apiGroups: [""""]; resources: [""services""]; verbs: [""*""]; - apiGroups: [""""]; resources: [""pods""]; verbs: [""*""]; ---; kind: RoleBinding; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: notebook-create-services-and-pods; subjects:; - kind: ServiceAccount; name: notebook; namespace: default; roleRef:; kind: Role; name: create-services-and-pods; apiGroup: """"; ---; ```. ### Results of test runs. Before:. ```sh; kubectl apply -f k8s-config.yaml; ERROR: (gcloud.compute.addresses.describe) Could not fetch resource:; - Required 'compute.addresses.get' permission for 'projects/hail-vdc-staging/regions/us-central1/addresses/site'. namespace/batch-pods unchanged; ...; The RoleBinding ""notebook-create-services-and-pods"" is invalid: roleRef: Invalid value: rbac.RoleRef{APIGroup:""rbac.authorization.k8s.io"", Kind:""Role"", Name:""create-services""}: cannot change roleRef; make: *** [k8s-",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5746
https://github.com/hail-is/hail/pull/5746:2087,Security,authoriz,authorization,2087,"Ref:; kind: Role; name: create-services; apiGroup: """"; ---; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: create-services-and-pods; rules:; - apiGroups: [""""]; resources: [""services""]; verbs: [""*""]; - apiGroups: [""""]; resources: [""pods""]; verbs: [""*""]; ---; kind: RoleBinding; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: notebook-create-services-and-pods; subjects:; - kind: ServiceAccount; name: notebook; namespace: default; roleRef:; kind: Role; name: create-services #this was causing the error, and of course the create-services role is superseded by the the create-services-and-pods role; apiGroup: """"; ---; ```. After:; ```yaml; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: create-services-and-pods; rules:; - apiGroups: [""""]; resources: [""services""]; verbs: [""*""]; - apiGroups: [""""]; resources: [""pods""]; verbs: [""*""]; ---; kind: RoleBinding; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: notebook-create-services-and-pods; subjects:; - kind: ServiceAccount; name: notebook; namespace: default; roleRef:; kind: Role; name: create-services-and-pods; apiGroup: """"; ---; ```. ### Results of test runs. Before:. ```sh; kubectl apply -f k8s-config.yaml; ERROR: (gcloud.compute.addresses.describe) Could not fetch resource:; - Required 'compute.addresses.get' permission for 'projects/hail-vdc-staging/regions/us-central1/addresses/site'. namespace/batch-pods unchanged; ...; The RoleBinding ""notebook-create-services-and-pods"" is invalid: roleRef: Invalid value: rbac.RoleRef{APIGroup:""rbac.authorization.k8s.io"", Kind:""Role"", Name:""create-services""}: cannot change roleRef; make: *** [k8s-config] Error 1; ```. After:; ```sh; ERROR: (gcloud.compute.addresses.describe) Could not fetch resource:; - Required 'compute.addresses.get' permission for 'projects/hail-vdc-staging/regions/us-central1/addresses/site'. ...; role.rbac.authorization",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5746
https://github.com/hail-is/hail/pull/5746:2746,Security,authoriz,authorization,2746,"ebook-create-services-and-pods; subjects:; - kind: ServiceAccount; name: notebook; namespace: default; roleRef:; kind: Role; name: create-services #this was causing the error, and of course the create-services role is superseded by the the create-services-and-pods role; apiGroup: """"; ---; ```. After:; ```yaml; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: create-services-and-pods; rules:; - apiGroups: [""""]; resources: [""services""]; verbs: [""*""]; - apiGroups: [""""]; resources: [""pods""]; verbs: [""*""]; ---; kind: RoleBinding; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: notebook-create-services-and-pods; subjects:; - kind: ServiceAccount; name: notebook; namespace: default; roleRef:; kind: Role; name: create-services-and-pods; apiGroup: """"; ---; ```. ### Results of test runs. Before:. ```sh; kubectl apply -f k8s-config.yaml; ERROR: (gcloud.compute.addresses.describe) Could not fetch resource:; - Required 'compute.addresses.get' permission for 'projects/hail-vdc-staging/regions/us-central1/addresses/site'. namespace/batch-pods unchanged; ...; The RoleBinding ""notebook-create-services-and-pods"" is invalid: roleRef: Invalid value: rbac.RoleRef{APIGroup:""rbac.authorization.k8s.io"", Kind:""Role"", Name:""create-services""}: cannot change roleRef; make: *** [k8s-config] Error 1; ```. After:; ```sh; ERROR: (gcloud.compute.addresses.describe) Could not fetch resource:; - Required 'compute.addresses.get' permission for 'projects/hail-vdc-staging/regions/us-central1/addresses/site'. ...; role.rbac.authorization.k8s.io/create-services-and-pods unchanged; rolebinding.rbac.authorization.k8s.io/notebook-create-services-and-pods configured; role.rbac.authorization.k8s.io/read-get-user-secret unchanged; rolebinding.rbac.authorization.k8s.io/notebook-read-get-users-secret configured; ```. I think the error just reflects my not having hail-vdc-staging permissions, that is unaffected by this PR. cc @cseed, @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5746
https://github.com/hail-is/hail/pull/5746:3081,Security,authoriz,authorization,3081,"ebook-create-services-and-pods; subjects:; - kind: ServiceAccount; name: notebook; namespace: default; roleRef:; kind: Role; name: create-services #this was causing the error, and of course the create-services role is superseded by the the create-services-and-pods role; apiGroup: """"; ---; ```. After:; ```yaml; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: create-services-and-pods; rules:; - apiGroups: [""""]; resources: [""services""]; verbs: [""*""]; - apiGroups: [""""]; resources: [""pods""]; verbs: [""*""]; ---; kind: RoleBinding; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: notebook-create-services-and-pods; subjects:; - kind: ServiceAccount; name: notebook; namespace: default; roleRef:; kind: Role; name: create-services-and-pods; apiGroup: """"; ---; ```. ### Results of test runs. Before:. ```sh; kubectl apply -f k8s-config.yaml; ERROR: (gcloud.compute.addresses.describe) Could not fetch resource:; - Required 'compute.addresses.get' permission for 'projects/hail-vdc-staging/regions/us-central1/addresses/site'. namespace/batch-pods unchanged; ...; The RoleBinding ""notebook-create-services-and-pods"" is invalid: roleRef: Invalid value: rbac.RoleRef{APIGroup:""rbac.authorization.k8s.io"", Kind:""Role"", Name:""create-services""}: cannot change roleRef; make: *** [k8s-config] Error 1; ```. After:; ```sh; ERROR: (gcloud.compute.addresses.describe) Could not fetch resource:; - Required 'compute.addresses.get' permission for 'projects/hail-vdc-staging/regions/us-central1/addresses/site'. ...; role.rbac.authorization.k8s.io/create-services-and-pods unchanged; rolebinding.rbac.authorization.k8s.io/notebook-create-services-and-pods configured; role.rbac.authorization.k8s.io/read-get-user-secret unchanged; rolebinding.rbac.authorization.k8s.io/notebook-read-get-users-secret configured; ```. I think the error just reflects my not having hail-vdc-staging permissions, that is unaffected by this PR. cc @cseed, @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5746
https://github.com/hail-is/hail/pull/5746:3155,Security,authoriz,authorization,3155,"ebook-create-services-and-pods; subjects:; - kind: ServiceAccount; name: notebook; namespace: default; roleRef:; kind: Role; name: create-services #this was causing the error, and of course the create-services role is superseded by the the create-services-and-pods role; apiGroup: """"; ---; ```. After:; ```yaml; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: create-services-and-pods; rules:; - apiGroups: [""""]; resources: [""services""]; verbs: [""*""]; - apiGroups: [""""]; resources: [""pods""]; verbs: [""*""]; ---; kind: RoleBinding; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: notebook-create-services-and-pods; subjects:; - kind: ServiceAccount; name: notebook; namespace: default; roleRef:; kind: Role; name: create-services-and-pods; apiGroup: """"; ---; ```. ### Results of test runs. Before:. ```sh; kubectl apply -f k8s-config.yaml; ERROR: (gcloud.compute.addresses.describe) Could not fetch resource:; - Required 'compute.addresses.get' permission for 'projects/hail-vdc-staging/regions/us-central1/addresses/site'. namespace/batch-pods unchanged; ...; The RoleBinding ""notebook-create-services-and-pods"" is invalid: roleRef: Invalid value: rbac.RoleRef{APIGroup:""rbac.authorization.k8s.io"", Kind:""Role"", Name:""create-services""}: cannot change roleRef; make: *** [k8s-config] Error 1; ```. After:; ```sh; ERROR: (gcloud.compute.addresses.describe) Could not fetch resource:; - Required 'compute.addresses.get' permission for 'projects/hail-vdc-staging/regions/us-central1/addresses/site'. ...; role.rbac.authorization.k8s.io/create-services-and-pods unchanged; rolebinding.rbac.authorization.k8s.io/notebook-create-services-and-pods configured; role.rbac.authorization.k8s.io/read-get-user-secret unchanged; rolebinding.rbac.authorization.k8s.io/notebook-read-get-users-secret configured; ```. I think the error just reflects my not having hail-vdc-staging permissions, that is unaffected by this PR. cc @cseed, @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5746
https://github.com/hail-is/hail/pull/5746:3232,Security,authoriz,authorization,3232,"ebook-create-services-and-pods; subjects:; - kind: ServiceAccount; name: notebook; namespace: default; roleRef:; kind: Role; name: create-services #this was causing the error, and of course the create-services role is superseded by the the create-services-and-pods role; apiGroup: """"; ---; ```. After:; ```yaml; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: create-services-and-pods; rules:; - apiGroups: [""""]; resources: [""services""]; verbs: [""*""]; - apiGroups: [""""]; resources: [""pods""]; verbs: [""*""]; ---; kind: RoleBinding; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: notebook-create-services-and-pods; subjects:; - kind: ServiceAccount; name: notebook; namespace: default; roleRef:; kind: Role; name: create-services-and-pods; apiGroup: """"; ---; ```. ### Results of test runs. Before:. ```sh; kubectl apply -f k8s-config.yaml; ERROR: (gcloud.compute.addresses.describe) Could not fetch resource:; - Required 'compute.addresses.get' permission for 'projects/hail-vdc-staging/regions/us-central1/addresses/site'. namespace/batch-pods unchanged; ...; The RoleBinding ""notebook-create-services-and-pods"" is invalid: roleRef: Invalid value: rbac.RoleRef{APIGroup:""rbac.authorization.k8s.io"", Kind:""Role"", Name:""create-services""}: cannot change roleRef; make: *** [k8s-config] Error 1; ```. After:; ```sh; ERROR: (gcloud.compute.addresses.describe) Could not fetch resource:; - Required 'compute.addresses.get' permission for 'projects/hail-vdc-staging/regions/us-central1/addresses/site'. ...; role.rbac.authorization.k8s.io/create-services-and-pods unchanged; rolebinding.rbac.authorization.k8s.io/notebook-create-services-and-pods configured; role.rbac.authorization.k8s.io/read-get-user-secret unchanged; rolebinding.rbac.authorization.k8s.io/notebook-read-get-users-secret configured; ```. I think the error just reflects my not having hail-vdc-staging permissions, that is unaffected by this PR. cc @cseed, @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5746
https://github.com/hail-is/hail/pull/5746:3302,Security,authoriz,authorization,3302,"ebook-create-services-and-pods; subjects:; - kind: ServiceAccount; name: notebook; namespace: default; roleRef:; kind: Role; name: create-services #this was causing the error, and of course the create-services role is superseded by the the create-services-and-pods role; apiGroup: """"; ---; ```. After:; ```yaml; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: create-services-and-pods; rules:; - apiGroups: [""""]; resources: [""services""]; verbs: [""*""]; - apiGroups: [""""]; resources: [""pods""]; verbs: [""*""]; ---; kind: RoleBinding; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: notebook-create-services-and-pods; subjects:; - kind: ServiceAccount; name: notebook; namespace: default; roleRef:; kind: Role; name: create-services-and-pods; apiGroup: """"; ---; ```. ### Results of test runs. Before:. ```sh; kubectl apply -f k8s-config.yaml; ERROR: (gcloud.compute.addresses.describe) Could not fetch resource:; - Required 'compute.addresses.get' permission for 'projects/hail-vdc-staging/regions/us-central1/addresses/site'. namespace/batch-pods unchanged; ...; The RoleBinding ""notebook-create-services-and-pods"" is invalid: roleRef: Invalid value: rbac.RoleRef{APIGroup:""rbac.authorization.k8s.io"", Kind:""Role"", Name:""create-services""}: cannot change roleRef; make: *** [k8s-config] Error 1; ```. After:; ```sh; ERROR: (gcloud.compute.addresses.describe) Could not fetch resource:; - Required 'compute.addresses.get' permission for 'projects/hail-vdc-staging/regions/us-central1/addresses/site'. ...; role.rbac.authorization.k8s.io/create-services-and-pods unchanged; rolebinding.rbac.authorization.k8s.io/notebook-create-services-and-pods configured; role.rbac.authorization.k8s.io/read-get-user-secret unchanged; rolebinding.rbac.authorization.k8s.io/notebook-read-get-users-secret configured; ```. I think the error just reflects my not having hail-vdc-staging permissions, that is unaffected by this PR. cc @cseed, @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5746
https://github.com/hail-is/hail/pull/5746:190,Testability,test,tested,190,"This fixes the notebook2 deployment permission issue that was resulting in CrashLoopBackoff (no permissions for the Table class to `read_namespaced_secret('get-users', 'default')`). Already tested, works (notebook2 back up). It also fixes an apparent error in the master branch RoleBinding. This diff looks slightly weird. I fixed the existing notebook Roles/RoleBindings by deleting the `create-services` `Role` and `notebook-create-services` `RoleBinding`, and then fixing the broken `notebook-create-servivces-and-pods` `RoleBiding`, by correctly updating the `roleRef` to read `create-services-and-pods`. When notebook1 totally goes away, we can probably remove the ""services"" permission. Before:; ```yaml; ---; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: create-services; rules:; - apiGroups: [""""]; resources: [""services""]; verbs: [""*""]; ---; kind: RoleBinding; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: notebook-create-services; subjects:; - kind: ServiceAccount; name: notebook; namespace: default; roleRef:; kind: Role; name: create-services; apiGroup: """"; ---; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: create-services-and-pods; rules:; - apiGroups: [""""]; resources: [""services""]; verbs: [""*""]; - apiGroups: [""""]; resources: [""pods""]; verbs: [""*""]; ---; kind: RoleBinding; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: notebook-create-services-and-pods; subjects:; - kind: ServiceAccount; name: notebook; namespace: default; roleRef:; kind: Role; name: create-services #this was causing the error, and of course the create-services role is superseded by the the create-services-and-pods role; apiGroup: """"; ---; ```. After:; ```yaml; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: create-services-and-pods; rules:; - apiGroups: [""""]; resources: [""services""]; verbs: [""*""]; - apiGrou",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5746
https://github.com/hail-is/hail/pull/5746:2348,Testability,test,test,2348,"ups: [""""]; resources: [""pods""]; verbs: [""*""]; ---; kind: RoleBinding; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: notebook-create-services-and-pods; subjects:; - kind: ServiceAccount; name: notebook; namespace: default; roleRef:; kind: Role; name: create-services #this was causing the error, and of course the create-services role is superseded by the the create-services-and-pods role; apiGroup: """"; ---; ```. After:; ```yaml; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: create-services-and-pods; rules:; - apiGroups: [""""]; resources: [""services""]; verbs: [""*""]; - apiGroups: [""""]; resources: [""pods""]; verbs: [""*""]; ---; kind: RoleBinding; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: notebook-create-services-and-pods; subjects:; - kind: ServiceAccount; name: notebook; namespace: default; roleRef:; kind: Role; name: create-services-and-pods; apiGroup: """"; ---; ```. ### Results of test runs. Before:. ```sh; kubectl apply -f k8s-config.yaml; ERROR: (gcloud.compute.addresses.describe) Could not fetch resource:; - Required 'compute.addresses.get' permission for 'projects/hail-vdc-staging/regions/us-central1/addresses/site'. namespace/batch-pods unchanged; ...; The RoleBinding ""notebook-create-services-and-pods"" is invalid: roleRef: Invalid value: rbac.RoleRef{APIGroup:""rbac.authorization.k8s.io"", Kind:""Role"", Name:""create-services""}: cannot change roleRef; make: *** [k8s-config] Error 1; ```. After:; ```sh; ERROR: (gcloud.compute.addresses.describe) Could not fetch resource:; - Required 'compute.addresses.get' permission for 'projects/hail-vdc-staging/regions/us-central1/addresses/site'. ...; role.rbac.authorization.k8s.io/create-services-and-pods unchanged; rolebinding.rbac.authorization.k8s.io/notebook-create-services-and-pods configured; role.rbac.authorization.k8s.io/read-get-user-secret unchanged; rolebinding.rbac.authorization.k8s.io/notebook-read-get-user",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5746
https://github.com/hail-is/hail/pull/5747:189,Deployability,update,updates,189,"Cotton requested this, to disambiguate the google service account secret name from other secret names, when, at some later date, we have that problem. Will add the corresponding definition updates in the script PR (https://github.com/hail-is/hail/pull/5618)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5747
https://github.com/hail-is/hail/pull/5749:26,Deployability,pipeline,pipeline,26,Needed to debug Laurent's pipeline. And we should be logging everything anyway -- this is the only way we get to see the post-extract-aggregators executed IR,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5749
https://github.com/hail-is/hail/pull/5749:53,Testability,log,logging,53,Needed to debug Laurent's pipeline. And we should be logging everything anyway -- this is the only way we get to see the post-extract-aggregators executed IR,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5749
https://github.com/hail-is/hail/pull/5751:7,Deployability,update,update,7,I only update to 3.2.11 since that's what listed on the Spark maven page:. https://mvnrepository.com/artifact/org.apache.spark/spark-core_2.11/2.2.0. When we update to Spark 2.4 (soon) we should update further. May address #5744,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5751
https://github.com/hail-is/hail/pull/5751:158,Deployability,update,update,158,I only update to 3.2.11 since that's what listed on the Spark maven page:. https://mvnrepository.com/artifact/org.apache.spark/spark-core_2.11/2.2.0. When we update to Spark 2.4 (soon) we should update further. May address #5744,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5751
https://github.com/hail-is/hail/pull/5751:195,Deployability,update,update,195,I only update to 3.2.11 since that's what listed on the Spark maven page:. https://mvnrepository.com/artifact/org.apache.spark/spark-core_2.11/2.2.0. When we update to Spark 2.4 (soon) we should update further. May address #5744,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5751
https://github.com/hail-is/hail/pull/5752:0,Usability,Simpl,Simply,0,"Simply adds a /user page. Assigned to dan, but he's sick, should I assign someone else?. cc @cseed, @danking @jigold",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5752
https://github.com/hail-is/hail/pull/5753:575,Deployability,update,update,575,"Allows the notebook container to have access to the user's gsa secret key, a necessary step in mounting the user's bucket. I modified `delete_worker_pod` to exclude the V1DeleteOptions, because the signature appears to have changed, and no options were actually provided. ```sh; File ""notebook/notebook.py"", line 443, in delete_worker_pod; kube.client.V1DeleteOptions()); TypeError: delete_namespaced_pod() takes 3 positional arguments but 4 were given; ```. Will double check that I have necessary permissions: its the case when running on local, but I think I will need to update vdc to provide broad secret read access for notebook for this to work on the cluster. Checking now. cc @cseed, @danking, @jigold",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5753
https://github.com/hail-is/hail/pull/5753:38,Security,access,access,38,"Allows the notebook container to have access to the user's gsa secret key, a necessary step in mounting the user's bucket. I modified `delete_worker_pod` to exclude the V1DeleteOptions, because the signature appears to have changed, and no options were actually provided. ```sh; File ""notebook/notebook.py"", line 443, in delete_worker_pod; kube.client.V1DeleteOptions()); TypeError: delete_namespaced_pod() takes 3 positional arguments but 4 were given; ```. Will double check that I have necessary permissions: its the case when running on local, but I think I will need to update vdc to provide broad secret read access for notebook for this to work on the cluster. Checking now. cc @cseed, @danking, @jigold",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5753
https://github.com/hail-is/hail/pull/5753:615,Security,access,access,615,"Allows the notebook container to have access to the user's gsa secret key, a necessary step in mounting the user's bucket. I modified `delete_worker_pod` to exclude the V1DeleteOptions, because the signature appears to have changed, and no options were actually provided. ```sh; File ""notebook/notebook.py"", line 443, in delete_worker_pod; kube.client.V1DeleteOptions()); TypeError: delete_namespaced_pod() takes 3 positional arguments but 4 were given; ```. Will double check that I have necessary permissions: its the case when running on local, but I think I will need to update vdc to provide broad secret read access for notebook for this to work on the cluster. Checking now. cc @cseed, @danking, @jigold",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5753
https://github.com/hail-is/hail/pull/5756:23,Deployability,update,updated,23,I have no idea if I've updated everything I need to update here. I haven't pushed the updated CI; build image or changed cloudtools (yet). Update: pushed ci build image. cc @cseed @danking,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5756
https://github.com/hail-is/hail/pull/5756:52,Deployability,update,update,52,I have no idea if I've updated everything I need to update here. I haven't pushed the updated CI; build image or changed cloudtools (yet). Update: pushed ci build image. cc @cseed @danking,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5756
https://github.com/hail-is/hail/pull/5756:86,Deployability,update,updated,86,I have no idea if I've updated everything I need to update here. I haven't pushed the updated CI; build image or changed cloudtools (yet). Update: pushed ci build image. cc @cseed @danking,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5756
https://github.com/hail-is/hail/pull/5756:139,Deployability,Update,Update,139,I have no idea if I've updated everything I need to update here. I haven't pushed the updated CI; build image or changed cloudtools (yet). Update: pushed ci build image. cc @cseed @danking,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5756
https://github.com/hail-is/hail/pull/5761:185,Availability,error,error,185,"We mix the two right now, with most of the new code using quotes. I; feel that quotes are a better solution given the prevalence of markdown; editors (like Zulip) where copy-pasting an error message with backticks; leads to badly formatted renderings. cc @cseed who I believe favored the use of the backtick style in the first place.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5761
https://github.com/hail-is/hail/pull/5761:191,Integrability,message,message,191,"We mix the two right now, with most of the new code using quotes. I; feel that quotes are a better solution given the prevalence of markdown; editors (like Zulip) where copy-pasting an error message with backticks; leads to badly formatted renderings. cc @cseed who I believe favored the use of the backtick style in the first place.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5761
https://github.com/hail-is/hail/pull/5762:9,Testability,test,test,9,"This new test runs fine locally without the service account key activation, but I think it will be needed on the cloud. We'll see if this just works!. cc: @konradjk",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5762
https://github.com/hail-is/hail/pull/5765:134,Performance,optimiz,optimized,134,Assigning Cotton for context since he implemented ArrayAgg back in January. It looks to me like this stuff was never actually getting optimized...,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5765
https://github.com/hail-is/hail/issues/5767:144,Deployability,Pipeline,Pipeline,144,"Not possible until more batch infrastructure is done, but want to get the change out to Konrad. ```; def test_gcs_file_localization(self):; p = Pipeline(); input = p.read_input(f'{gcs_input_dir}/hello.txt'); t = p.new_task(); t.command(f'cat {input} > {t.ofile}'); p.write_output(t.ofile, f'{gcs_output_dir}/hello.txt'); p.run(); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5767
https://github.com/hail-is/hail/pull/5781:0,Integrability,Depend,Depends,0,Depends on #5776.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5781
https://github.com/hail-is/hail/pull/5782:202,Deployability,deploy,deployed,202,"I'm forking ci => ci2 to address the existing ci issues. I plan to steal lots of code from ci, this is just a skeleton to get started. Along the way, I'm going to convert everything to async. I already deployed the ci2 service, so the gateway change shouldn't break anything.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5782
https://github.com/hail-is/hail/pull/5784:0,Integrability,Depend,Depends,0,Depends on #5776 and #5781,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5784
https://github.com/hail-is/hail/issues/5786:73,Availability,error,errors,73,"The children field in `table2` is the number of children with any mendel errors, not the number of children in that family. This should probably be fixed by doing an `annotate_cols` aggregation and then aggregating the result.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5786
https://github.com/hail-is/hail/pull/5792:83,Deployability,update,updates,83,"@cseed fyi. Not sure why I didn't always do it this way. `push` is the method that updates the internal state with knowledge of a new commit to a target branch (e.g. master, 0.1)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5792
https://github.com/hail-is/hail/pull/5795:28,Modifiability,variab,variables,28,`server.py` contains global variables that we really ought not to evaluate when running tests. This moves the minimal set of things out of `server.py` so that the tests do not evaluate `server.py`.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5795
https://github.com/hail-is/hail/pull/5795:88,Testability,test,tests,88,`server.py` contains global variables that we really ought not to evaluate when running tests. This moves the minimal set of things out of `server.py` so that the tests do not evaluate `server.py`.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5795
https://github.com/hail-is/hail/pull/5795:163,Testability,test,tests,163,`server.py` contains global variables that we really ought not to evaluate when running tests. This moves the minimal set of things out of `server.py` so that the tests do not evaluate `server.py`.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5795
https://github.com/hail-is/hail/issues/5796:613,Availability,Error,ErrorHandling,613,"Some delimited text processors use quotes to escape quotes so that the string `a""b` is rendered as `a""""b`. Moreover an individual entry of the delimited text is itself wrapped in double quotes, so, for example, a delimited text file representing one row containing the strings: `hello`, `a""b`, `goodbye` would contain the following bytes:; ```; ""hello"",""a""""b"",""goodbye""; ```. ---. Attempting to import and show the attached TSV file with `hl.import_table(""test.txt"", quote='""').show()` throws an exception:; ```; is.hail.utils.HailException: terminating quote character '""' not at end of field; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.expr.ir.TextTableReader$.splitLine(TextTableReader.scala:107); 	at is.hail.expr.ir.TextTableReader$$anonfun$28$$anonfun$apply$7$$anonfun$apply$8.apply(TextTableReader.scala:379); 	at is.hail.expr.ir.TextTableReader$$anonfun$28$$anonfun$apply$7$$anonfun$apply$8.apply(TextTableReader.scala:378); 	at is.hail.utils.WithContext.map(Context.scala:33); 	at is.hail.expr.ir.TextTableReader$$anonfun$28$$anonfun$apply$7.apply(TextTableReader.scala:378); 	at is.hail.expr.ir.TextTableReader$$anonfun$28$$anonfun$apply$7.apply(TextTableReader.scala:408); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:385); 	at is.hail.sparkextras.ContextRDD$$anonfun$14.apply(ContextRDD.scala:559); 	at is.hail.sparkextras.ContextRDD$$anonfun$14.apply(ContextRDD.scala:559); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:589); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:587); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.r",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5796
https://github.com/hail-is/hail/issues/5796:639,Availability,Error,ErrorHandling,639,"Some delimited text processors use quotes to escape quotes so that the string `a""b` is rendered as `a""""b`. Moreover an individual entry of the delimited text is itself wrapped in double quotes, so, for example, a delimited text file representing one row containing the strings: `hello`, `a""b`, `goodbye` would contain the following bytes:; ```; ""hello"",""a""""b"",""goodbye""; ```. ---. Attempting to import and show the attached TSV file with `hl.import_table(""test.txt"", quote='""').show()` throws an exception:; ```; is.hail.utils.HailException: terminating quote character '""' not at end of field; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.expr.ir.TextTableReader$.splitLine(TextTableReader.scala:107); 	at is.hail.expr.ir.TextTableReader$$anonfun$28$$anonfun$apply$7$$anonfun$apply$8.apply(TextTableReader.scala:379); 	at is.hail.expr.ir.TextTableReader$$anonfun$28$$anonfun$apply$7$$anonfun$apply$8.apply(TextTableReader.scala:378); 	at is.hail.utils.WithContext.map(Context.scala:33); 	at is.hail.expr.ir.TextTableReader$$anonfun$28$$anonfun$apply$7.apply(TextTableReader.scala:378); 	at is.hail.expr.ir.TextTableReader$$anonfun$28$$anonfun$apply$7.apply(TextTableReader.scala:408); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:385); 	at is.hail.sparkextras.ContextRDD$$anonfun$14.apply(ContextRDD.scala:559); 	at is.hail.sparkextras.ContextRDD$$anonfun$14.apply(ContextRDD.scala:559); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:589); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:587); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.r",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5796
https://github.com/hail-is/hail/issues/5796:1979,Energy Efficiency,schedul,scheduler,1979,"$.fatal(package.scala:28); 	at is.hail.expr.ir.TextTableReader$.splitLine(TextTableReader.scala:107); 	at is.hail.expr.ir.TextTableReader$$anonfun$28$$anonfun$apply$7$$anonfun$apply$8.apply(TextTableReader.scala:379); 	at is.hail.expr.ir.TextTableReader$$anonfun$28$$anonfun$apply$7$$anonfun$apply$8.apply(TextTableReader.scala:378); 	at is.hail.utils.WithContext.map(Context.scala:33); 	at is.hail.expr.ir.TextTableReader$$anonfun$28$$anonfun$apply$7.apply(TextTableReader.scala:378); 	at is.hail.expr.ir.TextTableReader$$anonfun$28$$anonfun$apply$7.apply(TextTableReader.scala:408); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:385); 	at is.hail.sparkextras.ContextRDD$$anonfun$14.apply(ContextRDD.scala:559); 	at is.hail.sparkextras.ContextRDD$$anonfun$14.apply(ContextRDD.scala:559); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:589); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:587); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ```. Expected output:; ```; +-----+-----------+; | idx | value |; +-----+-----------+; | str | str |; +-----+-----------+; | ""1"" | ""\""foo\"""" |; | ""2"" | ""\""bar\"""" |; | ""3"" | ""\""baz\"""" |; +-----+-----------+; ```. Hail version: 0.2.12-9409c0635781. [test.txt](https://github.com/hail-is/hail/files/3049195/test.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5796
https://github.com/hail-is/hail/issues/5796:2051,Energy Efficiency,schedul,scheduler,2051,"$.fatal(package.scala:28); 	at is.hail.expr.ir.TextTableReader$.splitLine(TextTableReader.scala:107); 	at is.hail.expr.ir.TextTableReader$$anonfun$28$$anonfun$apply$7$$anonfun$apply$8.apply(TextTableReader.scala:379); 	at is.hail.expr.ir.TextTableReader$$anonfun$28$$anonfun$apply$7$$anonfun$apply$8.apply(TextTableReader.scala:378); 	at is.hail.utils.WithContext.map(Context.scala:33); 	at is.hail.expr.ir.TextTableReader$$anonfun$28$$anonfun$apply$7.apply(TextTableReader.scala:378); 	at is.hail.expr.ir.TextTableReader$$anonfun$28$$anonfun$apply$7.apply(TextTableReader.scala:408); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:385); 	at is.hail.sparkextras.ContextRDD$$anonfun$14.apply(ContextRDD.scala:559); 	at is.hail.sparkextras.ContextRDD$$anonfun$14.apply(ContextRDD.scala:559); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:589); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:587); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ```. Expected output:; ```; +-----+-----------+; | idx | value |; +-----+-----------+; | str | str |; +-----+-----------+; | ""1"" | ""\""foo\"""" |; | ""2"" | ""\""bar\"""" |; | ""3"" | ""\""baz\"""" |; +-----+-----------+; ```. Hail version: 0.2.12-9409c0635781. [test.txt](https://github.com/hail-is/hail/files/3049195/test.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5796
https://github.com/hail-is/hail/issues/5796:168,Integrability,wrap,wrapped,168,"Some delimited text processors use quotes to escape quotes so that the string `a""b` is rendered as `a""""b`. Moreover an individual entry of the delimited text is itself wrapped in double quotes, so, for example, a delimited text file representing one row containing the strings: `hello`, `a""b`, `goodbye` would contain the following bytes:; ```; ""hello"",""a""""b"",""goodbye""; ```. ---. Attempting to import and show the attached TSV file with `hl.import_table(""test.txt"", quote='""').show()` throws an exception:; ```; is.hail.utils.HailException: terminating quote character '""' not at end of field; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.expr.ir.TextTableReader$.splitLine(TextTableReader.scala:107); 	at is.hail.expr.ir.TextTableReader$$anonfun$28$$anonfun$apply$7$$anonfun$apply$8.apply(TextTableReader.scala:379); 	at is.hail.expr.ir.TextTableReader$$anonfun$28$$anonfun$apply$7$$anonfun$apply$8.apply(TextTableReader.scala:378); 	at is.hail.utils.WithContext.map(Context.scala:33); 	at is.hail.expr.ir.TextTableReader$$anonfun$28$$anonfun$apply$7.apply(TextTableReader.scala:378); 	at is.hail.expr.ir.TextTableReader$$anonfun$28$$anonfun$apply$7.apply(TextTableReader.scala:408); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:385); 	at is.hail.sparkextras.ContextRDD$$anonfun$14.apply(ContextRDD.scala:559); 	at is.hail.sparkextras.ContextRDD$$anonfun$14.apply(ContextRDD.scala:559); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:589); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:587); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.r",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5796
https://github.com/hail-is/hail/issues/5796:2176,Performance,concurren,concurrent,2176,"$.fatal(package.scala:28); 	at is.hail.expr.ir.TextTableReader$.splitLine(TextTableReader.scala:107); 	at is.hail.expr.ir.TextTableReader$$anonfun$28$$anonfun$apply$7$$anonfun$apply$8.apply(TextTableReader.scala:379); 	at is.hail.expr.ir.TextTableReader$$anonfun$28$$anonfun$apply$7$$anonfun$apply$8.apply(TextTableReader.scala:378); 	at is.hail.utils.WithContext.map(Context.scala:33); 	at is.hail.expr.ir.TextTableReader$$anonfun$28$$anonfun$apply$7.apply(TextTableReader.scala:378); 	at is.hail.expr.ir.TextTableReader$$anonfun$28$$anonfun$apply$7.apply(TextTableReader.scala:408); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:385); 	at is.hail.sparkextras.ContextRDD$$anonfun$14.apply(ContextRDD.scala:559); 	at is.hail.sparkextras.ContextRDD$$anonfun$14.apply(ContextRDD.scala:559); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:589); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:587); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ```. Expected output:; ```; +-----+-----------+; | idx | value |; +-----+-----------+; | str | str |; +-----+-----------+; | ""1"" | ""\""foo\"""" |; | ""2"" | ""\""bar\"""" |; | ""3"" | ""\""baz\"""" |; +-----+-----------+; ```. Hail version: 0.2.12-9409c0635781. [test.txt](https://github.com/hail-is/hail/files/3049195/test.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5796
https://github.com/hail-is/hail/issues/5796:2261,Performance,concurren,concurrent,2261,"$.fatal(package.scala:28); 	at is.hail.expr.ir.TextTableReader$.splitLine(TextTableReader.scala:107); 	at is.hail.expr.ir.TextTableReader$$anonfun$28$$anonfun$apply$7$$anonfun$apply$8.apply(TextTableReader.scala:379); 	at is.hail.expr.ir.TextTableReader$$anonfun$28$$anonfun$apply$7$$anonfun$apply$8.apply(TextTableReader.scala:378); 	at is.hail.utils.WithContext.map(Context.scala:33); 	at is.hail.expr.ir.TextTableReader$$anonfun$28$$anonfun$apply$7.apply(TextTableReader.scala:378); 	at is.hail.expr.ir.TextTableReader$$anonfun$28$$anonfun$apply$7.apply(TextTableReader.scala:408); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:385); 	at is.hail.sparkextras.ContextRDD$$anonfun$14.apply(ContextRDD.scala:559); 	at is.hail.sparkextras.ContextRDD$$anonfun$14.apply(ContextRDD.scala:559); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:589); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:587); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ```. Expected output:; ```; +-----+-----------+; | idx | value |; +-----+-----------+; | str | str |; +-----+-----------+; | ""1"" | ""\""foo\"""" |; | ""2"" | ""\""bar\"""" |; | ""3"" | ""\""baz\"""" |; +-----+-----------+; ```. Hail version: 0.2.12-9409c0635781. [test.txt](https://github.com/hail-is/hail/files/3049195/test.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5796
https://github.com/hail-is/hail/issues/5796:456,Testability,test,test,456,"Some delimited text processors use quotes to escape quotes so that the string `a""b` is rendered as `a""""b`. Moreover an individual entry of the delimited text is itself wrapped in double quotes, so, for example, a delimited text file representing one row containing the strings: `hello`, `a""b`, `goodbye` would contain the following bytes:; ```; ""hello"",""a""""b"",""goodbye""; ```. ---. Attempting to import and show the attached TSV file with `hl.import_table(""test.txt"", quote='""').show()` throws an exception:; ```; is.hail.utils.HailException: terminating quote character '""' not at end of field; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.expr.ir.TextTableReader$.splitLine(TextTableReader.scala:107); 	at is.hail.expr.ir.TextTableReader$$anonfun$28$$anonfun$apply$7$$anonfun$apply$8.apply(TextTableReader.scala:379); 	at is.hail.expr.ir.TextTableReader$$anonfun$28$$anonfun$apply$7$$anonfun$apply$8.apply(TextTableReader.scala:378); 	at is.hail.utils.WithContext.map(Context.scala:33); 	at is.hail.expr.ir.TextTableReader$$anonfun$28$$anonfun$apply$7.apply(TextTableReader.scala:378); 	at is.hail.expr.ir.TextTableReader$$anonfun$28$$anonfun$apply$7.apply(TextTableReader.scala:408); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:385); 	at is.hail.sparkextras.ContextRDD$$anonfun$14.apply(ContextRDD.scala:559); 	at is.hail.sparkextras.ContextRDD$$anonfun$14.apply(ContextRDD.scala:559); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:589); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:587); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.r",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5796
https://github.com/hail-is/hail/issues/5796:2623,Testability,test,test,2623,"$.fatal(package.scala:28); 	at is.hail.expr.ir.TextTableReader$.splitLine(TextTableReader.scala:107); 	at is.hail.expr.ir.TextTableReader$$anonfun$28$$anonfun$apply$7$$anonfun$apply$8.apply(TextTableReader.scala:379); 	at is.hail.expr.ir.TextTableReader$$anonfun$28$$anonfun$apply$7$$anonfun$apply$8.apply(TextTableReader.scala:378); 	at is.hail.utils.WithContext.map(Context.scala:33); 	at is.hail.expr.ir.TextTableReader$$anonfun$28$$anonfun$apply$7.apply(TextTableReader.scala:378); 	at is.hail.expr.ir.TextTableReader$$anonfun$28$$anonfun$apply$7.apply(TextTableReader.scala:408); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:385); 	at is.hail.sparkextras.ContextRDD$$anonfun$14.apply(ContextRDD.scala:559); 	at is.hail.sparkextras.ContextRDD$$anonfun$14.apply(ContextRDD.scala:559); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:589); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:587); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ```. Expected output:; ```; +-----+-----------+; | idx | value |; +-----+-----------+; | str | str |; +-----+-----------+; | ""1"" | ""\""foo\"""" |; | ""2"" | ""\""bar\"""" |; | ""3"" | ""\""baz\"""" |; +-----+-----------+; ```. Hail version: 0.2.12-9409c0635781. [test.txt](https://github.com/hail-is/hail/files/3049195/test.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5796
https://github.com/hail-is/hail/issues/5796:2679,Testability,test,test,2679,"$.fatal(package.scala:28); 	at is.hail.expr.ir.TextTableReader$.splitLine(TextTableReader.scala:107); 	at is.hail.expr.ir.TextTableReader$$anonfun$28$$anonfun$apply$7$$anonfun$apply$8.apply(TextTableReader.scala:379); 	at is.hail.expr.ir.TextTableReader$$anonfun$28$$anonfun$apply$7$$anonfun$apply$8.apply(TextTableReader.scala:378); 	at is.hail.utils.WithContext.map(Context.scala:33); 	at is.hail.expr.ir.TextTableReader$$anonfun$28$$anonfun$apply$7.apply(TextTableReader.scala:378); 	at is.hail.expr.ir.TextTableReader$$anonfun$28$$anonfun$apply$7.apply(TextTableReader.scala:408); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:385); 	at is.hail.sparkextras.ContextRDD$$anonfun$14.apply(ContextRDD.scala:559); 	at is.hail.sparkextras.ContextRDD$$anonfun$14.apply(ContextRDD.scala:559); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:589); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:587); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ```. Expected output:; ```; +-----+-----------+; | idx | value |; +-----+-----------+; | str | str |; +-----+-----------+; | ""1"" | ""\""foo\"""" |; | ""2"" | ""\""bar\"""" |; | ""3"" | ""\""baz\"""" |; +-----+-----------+; ```. Hail version: 0.2.12-9409c0635781. [test.txt](https://github.com/hail-is/hail/files/3049195/test.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5796
https://github.com/hail-is/hail/pull/5798:79,Availability,error,errors,79,"I think old repos still work, which is why CI is mostly fine, but I was seeing errors when testing locally.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5798
https://github.com/hail-is/hail/pull/5798:91,Testability,test,testing,91,"I think old repos still work, which is why CI is mostly fine, but I was seeing errors when testing locally.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5798
https://github.com/hail-is/hail/pull/5799:34,Testability,benchmark,benchmark,34,"Fixes #5777. Timing in master for benchmark/matrix_table_entries_table:. run 1 took 91.15s; run 2 took 86.58s; run 3 took 85.45s; Mean, Median: 87.73s, 86.58s. Timing on this branch:. run 1 took 20.33s; run 2 took 20.98s; run 3 took 21.28s; Mean, Median: 20.86s, 20.98s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5799
https://github.com/hail-is/hail/pull/5800:69,Deployability,deploy,deployment,69,"Remove conda from ci. Mirrors the changes in batch. I already tested deployment works by hand deploying a bogus version. It didn't have any watched targets though, so I didn't test everything. The pod starts successfully in python 3.6 (ci previously used python 3.7, see changes in shell_helper.py).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5800
https://github.com/hail-is/hail/pull/5800:94,Deployability,deploy,deploying,94,"Remove conda from ci. Mirrors the changes in batch. I already tested deployment works by hand deploying a bogus version. It didn't have any watched targets though, so I didn't test everything. The pod starts successfully in python 3.6 (ci previously used python 3.7, see changes in shell_helper.py).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5800
https://github.com/hail-is/hail/pull/5800:62,Testability,test,tested,62,"Remove conda from ci. Mirrors the changes in batch. I already tested deployment works by hand deploying a bogus version. It didn't have any watched targets though, so I didn't test everything. The pod starts successfully in python 3.6 (ci previously used python 3.7, see changes in shell_helper.py).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5800
https://github.com/hail-is/hail/pull/5800:176,Testability,test,test,176,"Remove conda from ci. Mirrors the changes in batch. I already tested deployment works by hand deploying a bogus version. It didn't have any watched targets though, so I didn't test everything. The pod starts successfully in python 3.6 (ci previously used python 3.7, see changes in shell_helper.py).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5800
https://github.com/hail-is/hail/pull/5801:93,Testability,test,tests,93,There were some cases where GH or CI were a bit slow to respond to PR changes. This lets the tests wait a bit until CI is aware of a PR. I think some test flakiness came from this. cc: @cseed,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5801
https://github.com/hail-is/hail/pull/5801:150,Testability,test,test,150,There were some cases where GH or CI were a bit slow to respond to PR changes. This lets the tests wait a bit until CI is aware of a PR. I think some test flakiness came from this. cc: @cseed,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5801
https://github.com/hail-is/hail/pull/5802:125,Testability,test,test,125,This is intended to prevent the script from being terminated while it cleans up (c.f. people who spam ctrl-c to kill a local test). `set` is not the word for setting a `trap` though. Long standing bug 🤷‍♀️ .,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5802
https://github.com/hail-is/hail/pull/5807:63,Performance,cache,cache,63,Apply the same logic as used in batch to gateway to get better cache behavior.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5807
https://github.com/hail-is/hail/pull/5807:15,Testability,log,logic,15,Apply the same logic as used in batch to gateway to get better cache behavior.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5807
https://github.com/hail-is/hail/pull/5808:135,Deployability,deploy,deploy,135,"ci2 now tracks the Github state. Mostly copied from ci, with the following changes:; - watched branches instead of watched targets (no deploy setting for the moment); - / instead of /ui shows the state; - rename: ref => branch; - use Github async client library gidgethub: https://gidgethub.readthedocs.io/en/latest/; - no callbacks yet. Rough ci2 plan is:; - async batch library; - track batch state (assuming persistence); - write heal; - yaml-based build and deploy instructions that will be rendered as a batch pipeline (that's where it gets interested)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5808
https://github.com/hail-is/hail/pull/5808:462,Deployability,deploy,deploy,462,"ci2 now tracks the Github state. Mostly copied from ci, with the following changes:; - watched branches instead of watched targets (no deploy setting for the moment); - / instead of /ui shows the state; - rename: ref => branch; - use Github async client library gidgethub: https://gidgethub.readthedocs.io/en/latest/; - no callbacks yet. Rough ci2 plan is:; - async batch library; - track batch state (assuming persistence); - write heal; - yaml-based build and deploy instructions that will be rendered as a batch pipeline (that's where it gets interested)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5808
https://github.com/hail-is/hail/pull/5808:515,Deployability,pipeline,pipeline,515,"ci2 now tracks the Github state. Mostly copied from ci, with the following changes:; - watched branches instead of watched targets (no deploy setting for the moment); - / instead of /ui shows the state; - rename: ref => branch; - use Github async client library gidgethub: https://gidgethub.readthedocs.io/en/latest/; - no callbacks yet. Rough ci2 plan is:; - async batch library; - track batch state (assuming persistence); - write heal; - yaml-based build and deploy instructions that will be rendered as a batch pipeline (that's where it gets interested)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5808
https://github.com/hail-is/hail/pull/5809:152,Deployability,deploy,deploy,152,"This PR is almost done. I need to change one of the Makefile rules to include new targets from #5791 and make sure the database is cleaned and test the deploy works. My plan is to continually delete the production tables per deployment until we're happy with the schema and everything is working. Once this is all in, then the next PR will add the actual data to the tables in `server.py` and get rid of the global dictionaries / application state. I might try and do this in 2 stages (jobs and batch), but I'm not sure it's possible. Depends on #5781, #5784",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5809
https://github.com/hail-is/hail/pull/5809:225,Deployability,deploy,deployment,225,"This PR is almost done. I need to change one of the Makefile rules to include new targets from #5791 and make sure the database is cleaned and test the deploy works. My plan is to continually delete the production tables per deployment until we're happy with the schema and everything is working. Once this is all in, then the next PR will add the actual data to the tables in `server.py` and get rid of the global dictionaries / application state. I might try and do this in 2 stages (jobs and batch), but I'm not sure it's possible. Depends on #5781, #5784",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5809
https://github.com/hail-is/hail/pull/5809:535,Integrability,Depend,Depends,535,"This PR is almost done. I need to change one of the Makefile rules to include new targets from #5791 and make sure the database is cleaned and test the deploy works. My plan is to continually delete the production tables per deployment until we're happy with the schema and everything is working. Once this is all in, then the next PR will add the actual data to the tables in `server.py` and get rid of the global dictionaries / application state. I might try and do this in 2 stages (jobs and batch), but I'm not sure it's possible. Depends on #5781, #5784",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5809
https://github.com/hail-is/hail/pull/5809:143,Testability,test,test,143,"This PR is almost done. I need to change one of the Makefile rules to include new targets from #5791 and make sure the database is cleaned and test the deploy works. My plan is to continually delete the production tables per deployment until we're happy with the schema and everything is working. Once this is all in, then the next PR will add the actual data to the tables in `server.py` and get rid of the global dictionaries / application state. I might try and do this in 2 stages (jobs and batch), but I'm not sure it's possible. Depends on #5781, #5784",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5809
https://github.com/hail-is/hail/pull/5812:73,Deployability,update,update,73,"mostly copied from client.py, minimally tested. I changed is_complete to update the state if the job is not already complete, removed cached_status.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5812
https://github.com/hail-is/hail/pull/5812:40,Testability,test,tested,40,"mostly copied from client.py, minimally tested. I changed is_complete to update the state if the job is not already complete, removed cached_status.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5812
https://github.com/hail-is/hail/pull/5813:12,Energy Efficiency,reduce,reduce,12,This should reduce external cloud storage dependencies making; infrastructure changes easier.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5813
https://github.com/hail-is/hail/pull/5813:42,Integrability,depend,dependencies,42,This should reduce external cloud storage dependencies making; infrastructure changes easier.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5813
https://github.com/hail-is/hail/pull/5814:39,Testability,log,logic,39,"stacked on 3 other ci2 PRs. - add heal logic; - add job log endpoint; - some simplifications: PRs have a watched branch target (which has a sha) and a source sha, but don't need the source branch. Got rid of FQSHA.; - Ran by hand, seems to work. Will add tests back once build is actually doing something non-trivial.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5814
https://github.com/hail-is/hail/pull/5814:56,Testability,log,log,56,"stacked on 3 other ci2 PRs. - add heal logic; - add job log endpoint; - some simplifications: PRs have a watched branch target (which has a sha) and a source sha, but don't need the source branch. Got rid of FQSHA.; - Ran by hand, seems to work. Will add tests back once build is actually doing something non-trivial.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5814
https://github.com/hail-is/hail/pull/5814:255,Testability,test,tests,255,"stacked on 3 other ci2 PRs. - add heal logic; - add job log endpoint; - some simplifications: PRs have a watched branch target (which has a sha) and a source sha, but don't need the source branch. Got rid of FQSHA.; - Ran by hand, seems to work. Will add tests back once build is actually doing something non-trivial.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5814
https://github.com/hail-is/hail/pull/5814:77,Usability,simpl,simplifications,77,"stacked on 3 other ci2 PRs. - add heal logic; - add job log endpoint; - some simplifications: PRs have a watched branch target (which has a sha) and a source sha, but don't need the source branch. Got rid of FQSHA.; - Ran by hand, seems to work. Will add tests back once build is actually doing something non-trivial.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5814
https://github.com/hail-is/hail/pull/5815:235,Performance,bottleneck,bottleneck,235,"Fixes connection timeout after 8 hours. . When we transition to aiomysql, will port well to a pooled connection version (`async with self.pool.acquire() as conn:`. Even now however, the time it takes to acquire a connection is not the bottleneck during login. cc @danking assigned you as well in case you're on.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5815
https://github.com/hail-is/hail/pull/5815:17,Safety,timeout,timeout,17,"Fixes connection timeout after 8 hours. . When we transition to aiomysql, will port well to a pooled connection version (`async with self.pool.acquire() as conn:`. Even now however, the time it takes to acquire a connection is not the bottleneck during login. cc @danking assigned you as well in case you're on.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5815
https://github.com/hail-is/hail/pull/5815:253,Testability,log,login,253,"Fixes connection timeout after 8 hours. . When we transition to aiomysql, will port well to a pooled connection version (`async with self.pool.acquire() as conn:`. Even now however, the time it takes to acquire a connection is not the bottleneck during login. cc @danking assigned you as well in case you're on.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5815
https://github.com/hail-is/hail/issues/5817:75,Availability,FAILURE,FAILURES,75,"I see this happening quite a bit. ```; =================================== FAILURES ===================================; ______________________________ Test.test_callback ______________________________. self = <test.test_batch.Test testMethod=test_callback>. def test_callback(self):; app = Flask('test-client'); ; d = {}; ; @app.route('/test', methods=['POST']); def test():; d['status'] = request.get_json(); return Response(status=200); ; server = ServerThread(app); try:; server.start(); ; j = self.batch.create_job(; 'alpine',; ['echo', 'test'],; attributes={'foo': 'bar'},; callback=server.url_for('/test')); j.wait(); ; > status = d['status']; E KeyError: 'status'; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5817
https://github.com/hail-is/hail/issues/5817:535,Availability,echo,echo,535,"I see this happening quite a bit. ```; =================================== FAILURES ===================================; ______________________________ Test.test_callback ______________________________. self = <test.test_batch.Test testMethod=test_callback>. def test_callback(self):; app = Flask('test-client'); ; d = {}; ; @app.route('/test', methods=['POST']); def test():; d['status'] = request.get_json(); return Response(status=200); ; server = ServerThread(app); try:; server.start(); ; j = self.batch.create_job(; 'alpine',; ['echo', 'test'],; attributes={'foo': 'bar'},; callback=server.url_for('/test')); j.wait(); ; > status = d['status']; E KeyError: 'status'; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5817
https://github.com/hail-is/hail/issues/5817:330,Integrability,rout,route,330,"I see this happening quite a bit. ```; =================================== FAILURES ===================================; ______________________________ Test.test_callback ______________________________. self = <test.test_batch.Test testMethod=test_callback>. def test_callback(self):; app = Flask('test-client'); ; d = {}; ; @app.route('/test', methods=['POST']); def test():; d['status'] = request.get_json(); return Response(status=200); ; server = ServerThread(app); try:; server.start(); ; j = self.batch.create_job(; 'alpine',; ['echo', 'test'],; attributes={'foo': 'bar'},; callback=server.url_for('/test')); j.wait(); ; > status = d['status']; E KeyError: 'status'; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5817
https://github.com/hail-is/hail/issues/5817:152,Testability,Test,Test,152,"I see this happening quite a bit. ```; =================================== FAILURES ===================================; ______________________________ Test.test_callback ______________________________. self = <test.test_batch.Test testMethod=test_callback>. def test_callback(self):; app = Flask('test-client'); ; d = {}; ; @app.route('/test', methods=['POST']); def test():; d['status'] = request.get_json(); return Response(status=200); ; server = ServerThread(app); try:; server.start(); ; j = self.batch.create_job(; 'alpine',; ['echo', 'test'],; attributes={'foo': 'bar'},; callback=server.url_for('/test')); j.wait(); ; > status = d['status']; E KeyError: 'status'; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5817
https://github.com/hail-is/hail/issues/5817:211,Testability,test,test,211,"I see this happening quite a bit. ```; =================================== FAILURES ===================================; ______________________________ Test.test_callback ______________________________. self = <test.test_batch.Test testMethod=test_callback>. def test_callback(self):; app = Flask('test-client'); ; d = {}; ; @app.route('/test', methods=['POST']); def test():; d['status'] = request.get_json(); return Response(status=200); ; server = ServerThread(app); try:; server.start(); ; j = self.batch.create_job(; 'alpine',; ['echo', 'test'],; attributes={'foo': 'bar'},; callback=server.url_for('/test')); j.wait(); ; > status = d['status']; E KeyError: 'status'; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5817
https://github.com/hail-is/hail/issues/5817:227,Testability,Test,Test,227,"I see this happening quite a bit. ```; =================================== FAILURES ===================================; ______________________________ Test.test_callback ______________________________. self = <test.test_batch.Test testMethod=test_callback>. def test_callback(self):; app = Flask('test-client'); ; d = {}; ; @app.route('/test', methods=['POST']); def test():; d['status'] = request.get_json(); return Response(status=200); ; server = ServerThread(app); try:; server.start(); ; j = self.batch.create_job(; 'alpine',; ['echo', 'test'],; attributes={'foo': 'bar'},; callback=server.url_for('/test')); j.wait(); ; > status = d['status']; E KeyError: 'status'; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5817
https://github.com/hail-is/hail/issues/5817:232,Testability,test,testMethod,232,"I see this happening quite a bit. ```; =================================== FAILURES ===================================; ______________________________ Test.test_callback ______________________________. self = <test.test_batch.Test testMethod=test_callback>. def test_callback(self):; app = Flask('test-client'); ; d = {}; ; @app.route('/test', methods=['POST']); def test():; d['status'] = request.get_json(); return Response(status=200); ; server = ServerThread(app); try:; server.start(); ; j = self.batch.create_job(; 'alpine',; ['echo', 'test'],; attributes={'foo': 'bar'},; callback=server.url_for('/test')); j.wait(); ; > status = d['status']; E KeyError: 'status'; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5817
https://github.com/hail-is/hail/issues/5817:298,Testability,test,test-client,298,"I see this happening quite a bit. ```; =================================== FAILURES ===================================; ______________________________ Test.test_callback ______________________________. self = <test.test_batch.Test testMethod=test_callback>. def test_callback(self):; app = Flask('test-client'); ; d = {}; ; @app.route('/test', methods=['POST']); def test():; d['status'] = request.get_json(); return Response(status=200); ; server = ServerThread(app); try:; server.start(); ; j = self.batch.create_job(; 'alpine',; ['echo', 'test'],; attributes={'foo': 'bar'},; callback=server.url_for('/test')); j.wait(); ; > status = d['status']; E KeyError: 'status'; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5817
https://github.com/hail-is/hail/issues/5817:338,Testability,test,test,338,"I see this happening quite a bit. ```; =================================== FAILURES ===================================; ______________________________ Test.test_callback ______________________________. self = <test.test_batch.Test testMethod=test_callback>. def test_callback(self):; app = Flask('test-client'); ; d = {}; ; @app.route('/test', methods=['POST']); def test():; d['status'] = request.get_json(); return Response(status=200); ; server = ServerThread(app); try:; server.start(); ; j = self.batch.create_job(; 'alpine',; ['echo', 'test'],; attributes={'foo': 'bar'},; callback=server.url_for('/test')); j.wait(); ; > status = d['status']; E KeyError: 'status'; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5817
https://github.com/hail-is/hail/issues/5817:368,Testability,test,test,368,"I see this happening quite a bit. ```; =================================== FAILURES ===================================; ______________________________ Test.test_callback ______________________________. self = <test.test_batch.Test testMethod=test_callback>. def test_callback(self):; app = Flask('test-client'); ; d = {}; ; @app.route('/test', methods=['POST']); def test():; d['status'] = request.get_json(); return Response(status=200); ; server = ServerThread(app); try:; server.start(); ; j = self.batch.create_job(; 'alpine',; ['echo', 'test'],; attributes={'foo': 'bar'},; callback=server.url_for('/test')); j.wait(); ; > status = d['status']; E KeyError: 'status'; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5817
https://github.com/hail-is/hail/issues/5817:543,Testability,test,test,543,"I see this happening quite a bit. ```; =================================== FAILURES ===================================; ______________________________ Test.test_callback ______________________________. self = <test.test_batch.Test testMethod=test_callback>. def test_callback(self):; app = Flask('test-client'); ; d = {}; ; @app.route('/test', methods=['POST']); def test():; d['status'] = request.get_json(); return Response(status=200); ; server = ServerThread(app); try:; server.start(); ; j = self.batch.create_job(; 'alpine',; ['echo', 'test'],; attributes={'foo': 'bar'},; callback=server.url_for('/test')); j.wait(); ; > status = d['status']; E KeyError: 'status'; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5817
https://github.com/hail-is/hail/issues/5817:606,Testability,test,test,606,"I see this happening quite a bit. ```; =================================== FAILURES ===================================; ______________________________ Test.test_callback ______________________________. self = <test.test_batch.Test testMethod=test_callback>. def test_callback(self):; app = Flask('test-client'); ; d = {}; ; @app.route('/test', methods=['POST']); def test():; d['status'] = request.get_json(); return Response(status=200); ; server = ServerThread(app); try:; server.start(); ; j = self.batch.create_job(; 'alpine',; ['echo', 'test'],; attributes={'foo': 'bar'},; callback=server.url_for('/test')); j.wait(); ; > status = d['status']; E KeyError: 'status'; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5817
https://github.com/hail-is/hail/issues/5820:358,Integrability,depend,depending,358,"JGSCM has inconsistent handling of buckets, folders. Because of this, combined with the fact that both folder and files are blobs in the Google Cloud storage world, that Jupyter's ContentsManager class strips slashes](https://jupyter-notebook.readthedocs.io/en/stable/extending/contents.html#api-paths), and that JGSCM behaves differently in the root folder depending on whether or not `default_path` is set (where without it, the root folder is effectively a listing of buckets, and ""folders"" created within are buckets rather than folder-blobs), means that some operations fail. Currently this appears to be seen only with file moving operations, but may occur under other circumstances. cc @cseed.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5820
https://github.com/hail-is/hail/issues/5820:268,Modifiability,extend,extending,268,"JGSCM has inconsistent handling of buckets, folders. Because of this, combined with the fact that both folder and files are blobs in the Google Cloud storage world, that Jupyter's ContentsManager class strips slashes](https://jupyter-notebook.readthedocs.io/en/stable/extending/contents.html#api-paths), and that JGSCM behaves differently in the root folder depending on whether or not `default_path` is set (where without it, the root folder is effectively a listing of buckets, and ""folders"" created within are buckets rather than folder-blobs), means that some operations fail. Currently this appears to be seen only with file moving operations, but may occur under other circumstances. cc @cseed.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5820
https://github.com/hail-is/hail/issues/5822:132,Deployability,deploy,deployment,132,From Cotton:. Items to be address:. - [ ] Recursively make push the jupyter image and embed its entire hash in your Docker image or deployment; - [ ] Remove unused stuff for building images. cc @cseed,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5822
https://github.com/hail-is/hail/issues/5822:103,Security,hash,hash,103,From Cotton:. Items to be address:. - [ ] Recursively make push the jupyter image and embed its entire hash in your Docker image or deployment; - [ ] Remove unused stuff for building images. cc @cseed,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5822
https://github.com/hail-is/hail/pull/5824:0,Integrability,Depend,Depends,0,Depends on #5791 and #5809,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5824
https://github.com/hail-is/hail/pull/5825:284,Performance,load,loads,284,"YAML has an arbitrary extension mechanism. pyYAML defines a python extension that lets you create arbitrary python objets. This is clearly a huge security vulnerability. Apparently, pyYAML, by default, enables this extension (rather than just parsing vanilla YAML, 🤦‍♀️). `safe_load` loads vanilla YAML without the gaping security hole. I was getting warnings about this when testing.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5825
https://github.com/hail-is/hail/pull/5825:146,Security,secur,security,146,"YAML has an arbitrary extension mechanism. pyYAML defines a python extension that lets you create arbitrary python objets. This is clearly a huge security vulnerability. Apparently, pyYAML, by default, enables this extension (rather than just parsing vanilla YAML, 🤦‍♀️). `safe_load` loads vanilla YAML without the gaping security hole. I was getting warnings about this when testing.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5825
https://github.com/hail-is/hail/pull/5825:322,Security,secur,security,322,"YAML has an arbitrary extension mechanism. pyYAML defines a python extension that lets you create arbitrary python objets. This is clearly a huge security vulnerability. Apparently, pyYAML, by default, enables this extension (rather than just parsing vanilla YAML, 🤦‍♀️). `safe_load` loads vanilla YAML without the gaping security hole. I was getting warnings about this when testing.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5825
https://github.com/hail-is/hail/pull/5825:376,Testability,test,testing,376,"YAML has an arbitrary extension mechanism. pyYAML defines a python extension that lets you create arbitrary python objets. This is clearly a huge security vulnerability. Apparently, pyYAML, by default, enables this extension (rather than just parsing vanilla YAML, 🤦‍♀️). `safe_load` loads vanilla YAML without the gaping security hole. I was getting warnings about this when testing.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5825
https://github.com/hail-is/hail/pull/5825:131,Usability,clear,clearly,131,"YAML has an arbitrary extension mechanism. pyYAML defines a python extension that lets you create arbitrary python objets. This is clearly a huge security vulnerability. Apparently, pyYAML, by default, enables this extension (rather than just parsing vanilla YAML, 🤦‍♀️). `safe_load` loads vanilla YAML without the gaping security hole. I was getting warnings about this when testing.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5825
https://github.com/hail-is/hail/pull/5826:175,Deployability,PATCH,PATCH,175,stacked on: https://github.com/hail-is/hail/pull/5812 (which should hopefully go in soon?). simplified aioclient (added HTTP verb helper methods); changed cancel and close to PATCH endpoints; client side Batch object carries attributes,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5826
https://github.com/hail-is/hail/pull/5826:92,Usability,simpl,simplified,92,stacked on: https://github.com/hail-is/hail/pull/5812 (which should hopefully go in soon?). simplified aioclient (added HTTP verb helper methods); changed cancel and close to PATCH endpoints; client side Batch object carries attributes,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5826
https://github.com/hail-is/hail/pull/5827:481,Deployability,update,updated,481,"Notebook2 was _literally_ unusable (no favicon). Instead of copying and pasting the favicon link 5 times, I also extracted out the shared elements into a template, and extended it in all other views. How this works:; `layout.html`: contains all shared elements, and marks places where children can insert content (`{% block title %}{% endblock %}`, `{% block head %}{% endblock %}`, `{% block content %}{% endblock %}`). Every other file extends this. The 2 templates that weren't updated (admin-login.html, and workers.html) are placeholders from notebook1 that haven't been updated for notebook 2 yet; they should work, but don't use notebook2 styles, and therefore don't have shared elements to wrap in layout.html. This all works. cc @cseed, @jigold, @danking, @konradjk",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5827
https://github.com/hail-is/hail/pull/5827:576,Deployability,update,updated,576,"Notebook2 was _literally_ unusable (no favicon). Instead of copying and pasting the favicon link 5 times, I also extracted out the shared elements into a template, and extended it in all other views. How this works:; `layout.html`: contains all shared elements, and marks places where children can insert content (`{% block title %}{% endblock %}`, `{% block head %}{% endblock %}`, `{% block content %}{% endblock %}`). Every other file extends this. The 2 templates that weren't updated (admin-login.html, and workers.html) are placeholders from notebook1 that haven't been updated for notebook 2 yet; they should work, but don't use notebook2 styles, and therefore don't have shared elements to wrap in layout.html. This all works. cc @cseed, @jigold, @danking, @konradjk",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5827
https://github.com/hail-is/hail/pull/5827:698,Integrability,wrap,wrap,698,"Notebook2 was _literally_ unusable (no favicon). Instead of copying and pasting the favicon link 5 times, I also extracted out the shared elements into a template, and extended it in all other views. How this works:; `layout.html`: contains all shared elements, and marks places where children can insert content (`{% block title %}{% endblock %}`, `{% block head %}{% endblock %}`, `{% block content %}{% endblock %}`). Every other file extends this. The 2 templates that weren't updated (admin-login.html, and workers.html) are placeholders from notebook1 that haven't been updated for notebook 2 yet; they should work, but don't use notebook2 styles, and therefore don't have shared elements to wrap in layout.html. This all works. cc @cseed, @jigold, @danking, @konradjk",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5827
https://github.com/hail-is/hail/pull/5827:168,Modifiability,extend,extended,168,"Notebook2 was _literally_ unusable (no favicon). Instead of copying and pasting the favicon link 5 times, I also extracted out the shared elements into a template, and extended it in all other views. How this works:; `layout.html`: contains all shared elements, and marks places where children can insert content (`{% block title %}{% endblock %}`, `{% block head %}{% endblock %}`, `{% block content %}{% endblock %}`). Every other file extends this. The 2 templates that weren't updated (admin-login.html, and workers.html) are placeholders from notebook1 that haven't been updated for notebook 2 yet; they should work, but don't use notebook2 styles, and therefore don't have shared elements to wrap in layout.html. This all works. cc @cseed, @jigold, @danking, @konradjk",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5827
https://github.com/hail-is/hail/pull/5827:438,Modifiability,extend,extends,438,"Notebook2 was _literally_ unusable (no favicon). Instead of copying and pasting the favicon link 5 times, I also extracted out the shared elements into a template, and extended it in all other views. How this works:; `layout.html`: contains all shared elements, and marks places where children can insert content (`{% block title %}{% endblock %}`, `{% block head %}{% endblock %}`, `{% block content %}{% endblock %}`). Every other file extends this. The 2 templates that weren't updated (admin-login.html, and workers.html) are placeholders from notebook1 that haven't been updated for notebook 2 yet; they should work, but don't use notebook2 styles, and therefore don't have shared elements to wrap in layout.html. This all works. cc @cseed, @jigold, @danking, @konradjk",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5827
https://github.com/hail-is/hail/pull/5827:496,Testability,log,login,496,"Notebook2 was _literally_ unusable (no favicon). Instead of copying and pasting the favicon link 5 times, I also extracted out the shared elements into a template, and extended it in all other views. How this works:; `layout.html`: contains all shared elements, and marks places where children can insert content (`{% block title %}{% endblock %}`, `{% block head %}{% endblock %}`, `{% block content %}{% endblock %}`). Every other file extends this. The 2 templates that weren't updated (admin-login.html, and workers.html) are placeholders from notebook1 that haven't been updated for notebook 2 yet; they should work, but don't use notebook2 styles, and therefore don't have shared elements to wrap in layout.html. This all works. cc @cseed, @jigold, @danking, @konradjk",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5827
https://github.com/hail-is/hail/pull/5829:258,Deployability,deploy,deploy,258,"This fixes a few bugs, namely $(3) being not quite right, and the base image grep | head -n 1 ; sed not working (at least on some systems) due to --color=none missing. This is fixed by moving to more explicit rule declarations (at some verbosity cost). make deploy/push is also now CI specific. Caching behavior should match the last commit.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5829
https://github.com/hail-is/hail/pull/5834:63,Performance,optimiz,optimize,63,"Unused and incorrect -- if this method were used, it would not optimize; any IRs in the DAG, which may contain relational or value IRs.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5834
https://github.com/hail-is/hail/pull/5835:26,Testability,test,test,26,"Fixes #5830. Jon, can you test this?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5835
https://github.com/hail-is/hail/pull/5837:392,Energy Efficiency,allocate,allocate,392,"- Add Hadoop config as additional parameter to c++ compiled function; - Add C++ `HadoopConfig` wrapper and method to create an output stream; - Implement `NDArrayWrite` node and emit on backend. ### Notes; - `RichHadoopConfiguration` is an `AnyVal`, which is not implemented as a Java `Object`. In order to use it with JNI (and our `ObjectArray` pattern), I had to cast it to an `AnyRef` and allocate it as an object. There might be a way to call `AnyVal` methods and not do the allocation but haven't found one yet.; - Our current system doesn't support compiled functions that don't return values. I made it support void-type IRs but for now enforced that they will return an int (returned 0 after a successful write).; - All our bufferspecs are blocking which won't work for a numpy-compatible encoding. Going to follow-up on this PR with a simple non-blocking spec. cc: @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5837
https://github.com/hail-is/hail/pull/5837:95,Integrability,wrap,wrapper,95,"- Add Hadoop config as additional parameter to c++ compiled function; - Add C++ `HadoopConfig` wrapper and method to create an output stream; - Implement `NDArrayWrite` node and emit on backend. ### Notes; - `RichHadoopConfiguration` is an `AnyVal`, which is not implemented as a Java `Object`. In order to use it with JNI (and our `ObjectArray` pattern), I had to cast it to an `AnyRef` and allocate it as an object. There might be a way to call `AnyVal` methods and not do the allocation but haven't found one yet.; - Our current system doesn't support compiled functions that don't return values. I made it support void-type IRs but for now enforced that they will return an int (returned 0 after a successful write).; - All our bufferspecs are blocking which won't work for a numpy-compatible encoding. Going to follow-up on this PR with a simple non-blocking spec. cc: @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5837
https://github.com/hail-is/hail/pull/5837:13,Modifiability,config,config,13,"- Add Hadoop config as additional parameter to c++ compiled function; - Add C++ `HadoopConfig` wrapper and method to create an output stream; - Implement `NDArrayWrite` node and emit on backend. ### Notes; - `RichHadoopConfiguration` is an `AnyVal`, which is not implemented as a Java `Object`. In order to use it with JNI (and our `ObjectArray` pattern), I had to cast it to an `AnyRef` and allocate it as an object. There might be a way to call `AnyVal` methods and not do the allocation but haven't found one yet.; - Our current system doesn't support compiled functions that don't return values. I made it support void-type IRs but for now enforced that they will return an int (returned 0 after a successful write).; - All our bufferspecs are blocking which won't work for a numpy-compatible encoding. Going to follow-up on this PR with a simple non-blocking spec. cc: @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5837
https://github.com/hail-is/hail/pull/5837:844,Usability,simpl,simple,844,"- Add Hadoop config as additional parameter to c++ compiled function; - Add C++ `HadoopConfig` wrapper and method to create an output stream; - Implement `NDArrayWrite` node and emit on backend. ### Notes; - `RichHadoopConfiguration` is an `AnyVal`, which is not implemented as a Java `Object`. In order to use it with JNI (and our `ObjectArray` pattern), I had to cast it to an `AnyRef` and allocate it as an object. There might be a way to call `AnyVal` methods and not do the allocation but haven't found one yet.; - Our current system doesn't support compiled functions that don't return values. I made it support void-type IRs but for now enforced that they will return an int (returned 0 after a successful write).; - All our bufferspecs are blocking which won't work for a numpy-compatible encoding. Going to follow-up on this PR with a simple non-blocking spec. cc: @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5837
https://github.com/hail-is/hail/pull/5842:4,Testability,test,testing,4,"For testing ci2, not assigned yet.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5842
https://github.com/hail-is/hail/pull/5844:55,Availability,alive,alive,55,"batch checks authentication for all endpoints except `/alive`. Two endpoints are now considered ""internal"" and must originate from the batch server itself. Summary of Changes; - hailjwt is used by batch to verify cookies; - all batch endpoints except `/alive` require a valid cookie or are internal; - a make target `test-deploy` and associated files for testing a deploy of batch; - update pipeline to use batch and users",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5844
https://github.com/hail-is/hail/pull/5844:253,Availability,alive,alive,253,"batch checks authentication for all endpoints except `/alive`. Two endpoints are now considered ""internal"" and must originate from the batch server itself. Summary of Changes; - hailjwt is used by batch to verify cookies; - all batch endpoints except `/alive` require a valid cookie or are internal; - a make target `test-deploy` and associated files for testing a deploy of batch; - update pipeline to use batch and users",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5844
https://github.com/hail-is/hail/pull/5844:322,Deployability,deploy,deploy,322,"batch checks authentication for all endpoints except `/alive`. Two endpoints are now considered ""internal"" and must originate from the batch server itself. Summary of Changes; - hailjwt is used by batch to verify cookies; - all batch endpoints except `/alive` require a valid cookie or are internal; - a make target `test-deploy` and associated files for testing a deploy of batch; - update pipeline to use batch and users",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5844
https://github.com/hail-is/hail/pull/5844:365,Deployability,deploy,deploy,365,"batch checks authentication for all endpoints except `/alive`. Two endpoints are now considered ""internal"" and must originate from the batch server itself. Summary of Changes; - hailjwt is used by batch to verify cookies; - all batch endpoints except `/alive` require a valid cookie or are internal; - a make target `test-deploy` and associated files for testing a deploy of batch; - update pipeline to use batch and users",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5844
https://github.com/hail-is/hail/pull/5844:384,Deployability,update,update,384,"batch checks authentication for all endpoints except `/alive`. Two endpoints are now considered ""internal"" and must originate from the batch server itself. Summary of Changes; - hailjwt is used by batch to verify cookies; - all batch endpoints except `/alive` require a valid cookie or are internal; - a make target `test-deploy` and associated files for testing a deploy of batch; - update pipeline to use batch and users",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5844
https://github.com/hail-is/hail/pull/5844:391,Deployability,pipeline,pipeline,391,"batch checks authentication for all endpoints except `/alive`. Two endpoints are now considered ""internal"" and must originate from the batch server itself. Summary of Changes; - hailjwt is used by batch to verify cookies; - all batch endpoints except `/alive` require a valid cookie or are internal; - a make target `test-deploy` and associated files for testing a deploy of batch; - update pipeline to use batch and users",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5844
https://github.com/hail-is/hail/pull/5844:13,Security,authenticat,authentication,13,"batch checks authentication for all endpoints except `/alive`. Two endpoints are now considered ""internal"" and must originate from the batch server itself. Summary of Changes; - hailjwt is used by batch to verify cookies; - all batch endpoints except `/alive` require a valid cookie or are internal; - a make target `test-deploy` and associated files for testing a deploy of batch; - update pipeline to use batch and users",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5844
https://github.com/hail-is/hail/pull/5844:317,Testability,test,test-deploy,317,"batch checks authentication for all endpoints except `/alive`. Two endpoints are now considered ""internal"" and must originate from the batch server itself. Summary of Changes; - hailjwt is used by batch to verify cookies; - all batch endpoints except `/alive` require a valid cookie or are internal; - a make target `test-deploy` and associated files for testing a deploy of batch; - update pipeline to use batch and users",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5844
https://github.com/hail-is/hail/pull/5844:355,Testability,test,testing,355,"batch checks authentication for all endpoints except `/alive`. Two endpoints are now considered ""internal"" and must originate from the batch server itself. Summary of Changes; - hailjwt is used by batch to verify cookies; - all batch endpoints except `/alive` require a valid cookie or are internal; - a make target `test-deploy` and associated files for testing a deploy of batch; - update pipeline to use batch and users",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5844
https://github.com/hail-is/hail/issues/5845:72,Deployability,deploy,deployed,72,"Right now in master, the batch database gets cleared each time batch is deployed. Before we can remove this, we need to write all job task logs to GCS and insert the URI into the database. Otherwise, batch will try and read the logs for a previous job and not find them on the node.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5845
https://github.com/hail-is/hail/issues/5845:139,Testability,log,logs,139,"Right now in master, the batch database gets cleared each time batch is deployed. Before we can remove this, we need to write all job task logs to GCS and insert the URI into the database. Otherwise, batch will try and read the logs for a previous job and not find them on the node.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5845
https://github.com/hail-is/hail/issues/5845:228,Testability,log,logs,228,"Right now in master, the batch database gets cleared each time batch is deployed. Before we can remove this, we need to write all job task logs to GCS and insert the URI into the database. Otherwise, batch will try and read the logs for a previous job and not find them on the node.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5845
https://github.com/hail-is/hail/issues/5845:45,Usability,clear,cleared,45,"Right now in master, the batch database gets cleared each time batch is deployed. Before we can remove this, we need to write all job task logs to GCS and insert the URI into the database. Otherwise, batch will try and read the logs for a previous job and not find them on the node.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5845
https://github.com/hail-is/hail/issues/5846:115,Availability,error,error,115,"hail version: 0.2.12-13681278eb89. When running `x.aggregate(hl.agg.hist(x.FS, 0, 200, 100))`, I got the following error. First setting `-0.0` values to `0.0` fixed the problem. ```; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-56-b39eaa27c3df> in <module>; 9 ); 10 ); ---> 11 hist_data = x.aggregate(hl.agg.hist(x.FS, 0, 200, 100)); 12 # show(hl.plot.histogram(x.FS)). </opt/conda/lib/python3.6/site-packages/decorator.py:decorator-gen-931> in aggregate(self, expr, _localize). /home/hail/hail.zip/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 559 def wrapper(__original_func, *args, **kwargs):; 560 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 561 return __original_func(*args_, **kwargs_); 562 ; 563 return wrapper. /home/hail/hail.zip/hail/table.py in aggregate(self, expr, _localize); 1138 ; 1139 if _localize:; -> 1140 return Env.backend().execute(agg_ir); 1141 else:; 1142 return construct_expr(agg_ir, expr.dtype). /home/hail/hail.zip/hail/backend/backend.py in execute(self, ir); 91 return ir.typ._from_json(; 92 Env.hail().backend.spark.SparkBackend.executeJSON(; ---> 93 self._to_java_ir(ir))); 94 ; 95 def value_type(self, ir):. /usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 226 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 227 'Hail version: %s\n'; --> 228 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 229 except pyspark.sql.utils.CapturedException as e:; 230 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: HailException: found out of bounds index -1; Resul",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5846
https://github.com/hail-is/hail/issues/5846:1755,Availability,Error,Error,1755," args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 561 return __original_func(*args_, **kwargs_); 562 ; 563 return wrapper. /home/hail/hail.zip/hail/table.py in aggregate(self, expr, _localize); 1138 ; 1139 if _localize:; -> 1140 return Env.backend().execute(agg_ir); 1141 else:; 1142 return construct_expr(agg_ir, expr.dtype). /home/hail/hail.zip/hail/backend/backend.py in execute(self, ir); 91 return ir.typ._from_json(; 92 Env.hail().backend.spark.SparkBackend.executeJSON(; ---> 93 self._to_java_ir(ir))); 94 ; 95 def value_type(self, ir):. /usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 226 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 227 'Hail version: %s\n'; --> 228 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 229 except pyspark.sql.utils.CapturedException as e:; 230 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: HailException: found out of bounds index -1; Resulted from trying to merge -0.0; Indices are [0.0, 2.0, 4.0, 6.0, 8.0, 10.0, 12.0, 14.0, 16.0, 18.0, 20.0, 22.0, 24.0, 26.0, 28.0, 30.0, 32.0, 34.0, 36.0, 38.0, 40.0, 42.0, 44.0, 46.0, 48.0, 50.0, 52.0, 54.0, 56.0, 58.0, 60.0, 62.0, 64.0, 66.0, 68.0, 70.0, 72.0, 74.0, 76.0, 78.0, 80.0, 82.0, 84.0, 86.0, 88.0, 90.0, 92.0, 94.0, 96.0, 98.0, 100.0, 102.0, 104.0, 106.0, 108.0, 110.0, 112.0, 114.0, 116.0, 118.0, 120.0, 122.0, 124.0, 126.0, 128.0, 130.0, 132.0, 134.0, 136.0, 138.0, 140.0, 142.0, 144.0, 146.0, 148.0, 150.0, 152.0, 154.0, 156.0, 158.0, 160.0, 162.0, 164.0, 166.0, 168.0, 170.0, 172.0, 174.0, 176.0, 178.0, 180.0, 182.0, 184.0, 186.0, 188.0, 190.0, 192.0, 194.0, 196.0, 198.0, 200.0]; Binary search index",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5846
https://github.com/hail-is/hail/issues/5846:2803,Availability,failure,failure,2803,".__version__, deepest)) from None; 229 except pyspark.sql.utils.CapturedException as e:; 230 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: HailException: found out of bounds index -1; Resulted from trying to merge -0.0; Indices are [0.0, 2.0, 4.0, 6.0, 8.0, 10.0, 12.0, 14.0, 16.0, 18.0, 20.0, 22.0, 24.0, 26.0, 28.0, 30.0, 32.0, 34.0, 36.0, 38.0, 40.0, 42.0, 44.0, 46.0, 48.0, 50.0, 52.0, 54.0, 56.0, 58.0, 60.0, 62.0, 64.0, 66.0, 68.0, 70.0, 72.0, 74.0, 76.0, 78.0, 80.0, 82.0, 84.0, 86.0, 88.0, 90.0, 92.0, 94.0, 96.0, 98.0, 100.0, 102.0, 104.0, 106.0, 108.0, 110.0, 112.0, 114.0, 116.0, 118.0, 120.0, 122.0, 124.0, 126.0, 128.0, 130.0, 132.0, 134.0, 136.0, 138.0, 140.0, 142.0, 144.0, 146.0, 148.0, 150.0, 152.0, 154.0, 156.0, 158.0, 160.0, 162.0, 164.0, 166.0, 168.0, 170.0, 172.0, 174.0, 176.0, 178.0, 180.0, 182.0, 184.0, 186.0, 188.0, 190.0, 192.0, 194.0, 196.0, 198.0, 200.0]; Binary search index was -1. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 24.0 failed 20 times, most recent failure: Lost task 0.19 in stage 24.0 (TID 1813, lfrani-sw-hqb8.c.broad-mpg-gnomad.internal, executor 159): is.hail.utils.HailException: found out of bounds index -1; Resulted from trying to merge -0.0; Indices are [0.0, 2.0, 4.0, 6.0, 8.0, 10.0, 12.0, 14.0, 16.0, 18.0, 20.0, 22.0, 24.0, 26.0, 28.0, 30.0, 32.0, 34.0, 36.0, 38.0, 40.0, 42.0, 44.0, 46.0, 48.0, 50.0, 52.0, 54.0, 56.0, 58.0, 60.0, 62.0, 64.0, 66.0, 68.0, 70.0, 72.0, 74.0, 76.0, 78.0, 80.0, 82.0, 84.0, 86.0, 88.0, 90.0, 92.0, 94.0, 96.0, 98.0, 100.0, 102.0, 104.0, 106.0, 108.0, 110.0, 112.0, 114.0, 116.0, 118.0, 120.0, 122.0, 124.0, 126.0, 128.0, 130.0, 132.0, 134.0, 136.0, 138.0, 140.0, 142.0, 144.0, 146.0, 148.0, 150.0, 152.0, 154.0, 156.0, 158.0, 160.0, 162.0, 164.0, 166.0, 168.0, 170.0, 172.0, 174.0, 176.0, 178.0, 180.0, 182.0, 184.0, 186.0, 188.0, 190.0, 192.0, 194.0, 196.0, 198.0, 200.0]; Binary search index was -1; 	at is.hail.utils.ErrorHandling$class.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5846
https://github.com/hail-is/hail/issues/5846:2862,Availability,failure,failure,2862,"ls.CapturedException as e:; 230 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: HailException: found out of bounds index -1; Resulted from trying to merge -0.0; Indices are [0.0, 2.0, 4.0, 6.0, 8.0, 10.0, 12.0, 14.0, 16.0, 18.0, 20.0, 22.0, 24.0, 26.0, 28.0, 30.0, 32.0, 34.0, 36.0, 38.0, 40.0, 42.0, 44.0, 46.0, 48.0, 50.0, 52.0, 54.0, 56.0, 58.0, 60.0, 62.0, 64.0, 66.0, 68.0, 70.0, 72.0, 74.0, 76.0, 78.0, 80.0, 82.0, 84.0, 86.0, 88.0, 90.0, 92.0, 94.0, 96.0, 98.0, 100.0, 102.0, 104.0, 106.0, 108.0, 110.0, 112.0, 114.0, 116.0, 118.0, 120.0, 122.0, 124.0, 126.0, 128.0, 130.0, 132.0, 134.0, 136.0, 138.0, 140.0, 142.0, 144.0, 146.0, 148.0, 150.0, 152.0, 154.0, 156.0, 158.0, 160.0, 162.0, 164.0, 166.0, 168.0, 170.0, 172.0, 174.0, 176.0, 178.0, 180.0, 182.0, 184.0, 186.0, 188.0, 190.0, 192.0, 194.0, 196.0, 198.0, 200.0]; Binary search index was -1. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 24.0 failed 20 times, most recent failure: Lost task 0.19 in stage 24.0 (TID 1813, lfrani-sw-hqb8.c.broad-mpg-gnomad.internal, executor 159): is.hail.utils.HailException: found out of bounds index -1; Resulted from trying to merge -0.0; Indices are [0.0, 2.0, 4.0, 6.0, 8.0, 10.0, 12.0, 14.0, 16.0, 18.0, 20.0, 22.0, 24.0, 26.0, 28.0, 30.0, 32.0, 34.0, 36.0, 38.0, 40.0, 42.0, 44.0, 46.0, 48.0, 50.0, 52.0, 54.0, 56.0, 58.0, 60.0, 62.0, 64.0, 66.0, 68.0, 70.0, 72.0, 74.0, 76.0, 78.0, 80.0, 82.0, 84.0, 86.0, 88.0, 90.0, 92.0, 94.0, 96.0, 98.0, 100.0, 102.0, 104.0, 106.0, 108.0, 110.0, 112.0, 114.0, 116.0, 118.0, 120.0, 122.0, 124.0, 126.0, 128.0, 130.0, 132.0, 134.0, 136.0, 138.0, 140.0, 142.0, 144.0, 146.0, 148.0, 150.0, 152.0, 154.0, 156.0, 158.0, 160.0, 162.0, 164.0, 166.0, 168.0, 170.0, 172.0, 174.0, 176.0, 178.0, 180.0, 182.0, 184.0, 186.0, 188.0, 190.0, 192.0, 194.0, 196.0, 198.0, 200.0]; Binary search index was -1; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fat",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5846
https://github.com/hail-is/hail/issues/5846:3777,Availability,Error,ErrorHandling,3777,"ed due to stage failure: Task 0 in stage 24.0 failed 20 times, most recent failure: Lost task 0.19 in stage 24.0 (TID 1813, lfrani-sw-hqb8.c.broad-mpg-gnomad.internal, executor 159): is.hail.utils.HailException: found out of bounds index -1; Resulted from trying to merge -0.0; Indices are [0.0, 2.0, 4.0, 6.0, 8.0, 10.0, 12.0, 14.0, 16.0, 18.0, 20.0, 22.0, 24.0, 26.0, 28.0, 30.0, 32.0, 34.0, 36.0, 38.0, 40.0, 42.0, 44.0, 46.0, 48.0, 50.0, 52.0, 54.0, 56.0, 58.0, 60.0, 62.0, 64.0, 66.0, 68.0, 70.0, 72.0, 74.0, 76.0, 78.0, 80.0, 82.0, 84.0, 86.0, 88.0, 90.0, 92.0, 94.0, 96.0, 98.0, 100.0, 102.0, 104.0, 106.0, 108.0, 110.0, 112.0, 114.0, 116.0, 118.0, 120.0, 122.0, 124.0, 126.0, 128.0, 130.0, 132.0, 134.0, 136.0, 138.0, 140.0, 142.0, 144.0, 146.0, 148.0, 150.0, 152.0, 154.0, 156.0, 158.0, 160.0, 162.0, 164.0, 166.0, 168.0, 170.0, 172.0, 174.0, 176.0, 178.0, 180.0, 182.0, 184.0, 186.0, 188.0, 190.0, 192.0, 194.0, 196.0, 198.0, 200.0]; Binary search index was -1; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.stats.HistogramCombiner.merge(HistogramCombiner.scala:42); 	at is.hail.annotations.aggregators.RegionValueHistogramAggregator.seqOp(RegionValueHistogramAggregator.scala:31); 	at is.hail.codegen.generated.C95.apply(Unknown Source); 	at is.hail.codegen.generated.C95.apply(Unknown Source); 	at is.hail.expr.ir.Interpret$$anonfun$27.apply(Interpret.scala:809); 	at is.hail.expr.ir.Interpret$$anonfun$27.apply(Interpret.scala:808); 	at is.hail.rvd.RVD$$anonfun$20$$anonfun$apply$11.apply(RVD.scala:551); 	at is.hail.rvd.RVD$$anonfun$20$$anonfun$apply$11.apply(RVD.scala:550); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.rvd.RVD$$anonfun$20.apply(RVD.scala:550); 	at is.hail.rvd.RVD$$anonfun$20.apply(RVD.scala:547); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$appl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5846
https://github.com/hail-is/hail/issues/5846:3803,Availability,Error,ErrorHandling,3803,"ure: Task 0 in stage 24.0 failed 20 times, most recent failure: Lost task 0.19 in stage 24.0 (TID 1813, lfrani-sw-hqb8.c.broad-mpg-gnomad.internal, executor 159): is.hail.utils.HailException: found out of bounds index -1; Resulted from trying to merge -0.0; Indices are [0.0, 2.0, 4.0, 6.0, 8.0, 10.0, 12.0, 14.0, 16.0, 18.0, 20.0, 22.0, 24.0, 26.0, 28.0, 30.0, 32.0, 34.0, 36.0, 38.0, 40.0, 42.0, 44.0, 46.0, 48.0, 50.0, 52.0, 54.0, 56.0, 58.0, 60.0, 62.0, 64.0, 66.0, 68.0, 70.0, 72.0, 74.0, 76.0, 78.0, 80.0, 82.0, 84.0, 86.0, 88.0, 90.0, 92.0, 94.0, 96.0, 98.0, 100.0, 102.0, 104.0, 106.0, 108.0, 110.0, 112.0, 114.0, 116.0, 118.0, 120.0, 122.0, 124.0, 126.0, 128.0, 130.0, 132.0, 134.0, 136.0, 138.0, 140.0, 142.0, 144.0, 146.0, 148.0, 150.0, 152.0, 154.0, 156.0, 158.0, 160.0, 162.0, 164.0, 166.0, 168.0, 170.0, 172.0, 174.0, 176.0, 178.0, 180.0, 182.0, 184.0, 186.0, 188.0, 190.0, 192.0, 194.0, 196.0, 198.0, 200.0]; Binary search index was -1; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.stats.HistogramCombiner.merge(HistogramCombiner.scala:42); 	at is.hail.annotations.aggregators.RegionValueHistogramAggregator.seqOp(RegionValueHistogramAggregator.scala:31); 	at is.hail.codegen.generated.C95.apply(Unknown Source); 	at is.hail.codegen.generated.C95.apply(Unknown Source); 	at is.hail.expr.ir.Interpret$$anonfun$27.apply(Interpret.scala:809); 	at is.hail.expr.ir.Interpret$$anonfun$27.apply(Interpret.scala:808); 	at is.hail.rvd.RVD$$anonfun$20$$anonfun$apply$11.apply(RVD.scala:551); 	at is.hail.rvd.RVD$$anonfun$20$$anonfun$apply$11.apply(RVD.scala:550); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.rvd.RVD$$anonfun$20.apply(RVD.scala:550); 	at is.hail.rvd.RVD$$anonfun$20.apply(RVD.scala:547); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$32.apply(ContextRD",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5846
https://github.com/hail-is/hail/issues/5846:10345,Availability,Error,ErrorHandling,10345,"tCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). is.hail.utils.HailException: found out of bounds index -1; Resulted from trying to merge -0.0; Indices are [0.0, 2.0, 4.0, 6.0, 8.0, 10.0, 12.0, 14.0, 16.0, 18.0, 20.0, 22.0, 24.0, 26.0, 28.0, 30.0, 32.0, 34.0, 36.0, 38.0, 40.0, 42.0, 44.0, 46.0, 48.0, 50.0, 52.0, 54.0, 56.0, 58.0, 60.0, 62.0, 64.0, 66.0, 68.0, 70.0, 72.0, 74.0, 76.0, 78.0, 80.0, 82.0, 84.0, 86.0, 88.0, 90.0, 92.0, 94.0, 96.0, 98.0, 100.0, 102.0, 104.0, 106.0, 108.0, 110.0, 112.0, 114.0, 116.0, 118.0, 120.0, 122.0, 124.0, 126.0, 128.0, 130.0, 132.0, 134.0, 136.0, 138.0, 140.0, 142.0, 144.0, 146.0, 148.0, 150.0, 152.0, 154.0, 156.0, 158.0, 160.0, 162.0, 164.0, 166.0, 168.0, 170.0, 172.0, 174.0, 176.0, 178.0, 180.0, 182.0, 184.0, 186.0, 188.0, 190.0, 192.0, 194.0, 196.0, 198.0, 200.0]; Binary search index was -1; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.stats.HistogramCombiner.merge(HistogramCombiner.scala:42); 	at is.hail.annotations.aggregators.RegionValueHistogramAggregator.seqOp(RegionValueHistogramAggregator.scala:31); 	at is.hail.codegen.generated.C95.apply(Unknown Source); 	at is.hail.codegen.generated.C95.apply(Unknown Source); 	at is.hail.expr.ir.Interpret$$anonfun$27.apply(Interpret.scala:809); 	at is.hail.expr.ir.Interpret$$anonfun$27.apply(Interpret.scala:808); 	at is.hail.rvd.RVD$$anonfun$20$$anonfun$apply$11.apply(RVD.scala:551); 	at is.hail.rvd.RVD$$anonfun$20$$anonfun$apply$11.apply(RVD.scala:550); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.rvd.RVD$$anonfun$20.apply(RVD.scala:550); 	at is.hail.rvd.RVD$$anonfun$20.apply(RVD.scala:547); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$appl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5846
https://github.com/hail-is/hail/issues/5846:10371,Availability,Error,ErrorHandling,10371,"	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). is.hail.utils.HailException: found out of bounds index -1; Resulted from trying to merge -0.0; Indices are [0.0, 2.0, 4.0, 6.0, 8.0, 10.0, 12.0, 14.0, 16.0, 18.0, 20.0, 22.0, 24.0, 26.0, 28.0, 30.0, 32.0, 34.0, 36.0, 38.0, 40.0, 42.0, 44.0, 46.0, 48.0, 50.0, 52.0, 54.0, 56.0, 58.0, 60.0, 62.0, 64.0, 66.0, 68.0, 70.0, 72.0, 74.0, 76.0, 78.0, 80.0, 82.0, 84.0, 86.0, 88.0, 90.0, 92.0, 94.0, 96.0, 98.0, 100.0, 102.0, 104.0, 106.0, 108.0, 110.0, 112.0, 114.0, 116.0, 118.0, 120.0, 122.0, 124.0, 126.0, 128.0, 130.0, 132.0, 134.0, 136.0, 138.0, 140.0, 142.0, 144.0, 146.0, 148.0, 150.0, 152.0, 154.0, 156.0, 158.0, 160.0, 162.0, 164.0, 166.0, 168.0, 170.0, 172.0, 174.0, 176.0, 178.0, 180.0, 182.0, 184.0, 186.0, 188.0, 190.0, 192.0, 194.0, 196.0, 198.0, 200.0]; Binary search index was -1; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.stats.HistogramCombiner.merge(HistogramCombiner.scala:42); 	at is.hail.annotations.aggregators.RegionValueHistogramAggregator.seqOp(RegionValueHistogramAggregator.scala:31); 	at is.hail.codegen.generated.C95.apply(Unknown Source); 	at is.hail.codegen.generated.C95.apply(Unknown Source); 	at is.hail.expr.ir.Interpret$$anonfun$27.apply(Interpret.scala:809); 	at is.hail.expr.ir.Interpret$$anonfun$27.apply(Interpret.scala:808); 	at is.hail.rvd.RVD$$anonfun$20$$anonfun$apply$11.apply(RVD.scala:551); 	at is.hail.rvd.RVD$$anonfun$20$$anonfun$apply$11.apply(RVD.scala:550); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.rvd.RVD$$anonfun$20.apply(RVD.scala:550); 	at is.hail.rvd.RVD$$anonfun$20.apply(RVD.scala:547); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$32.apply(ContextRD",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5846
https://github.com/hail-is/hail/issues/5846:12727,Availability,Error,Error,12727,"); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.utils.package$.singletonElement(package.scala:603); 	at is.hail.rvd.RVD$$anonfun$aggregateWithPartitionOp$1.apply(RVD.scala:558); 	at is.hail.rvd.RVD$$anonfun$aggregateWithPartitionOp$1.apply(RVD.scala:558); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.12-13681278eb89; Error summary: HailException: found out of bounds index -1; Resulted from trying to merge -0.0; Indices are [0.0, 2.0, 4.0, 6.0, 8.0, 10.0, 12.0, 14.0, 16.0, 18.0, 20.0, 22.0, 24.0, 26.0, 28.0, 30.0, 32.0, 34.0, 36.0, 38.0, 40.0, 42.0, 44.0, 46.0, 48.0, 50.0, 52.0, 54.0, 56.0, 58.0, 60.0, 62.0, 64.0, 66.0, 68.0, 70.0, 72.0, 74.0, 76.0, 78.0, 80.0, 82.0, 84.0, 86.0, 88.0, 90.0, 92.0, 94.0, 96.0, 98.0, 100.0, 102.0, 104.0, 106.0, 108.0, 110.0, 112.0, 114.0, 116.0, 118.0, 120.0, 122.0, 124.0, 126.0, 128.0, 130.0, 132.0, 134.0, 136.0, 138.0, 140.0, 142.0, 144.0, 146.0, 148.0, 150.0, 152.0, 154.0, 156.0, 158.0, 160.0, 162.0, 164.0, 166.0, 168.0, 170.0, 172.0, 174.0, 176.0, 178.0, 180.0, 182.0, 184.0, 186.0, 188.0, 190.0, 192.0, 194.0, 196.0, 198.0, 200.0]; Binary search index was -1; ```. -----------------------------------------------------------------------------",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5846
https://github.com/hail-is/hail/issues/5846:5728,Energy Efficiency,schedul,scheduler,5728,xtRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$32.apply(ContextRDD.scala:422); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$32.apply(ContextRDD.scala:422); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.utils.package$.singletonElement(package.scala:603); 	at is.hail.rvd.RVD$$anonfun$aggregateWithPartitionOp$1.apply(RVD.scala:558); 	at is.hail.rvd.RVD$$anonfun$aggregateWithPartitionOp$1.apply(RVD.scala:558); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1575); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1563); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1562); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1562); 	at org.apache.sp,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5846
https://github.com/hail-is/hail/issues/5846:5800,Energy Efficiency,schedul,scheduler,5800,RDD.scala:422); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$32.apply(ContextRDD.scala:422); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.utils.package$.singletonElement(package.scala:603); 	at is.hail.rvd.RVD$$anonfun$aggregateWithPartitionOp$1.apply(RVD.scala:558); 	at is.hail.rvd.RVD$$anonfun$aggregateWithPartitionOp$1.apply(RVD.scala:558); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1575); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1563); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1562); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1562); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGSched,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5846
https://github.com/hail-is/hail/issues/5846:6165,Energy Efficiency,schedul,scheduler,6165,extCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.utils.package$.singletonElement(package.scala:603); 	at is.hail.rvd.RVD$$anonfun$aggregateWithPartitionOp$1.apply(RVD.scala:558); 	at is.hail.rvd.RVD$$anonfun$aggregateWithPartitionOp$1.apply(RVD.scala:558); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1575); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1563); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1562); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1562); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1790); 	at org.apa,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5846
https://github.com/hail-is/hail/issues/5846:6205,Energy Efficiency,schedul,scheduler,6205,anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.utils.package$.singletonElement(package.scala:603); 	at is.hail.rvd.RVD$$anonfun$aggregateWithPartitionOp$1.apply(RVD.scala:558); 	at is.hail.rvd.RVD$$anonfun$aggregateWithPartitionOp$1.apply(RVD.scala:558); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1575); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1563); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1562); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1562); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1790); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5846
https://github.com/hail-is/hail/issues/5846:6304,Energy Efficiency,schedul,scheduler,6304,(Iterator.scala:444); 	at is.hail.utils.package$.singletonElement(package.scala:603); 	at is.hail.rvd.RVD$$anonfun$aggregateWithPartitionOp$1.apply(RVD.scala:558); 	at is.hail.rvd.RVD$$anonfun$aggregateWithPartitionOp$1.apply(RVD.scala:558); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1575); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1563); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1562); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1562); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1790); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1745); 	at org.apache.spark.scheduler.DAGSchedulerEventProces,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5846
https://github.com/hail-is/hail/issues/5846:6402,Energy Efficiency,schedul,scheduler,6402,rvd.RVD$$anonfun$aggregateWithPartitionOp$1.apply(RVD.scala:558); 	at is.hail.rvd.RVD$$anonfun$aggregateWithPartitionOp$1.apply(RVD.scala:558); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1575); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1563); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1562); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1562); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1790); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1745); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1734); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoo,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5846
https://github.com/hail-is/hail/issues/5846:6656,Energy Efficiency,schedul,scheduler,6656,t$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1575); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1563); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1562); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1562); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1790); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1745); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1734); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:619); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); 	at is.hail.rvd.RVD.aggregateWi,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5846
https://github.com/hail-is/hail/issues/5846:6737,Energy Efficiency,schedul,scheduler,6737,ltTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1575); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1563); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1562); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1562); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1790); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1745); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1734); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:619); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); 	at is.hail.rvd.RVD.aggregateWithPartitionOp(RVD.scala:558); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scal,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5846
https://github.com/hail-is/hail/issues/5846:6843,Energy Efficiency,schedul,scheduler,6843,he.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1575); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1563); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1562); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1562); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1790); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1745); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1734); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:619); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); 	at is.hail.rvd.RVD.aggregateWithPartitionOp(RVD.scala:558); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:808); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:87); 	at is.hail.expr.ir.Interpret$.apply(Int,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5846
https://github.com/hail-is/hail/issues/5846:6993,Energy Efficiency,schedul,scheduler,6993,t java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1575); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1563); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1562); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1562); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1790); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1745); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1734); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:619); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); 	at is.hail.rvd.RVD.aggregateWithPartitionOp(RVD.scala:558); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:808); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:87); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:59); 	at is.hail.expr.ir.InterpretNonCompilable$$anonfun$5.apply(InterpretNonCompilable.scala:16); 	at is.hail.expr.ir.InterpretNonCompil,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5846
https://github.com/hail-is/hail/issues/5846:7082,Energy Efficiency,schedul,scheduler,7082,va.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1575); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1563); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1562); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1562); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1790); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1745); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1734); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:619); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); 	at is.hail.rvd.RVD.aggregateWithPartitionOp(RVD.scala:558); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:808); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:87); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:59); 	at is.hail.expr.ir.InterpretNonCompilable$$anonfun$5.apply(InterpretNonCompilable.scala:16); 	at is.hail.expr.ir.InterpretNonCompilable$$anonfun$5.apply(InterpretNonCompilable.scala:16); 	at scala.collection.TraversableL,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5846
https://github.com/hail-is/hail/issues/5846:7180,Energy Efficiency,schedul,scheduler,7180,er.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1575); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1563); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1562); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1562); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1790); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1745); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1734); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:619); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); 	at is.hail.rvd.RVD.aggregateWithPartitionOp(RVD.scala:558); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:808); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:87); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:59); 	at is.hail.expr.ir.InterpretNonCompilable$$anonfun$5.apply(InterpretNonCompilable.scala:16); 	at is.hail.expr.ir.InterpretNonCompilable$$anonfun$5.apply(InterpretNonCompilable.scala:16); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5846
https://github.com/hail-is/hail/issues/5846:7276,Energy Efficiency,schedul,scheduler,7276,; 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1563); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1562); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1562); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1790); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1745); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1734); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:619); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); 	at is.hail.rvd.RVD.aggregateWithPartitionOp(RVD.scala:558); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:808); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:87); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:59); 	at is.hail.expr.ir.InterpretNonCompilable$$anonfun$5.apply(InterpretNonCompilable.scala:16); 	at is.hail.expr.ir.InterpretNonCompilable$$anonfun$5.apply(InterpretNonCompilable.scala:16); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5846
https://github.com/hail-is/hail/issues/5846:7441,Energy Efficiency,schedul,scheduler,7441,.apply(DAGScheduler.scala:1562); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1562); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1790); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1745); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1734); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:619); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); 	at is.hail.rvd.RVD.aggregateWithPartitionOp(RVD.scala:558); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:808); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:87); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:59); 	at is.hail.expr.ir.InterpretNonCompilable$$anonfun$5.apply(InterpretNonCompilable.scala:16); 	at is.hail.expr.ir.InterpretNonCompilable$$anonfun$5.apply(InterpretNonCompilable.scala:16); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLik,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5846
https://github.com/hail-is/hail/issues/5846:12296,Energy Efficiency,schedul,scheduler,12296,"xtRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$32.apply(ContextRDD.scala:422); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$32.apply(ContextRDD.scala:422); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.utils.package$.singletonElement(package.scala:603); 	at is.hail.rvd.RVD$$anonfun$aggregateWithPartitionOp$1.apply(RVD.scala:558); 	at is.hail.rvd.RVD$$anonfun$aggregateWithPartitionOp$1.apply(RVD.scala:558); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.12-13681278eb89; Error summary: HailException: found out of bounds index -1; Resulted from trying to merge -0.0; Indices are [0.0, 2.0, 4.0, 6.0, 8.0, 10.0, 12.0, 14.0, 16.0, 18.0, 20.0, 22.0, 24.0, 26.0, 28.0, 30.0, 32.0, 34.0, 36.0, 38.0, 40.0, 42.0, 44.0, 46.0, 48.0, 50.0, 52.0, 54.0, 56.0, 58.0, 60.0, 62.0, 64.0, 66.0, 68.0, 70.0, 72.0, 74.0, 76.0, 78.0, 80.0, 82.0, 84.0, 86.0, 88.0, 90.0, 92.0, 94.0, 96.0, 98.0, 100.0, 102.0, 104.0, 106.0, 108.0, 110.0, 112.0, 114.0, 116.0, 118.0, 120.0, 122.0, 124.0, 126.0, 128.0, 130.0, 132.0, 134.0, 136.0, 138.0, 140.0, 142.0, 144.0, 146.0, 14",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5846
https://github.com/hail-is/hail/issues/5846:12368,Energy Efficiency,schedul,scheduler,12368,"RDD.scala:422); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$32.apply(ContextRDD.scala:422); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.utils.package$.singletonElement(package.scala:603); 	at is.hail.rvd.RVD$$anonfun$aggregateWithPartitionOp$1.apply(RVD.scala:558); 	at is.hail.rvd.RVD$$anonfun$aggregateWithPartitionOp$1.apply(RVD.scala:558); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.12-13681278eb89; Error summary: HailException: found out of bounds index -1; Resulted from trying to merge -0.0; Indices are [0.0, 2.0, 4.0, 6.0, 8.0, 10.0, 12.0, 14.0, 16.0, 18.0, 20.0, 22.0, 24.0, 26.0, 28.0, 30.0, 32.0, 34.0, 36.0, 38.0, 40.0, 42.0, 44.0, 46.0, 48.0, 50.0, 52.0, 54.0, 56.0, 58.0, 60.0, 62.0, 64.0, 66.0, 68.0, 70.0, 72.0, 74.0, 76.0, 78.0, 80.0, 82.0, 84.0, 86.0, 88.0, 90.0, 92.0, 94.0, 96.0, 98.0, 100.0, 102.0, 104.0, 106.0, 108.0, 110.0, 112.0, 114.0, 116.0, 118.0, 120.0, 122.0, 124.0, 126.0, 128.0, 130.0, 132.0, 134.0, 136.0, 138.0, 140.0, 142.0, 144.0, 146.0, 148.0, 150.0, 152.0, 154.0, 156.0, 158.0, 160.0, 162.0, 164.0, 166.0, 168.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5846
https://github.com/hail-is/hail/issues/5846:619,Integrability,wrap,wrapper,619,"hail version: 0.2.12-13681278eb89. When running `x.aggregate(hl.agg.hist(x.FS, 0, 200, 100))`, I got the following error. First setting `-0.0` values to `0.0` fixed the problem. ```; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-56-b39eaa27c3df> in <module>; 9 ); 10 ); ---> 11 hist_data = x.aggregate(hl.agg.hist(x.FS, 0, 200, 100)); 12 # show(hl.plot.histogram(x.FS)). </opt/conda/lib/python3.6/site-packages/decorator.py:decorator-gen-931> in aggregate(self, expr, _localize). /home/hail/hail.zip/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 559 def wrapper(__original_func, *args, **kwargs):; 560 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 561 return __original_func(*args_, **kwargs_); 562 ; 563 return wrapper. /home/hail/hail.zip/hail/table.py in aggregate(self, expr, _localize); 1138 ; 1139 if _localize:; -> 1140 return Env.backend().execute(agg_ir); 1141 else:; 1142 return construct_expr(agg_ir, expr.dtype). /home/hail/hail.zip/hail/backend/backend.py in execute(self, ir); 91 return ir.typ._from_json(; 92 Env.hail().backend.spark.SparkBackend.executeJSON(; ---> 93 self._to_java_ir(ir))); 94 ; 95 def value_type(self, ir):. /usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 226 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 227 'Hail version: %s\n'; --> 228 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 229 except pyspark.sql.utils.CapturedException as e:; 230 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: HailException: found out of bounds index -1; Resul",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5846
https://github.com/hail-is/hail/issues/5846:670,Integrability,wrap,wrapper,670,"hail version: 0.2.12-13681278eb89. When running `x.aggregate(hl.agg.hist(x.FS, 0, 200, 100))`, I got the following error. First setting `-0.0` values to `0.0` fixed the problem. ```; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-56-b39eaa27c3df> in <module>; 9 ); 10 ); ---> 11 hist_data = x.aggregate(hl.agg.hist(x.FS, 0, 200, 100)); 12 # show(hl.plot.histogram(x.FS)). </opt/conda/lib/python3.6/site-packages/decorator.py:decorator-gen-931> in aggregate(self, expr, _localize). /home/hail/hail.zip/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 559 def wrapper(__original_func, *args, **kwargs):; 560 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 561 return __original_func(*args_, **kwargs_); 562 ; 563 return wrapper. /home/hail/hail.zip/hail/table.py in aggregate(self, expr, _localize); 1138 ; 1139 if _localize:; -> 1140 return Env.backend().execute(agg_ir); 1141 else:; 1142 return construct_expr(agg_ir, expr.dtype). /home/hail/hail.zip/hail/backend/backend.py in execute(self, ir); 91 return ir.typ._from_json(; 92 Env.hail().backend.spark.SparkBackend.executeJSON(; ---> 93 self._to_java_ir(ir))); 94 ; 95 def value_type(self, ir):. /usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 226 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 227 'Hail version: %s\n'; --> 228 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 229 except pyspark.sql.utils.CapturedException as e:; 230 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: HailException: found out of bounds index -1; Resul",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5846
https://github.com/hail-is/hail/issues/5846:876,Integrability,wrap,wrapper,876,"hail version: 0.2.12-13681278eb89. When running `x.aggregate(hl.agg.hist(x.FS, 0, 200, 100))`, I got the following error. First setting `-0.0` values to `0.0` fixed the problem. ```; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-56-b39eaa27c3df> in <module>; 9 ); 10 ); ---> 11 hist_data = x.aggregate(hl.agg.hist(x.FS, 0, 200, 100)); 12 # show(hl.plot.histogram(x.FS)). </opt/conda/lib/python3.6/site-packages/decorator.py:decorator-gen-931> in aggregate(self, expr, _localize). /home/hail/hail.zip/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 559 def wrapper(__original_func, *args, **kwargs):; 560 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 561 return __original_func(*args_, **kwargs_); 562 ; 563 return wrapper. /home/hail/hail.zip/hail/table.py in aggregate(self, expr, _localize); 1138 ; 1139 if _localize:; -> 1140 return Env.backend().execute(agg_ir); 1141 else:; 1142 return construct_expr(agg_ir, expr.dtype). /home/hail/hail.zip/hail/backend/backend.py in execute(self, ir); 91 return ir.typ._from_json(; 92 Env.hail().backend.spark.SparkBackend.executeJSON(; ---> 93 self._to_java_ir(ir))); 94 ; 95 def value_type(self, ir):. /usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 226 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 227 'Hail version: %s\n'; --> 228 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 229 except pyspark.sql.utils.CapturedException as e:; 230 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: HailException: found out of bounds index -1; Resul",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5846
https://github.com/hail-is/hail/issues/5846:5925,Performance,concurren,concurrent,5925,a:422); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.utils.package$.singletonElement(package.scala:603); 	at is.hail.rvd.RVD$$anonfun$aggregateWithPartitionOp$1.apply(RVD.scala:558); 	at is.hail.rvd.RVD$$anonfun$aggregateWithPartitionOp$1.apply(RVD.scala:558); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1575); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1563); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1562); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1562); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5846
https://github.com/hail-is/hail/issues/5846:6010,Performance,concurren,concurrent,6010,extRDD.scala:192); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.utils.package$.singletonElement(package.scala:603); 	at is.hail.rvd.RVD$$anonfun$aggregateWithPartitionOp$1.apply(RVD.scala:558); 	at is.hail.rvd.RVD$$anonfun$aggregateWithPartitionOp$1.apply(RVD.scala:558); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1575); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1563); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1562); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1562); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5846
https://github.com/hail-is/hail/issues/5846:12493,Performance,concurren,concurrent,12493,"a:422); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.utils.package$.singletonElement(package.scala:603); 	at is.hail.rvd.RVD$$anonfun$aggregateWithPartitionOp$1.apply(RVD.scala:558); 	at is.hail.rvd.RVD$$anonfun$aggregateWithPartitionOp$1.apply(RVD.scala:558); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.12-13681278eb89; Error summary: HailException: found out of bounds index -1; Resulted from trying to merge -0.0; Indices are [0.0, 2.0, 4.0, 6.0, 8.0, 10.0, 12.0, 14.0, 16.0, 18.0, 20.0, 22.0, 24.0, 26.0, 28.0, 30.0, 32.0, 34.0, 36.0, 38.0, 40.0, 42.0, 44.0, 46.0, 48.0, 50.0, 52.0, 54.0, 56.0, 58.0, 60.0, 62.0, 64.0, 66.0, 68.0, 70.0, 72.0, 74.0, 76.0, 78.0, 80.0, 82.0, 84.0, 86.0, 88.0, 90.0, 92.0, 94.0, 96.0, 98.0, 100.0, 102.0, 104.0, 106.0, 108.0, 110.0, 112.0, 114.0, 116.0, 118.0, 120.0, 122.0, 124.0, 126.0, 128.0, 130.0, 132.0, 134.0, 136.0, 138.0, 140.0, 142.0, 144.0, 146.0, 148.0, 150.0, 152.0, 154.0, 156.0, 158.0, 160.0, 162.0, 164.0, 166.0, 168.0, 170.0, 172.0, 174.0, 176.0, 178.0, 180.0, 182.0, 184.0, 186.0, 188.0, 190.0, 192.0, 194.0, 196.0, 198.0, 200.0]; Binary sea",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5846
https://github.com/hail-is/hail/issues/5846:12578,Performance,concurren,concurrent,12578,"extRDD.scala:192); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.utils.package$.singletonElement(package.scala:603); 	at is.hail.rvd.RVD$$anonfun$aggregateWithPartitionOp$1.apply(RVD.scala:558); 	at is.hail.rvd.RVD$$anonfun$aggregateWithPartitionOp$1.apply(RVD.scala:558); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.12-13681278eb89; Error summary: HailException: found out of bounds index -1; Resulted from trying to merge -0.0; Indices are [0.0, 2.0, 4.0, 6.0, 8.0, 10.0, 12.0, 14.0, 16.0, 18.0, 20.0, 22.0, 24.0, 26.0, 28.0, 30.0, 32.0, 34.0, 36.0, 38.0, 40.0, 42.0, 44.0, 46.0, 48.0, 50.0, 52.0, 54.0, 56.0, 58.0, 60.0, 62.0, 64.0, 66.0, 68.0, 70.0, 72.0, 74.0, 76.0, 78.0, 80.0, 82.0, 84.0, 86.0, 88.0, 90.0, 92.0, 94.0, 96.0, 98.0, 100.0, 102.0, 104.0, 106.0, 108.0, 110.0, 112.0, 114.0, 116.0, 118.0, 120.0, 122.0, 124.0, 126.0, 128.0, 130.0, 132.0, 134.0, 136.0, 138.0, 140.0, 142.0, 144.0, 146.0, 148.0, 150.0, 152.0, 154.0, 156.0, 158.0, 160.0, 162.0, 164.0, 166.0, 168.0, 170.0, 172.0, 174.0, 176.0, 178.0, 180.0, 182.0, 184.0, 186.0, 188.0, 190.0, 192.0, 194.0, 196.0, 198.0, 200.0]; Binary search index was -1; ```. --------------------------------------------------------------",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5846
https://github.com/hail-is/hail/issues/5846:2782,Safety,abort,aborted,2782,".__version__, deepest)) from None; 229 except pyspark.sql.utils.CapturedException as e:; 230 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: HailException: found out of bounds index -1; Resulted from trying to merge -0.0; Indices are [0.0, 2.0, 4.0, 6.0, 8.0, 10.0, 12.0, 14.0, 16.0, 18.0, 20.0, 22.0, 24.0, 26.0, 28.0, 30.0, 32.0, 34.0, 36.0, 38.0, 40.0, 42.0, 44.0, 46.0, 48.0, 50.0, 52.0, 54.0, 56.0, 58.0, 60.0, 62.0, 64.0, 66.0, 68.0, 70.0, 72.0, 74.0, 76.0, 78.0, 80.0, 82.0, 84.0, 86.0, 88.0, 90.0, 92.0, 94.0, 96.0, 98.0, 100.0, 102.0, 104.0, 106.0, 108.0, 110.0, 112.0, 114.0, 116.0, 118.0, 120.0, 122.0, 124.0, 126.0, 128.0, 130.0, 132.0, 134.0, 136.0, 138.0, 140.0, 142.0, 144.0, 146.0, 148.0, 150.0, 152.0, 154.0, 156.0, 158.0, 160.0, 162.0, 164.0, 166.0, 168.0, 170.0, 172.0, 174.0, 176.0, 178.0, 180.0, 182.0, 184.0, 186.0, 188.0, 190.0, 192.0, 194.0, 196.0, 198.0, 200.0]; Binary search index was -1. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 24.0 failed 20 times, most recent failure: Lost task 0.19 in stage 24.0 (TID 1813, lfrani-sw-hqb8.c.broad-mpg-gnomad.internal, executor 159): is.hail.utils.HailException: found out of bounds index -1; Resulted from trying to merge -0.0; Indices are [0.0, 2.0, 4.0, 6.0, 8.0, 10.0, 12.0, 14.0, 16.0, 18.0, 20.0, 22.0, 24.0, 26.0, 28.0, 30.0, 32.0, 34.0, 36.0, 38.0, 40.0, 42.0, 44.0, 46.0, 48.0, 50.0, 52.0, 54.0, 56.0, 58.0, 60.0, 62.0, 64.0, 66.0, 68.0, 70.0, 72.0, 74.0, 76.0, 78.0, 80.0, 82.0, 84.0, 86.0, 88.0, 90.0, 92.0, 94.0, 96.0, 98.0, 100.0, 102.0, 104.0, 106.0, 108.0, 110.0, 112.0, 114.0, 116.0, 118.0, 120.0, 122.0, 124.0, 126.0, 128.0, 130.0, 132.0, 134.0, 136.0, 138.0, 140.0, 142.0, 144.0, 146.0, 148.0, 150.0, 152.0, 154.0, 156.0, 158.0, 160.0, 162.0, 164.0, 166.0, 168.0, 170.0, 172.0, 174.0, 176.0, 178.0, 180.0, 182.0, 184.0, 186.0, 188.0, 190.0, 192.0, 194.0, 196.0, 198.0, 200.0]; Binary search index was -1; 	at is.hail.utils.ErrorHandling$class.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5846
https://github.com/hail-is/hail/issues/5846:6336,Safety,abort,abortStage,6336,	at is.hail.utils.package$.singletonElement(package.scala:603); 	at is.hail.rvd.RVD$$anonfun$aggregateWithPartitionOp$1.apply(RVD.scala:558); 	at is.hail.rvd.RVD$$anonfun$aggregateWithPartitionOp$1.apply(RVD.scala:558); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1575); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1563); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1562); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1562); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1790); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1745); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSche,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5846
https://github.com/hail-is/hail/issues/5846:6434,Safety,abort,abortStage,6434,gateWithPartitionOp$1.apply(RVD.scala:558); 	at is.hail.rvd.RVD$$anonfun$aggregateWithPartitionOp$1.apply(RVD.scala:558); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1575); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1563); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1562); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1562); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1790); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1745); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1734); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.ap,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5846
https://github.com/hail-is/hail/issues/5846:6679,Safety,abort,abortStage,6679,xt.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1575); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1563); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1562); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1562); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1790); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1745); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1734); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:619); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); 	at is.hail.rvd.RVD.aggregateWithPartitionOp(RVD.scala:558); ,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5846
https://github.com/hail-is/hail/pull/5848:90,Deployability,install,install,90,"For ci2. We might prefer just the jre in the base, but the Hail build doesn't work if you install the jre and then the jdk, java.home still points to the jre and I haven't figured out a fix yet.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5848
https://github.com/hail-is/hail/pull/5850:36,Availability,error,error,36,"Working in cluster. Fixes the below error, the origin of which I'm not quite sure: does it happen because wherever CI builds this has spark2.4 installed, or is spark2.4 pulled by gradlew shadowJar (I don't see where this happens, but I also haven't looked very carefully). ```; install-hail-locally:; 	rm -rf build; 	(cd ../hail && GRADLE_OPTS=-Xmx2048m ./gradlew shadowJar --gradle-user-home /gradle-cache); 	mkdir -p build/hail/jars; 	mkdir -p build/hail/python; 	cp -a ../hail/build/libs/hail-all-spark.jar build/hail/jars; 	cp -a ../hail/python/hail build/hail/python. build-hail-base: build-spark-base install-hail-locally; ```. <img width=""814"" alt=""Screenshot 2019-04-10 13 22 01"" src=""https://user-images.githubusercontent.com/5543229/55902941-79ea4c00-5b9a-11e9-9899-8e37311c4d06.png"">. ; Only issue I see is; """"""; 2019-04-10 18:00:59 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Setting default log level to ""WARN""; """""""". not sure if that's new, but googling around suggests the typical solution is warning suppression. cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5850
https://github.com/hail-is/hail/pull/5850:143,Deployability,install,installed,143,"Working in cluster. Fixes the below error, the origin of which I'm not quite sure: does it happen because wherever CI builds this has spark2.4 installed, or is spark2.4 pulled by gradlew shadowJar (I don't see where this happens, but I also haven't looked very carefully). ```; install-hail-locally:; 	rm -rf build; 	(cd ../hail && GRADLE_OPTS=-Xmx2048m ./gradlew shadowJar --gradle-user-home /gradle-cache); 	mkdir -p build/hail/jars; 	mkdir -p build/hail/python; 	cp -a ../hail/build/libs/hail-all-spark.jar build/hail/jars; 	cp -a ../hail/python/hail build/hail/python. build-hail-base: build-spark-base install-hail-locally; ```. <img width=""814"" alt=""Screenshot 2019-04-10 13 22 01"" src=""https://user-images.githubusercontent.com/5543229/55902941-79ea4c00-5b9a-11e9-9899-8e37311c4d06.png"">. ; Only issue I see is; """"""; 2019-04-10 18:00:59 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Setting default log level to ""WARN""; """""""". not sure if that's new, but googling around suggests the typical solution is warning suppression. cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5850
https://github.com/hail-is/hail/pull/5850:278,Deployability,install,install-hail-locally,278,"Working in cluster. Fixes the below error, the origin of which I'm not quite sure: does it happen because wherever CI builds this has spark2.4 installed, or is spark2.4 pulled by gradlew shadowJar (I don't see where this happens, but I also haven't looked very carefully). ```; install-hail-locally:; 	rm -rf build; 	(cd ../hail && GRADLE_OPTS=-Xmx2048m ./gradlew shadowJar --gradle-user-home /gradle-cache); 	mkdir -p build/hail/jars; 	mkdir -p build/hail/python; 	cp -a ../hail/build/libs/hail-all-spark.jar build/hail/jars; 	cp -a ../hail/python/hail build/hail/python. build-hail-base: build-spark-base install-hail-locally; ```. <img width=""814"" alt=""Screenshot 2019-04-10 13 22 01"" src=""https://user-images.githubusercontent.com/5543229/55902941-79ea4c00-5b9a-11e9-9899-8e37311c4d06.png"">. ; Only issue I see is; """"""; 2019-04-10 18:00:59 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Setting default log level to ""WARN""; """""""". not sure if that's new, but googling around suggests the typical solution is warning suppression. cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5850
https://github.com/hail-is/hail/pull/5850:607,Deployability,install,install-hail-locally,607,"Working in cluster. Fixes the below error, the origin of which I'm not quite sure: does it happen because wherever CI builds this has spark2.4 installed, or is spark2.4 pulled by gradlew shadowJar (I don't see where this happens, but I also haven't looked very carefully). ```; install-hail-locally:; 	rm -rf build; 	(cd ../hail && GRADLE_OPTS=-Xmx2048m ./gradlew shadowJar --gradle-user-home /gradle-cache); 	mkdir -p build/hail/jars; 	mkdir -p build/hail/python; 	cp -a ../hail/build/libs/hail-all-spark.jar build/hail/jars; 	cp -a ../hail/python/hail build/hail/python. build-hail-base: build-spark-base install-hail-locally; ```. <img width=""814"" alt=""Screenshot 2019-04-10 13 22 01"" src=""https://user-images.githubusercontent.com/5543229/55902941-79ea4c00-5b9a-11e9-9899-8e37311c4d06.png"">. ; Only issue I see is; """"""; 2019-04-10 18:00:59 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Setting default log level to ""WARN""; """""""". not sure if that's new, but googling around suggests the typical solution is warning suppression. cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5850
https://github.com/hail-is/hail/pull/5850:401,Performance,cache,cache,401,"Working in cluster. Fixes the below error, the origin of which I'm not quite sure: does it happen because wherever CI builds this has spark2.4 installed, or is spark2.4 pulled by gradlew shadowJar (I don't see where this happens, but I also haven't looked very carefully). ```; install-hail-locally:; 	rm -rf build; 	(cd ../hail && GRADLE_OPTS=-Xmx2048m ./gradlew shadowJar --gradle-user-home /gradle-cache); 	mkdir -p build/hail/jars; 	mkdir -p build/hail/python; 	cp -a ../hail/build/libs/hail-all-spark.jar build/hail/jars; 	cp -a ../hail/python/hail build/hail/python. build-hail-base: build-spark-base install-hail-locally; ```. <img width=""814"" alt=""Screenshot 2019-04-10 13 22 01"" src=""https://user-images.githubusercontent.com/5543229/55902941-79ea4c00-5b9a-11e9-9899-8e37311c4d06.png"">. ; Only issue I see is; """"""; 2019-04-10 18:00:59 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Setting default log level to ""WARN""; """""""". not sure if that's new, but googling around suggests the typical solution is warning suppression. cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5850
https://github.com/hail-is/hail/pull/5850:881,Performance,load,load,881,"Working in cluster. Fixes the below error, the origin of which I'm not quite sure: does it happen because wherever CI builds this has spark2.4 installed, or is spark2.4 pulled by gradlew shadowJar (I don't see where this happens, but I also haven't looked very carefully). ```; install-hail-locally:; 	rm -rf build; 	(cd ../hail && GRADLE_OPTS=-Xmx2048m ./gradlew shadowJar --gradle-user-home /gradle-cache); 	mkdir -p build/hail/jars; 	mkdir -p build/hail/python; 	cp -a ../hail/build/libs/hail-all-spark.jar build/hail/jars; 	cp -a ../hail/python/hail build/hail/python. build-hail-base: build-spark-base install-hail-locally; ```. <img width=""814"" alt=""Screenshot 2019-04-10 13 22 01"" src=""https://user-images.githubusercontent.com/5543229/55902941-79ea4c00-5b9a-11e9-9899-8e37311c4d06.png"">. ; Only issue I see is; """"""; 2019-04-10 18:00:59 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Setting default log level to ""WARN""; """""""". not sure if that's new, but googling around suggests the typical solution is warning suppression. cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5850
https://github.com/hail-is/hail/pull/5850:990,Testability,log,log,990,"Working in cluster. Fixes the below error, the origin of which I'm not quite sure: does it happen because wherever CI builds this has spark2.4 installed, or is spark2.4 pulled by gradlew shadowJar (I don't see where this happens, but I also haven't looked very carefully). ```; install-hail-locally:; 	rm -rf build; 	(cd ../hail && GRADLE_OPTS=-Xmx2048m ./gradlew shadowJar --gradle-user-home /gradle-cache); 	mkdir -p build/hail/jars; 	mkdir -p build/hail/python; 	cp -a ../hail/build/libs/hail-all-spark.jar build/hail/jars; 	cp -a ../hail/python/hail build/hail/python. build-hail-base: build-spark-base install-hail-locally; ```. <img width=""814"" alt=""Screenshot 2019-04-10 13 22 01"" src=""https://user-images.githubusercontent.com/5543229/55902941-79ea4c00-5b9a-11e9-9899-8e37311c4d06.png"">. ; Only issue I see is; """"""; 2019-04-10 18:00:59 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Setting default log level to ""WARN""; """""""". not sure if that's new, but googling around suggests the typical solution is warning suppression. cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5850
https://github.com/hail-is/hail/pull/5852:27,Testability,log,log,27,Summary; - the deleted job log test was failing for me @jigold was there an issue with this one?; - give batch 2 seconds to clean up before kill -9; - decrease max `wait` time so we get faster notification when a test is finished; - dumb and inefficient pool: always keep five PVCs around ready to go. I see about a 10 second improvement on total test time from this change.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5852
https://github.com/hail-is/hail/pull/5852:31,Testability,test,test,31,Summary; - the deleted job log test was failing for me @jigold was there an issue with this one?; - give batch 2 seconds to clean up before kill -9; - decrease max `wait` time so we get faster notification when a test is finished; - dumb and inefficient pool: always keep five PVCs around ready to go. I see about a 10 second improvement on total test time from this change.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5852
https://github.com/hail-is/hail/pull/5852:213,Testability,test,test,213,Summary; - the deleted job log test was failing for me @jigold was there an issue with this one?; - give batch 2 seconds to clean up before kill -9; - decrease max `wait` time so we get faster notification when a test is finished; - dumb and inefficient pool: always keep five PVCs around ready to go. I see about a 10 second improvement on total test time from this change.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5852
https://github.com/hail-is/hail/pull/5852:347,Testability,test,test,347,Summary; - the deleted job log test was failing for me @jigold was there an issue with this one?; - give batch 2 seconds to clean up before kill -9; - decrease max `wait` time so we get faster notification when a test is finished; - dumb and inefficient pool: always keep five PVCs around ready to go. I see about a 10 second improvement on total test time from this change.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5852
https://github.com/hail-is/hail/issues/5854:162,Availability,error,error,162,"Should unify the two schemas, setting fields to missing as needed. If a similarly-named field in the two tables can be coerced (int32 / int64), do that. Throw an error if coercion is not posible.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5854
https://github.com/hail-is/hail/pull/5855:60,Availability,error,errors,60,New entry filtering semantics was removing children with no errors; leading to incorrect counts. fixes #5786,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5855
https://github.com/hail-is/hail/pull/5857:40,Performance,optimiz,optimization,40,"- Time execution stages in the backend (optimization, compilation, runtime); - Return a dictionary back to the front end of nanoseconds and formatted times for each stage.; - Add `hl.eval_timed` which propagates returns the evaluated IR as well as the timings dictionary. If called with `timed=False` (default), `Env.backend.execute()` drops the timings it received from the backend. Could change it to log them instead or always.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5857
https://github.com/hail-is/hail/pull/5857:403,Testability,log,log,403,"- Time execution stages in the backend (optimization, compilation, runtime); - Return a dictionary back to the front end of nanoseconds and formatted times for each stage.; - Add `hl.eval_timed` which propagates returns the evaluated IR as well as the timings dictionary. If called with `timed=False` (default), `Env.backend.execute()` drops the timings it received from the backend. Could change it to log them instead or always.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5857
https://github.com/hail-is/hail/pull/5860:74,Performance,optimiz,optimize,74,"This is used in lowering `MatrixAnnotateColsTable`, and will allow; us to optimize away unnecessary keying shuffles that happen in column; annotation table manipulations.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5860
https://github.com/hail-is/hail/pull/5861:7,Performance,race condition,race condition,7,remove race condition where we might observe a complete job before the callback is sent.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5861
https://github.com/hail-is/hail/pull/5863:22,Deployability,upgrade,upgrade,22,"I guess the Spark 2.4 upgrade broke apiserver, but honestly I don't know how this could have worked before. Deployed by hand and verified it's working. I'm talking in methods tomorrow, might try to demo this. It would be nice if it is working.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5863
https://github.com/hail-is/hail/pull/5863:108,Deployability,Deploy,Deployed,108,"I guess the Spark 2.4 upgrade broke apiserver, but honestly I don't know how this could have worked before. Deployed by hand and verified it's working. I'm talking in methods tomorrow, might try to demo this. It would be nice if it is working.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5863
https://github.com/hail-is/hail/pull/5866:236,Deployability,pipeline,pipeline,236,"This PR should fail because the gsa key is not in the test namespace -- I think we should have a second user account for testing. Still to do is to expose all of the user key infrastructure in the batch `Makefile` and `test-locally` in pipeline and ci. At some point, we should consolidate the `google_storage.py` file so not duplicating with `ci`. . But first I wanted to get feedback. @danking @cseed @akotlar",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5866
https://github.com/hail-is/hail/pull/5866:148,Security,expose,expose,148,"This PR should fail because the gsa key is not in the test namespace -- I think we should have a second user account for testing. Still to do is to expose all of the user key infrastructure in the batch `Makefile` and `test-locally` in pipeline and ci. At some point, we should consolidate the `google_storage.py` file so not duplicating with `ci`. . But first I wanted to get feedback. @danking @cseed @akotlar",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5866
https://github.com/hail-is/hail/pull/5866:54,Testability,test,test,54,"This PR should fail because the gsa key is not in the test namespace -- I think we should have a second user account for testing. Still to do is to expose all of the user key infrastructure in the batch `Makefile` and `test-locally` in pipeline and ci. At some point, we should consolidate the `google_storage.py` file so not duplicating with `ci`. . But first I wanted to get feedback. @danking @cseed @akotlar",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5866
https://github.com/hail-is/hail/pull/5866:121,Testability,test,testing,121,"This PR should fail because the gsa key is not in the test namespace -- I think we should have a second user account for testing. Still to do is to expose all of the user key infrastructure in the batch `Makefile` and `test-locally` in pipeline and ci. At some point, we should consolidate the `google_storage.py` file so not duplicating with `ci`. . But first I wanted to get feedback. @danking @cseed @akotlar",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5866
https://github.com/hail-is/hail/pull/5866:219,Testability,test,test-locally,219,"This PR should fail because the gsa key is not in the test namespace -- I think we should have a second user account for testing. Still to do is to expose all of the user key infrastructure in the batch `Makefile` and `test-locally` in pipeline and ci. At some point, we should consolidate the `google_storage.py` file so not duplicating with `ci`. . But first I wanted to get feedback. @danking @cseed @akotlar",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5866
https://github.com/hail-is/hail/pull/5866:377,Usability,feedback,feedback,377,"This PR should fail because the gsa key is not in the test namespace -- I think we should have a second user account for testing. Still to do is to expose all of the user key infrastructure in the batch `Makefile` and `test-locally` in pipeline and ci. At some point, we should consolidate the `google_storage.py` file so not duplicating with `ci`. . But first I wanted to get feedback. @danking @cseed @akotlar",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5866
https://github.com/hail-is/hail/pull/5867:225,Integrability,rout,router,225,"As part of the ci2 work, I want to set things up so it is possible (and easy!) to spin up independent copies of the entire stack for development, testing and staging. To that end, I'm breaking apart gateway, into gateway and router. Each publicly accessible namespace will have a router, and gateway will only be responsible for stripping encryption and forwarding requests to these routers. Requests like `...mynamespace.internal.hail.is` will get forwarded to the router for `mynamespace`. All other requests will get forwarded to the default namespace router. I will so modify gateway in another PR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5867
https://github.com/hail-is/hail/pull/5867:280,Integrability,rout,router,280,"As part of the ci2 work, I want to set things up so it is possible (and easy!) to spin up independent copies of the entire stack for development, testing and staging. To that end, I'm breaking apart gateway, into gateway and router. Each publicly accessible namespace will have a router, and gateway will only be responsible for stripping encryption and forwarding requests to these routers. Requests like `...mynamespace.internal.hail.is` will get forwarded to the router for `mynamespace`. All other requests will get forwarded to the default namespace router. I will so modify gateway in another PR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5867
https://github.com/hail-is/hail/pull/5867:383,Integrability,rout,routers,383,"As part of the ci2 work, I want to set things up so it is possible (and easy!) to spin up independent copies of the entire stack for development, testing and staging. To that end, I'm breaking apart gateway, into gateway and router. Each publicly accessible namespace will have a router, and gateway will only be responsible for stripping encryption and forwarding requests to these routers. Requests like `...mynamespace.internal.hail.is` will get forwarded to the router for `mynamespace`. All other requests will get forwarded to the default namespace router. I will so modify gateway in another PR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5867
https://github.com/hail-is/hail/pull/5867:466,Integrability,rout,router,466,"As part of the ci2 work, I want to set things up so it is possible (and easy!) to spin up independent copies of the entire stack for development, testing and staging. To that end, I'm breaking apart gateway, into gateway and router. Each publicly accessible namespace will have a router, and gateway will only be responsible for stripping encryption and forwarding requests to these routers. Requests like `...mynamespace.internal.hail.is` will get forwarded to the router for `mynamespace`. All other requests will get forwarded to the default namespace router. I will so modify gateway in another PR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5867
https://github.com/hail-is/hail/pull/5867:555,Integrability,rout,router,555,"As part of the ci2 work, I want to set things up so it is possible (and easy!) to spin up independent copies of the entire stack for development, testing and staging. To that end, I'm breaking apart gateway, into gateway and router. Each publicly accessible namespace will have a router, and gateway will only be responsible for stripping encryption and forwarding requests to these routers. Requests like `...mynamespace.internal.hail.is` will get forwarded to the router for `mynamespace`. All other requests will get forwarded to the default namespace router. I will so modify gateway in another PR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5867
https://github.com/hail-is/hail/pull/5867:247,Security,access,accessible,247,"As part of the ci2 work, I want to set things up so it is possible (and easy!) to spin up independent copies of the entire stack for development, testing and staging. To that end, I'm breaking apart gateway, into gateway and router. Each publicly accessible namespace will have a router, and gateway will only be responsible for stripping encryption and forwarding requests to these routers. Requests like `...mynamespace.internal.hail.is` will get forwarded to the router for `mynamespace`. All other requests will get forwarded to the default namespace router. I will so modify gateway in another PR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5867
https://github.com/hail-is/hail/pull/5867:339,Security,encrypt,encryption,339,"As part of the ci2 work, I want to set things up so it is possible (and easy!) to spin up independent copies of the entire stack for development, testing and staging. To that end, I'm breaking apart gateway, into gateway and router. Each publicly accessible namespace will have a router, and gateway will only be responsible for stripping encryption and forwarding requests to these routers. Requests like `...mynamespace.internal.hail.is` will get forwarded to the router for `mynamespace`. All other requests will get forwarded to the default namespace router. I will so modify gateway in another PR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5867
https://github.com/hail-is/hail/pull/5867:146,Testability,test,testing,146,"As part of the ci2 work, I want to set things up so it is possible (and easy!) to spin up independent copies of the entire stack for development, testing and staging. To that end, I'm breaking apart gateway, into gateway and router. Each publicly accessible namespace will have a router, and gateway will only be responsible for stripping encryption and forwarding requests to these routers. Requests like `...mynamespace.internal.hail.is` will get forwarded to the router for `mynamespace`. All other requests will get forwarded to the default namespace router. I will so modify gateway in another PR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5867
https://github.com/hail-is/hail/issues/5868:91,Usability,simpl,simplest,91,"`write_pipeline_outputs` returns a list for resource groups (which kills the command run). simplest fix is to change the last line in that function to:; ```; return [x for ext, rf in r._resources.items() for x in write_pipeline_outputs(rf, dest + '.' + ext)]; ```; A `yield` type function might be more elegant, but sounds like you might be tearing this up a little bit anyway, so I leave that to you.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5868
https://github.com/hail-is/hail/pull/5871:78,Deployability,patch,patch,78,"The `HailContext.getOrCreate` method seems to have been broken in #5512. This patch fixes the issue and adds a regression test so that it won't break again. Since this test must add create a new Hail context, I had to add a gradle task that runs every suite in a separate JVM. I'm not a gradle expert, so if there's a simpler way to accomplish this execution mode, feel free to suggest :).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5871
https://github.com/hail-is/hail/pull/5871:122,Testability,test,test,122,"The `HailContext.getOrCreate` method seems to have been broken in #5512. This patch fixes the issue and adds a regression test so that it won't break again. Since this test must add create a new Hail context, I had to add a gradle task that runs every suite in a separate JVM. I'm not a gradle expert, so if there's a simpler way to accomplish this execution mode, feel free to suggest :).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5871
https://github.com/hail-is/hail/pull/5871:168,Testability,test,test,168,"The `HailContext.getOrCreate` method seems to have been broken in #5512. This patch fixes the issue and adds a regression test so that it won't break again. Since this test must add create a new Hail context, I had to add a gradle task that runs every suite in a separate JVM. I'm not a gradle expert, so if there's a simpler way to accomplish this execution mode, feel free to suggest :).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5871
https://github.com/hail-is/hail/pull/5871:318,Usability,simpl,simpler,318,"The `HailContext.getOrCreate` method seems to have been broken in #5512. This patch fixes the issue and adds a regression test so that it won't break again. Since this test must add create a new Hail context, I had to add a gradle task that runs every suite in a separate JVM. I'm not a gradle expert, so if there's a simpler way to accomplish this execution mode, feel free to suggest :).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5871
https://github.com/hail-is/hail/pull/5873:22,Integrability,rout,router,22,"Proxy from gateway to router. Logic for proxying web sockets came from here: https://stackoverflow.com/a/15198581/431282. The Let's Encrypt stuff here isn't used anymore, not since we put letsencrypt in its own subproject. I tested this by hand and then live on the cluster (!) and it's working fine.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5873
https://github.com/hail-is/hail/pull/5873:132,Security,Encrypt,Encrypt,132,"Proxy from gateway to router. Logic for proxying web sockets came from here: https://stackoverflow.com/a/15198581/431282. The Let's Encrypt stuff here isn't used anymore, not since we put letsencrypt in its own subproject. I tested this by hand and then live on the cluster (!) and it's working fine.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5873
https://github.com/hail-is/hail/pull/5873:30,Testability,Log,Logic,30,"Proxy from gateway to router. Logic for proxying web sockets came from here: https://stackoverflow.com/a/15198581/431282. The Let's Encrypt stuff here isn't used anymore, not since we put letsencrypt in its own subproject. I tested this by hand and then live on the cluster (!) and it's working fine.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5873
https://github.com/hail-is/hail/pull/5873:225,Testability,test,tested,225,"Proxy from gateway to router. Logic for proxying web sockets came from here: https://stackoverflow.com/a/15198581/431282. The Let's Encrypt stuff here isn't used anymore, not since we put letsencrypt in its own subproject. I tested this by hand and then live on the cluster (!) and it's working fine.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5873
https://github.com/hail-is/hail/pull/5874:64,Testability,Test,Tests,64,"Basic filesystem class, currently only implements Hadoop calls. Tests pass. Not sure what we'd like to do with the docstring, so left it. All apparently random formatting changes are to force compliance with pep8",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5874
https://github.com/hail-is/hail/pull/5876:225,Modifiability,variab,variables,225,"- Changed NDArrayRef node to take a _sequence_ of IR for the indices as opposed to a single IR. This will make operations like slicing much easier, simplifies IR emission and allows us to typecheck that you have enough index variables; - Refactored `linearizeIndices`, a function that takes a Scala sequence of index variables for a ndarray and emits the single array index, so that it actually checks now that each index is within the shape length bounds of that dimension.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5876
https://github.com/hail-is/hail/pull/5876:238,Modifiability,Refactor,Refactored,238,"- Changed NDArrayRef node to take a _sequence_ of IR for the indices as opposed to a single IR. This will make operations like slicing much easier, simplifies IR emission and allows us to typecheck that you have enough index variables; - Refactored `linearizeIndices`, a function that takes a Scala sequence of index variables for a ndarray and emits the single array index, so that it actually checks now that each index is within the shape length bounds of that dimension.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5876
https://github.com/hail-is/hail/pull/5876:317,Modifiability,variab,variables,317,"- Changed NDArrayRef node to take a _sequence_ of IR for the indices as opposed to a single IR. This will make operations like slicing much easier, simplifies IR emission and allows us to typecheck that you have enough index variables; - Refactored `linearizeIndices`, a function that takes a Scala sequence of index variables for a ndarray and emits the single array index, so that it actually checks now that each index is within the shape length bounds of that dimension.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5876
https://github.com/hail-is/hail/pull/5876:148,Usability,simpl,simplifies,148,"- Changed NDArrayRef node to take a _sequence_ of IR for the indices as opposed to a single IR. This will make operations like slicing much easier, simplifies IR emission and allows us to typecheck that you have enough index variables; - Refactored `linearizeIndices`, a function that takes a Scala sequence of index variables for a ndarray and emits the single array index, so that it actually checks now that each index is within the shape length bounds of that dimension.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5876
https://github.com/hail-is/hail/pull/5878:283,Availability,avail,available,283,"Stacks on #5874. Commit specific to this pr are: https://github.com/hail-is/hail/pull/5878/commits/e959eaf270c5dd9966e3c9c96f21d4f914097012, https://github.com/hail-is/hail/pull/5878/commits/fcbb0dc6ec6c678a54655afda996ee6b1148f2a1. Minor oddity: the 'updated' property isn't always available for folders. I can get around this if needed. I return the bucket for the ""owner"" property because the google sa isn't returned in the response. Will change this to the sa email (read at GoogleStorageFS instantiation)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5878
https://github.com/hail-is/hail/pull/5878:252,Deployability,update,updated,252,"Stacks on #5874. Commit specific to this pr are: https://github.com/hail-is/hail/pull/5878/commits/e959eaf270c5dd9966e3c9c96f21d4f914097012, https://github.com/hail-is/hail/pull/5878/commits/fcbb0dc6ec6c678a54655afda996ee6b1148f2a1. Minor oddity: the 'updated' property isn't always available for folders. I can get around this if needed. I return the bucket for the ""owner"" property because the google sa isn't returned in the response. Will change this to the sa email (read at GoogleStorageFS instantiation)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5878
https://github.com/hail-is/hail/pull/5879:160,Deployability,Pipeline,Pipeline,160,Here's the reference for the behavior of `gsutil cp` with directory naming. https://cloud.google.com/storage/docs/gsutil/commands/cp#how-names-are-constructed. Pipeline and CI will need to make sure the inputs to Batch actually produce what is intended.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5879
https://github.com/hail-is/hail/pull/5880:0,Deployability,deploy,deployed,0,"deployed by hand, working fine.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5880
https://github.com/hail-is/hail/pull/5883:17,Availability,avail,available,17,"`va` needs to be available inside the aggregator init operations, so needs to be bound outside the arrayagg.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5883
https://github.com/hail-is/hail/pull/5890:12,Usability,simpl,simple,12,"Implemented simple buffers that read/write data directly from/to streams without blocking. Our other buffer specs write/read blocked data, so the data is encoded with the block info. To write out NDArrays that are compatibly with numpy, we need to just write out the straight bytes.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5890
https://github.com/hail-is/hail/pull/5891:880,Availability,error,error,880,"OK, I gave you maximum spicy. I don't think it's so bad, but let me know if you want me to cut it up. Some remarks:; - This PR successfully tests (and it passes!) and then cleans up this branch: https://github.com/hail-is/hail/pull/5842. See `build.yaml`. It's a thing of beauty (I think).; - That branch has everything but Scala tests and dataproc/cloudtools tests. The latter are easy, the former are a little messy since I want to test against a test jar, and I've decided to switch to maven for that.; - No support for publish or deploy yet.; - There are synchronous calls it `git` in various places which can make the UI sluggish. I'll fix those in another PR.; - Work remains to validate build.yaml and the deploy step yaml.; - I currently run jinja2 if the file (Dockerfile or deployment yaml) ends in `.in`, but I think I'm going to make it unconditional. `.in` just seem error prone.; - In CreateDatabaseStep, I put secret credentials in the pod configuration. That's not ideal, but I don't think it is a serious problem, because nobody who isn't privileged can read the pods, and I can fix it in a later PR (the create database step should generate the passwords, not ci2).; - I disabled the fixme pylint message (on # FIXME comments), since are fixmes are longer lived than a single change sometimes.; - I'm slightly confused about runImage (which generates a batch job) and deploy of a pod spec (which runs kubectl apply as a batch job). Right now, runImage always runs in batch-pods, and a deploy job runs in whatever namespace you specify. Fixes https://github.com/hail-is/hail/issues/5903",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5891
https://github.com/hail-is/hail/pull/5891:534,Deployability,deploy,deploy,534,"OK, I gave you maximum spicy. I don't think it's so bad, but let me know if you want me to cut it up. Some remarks:; - This PR successfully tests (and it passes!) and then cleans up this branch: https://github.com/hail-is/hail/pull/5842. See `build.yaml`. It's a thing of beauty (I think).; - That branch has everything but Scala tests and dataproc/cloudtools tests. The latter are easy, the former are a little messy since I want to test against a test jar, and I've decided to switch to maven for that.; - No support for publish or deploy yet.; - There are synchronous calls it `git` in various places which can make the UI sluggish. I'll fix those in another PR.; - Work remains to validate build.yaml and the deploy step yaml.; - I currently run jinja2 if the file (Dockerfile or deployment yaml) ends in `.in`, but I think I'm going to make it unconditional. `.in` just seem error prone.; - In CreateDatabaseStep, I put secret credentials in the pod configuration. That's not ideal, but I don't think it is a serious problem, because nobody who isn't privileged can read the pods, and I can fix it in a later PR (the create database step should generate the passwords, not ci2).; - I disabled the fixme pylint message (on # FIXME comments), since are fixmes are longer lived than a single change sometimes.; - I'm slightly confused about runImage (which generates a batch job) and deploy of a pod spec (which runs kubectl apply as a batch job). Right now, runImage always runs in batch-pods, and a deploy job runs in whatever namespace you specify. Fixes https://github.com/hail-is/hail/issues/5903",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5891
https://github.com/hail-is/hail/pull/5891:713,Deployability,deploy,deploy,713,"OK, I gave you maximum spicy. I don't think it's so bad, but let me know if you want me to cut it up. Some remarks:; - This PR successfully tests (and it passes!) and then cleans up this branch: https://github.com/hail-is/hail/pull/5842. See `build.yaml`. It's a thing of beauty (I think).; - That branch has everything but Scala tests and dataproc/cloudtools tests. The latter are easy, the former are a little messy since I want to test against a test jar, and I've decided to switch to maven for that.; - No support for publish or deploy yet.; - There are synchronous calls it `git` in various places which can make the UI sluggish. I'll fix those in another PR.; - Work remains to validate build.yaml and the deploy step yaml.; - I currently run jinja2 if the file (Dockerfile or deployment yaml) ends in `.in`, but I think I'm going to make it unconditional. `.in` just seem error prone.; - In CreateDatabaseStep, I put secret credentials in the pod configuration. That's not ideal, but I don't think it is a serious problem, because nobody who isn't privileged can read the pods, and I can fix it in a later PR (the create database step should generate the passwords, not ci2).; - I disabled the fixme pylint message (on # FIXME comments), since are fixmes are longer lived than a single change sometimes.; - I'm slightly confused about runImage (which generates a batch job) and deploy of a pod spec (which runs kubectl apply as a batch job). Right now, runImage always runs in batch-pods, and a deploy job runs in whatever namespace you specify. Fixes https://github.com/hail-is/hail/issues/5903",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5891
https://github.com/hail-is/hail/pull/5891:784,Deployability,deploy,deployment,784,"OK, I gave you maximum spicy. I don't think it's so bad, but let me know if you want me to cut it up. Some remarks:; - This PR successfully tests (and it passes!) and then cleans up this branch: https://github.com/hail-is/hail/pull/5842. See `build.yaml`. It's a thing of beauty (I think).; - That branch has everything but Scala tests and dataproc/cloudtools tests. The latter are easy, the former are a little messy since I want to test against a test jar, and I've decided to switch to maven for that.; - No support for publish or deploy yet.; - There are synchronous calls it `git` in various places which can make the UI sluggish. I'll fix those in another PR.; - Work remains to validate build.yaml and the deploy step yaml.; - I currently run jinja2 if the file (Dockerfile or deployment yaml) ends in `.in`, but I think I'm going to make it unconditional. `.in` just seem error prone.; - In CreateDatabaseStep, I put secret credentials in the pod configuration. That's not ideal, but I don't think it is a serious problem, because nobody who isn't privileged can read the pods, and I can fix it in a later PR (the create database step should generate the passwords, not ci2).; - I disabled the fixme pylint message (on # FIXME comments), since are fixmes are longer lived than a single change sometimes.; - I'm slightly confused about runImage (which generates a batch job) and deploy of a pod spec (which runs kubectl apply as a batch job). Right now, runImage always runs in batch-pods, and a deploy job runs in whatever namespace you specify. Fixes https://github.com/hail-is/hail/issues/5903",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5891
https://github.com/hail-is/hail/pull/5891:955,Deployability,configurat,configuration,955,"OK, I gave you maximum spicy. I don't think it's so bad, but let me know if you want me to cut it up. Some remarks:; - This PR successfully tests (and it passes!) and then cleans up this branch: https://github.com/hail-is/hail/pull/5842. See `build.yaml`. It's a thing of beauty (I think).; - That branch has everything but Scala tests and dataproc/cloudtools tests. The latter are easy, the former are a little messy since I want to test against a test jar, and I've decided to switch to maven for that.; - No support for publish or deploy yet.; - There are synchronous calls it `git` in various places which can make the UI sluggish. I'll fix those in another PR.; - Work remains to validate build.yaml and the deploy step yaml.; - I currently run jinja2 if the file (Dockerfile or deployment yaml) ends in `.in`, but I think I'm going to make it unconditional. `.in` just seem error prone.; - In CreateDatabaseStep, I put secret credentials in the pod configuration. That's not ideal, but I don't think it is a serious problem, because nobody who isn't privileged can read the pods, and I can fix it in a later PR (the create database step should generate the passwords, not ci2).; - I disabled the fixme pylint message (on # FIXME comments), since are fixmes are longer lived than a single change sometimes.; - I'm slightly confused about runImage (which generates a batch job) and deploy of a pod spec (which runs kubectl apply as a batch job). Right now, runImage always runs in batch-pods, and a deploy job runs in whatever namespace you specify. Fixes https://github.com/hail-is/hail/issues/5903",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5891
https://github.com/hail-is/hail/pull/5891:1386,Deployability,deploy,deploy,1386,"OK, I gave you maximum spicy. I don't think it's so bad, but let me know if you want me to cut it up. Some remarks:; - This PR successfully tests (and it passes!) and then cleans up this branch: https://github.com/hail-is/hail/pull/5842. See `build.yaml`. It's a thing of beauty (I think).; - That branch has everything but Scala tests and dataproc/cloudtools tests. The latter are easy, the former are a little messy since I want to test against a test jar, and I've decided to switch to maven for that.; - No support for publish or deploy yet.; - There are synchronous calls it `git` in various places which can make the UI sluggish. I'll fix those in another PR.; - Work remains to validate build.yaml and the deploy step yaml.; - I currently run jinja2 if the file (Dockerfile or deployment yaml) ends in `.in`, but I think I'm going to make it unconditional. `.in` just seem error prone.; - In CreateDatabaseStep, I put secret credentials in the pod configuration. That's not ideal, but I don't think it is a serious problem, because nobody who isn't privileged can read the pods, and I can fix it in a later PR (the create database step should generate the passwords, not ci2).; - I disabled the fixme pylint message (on # FIXME comments), since are fixmes are longer lived than a single change sometimes.; - I'm slightly confused about runImage (which generates a batch job) and deploy of a pod spec (which runs kubectl apply as a batch job). Right now, runImage always runs in batch-pods, and a deploy job runs in whatever namespace you specify. Fixes https://github.com/hail-is/hail/issues/5903",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5891
https://github.com/hail-is/hail/pull/5891:1503,Deployability,deploy,deploy,1503,"OK, I gave you maximum spicy. I don't think it's so bad, but let me know if you want me to cut it up. Some remarks:; - This PR successfully tests (and it passes!) and then cleans up this branch: https://github.com/hail-is/hail/pull/5842. See `build.yaml`. It's a thing of beauty (I think).; - That branch has everything but Scala tests and dataproc/cloudtools tests. The latter are easy, the former are a little messy since I want to test against a test jar, and I've decided to switch to maven for that.; - No support for publish or deploy yet.; - There are synchronous calls it `git` in various places which can make the UI sluggish. I'll fix those in another PR.; - Work remains to validate build.yaml and the deploy step yaml.; - I currently run jinja2 if the file (Dockerfile or deployment yaml) ends in `.in`, but I think I'm going to make it unconditional. `.in` just seem error prone.; - In CreateDatabaseStep, I put secret credentials in the pod configuration. That's not ideal, but I don't think it is a serious problem, because nobody who isn't privileged can read the pods, and I can fix it in a later PR (the create database step should generate the passwords, not ci2).; - I disabled the fixme pylint message (on # FIXME comments), since are fixmes are longer lived than a single change sometimes.; - I'm slightly confused about runImage (which generates a batch job) and deploy of a pod spec (which runs kubectl apply as a batch job). Right now, runImage always runs in batch-pods, and a deploy job runs in whatever namespace you specify. Fixes https://github.com/hail-is/hail/issues/5903",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5891
https://github.com/hail-is/hail/pull/5891:1215,Integrability,message,message,1215,"OK, I gave you maximum spicy. I don't think it's so bad, but let me know if you want me to cut it up. Some remarks:; - This PR successfully tests (and it passes!) and then cleans up this branch: https://github.com/hail-is/hail/pull/5842. See `build.yaml`. It's a thing of beauty (I think).; - That branch has everything but Scala tests and dataproc/cloudtools tests. The latter are easy, the former are a little messy since I want to test against a test jar, and I've decided to switch to maven for that.; - No support for publish or deploy yet.; - There are synchronous calls it `git` in various places which can make the UI sluggish. I'll fix those in another PR.; - Work remains to validate build.yaml and the deploy step yaml.; - I currently run jinja2 if the file (Dockerfile or deployment yaml) ends in `.in`, but I think I'm going to make it unconditional. `.in` just seem error prone.; - In CreateDatabaseStep, I put secret credentials in the pod configuration. That's not ideal, but I don't think it is a serious problem, because nobody who isn't privileged can read the pods, and I can fix it in a later PR (the create database step should generate the passwords, not ci2).; - I disabled the fixme pylint message (on # FIXME comments), since are fixmes are longer lived than a single change sometimes.; - I'm slightly confused about runImage (which generates a batch job) and deploy of a pod spec (which runs kubectl apply as a batch job). Right now, runImage always runs in batch-pods, and a deploy job runs in whatever namespace you specify. Fixes https://github.com/hail-is/hail/issues/5903",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5891
https://github.com/hail-is/hail/pull/5891:955,Modifiability,config,configuration,955,"OK, I gave you maximum spicy. I don't think it's so bad, but let me know if you want me to cut it up. Some remarks:; - This PR successfully tests (and it passes!) and then cleans up this branch: https://github.com/hail-is/hail/pull/5842. See `build.yaml`. It's a thing of beauty (I think).; - That branch has everything but Scala tests and dataproc/cloudtools tests. The latter are easy, the former are a little messy since I want to test against a test jar, and I've decided to switch to maven for that.; - No support for publish or deploy yet.; - There are synchronous calls it `git` in various places which can make the UI sluggish. I'll fix those in another PR.; - Work remains to validate build.yaml and the deploy step yaml.; - I currently run jinja2 if the file (Dockerfile or deployment yaml) ends in `.in`, but I think I'm going to make it unconditional. `.in` just seem error prone.; - In CreateDatabaseStep, I put secret credentials in the pod configuration. That's not ideal, but I don't think it is a serious problem, because nobody who isn't privileged can read the pods, and I can fix it in a later PR (the create database step should generate the passwords, not ci2).; - I disabled the fixme pylint message (on # FIXME comments), since are fixmes are longer lived than a single change sometimes.; - I'm slightly confused about runImage (which generates a batch job) and deploy of a pod spec (which runs kubectl apply as a batch job). Right now, runImage always runs in batch-pods, and a deploy job runs in whatever namespace you specify. Fixes https://github.com/hail-is/hail/issues/5903",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5891
https://github.com/hail-is/hail/pull/5891:685,Security,validat,validate,685,"OK, I gave you maximum spicy. I don't think it's so bad, but let me know if you want me to cut it up. Some remarks:; - This PR successfully tests (and it passes!) and then cleans up this branch: https://github.com/hail-is/hail/pull/5842. See `build.yaml`. It's a thing of beauty (I think).; - That branch has everything but Scala tests and dataproc/cloudtools tests. The latter are easy, the former are a little messy since I want to test against a test jar, and I've decided to switch to maven for that.; - No support for publish or deploy yet.; - There are synchronous calls it `git` in various places which can make the UI sluggish. I'll fix those in another PR.; - Work remains to validate build.yaml and the deploy step yaml.; - I currently run jinja2 if the file (Dockerfile or deployment yaml) ends in `.in`, but I think I'm going to make it unconditional. `.in` just seem error prone.; - In CreateDatabaseStep, I put secret credentials in the pod configuration. That's not ideal, but I don't think it is a serious problem, because nobody who isn't privileged can read the pods, and I can fix it in a later PR (the create database step should generate the passwords, not ci2).; - I disabled the fixme pylint message (on # FIXME comments), since are fixmes are longer lived than a single change sometimes.; - I'm slightly confused about runImage (which generates a batch job) and deploy of a pod spec (which runs kubectl apply as a batch job). Right now, runImage always runs in batch-pods, and a deploy job runs in whatever namespace you specify. Fixes https://github.com/hail-is/hail/issues/5903",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5891
https://github.com/hail-is/hail/pull/5891:1163,Security,password,passwords,1163,"OK, I gave you maximum spicy. I don't think it's so bad, but let me know if you want me to cut it up. Some remarks:; - This PR successfully tests (and it passes!) and then cleans up this branch: https://github.com/hail-is/hail/pull/5842. See `build.yaml`. It's a thing of beauty (I think).; - That branch has everything but Scala tests and dataproc/cloudtools tests. The latter are easy, the former are a little messy since I want to test against a test jar, and I've decided to switch to maven for that.; - No support for publish or deploy yet.; - There are synchronous calls it `git` in various places which can make the UI sluggish. I'll fix those in another PR.; - Work remains to validate build.yaml and the deploy step yaml.; - I currently run jinja2 if the file (Dockerfile or deployment yaml) ends in `.in`, but I think I'm going to make it unconditional. `.in` just seem error prone.; - In CreateDatabaseStep, I put secret credentials in the pod configuration. That's not ideal, but I don't think it is a serious problem, because nobody who isn't privileged can read the pods, and I can fix it in a later PR (the create database step should generate the passwords, not ci2).; - I disabled the fixme pylint message (on # FIXME comments), since are fixmes are longer lived than a single change sometimes.; - I'm slightly confused about runImage (which generates a batch job) and deploy of a pod spec (which runs kubectl apply as a batch job). Right now, runImage always runs in batch-pods, and a deploy job runs in whatever namespace you specify. Fixes https://github.com/hail-is/hail/issues/5903",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5891
https://github.com/hail-is/hail/pull/5891:140,Testability,test,tests,140,"OK, I gave you maximum spicy. I don't think it's so bad, but let me know if you want me to cut it up. Some remarks:; - This PR successfully tests (and it passes!) and then cleans up this branch: https://github.com/hail-is/hail/pull/5842. See `build.yaml`. It's a thing of beauty (I think).; - That branch has everything but Scala tests and dataproc/cloudtools tests. The latter are easy, the former are a little messy since I want to test against a test jar, and I've decided to switch to maven for that.; - No support for publish or deploy yet.; - There are synchronous calls it `git` in various places which can make the UI sluggish. I'll fix those in another PR.; - Work remains to validate build.yaml and the deploy step yaml.; - I currently run jinja2 if the file (Dockerfile or deployment yaml) ends in `.in`, but I think I'm going to make it unconditional. `.in` just seem error prone.; - In CreateDatabaseStep, I put secret credentials in the pod configuration. That's not ideal, but I don't think it is a serious problem, because nobody who isn't privileged can read the pods, and I can fix it in a later PR (the create database step should generate the passwords, not ci2).; - I disabled the fixme pylint message (on # FIXME comments), since are fixmes are longer lived than a single change sometimes.; - I'm slightly confused about runImage (which generates a batch job) and deploy of a pod spec (which runs kubectl apply as a batch job). Right now, runImage always runs in batch-pods, and a deploy job runs in whatever namespace you specify. Fixes https://github.com/hail-is/hail/issues/5903",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5891
https://github.com/hail-is/hail/pull/5891:330,Testability,test,tests,330,"OK, I gave you maximum spicy. I don't think it's so bad, but let me know if you want me to cut it up. Some remarks:; - This PR successfully tests (and it passes!) and then cleans up this branch: https://github.com/hail-is/hail/pull/5842. See `build.yaml`. It's a thing of beauty (I think).; - That branch has everything but Scala tests and dataproc/cloudtools tests. The latter are easy, the former are a little messy since I want to test against a test jar, and I've decided to switch to maven for that.; - No support for publish or deploy yet.; - There are synchronous calls it `git` in various places which can make the UI sluggish. I'll fix those in another PR.; - Work remains to validate build.yaml and the deploy step yaml.; - I currently run jinja2 if the file (Dockerfile or deployment yaml) ends in `.in`, but I think I'm going to make it unconditional. `.in` just seem error prone.; - In CreateDatabaseStep, I put secret credentials in the pod configuration. That's not ideal, but I don't think it is a serious problem, because nobody who isn't privileged can read the pods, and I can fix it in a later PR (the create database step should generate the passwords, not ci2).; - I disabled the fixme pylint message (on # FIXME comments), since are fixmes are longer lived than a single change sometimes.; - I'm slightly confused about runImage (which generates a batch job) and deploy of a pod spec (which runs kubectl apply as a batch job). Right now, runImage always runs in batch-pods, and a deploy job runs in whatever namespace you specify. Fixes https://github.com/hail-is/hail/issues/5903",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5891
https://github.com/hail-is/hail/pull/5891:360,Testability,test,tests,360,"OK, I gave you maximum spicy. I don't think it's so bad, but let me know if you want me to cut it up. Some remarks:; - This PR successfully tests (and it passes!) and then cleans up this branch: https://github.com/hail-is/hail/pull/5842. See `build.yaml`. It's a thing of beauty (I think).; - That branch has everything but Scala tests and dataproc/cloudtools tests. The latter are easy, the former are a little messy since I want to test against a test jar, and I've decided to switch to maven for that.; - No support for publish or deploy yet.; - There are synchronous calls it `git` in various places which can make the UI sluggish. I'll fix those in another PR.; - Work remains to validate build.yaml and the deploy step yaml.; - I currently run jinja2 if the file (Dockerfile or deployment yaml) ends in `.in`, but I think I'm going to make it unconditional. `.in` just seem error prone.; - In CreateDatabaseStep, I put secret credentials in the pod configuration. That's not ideal, but I don't think it is a serious problem, because nobody who isn't privileged can read the pods, and I can fix it in a later PR (the create database step should generate the passwords, not ci2).; - I disabled the fixme pylint message (on # FIXME comments), since are fixmes are longer lived than a single change sometimes.; - I'm slightly confused about runImage (which generates a batch job) and deploy of a pod spec (which runs kubectl apply as a batch job). Right now, runImage always runs in batch-pods, and a deploy job runs in whatever namespace you specify. Fixes https://github.com/hail-is/hail/issues/5903",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5891
https://github.com/hail-is/hail/pull/5891:434,Testability,test,test,434,"OK, I gave you maximum spicy. I don't think it's so bad, but let me know if you want me to cut it up. Some remarks:; - This PR successfully tests (and it passes!) and then cleans up this branch: https://github.com/hail-is/hail/pull/5842. See `build.yaml`. It's a thing of beauty (I think).; - That branch has everything but Scala tests and dataproc/cloudtools tests. The latter are easy, the former are a little messy since I want to test against a test jar, and I've decided to switch to maven for that.; - No support for publish or deploy yet.; - There are synchronous calls it `git` in various places which can make the UI sluggish. I'll fix those in another PR.; - Work remains to validate build.yaml and the deploy step yaml.; - I currently run jinja2 if the file (Dockerfile or deployment yaml) ends in `.in`, but I think I'm going to make it unconditional. `.in` just seem error prone.; - In CreateDatabaseStep, I put secret credentials in the pod configuration. That's not ideal, but I don't think it is a serious problem, because nobody who isn't privileged can read the pods, and I can fix it in a later PR (the create database step should generate the passwords, not ci2).; - I disabled the fixme pylint message (on # FIXME comments), since are fixmes are longer lived than a single change sometimes.; - I'm slightly confused about runImage (which generates a batch job) and deploy of a pod spec (which runs kubectl apply as a batch job). Right now, runImage always runs in batch-pods, and a deploy job runs in whatever namespace you specify. Fixes https://github.com/hail-is/hail/issues/5903",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5891
https://github.com/hail-is/hail/pull/5891:449,Testability,test,test,449,"OK, I gave you maximum spicy. I don't think it's so bad, but let me know if you want me to cut it up. Some remarks:; - This PR successfully tests (and it passes!) and then cleans up this branch: https://github.com/hail-is/hail/pull/5842. See `build.yaml`. It's a thing of beauty (I think).; - That branch has everything but Scala tests and dataproc/cloudtools tests. The latter are easy, the former are a little messy since I want to test against a test jar, and I've decided to switch to maven for that.; - No support for publish or deploy yet.; - There are synchronous calls it `git` in various places which can make the UI sluggish. I'll fix those in another PR.; - Work remains to validate build.yaml and the deploy step yaml.; - I currently run jinja2 if the file (Dockerfile or deployment yaml) ends in `.in`, but I think I'm going to make it unconditional. `.in` just seem error prone.; - In CreateDatabaseStep, I put secret credentials in the pod configuration. That's not ideal, but I don't think it is a serious problem, because nobody who isn't privileged can read the pods, and I can fix it in a later PR (the create database step should generate the passwords, not ci2).; - I disabled the fixme pylint message (on # FIXME comments), since are fixmes are longer lived than a single change sometimes.; - I'm slightly confused about runImage (which generates a batch job) and deploy of a pod spec (which runs kubectl apply as a batch job). Right now, runImage always runs in batch-pods, and a deploy job runs in whatever namespace you specify. Fixes https://github.com/hail-is/hail/issues/5903",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5891
https://github.com/hail-is/hail/pull/5893:4,Security,authenticat,authentication,4,"add authentication to apiserver. When both PRs are in I will abstract this duplicate code into hailjwt or somewhere else. cc: @cseed, I'm pretty sure we were never testing apiserver?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5893
https://github.com/hail-is/hail/pull/5893:164,Testability,test,testing,164,"add authentication to apiserver. When both PRs are in I will abstract this duplicate code into hailjwt or somewhere else. cc: @cseed, I'm pretty sure we were never testing apiserver?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5893
https://github.com/hail-is/hail/pull/5897:26,Testability,test,tests,26,Needed so #5866 will pass tests.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5897
https://github.com/hail-is/hail/issues/5898:22,Availability,Error,Error,22,"HTTPError: 413 Client Error: Request Entity Too Large. FYI, cranking up the relevant setting will only sort of fix the problem.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5898
https://github.com/hail-is/hail/pull/5905:15,Deployability,install,installed,15,"Conda is still installed in the pr-builder, and in hail/hail-ci-build.sh to test pip install. I'll rip those out in later PRs and we'll be 100% conda-free.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5905
https://github.com/hail-is/hail/pull/5905:85,Deployability,install,install,85,"Conda is still installed in the pr-builder, and in hail/hail-ci-build.sh to test pip install. I'll rip those out in later PRs and we'll be 100% conda-free.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5905
https://github.com/hail-is/hail/pull/5905:76,Testability,test,test,76,"Conda is still installed in the pr-builder, and in hail/hail-ci-build.sh to test pip install. I'll rip those out in later PRs and we'll be 100% conda-free.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5905
https://github.com/hail-is/hail/pull/5906:249,Deployability,configurat,configurations,249,To make uber jar: `mvn assembly:single`. `compile` automatically runs `make` for the NativeLib stuff.; `clean` automatically runs `make clean` for NativeLib; Not sure if I needed to incorporate `nativeLibTest` or `nativeLibPrebuilt`. Added two test configurations. One is for all tests and the other is for the set of tests with HAIL_ENABLE_CPP_CODEGEN=1. I double checked the Python tests pass with the uber jar. The test output doesn't have the nice formatting that we have in Gradle. Would be some work with listeners and reporters to do that: http://maven.apache.org/surefire/maven-surefire-plugin/examples/testng.html#. There also isn't the `check` input and some other bells and whistles we have in Gradle. I had to add `com.google.inject:guice` to get rid of some compile warnings with the test-jar. Let me know if there's other things to add/enable or if this is good enough for ci2. We should probably add some CI tests for this in a makefile somewhere.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5906
https://github.com/hail-is/hail/pull/5906:738,Integrability,inject,inject,738,To make uber jar: `mvn assembly:single`. `compile` automatically runs `make` for the NativeLib stuff.; `clean` automatically runs `make clean` for NativeLib; Not sure if I needed to incorporate `nativeLibTest` or `nativeLibPrebuilt`. Added two test configurations. One is for all tests and the other is for the set of tests with HAIL_ENABLE_CPP_CODEGEN=1. I double checked the Python tests pass with the uber jar. The test output doesn't have the nice formatting that we have in Gradle. Would be some work with listeners and reporters to do that: http://maven.apache.org/surefire/maven-surefire-plugin/examples/testng.html#. There also isn't the `check` input and some other bells and whistles we have in Gradle. I had to add `com.google.inject:guice` to get rid of some compile warnings with the test-jar. Let me know if there's other things to add/enable or if this is good enough for ci2. We should probably add some CI tests for this in a makefile somewhere.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5906
https://github.com/hail-is/hail/pull/5906:249,Modifiability,config,configurations,249,To make uber jar: `mvn assembly:single`. `compile` automatically runs `make` for the NativeLib stuff.; `clean` automatically runs `make clean` for NativeLib; Not sure if I needed to incorporate `nativeLibTest` or `nativeLibPrebuilt`. Added two test configurations. One is for all tests and the other is for the set of tests with HAIL_ENABLE_CPP_CODEGEN=1. I double checked the Python tests pass with the uber jar. The test output doesn't have the nice formatting that we have in Gradle. Would be some work with listeners and reporters to do that: http://maven.apache.org/surefire/maven-surefire-plugin/examples/testng.html#. There also isn't the `check` input and some other bells and whistles we have in Gradle. I had to add `com.google.inject:guice` to get rid of some compile warnings with the test-jar. Let me know if there's other things to add/enable or if this is good enough for ci2. We should probably add some CI tests for this in a makefile somewhere.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5906
https://github.com/hail-is/hail/pull/5906:595,Modifiability,plugin,plugin,595,To make uber jar: `mvn assembly:single`. `compile` automatically runs `make` for the NativeLib stuff.; `clean` automatically runs `make clean` for NativeLib; Not sure if I needed to incorporate `nativeLibTest` or `nativeLibPrebuilt`. Added two test configurations. One is for all tests and the other is for the set of tests with HAIL_ENABLE_CPP_CODEGEN=1. I double checked the Python tests pass with the uber jar. The test output doesn't have the nice formatting that we have in Gradle. Would be some work with listeners and reporters to do that: http://maven.apache.org/surefire/maven-surefire-plugin/examples/testng.html#. There also isn't the `check` input and some other bells and whistles we have in Gradle. I had to add `com.google.inject:guice` to get rid of some compile warnings with the test-jar. Let me know if there's other things to add/enable or if this is good enough for ci2. We should probably add some CI tests for this in a makefile somewhere.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5906
https://github.com/hail-is/hail/pull/5906:738,Security,inject,inject,738,To make uber jar: `mvn assembly:single`. `compile` automatically runs `make` for the NativeLib stuff.; `clean` automatically runs `make clean` for NativeLib; Not sure if I needed to incorporate `nativeLibTest` or `nativeLibPrebuilt`. Added two test configurations. One is for all tests and the other is for the set of tests with HAIL_ENABLE_CPP_CODEGEN=1. I double checked the Python tests pass with the uber jar. The test output doesn't have the nice formatting that we have in Gradle. Would be some work with listeners and reporters to do that: http://maven.apache.org/surefire/maven-surefire-plugin/examples/testng.html#. There also isn't the `check` input and some other bells and whistles we have in Gradle. I had to add `com.google.inject:guice` to get rid of some compile warnings with the test-jar. Let me know if there's other things to add/enable or if this is good enough for ci2. We should probably add some CI tests for this in a makefile somewhere.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5906
https://github.com/hail-is/hail/pull/5906:244,Testability,test,test,244,To make uber jar: `mvn assembly:single`. `compile` automatically runs `make` for the NativeLib stuff.; `clean` automatically runs `make clean` for NativeLib; Not sure if I needed to incorporate `nativeLibTest` or `nativeLibPrebuilt`. Added two test configurations. One is for all tests and the other is for the set of tests with HAIL_ENABLE_CPP_CODEGEN=1. I double checked the Python tests pass with the uber jar. The test output doesn't have the nice formatting that we have in Gradle. Would be some work with listeners and reporters to do that: http://maven.apache.org/surefire/maven-surefire-plugin/examples/testng.html#. There also isn't the `check` input and some other bells and whistles we have in Gradle. I had to add `com.google.inject:guice` to get rid of some compile warnings with the test-jar. Let me know if there's other things to add/enable or if this is good enough for ci2. We should probably add some CI tests for this in a makefile somewhere.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5906
https://github.com/hail-is/hail/pull/5906:280,Testability,test,tests,280,To make uber jar: `mvn assembly:single`. `compile` automatically runs `make` for the NativeLib stuff.; `clean` automatically runs `make clean` for NativeLib; Not sure if I needed to incorporate `nativeLibTest` or `nativeLibPrebuilt`. Added two test configurations. One is for all tests and the other is for the set of tests with HAIL_ENABLE_CPP_CODEGEN=1. I double checked the Python tests pass with the uber jar. The test output doesn't have the nice formatting that we have in Gradle. Would be some work with listeners and reporters to do that: http://maven.apache.org/surefire/maven-surefire-plugin/examples/testng.html#. There also isn't the `check` input and some other bells and whistles we have in Gradle. I had to add `com.google.inject:guice` to get rid of some compile warnings with the test-jar. Let me know if there's other things to add/enable or if this is good enough for ci2. We should probably add some CI tests for this in a makefile somewhere.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5906
https://github.com/hail-is/hail/pull/5906:318,Testability,test,tests,318,To make uber jar: `mvn assembly:single`. `compile` automatically runs `make` for the NativeLib stuff.; `clean` automatically runs `make clean` for NativeLib; Not sure if I needed to incorporate `nativeLibTest` or `nativeLibPrebuilt`. Added two test configurations. One is for all tests and the other is for the set of tests with HAIL_ENABLE_CPP_CODEGEN=1. I double checked the Python tests pass with the uber jar. The test output doesn't have the nice formatting that we have in Gradle. Would be some work with listeners and reporters to do that: http://maven.apache.org/surefire/maven-surefire-plugin/examples/testng.html#. There also isn't the `check` input and some other bells and whistles we have in Gradle. I had to add `com.google.inject:guice` to get rid of some compile warnings with the test-jar. Let me know if there's other things to add/enable or if this is good enough for ci2. We should probably add some CI tests for this in a makefile somewhere.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5906
https://github.com/hail-is/hail/pull/5906:384,Testability,test,tests,384,To make uber jar: `mvn assembly:single`. `compile` automatically runs `make` for the NativeLib stuff.; `clean` automatically runs `make clean` for NativeLib; Not sure if I needed to incorporate `nativeLibTest` or `nativeLibPrebuilt`. Added two test configurations. One is for all tests and the other is for the set of tests with HAIL_ENABLE_CPP_CODEGEN=1. I double checked the Python tests pass with the uber jar. The test output doesn't have the nice formatting that we have in Gradle. Would be some work with listeners and reporters to do that: http://maven.apache.org/surefire/maven-surefire-plugin/examples/testng.html#. There also isn't the `check` input and some other bells and whistles we have in Gradle. I had to add `com.google.inject:guice` to get rid of some compile warnings with the test-jar. Let me know if there's other things to add/enable or if this is good enough for ci2. We should probably add some CI tests for this in a makefile somewhere.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5906
https://github.com/hail-is/hail/pull/5906:418,Testability,test,test,418,To make uber jar: `mvn assembly:single`. `compile` automatically runs `make` for the NativeLib stuff.; `clean` automatically runs `make clean` for NativeLib; Not sure if I needed to incorporate `nativeLibTest` or `nativeLibPrebuilt`. Added two test configurations. One is for all tests and the other is for the set of tests with HAIL_ENABLE_CPP_CODEGEN=1. I double checked the Python tests pass with the uber jar. The test output doesn't have the nice formatting that we have in Gradle. Would be some work with listeners and reporters to do that: http://maven.apache.org/surefire/maven-surefire-plugin/examples/testng.html#. There also isn't the `check` input and some other bells and whistles we have in Gradle. I had to add `com.google.inject:guice` to get rid of some compile warnings with the test-jar. Let me know if there's other things to add/enable or if this is good enough for ci2. We should probably add some CI tests for this in a makefile somewhere.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5906
https://github.com/hail-is/hail/pull/5906:611,Testability,test,testng,611,To make uber jar: `mvn assembly:single`. `compile` automatically runs `make` for the NativeLib stuff.; `clean` automatically runs `make clean` for NativeLib; Not sure if I needed to incorporate `nativeLibTest` or `nativeLibPrebuilt`. Added two test configurations. One is for all tests and the other is for the set of tests with HAIL_ENABLE_CPP_CODEGEN=1. I double checked the Python tests pass with the uber jar. The test output doesn't have the nice formatting that we have in Gradle. Would be some work with listeners and reporters to do that: http://maven.apache.org/surefire/maven-surefire-plugin/examples/testng.html#. There also isn't the `check` input and some other bells and whistles we have in Gradle. I had to add `com.google.inject:guice` to get rid of some compile warnings with the test-jar. Let me know if there's other things to add/enable or if this is good enough for ci2. We should probably add some CI tests for this in a makefile somewhere.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5906
https://github.com/hail-is/hail/pull/5906:797,Testability,test,test-jar,797,To make uber jar: `mvn assembly:single`. `compile` automatically runs `make` for the NativeLib stuff.; `clean` automatically runs `make clean` for NativeLib; Not sure if I needed to incorporate `nativeLibTest` or `nativeLibPrebuilt`. Added two test configurations. One is for all tests and the other is for the set of tests with HAIL_ENABLE_CPP_CODEGEN=1. I double checked the Python tests pass with the uber jar. The test output doesn't have the nice formatting that we have in Gradle. Would be some work with listeners and reporters to do that: http://maven.apache.org/surefire/maven-surefire-plugin/examples/testng.html#. There also isn't the `check` input and some other bells and whistles we have in Gradle. I had to add `com.google.inject:guice` to get rid of some compile warnings with the test-jar. Let me know if there's other things to add/enable or if this is good enough for ci2. We should probably add some CI tests for this in a makefile somewhere.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5906
https://github.com/hail-is/hail/pull/5906:923,Testability,test,tests,923,To make uber jar: `mvn assembly:single`. `compile` automatically runs `make` for the NativeLib stuff.; `clean` automatically runs `make clean` for NativeLib; Not sure if I needed to incorporate `nativeLibTest` or `nativeLibPrebuilt`. Added two test configurations. One is for all tests and the other is for the set of tests with HAIL_ENABLE_CPP_CODEGEN=1. I double checked the Python tests pass with the uber jar. The test output doesn't have the nice formatting that we have in Gradle. Would be some work with listeners and reporters to do that: http://maven.apache.org/surefire/maven-surefire-plugin/examples/testng.html#. There also isn't the `check` input and some other bells and whistles we have in Gradle. I had to add `com.google.inject:guice` to get rid of some compile warnings with the test-jar. Let me know if there's other things to add/enable or if this is good enough for ci2. We should probably add some CI tests for this in a makefile somewhere.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5906
https://github.com/hail-is/hail/pull/5907:144,Availability,error,error,144,Stacked on: https://github.com/hail-is/hail/pull/5891. I found getting .in (or not) consistent between the configuration and the files was just error prone. I think this is just simpler.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5907
https://github.com/hail-is/hail/pull/5907:107,Deployability,configurat,configuration,107,Stacked on: https://github.com/hail-is/hail/pull/5891. I found getting .in (or not) consistent between the configuration and the files was just error prone. I think this is just simpler.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5907
https://github.com/hail-is/hail/pull/5907:107,Modifiability,config,configuration,107,Stacked on: https://github.com/hail-is/hail/pull/5891. I found getting .in (or not) consistent between the configuration and the files was just error prone. I think this is just simpler.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5907
https://github.com/hail-is/hail/pull/5907:178,Usability,simpl,simpler,178,Stacked on: https://github.com/hail-is/hail/pull/5891. I found getting .in (or not) consistent between the configuration and the files was just error prone. I think this is just simpler.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5907
https://github.com/hail-is/hail/pull/5910:280,Testability,log,log,280,"@cseed @tpoterba thoughts?. Once this is merged, we can just tell people to run. ```bash; curl -LO https://raw.githubusercontent.com/hail-is/hail/master/hail/diagnose.sh | bash; ```. and send us the output. We can include all sorts of useful diagnostics here. . I only upload the log and core files to a pastebin if you pass an argument to the script a la:. ```bash; curl -LO https://raw.githubusercontent.com/hail-is/hail/master/hail/diagnose.sh | bash -s foo; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5910
https://github.com/hail-is/hail/issues/5911:87,Testability,log,log,87,open issue at k8s: https://github.com/kubernetes-client/python-base/issues/57. Failing log for https://github.com/hail-is/hail/pull/5893: https://storage.googleapis.com/hail-ci-0-1/ci/734812b4147dfa7381ac4ff60bac1ead21a58af0/117c365c320fd4ae1f1ea06c3fcbd668cf318fc2/job.log (search for kubernetes.client.rest.ApiException). Definitely looks like k8s hiccuped based on some bad behavior in batch at this time as well. See attached logs; [ci.log](https://github.com/hail-is/hail/files/3096383/ci.log); [batch.log](https://github.com/hail-is/hail/files/3096386/batch.log),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5911
https://github.com/hail-is/hail/issues/5911:270,Testability,log,log,270,open issue at k8s: https://github.com/kubernetes-client/python-base/issues/57. Failing log for https://github.com/hail-is/hail/pull/5893: https://storage.googleapis.com/hail-ci-0-1/ci/734812b4147dfa7381ac4ff60bac1ead21a58af0/117c365c320fd4ae1f1ea06c3fcbd668cf318fc2/job.log (search for kubernetes.client.rest.ApiException). Definitely looks like k8s hiccuped based on some bad behavior in batch at this time as well. See attached logs; [ci.log](https://github.com/hail-is/hail/files/3096383/ci.log); [batch.log](https://github.com/hail-is/hail/files/3096386/batch.log),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5911
https://github.com/hail-is/hail/issues/5911:430,Testability,log,logs,430,open issue at k8s: https://github.com/kubernetes-client/python-base/issues/57. Failing log for https://github.com/hail-is/hail/pull/5893: https://storage.googleapis.com/hail-ci-0-1/ci/734812b4147dfa7381ac4ff60bac1ead21a58af0/117c365c320fd4ae1f1ea06c3fcbd668cf318fc2/job.log (search for kubernetes.client.rest.ApiException). Definitely looks like k8s hiccuped based on some bad behavior in batch at this time as well. See attached logs; [ci.log](https://github.com/hail-is/hail/files/3096383/ci.log); [batch.log](https://github.com/hail-is/hail/files/3096386/batch.log),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5911
https://github.com/hail-is/hail/issues/5911:440,Testability,log,log,440,open issue at k8s: https://github.com/kubernetes-client/python-base/issues/57. Failing log for https://github.com/hail-is/hail/pull/5893: https://storage.googleapis.com/hail-ci-0-1/ci/734812b4147dfa7381ac4ff60bac1ead21a58af0/117c365c320fd4ae1f1ea06c3fcbd668cf318fc2/job.log (search for kubernetes.client.rest.ApiException). Definitely looks like k8s hiccuped based on some bad behavior in batch at this time as well. See attached logs; [ci.log](https://github.com/hail-is/hail/files/3096383/ci.log); [batch.log](https://github.com/hail-is/hail/files/3096386/batch.log),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5911
https://github.com/hail-is/hail/issues/5911:494,Testability,log,log,494,open issue at k8s: https://github.com/kubernetes-client/python-base/issues/57. Failing log for https://github.com/hail-is/hail/pull/5893: https://storage.googleapis.com/hail-ci-0-1/ci/734812b4147dfa7381ac4ff60bac1ead21a58af0/117c365c320fd4ae1f1ea06c3fcbd668cf318fc2/job.log (search for kubernetes.client.rest.ApiException). Definitely looks like k8s hiccuped based on some bad behavior in batch at this time as well. See attached logs; [ci.log](https://github.com/hail-is/hail/files/3096383/ci.log); [batch.log](https://github.com/hail-is/hail/files/3096386/batch.log),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5911
https://github.com/hail-is/hail/issues/5911:507,Testability,log,log,507,open issue at k8s: https://github.com/kubernetes-client/python-base/issues/57. Failing log for https://github.com/hail-is/hail/pull/5893: https://storage.googleapis.com/hail-ci-0-1/ci/734812b4147dfa7381ac4ff60bac1ead21a58af0/117c365c320fd4ae1f1ea06c3fcbd668cf318fc2/job.log (search for kubernetes.client.rest.ApiException). Definitely looks like k8s hiccuped based on some bad behavior in batch at this time as well. See attached logs; [ci.log](https://github.com/hail-is/hail/files/3096383/ci.log); [batch.log](https://github.com/hail-is/hail/files/3096386/batch.log),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5911
https://github.com/hail-is/hail/issues/5911:564,Testability,log,log,564,open issue at k8s: https://github.com/kubernetes-client/python-base/issues/57. Failing log for https://github.com/hail-is/hail/pull/5893: https://storage.googleapis.com/hail-ci-0-1/ci/734812b4147dfa7381ac4ff60bac1ead21a58af0/117c365c320fd4ae1f1ea06c3fcbd668cf318fc2/job.log (search for kubernetes.client.rest.ApiException). Definitely looks like k8s hiccuped based on some bad behavior in batch at this time as well. See attached logs; [ci.log](https://github.com/hail-is/hail/files/3096383/ci.log); [batch.log](https://github.com/hail-is/hail/files/3096386/batch.log),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5911
https://github.com/hail-is/hail/pull/5916:120,Deployability,deploy,deploying,120,I'm using one hail environment or everything now and I need these to test hail. I also added twine which we'll need for deploying anything to PyPI including cloudtools.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5916
https://github.com/hail-is/hail/pull/5916:69,Testability,test,test,69,I'm using one hail environment or everything now and I need these to test hail. I also added twine which we'll need for deploying anything to PyPI including cloudtools.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5916
https://github.com/hail-is/hail/pull/5918:35,Deployability,deploy,deploy,35,fix Makefile typo. This caused the deploy to fail.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5918
https://github.com/hail-is/hail/issues/5920:357,Security,access,access,357,"- [x] Remove the reader permission created during testing. - [ ] Convert user service accounts permission from owner to read/write. - [ ] Give user email address read/write permissions to the bucket that we create on their behalf. - [ ] Ensure equivalent of chmod -R 600; right now appears that (at least) legacy owners don't have automatic, recursive read access to objects in their bucket.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5920
https://github.com/hail-is/hail/issues/5920:50,Testability,test,testing,50,"- [x] Remove the reader permission created during testing. - [ ] Convert user service accounts permission from owner to read/write. - [ ] Give user email address read/write permissions to the bucket that we create on their behalf. - [ ] Ensure equivalent of chmod -R 600; right now appears that (at least) legacy owners don't have automatic, recursive read access to objects in their bucket.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5920
https://github.com/hail-is/hail/pull/5921:234,Availability,error,error,234,"- All subprocess calls are async. The UI is much more responsive now.; - Make refresh (rather than heal) non-reentrant. There was a race condition where we could update the Github state of a PR we were actively deploying. This seemed error prone.; - We need to lock the repos until the build is fully deployed. I now protect pr.heal.; - Set `batch_changed = True` whenever heal needs to be rerun becomes some of its inputs (build_state, source or target sha, collection of PRs) changed. Next up: deploy.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5921
https://github.com/hail-is/hail/pull/5921:162,Deployability,update,update,162,"- All subprocess calls are async. The UI is much more responsive now.; - Make refresh (rather than heal) non-reentrant. There was a race condition where we could update the Github state of a PR we were actively deploying. This seemed error prone.; - We need to lock the repos until the build is fully deployed. I now protect pr.heal.; - Set `batch_changed = True` whenever heal needs to be rerun becomes some of its inputs (build_state, source or target sha, collection of PRs) changed. Next up: deploy.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5921
https://github.com/hail-is/hail/pull/5921:211,Deployability,deploy,deploying,211,"- All subprocess calls are async. The UI is much more responsive now.; - Make refresh (rather than heal) non-reentrant. There was a race condition where we could update the Github state of a PR we were actively deploying. This seemed error prone.; - We need to lock the repos until the build is fully deployed. I now protect pr.heal.; - Set `batch_changed = True` whenever heal needs to be rerun becomes some of its inputs (build_state, source or target sha, collection of PRs) changed. Next up: deploy.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5921
https://github.com/hail-is/hail/pull/5921:301,Deployability,deploy,deployed,301,"- All subprocess calls are async. The UI is much more responsive now.; - Make refresh (rather than heal) non-reentrant. There was a race condition where we could update the Github state of a PR we were actively deploying. This seemed error prone.; - We need to lock the repos until the build is fully deployed. I now protect pr.heal.; - Set `batch_changed = True` whenever heal needs to be rerun becomes some of its inputs (build_state, source or target sha, collection of PRs) changed. Next up: deploy.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5921
https://github.com/hail-is/hail/pull/5921:496,Deployability,deploy,deploy,496,"- All subprocess calls are async. The UI is much more responsive now.; - Make refresh (rather than heal) non-reentrant. There was a race condition where we could update the Github state of a PR we were actively deploying. This seemed error prone.; - We need to lock the repos until the build is fully deployed. I now protect pr.heal.; - Set `batch_changed = True` whenever heal needs to be rerun becomes some of its inputs (build_state, source or target sha, collection of PRs) changed. Next up: deploy.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5921
https://github.com/hail-is/hail/pull/5921:132,Performance,race condition,race condition,132,"- All subprocess calls are async. The UI is much more responsive now.; - Make refresh (rather than heal) non-reentrant. There was a race condition where we could update the Github state of a PR we were actively deploying. This seemed error prone.; - We need to lock the repos until the build is fully deployed. I now protect pr.heal.; - Set `batch_changed = True` whenever heal needs to be rerun becomes some of its inputs (build_state, source or target sha, collection of PRs) changed. Next up: deploy.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5921
https://github.com/hail-is/hail/pull/5921:54,Usability,responsiv,responsive,54,"- All subprocess calls are async. The UI is much more responsive now.; - Make refresh (rather than heal) non-reentrant. There was a race condition where we could update the Github state of a PR we were actively deploying. This seemed error prone.; - We need to lock the repos until the build is fully deployed. I now protect pr.heal.; - Set `batch_changed = True` whenever heal needs to be rerun becomes some of its inputs (build_state, source or target sha, collection of PRs) changed. Next up: deploy.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5921
https://github.com/hail-is/hail/pull/5922:754,Modifiability,variab,variables,754,"- Implement broadcasting of NDArrays. This essentially just gives NDArrays additional dimensions of length one, which changes the view onto the NDArray but not the underlying data. This allows us to promote smaller-dimensional NDArrays before a Map2 so both NDArrays have the same number of dimensions. The loops for the map will loop through the _unified shape_ of both child NDArrays with the same semantics as numpy (if corresponding dimensions are not equal, they are still compatible if one is 1 in which case you would take the larger one). Dimensions of length 1 have a stride of 0, so indexing into the array as if it were broadcast works and is totally free. The only additional benefit deforesting would actually give you is putting fewer loop variables into the calculation of the index.; - Restructured emit for NDArrays with stub-like emit method for deforesting so deforesting will be very easy to implement, but not currently deforesting anything. Thought this was necessary for `NDArrayReindex`. Ultimately it wasn't but is what we'll want to do next with maps anyway.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5922
https://github.com/hail-is/hail/pull/5922:838,Testability,stub,stub-like,838,"- Implement broadcasting of NDArrays. This essentially just gives NDArrays additional dimensions of length one, which changes the view onto the NDArray but not the underlying data. This allows us to promote smaller-dimensional NDArrays before a Map2 so both NDArrays have the same number of dimensions. The loops for the map will loop through the _unified shape_ of both child NDArrays with the same semantics as numpy (if corresponding dimensions are not equal, they are still compatible if one is 1 in which case you would take the larger one). Dimensions of length 1 have a stride of 0, so indexing into the array as if it were broadcast works and is totally free. The only additional benefit deforesting would actually give you is putting fewer loop variables into the calculation of the index.; - Restructured emit for NDArrays with stub-like emit method for deforesting so deforesting will be very easy to implement, but not currently deforesting anything. Thought this was necessary for `NDArrayReindex`. Ultimately it wasn't but is what we'll want to do next with maps anyway.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5922
https://github.com/hail-is/hail/pull/5927:4,Deployability,deploy,deployment,4,ci2 deployment fix,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5927
https://github.com/hail-is/hail/pull/5929:91,Integrability,depend,dependent,91,This will be easier to put right once we get rid of pr-builder and ci2 is building all the dependent images each time.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5929
https://github.com/hail-is/hail/pull/5935:60,Safety,avoid,avoid,60,"Originally #5355. Unfortunately, I had to squash commits to avoid bad rebasing conflicts, so I can't open the original PR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5935
https://github.com/hail-is/hail/pull/5937:140,Energy Efficiency,reduce,reduce,140,"Add `product` option to `Table.index`, and `MatrixTable.{index_rows, index_cols}`. Supports interval joins. Refactored the index methods to reduce code duplication, and make them more consistent with each other. Only case not supporting `product=True` is annotating columns of a matrix table with an interval keyed table.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5937
https://github.com/hail-is/hail/pull/5937:108,Modifiability,Refactor,Refactored,108,"Add `product` option to `Table.index`, and `MatrixTable.{index_rows, index_cols}`. Supports interval joins. Refactored the index methods to reduce code duplication, and make them more consistent with each other. Only case not supporting `product=True` is annotating columns of a matrix table with an interval keyed table.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5937
https://github.com/hail-is/hail/pull/5938:31,Deployability,deploy,deployable,31,"- but don't enable; - add back deployable to watched branches; - add deploy flag to build configuration; - added Code option that represents a version of code (but not quite a sha, because it might do a merge that produces a new sha) and can check itself out. I think this gets us pretty close. Working on ci2 tests now.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5938
https://github.com/hail-is/hail/pull/5938:69,Deployability,deploy,deploy,69,"- but don't enable; - add back deployable to watched branches; - add deploy flag to build configuration; - added Code option that represents a version of code (but not quite a sha, because it might do a merge that produces a new sha) and can check itself out. I think this gets us pretty close. Working on ci2 tests now.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5938
https://github.com/hail-is/hail/pull/5938:90,Deployability,configurat,configuration,90,"- but don't enable; - add back deployable to watched branches; - add deploy flag to build configuration; - added Code option that represents a version of code (but not quite a sha, because it might do a merge that produces a new sha) and can check itself out. I think this gets us pretty close. Working on ci2 tests now.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5938
https://github.com/hail-is/hail/pull/5938:90,Modifiability,config,configuration,90,"- but don't enable; - add back deployable to watched branches; - add deploy flag to build configuration; - added Code option that represents a version of code (but not quite a sha, because it might do a merge that produces a new sha) and can check itself out. I think this gets us pretty close. Working on ci2 tests now.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5938
https://github.com/hail-is/hail/pull/5938:310,Testability,test,tests,310,"- but don't enable; - add back deployable to watched branches; - add deploy flag to build configuration; - added Code option that represents a version of code (but not quite a sha, because it might do a merge that produces a new sha) and can check itself out. I think this gets us pretty close. Working on ci2 tests now.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5938
https://github.com/hail-is/hail/pull/5942:251,Security,authenticat,authentication,251,"I added `user` and `userdata` to the jobs and batch table. I also added `user` as a secondary index, which @danking also did. . I used the `ksa_name` in the jwt for now. @akotlar We should figure out what this should be instead. . This does not check authentication for `get_recent_events`. I also am concerned about the case where we can't access jobs, batches easily as there's no support for a super user who can view everything. Stacked on #5934",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5942
https://github.com/hail-is/hail/pull/5942:341,Security,access,access,341,"I added `user` and `userdata` to the jobs and batch table. I also added `user` as a secondary index, which @danking also did. . I used the `ksa_name` in the jwt for now. @akotlar We should figure out what this should be instead. . This does not check authentication for `get_recent_events`. I also am concerned about the case where we can't access jobs, batches easily as there's no support for a super user who can view everything. Stacked on #5934",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5942
https://github.com/hail-is/hail/issues/5944:37,Modifiability,variab,variables,37,"We should use __init__ to initialize variables to avoid duplication. This requires changing the init methods to not do any work and have static methods that do the database calls, construct pods, etc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5944
https://github.com/hail-is/hail/issues/5944:50,Safety,avoid,avoid,50,"We should use __init__ to initialize variables to avoid duplication. This requires changing the init methods to not do any work and have static methods that do the database calls, construct pods, etc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5944
https://github.com/hail-is/hail/pull/5951:302,Modifiability,variab,variables,302,"@cseed @tpoterba . There's now an option to disable the Unsafe warnings in javac. You have to `-XDenableSunApiLintControl` and then you can `@SuppressWarnings(""sunapi"")`. The changes that are not in build.gradle and build.sbt are just me fixing warnings. There were some meaningful things, like unused variables and some places we were unnecessarily allocating a tuple.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5951
https://github.com/hail-is/hail/pull/5951:56,Safety,Unsafe,Unsafe,56,"@cseed @tpoterba . There's now an option to disable the Unsafe warnings in javac. You have to `-XDenableSunApiLintControl` and then you can `@SuppressWarnings(""sunapi"")`. The changes that are not in build.gradle and build.sbt are just me fixing warnings. There were some meaningful things, like unused variables and some places we were unnecessarily allocating a tuple.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5951
https://github.com/hail-is/hail/pull/5952:84,Integrability,depend,dependency,84,ci needs hailjwt. this PR https://github.com/hail-is/hail/pull/5941 added a hailjwt dependency to batch client which broke CI.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5952
https://github.com/hail-is/hail/pull/5961:76,Security,sanitiz,sanitization,76,"These actually all pass, much to my surprise, because we're; doing great IR sanitization in Python. I have some changes in; another branch that introduced bugs that tests like this will; make easier to debug. It's quite satisfying to add a PR with only tests and no code changes.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5961
https://github.com/hail-is/hail/pull/5961:165,Testability,test,tests,165,"These actually all pass, much to my surprise, because we're; doing great IR sanitization in Python. I have some changes in; another branch that introduced bugs that tests like this will; make easier to debug. It's quite satisfying to add a PR with only tests and no code changes.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5961
https://github.com/hail-is/hail/pull/5961:253,Testability,test,tests,253,"These actually all pass, much to my surprise, because we're; doing great IR sanitization in Python. I have some changes in; another branch that introduced bugs that tests like this will; make easier to debug. It's quite satisfying to add a PR with only tests and no code changes.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5961
https://github.com/hail-is/hail/pull/5962:546,Deployability,pipeline,pipeline,546,"- Added idea of two types of outputs: internal and external. Internal outputs are copied between tasks and external outputs are user specified copies of files.; - Added `valid` and `mentioned` which are sets of resources tasks keep track of. `valid` checks whether the resources were properly declared. `mentioned` keeps a record of the resources used in the command for variables that need to be declared.; - Changed when outputs are copied for external outputs. TaskResourceFiles are copied when the task finishes rather than at the end of the pipeline. InputResourceFiles are copied at the beginning of the pipeline as one job.; - `inputs`, `internal_outputs`, and `external_outputs` only contain resource files (not resource groups) which simplified the code quite a bit!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5962
https://github.com/hail-is/hail/pull/5962:610,Deployability,pipeline,pipeline,610,"- Added idea of two types of outputs: internal and external. Internal outputs are copied between tasks and external outputs are user specified copies of files.; - Added `valid` and `mentioned` which are sets of resources tasks keep track of. `valid` checks whether the resources were properly declared. `mentioned` keeps a record of the resources used in the command for variables that need to be declared.; - Changed when outputs are copied for external outputs. TaskResourceFiles are copied when the task finishes rather than at the end of the pipeline. InputResourceFiles are copied at the beginning of the pipeline as one job.; - `inputs`, `internal_outputs`, and `external_outputs` only contain resource files (not resource groups) which simplified the code quite a bit!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5962
https://github.com/hail-is/hail/pull/5962:371,Modifiability,variab,variables,371,"- Added idea of two types of outputs: internal and external. Internal outputs are copied between tasks and external outputs are user specified copies of files.; - Added `valid` and `mentioned` which are sets of resources tasks keep track of. `valid` checks whether the resources were properly declared. `mentioned` keeps a record of the resources used in the command for variables that need to be declared.; - Changed when outputs are copied for external outputs. TaskResourceFiles are copied when the task finishes rather than at the end of the pipeline. InputResourceFiles are copied at the beginning of the pipeline as one job.; - `inputs`, `internal_outputs`, and `external_outputs` only contain resource files (not resource groups) which simplified the code quite a bit!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5962
https://github.com/hail-is/hail/pull/5962:743,Usability,simpl,simplified,743,"- Added idea of two types of outputs: internal and external. Internal outputs are copied between tasks and external outputs are user specified copies of files.; - Added `valid` and `mentioned` which are sets of resources tasks keep track of. `valid` checks whether the resources were properly declared. `mentioned` keeps a record of the resources used in the command for variables that need to be declared.; - Changed when outputs are copied for external outputs. TaskResourceFiles are copied when the task finishes rather than at the end of the pipeline. InputResourceFiles are copied at the beginning of the pipeline as one job.; - `inputs`, `internal_outputs`, and `external_outputs` only contain resource files (not resource groups) which simplified the code quite a bit!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5962
https://github.com/hail-is/hail/pull/5972:10,Deployability,update,updated,10,I already updated the test namespace secret.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5972
https://github.com/hail-is/hail/pull/5972:22,Testability,test,test,22,I already updated the test namespace secret.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5972
https://github.com/hail-is/hail/pull/5974:94,Deployability,configurat,configuration,94,"- removes everything related to ci1, pr-builder, hail-ci-*, etc.; - adds build.yaml ci2 build configuration; - makes master deployable (will probably fail)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5974
https://github.com/hail-is/hail/pull/5974:124,Deployability,deploy,deployable,124,"- removes everything related to ci1, pr-builder, hail-ci-*, etc.; - adds build.yaml ci2 build configuration; - makes master deployable (will probably fail)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5974
https://github.com/hail-is/hail/pull/5974:94,Modifiability,config,configuration,94,"- removes everything related to ci1, pr-builder, hail-ci-*, etc.; - adds build.yaml ci2 build configuration; - makes master deployable (will probably fail)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5974
https://github.com/hail-is/hail/pull/5979:269,Performance,optimiz,optimizer,269,"This PR will not be merged as-is, but split along the 3 commits contained within: ; - Add `Coalesce` IR node; - Expose pruning on FilterIntervals relational functions. These should be promoted to full IR nodes, especially after this PR.; - Add `ExtractIntervalFilters` optimizer pass. I also have yet to add tests for the last commit. What does this PR do?. ```python; In [2]: mt = hl.read_matrix_table('data/1kg.rep.mt'). In [3]: mt.filter_rows(mt.locus.contig == '16').count(); Hail: INFO: interval filter loaded 5 of 128 partitions; Out[3]: (384, 284). In [4]: mt.filter_rows(mt.locus.contig == '16').count_rows(); Hail: INFO: interval filter loaded 5 of 128 partitions; Out[4]: 384. In [5]: mt.filter_rows((mt.locus.contig == '16') | (mt.locus.contig == '19')).count(); Hail: INFO: interval filter loaded 10 of 128 partitions; Out[5]: (730, 284). In [6]: mt.filter_rows(hl.literal({'16', '19'}).contains(mt.locus.contig)).count_rows(); Hail: INFO: interval filter loaded 10 of 128 partitions; Out[6]: 730. In [7]: mt.filter_rows((mt.locus.contig == '16') & (mt.locus.position > 10_000_000)).count_rows(); Hail: INFO: interval filter loaded 2 of 128 partitions; Out[7]: 82. In [8]: mt.filter_rows((mt.locus.contig == '16') & (mt.locus.position > 10_000_000) & (mt.locus.position < 12_000_000)).count_rows(); Hail: INFO: interval filter loaded 5 of 128 partitions; Out[8]: 384. In [9]: mt.filter_rows(mt.locus == hl.parse_locus('1:3761547')).count(); Hail: INFO: interval filter loaded 1 of 128 partitions; Out[9]: (1, 284). In [10]: mt.filter_rows(hl.parse_locus_interval('16:20000000-30000000').contains(mt.locus)).count(); Hail: INFO: interval filter loaded 1 of 128 partitions; Out[10]: (35, 284); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5979
https://github.com/hail-is/hail/pull/5979:508,Performance,load,loaded,508,"This PR will not be merged as-is, but split along the 3 commits contained within: ; - Add `Coalesce` IR node; - Expose pruning on FilterIntervals relational functions. These should be promoted to full IR nodes, especially after this PR.; - Add `ExtractIntervalFilters` optimizer pass. I also have yet to add tests for the last commit. What does this PR do?. ```python; In [2]: mt = hl.read_matrix_table('data/1kg.rep.mt'). In [3]: mt.filter_rows(mt.locus.contig == '16').count(); Hail: INFO: interval filter loaded 5 of 128 partitions; Out[3]: (384, 284). In [4]: mt.filter_rows(mt.locus.contig == '16').count_rows(); Hail: INFO: interval filter loaded 5 of 128 partitions; Out[4]: 384. In [5]: mt.filter_rows((mt.locus.contig == '16') | (mt.locus.contig == '19')).count(); Hail: INFO: interval filter loaded 10 of 128 partitions; Out[5]: (730, 284). In [6]: mt.filter_rows(hl.literal({'16', '19'}).contains(mt.locus.contig)).count_rows(); Hail: INFO: interval filter loaded 10 of 128 partitions; Out[6]: 730. In [7]: mt.filter_rows((mt.locus.contig == '16') & (mt.locus.position > 10_000_000)).count_rows(); Hail: INFO: interval filter loaded 2 of 128 partitions; Out[7]: 82. In [8]: mt.filter_rows((mt.locus.contig == '16') & (mt.locus.position > 10_000_000) & (mt.locus.position < 12_000_000)).count_rows(); Hail: INFO: interval filter loaded 5 of 128 partitions; Out[8]: 384. In [9]: mt.filter_rows(mt.locus == hl.parse_locus('1:3761547')).count(); Hail: INFO: interval filter loaded 1 of 128 partitions; Out[9]: (1, 284). In [10]: mt.filter_rows(hl.parse_locus_interval('16:20000000-30000000').contains(mt.locus)).count(); Hail: INFO: interval filter loaded 1 of 128 partitions; Out[10]: (35, 284); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5979
https://github.com/hail-is/hail/pull/5979:646,Performance,load,loaded,646,"This PR will not be merged as-is, but split along the 3 commits contained within: ; - Add `Coalesce` IR node; - Expose pruning on FilterIntervals relational functions. These should be promoted to full IR nodes, especially after this PR.; - Add `ExtractIntervalFilters` optimizer pass. I also have yet to add tests for the last commit. What does this PR do?. ```python; In [2]: mt = hl.read_matrix_table('data/1kg.rep.mt'). In [3]: mt.filter_rows(mt.locus.contig == '16').count(); Hail: INFO: interval filter loaded 5 of 128 partitions; Out[3]: (384, 284). In [4]: mt.filter_rows(mt.locus.contig == '16').count_rows(); Hail: INFO: interval filter loaded 5 of 128 partitions; Out[4]: 384. In [5]: mt.filter_rows((mt.locus.contig == '16') | (mt.locus.contig == '19')).count(); Hail: INFO: interval filter loaded 10 of 128 partitions; Out[5]: (730, 284). In [6]: mt.filter_rows(hl.literal({'16', '19'}).contains(mt.locus.contig)).count_rows(); Hail: INFO: interval filter loaded 10 of 128 partitions; Out[6]: 730. In [7]: mt.filter_rows((mt.locus.contig == '16') & (mt.locus.position > 10_000_000)).count_rows(); Hail: INFO: interval filter loaded 2 of 128 partitions; Out[7]: 82. In [8]: mt.filter_rows((mt.locus.contig == '16') & (mt.locus.position > 10_000_000) & (mt.locus.position < 12_000_000)).count_rows(); Hail: INFO: interval filter loaded 5 of 128 partitions; Out[8]: 384. In [9]: mt.filter_rows(mt.locus == hl.parse_locus('1:3761547')).count(); Hail: INFO: interval filter loaded 1 of 128 partitions; Out[9]: (1, 284). In [10]: mt.filter_rows(hl.parse_locus_interval('16:20000000-30000000').contains(mt.locus)).count(); Hail: INFO: interval filter loaded 1 of 128 partitions; Out[10]: (35, 284); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5979
https://github.com/hail-is/hail/pull/5979:802,Performance,load,loaded,802,"This PR will not be merged as-is, but split along the 3 commits contained within: ; - Add `Coalesce` IR node; - Expose pruning on FilterIntervals relational functions. These should be promoted to full IR nodes, especially after this PR.; - Add `ExtractIntervalFilters` optimizer pass. I also have yet to add tests for the last commit. What does this PR do?. ```python; In [2]: mt = hl.read_matrix_table('data/1kg.rep.mt'). In [3]: mt.filter_rows(mt.locus.contig == '16').count(); Hail: INFO: interval filter loaded 5 of 128 partitions; Out[3]: (384, 284). In [4]: mt.filter_rows(mt.locus.contig == '16').count_rows(); Hail: INFO: interval filter loaded 5 of 128 partitions; Out[4]: 384. In [5]: mt.filter_rows((mt.locus.contig == '16') | (mt.locus.contig == '19')).count(); Hail: INFO: interval filter loaded 10 of 128 partitions; Out[5]: (730, 284). In [6]: mt.filter_rows(hl.literal({'16', '19'}).contains(mt.locus.contig)).count_rows(); Hail: INFO: interval filter loaded 10 of 128 partitions; Out[6]: 730. In [7]: mt.filter_rows((mt.locus.contig == '16') & (mt.locus.position > 10_000_000)).count_rows(); Hail: INFO: interval filter loaded 2 of 128 partitions; Out[7]: 82. In [8]: mt.filter_rows((mt.locus.contig == '16') & (mt.locus.position > 10_000_000) & (mt.locus.position < 12_000_000)).count_rows(); Hail: INFO: interval filter loaded 5 of 128 partitions; Out[8]: 384. In [9]: mt.filter_rows(mt.locus == hl.parse_locus('1:3761547')).count(); Hail: INFO: interval filter loaded 1 of 128 partitions; Out[9]: (1, 284). In [10]: mt.filter_rows(hl.parse_locus_interval('16:20000000-30000000').contains(mt.locus)).count(); Hail: INFO: interval filter loaded 1 of 128 partitions; Out[10]: (35, 284); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5979
https://github.com/hail-is/hail/pull/5979:968,Performance,load,loaded,968,"This PR will not be merged as-is, but split along the 3 commits contained within: ; - Add `Coalesce` IR node; - Expose pruning on FilterIntervals relational functions. These should be promoted to full IR nodes, especially after this PR.; - Add `ExtractIntervalFilters` optimizer pass. I also have yet to add tests for the last commit. What does this PR do?. ```python; In [2]: mt = hl.read_matrix_table('data/1kg.rep.mt'). In [3]: mt.filter_rows(mt.locus.contig == '16').count(); Hail: INFO: interval filter loaded 5 of 128 partitions; Out[3]: (384, 284). In [4]: mt.filter_rows(mt.locus.contig == '16').count_rows(); Hail: INFO: interval filter loaded 5 of 128 partitions; Out[4]: 384. In [5]: mt.filter_rows((mt.locus.contig == '16') | (mt.locus.contig == '19')).count(); Hail: INFO: interval filter loaded 10 of 128 partitions; Out[5]: (730, 284). In [6]: mt.filter_rows(hl.literal({'16', '19'}).contains(mt.locus.contig)).count_rows(); Hail: INFO: interval filter loaded 10 of 128 partitions; Out[6]: 730. In [7]: mt.filter_rows((mt.locus.contig == '16') & (mt.locus.position > 10_000_000)).count_rows(); Hail: INFO: interval filter loaded 2 of 128 partitions; Out[7]: 82. In [8]: mt.filter_rows((mt.locus.contig == '16') & (mt.locus.position > 10_000_000) & (mt.locus.position < 12_000_000)).count_rows(); Hail: INFO: interval filter loaded 5 of 128 partitions; Out[8]: 384. In [9]: mt.filter_rows(mt.locus == hl.parse_locus('1:3761547')).count(); Hail: INFO: interval filter loaded 1 of 128 partitions; Out[9]: (1, 284). In [10]: mt.filter_rows(hl.parse_locus_interval('16:20000000-30000000').contains(mt.locus)).count(); Hail: INFO: interval filter loaded 1 of 128 partitions; Out[10]: (35, 284); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5979
https://github.com/hail-is/hail/pull/5979:1137,Performance,load,loaded,1137,"This PR will not be merged as-is, but split along the 3 commits contained within: ; - Add `Coalesce` IR node; - Expose pruning on FilterIntervals relational functions. These should be promoted to full IR nodes, especially after this PR.; - Add `ExtractIntervalFilters` optimizer pass. I also have yet to add tests for the last commit. What does this PR do?. ```python; In [2]: mt = hl.read_matrix_table('data/1kg.rep.mt'). In [3]: mt.filter_rows(mt.locus.contig == '16').count(); Hail: INFO: interval filter loaded 5 of 128 partitions; Out[3]: (384, 284). In [4]: mt.filter_rows(mt.locus.contig == '16').count_rows(); Hail: INFO: interval filter loaded 5 of 128 partitions; Out[4]: 384. In [5]: mt.filter_rows((mt.locus.contig == '16') | (mt.locus.contig == '19')).count(); Hail: INFO: interval filter loaded 10 of 128 partitions; Out[5]: (730, 284). In [6]: mt.filter_rows(hl.literal({'16', '19'}).contains(mt.locus.contig)).count_rows(); Hail: INFO: interval filter loaded 10 of 128 partitions; Out[6]: 730. In [7]: mt.filter_rows((mt.locus.contig == '16') & (mt.locus.position > 10_000_000)).count_rows(); Hail: INFO: interval filter loaded 2 of 128 partitions; Out[7]: 82. In [8]: mt.filter_rows((mt.locus.contig == '16') & (mt.locus.position > 10_000_000) & (mt.locus.position < 12_000_000)).count_rows(); Hail: INFO: interval filter loaded 5 of 128 partitions; Out[8]: 384. In [9]: mt.filter_rows(mt.locus == hl.parse_locus('1:3761547')).count(); Hail: INFO: interval filter loaded 1 of 128 partitions; Out[9]: (1, 284). In [10]: mt.filter_rows(hl.parse_locus_interval('16:20000000-30000000').contains(mt.locus)).count(); Hail: INFO: interval filter loaded 1 of 128 partitions; Out[10]: (35, 284); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5979
https://github.com/hail-is/hail/pull/5979:1339,Performance,load,loaded,1339,"This PR will not be merged as-is, but split along the 3 commits contained within: ; - Add `Coalesce` IR node; - Expose pruning on FilterIntervals relational functions. These should be promoted to full IR nodes, especially after this PR.; - Add `ExtractIntervalFilters` optimizer pass. I also have yet to add tests for the last commit. What does this PR do?. ```python; In [2]: mt = hl.read_matrix_table('data/1kg.rep.mt'). In [3]: mt.filter_rows(mt.locus.contig == '16').count(); Hail: INFO: interval filter loaded 5 of 128 partitions; Out[3]: (384, 284). In [4]: mt.filter_rows(mt.locus.contig == '16').count_rows(); Hail: INFO: interval filter loaded 5 of 128 partitions; Out[4]: 384. In [5]: mt.filter_rows((mt.locus.contig == '16') | (mt.locus.contig == '19')).count(); Hail: INFO: interval filter loaded 10 of 128 partitions; Out[5]: (730, 284). In [6]: mt.filter_rows(hl.literal({'16', '19'}).contains(mt.locus.contig)).count_rows(); Hail: INFO: interval filter loaded 10 of 128 partitions; Out[6]: 730. In [7]: mt.filter_rows((mt.locus.contig == '16') & (mt.locus.position > 10_000_000)).count_rows(); Hail: INFO: interval filter loaded 2 of 128 partitions; Out[7]: 82. In [8]: mt.filter_rows((mt.locus.contig == '16') & (mt.locus.position > 10_000_000) & (mt.locus.position < 12_000_000)).count_rows(); Hail: INFO: interval filter loaded 5 of 128 partitions; Out[8]: 384. In [9]: mt.filter_rows(mt.locus == hl.parse_locus('1:3761547')).count(); Hail: INFO: interval filter loaded 1 of 128 partitions; Out[9]: (1, 284). In [10]: mt.filter_rows(hl.parse_locus_interval('16:20000000-30000000').contains(mt.locus)).count(); Hail: INFO: interval filter loaded 1 of 128 partitions; Out[10]: (35, 284); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5979
https://github.com/hail-is/hail/pull/5979:1481,Performance,load,loaded,1481,"This PR will not be merged as-is, but split along the 3 commits contained within: ; - Add `Coalesce` IR node; - Expose pruning on FilterIntervals relational functions. These should be promoted to full IR nodes, especially after this PR.; - Add `ExtractIntervalFilters` optimizer pass. I also have yet to add tests for the last commit. What does this PR do?. ```python; In [2]: mt = hl.read_matrix_table('data/1kg.rep.mt'). In [3]: mt.filter_rows(mt.locus.contig == '16').count(); Hail: INFO: interval filter loaded 5 of 128 partitions; Out[3]: (384, 284). In [4]: mt.filter_rows(mt.locus.contig == '16').count_rows(); Hail: INFO: interval filter loaded 5 of 128 partitions; Out[4]: 384. In [5]: mt.filter_rows((mt.locus.contig == '16') | (mt.locus.contig == '19')).count(); Hail: INFO: interval filter loaded 10 of 128 partitions; Out[5]: (730, 284). In [6]: mt.filter_rows(hl.literal({'16', '19'}).contains(mt.locus.contig)).count_rows(); Hail: INFO: interval filter loaded 10 of 128 partitions; Out[6]: 730. In [7]: mt.filter_rows((mt.locus.contig == '16') & (mt.locus.position > 10_000_000)).count_rows(); Hail: INFO: interval filter loaded 2 of 128 partitions; Out[7]: 82. In [8]: mt.filter_rows((mt.locus.contig == '16') & (mt.locus.position > 10_000_000) & (mt.locus.position < 12_000_000)).count_rows(); Hail: INFO: interval filter loaded 5 of 128 partitions; Out[8]: 384. In [9]: mt.filter_rows(mt.locus == hl.parse_locus('1:3761547')).count(); Hail: INFO: interval filter loaded 1 of 128 partitions; Out[9]: (1, 284). In [10]: mt.filter_rows(hl.parse_locus_interval('16:20000000-30000000').contains(mt.locus)).count(); Hail: INFO: interval filter loaded 1 of 128 partitions; Out[10]: (35, 284); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5979
https://github.com/hail-is/hail/pull/5979:1656,Performance,load,loaded,1656,"This PR will not be merged as-is, but split along the 3 commits contained within: ; - Add `Coalesce` IR node; - Expose pruning on FilterIntervals relational functions. These should be promoted to full IR nodes, especially after this PR.; - Add `ExtractIntervalFilters` optimizer pass. I also have yet to add tests for the last commit. What does this PR do?. ```python; In [2]: mt = hl.read_matrix_table('data/1kg.rep.mt'). In [3]: mt.filter_rows(mt.locus.contig == '16').count(); Hail: INFO: interval filter loaded 5 of 128 partitions; Out[3]: (384, 284). In [4]: mt.filter_rows(mt.locus.contig == '16').count_rows(); Hail: INFO: interval filter loaded 5 of 128 partitions; Out[4]: 384. In [5]: mt.filter_rows((mt.locus.contig == '16') | (mt.locus.contig == '19')).count(); Hail: INFO: interval filter loaded 10 of 128 partitions; Out[5]: (730, 284). In [6]: mt.filter_rows(hl.literal({'16', '19'}).contains(mt.locus.contig)).count_rows(); Hail: INFO: interval filter loaded 10 of 128 partitions; Out[6]: 730. In [7]: mt.filter_rows((mt.locus.contig == '16') & (mt.locus.position > 10_000_000)).count_rows(); Hail: INFO: interval filter loaded 2 of 128 partitions; Out[7]: 82. In [8]: mt.filter_rows((mt.locus.contig == '16') & (mt.locus.position > 10_000_000) & (mt.locus.position < 12_000_000)).count_rows(); Hail: INFO: interval filter loaded 5 of 128 partitions; Out[8]: 384. In [9]: mt.filter_rows(mt.locus == hl.parse_locus('1:3761547')).count(); Hail: INFO: interval filter loaded 1 of 128 partitions; Out[9]: (1, 284). In [10]: mt.filter_rows(hl.parse_locus_interval('16:20000000-30000000').contains(mt.locus)).count(); Hail: INFO: interval filter loaded 1 of 128 partitions; Out[10]: (35, 284); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5979
https://github.com/hail-is/hail/pull/5979:112,Security,Expose,Expose,112,"This PR will not be merged as-is, but split along the 3 commits contained within: ; - Add `Coalesce` IR node; - Expose pruning on FilterIntervals relational functions. These should be promoted to full IR nodes, especially after this PR.; - Add `ExtractIntervalFilters` optimizer pass. I also have yet to add tests for the last commit. What does this PR do?. ```python; In [2]: mt = hl.read_matrix_table('data/1kg.rep.mt'). In [3]: mt.filter_rows(mt.locus.contig == '16').count(); Hail: INFO: interval filter loaded 5 of 128 partitions; Out[3]: (384, 284). In [4]: mt.filter_rows(mt.locus.contig == '16').count_rows(); Hail: INFO: interval filter loaded 5 of 128 partitions; Out[4]: 384. In [5]: mt.filter_rows((mt.locus.contig == '16') | (mt.locus.contig == '19')).count(); Hail: INFO: interval filter loaded 10 of 128 partitions; Out[5]: (730, 284). In [6]: mt.filter_rows(hl.literal({'16', '19'}).contains(mt.locus.contig)).count_rows(); Hail: INFO: interval filter loaded 10 of 128 partitions; Out[6]: 730. In [7]: mt.filter_rows((mt.locus.contig == '16') & (mt.locus.position > 10_000_000)).count_rows(); Hail: INFO: interval filter loaded 2 of 128 partitions; Out[7]: 82. In [8]: mt.filter_rows((mt.locus.contig == '16') & (mt.locus.position > 10_000_000) & (mt.locus.position < 12_000_000)).count_rows(); Hail: INFO: interval filter loaded 5 of 128 partitions; Out[8]: 384. In [9]: mt.filter_rows(mt.locus == hl.parse_locus('1:3761547')).count(); Hail: INFO: interval filter loaded 1 of 128 partitions; Out[9]: (1, 284). In [10]: mt.filter_rows(hl.parse_locus_interval('16:20000000-30000000').contains(mt.locus)).count(); Hail: INFO: interval filter loaded 1 of 128 partitions; Out[10]: (35, 284); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5979
https://github.com/hail-is/hail/pull/5979:308,Testability,test,tests,308,"This PR will not be merged as-is, but split along the 3 commits contained within: ; - Add `Coalesce` IR node; - Expose pruning on FilterIntervals relational functions. These should be promoted to full IR nodes, especially after this PR.; - Add `ExtractIntervalFilters` optimizer pass. I also have yet to add tests for the last commit. What does this PR do?. ```python; In [2]: mt = hl.read_matrix_table('data/1kg.rep.mt'). In [3]: mt.filter_rows(mt.locus.contig == '16').count(); Hail: INFO: interval filter loaded 5 of 128 partitions; Out[3]: (384, 284). In [4]: mt.filter_rows(mt.locus.contig == '16').count_rows(); Hail: INFO: interval filter loaded 5 of 128 partitions; Out[4]: 384. In [5]: mt.filter_rows((mt.locus.contig == '16') | (mt.locus.contig == '19')).count(); Hail: INFO: interval filter loaded 10 of 128 partitions; Out[5]: (730, 284). In [6]: mt.filter_rows(hl.literal({'16', '19'}).contains(mt.locus.contig)).count_rows(); Hail: INFO: interval filter loaded 10 of 128 partitions; Out[6]: 730. In [7]: mt.filter_rows((mt.locus.contig == '16') & (mt.locus.position > 10_000_000)).count_rows(); Hail: INFO: interval filter loaded 2 of 128 partitions; Out[7]: 82. In [8]: mt.filter_rows((mt.locus.contig == '16') & (mt.locus.position > 10_000_000) & (mt.locus.position < 12_000_000)).count_rows(); Hail: INFO: interval filter loaded 5 of 128 partitions; Out[8]: 384. In [9]: mt.filter_rows(mt.locus == hl.parse_locus('1:3761547')).count(); Hail: INFO: interval filter loaded 1 of 128 partitions; Out[9]: (1, 284). In [10]: mt.filter_rows(hl.parse_locus_interval('16:20000000-30000000').contains(mt.locus)).count(); Hail: INFO: interval filter loaded 1 of 128 partitions; Out[10]: (35, 284); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5979
https://github.com/hail-is/hail/pull/5980:5,Testability,log,log,5,Adds log option to `plot.histogram2d`; Also adds `joint_plot` to plot init,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5980
https://github.com/hail-is/hail/pull/5983:29,Testability,test,test,29,create batch tables twice to test idempotency,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5983
https://github.com/hail-is/hail/pull/5984:59,Testability,test,tests,59,"for pushing region stuff into JVM emit. I wrote some basic tests, mostly copied from C++, to ensure that the regions were being returned to the pool as expected.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5984
https://github.com/hail-is/hail/pull/5986:95,Performance,optimiz,optimize,95,"This node encapsulates a pattern we use in several places, and lets us generate IR that we can optimize more effectively.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5986
https://github.com/hail-is/hail/pull/5990:4,Testability,TEST,TESTING,4,FOR TESTING ONLY,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5990
https://github.com/hail-is/hail/pull/5991:46,Usability,clear,clear,46,- renamed `get_record` to `get_records` to be clear that this returns all matching records; - Added `state` and `complete` to the Batch class in server. These are computed when the Batch is queried from the database.; - Batch `to_dict` returns `state` and `complete` in the result. It also has an option on whether to include `jobs` in the output.; - Batch `list_batches` in `server.py` no longer outputs the `jobs` per Batch; - Changed `ci2` to use the new Batch status with the state and complete included.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5991
https://github.com/hail-is/hail/issues/5993:186,Availability,error,error,186,"Example from Zulip:. ```; mt_one.count(); # (239279, 153); mt_two.count(); # (522049, 188); mt_merge = mt_one.union_cols(mt_two); mt_merge.count(); (242190, 341); ```. We should warn or error in this case.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5993
https://github.com/hail-is/hail/pull/5999:52,Testability,test,testing,52,This is a temporary workaround to enable easy local testing to continue,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5999
https://github.com/hail-is/hail/pull/6006:26,Testability,test,tests,26,Just cleaning up some old tests to use assertEvalsTo instead of manually constructing the functions.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6006
https://github.com/hail-is/hail/pull/6006:39,Testability,assert,assertEvalsTo,39,Just cleaning up some old tests to use assertEvalsTo instead of manually constructing the functions.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6006
https://github.com/hail-is/hail/pull/6007:106,Availability,failure,failure,106,"This PR attempts to make CI more useful as a developer feedback tool. Developers need to know the success/failure state of their PR ASAP. It is better to know that state for an old tip sha pair than to know nothing. It is better still to know that state for a more recent sha pair. It is best to know that state for the tip sha pair. We aim to test a PR's tip source sha (perhaps against an out of date target sha). ---. Our *target state* for remembering batches is:; - one is complete and the other is in-progress; the complete one is for an out-of-date tip sha pair, or ; - only one batch is in-progress or complete; it's for the tip sha pair. We forget an in-progress batch for a PR only if:; - a batch for a more recent sha pair is complete, or; - a more recent, but not tip, source sha build is also in progress. As is the case for our services, at any time we may not be in our target state. For CI, if we are not in our target state, `_refresh` and `_heal` are intended to move us towards the desired state. `_refresh` incorporates new GitHub information. `_heal` incorporates new batch state and perturbs batch as necessary. cc: services crew: @akotlar, @jigold . EDIT:. I've gone round and round with my thoughts on what the right idea here is. I would appreciate some discussion around what we should aim for.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6007
https://github.com/hail-is/hail/pull/6007:344,Testability,test,test,344,"This PR attempts to make CI more useful as a developer feedback tool. Developers need to know the success/failure state of their PR ASAP. It is better to know that state for an old tip sha pair than to know nothing. It is better still to know that state for a more recent sha pair. It is best to know that state for the tip sha pair. We aim to test a PR's tip source sha (perhaps against an out of date target sha). ---. Our *target state* for remembering batches is:; - one is complete and the other is in-progress; the complete one is for an out-of-date tip sha pair, or ; - only one batch is in-progress or complete; it's for the tip sha pair. We forget an in-progress batch for a PR only if:; - a batch for a more recent sha pair is complete, or; - a more recent, but not tip, source sha build is also in progress. As is the case for our services, at any time we may not be in our target state. For CI, if we are not in our target state, `_refresh` and `_heal` are intended to move us towards the desired state. `_refresh` incorporates new GitHub information. `_heal` incorporates new batch state and perturbs batch as necessary. cc: services crew: @akotlar, @jigold . EDIT:. I've gone round and round with my thoughts on what the right idea here is. I would appreciate some discussion around what we should aim for.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6007
https://github.com/hail-is/hail/pull/6007:55,Usability,feedback,feedback,55,"This PR attempts to make CI more useful as a developer feedback tool. Developers need to know the success/failure state of their PR ASAP. It is better to know that state for an old tip sha pair than to know nothing. It is better still to know that state for a more recent sha pair. It is best to know that state for the tip sha pair. We aim to test a PR's tip source sha (perhaps against an out of date target sha). ---. Our *target state* for remembering batches is:; - one is complete and the other is in-progress; the complete one is for an out-of-date tip sha pair, or ; - only one batch is in-progress or complete; it's for the tip sha pair. We forget an in-progress batch for a PR only if:; - a batch for a more recent sha pair is complete, or; - a more recent, but not tip, source sha build is also in progress. As is the case for our services, at any time we may not be in our target state. For CI, if we are not in our target state, `_refresh` and `_heal` are intended to move us towards the desired state. `_refresh` incorporates new GitHub information. `_heal` incorporates new batch state and perturbs batch as necessary. cc: services crew: @akotlar, @jigold . EDIT:. I've gone round and round with my thoughts on what the right idea here is. I would appreciate some discussion around what we should aim for.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6007
https://github.com/hail-is/hail/pull/6008:130,Availability,error,error,130,"Fixes this:. ```; WARNING	| 2019-04-30 23:31:34,751 	| server.py 	| handler:818 | callback for batch 33, job 993 failed due to an error, I will not retry. Error: Invalid URL 'ci2/batch_callback': No schema supplied. Perhaps you meant http://ci2/batch_callback?; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6008
https://github.com/hail-is/hail/pull/6008:155,Availability,Error,Error,155,"Fixes this:. ```; WARNING	| 2019-04-30 23:31:34,751 	| server.py 	| handler:818 | callback for batch 33, job 993 failed due to an error, I will not retry. Error: Invalid URL 'ci2/batch_callback': No schema supplied. Perhaps you meant http://ci2/batch_callback?; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6008
https://github.com/hail-is/hail/pull/6009:38,Integrability,depend,dependencies,38,It get lost in the shuffle. The other dependencies are now picked up in `hail/Dockerfile.hail-run` which has the dependencies to run Hail and is the base image for hail-base and the other apiserver images.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6009
https://github.com/hail-is/hail/pull/6009:113,Integrability,depend,dependencies,113,It get lost in the shuffle. The other dependencies are now picked up in `hail/Dockerfile.hail-run` which has the dependencies to run Hail and is the base image for hail-base and the other apiserver images.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6009
https://github.com/hail-is/hail/pull/6011:153,Availability,down,down,153,"I think tests of services are failing during deployment because wait-for Service (which probes /healthcheck) hits the old service, then the service goes down during (re)deployment. Here is an example test failure: first few tests pass then the rest fail due to connection timeout: https://ci2.hail.is/jobs/1413/log. This doesn't quite make sense, because batch and apiserver both have readiness checks, so the rollout should be have now downtime (although some of the tests could hit the old service which could fail if there were differences). I think this is a good chance but I'm not totally confident. Interested in your thoughts. Also, I can't seem the find the different between `wait deployment` and `rollout status`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6011
https://github.com/hail-is/hail/pull/6011:205,Availability,failure,failure,205,"I think tests of services are failing during deployment because wait-for Service (which probes /healthcheck) hits the old service, then the service goes down during (re)deployment. Here is an example test failure: first few tests pass then the rest fail due to connection timeout: https://ci2.hail.is/jobs/1413/log. This doesn't quite make sense, because batch and apiserver both have readiness checks, so the rollout should be have now downtime (although some of the tests could hit the old service which could fail if there were differences). I think this is a good chance but I'm not totally confident. Interested in your thoughts. Also, I can't seem the find the different between `wait deployment` and `rollout status`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6011
https://github.com/hail-is/hail/pull/6011:437,Availability,downtime,downtime,437,"I think tests of services are failing during deployment because wait-for Service (which probes /healthcheck) hits the old service, then the service goes down during (re)deployment. Here is an example test failure: first few tests pass then the rest fail due to connection timeout: https://ci2.hail.is/jobs/1413/log. This doesn't quite make sense, because batch and apiserver both have readiness checks, so the rollout should be have now downtime (although some of the tests could hit the old service which could fail if there were differences). I think this is a good chance but I'm not totally confident. Interested in your thoughts. Also, I can't seem the find the different between `wait deployment` and `rollout status`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6011
https://github.com/hail-is/hail/pull/6011:45,Deployability,deploy,deployment,45,"I think tests of services are failing during deployment because wait-for Service (which probes /healthcheck) hits the old service, then the service goes down during (re)deployment. Here is an example test failure: first few tests pass then the rest fail due to connection timeout: https://ci2.hail.is/jobs/1413/log. This doesn't quite make sense, because batch and apiserver both have readiness checks, so the rollout should be have now downtime (although some of the tests could hit the old service which could fail if there were differences). I think this is a good chance but I'm not totally confident. Interested in your thoughts. Also, I can't seem the find the different between `wait deployment` and `rollout status`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6011
https://github.com/hail-is/hail/pull/6011:169,Deployability,deploy,deployment,169,"I think tests of services are failing during deployment because wait-for Service (which probes /healthcheck) hits the old service, then the service goes down during (re)deployment. Here is an example test failure: first few tests pass then the rest fail due to connection timeout: https://ci2.hail.is/jobs/1413/log. This doesn't quite make sense, because batch and apiserver both have readiness checks, so the rollout should be have now downtime (although some of the tests could hit the old service which could fail if there were differences). I think this is a good chance but I'm not totally confident. Interested in your thoughts. Also, I can't seem the find the different between `wait deployment` and `rollout status`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6011
https://github.com/hail-is/hail/pull/6011:410,Deployability,rollout,rollout,410,"I think tests of services are failing during deployment because wait-for Service (which probes /healthcheck) hits the old service, then the service goes down during (re)deployment. Here is an example test failure: first few tests pass then the rest fail due to connection timeout: https://ci2.hail.is/jobs/1413/log. This doesn't quite make sense, because batch and apiserver both have readiness checks, so the rollout should be have now downtime (although some of the tests could hit the old service which could fail if there were differences). I think this is a good chance but I'm not totally confident. Interested in your thoughts. Also, I can't seem the find the different between `wait deployment` and `rollout status`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6011
https://github.com/hail-is/hail/pull/6011:691,Deployability,deploy,deployment,691,"I think tests of services are failing during deployment because wait-for Service (which probes /healthcheck) hits the old service, then the service goes down during (re)deployment. Here is an example test failure: first few tests pass then the rest fail due to connection timeout: https://ci2.hail.is/jobs/1413/log. This doesn't quite make sense, because batch and apiserver both have readiness checks, so the rollout should be have now downtime (although some of the tests could hit the old service which could fail if there were differences). I think this is a good chance but I'm not totally confident. Interested in your thoughts. Also, I can't seem the find the different between `wait deployment` and `rollout status`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6011
https://github.com/hail-is/hail/pull/6011:708,Deployability,rollout,rollout,708,"I think tests of services are failing during deployment because wait-for Service (which probes /healthcheck) hits the old service, then the service goes down during (re)deployment. Here is an example test failure: first few tests pass then the rest fail due to connection timeout: https://ci2.hail.is/jobs/1413/log. This doesn't quite make sense, because batch and apiserver both have readiness checks, so the rollout should be have now downtime (although some of the tests could hit the old service which could fail if there were differences). I think this is a good chance but I'm not totally confident. Interested in your thoughts. Also, I can't seem the find the different between `wait deployment` and `rollout status`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6011
https://github.com/hail-is/hail/pull/6011:272,Safety,timeout,timeout,272,"I think tests of services are failing during deployment because wait-for Service (which probes /healthcheck) hits the old service, then the service goes down during (re)deployment. Here is an example test failure: first few tests pass then the rest fail due to connection timeout: https://ci2.hail.is/jobs/1413/log. This doesn't quite make sense, because batch and apiserver both have readiness checks, so the rollout should be have now downtime (although some of the tests could hit the old service which could fail if there were differences). I think this is a good chance but I'm not totally confident. Interested in your thoughts. Also, I can't seem the find the different between `wait deployment` and `rollout status`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6011
https://github.com/hail-is/hail/pull/6011:8,Testability,test,tests,8,"I think tests of services are failing during deployment because wait-for Service (which probes /healthcheck) hits the old service, then the service goes down during (re)deployment. Here is an example test failure: first few tests pass then the rest fail due to connection timeout: https://ci2.hail.is/jobs/1413/log. This doesn't quite make sense, because batch and apiserver both have readiness checks, so the rollout should be have now downtime (although some of the tests could hit the old service which could fail if there were differences). I think this is a good chance but I'm not totally confident. Interested in your thoughts. Also, I can't seem the find the different between `wait deployment` and `rollout status`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6011
https://github.com/hail-is/hail/pull/6011:200,Testability,test,test,200,"I think tests of services are failing during deployment because wait-for Service (which probes /healthcheck) hits the old service, then the service goes down during (re)deployment. Here is an example test failure: first few tests pass then the rest fail due to connection timeout: https://ci2.hail.is/jobs/1413/log. This doesn't quite make sense, because batch and apiserver both have readiness checks, so the rollout should be have now downtime (although some of the tests could hit the old service which could fail if there were differences). I think this is a good chance but I'm not totally confident. Interested in your thoughts. Also, I can't seem the find the different between `wait deployment` and `rollout status`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6011
https://github.com/hail-is/hail/pull/6011:224,Testability,test,tests,224,"I think tests of services are failing during deployment because wait-for Service (which probes /healthcheck) hits the old service, then the service goes down during (re)deployment. Here is an example test failure: first few tests pass then the rest fail due to connection timeout: https://ci2.hail.is/jobs/1413/log. This doesn't quite make sense, because batch and apiserver both have readiness checks, so the rollout should be have now downtime (although some of the tests could hit the old service which could fail if there were differences). I think this is a good chance but I'm not totally confident. Interested in your thoughts. Also, I can't seem the find the different between `wait deployment` and `rollout status`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6011
https://github.com/hail-is/hail/pull/6011:311,Testability,log,log,311,"I think tests of services are failing during deployment because wait-for Service (which probes /healthcheck) hits the old service, then the service goes down during (re)deployment. Here is an example test failure: first few tests pass then the rest fail due to connection timeout: https://ci2.hail.is/jobs/1413/log. This doesn't quite make sense, because batch and apiserver both have readiness checks, so the rollout should be have now downtime (although some of the tests could hit the old service which could fail if there were differences). I think this is a good chance but I'm not totally confident. Interested in your thoughts. Also, I can't seem the find the different between `wait deployment` and `rollout status`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6011
https://github.com/hail-is/hail/pull/6011:468,Testability,test,tests,468,"I think tests of services are failing during deployment because wait-for Service (which probes /healthcheck) hits the old service, then the service goes down during (re)deployment. Here is an example test failure: first few tests pass then the rest fail due to connection timeout: https://ci2.hail.is/jobs/1413/log. This doesn't quite make sense, because batch and apiserver both have readiness checks, so the rollout should be have now downtime (although some of the tests could hit the old service which could fail if there were differences). I think this is a good chance but I'm not totally confident. Interested in your thoughts. Also, I can't seem the find the different between `wait deployment` and `rollout status`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6011
https://github.com/hail-is/hail/issues/6012:275,Availability,error,error,275,"Hi folks,. In evaluating Hail to see whether it fits my use case (a variant frequency database) I ran into an issue with importing VCF files from GIAB. It turns out that these use type `String` for the `PS` `##FORMAT` entry. Subsequently, Hail fails to import these with the error:; ```; is.hail.utils.HailException: HG001.vcf.gz:column 492: invalid character 'P' in integer literal; ```; This is because of the default behaviour of `htsjdk` to ""repair"" these according to the VCF ""standard"". `htsjdk` exposes `codec.disableOnTheFlyModifications` to toggle this behaviour which can be called from somewhere around https://github.com/hail-is/hail/blob/master/hail/src/main/scala/is/hail/io/vcf/LoadVCF.scala#L1143. Ideally I would like to expose this toggle also at the `import_vcf` method of Hail.; I'll create a PR to do so accordingly ASAP. Comments/questions?. Thanks!. Regards,. Mark",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6012
https://github.com/hail-is/hail/issues/6012:446,Availability,repair,repair,446,"Hi folks,. In evaluating Hail to see whether it fits my use case (a variant frequency database) I ran into an issue with importing VCF files from GIAB. It turns out that these use type `String` for the `PS` `##FORMAT` entry. Subsequently, Hail fails to import these with the error:; ```; is.hail.utils.HailException: HG001.vcf.gz:column 492: invalid character 'P' in integer literal; ```; This is because of the default behaviour of `htsjdk` to ""repair"" these according to the VCF ""standard"". `htsjdk` exposes `codec.disableOnTheFlyModifications` to toggle this behaviour which can be called from somewhere around https://github.com/hail-is/hail/blob/master/hail/src/main/scala/is/hail/io/vcf/LoadVCF.scala#L1143. Ideally I would like to expose this toggle also at the `import_vcf` method of Hail.; I'll create a PR to do so accordingly ASAP. Comments/questions?. Thanks!. Regards,. Mark",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6012
https://github.com/hail-is/hail/issues/6012:550,Deployability,toggle,toggle,550,"Hi folks,. In evaluating Hail to see whether it fits my use case (a variant frequency database) I ran into an issue with importing VCF files from GIAB. It turns out that these use type `String` for the `PS` `##FORMAT` entry. Subsequently, Hail fails to import these with the error:; ```; is.hail.utils.HailException: HG001.vcf.gz:column 492: invalid character 'P' in integer literal; ```; This is because of the default behaviour of `htsjdk` to ""repair"" these according to the VCF ""standard"". `htsjdk` exposes `codec.disableOnTheFlyModifications` to toggle this behaviour which can be called from somewhere around https://github.com/hail-is/hail/blob/master/hail/src/main/scala/is/hail/io/vcf/LoadVCF.scala#L1143. Ideally I would like to expose this toggle also at the `import_vcf` method of Hail.; I'll create a PR to do so accordingly ASAP. Comments/questions?. Thanks!. Regards,. Mark",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6012
https://github.com/hail-is/hail/issues/6012:750,Deployability,toggle,toggle,750,"Hi folks,. In evaluating Hail to see whether it fits my use case (a variant frequency database) I ran into an issue with importing VCF files from GIAB. It turns out that these use type `String` for the `PS` `##FORMAT` entry. Subsequently, Hail fails to import these with the error:; ```; is.hail.utils.HailException: HG001.vcf.gz:column 492: invalid character 'P' in integer literal; ```; This is because of the default behaviour of `htsjdk` to ""repair"" these according to the VCF ""standard"". `htsjdk` exposes `codec.disableOnTheFlyModifications` to toggle this behaviour which can be called from somewhere around https://github.com/hail-is/hail/blob/master/hail/src/main/scala/is/hail/io/vcf/LoadVCF.scala#L1143. Ideally I would like to expose this toggle also at the `import_vcf` method of Hail.; I'll create a PR to do so accordingly ASAP. Comments/questions?. Thanks!. Regards,. Mark",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6012
https://github.com/hail-is/hail/issues/6012:693,Performance,Load,LoadVCF,693,"Hi folks,. In evaluating Hail to see whether it fits my use case (a variant frequency database) I ran into an issue with importing VCF files from GIAB. It turns out that these use type `String` for the `PS` `##FORMAT` entry. Subsequently, Hail fails to import these with the error:; ```; is.hail.utils.HailException: HG001.vcf.gz:column 492: invalid character 'P' in integer literal; ```; This is because of the default behaviour of `htsjdk` to ""repair"" these according to the VCF ""standard"". `htsjdk` exposes `codec.disableOnTheFlyModifications` to toggle this behaviour which can be called from somewhere around https://github.com/hail-is/hail/blob/master/hail/src/main/scala/is/hail/io/vcf/LoadVCF.scala#L1143. Ideally I would like to expose this toggle also at the `import_vcf` method of Hail.; I'll create a PR to do so accordingly ASAP. Comments/questions?. Thanks!. Regards,. Mark",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6012
https://github.com/hail-is/hail/issues/6012:502,Security,expose,exposes,502,"Hi folks,. In evaluating Hail to see whether it fits my use case (a variant frequency database) I ran into an issue with importing VCF files from GIAB. It turns out that these use type `String` for the `PS` `##FORMAT` entry. Subsequently, Hail fails to import these with the error:; ```; is.hail.utils.HailException: HG001.vcf.gz:column 492: invalid character 'P' in integer literal; ```; This is because of the default behaviour of `htsjdk` to ""repair"" these according to the VCF ""standard"". `htsjdk` exposes `codec.disableOnTheFlyModifications` to toggle this behaviour which can be called from somewhere around https://github.com/hail-is/hail/blob/master/hail/src/main/scala/is/hail/io/vcf/LoadVCF.scala#L1143. Ideally I would like to expose this toggle also at the `import_vcf` method of Hail.; I'll create a PR to do so accordingly ASAP. Comments/questions?. Thanks!. Regards,. Mark",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6012
https://github.com/hail-is/hail/issues/6012:738,Security,expose,expose,738,"Hi folks,. In evaluating Hail to see whether it fits my use case (a variant frequency database) I ran into an issue with importing VCF files from GIAB. It turns out that these use type `String` for the `PS` `##FORMAT` entry. Subsequently, Hail fails to import these with the error:; ```; is.hail.utils.HailException: HG001.vcf.gz:column 492: invalid character 'P' in integer literal; ```; This is because of the default behaviour of `htsjdk` to ""repair"" these according to the VCF ""standard"". `htsjdk` exposes `codec.disableOnTheFlyModifications` to toggle this behaviour which can be called from somewhere around https://github.com/hail-is/hail/blob/master/hail/src/main/scala/is/hail/io/vcf/LoadVCF.scala#L1143. Ideally I would like to expose this toggle also at the `import_vcf` method of Hail.; I'll create a PR to do so accordingly ASAP. Comments/questions?. Thanks!. Regards,. Mark",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6012
https://github.com/hail-is/hail/pull/6017:54,Testability,test,tests,54,@cseed I tried following the structure from the batch tests. This PR should go in before #5962.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6017
https://github.com/hail-is/hail/pull/6018:355,Testability,assert,assert,355,"Because `BlockMatrix` is strictly 2-D but the IR backing it represents 0, 1 and 2-D arrays, we have to track whether 1-D vectors are really row or column vectors, and convert back and forth to their ""matrix shape"". This caused a mismatch of the 2-D vs 1-D type when transposing a row vector to a column vector and trying to do col vector + row vector. We assert that dimensions that aren't being broadcasted to a larger length retain their original length. This uncovered a related bug in the conversion between 1-D vector length and 2-D matrix dimensions.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6018
https://github.com/hail-is/hail/pull/6020:29,Integrability,wrap,wrapper,29,"Quality of life improvement, wrapper around saving an ndarray and loading through numpy. Most changes are in the tests which previously could only extract elements from ndarrays to assert the transformation worked.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6020
https://github.com/hail-is/hail/pull/6020:66,Performance,load,loading,66,"Quality of life improvement, wrapper around saving an ndarray and loading through numpy. Most changes are in the tests which previously could only extract elements from ndarrays to assert the transformation worked.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6020
https://github.com/hail-is/hail/pull/6020:113,Testability,test,tests,113,"Quality of life improvement, wrapper around saving an ndarray and loading through numpy. Most changes are in the tests which previously could only extract elements from ndarrays to assert the transformation worked.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6020
https://github.com/hail-is/hail/pull/6020:181,Testability,assert,assert,181,"Quality of life improvement, wrapper around saving an ndarray and loading through numpy. Most changes are in the tests which previously could only extract elements from ndarrays to assert the transformation worked.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6020
https://github.com/hail-is/hail/pull/6023:336,Performance,race condition,race conditions,336,It doesn't make sense to be able to delete or cancel an individual job since they must be part of a batch now. I also deleted `list_jobs` since a job must be a part of a batch. I left in `job.wait()` because I felt the tests in `test_dag` were important and shouldn't be deleted and the wait functionality is needed for there not to be race conditions.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6023
https://github.com/hail-is/hail/pull/6023:219,Testability,test,tests,219,It doesn't make sense to be able to delete or cancel an individual job since they must be part of a batch now. I also deleted `list_jobs` since a job must be a part of a batch. I left in `job.wait()` because I felt the tests in `test_dag` were important and shouldn't be deleted and the wait functionality is needed for there not to be race conditions.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6023
https://github.com/hail-is/hail/pull/6026:343,Modifiability,variab,variables,343,"- Most of the code is actually the same, but I was intentionally not deforesting until now to get some benchmarks.; - Basically all you need to do is compose the `outputElement`s of your children (the body of the loops) and compute what the ultimate bounds (shape) of the nested loops should be.; - For Reindex, we statically reorder the loop variables used to index into the NDArray instead of permuting the shape/strides at runtime. Not a huge performance improvement but in the broadcasting case (adding dimensions) it's at least fewer multiplications at runtime in the loop body to compute the index.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6026
https://github.com/hail-is/hail/pull/6026:446,Performance,perform,performance,446,"- Most of the code is actually the same, but I was intentionally not deforesting until now to get some benchmarks.; - Basically all you need to do is compose the `outputElement`s of your children (the body of the loops) and compute what the ultimate bounds (shape) of the nested loops should be.; - For Reindex, we statically reorder the loop variables used to index into the NDArray instead of permuting the shape/strides at runtime. Not a huge performance improvement but in the broadcasting case (adding dimensions) it's at least fewer multiplications at runtime in the loop body to compute the index.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6026
https://github.com/hail-is/hail/pull/6026:103,Testability,benchmark,benchmarks,103,"- Most of the code is actually the same, but I was intentionally not deforesting until now to get some benchmarks.; - Basically all you need to do is compose the `outputElement`s of your children (the body of the loops) and compute what the ultimate bounds (shape) of the nested loops should be.; - For Reindex, we statically reorder the loop variables used to index into the NDArray instead of permuting the shape/strides at runtime. Not a huge performance improvement but in the broadcasting case (adding dimensions) it's at least fewer multiplications at runtime in the loop body to compute the index.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6026
https://github.com/hail-is/hail/pull/6028:213,Deployability,update,update,213,"@cseed I have a question about batch state when n_jobs == 0. Should this be `ready`? Should `complete` be True or False? I think this will be obsolete once all jobs are submitted with a batch. - Added triggers to update the Batch state on Job insert, state update; - Got rid of delete and cancel on Job and list_jobs; - Added foreign keys to database with delete cascade; - Misc naming changes for clarity: get_record -> get_records; - Changed list_batches to not include individual jobs but batches/N still returns the jobs",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6028
https://github.com/hail-is/hail/pull/6028:257,Deployability,update,update,257,"@cseed I have a question about batch state when n_jobs == 0. Should this be `ready`? Should `complete` be True or False? I think this will be obsolete once all jobs are submitted with a batch. - Added triggers to update the Batch state on Job insert, state update; - Got rid of delete and cancel on Job and list_jobs; - Added foreign keys to database with delete cascade; - Misc naming changes for clarity: get_record -> get_records; - Changed list_batches to not include individual jobs but batches/N still returns the jobs",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6028
https://github.com/hail-is/hail/pull/6029:131,Availability,down,down,131,- I don't know why I was recomputing the whole output index instead of just moving forward one. The loops will always just iterate down the new data buffer. Also means I can use the normal array builder abstraction instead.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6029
https://github.com/hail-is/hail/pull/6039:18,Availability,error,error,18,Add an a postiori error estimate for the approximate cdf aggregator. Use in new pdf plotting method than makes no assumption about the smoothness of the true distribution.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6039
https://github.com/hail-is/hail/pull/6040:6,Performance,concurren,concurrent,6,boost concurrent builds to 4,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6040
https://github.com/hail-is/hail/pull/6043:150,Safety,unsafe,unsafe,150,"This overrides each vcf's header with a user provided one, and; overrides the vcfs' samples' names with a user provided list of lists. This is wildly unsafe. I don't like this code. However it seems that it; is the best way to proceed with data production for gnomAD v3. cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6043
https://github.com/hail-is/hail/pull/6049:105,Modifiability,extend,extending,105,"- Aggregate along a set of dimensions of an NDArray; - As a first pass I made it only sum, but intend on extending the body to generic hail aggregators",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6049
https://github.com/hail-is/hail/pull/6050:385,Deployability,install,install,385,"@jigold, I made the following modifications to your changes:; - added a jars target to the hail/ Makefile, which does the C build and then runs maven, and removed the corresponding section of the pom.xml,; - run the Java tests in its own build step invoking java directory, so I removed the test step from pom.xml; - modified hail_build_image (the image used to run the Hail build) to install maven and the Hail dependencies (jars), no longer using gradle. The testng tests probably need better logging which I'm trying to figure out now.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6050
https://github.com/hail-is/hail/pull/6050:412,Integrability,depend,dependencies,412,"@jigold, I made the following modifications to your changes:; - added a jars target to the hail/ Makefile, which does the C build and then runs maven, and removed the corresponding section of the pom.xml,; - run the Java tests in its own build step invoking java directory, so I removed the test step from pom.xml; - modified hail_build_image (the image used to run the Hail build) to install maven and the Hail dependencies (jars), no longer using gradle. The testng tests probably need better logging which I'm trying to figure out now.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6050
https://github.com/hail-is/hail/pull/6050:221,Testability,test,tests,221,"@jigold, I made the following modifications to your changes:; - added a jars target to the hail/ Makefile, which does the C build and then runs maven, and removed the corresponding section of the pom.xml,; - run the Java tests in its own build step invoking java directory, so I removed the test step from pom.xml; - modified hail_build_image (the image used to run the Hail build) to install maven and the Hail dependencies (jars), no longer using gradle. The testng tests probably need better logging which I'm trying to figure out now.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6050
https://github.com/hail-is/hail/pull/6050:291,Testability,test,test,291,"@jigold, I made the following modifications to your changes:; - added a jars target to the hail/ Makefile, which does the C build and then runs maven, and removed the corresponding section of the pom.xml,; - run the Java tests in its own build step invoking java directory, so I removed the test step from pom.xml; - modified hail_build_image (the image used to run the Hail build) to install maven and the Hail dependencies (jars), no longer using gradle. The testng tests probably need better logging which I'm trying to figure out now.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6050
https://github.com/hail-is/hail/pull/6050:461,Testability,test,testng,461,"@jigold, I made the following modifications to your changes:; - added a jars target to the hail/ Makefile, which does the C build and then runs maven, and removed the corresponding section of the pom.xml,; - run the Java tests in its own build step invoking java directory, so I removed the test step from pom.xml; - modified hail_build_image (the image used to run the Hail build) to install maven and the Hail dependencies (jars), no longer using gradle. The testng tests probably need better logging which I'm trying to figure out now.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6050
https://github.com/hail-is/hail/pull/6050:468,Testability,test,tests,468,"@jigold, I made the following modifications to your changes:; - added a jars target to the hail/ Makefile, which does the C build and then runs maven, and removed the corresponding section of the pom.xml,; - run the Java tests in its own build step invoking java directory, so I removed the test step from pom.xml; - modified hail_build_image (the image used to run the Hail build) to install maven and the Hail dependencies (jars), no longer using gradle. The testng tests probably need better logging which I'm trying to figure out now.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6050
https://github.com/hail-is/hail/pull/6050:495,Testability,log,logging,495,"@jigold, I made the following modifications to your changes:; - added a jars target to the hail/ Makefile, which does the C build and then runs maven, and removed the corresponding section of the pom.xml,; - run the Java tests in its own build step invoking java directory, so I removed the test step from pom.xml; - modified hail_build_image (the image used to run the Hail build) to install maven and the Hail dependencies (jars), no longer using gradle. The testng tests probably need better logging which I'm trying to figure out now.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6050
https://github.com/hail-is/hail/pull/6053:133,Performance,optimiz,optimization,133,since the lowering is not actually a C++ step. (also pulled out the table lowering step into an explicit step so that I could add an optimization pass afterwards),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6053
https://github.com/hail-is/hail/pull/6054:56,Availability,failure,failure,56,"cc: @tpoterba @konradjk . Include the words success and failure and some color, but preserve the exit code. Also, banish tabs.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6054
https://github.com/hail-is/hail/pull/6058:83,Integrability,rout,route,83,"Also removed unused domain names (test, dev1) and added internal for future use to route to internal namespaces.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6058
https://github.com/hail-is/hail/pull/6058:34,Testability,test,test,34,"Also removed unused domain names (test, dev1) and added internal for future use to route to internal namespaces.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6058
https://github.com/hail-is/hail/pull/6059:2,Security,Expose,Expose,2,- Expose the ability to permute dimensions on NDArrays in Python,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6059
https://github.com/hail-is/hail/pull/6061:207,Energy Efficiency,schedul,schedule,207,"This sets up auto-scaling for site. Request 100m, min 2, max 10 for now (unlikely we'll exceed that any time soon), anti-affinity to get instances on different nodes. With the replication, we could probably schedule them on preemptibles.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6061
https://github.com/hail-is/hail/issues/6066:286,Performance,optimiz,optimize,286,"Large trees of union_cols have to concatenate entries arrays pairwise, creating a lot of junk in memory. A `MatrixMultiWayUnionCols` IR node should be straightforward to lower to `TableMultiWayZipJoin`, such that concatenating the entries arrays is completely deforested. We could then optimize nested `MatrixUnionCols` to a single `MatrixMultiWayUnionCols`, and/or expose a `multi_way_union_cols` method in python. https://hail.zulipchat.com/#narrow/stream/123010-Hail-0.2E2.20support/topic/how.20does.20union_cols.20work/near/165089884",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6066
https://github.com/hail-is/hail/issues/6066:366,Security,expose,expose,366,"Large trees of union_cols have to concatenate entries arrays pairwise, creating a lot of junk in memory. A `MatrixMultiWayUnionCols` IR node should be straightforward to lower to `TableMultiWayZipJoin`, such that concatenating the entries arrays is completely deforested. We could then optimize nested `MatrixUnionCols` to a single `MatrixMultiWayUnionCols`, and/or expose a `multi_way_union_cols` method in python. https://hail.zulipchat.com/#narrow/stream/123010-Hail-0.2E2.20support/topic/how.20does.20union_cols.20work/near/165089884",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6066
https://github.com/hail-is/hail/pull/6069:140,Availability,error,error,140,github status should be pending if success but build out of date; protect entire build/deploy with try/catch; build_state: merge_failure => error; report build error in pr page; don't merge/deploy if target sha is None. Not showing deploy error yet.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6069
https://github.com/hail-is/hail/pull/6069:160,Availability,error,error,160,github status should be pending if success but build out of date; protect entire build/deploy with try/catch; build_state: merge_failure => error; report build error in pr page; don't merge/deploy if target sha is None. Not showing deploy error yet.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6069
https://github.com/hail-is/hail/pull/6069:239,Availability,error,error,239,github status should be pending if success but build out of date; protect entire build/deploy with try/catch; build_state: merge_failure => error; report build error in pr page; don't merge/deploy if target sha is None. Not showing deploy error yet.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6069
https://github.com/hail-is/hail/pull/6069:87,Deployability,deploy,deploy,87,github status should be pending if success but build out of date; protect entire build/deploy with try/catch; build_state: merge_failure => error; report build error in pr page; don't merge/deploy if target sha is None. Not showing deploy error yet.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6069
https://github.com/hail-is/hail/pull/6069:190,Deployability,deploy,deploy,190,github status should be pending if success but build out of date; protect entire build/deploy with try/catch; build_state: merge_failure => error; report build error in pr page; don't merge/deploy if target sha is None. Not showing deploy error yet.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6069
https://github.com/hail-is/hail/pull/6069:232,Deployability,deploy,deploy,232,github status should be pending if success but build out of date; protect entire build/deploy with try/catch; build_state: merge_failure => error; report build error in pr page; don't merge/deploy if target sha is None. Not showing deploy error yet.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6069
https://github.com/hail-is/hail/issues/6074:946,Availability,error,error,946,"So this works (`cuts` is an array of 50 entries, so this is 2500 `counters`):; ```; joint_sfs = ht.aggregate(hl.struct(; joint_freq_bin_counters=[[hl.agg.counter((ht.freq_bins[i], ht.freq_bins[j], ht.consequence)); for i, _ in enumerate(cuts)] for j, _ in enumerate(cuts)])); ```; but this:; ```; counters = ht.aggregate(hl.struct(; enrichment_counters=[hl.agg.array_agg(lambda x: hl.agg.counter(x), ht.enrichments[i]); for i, _ in enumerate(cuts)],; enrichment_pseudo_counters=[hl.agg.array_agg(lambda x: hl.agg.counter(x), ht.enrichments_pseudo[i]); for i, _ in enumerate(cuts)])); ```; immediately results in OOMs. Each of `enrichments[i]` is also 50 elements, so this should be the same amount of work (well double since I have 2). But a few tasks finish but they generally struggle and eventually die with:; ```; [Stage 3:> (4 + 13) / 9997]OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00007f94d8700000, 5428477952, 0) failed; error='Cannot allocate memory' (errno=12); #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 5428477952 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /tmp/04eb6abfd9594f99ad2fac1a8e4cd0d1/hs_err_pid25110.log; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6074
https://github.com/hail-is/hail/issues/6074:1173,Availability,error,error,1173,"So this works (`cuts` is an array of 50 entries, so this is 2500 `counters`):; ```; joint_sfs = ht.aggregate(hl.struct(; joint_freq_bin_counters=[[hl.agg.counter((ht.freq_bins[i], ht.freq_bins[j], ht.consequence)); for i, _ in enumerate(cuts)] for j, _ in enumerate(cuts)])); ```; but this:; ```; counters = ht.aggregate(hl.struct(; enrichment_counters=[hl.agg.array_agg(lambda x: hl.agg.counter(x), ht.enrichments[i]); for i, _ in enumerate(cuts)],; enrichment_pseudo_counters=[hl.agg.array_agg(lambda x: hl.agg.counter(x), ht.enrichments_pseudo[i]); for i, _ in enumerate(cuts)])); ```; immediately results in OOMs. Each of `enrichments[i]` is also 50 elements, so this should be the same amount of work (well double since I have 2). But a few tasks finish but they generally struggle and eventually die with:; ```; [Stage 3:> (4 + 13) / 9997]OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00007f94d8700000, 5428477952, 0) failed; error='Cannot allocate memory' (errno=12); #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 5428477952 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /tmp/04eb6abfd9594f99ad2fac1a8e4cd0d1/hs_err_pid25110.log; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6074
https://github.com/hail-is/hail/issues/6074:960,Energy Efficiency,allocate,allocate,960,"So this works (`cuts` is an array of 50 entries, so this is 2500 `counters`):; ```; joint_sfs = ht.aggregate(hl.struct(; joint_freq_bin_counters=[[hl.agg.counter((ht.freq_bins[i], ht.freq_bins[j], ht.consequence)); for i, _ in enumerate(cuts)] for j, _ in enumerate(cuts)])); ```; but this:; ```; counters = ht.aggregate(hl.struct(; enrichment_counters=[hl.agg.array_agg(lambda x: hl.agg.counter(x), ht.enrichments[i]); for i, _ in enumerate(cuts)],; enrichment_pseudo_counters=[hl.agg.array_agg(lambda x: hl.agg.counter(x), ht.enrichments_pseudo[i]); for i, _ in enumerate(cuts)])); ```; immediately results in OOMs. Each of `enrichments[i]` is also 50 elements, so this should be the same amount of work (well double since I have 2). But a few tasks finish but they generally struggle and eventually die with:; ```; [Stage 3:> (4 + 13) / 9997]OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00007f94d8700000, 5428477952, 0) failed; error='Cannot allocate memory' (errno=12); #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 5428477952 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /tmp/04eb6abfd9594f99ad2fac1a8e4cd0d1/hs_err_pid25110.log; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6074
https://github.com/hail-is/hail/issues/6074:1283,Testability,log,log,1283,"So this works (`cuts` is an array of 50 entries, so this is 2500 `counters`):; ```; joint_sfs = ht.aggregate(hl.struct(; joint_freq_bin_counters=[[hl.agg.counter((ht.freq_bins[i], ht.freq_bins[j], ht.consequence)); for i, _ in enumerate(cuts)] for j, _ in enumerate(cuts)])); ```; but this:; ```; counters = ht.aggregate(hl.struct(; enrichment_counters=[hl.agg.array_agg(lambda x: hl.agg.counter(x), ht.enrichments[i]); for i, _ in enumerate(cuts)],; enrichment_pseudo_counters=[hl.agg.array_agg(lambda x: hl.agg.counter(x), ht.enrichments_pseudo[i]); for i, _ in enumerate(cuts)])); ```; immediately results in OOMs. Each of `enrichments[i]` is also 50 elements, so this should be the same amount of work (well double since I have 2). But a few tasks finish but they generally struggle and eventually die with:; ```; [Stage 3:> (4 + 13) / 9997]OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00007f94d8700000, 5428477952, 0) failed; error='Cannot allocate memory' (errno=12); #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 5428477952 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /tmp/04eb6abfd9594f99ad2fac1a8e4cd0d1/hs_err_pid25110.log; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6074
https://github.com/hail-is/hail/pull/6075:275,Availability,error,error,275,@cseed I think this is a better organization. The pod specs in the database are static and can be inserted into the database upon job creation. So we now assert the job tasks are never null in the database. This doesn't change the problem of how to handle a pvc/pod creation error in the database. Should we delete the record upon failure? Poll and wait for creation to succeed up to N times?,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6075
https://github.com/hail-is/hail/pull/6075:331,Availability,failure,failure,331,@cseed I think this is a better organization. The pod specs in the database are static and can be inserted into the database upon job creation. So we now assert the job tasks are never null in the database. This doesn't change the problem of how to handle a pvc/pod creation error in the database. Should we delete the record upon failure? Poll and wait for creation to succeed up to N times?,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6075
https://github.com/hail-is/hail/pull/6075:154,Testability,assert,assert,154,@cseed I think this is a better organization. The pod specs in the database are static and can be inserted into the database upon job creation. So we now assert the job tasks are never null in the database. This doesn't change the problem of how to handle a pvc/pod creation error in the database. Should we delete the record upon failure? Poll and wait for creation to succeed up to N times?,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6075
https://github.com/hail-is/hail/pull/6076:239,Availability,down,downstream,239,"Add a `keepRatio` parameter to the ApproxCDF aggregator. When compacting any level, always keep a fixed fraction of the smallest and largest values at that level. Also keep counts of the number of times each level is compacted, for use in downstream error estimates.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6076
https://github.com/hail-is/hail/pull/6076:250,Availability,error,error,250,"Add a `keepRatio` parameter to the ApproxCDF aggregator. When compacting any level, always keep a fixed fraction of the smallest and largest values at that level. Also keep counts of the number of times each level is compacted, for use in downstream error estimates.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6076
https://github.com/hail-is/hail/pull/6077:100,Performance,perform,perform,100,"- Get shape for an `NDArrayExpression`. While this should (and in some cases does) **not** actually perform computations like element wise maps or matmul's, it doesn't in all cases because not everything is worked into being deforested. I'm going to take a closer look at deforesting/shape calculation and tie up loose ends, but there are some subtleties and I think this is a good chunk.; - Changed IR nodes that are concerned with shape (`MakeNDArray`, `NDArrayReshape`) take shapes as hail tuples (instead of Scala sequences of IR, or IR with another parameter for length). The only thing Scala sequences were really useful for were statically knowing the number of dimensions which we can get with tuples.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6077
https://github.com/hail-is/hail/pull/6078:583,Availability,error,error,583,"Assigning to Daniel 2 because the scorecard beacon is tired. This removes the workshop login option (previously agreed upon with Cotton), which makes the login.html page totally useless; so I've converted the login link to hit the old /login POST endpoint, and converted the POST to a GET. I think this is semantically fine, because no credentials (or other data) is actually sent to that endpoint (as workshop password is kaput), making that endpoint solely issue a redirect. Since login.html is gone, I also no longer redirect to it. Instead, unauthorized users are redirected to /error, and I refactored this redirect into a function since it's now used identically in 2 places. I've also imported the jwt library, so that jwt.exceptions.InvalidTokenError is in scope, and made some minor cleanup. cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6078
https://github.com/hail-is/hail/pull/6078:596,Modifiability,refactor,refactored,596,"Assigning to Daniel 2 because the scorecard beacon is tired. This removes the workshop login option (previously agreed upon with Cotton), which makes the login.html page totally useless; so I've converted the login link to hit the old /login POST endpoint, and converted the POST to a GET. I think this is semantically fine, because no credentials (or other data) is actually sent to that endpoint (as workshop password is kaput), making that endpoint solely issue a redirect. Since login.html is gone, I also no longer redirect to it. Instead, unauthorized users are redirected to /error, and I refactored this redirect into a function since it's now used identically in 2 places. I've also imported the jwt library, so that jwt.exceptions.InvalidTokenError is in scope, and made some minor cleanup. cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6078
https://github.com/hail-is/hail/pull/6078:411,Security,password,password,411,"Assigning to Daniel 2 because the scorecard beacon is tired. This removes the workshop login option (previously agreed upon with Cotton), which makes the login.html page totally useless; so I've converted the login link to hit the old /login POST endpoint, and converted the POST to a GET. I think this is semantically fine, because no credentials (or other data) is actually sent to that endpoint (as workshop password is kaput), making that endpoint solely issue a redirect. Since login.html is gone, I also no longer redirect to it. Instead, unauthorized users are redirected to /error, and I refactored this redirect into a function since it's now used identically in 2 places. I've also imported the jwt library, so that jwt.exceptions.InvalidTokenError is in scope, and made some minor cleanup. cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6078
https://github.com/hail-is/hail/pull/6078:87,Testability,log,login,87,"Assigning to Daniel 2 because the scorecard beacon is tired. This removes the workshop login option (previously agreed upon with Cotton), which makes the login.html page totally useless; so I've converted the login link to hit the old /login POST endpoint, and converted the POST to a GET. I think this is semantically fine, because no credentials (or other data) is actually sent to that endpoint (as workshop password is kaput), making that endpoint solely issue a redirect. Since login.html is gone, I also no longer redirect to it. Instead, unauthorized users are redirected to /error, and I refactored this redirect into a function since it's now used identically in 2 places. I've also imported the jwt library, so that jwt.exceptions.InvalidTokenError is in scope, and made some minor cleanup. cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6078
https://github.com/hail-is/hail/pull/6078:154,Testability,log,login,154,"Assigning to Daniel 2 because the scorecard beacon is tired. This removes the workshop login option (previously agreed upon with Cotton), which makes the login.html page totally useless; so I've converted the login link to hit the old /login POST endpoint, and converted the POST to a GET. I think this is semantically fine, because no credentials (or other data) is actually sent to that endpoint (as workshop password is kaput), making that endpoint solely issue a redirect. Since login.html is gone, I also no longer redirect to it. Instead, unauthorized users are redirected to /error, and I refactored this redirect into a function since it's now used identically in 2 places. I've also imported the jwt library, so that jwt.exceptions.InvalidTokenError is in scope, and made some minor cleanup. cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6078
https://github.com/hail-is/hail/pull/6078:209,Testability,log,login,209,"Assigning to Daniel 2 because the scorecard beacon is tired. This removes the workshop login option (previously agreed upon with Cotton), which makes the login.html page totally useless; so I've converted the login link to hit the old /login POST endpoint, and converted the POST to a GET. I think this is semantically fine, because no credentials (or other data) is actually sent to that endpoint (as workshop password is kaput), making that endpoint solely issue a redirect. Since login.html is gone, I also no longer redirect to it. Instead, unauthorized users are redirected to /error, and I refactored this redirect into a function since it's now used identically in 2 places. I've also imported the jwt library, so that jwt.exceptions.InvalidTokenError is in scope, and made some minor cleanup. cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6078
https://github.com/hail-is/hail/pull/6078:236,Testability,log,login,236,"Assigning to Daniel 2 because the scorecard beacon is tired. This removes the workshop login option (previously agreed upon with Cotton), which makes the login.html page totally useless; so I've converted the login link to hit the old /login POST endpoint, and converted the POST to a GET. I think this is semantically fine, because no credentials (or other data) is actually sent to that endpoint (as workshop password is kaput), making that endpoint solely issue a redirect. Since login.html is gone, I also no longer redirect to it. Instead, unauthorized users are redirected to /error, and I refactored this redirect into a function since it's now used identically in 2 places. I've also imported the jwt library, so that jwt.exceptions.InvalidTokenError is in scope, and made some minor cleanup. cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6078
https://github.com/hail-is/hail/pull/6078:483,Testability,log,login,483,"Assigning to Daniel 2 because the scorecard beacon is tired. This removes the workshop login option (previously agreed upon with Cotton), which makes the login.html page totally useless; so I've converted the login link to hit the old /login POST endpoint, and converted the POST to a GET. I think this is semantically fine, because no credentials (or other data) is actually sent to that endpoint (as workshop password is kaput), making that endpoint solely issue a redirect. Since login.html is gone, I also no longer redirect to it. Instead, unauthorized users are redirected to /error, and I refactored this redirect into a function since it's now used identically in 2 places. I've also imported the jwt library, so that jwt.exceptions.InvalidTokenError is in scope, and made some minor cleanup. cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6078
https://github.com/hail-is/hail/issues/6079:94,Testability,test,tests,94,"It got removed temporarily moving to the new CI because the existing code couldn't handle the tests being in a jar. However, if the data providers do fail, the test will be skipped, which actually translated to a non-zero (2) exit code from testng. So in fact this case will fail in the tests.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6079
https://github.com/hail-is/hail/issues/6079:160,Testability,test,test,160,"It got removed temporarily moving to the new CI because the existing code couldn't handle the tests being in a jar. However, if the data providers do fail, the test will be skipped, which actually translated to a non-zero (2) exit code from testng. So in fact this case will fail in the tests.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6079
https://github.com/hail-is/hail/issues/6079:241,Testability,test,testng,241,"It got removed temporarily moving to the new CI because the existing code couldn't handle the tests being in a jar. However, if the data providers do fail, the test will be skipped, which actually translated to a non-zero (2) exit code from testng. So in fact this case will fail in the tests.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6079
https://github.com/hail-is/hail/issues/6079:287,Testability,test,tests,287,"It got removed temporarily moving to the new CI because the existing code couldn't handle the tests being in a jar. However, if the data providers do fail, the test will be skipped, which actually translated to a non-zero (2) exit code from testng. So in fact this case will fail in the tests.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6079
https://github.com/hail-is/hail/issues/6080:60,Deployability,install,installing,60,Got removed moving to the new CI. Don't want to bother with installing R.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6080
https://github.com/hail-is/hail/pull/6082:32,Energy Efficiency,schedul,scheduled,32,wait for 1h for resources to be scheduled; add timeouts for pods and services; set timeout to 2x average time from a dozen recent successful runs of the step,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6082
https://github.com/hail-is/hail/pull/6082:47,Safety,timeout,timeouts,47,wait for 1h for resources to be scheduled; add timeouts for pods and services; set timeout to 2x average time from a dozen recent successful runs of the step,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6082
https://github.com/hail-is/hail/pull/6082:83,Safety,timeout,timeout,83,wait for 1h for resources to be scheduled; add timeouts for pods and services; set timeout to 2x average time from a dozen recent successful runs of the step,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6082
https://github.com/hail-is/hail/pull/6083:91,Deployability,Configurat,Configuration,91,"Ready to look at. . Abstracts file system functionality. We no longer pass around a Hadoop Configuration w/ implicit methods defined in RichHadoopConfiguration. Instead we define an abstract FS class (could be a trait as well) to serve as our file system interface, and provide one Hadoop implementation to maintain existing functionality. The PR has many lines, but should hopefully be relatively easy to follow; mostly involves renaming. . cc @cseed , thanks @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083
https://github.com/hail-is/hail/pull/6083:255,Integrability,interface,interface,255,"Ready to look at. . Abstracts file system functionality. We no longer pass around a Hadoop Configuration w/ implicit methods defined in RichHadoopConfiguration. Instead we define an abstract FS class (could be a trait as well) to serve as our file system interface, and provide one Hadoop implementation to maintain existing functionality. The PR has many lines, but should hopefully be relatively easy to follow; mostly involves renaming. . cc @cseed , thanks @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083
https://github.com/hail-is/hail/pull/6083:91,Modifiability,Config,Configuration,91,"Ready to look at. . Abstracts file system functionality. We no longer pass around a Hadoop Configuration w/ implicit methods defined in RichHadoopConfiguration. Instead we define an abstract FS class (could be a trait as well) to serve as our file system interface, and provide one Hadoop implementation to maintain existing functionality. The PR has many lines, but should hopefully be relatively easy to follow; mostly involves renaming. . cc @cseed , thanks @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083
https://github.com/hail-is/hail/issues/6086:33,Availability,failure,failures,33,This is the cause of master test failures. More detail to follow.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6086
https://github.com/hail-is/hail/issues/6086:28,Testability,test,test,28,This is the cause of master test failures. More detail to follow.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6086
https://github.com/hail-is/hail/pull/6087:31,Modifiability,refactor,refactoring,31,"there's also a small amount of refactoring that was necessary to send TableIRSuite.testRangeCount through the lowered, jvm-emitted path to test the CollectDistributedArray implementation. cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6087
https://github.com/hail-is/hail/pull/6087:83,Testability,test,testRangeCount,83,"there's also a small amount of refactoring that was necessary to send TableIRSuite.testRangeCount through the lowered, jvm-emitted path to test the CollectDistributedArray implementation. cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6087
https://github.com/hail-is/hail/pull/6087:139,Testability,test,test,139,"there's also a small amount of refactoring that was necessary to send TableIRSuite.testRangeCount through the lowered, jvm-emitted path to test the CollectDistributedArray implementation. cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6087
https://github.com/hail-is/hail/pull/6089:90,Integrability,interface,interface,90,"Summary of Changes; - Preserve MT structure for entry fields; - Harmonize MT and T `show` interface in a backwards compatible way; - Simplify `Expression.show` code slightly; - A comprehensive, colocated set of `show` tests",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6089
https://github.com/hail-is/hail/pull/6089:218,Testability,test,tests,218,"Summary of Changes; - Preserve MT structure for entry fields; - Harmonize MT and T `show` interface in a backwards compatible way; - Simplify `Expression.show` code slightly; - A comprehensive, colocated set of `show` tests",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6089
https://github.com/hail-is/hail/pull/6089:133,Usability,Simpl,Simplify,133,"Summary of Changes; - Preserve MT structure for entry fields; - Harmonize MT and T `show` interface in a backwards compatible way; - Simplify `Expression.show` code slightly; - A comprehensive, colocated set of `show` tests",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6089
https://github.com/hail-is/hail/pull/6091:13,Security,authoriz,authorized,13,"whitelist PR authorized, ignore the rest; require authorized users. move authenticated_users_only batch => hailjwt",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6091
https://github.com/hail-is/hail/pull/6091:50,Security,authoriz,authorized,50,"whitelist PR authorized, ignore the rest; require authorized users. move authenticated_users_only batch => hailjwt",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6091
https://github.com/hail-is/hail/pull/6093:18,Availability,failure,failures,18,"I'm seeing deploy failures where the tests start failing part way through because batch becomes unavailable, for example: https://ci2.hail.is/jobs/2886/log. However, this can't be the whole story, because batch has a readiness check and it isn't clear why it should go unavailable. Either way, this seems safer because it makes sure you pick up the intended version.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6093
https://github.com/hail-is/hail/pull/6093:11,Deployability,deploy,deploy,11,"I'm seeing deploy failures where the tests start failing part way through because batch becomes unavailable, for example: https://ci2.hail.is/jobs/2886/log. However, this can't be the whole story, because batch has a readiness check and it isn't clear why it should go unavailable. Either way, this seems safer because it makes sure you pick up the intended version.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6093
https://github.com/hail-is/hail/pull/6093:305,Safety,safe,safer,305,"I'm seeing deploy failures where the tests start failing part way through because batch becomes unavailable, for example: https://ci2.hail.is/jobs/2886/log. However, this can't be the whole story, because batch has a readiness check and it isn't clear why it should go unavailable. Either way, this seems safer because it makes sure you pick up the intended version.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6093
https://github.com/hail-is/hail/pull/6093:37,Testability,test,tests,37,"I'm seeing deploy failures where the tests start failing part way through because batch becomes unavailable, for example: https://ci2.hail.is/jobs/2886/log. However, this can't be the whole story, because batch has a readiness check and it isn't clear why it should go unavailable. Either way, this seems safer because it makes sure you pick up the intended version.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6093
https://github.com/hail-is/hail/pull/6093:152,Testability,log,log,152,"I'm seeing deploy failures where the tests start failing part way through because batch becomes unavailable, for example: https://ci2.hail.is/jobs/2886/log. However, this can't be the whole story, because batch has a readiness check and it isn't clear why it should go unavailable. Either way, this seems safer because it makes sure you pick up the intended version.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6093
https://github.com/hail-is/hail/pull/6093:246,Usability,clear,clear,246,"I'm seeing deploy failures where the tests start failing part way through because batch becomes unavailable, for example: https://ci2.hail.is/jobs/2886/log. However, this can't be the whole story, because batch has a readiness check and it isn't clear why it should go unavailable. Either way, this seems safer because it makes sure you pick up the intended version.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6093
https://github.com/hail-is/hail/pull/6095:41,Usability,feedback,feedback,41,"This is an initial revision intended for feedback, there is much left to do; ### TODOs; - [x] Implement Read from index, notably for two operations, filter and repartition.; - [ ] Write matrix table indices; - There needs to be some work done on figuring out how to index both the rows and entries, especially when reading the entries as a table. Indices support an arbitrary `Annotation` payload that could be used. One thought I had for handling this is changing Indices rather than requiring a `offset: Long` and an `Annotation` changing them so that they only take some `Annotation`. Then the `TableSpec` or `MatrixTableSpec` would have a list of indices that would describe what needs to be pulled out of the `Annotation` in order to get the key's offset. cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6095
https://github.com/hail-is/hail/pull/6098:190,Energy Efficiency,efficient,efficiently,190,"Stacked on #6075 . - Changed the job id to be a compound key (batch_id, job_id) where batch_id is unique while job_id is 1->N; - Added a batch builder to create the batch in the database as efficiently as possible in one transaction (or at least that was my intent); - Changed both client interfaces to have a `run` function rather than `close`. Sends one request once the batch has been fully created.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6098
https://github.com/hail-is/hail/pull/6098:289,Integrability,interface,interfaces,289,"Stacked on #6075 . - Changed the job id to be a compound key (batch_id, job_id) where batch_id is unique while job_id is 1->N; - Added a batch builder to create the batch in the database as efficiently as possible in one transaction (or at least that was my intent); - Changed both client interfaces to have a `run` function rather than `close`. Sends one request once the batch has been fully created.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6098
https://github.com/hail-is/hail/pull/6100:642,Modifiability,parameteriz,parameterized,642,"Set and Dict used an inconsistent definition in the JVM backend, and what's more, it is different from the Scala code. This fixes that, and in particular, it is technically a breaking change. There are two orderings on types, the default coming from <, <=, etc. and a total ordering coming from `compare`. The default can compare ""strangely"", e.g. for Doubles every comparison with nan returns false. This code changes Set and Dict to use the total ordering on types for comparison of elements and keys. The representation of Set and Dict in Java are now SortedSet and SortedMap, which are implemented as TreeSet and TreeMap, which is always parameterized to take the total ordering. Note, I left the tests disabled because there's another comparison bug related to intervals I'm sorting out with @patrick-schultz.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6100
https://github.com/hail-is/hail/pull/6100:701,Testability,test,tests,701,"Set and Dict used an inconsistent definition in the JVM backend, and what's more, it is different from the Scala code. This fixes that, and in particular, it is technically a breaking change. There are two orderings on types, the default coming from <, <=, etc. and a total ordering coming from `compare`. The default can compare ""strangely"", e.g. for Doubles every comparison with nan returns false. This code changes Set and Dict to use the total ordering on types for comparison of elements and keys. The representation of Set and Dict in Java are now SortedSet and SortedMap, which are implemented as TreeSet and TreeMap, which is always parameterized to take the total ordering. Note, I left the tests disabled because there's another comparison bug related to intervals I'm sorting out with @patrick-schultz.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6100
https://github.com/hail-is/hail/pull/6107:184,Deployability,deploy,deploy,184,"Batch had a bug that left a batch in a bad state. To unstick CI, we deleted the bad batch manually, but CI could not handle a 404'ing batch. We change CI's logic to now handle 404'ing deploy batches.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6107
https://github.com/hail-is/hail/pull/6107:156,Testability,log,logic,156,"Batch had a bug that left a batch in a bad state. To unstick CI, we deleted the bad batch manually, but CI could not handle a 404'ing batch. We change CI's logic to now handle 404'ing deploy batches.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6107
https://github.com/hail-is/hail/pull/6118:24,Deployability,integrat,integrated,24,These changes should be integrated into the next cloud tools package deployment. Not sure whether the changes should be here or the old cloud tools repo.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6118
https://github.com/hail-is/hail/pull/6118:69,Deployability,deploy,deployment,69,These changes should be integrated into the next cloud tools package deployment. Not sure whether the changes should be here or the old cloud tools repo.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6118
https://github.com/hail-is/hail/pull/6118:24,Integrability,integrat,integrated,24,These changes should be integrated into the next cloud tools package deployment. Not sure whether the changes should be here or the old cloud tools repo.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6118
https://github.com/hail-is/hail/issues/6120:70,Deployability,update,update,70,"```; INFO | 2019-05-16 14:12:05,217 | github.py | _update_batch:593 | update batch br-hail-is-hail-master; INFO | 2019-05-16 14:12:05,956 | github.py | merge:392 | merge hail-is/hail:master 6119 failed due to exception: 405, message='Method Not Allowed'; INFO | 2019-05-16 14:12:05,957 | github.py | _heal:602 | heal br-hail-is-hail-master; INFO | 2019-05-16 14:12:05,957 | github.py | _heal:619 | merge candidate 6114; INFO | 2019-05-16 14:12:05,963 | github.py | post_github_status:228 | pr-6119: notify github state: success; INFO | 2019-05-16 14:12:06,162 | github.py | _update:503 | update done br-hail-is-hail-master; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6120
https://github.com/hail-is/hail/issues/6120:588,Deployability,update,update,588,"```; INFO | 2019-05-16 14:12:05,217 | github.py | _update_batch:593 | update batch br-hail-is-hail-master; INFO | 2019-05-16 14:12:05,956 | github.py | merge:392 | merge hail-is/hail:master 6119 failed due to exception: 405, message='Method Not Allowed'; INFO | 2019-05-16 14:12:05,957 | github.py | _heal:602 | heal br-hail-is-hail-master; INFO | 2019-05-16 14:12:05,957 | github.py | _heal:619 | merge candidate 6114; INFO | 2019-05-16 14:12:05,963 | github.py | post_github_status:228 | pr-6119: notify github state: success; INFO | 2019-05-16 14:12:06,162 | github.py | _update:503 | update done br-hail-is-hail-master; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6120
https://github.com/hail-is/hail/issues/6120:225,Integrability,message,message,225,"```; INFO | 2019-05-16 14:12:05,217 | github.py | _update_batch:593 | update batch br-hail-is-hail-master; INFO | 2019-05-16 14:12:05,956 | github.py | merge:392 | merge hail-is/hail:master 6119 failed due to exception: 405, message='Method Not Allowed'; INFO | 2019-05-16 14:12:05,957 | github.py | _heal:602 | heal br-hail-is-hail-master; INFO | 2019-05-16 14:12:05,957 | github.py | _heal:619 | merge candidate 6114; INFO | 2019-05-16 14:12:05,963 | github.py | post_github_status:228 | pr-6119: notify github state: success; INFO | 2019-05-16 14:12:06,162 | github.py | _update:503 | update done br-hail-is-hail-master; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6120
https://github.com/hail-is/hail/pull/6125:74,Integrability,depend,dependent,74,"This is required to implement lowering for MatrixMapCols (coming after; a dependent PR goes in). We lower the per-column aggregations to an; `AggArrayPerElement`, but if there are no rows, the result would be; missing. This PR will allow us to feed in the known number of columns as the; base case.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6125
https://github.com/hail-is/hail/pull/6131:30,Deployability,Deploy,Deployment,30,Moving everything to hailctl. Deployment PR coming soon.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6131
https://github.com/hail-is/hail/pull/6134:70,Deployability,configurat,configuration,70,"Users who build from source are often confused by the native; library configuration. This adds a target that handles native; library configuration for the user. I also changed the docs to; encourage the use of this target. This means everyone building hail; from source will need to build the C libraries, but I think this; is for the best, since most people building from source need to; correctly handle native libraries anyway. Resolves https://github.com/hail-is/hail/issues/6132",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6134
https://github.com/hail-is/hail/pull/6134:133,Deployability,configurat,configuration,133,"Users who build from source are often confused by the native; library configuration. This adds a target that handles native; library configuration for the user. I also changed the docs to; encourage the use of this target. This means everyone building hail; from source will need to build the C libraries, but I think this; is for the best, since most people building from source need to; correctly handle native libraries anyway. Resolves https://github.com/hail-is/hail/issues/6132",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6134
https://github.com/hail-is/hail/pull/6134:70,Modifiability,config,configuration,70,"Users who build from source are often confused by the native; library configuration. This adds a target that handles native; library configuration for the user. I also changed the docs to; encourage the use of this target. This means everyone building hail; from source will need to build the C libraries, but I think this; is for the best, since most people building from source need to; correctly handle native libraries anyway. Resolves https://github.com/hail-is/hail/issues/6132",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6134
https://github.com/hail-is/hail/pull/6134:133,Modifiability,config,configuration,133,"Users who build from source are often confused by the native; library configuration. This adds a target that handles native; library configuration for the user. I also changed the docs to; encourage the use of this target. This means everyone building hail; from source will need to build the C libraries, but I think this; is for the best, since most people building from source need to; correctly handle native libraries anyway. Resolves https://github.com/hail-is/hail/issues/6132",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6134
https://github.com/hail-is/hail/pull/6136:192,Deployability,configurat,configuration,192,Other changes:. - stop supporting python 2; - remove support for 0.1; - formatting. Coming soon to a PR near you:. - use google client library instead of `gsutil cat` (huge speedup); - upload configuration in deploy; include paths in hailctl package,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6136
https://github.com/hail-is/hail/pull/6136:209,Deployability,deploy,deploy,209,Other changes:. - stop supporting python 2; - remove support for 0.1; - formatting. Coming soon to a PR near you:. - use google client library instead of `gsutil cat` (huge speedup); - upload configuration in deploy; include paths in hailctl package,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6136
https://github.com/hail-is/hail/pull/6136:192,Modifiability,config,configuration,192,Other changes:. - stop supporting python 2; - remove support for 0.1; - formatting. Coming soon to a PR near you:. - use google client library instead of `gsutil cat` (huge speedup); - upload configuration in deploy; include paths in hailctl package,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6136
https://github.com/hail-is/hail/pull/6140:62,Testability,test,test,62,stacked on: https://github.com/hail-is/hail/pull/6139. create test jwt-secret-key secret and corresponding Hail service accounts.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6140
https://github.com/hail-is/hail/pull/6152:21,Availability,error,error,21,"Fixes this assertion error in ci2 logs:; ```; ERROR | 2019-05-21 20:22:56,019 | ci.py | update_loop:239 | hail-is/hail:master update failed due to exception: Traceback (most recent call last):; File ""/ci/ci.py"", line 235, in update_loop; await wb.update(app); File ""/ci/github.py"", line 465, in update; await self._update(app); File ""/ci/github.py"", line 481, in _update; await self._update_github(gh); File ""/ci/github.py"", line 543, in _update_github; await pr._update_github_review_state(gh); File ""/ci/github.py"", line 261, in _update_github_review_state; assert state in ('DISMISSED', 'COMMENTED'), state; AssertionError: PENDING; PENDING; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6152
https://github.com/hail-is/hail/pull/6152:46,Availability,ERROR,ERROR,46,"Fixes this assertion error in ci2 logs:; ```; ERROR | 2019-05-21 20:22:56,019 | ci.py | update_loop:239 | hail-is/hail:master update failed due to exception: Traceback (most recent call last):; File ""/ci/ci.py"", line 235, in update_loop; await wb.update(app); File ""/ci/github.py"", line 465, in update; await self._update(app); File ""/ci/github.py"", line 481, in _update; await self._update_github(gh); File ""/ci/github.py"", line 543, in _update_github; await pr._update_github_review_state(gh); File ""/ci/github.py"", line 261, in _update_github_review_state; assert state in ('DISMISSED', 'COMMENTED'), state; AssertionError: PENDING; PENDING; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6152
https://github.com/hail-is/hail/pull/6152:126,Deployability,update,update,126,"Fixes this assertion error in ci2 logs:; ```; ERROR | 2019-05-21 20:22:56,019 | ci.py | update_loop:239 | hail-is/hail:master update failed due to exception: Traceback (most recent call last):; File ""/ci/ci.py"", line 235, in update_loop; await wb.update(app); File ""/ci/github.py"", line 465, in update; await self._update(app); File ""/ci/github.py"", line 481, in _update; await self._update_github(gh); File ""/ci/github.py"", line 543, in _update_github; await pr._update_github_review_state(gh); File ""/ci/github.py"", line 261, in _update_github_review_state; assert state in ('DISMISSED', 'COMMENTED'), state; AssertionError: PENDING; PENDING; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6152
https://github.com/hail-is/hail/pull/6152:247,Deployability,update,update,247,"Fixes this assertion error in ci2 logs:; ```; ERROR | 2019-05-21 20:22:56,019 | ci.py | update_loop:239 | hail-is/hail:master update failed due to exception: Traceback (most recent call last):; File ""/ci/ci.py"", line 235, in update_loop; await wb.update(app); File ""/ci/github.py"", line 465, in update; await self._update(app); File ""/ci/github.py"", line 481, in _update; await self._update_github(gh); File ""/ci/github.py"", line 543, in _update_github; await pr._update_github_review_state(gh); File ""/ci/github.py"", line 261, in _update_github_review_state; assert state in ('DISMISSED', 'COMMENTED'), state; AssertionError: PENDING; PENDING; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6152
https://github.com/hail-is/hail/pull/6152:295,Deployability,update,update,295,"Fixes this assertion error in ci2 logs:; ```; ERROR | 2019-05-21 20:22:56,019 | ci.py | update_loop:239 | hail-is/hail:master update failed due to exception: Traceback (most recent call last):; File ""/ci/ci.py"", line 235, in update_loop; await wb.update(app); File ""/ci/github.py"", line 465, in update; await self._update(app); File ""/ci/github.py"", line 481, in _update; await self._update_github(gh); File ""/ci/github.py"", line 543, in _update_github; await pr._update_github_review_state(gh); File ""/ci/github.py"", line 261, in _update_github_review_state; assert state in ('DISMISSED', 'COMMENTED'), state; AssertionError: PENDING; PENDING; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6152
https://github.com/hail-is/hail/pull/6152:11,Testability,assert,assertion,11,"Fixes this assertion error in ci2 logs:; ```; ERROR | 2019-05-21 20:22:56,019 | ci.py | update_loop:239 | hail-is/hail:master update failed due to exception: Traceback (most recent call last):; File ""/ci/ci.py"", line 235, in update_loop; await wb.update(app); File ""/ci/github.py"", line 465, in update; await self._update(app); File ""/ci/github.py"", line 481, in _update; await self._update_github(gh); File ""/ci/github.py"", line 543, in _update_github; await pr._update_github_review_state(gh); File ""/ci/github.py"", line 261, in _update_github_review_state; assert state in ('DISMISSED', 'COMMENTED'), state; AssertionError: PENDING; PENDING; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6152
https://github.com/hail-is/hail/pull/6152:34,Testability,log,logs,34,"Fixes this assertion error in ci2 logs:; ```; ERROR | 2019-05-21 20:22:56,019 | ci.py | update_loop:239 | hail-is/hail:master update failed due to exception: Traceback (most recent call last):; File ""/ci/ci.py"", line 235, in update_loop; await wb.update(app); File ""/ci/github.py"", line 465, in update; await self._update(app); File ""/ci/github.py"", line 481, in _update; await self._update_github(gh); File ""/ci/github.py"", line 543, in _update_github; await pr._update_github_review_state(gh); File ""/ci/github.py"", line 261, in _update_github_review_state; assert state in ('DISMISSED', 'COMMENTED'), state; AssertionError: PENDING; PENDING; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6152
https://github.com/hail-is/hail/pull/6152:560,Testability,assert,assert,560,"Fixes this assertion error in ci2 logs:; ```; ERROR | 2019-05-21 20:22:56,019 | ci.py | update_loop:239 | hail-is/hail:master update failed due to exception: Traceback (most recent call last):; File ""/ci/ci.py"", line 235, in update_loop; await wb.update(app); File ""/ci/github.py"", line 465, in update; await self._update(app); File ""/ci/github.py"", line 481, in _update; await self._update_github(gh); File ""/ci/github.py"", line 543, in _update_github; await pr._update_github_review_state(gh); File ""/ci/github.py"", line 261, in _update_github_review_state; assert state in ('DISMISSED', 'COMMENTED'), state; AssertionError: PENDING; PENDING; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6152
https://github.com/hail-is/hail/pull/6152:611,Testability,Assert,AssertionError,611,"Fixes this assertion error in ci2 logs:; ```; ERROR | 2019-05-21 20:22:56,019 | ci.py | update_loop:239 | hail-is/hail:master update failed due to exception: Traceback (most recent call last):; File ""/ci/ci.py"", line 235, in update_loop; await wb.update(app); File ""/ci/github.py"", line 465, in update; await self._update(app); File ""/ci/github.py"", line 481, in _update; await self._update_github(gh); File ""/ci/github.py"", line 543, in _update_github; await pr._update_github_review_state(gh); File ""/ci/github.py"", line 261, in _update_github_review_state; assert state in ('DISMISSED', 'COMMENTED'), state; AssertionError: PENDING; PENDING; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6152
https://github.com/hail-is/hail/pull/6154:859,Availability,robust,robust,859,"~~Stride for a dimension should be calculated based off the length * stride of the previous innermost dimension with length > 1. I was remembering to do that for the stride but just taking the length of the adjacent dimension.~~. ~~Length-1 dimensions have stride 0 so broadcasting happens implicitly without the need for checks while indexing into the ndarray.~~. Broadcasting was previously handled implicitly by having 0 stride for dimensions of length 1. This doesn't actually work in all scenarios and conflates the concept of a multidimensional index with the underlying representation. Since we already compute the shapes of all intermediate NDArrays before doing any iteration, we can identify broadcasting ""statically"". Instead, loop variables associated with a braodcast are replaced with 0 when indexing into the backing array. This method is more robust and I've already seen works with slicing, which will follow this PR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6154
https://github.com/hail-is/hail/pull/6154:743,Modifiability,variab,variables,743,"~~Stride for a dimension should be calculated based off the length * stride of the previous innermost dimension with length > 1. I was remembering to do that for the stride but just taking the length of the adjacent dimension.~~. ~~Length-1 dimensions have stride 0 so broadcasting happens implicitly without the need for checks while indexing into the ndarray.~~. Broadcasting was previously handled implicitly by having 0 stride for dimensions of length 1. This doesn't actually work in all scenarios and conflates the concept of a multidimensional index with the underlying representation. Since we already compute the shapes of all intermediate NDArrays before doing any iteration, we can identify broadcasting ""statically"". Instead, loop variables associated with a braodcast are replaced with 0 when indexing into the backing array. This method is more robust and I've already seen works with slicing, which will follow this PR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6154
https://github.com/hail-is/hail/pull/6156:15,Availability,mask,masking,15,"This catch was masking a `org.json4s.package$MappingException: Parsed JSON values do not match with class constructor` as a `NullPointerException` on line 148. Exceptions always include their cause in the stack trace, I see no compelling reason to maintain this try-catch.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6156
https://github.com/hail-is/hail/pull/6161:91,Modifiability,parameteriz,parameterizes,91,StagedExtractedAggregators had a lot of duplicated code from ExtractAggregators. This just parameterizes the internal functions so that we don't have two identical copies of the aggregator logic.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6161
https://github.com/hail-is/hail/pull/6161:189,Testability,log,logic,189,StagedExtractedAggregators had a lot of duplicated code from ExtractAggregators. This just parameterizes the internal functions so that we don't have two identical copies of the aggregator logic.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6161
https://github.com/hail-is/hail/pull/6166:65,Availability,failure,failure,65,@cseed @jigold I think this resolves our PVC issues modulo batch failure. We still need to add the batch refresh loop.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6166
https://github.com/hail-is/hail/pull/6170:0,Energy Efficiency,Reduce,Reduce,0,Reduce. Reuse. Refactor.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6170
https://github.com/hail-is/hail/pull/6170:15,Modifiability,Refactor,Refactor,15,Reduce. Reuse. Refactor.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6170
https://github.com/hail-is/hail/pull/6174:54,Deployability,update,update,54,"This copies all the artifacts into place, but doesn't update the latest hash. Once we verify the files look correct after one round of deploy, I'll comment in the code to update latest hash. This script is basically copied exactly from the old hail/hail-ci-deploy.sh.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6174
https://github.com/hail-is/hail/pull/6174:135,Deployability,deploy,deploy,135,"This copies all the artifacts into place, but doesn't update the latest hash. Once we verify the files look correct after one round of deploy, I'll comment in the code to update latest hash. This script is basically copied exactly from the old hail/hail-ci-deploy.sh.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6174
https://github.com/hail-is/hail/pull/6174:171,Deployability,update,update,171,"This copies all the artifacts into place, but doesn't update the latest hash. Once we verify the files look correct after one round of deploy, I'll comment in the code to update latest hash. This script is basically copied exactly from the old hail/hail-ci-deploy.sh.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6174
https://github.com/hail-is/hail/pull/6174:257,Deployability,deploy,deploy,257,"This copies all the artifacts into place, but doesn't update the latest hash. Once we verify the files look correct after one round of deploy, I'll comment in the code to update latest hash. This script is basically copied exactly from the old hail/hail-ci-deploy.sh.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6174
https://github.com/hail-is/hail/pull/6174:72,Security,hash,hash,72,"This copies all the artifacts into place, but doesn't update the latest hash. Once we verify the files look correct after one round of deploy, I'll comment in the code to update latest hash. This script is basically copied exactly from the old hail/hail-ci-deploy.sh.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6174
https://github.com/hail-is/hail/pull/6174:185,Security,hash,hash,185,"This copies all the artifacts into place, but doesn't update the latest hash. Once we verify the files look correct after one round of deploy, I'll comment in the code to update latest hash. This script is basically copied exactly from the old hail/hail-ci-deploy.sh.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6174
https://github.com/hail-is/hail/pull/6175:166,Integrability,wrap,wrapper,166,"cc: @cseed. I wanted to delete pods when they were done, but I felt we needed to be more; careful in general with how we handled k8s API calls. I introduce a little; wrapper to the k8s api that coerces exceptions into return values, unblocks; network calls, and sets some default kwargs. I also delete pods if we no longer need them. The only reason to keep a pod; around is if it is running or we haven't durably stored the logs. ## K8s class; 1. convert blocking calls to non-blocking ones; 2. set default kwargs; 3. convert ApiExceptions to return values. Instead of the following (which we forgot to make non-blocking):; ```python; try:; v1.delete_namespaced_persistent_volume_claim(; self._pvc_name,; HAIL_POD_NAMESPACE,; _request_timeout=KUBERNETES_TIMEOUT_IN_SECONDS); except kube.client.rest.ApiException as err:; if err.status == 404:; log.info(f'persistent volume claim {self._pvc_name} is already deleted'); return; raise; finally:; await db.jobs.update_record(self.id, pvc_name=None); self._pvc_name = None; ```; We do:; ```python; err = await app['k8s'].delete_pvc(self._pvc_name); if err; if err.status != 404:; raise ValueError('could not delete pvc {self._pvc_name}') from err; log.info(f'persistent volume claim {self._pvc_name} is already deleted'); await db.jobs.update_record(self.id, pvc_name=None); self._pvc_name = None; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6175
https://github.com/hail-is/hail/pull/6175:425,Testability,log,logs,425,"cc: @cseed. I wanted to delete pods when they were done, but I felt we needed to be more; careful in general with how we handled k8s API calls. I introduce a little; wrapper to the k8s api that coerces exceptions into return values, unblocks; network calls, and sets some default kwargs. I also delete pods if we no longer need them. The only reason to keep a pod; around is if it is running or we haven't durably stored the logs. ## K8s class; 1. convert blocking calls to non-blocking ones; 2. set default kwargs; 3. convert ApiExceptions to return values. Instead of the following (which we forgot to make non-blocking):; ```python; try:; v1.delete_namespaced_persistent_volume_claim(; self._pvc_name,; HAIL_POD_NAMESPACE,; _request_timeout=KUBERNETES_TIMEOUT_IN_SECONDS); except kube.client.rest.ApiException as err:; if err.status == 404:; log.info(f'persistent volume claim {self._pvc_name} is already deleted'); return; raise; finally:; await db.jobs.update_record(self.id, pvc_name=None); self._pvc_name = None; ```; We do:; ```python; err = await app['k8s'].delete_pvc(self._pvc_name); if err; if err.status != 404:; raise ValueError('could not delete pvc {self._pvc_name}') from err; log.info(f'persistent volume claim {self._pvc_name} is already deleted'); await db.jobs.update_record(self.id, pvc_name=None); self._pvc_name = None; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6175
https://github.com/hail-is/hail/pull/6175:845,Testability,log,log,845,"cc: @cseed. I wanted to delete pods when they were done, but I felt we needed to be more; careful in general with how we handled k8s API calls. I introduce a little; wrapper to the k8s api that coerces exceptions into return values, unblocks; network calls, and sets some default kwargs. I also delete pods if we no longer need them. The only reason to keep a pod; around is if it is running or we haven't durably stored the logs. ## K8s class; 1. convert blocking calls to non-blocking ones; 2. set default kwargs; 3. convert ApiExceptions to return values. Instead of the following (which we forgot to make non-blocking):; ```python; try:; v1.delete_namespaced_persistent_volume_claim(; self._pvc_name,; HAIL_POD_NAMESPACE,; _request_timeout=KUBERNETES_TIMEOUT_IN_SECONDS); except kube.client.rest.ApiException as err:; if err.status == 404:; log.info(f'persistent volume claim {self._pvc_name} is already deleted'); return; raise; finally:; await db.jobs.update_record(self.id, pvc_name=None); self._pvc_name = None; ```; We do:; ```python; err = await app['k8s'].delete_pvc(self._pvc_name); if err; if err.status != 404:; raise ValueError('could not delete pvc {self._pvc_name}') from err; log.info(f'persistent volume claim {self._pvc_name} is already deleted'); await db.jobs.update_record(self.id, pvc_name=None); self._pvc_name = None; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6175
https://github.com/hail-is/hail/pull/6175:1194,Testability,log,log,1194,"cc: @cseed. I wanted to delete pods when they were done, but I felt we needed to be more; careful in general with how we handled k8s API calls. I introduce a little; wrapper to the k8s api that coerces exceptions into return values, unblocks; network calls, and sets some default kwargs. I also delete pods if we no longer need them. The only reason to keep a pod; around is if it is running or we haven't durably stored the logs. ## K8s class; 1. convert blocking calls to non-blocking ones; 2. set default kwargs; 3. convert ApiExceptions to return values. Instead of the following (which we forgot to make non-blocking):; ```python; try:; v1.delete_namespaced_persistent_volume_claim(; self._pvc_name,; HAIL_POD_NAMESPACE,; _request_timeout=KUBERNETES_TIMEOUT_IN_SECONDS); except kube.client.rest.ApiException as err:; if err.status == 404:; log.info(f'persistent volume claim {self._pvc_name} is already deleted'); return; raise; finally:; await db.jobs.update_record(self.id, pvc_name=None); self._pvc_name = None; ```; We do:; ```python; err = await app['k8s'].delete_pvc(self._pvc_name); if err; if err.status != 404:; raise ValueError('could not delete pvc {self._pvc_name}') from err; log.info(f'persistent volume claim {self._pvc_name} is already deleted'); await db.jobs.update_record(self.id, pvc_name=None); self._pvc_name = None; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6175
https://github.com/hail-is/hail/pull/6182:0,Deployability,Deploy,Deploy,0,"Deploy failed, I was missing dependencies.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6182
https://github.com/hail-is/hail/pull/6182:29,Integrability,depend,dependencies,29,"Deploy failed, I was missing dependencies.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6182
https://github.com/hail-is/hail/pull/6183:463,Deployability,pipeline,pipeline,463,"This PR is needed for atomic batch creation where the job_id is 0 to n_jobs and is not unique across batches. - Job now has a compound key in the database (batch_id, job_id); - Job.id on the server side is `(batch_id, job_id)`; - Batch on the client side has been modified to not require the attributes since Job now needs a reference to the batch.; - Changed some routes to be '/batches/{batch_id}/jobs/{job_id}/...`; - Changed the appropriate places in ci2 and pipeline that use the job status interface; - Changed the create_job interface for parents to take the Job objects and not the id. I think this is clearer and will allow me to check the batch of the Job is the correct batch in the future.; - deleted creating batch from a file",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6183
https://github.com/hail-is/hail/pull/6183:365,Integrability,rout,routes,365,"This PR is needed for atomic batch creation where the job_id is 0 to n_jobs and is not unique across batches. - Job now has a compound key in the database (batch_id, job_id); - Job.id on the server side is `(batch_id, job_id)`; - Batch on the client side has been modified to not require the attributes since Job now needs a reference to the batch.; - Changed some routes to be '/batches/{batch_id}/jobs/{job_id}/...`; - Changed the appropriate places in ci2 and pipeline that use the job status interface; - Changed the create_job interface for parents to take the Job objects and not the id. I think this is clearer and will allow me to check the batch of the Job is the correct batch in the future.; - deleted creating batch from a file",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6183
https://github.com/hail-is/hail/pull/6183:496,Integrability,interface,interface,496,"This PR is needed for atomic batch creation where the job_id is 0 to n_jobs and is not unique across batches. - Job now has a compound key in the database (batch_id, job_id); - Job.id on the server side is `(batch_id, job_id)`; - Batch on the client side has been modified to not require the attributes since Job now needs a reference to the batch.; - Changed some routes to be '/batches/{batch_id}/jobs/{job_id}/...`; - Changed the appropriate places in ci2 and pipeline that use the job status interface; - Changed the create_job interface for parents to take the Job objects and not the id. I think this is clearer and will allow me to check the batch of the Job is the correct batch in the future.; - deleted creating batch from a file",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6183
https://github.com/hail-is/hail/pull/6183:532,Integrability,interface,interface,532,"This PR is needed for atomic batch creation where the job_id is 0 to n_jobs and is not unique across batches. - Job now has a compound key in the database (batch_id, job_id); - Job.id on the server side is `(batch_id, job_id)`; - Batch on the client side has been modified to not require the attributes since Job now needs a reference to the batch.; - Changed some routes to be '/batches/{batch_id}/jobs/{job_id}/...`; - Changed the appropriate places in ci2 and pipeline that use the job status interface; - Changed the create_job interface for parents to take the Job objects and not the id. I think this is clearer and will allow me to check the batch of the Job is the correct batch in the future.; - deleted creating batch from a file",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6183
https://github.com/hail-is/hail/pull/6183:610,Usability,clear,clearer,610,"This PR is needed for atomic batch creation where the job_id is 0 to n_jobs and is not unique across batches. - Job now has a compound key in the database (batch_id, job_id); - Job.id on the server side is `(batch_id, job_id)`; - Batch on the client side has been modified to not require the attributes since Job now needs a reference to the batch.; - Changed some routes to be '/batches/{batch_id}/jobs/{job_id}/...`; - Changed the appropriate places in ci2 and pipeline that use the job status interface; - Changed the create_job interface for parents to take the Job objects and not the id. I think this is clearer and will allow me to check the batch of the Job is the correct batch in the future.; - deleted creating batch from a file",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6183
https://github.com/hail-is/hail/pull/6184:44,Security,access,access,44,"Now that user emails are granted read/write access, lets expose the link. Also removes the errant span.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6184
https://github.com/hail-is/hail/pull/6184:57,Security,expose,expose,57,"Now that user emails are granted read/write access, lets expose the link. Also removes the errant span.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6184
https://github.com/hail-is/hail/pull/6189:17,Deployability,update,updated,17,(this is your PR updated against hailctl),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6189
https://github.com/hail-is/hail/pull/6192:108,Energy Efficiency,monitor,monitoring,108,"We're not moving forward with auth-gateway. Give the direction with the service, I think we should focus on monitoring there instead collecting metrics from external users running Spark. @tpoterba FYI, in case you want to give pushback on this.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6192
https://github.com/hail-is/hail/pull/6193:145,Deployability,deploy,deployment,145,"I grepped through and fixed all instances of 'ci2' and 'hail-ci' (the old service name). When this goes in, we'll have to had-delete the old ci2 deployment and service.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6193
https://github.com/hail-is/hail/pull/6195:18,Deployability,deploy,deploy,18,Disable apiserver deploy for the moment. Remove everything related to Spark. Saved existing deploy configuration in apiserver/build.yaml_disabled.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6195
https://github.com/hail-is/hail/pull/6195:92,Deployability,deploy,deploy,92,Disable apiserver deploy for the moment. Remove everything related to Spark. Saved existing deploy configuration in apiserver/build.yaml_disabled.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6195
https://github.com/hail-is/hail/pull/6195:99,Deployability,configurat,configuration,99,Disable apiserver deploy for the moment. Remove everything related to Spark. Saved existing deploy configuration in apiserver/build.yaml_disabled.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6195
https://github.com/hail-is/hail/pull/6195:99,Modifiability,config,configuration,99,Disable apiserver deploy for the moment. Remove everything related to Spark. Saved existing deploy configuration in apiserver/build.yaml_disabled.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6195
https://github.com/hail-is/hail/pull/6196:27,Deployability,deploy,deployment,27,Change Makefile to fold in deployment and build local installation,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6196
https://github.com/hail-is/hail/pull/6196:54,Deployability,install,installation,54,Change Makefile to fold in deployment and build local installation,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6196
https://github.com/hail-is/hail/pull/6200:115,Availability,down,down,115,The SNP manipulation functions are huge and get inlined a million; times in sample_qc. This will help keep IR size down.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6200
https://github.com/hail-is/hail/pull/6201:14,Availability,resilien,resiliency,14,"Make the same resiliency changes I made to site recent to the other stateless services. Schedule them on 3 nodes, tolerate pre-emptibles, and autoscale 3-10 replicas. Preemptibles might be too aggressive, we should watch uptime. We probably want at least once instance running on non-preemptibles. We can do that explicitly by duplicating the pod spec, but I don't see a way to do it with tolerations and/or (anti-)affinities. We can also do this with notebook2 since it is stateless, but I'll leave that for another PR. I changed the Makefile structure, basically, don't support local docker build anymore and always pull from the repo and use --cache-from. I will modify the rest of the projects analogously in a separate PR. Switch infrastructure modules (gateway, router-resolver) to new jinja2 templating, instead of old @foo@ sed-based templating.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6201
https://github.com/hail-is/hail/pull/6201:114,Availability,toler,tolerate,114,"Make the same resiliency changes I made to site recent to the other stateless services. Schedule them on 3 nodes, tolerate pre-emptibles, and autoscale 3-10 replicas. Preemptibles might be too aggressive, we should watch uptime. We probably want at least once instance running on non-preemptibles. We can do that explicitly by duplicating the pod spec, but I don't see a way to do it with tolerations and/or (anti-)affinities. We can also do this with notebook2 since it is stateless, but I'll leave that for another PR. I changed the Makefile structure, basically, don't support local docker build anymore and always pull from the repo and use --cache-from. I will modify the rest of the projects analogously in a separate PR. Switch infrastructure modules (gateway, router-resolver) to new jinja2 templating, instead of old @foo@ sed-based templating.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6201
https://github.com/hail-is/hail/pull/6201:389,Availability,toler,tolerations,389,"Make the same resiliency changes I made to site recent to the other stateless services. Schedule them on 3 nodes, tolerate pre-emptibles, and autoscale 3-10 replicas. Preemptibles might be too aggressive, we should watch uptime. We probably want at least once instance running on non-preemptibles. We can do that explicitly by duplicating the pod spec, but I don't see a way to do it with tolerations and/or (anti-)affinities. We can also do this with notebook2 since it is stateless, but I'll leave that for another PR. I changed the Makefile structure, basically, don't support local docker build anymore and always pull from the repo and use --cache-from. I will modify the rest of the projects analogously in a separate PR. Switch infrastructure modules (gateway, router-resolver) to new jinja2 templating, instead of old @foo@ sed-based templating.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6201
https://github.com/hail-is/hail/pull/6201:88,Energy Efficiency,Schedul,Schedule,88,"Make the same resiliency changes I made to site recent to the other stateless services. Schedule them on 3 nodes, tolerate pre-emptibles, and autoscale 3-10 replicas. Preemptibles might be too aggressive, we should watch uptime. We probably want at least once instance running on non-preemptibles. We can do that explicitly by duplicating the pod spec, but I don't see a way to do it with tolerations and/or (anti-)affinities. We can also do this with notebook2 since it is stateless, but I'll leave that for another PR. I changed the Makefile structure, basically, don't support local docker build anymore and always pull from the repo and use --cache-from. I will modify the rest of the projects analogously in a separate PR. Switch infrastructure modules (gateway, router-resolver) to new jinja2 templating, instead of old @foo@ sed-based templating.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6201
https://github.com/hail-is/hail/pull/6201:768,Integrability,rout,router-resolver,768,"Make the same resiliency changes I made to site recent to the other stateless services. Schedule them on 3 nodes, tolerate pre-emptibles, and autoscale 3-10 replicas. Preemptibles might be too aggressive, we should watch uptime. We probably want at least once instance running on non-preemptibles. We can do that explicitly by duplicating the pod spec, but I don't see a way to do it with tolerations and/or (anti-)affinities. We can also do this with notebook2 since it is stateless, but I'll leave that for another PR. I changed the Makefile structure, basically, don't support local docker build anymore and always pull from the repo and use --cache-from. I will modify the rest of the projects analogously in a separate PR. Switch infrastructure modules (gateway, router-resolver) to new jinja2 templating, instead of old @foo@ sed-based templating.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6201
https://github.com/hail-is/hail/pull/6201:647,Performance,cache,cache-from,647,"Make the same resiliency changes I made to site recent to the other stateless services. Schedule them on 3 nodes, tolerate pre-emptibles, and autoscale 3-10 replicas. Preemptibles might be too aggressive, we should watch uptime. We probably want at least once instance running on non-preemptibles. We can do that explicitly by duplicating the pod spec, but I don't see a way to do it with tolerations and/or (anti-)affinities. We can also do this with notebook2 since it is stateless, but I'll leave that for another PR. I changed the Makefile structure, basically, don't support local docker build anymore and always pull from the repo and use --cache-from. I will modify the rest of the projects analogously in a separate PR. Switch infrastructure modules (gateway, router-resolver) to new jinja2 templating, instead of old @foo@ sed-based templating.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6201
https://github.com/hail-is/hail/pull/6205:12,Testability,test,testing,12,This is for testing purposes only.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6205
https://github.com/hail-is/hail/pull/6209:107,Energy Efficiency,reduce,reduce,107,Another attempt at getting the batch ui up. Add ui tests (just verify successful status code) to hopefully reduce the iteration time.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6209
https://github.com/hail-is/hail/pull/6209:51,Testability,test,tests,51,Another attempt at getting the batch ui up. Add ui tests (just verify successful status code) to hopefully reduce the iteration time.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6209
https://github.com/hail-is/hail/pull/6210:205,Integrability,interface,interface,205,"I think this is a good change, but we should keep in eye out, it is going to increase preemptions and restarts, and will almost certainly expose issues. Poor man's chaos. I will add an option to the batch interface to disable this for longer-running jobs.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6210
https://github.com/hail-is/hail/pull/6210:138,Security,expose,expose,138,"I think this is a good change, but we should keep in eye out, it is going to increase preemptions and restarts, and will almost certainly expose issues. Poor man's chaos. I will add an option to the batch interface to disable this for longer-running jobs.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6210
https://github.com/hail-is/hail/pull/6211:24,Availability,resilien,resiliency,24,Some fixes to my recent resiliency changes. These weren't caught because gateway and router-resolver are part of infrastructure that isn't automated by ci yet. I needed to make these changes to deploy them by hand (which I did successfully).,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6211
https://github.com/hail-is/hail/pull/6211:194,Deployability,deploy,deploy,194,Some fixes to my recent resiliency changes. These weren't caught because gateway and router-resolver are part of infrastructure that isn't automated by ci yet. I needed to make these changes to deploy them by hand (which I did successfully).,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6211
https://github.com/hail-is/hail/pull/6211:85,Integrability,rout,router-resolver,85,Some fixes to my recent resiliency changes. These weren't caught because gateway and router-resolver are part of infrastructure that isn't automated by ci yet. I needed to make these changes to deploy them by hand (which I did successfully).,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6211
https://github.com/hail-is/hail/pull/6219:32,Deployability,pipeline,pipeline,32,"Use name attribute for label in pipeline, this matches the ci convention. Display the name attribute.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6219
https://github.com/hail-is/hail/issues/6223:69,Availability,error,error,69,bug report here: https://discuss.hail.is/t/ld-prune-starts-and-stops-error/. @jbloom22 any ideas? You've worked on this stuff a bit,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6223
https://github.com/hail-is/hail/pull/6224:224,Availability,down,down,224,"This is unfortunately about 2x slower -- partly due to the fact; that the column + global concordance calculations are not fused,; and partly because the AggArrayPerElement stuff seems pretty; slow right now and is dragging down the per-sample concordance.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6224
https://github.com/hail-is/hail/pull/6225:15,Availability,error,error,15,Left to do:; - error checking on bounds of slices. This could either be done in the emitted code for slice or in the IR with Die and what not. Open to advice on which is better.; - bind shape in the python iR generation so it doesn't get repeated a bunch of times. This would go along with the bounds checking in python,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6225
https://github.com/hail-is/hail/pull/6226:30,Integrability,interface,interface,30,"We had these semantics in our interface in the past, which was; lost in the Python reimplementation. Fixes #6122",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6226
https://github.com/hail-is/hail/issues/6232:38,Testability,log,logic,38,To be dropped in both header and data logic,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6232
https://github.com/hail-is/hail/pull/6233:10,Testability,log,logger,10,Somehow a logger object was getting imported (maybe from bokeh?); and overwriting `log`. `import *` is very bad style when we we're not sure what we're doing.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6233
https://github.com/hail-is/hail/pull/6233:83,Testability,log,log,83,Somehow a logger object was getting imported (maybe from bokeh?); and overwriting `log`. `import *` is very bad style when we we're not sure what we're doing.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6233
https://github.com/hail-is/hail/issues/6234:43,Usability,Simpl,Simplify,43,Use to fix TableHead(TableOrderBy) rule in Simplify,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6234
https://github.com/hail-is/hail/issues/6236:192,Safety,Safe,SafeRow,192,```; def collectRowKeys(): Array[Annotation] = {; val rowKeyIdx = mv.typ.rowKeyFieldIdx. mv.rvd.toRows.map[Any] { r =>; Row.fromSeq(rowKeyIdx.map(r.get)); }; .collect(); }; ```. toRows calls `SafeRow`. WTF,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6236
https://github.com/hail-is/hail/pull/6241:13,Integrability,interface,interface,13,I think this interface is clearer and it will be a necessary change when atomic batch creation causes the batch_id to be None until the entire batch is submitted.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6241
https://github.com/hail-is/hail/pull/6241:26,Usability,clear,clearer,26,I think this interface is clearer and it will be a necessary change when atomic batch creation causes the batch_id to be None until the entire batch is submitted.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6241
https://github.com/hail-is/hail/pull/6242:201,Deployability,deploy,deployed,201,"Changes:; - added monitoring setup (Prometheus, Grafana) to monitoring namespace; - I'm considering monitoring part of ""infrastructure"", no automated tests, gateway and router-resolver changes already deployed; - authenticated_users_only always passes userdata as second argument; - added authenticated_developers_only decorator to hailjwt, no userdata; - gateway forwards to internal namespaces: internal.hail.is/namespace proxies to router.namespace, so in general you'll go to internal.hail.is/namespace/service/the/real/url; - proxy only if namespace has router service and authorized developer; - add router to monitoring namespace that proxies for prometheus and grafana; - restrict ci to authorized developers. monitoring/grafana-cluster.json is an export of an initial Grafana monitoring dashboard that I constructed through the UI. If you're logged in as a developer, you can see Grafana at internal.hail.is/monitoring/grafana. The admin password is in the usual place.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6242
https://github.com/hail-is/hail/pull/6242:18,Energy Efficiency,monitor,monitoring,18,"Changes:; - added monitoring setup (Prometheus, Grafana) to monitoring namespace; - I'm considering monitoring part of ""infrastructure"", no automated tests, gateway and router-resolver changes already deployed; - authenticated_users_only always passes userdata as second argument; - added authenticated_developers_only decorator to hailjwt, no userdata; - gateway forwards to internal namespaces: internal.hail.is/namespace proxies to router.namespace, so in general you'll go to internal.hail.is/namespace/service/the/real/url; - proxy only if namespace has router service and authorized developer; - add router to monitoring namespace that proxies for prometheus and grafana; - restrict ci to authorized developers. monitoring/grafana-cluster.json is an export of an initial Grafana monitoring dashboard that I constructed through the UI. If you're logged in as a developer, you can see Grafana at internal.hail.is/monitoring/grafana. The admin password is in the usual place.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6242
https://github.com/hail-is/hail/pull/6242:60,Energy Efficiency,monitor,monitoring,60,"Changes:; - added monitoring setup (Prometheus, Grafana) to monitoring namespace; - I'm considering monitoring part of ""infrastructure"", no automated tests, gateway and router-resolver changes already deployed; - authenticated_users_only always passes userdata as second argument; - added authenticated_developers_only decorator to hailjwt, no userdata; - gateway forwards to internal namespaces: internal.hail.is/namespace proxies to router.namespace, so in general you'll go to internal.hail.is/namespace/service/the/real/url; - proxy only if namespace has router service and authorized developer; - add router to monitoring namespace that proxies for prometheus and grafana; - restrict ci to authorized developers. monitoring/grafana-cluster.json is an export of an initial Grafana monitoring dashboard that I constructed through the UI. If you're logged in as a developer, you can see Grafana at internal.hail.is/monitoring/grafana. The admin password is in the usual place.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6242
https://github.com/hail-is/hail/pull/6242:100,Energy Efficiency,monitor,monitoring,100,"Changes:; - added monitoring setup (Prometheus, Grafana) to monitoring namespace; - I'm considering monitoring part of ""infrastructure"", no automated tests, gateway and router-resolver changes already deployed; - authenticated_users_only always passes userdata as second argument; - added authenticated_developers_only decorator to hailjwt, no userdata; - gateway forwards to internal namespaces: internal.hail.is/namespace proxies to router.namespace, so in general you'll go to internal.hail.is/namespace/service/the/real/url; - proxy only if namespace has router service and authorized developer; - add router to monitoring namespace that proxies for prometheus and grafana; - restrict ci to authorized developers. monitoring/grafana-cluster.json is an export of an initial Grafana monitoring dashboard that I constructed through the UI. If you're logged in as a developer, you can see Grafana at internal.hail.is/monitoring/grafana. The admin password is in the usual place.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6242
https://github.com/hail-is/hail/pull/6242:616,Energy Efficiency,monitor,monitoring,616,"Changes:; - added monitoring setup (Prometheus, Grafana) to monitoring namespace; - I'm considering monitoring part of ""infrastructure"", no automated tests, gateway and router-resolver changes already deployed; - authenticated_users_only always passes userdata as second argument; - added authenticated_developers_only decorator to hailjwt, no userdata; - gateway forwards to internal namespaces: internal.hail.is/namespace proxies to router.namespace, so in general you'll go to internal.hail.is/namespace/service/the/real/url; - proxy only if namespace has router service and authorized developer; - add router to monitoring namespace that proxies for prometheus and grafana; - restrict ci to authorized developers. monitoring/grafana-cluster.json is an export of an initial Grafana monitoring dashboard that I constructed through the UI. If you're logged in as a developer, you can see Grafana at internal.hail.is/monitoring/grafana. The admin password is in the usual place.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6242
https://github.com/hail-is/hail/pull/6242:718,Energy Efficiency,monitor,monitoring,718,"Changes:; - added monitoring setup (Prometheus, Grafana) to monitoring namespace; - I'm considering monitoring part of ""infrastructure"", no automated tests, gateway and router-resolver changes already deployed; - authenticated_users_only always passes userdata as second argument; - added authenticated_developers_only decorator to hailjwt, no userdata; - gateway forwards to internal namespaces: internal.hail.is/namespace proxies to router.namespace, so in general you'll go to internal.hail.is/namespace/service/the/real/url; - proxy only if namespace has router service and authorized developer; - add router to monitoring namespace that proxies for prometheus and grafana; - restrict ci to authorized developers. monitoring/grafana-cluster.json is an export of an initial Grafana monitoring dashboard that I constructed through the UI. If you're logged in as a developer, you can see Grafana at internal.hail.is/monitoring/grafana. The admin password is in the usual place.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6242
https://github.com/hail-is/hail/pull/6242:785,Energy Efficiency,monitor,monitoring,785,"Changes:; - added monitoring setup (Prometheus, Grafana) to monitoring namespace; - I'm considering monitoring part of ""infrastructure"", no automated tests, gateway and router-resolver changes already deployed; - authenticated_users_only always passes userdata as second argument; - added authenticated_developers_only decorator to hailjwt, no userdata; - gateway forwards to internal namespaces: internal.hail.is/namespace proxies to router.namespace, so in general you'll go to internal.hail.is/namespace/service/the/real/url; - proxy only if namespace has router service and authorized developer; - add router to monitoring namespace that proxies for prometheus and grafana; - restrict ci to authorized developers. monitoring/grafana-cluster.json is an export of an initial Grafana monitoring dashboard that I constructed through the UI. If you're logged in as a developer, you can see Grafana at internal.hail.is/monitoring/grafana. The admin password is in the usual place.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6242
https://github.com/hail-is/hail/pull/6242:917,Energy Efficiency,monitor,monitoring,917,"Changes:; - added monitoring setup (Prometheus, Grafana) to monitoring namespace; - I'm considering monitoring part of ""infrastructure"", no automated tests, gateway and router-resolver changes already deployed; - authenticated_users_only always passes userdata as second argument; - added authenticated_developers_only decorator to hailjwt, no userdata; - gateway forwards to internal namespaces: internal.hail.is/namespace proxies to router.namespace, so in general you'll go to internal.hail.is/namespace/service/the/real/url; - proxy only if namespace has router service and authorized developer; - add router to monitoring namespace that proxies for prometheus and grafana; - restrict ci to authorized developers. monitoring/grafana-cluster.json is an export of an initial Grafana monitoring dashboard that I constructed through the UI. If you're logged in as a developer, you can see Grafana at internal.hail.is/monitoring/grafana. The admin password is in the usual place.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6242
https://github.com/hail-is/hail/pull/6242:169,Integrability,rout,router-resolver,169,"Changes:; - added monitoring setup (Prometheus, Grafana) to monitoring namespace; - I'm considering monitoring part of ""infrastructure"", no automated tests, gateway and router-resolver changes already deployed; - authenticated_users_only always passes userdata as second argument; - added authenticated_developers_only decorator to hailjwt, no userdata; - gateway forwards to internal namespaces: internal.hail.is/namespace proxies to router.namespace, so in general you'll go to internal.hail.is/namespace/service/the/real/url; - proxy only if namespace has router service and authorized developer; - add router to monitoring namespace that proxies for prometheus and grafana; - restrict ci to authorized developers. monitoring/grafana-cluster.json is an export of an initial Grafana monitoring dashboard that I constructed through the UI. If you're logged in as a developer, you can see Grafana at internal.hail.is/monitoring/grafana. The admin password is in the usual place.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6242
https://github.com/hail-is/hail/pull/6242:435,Integrability,rout,router,435,"Changes:; - added monitoring setup (Prometheus, Grafana) to monitoring namespace; - I'm considering monitoring part of ""infrastructure"", no automated tests, gateway and router-resolver changes already deployed; - authenticated_users_only always passes userdata as second argument; - added authenticated_developers_only decorator to hailjwt, no userdata; - gateway forwards to internal namespaces: internal.hail.is/namespace proxies to router.namespace, so in general you'll go to internal.hail.is/namespace/service/the/real/url; - proxy only if namespace has router service and authorized developer; - add router to monitoring namespace that proxies for prometheus and grafana; - restrict ci to authorized developers. monitoring/grafana-cluster.json is an export of an initial Grafana monitoring dashboard that I constructed through the UI. If you're logged in as a developer, you can see Grafana at internal.hail.is/monitoring/grafana. The admin password is in the usual place.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6242
https://github.com/hail-is/hail/pull/6242:559,Integrability,rout,router,559,"Changes:; - added monitoring setup (Prometheus, Grafana) to monitoring namespace; - I'm considering monitoring part of ""infrastructure"", no automated tests, gateway and router-resolver changes already deployed; - authenticated_users_only always passes userdata as second argument; - added authenticated_developers_only decorator to hailjwt, no userdata; - gateway forwards to internal namespaces: internal.hail.is/namespace proxies to router.namespace, so in general you'll go to internal.hail.is/namespace/service/the/real/url; - proxy only if namespace has router service and authorized developer; - add router to monitoring namespace that proxies for prometheus and grafana; - restrict ci to authorized developers. monitoring/grafana-cluster.json is an export of an initial Grafana monitoring dashboard that I constructed through the UI. If you're logged in as a developer, you can see Grafana at internal.hail.is/monitoring/grafana. The admin password is in the usual place.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6242
https://github.com/hail-is/hail/pull/6242:606,Integrability,rout,router,606,"Changes:; - added monitoring setup (Prometheus, Grafana) to monitoring namespace; - I'm considering monitoring part of ""infrastructure"", no automated tests, gateway and router-resolver changes already deployed; - authenticated_users_only always passes userdata as second argument; - added authenticated_developers_only decorator to hailjwt, no userdata; - gateway forwards to internal namespaces: internal.hail.is/namespace proxies to router.namespace, so in general you'll go to internal.hail.is/namespace/service/the/real/url; - proxy only if namespace has router service and authorized developer; - add router to monitoring namespace that proxies for prometheus and grafana; - restrict ci to authorized developers. monitoring/grafana-cluster.json is an export of an initial Grafana monitoring dashboard that I constructed through the UI. If you're logged in as a developer, you can see Grafana at internal.hail.is/monitoring/grafana. The admin password is in the usual place.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6242
https://github.com/hail-is/hail/pull/6242:578,Security,authoriz,authorized,578,"Changes:; - added monitoring setup (Prometheus, Grafana) to monitoring namespace; - I'm considering monitoring part of ""infrastructure"", no automated tests, gateway and router-resolver changes already deployed; - authenticated_users_only always passes userdata as second argument; - added authenticated_developers_only decorator to hailjwt, no userdata; - gateway forwards to internal namespaces: internal.hail.is/namespace proxies to router.namespace, so in general you'll go to internal.hail.is/namespace/service/the/real/url; - proxy only if namespace has router service and authorized developer; - add router to monitoring namespace that proxies for prometheus and grafana; - restrict ci to authorized developers. monitoring/grafana-cluster.json is an export of an initial Grafana monitoring dashboard that I constructed through the UI. If you're logged in as a developer, you can see Grafana at internal.hail.is/monitoring/grafana. The admin password is in the usual place.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6242
https://github.com/hail-is/hail/pull/6242:695,Security,authoriz,authorized,695,"Changes:; - added monitoring setup (Prometheus, Grafana) to monitoring namespace; - I'm considering monitoring part of ""infrastructure"", no automated tests, gateway and router-resolver changes already deployed; - authenticated_users_only always passes userdata as second argument; - added authenticated_developers_only decorator to hailjwt, no userdata; - gateway forwards to internal namespaces: internal.hail.is/namespace proxies to router.namespace, so in general you'll go to internal.hail.is/namespace/service/the/real/url; - proxy only if namespace has router service and authorized developer; - add router to monitoring namespace that proxies for prometheus and grafana; - restrict ci to authorized developers. monitoring/grafana-cluster.json is an export of an initial Grafana monitoring dashboard that I constructed through the UI. If you're logged in as a developer, you can see Grafana at internal.hail.is/monitoring/grafana. The admin password is in the usual place.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6242
https://github.com/hail-is/hail/pull/6242:947,Security,password,password,947,"Changes:; - added monitoring setup (Prometheus, Grafana) to monitoring namespace; - I'm considering monitoring part of ""infrastructure"", no automated tests, gateway and router-resolver changes already deployed; - authenticated_users_only always passes userdata as second argument; - added authenticated_developers_only decorator to hailjwt, no userdata; - gateway forwards to internal namespaces: internal.hail.is/namespace proxies to router.namespace, so in general you'll go to internal.hail.is/namespace/service/the/real/url; - proxy only if namespace has router service and authorized developer; - add router to monitoring namespace that proxies for prometheus and grafana; - restrict ci to authorized developers. monitoring/grafana-cluster.json is an export of an initial Grafana monitoring dashboard that I constructed through the UI. If you're logged in as a developer, you can see Grafana at internal.hail.is/monitoring/grafana. The admin password is in the usual place.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6242
https://github.com/hail-is/hail/pull/6242:150,Testability,test,tests,150,"Changes:; - added monitoring setup (Prometheus, Grafana) to monitoring namespace; - I'm considering monitoring part of ""infrastructure"", no automated tests, gateway and router-resolver changes already deployed; - authenticated_users_only always passes userdata as second argument; - added authenticated_developers_only decorator to hailjwt, no userdata; - gateway forwards to internal namespaces: internal.hail.is/namespace proxies to router.namespace, so in general you'll go to internal.hail.is/namespace/service/the/real/url; - proxy only if namespace has router service and authorized developer; - add router to monitoring namespace that proxies for prometheus and grafana; - restrict ci to authorized developers. monitoring/grafana-cluster.json is an export of an initial Grafana monitoring dashboard that I constructed through the UI. If you're logged in as a developer, you can see Grafana at internal.hail.is/monitoring/grafana. The admin password is in the usual place.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6242
https://github.com/hail-is/hail/pull/6242:851,Testability,log,logged,851,"Changes:; - added monitoring setup (Prometheus, Grafana) to monitoring namespace; - I'm considering monitoring part of ""infrastructure"", no automated tests, gateway and router-resolver changes already deployed; - authenticated_users_only always passes userdata as second argument; - added authenticated_developers_only decorator to hailjwt, no userdata; - gateway forwards to internal namespaces: internal.hail.is/namespace proxies to router.namespace, so in general you'll go to internal.hail.is/namespace/service/the/real/url; - proxy only if namespace has router service and authorized developer; - add router to monitoring namespace that proxies for prometheus and grafana; - restrict ci to authorized developers. monitoring/grafana-cluster.json is an export of an initial Grafana monitoring dashboard that I constructed through the UI. If you're logged in as a developer, you can see Grafana at internal.hail.is/monitoring/grafana. The admin password is in the usual place.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6242
https://github.com/hail-is/hail/pull/6245:308,Testability,test,tests,308,"- Removed `close`, `is_open`, and `ttl` on batches; - Changed `create_job` and `create_batch` to be synchronous (bunch of ci code changed as a result); - Added `submit` to batches that actually submits to request to the server; - Server still inserts records not in one transaction (future PR); - Added some tests of operations not allowed when the batch is being created. I'm not entirely happy with the client code. I'm open to suggestions!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6245
https://github.com/hail-is/hail/pull/6246:272,Energy Efficiency,allocate,allocated,272,"and propagate resources to docker build command. Without this, we can overload the node by running a bunch of build image jobs which tiny cpu allocations that invoke docker build which run unconstrained docker builds. This should resolve the docker timeout issue I saw. I allocated 2G per build job, and pass 1.5G of that to the docker build. Just guessing on that -- I'm not sure how to figure out how much memory a docker build required.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6246
https://github.com/hail-is/hail/pull/6246:249,Safety,timeout,timeout,249,"and propagate resources to docker build command. Without this, we can overload the node by running a bunch of build image jobs which tiny cpu allocations that invoke docker build which run unconstrained docker builds. This should resolve the docker timeout issue I saw. I allocated 2G per build job, and pass 1.5G of that to the docker build. Just guessing on that -- I'm not sure how to figure out how much memory a docker build required.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6246
https://github.com/hail-is/hail/pull/6248:720,Availability,error,error,720,"a few things happening here, most of which was me trying to not have to explicitly list dependencies in the shadowJar/shadowTestJar tasks:; - upgraded gradle to 5.0 and some plugins to be compatible; - split compile dependencies into ""bundled"" and ""unbundled"" to more explicitly separate the things we want in the jars and dependencies that we don't want bundled/are currently depending on the spark installation for. I did it this way because the shadowJar `exclude` filter does not let you exclude transitive dependencies, and I just wanted to exclude the entire spark/scala dependency tree.; - there was a problem where trying to run the tests kept giving me the ""Could not find or load main class org.testng.TestNG"" error, despite the class clearly being findable from the classpath I was providing. I added some excludes per this:; https://stackoverflow.com/questions/51455197/gradle-fatjar-could-not-find-or-load-main-class; (although I believe this is no longer strictly necessary after excluding all the transitive spark dependencies)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6248
https://github.com/hail-is/hail/pull/6248:142,Deployability,upgrade,upgraded,142,"a few things happening here, most of which was me trying to not have to explicitly list dependencies in the shadowJar/shadowTestJar tasks:; - upgraded gradle to 5.0 and some plugins to be compatible; - split compile dependencies into ""bundled"" and ""unbundled"" to more explicitly separate the things we want in the jars and dependencies that we don't want bundled/are currently depending on the spark installation for. I did it this way because the shadowJar `exclude` filter does not let you exclude transitive dependencies, and I just wanted to exclude the entire spark/scala dependency tree.; - there was a problem where trying to run the tests kept giving me the ""Could not find or load main class org.testng.TestNG"" error, despite the class clearly being findable from the classpath I was providing. I added some excludes per this:; https://stackoverflow.com/questions/51455197/gradle-fatjar-could-not-find-or-load-main-class; (although I believe this is no longer strictly necessary after excluding all the transitive spark dependencies)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6248
https://github.com/hail-is/hail/pull/6248:400,Deployability,install,installation,400,"a few things happening here, most of which was me trying to not have to explicitly list dependencies in the shadowJar/shadowTestJar tasks:; - upgraded gradle to 5.0 and some plugins to be compatible; - split compile dependencies into ""bundled"" and ""unbundled"" to more explicitly separate the things we want in the jars and dependencies that we don't want bundled/are currently depending on the spark installation for. I did it this way because the shadowJar `exclude` filter does not let you exclude transitive dependencies, and I just wanted to exclude the entire spark/scala dependency tree.; - there was a problem where trying to run the tests kept giving me the ""Could not find or load main class org.testng.TestNG"" error, despite the class clearly being findable from the classpath I was providing. I added some excludes per this:; https://stackoverflow.com/questions/51455197/gradle-fatjar-could-not-find-or-load-main-class; (although I believe this is no longer strictly necessary after excluding all the transitive spark dependencies)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6248
https://github.com/hail-is/hail/pull/6248:88,Integrability,depend,dependencies,88,"a few things happening here, most of which was me trying to not have to explicitly list dependencies in the shadowJar/shadowTestJar tasks:; - upgraded gradle to 5.0 and some plugins to be compatible; - split compile dependencies into ""bundled"" and ""unbundled"" to more explicitly separate the things we want in the jars and dependencies that we don't want bundled/are currently depending on the spark installation for. I did it this way because the shadowJar `exclude` filter does not let you exclude transitive dependencies, and I just wanted to exclude the entire spark/scala dependency tree.; - there was a problem where trying to run the tests kept giving me the ""Could not find or load main class org.testng.TestNG"" error, despite the class clearly being findable from the classpath I was providing. I added some excludes per this:; https://stackoverflow.com/questions/51455197/gradle-fatjar-could-not-find-or-load-main-class; (although I believe this is no longer strictly necessary after excluding all the transitive spark dependencies)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6248
https://github.com/hail-is/hail/pull/6248:216,Integrability,depend,dependencies,216,"a few things happening here, most of which was me trying to not have to explicitly list dependencies in the shadowJar/shadowTestJar tasks:; - upgraded gradle to 5.0 and some plugins to be compatible; - split compile dependencies into ""bundled"" and ""unbundled"" to more explicitly separate the things we want in the jars and dependencies that we don't want bundled/are currently depending on the spark installation for. I did it this way because the shadowJar `exclude` filter does not let you exclude transitive dependencies, and I just wanted to exclude the entire spark/scala dependency tree.; - there was a problem where trying to run the tests kept giving me the ""Could not find or load main class org.testng.TestNG"" error, despite the class clearly being findable from the classpath I was providing. I added some excludes per this:; https://stackoverflow.com/questions/51455197/gradle-fatjar-could-not-find-or-load-main-class; (although I believe this is no longer strictly necessary after excluding all the transitive spark dependencies)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6248
https://github.com/hail-is/hail/pull/6248:323,Integrability,depend,dependencies,323,"a few things happening here, most of which was me trying to not have to explicitly list dependencies in the shadowJar/shadowTestJar tasks:; - upgraded gradle to 5.0 and some plugins to be compatible; - split compile dependencies into ""bundled"" and ""unbundled"" to more explicitly separate the things we want in the jars and dependencies that we don't want bundled/are currently depending on the spark installation for. I did it this way because the shadowJar `exclude` filter does not let you exclude transitive dependencies, and I just wanted to exclude the entire spark/scala dependency tree.; - there was a problem where trying to run the tests kept giving me the ""Could not find or load main class org.testng.TestNG"" error, despite the class clearly being findable from the classpath I was providing. I added some excludes per this:; https://stackoverflow.com/questions/51455197/gradle-fatjar-could-not-find-or-load-main-class; (although I believe this is no longer strictly necessary after excluding all the transitive spark dependencies)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6248
https://github.com/hail-is/hail/pull/6248:377,Integrability,depend,depending,377,"a few things happening here, most of which was me trying to not have to explicitly list dependencies in the shadowJar/shadowTestJar tasks:; - upgraded gradle to 5.0 and some plugins to be compatible; - split compile dependencies into ""bundled"" and ""unbundled"" to more explicitly separate the things we want in the jars and dependencies that we don't want bundled/are currently depending on the spark installation for. I did it this way because the shadowJar `exclude` filter does not let you exclude transitive dependencies, and I just wanted to exclude the entire spark/scala dependency tree.; - there was a problem where trying to run the tests kept giving me the ""Could not find or load main class org.testng.TestNG"" error, despite the class clearly being findable from the classpath I was providing. I added some excludes per this:; https://stackoverflow.com/questions/51455197/gradle-fatjar-could-not-find-or-load-main-class; (although I believe this is no longer strictly necessary after excluding all the transitive spark dependencies)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6248
https://github.com/hail-is/hail/pull/6248:511,Integrability,depend,dependencies,511,"a few things happening here, most of which was me trying to not have to explicitly list dependencies in the shadowJar/shadowTestJar tasks:; - upgraded gradle to 5.0 and some plugins to be compatible; - split compile dependencies into ""bundled"" and ""unbundled"" to more explicitly separate the things we want in the jars and dependencies that we don't want bundled/are currently depending on the spark installation for. I did it this way because the shadowJar `exclude` filter does not let you exclude transitive dependencies, and I just wanted to exclude the entire spark/scala dependency tree.; - there was a problem where trying to run the tests kept giving me the ""Could not find or load main class org.testng.TestNG"" error, despite the class clearly being findable from the classpath I was providing. I added some excludes per this:; https://stackoverflow.com/questions/51455197/gradle-fatjar-could-not-find-or-load-main-class; (although I believe this is no longer strictly necessary after excluding all the transitive spark dependencies)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6248
https://github.com/hail-is/hail/pull/6248:577,Integrability,depend,dependency,577,"a few things happening here, most of which was me trying to not have to explicitly list dependencies in the shadowJar/shadowTestJar tasks:; - upgraded gradle to 5.0 and some plugins to be compatible; - split compile dependencies into ""bundled"" and ""unbundled"" to more explicitly separate the things we want in the jars and dependencies that we don't want bundled/are currently depending on the spark installation for. I did it this way because the shadowJar `exclude` filter does not let you exclude transitive dependencies, and I just wanted to exclude the entire spark/scala dependency tree.; - there was a problem where trying to run the tests kept giving me the ""Could not find or load main class org.testng.TestNG"" error, despite the class clearly being findable from the classpath I was providing. I added some excludes per this:; https://stackoverflow.com/questions/51455197/gradle-fatjar-could-not-find-or-load-main-class; (although I believe this is no longer strictly necessary after excluding all the transitive spark dependencies)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6248
https://github.com/hail-is/hail/pull/6248:1029,Integrability,depend,dependencies,1029,"a few things happening here, most of which was me trying to not have to explicitly list dependencies in the shadowJar/shadowTestJar tasks:; - upgraded gradle to 5.0 and some plugins to be compatible; - split compile dependencies into ""bundled"" and ""unbundled"" to more explicitly separate the things we want in the jars and dependencies that we don't want bundled/are currently depending on the spark installation for. I did it this way because the shadowJar `exclude` filter does not let you exclude transitive dependencies, and I just wanted to exclude the entire spark/scala dependency tree.; - there was a problem where trying to run the tests kept giving me the ""Could not find or load main class org.testng.TestNG"" error, despite the class clearly being findable from the classpath I was providing. I added some excludes per this:; https://stackoverflow.com/questions/51455197/gradle-fatjar-could-not-find-or-load-main-class; (although I believe this is no longer strictly necessary after excluding all the transitive spark dependencies)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6248
https://github.com/hail-is/hail/pull/6248:174,Modifiability,plugin,plugins,174,"a few things happening here, most of which was me trying to not have to explicitly list dependencies in the shadowJar/shadowTestJar tasks:; - upgraded gradle to 5.0 and some plugins to be compatible; - split compile dependencies into ""bundled"" and ""unbundled"" to more explicitly separate the things we want in the jars and dependencies that we don't want bundled/are currently depending on the spark installation for. I did it this way because the shadowJar `exclude` filter does not let you exclude transitive dependencies, and I just wanted to exclude the entire spark/scala dependency tree.; - there was a problem where trying to run the tests kept giving me the ""Could not find or load main class org.testng.TestNG"" error, despite the class clearly being findable from the classpath I was providing. I added some excludes per this:; https://stackoverflow.com/questions/51455197/gradle-fatjar-could-not-find-or-load-main-class; (although I believe this is no longer strictly necessary after excluding all the transitive spark dependencies)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6248
https://github.com/hail-is/hail/pull/6248:685,Performance,load,load,685,"a few things happening here, most of which was me trying to not have to explicitly list dependencies in the shadowJar/shadowTestJar tasks:; - upgraded gradle to 5.0 and some plugins to be compatible; - split compile dependencies into ""bundled"" and ""unbundled"" to more explicitly separate the things we want in the jars and dependencies that we don't want bundled/are currently depending on the spark installation for. I did it this way because the shadowJar `exclude` filter does not let you exclude transitive dependencies, and I just wanted to exclude the entire spark/scala dependency tree.; - there was a problem where trying to run the tests kept giving me the ""Could not find or load main class org.testng.TestNG"" error, despite the class clearly being findable from the classpath I was providing. I added some excludes per this:; https://stackoverflow.com/questions/51455197/gradle-fatjar-could-not-find-or-load-main-class; (although I believe this is no longer strictly necessary after excluding all the transitive spark dependencies)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6248
https://github.com/hail-is/hail/pull/6248:914,Performance,load,load-main-class,914,"a few things happening here, most of which was me trying to not have to explicitly list dependencies in the shadowJar/shadowTestJar tasks:; - upgraded gradle to 5.0 and some plugins to be compatible; - split compile dependencies into ""bundled"" and ""unbundled"" to more explicitly separate the things we want in the jars and dependencies that we don't want bundled/are currently depending on the spark installation for. I did it this way because the shadowJar `exclude` filter does not let you exclude transitive dependencies, and I just wanted to exclude the entire spark/scala dependency tree.; - there was a problem where trying to run the tests kept giving me the ""Could not find or load main class org.testng.TestNG"" error, despite the class clearly being findable from the classpath I was providing. I added some excludes per this:; https://stackoverflow.com/questions/51455197/gradle-fatjar-could-not-find-or-load-main-class; (although I believe this is no longer strictly necessary after excluding all the transitive spark dependencies)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6248
https://github.com/hail-is/hail/pull/6248:641,Testability,test,tests,641,"a few things happening here, most of which was me trying to not have to explicitly list dependencies in the shadowJar/shadowTestJar tasks:; - upgraded gradle to 5.0 and some plugins to be compatible; - split compile dependencies into ""bundled"" and ""unbundled"" to more explicitly separate the things we want in the jars and dependencies that we don't want bundled/are currently depending on the spark installation for. I did it this way because the shadowJar `exclude` filter does not let you exclude transitive dependencies, and I just wanted to exclude the entire spark/scala dependency tree.; - there was a problem where trying to run the tests kept giving me the ""Could not find or load main class org.testng.TestNG"" error, despite the class clearly being findable from the classpath I was providing. I added some excludes per this:; https://stackoverflow.com/questions/51455197/gradle-fatjar-could-not-find-or-load-main-class; (although I believe this is no longer strictly necessary after excluding all the transitive spark dependencies)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6248
https://github.com/hail-is/hail/pull/6248:705,Testability,test,testng,705,"a few things happening here, most of which was me trying to not have to explicitly list dependencies in the shadowJar/shadowTestJar tasks:; - upgraded gradle to 5.0 and some plugins to be compatible; - split compile dependencies into ""bundled"" and ""unbundled"" to more explicitly separate the things we want in the jars and dependencies that we don't want bundled/are currently depending on the spark installation for. I did it this way because the shadowJar `exclude` filter does not let you exclude transitive dependencies, and I just wanted to exclude the entire spark/scala dependency tree.; - there was a problem where trying to run the tests kept giving me the ""Could not find or load main class org.testng.TestNG"" error, despite the class clearly being findable from the classpath I was providing. I added some excludes per this:; https://stackoverflow.com/questions/51455197/gradle-fatjar-could-not-find-or-load-main-class; (although I believe this is no longer strictly necessary after excluding all the transitive spark dependencies)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6248
https://github.com/hail-is/hail/pull/6248:712,Testability,Test,TestNG,712,"a few things happening here, most of which was me trying to not have to explicitly list dependencies in the shadowJar/shadowTestJar tasks:; - upgraded gradle to 5.0 and some plugins to be compatible; - split compile dependencies into ""bundled"" and ""unbundled"" to more explicitly separate the things we want in the jars and dependencies that we don't want bundled/are currently depending on the spark installation for. I did it this way because the shadowJar `exclude` filter does not let you exclude transitive dependencies, and I just wanted to exclude the entire spark/scala dependency tree.; - there was a problem where trying to run the tests kept giving me the ""Could not find or load main class org.testng.TestNG"" error, despite the class clearly being findable from the classpath I was providing. I added some excludes per this:; https://stackoverflow.com/questions/51455197/gradle-fatjar-could-not-find-or-load-main-class; (although I believe this is no longer strictly necessary after excluding all the transitive spark dependencies)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6248
https://github.com/hail-is/hail/pull/6248:745,Usability,clear,clearly,745,"a few things happening here, most of which was me trying to not have to explicitly list dependencies in the shadowJar/shadowTestJar tasks:; - upgraded gradle to 5.0 and some plugins to be compatible; - split compile dependencies into ""bundled"" and ""unbundled"" to more explicitly separate the things we want in the jars and dependencies that we don't want bundled/are currently depending on the spark installation for. I did it this way because the shadowJar `exclude` filter does not let you exclude transitive dependencies, and I just wanted to exclude the entire spark/scala dependency tree.; - there was a problem where trying to run the tests kept giving me the ""Could not find or load main class org.testng.TestNG"" error, despite the class clearly being findable from the classpath I was providing. I added some excludes per this:; https://stackoverflow.com/questions/51455197/gradle-fatjar-could-not-find-or-load-main-class; (although I believe this is no longer strictly necessary after excluding all the transitive spark dependencies)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6248
https://github.com/hail-is/hail/pull/6250:116,Deployability,deploy,deploys,116,"Made the following changes:. - Disabled dataproc tests; - Moved dataproc tests to Makefile, to be run before manual deploys; - Add back VEP cluster test script; - Removed cloudtools config files; - Removed the latest-build functionality; - Added VEP scripts to hailctl/dataproc/resources; - Changed init_notebook to pip install hail wheels, picking up; dependencies automatically; - add out-of-date check (once per day) to hailctl startup",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6250
https://github.com/hail-is/hail/pull/6250:320,Deployability,install,install,320,"Made the following changes:. - Disabled dataproc tests; - Moved dataproc tests to Makefile, to be run before manual deploys; - Add back VEP cluster test script; - Removed cloudtools config files; - Removed the latest-build functionality; - Added VEP scripts to hailctl/dataproc/resources; - Changed init_notebook to pip install hail wheels, picking up; dependencies automatically; - add out-of-date check (once per day) to hailctl startup",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6250
https://github.com/hail-is/hail/pull/6250:353,Integrability,depend,dependencies,353,"Made the following changes:. - Disabled dataproc tests; - Moved dataproc tests to Makefile, to be run before manual deploys; - Add back VEP cluster test script; - Removed cloudtools config files; - Removed the latest-build functionality; - Added VEP scripts to hailctl/dataproc/resources; - Changed init_notebook to pip install hail wheels, picking up; dependencies automatically; - add out-of-date check (once per day) to hailctl startup",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6250
https://github.com/hail-is/hail/pull/6250:182,Modifiability,config,config,182,"Made the following changes:. - Disabled dataproc tests; - Moved dataproc tests to Makefile, to be run before manual deploys; - Add back VEP cluster test script; - Removed cloudtools config files; - Removed the latest-build functionality; - Added VEP scripts to hailctl/dataproc/resources; - Changed init_notebook to pip install hail wheels, picking up; dependencies automatically; - add out-of-date check (once per day) to hailctl startup",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6250
https://github.com/hail-is/hail/pull/6250:49,Testability,test,tests,49,"Made the following changes:. - Disabled dataproc tests; - Moved dataproc tests to Makefile, to be run before manual deploys; - Add back VEP cluster test script; - Removed cloudtools config files; - Removed the latest-build functionality; - Added VEP scripts to hailctl/dataproc/resources; - Changed init_notebook to pip install hail wheels, picking up; dependencies automatically; - add out-of-date check (once per day) to hailctl startup",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6250
https://github.com/hail-is/hail/pull/6250:73,Testability,test,tests,73,"Made the following changes:. - Disabled dataproc tests; - Moved dataproc tests to Makefile, to be run before manual deploys; - Add back VEP cluster test script; - Removed cloudtools config files; - Removed the latest-build functionality; - Added VEP scripts to hailctl/dataproc/resources; - Changed init_notebook to pip install hail wheels, picking up; dependencies automatically; - add out-of-date check (once per day) to hailctl startup",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6250
https://github.com/hail-is/hail/pull/6250:148,Testability,test,test,148,"Made the following changes:. - Disabled dataproc tests; - Moved dataproc tests to Makefile, to be run before manual deploys; - Add back VEP cluster test script; - Removed cloudtools config files; - Removed the latest-build functionality; - Added VEP scripts to hailctl/dataproc/resources; - Changed init_notebook to pip install hail wheels, picking up; dependencies automatically; - add out-of-date check (once per day) to hailctl startup",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6250
https://github.com/hail-is/hail/pull/6259:8,Testability,log,logs,8,"kubectl logs defaults to 10 lines if a selector is used:. > --tail=-1: Lines of recent log file to display. Defaults to -1 with no selector, showing all log lines otherwise; > 10, if a selector is provided. Add `--tail=999999` in those cases. `--tail=-1` doesn't seem to do anything (still only get 10 lines). If there are multiple replicas, I find logs to border on useless. I think longer term, we want to get logs (even for tests) into centralized logging in real-time and we can replace these logging steps to links to the logging infrastructure.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6259
https://github.com/hail-is/hail/pull/6259:87,Testability,log,log,87,"kubectl logs defaults to 10 lines if a selector is used:. > --tail=-1: Lines of recent log file to display. Defaults to -1 with no selector, showing all log lines otherwise; > 10, if a selector is provided. Add `--tail=999999` in those cases. `--tail=-1` doesn't seem to do anything (still only get 10 lines). If there are multiple replicas, I find logs to border on useless. I think longer term, we want to get logs (even for tests) into centralized logging in real-time and we can replace these logging steps to links to the logging infrastructure.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6259
https://github.com/hail-is/hail/pull/6259:153,Testability,log,log,153,"kubectl logs defaults to 10 lines if a selector is used:. > --tail=-1: Lines of recent log file to display. Defaults to -1 with no selector, showing all log lines otherwise; > 10, if a selector is provided. Add `--tail=999999` in those cases. `--tail=-1` doesn't seem to do anything (still only get 10 lines). If there are multiple replicas, I find logs to border on useless. I think longer term, we want to get logs (even for tests) into centralized logging in real-time and we can replace these logging steps to links to the logging infrastructure.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6259
https://github.com/hail-is/hail/pull/6259:349,Testability,log,logs,349,"kubectl logs defaults to 10 lines if a selector is used:. > --tail=-1: Lines of recent log file to display. Defaults to -1 with no selector, showing all log lines otherwise; > 10, if a selector is provided. Add `--tail=999999` in those cases. `--tail=-1` doesn't seem to do anything (still only get 10 lines). If there are multiple replicas, I find logs to border on useless. I think longer term, we want to get logs (even for tests) into centralized logging in real-time and we can replace these logging steps to links to the logging infrastructure.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6259
https://github.com/hail-is/hail/pull/6259:412,Testability,log,logs,412,"kubectl logs defaults to 10 lines if a selector is used:. > --tail=-1: Lines of recent log file to display. Defaults to -1 with no selector, showing all log lines otherwise; > 10, if a selector is provided. Add `--tail=999999` in those cases. `--tail=-1` doesn't seem to do anything (still only get 10 lines). If there are multiple replicas, I find logs to border on useless. I think longer term, we want to get logs (even for tests) into centralized logging in real-time and we can replace these logging steps to links to the logging infrastructure.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6259
https://github.com/hail-is/hail/pull/6259:427,Testability,test,tests,427,"kubectl logs defaults to 10 lines if a selector is used:. > --tail=-1: Lines of recent log file to display. Defaults to -1 with no selector, showing all log lines otherwise; > 10, if a selector is provided. Add `--tail=999999` in those cases. `--tail=-1` doesn't seem to do anything (still only get 10 lines). If there are multiple replicas, I find logs to border on useless. I think longer term, we want to get logs (even for tests) into centralized logging in real-time and we can replace these logging steps to links to the logging infrastructure.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6259
https://github.com/hail-is/hail/pull/6259:451,Testability,log,logging,451,"kubectl logs defaults to 10 lines if a selector is used:. > --tail=-1: Lines of recent log file to display. Defaults to -1 with no selector, showing all log lines otherwise; > 10, if a selector is provided. Add `--tail=999999` in those cases. `--tail=-1` doesn't seem to do anything (still only get 10 lines). If there are multiple replicas, I find logs to border on useless. I think longer term, we want to get logs (even for tests) into centralized logging in real-time and we can replace these logging steps to links to the logging infrastructure.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6259
https://github.com/hail-is/hail/pull/6259:497,Testability,log,logging,497,"kubectl logs defaults to 10 lines if a selector is used:. > --tail=-1: Lines of recent log file to display. Defaults to -1 with no selector, showing all log lines otherwise; > 10, if a selector is provided. Add `--tail=999999` in those cases. `--tail=-1` doesn't seem to do anything (still only get 10 lines). If there are multiple replicas, I find logs to border on useless. I think longer term, we want to get logs (even for tests) into centralized logging in real-time and we can replace these logging steps to links to the logging infrastructure.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6259
https://github.com/hail-is/hail/pull/6259:527,Testability,log,logging,527,"kubectl logs defaults to 10 lines if a selector is used:. > --tail=-1: Lines of recent log file to display. Defaults to -1 with no selector, showing all log lines otherwise; > 10, if a selector is provided. Add `--tail=999999` in those cases. `--tail=-1` doesn't seem to do anything (still only get 10 lines). If there are multiple replicas, I find logs to border on useless. I think longer term, we want to get logs (even for tests) into centralized logging in real-time and we can replace these logging steps to links to the logging infrastructure.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6259
https://github.com/hail-is/hail/pull/6261:22,Testability,test,test,22,I have no idea how to test this. Any ideas @danking @cseed?. I should also disable the button after a successful request goes through.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6261
https://github.com/hail-is/hail/pull/6263:29,Availability,failure,failure,29,"Had this refractory Dataproc failure, that kind-of pointed to serialization errors, but which @tpoterba clearly saw wasn't due to serialization, as a test in which the HadoopFS class was explicitly serialized and deserialized succeeded. The problem appeared to be in something affecting sparkContext's ability to broadcast, as even the standard SerializableHadoopConfiguration would appear null in map-reduce operations. I therefore created a clean-slate branch from master, and have issued this here. It passes all tests, including a local reproduction of the Dataproc test, by spinning up 1 spark master, 2 workers, and passing initializing hail with master=spark-master:7077 (thanks @cseed).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6263
https://github.com/hail-is/hail/pull/6263:76,Availability,error,errors,76,"Had this refractory Dataproc failure, that kind-of pointed to serialization errors, but which @tpoterba clearly saw wasn't due to serialization, as a test in which the HadoopFS class was explicitly serialized and deserialized succeeded. The problem appeared to be in something affecting sparkContext's ability to broadcast, as even the standard SerializableHadoopConfiguration would appear null in map-reduce operations. I therefore created a clean-slate branch from master, and have issued this here. It passes all tests, including a local reproduction of the Dataproc test, by spinning up 1 spark master, 2 workers, and passing initializing hail with master=spark-master:7077 (thanks @cseed).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6263
https://github.com/hail-is/hail/pull/6263:402,Energy Efficiency,reduce,reduce,402,"Had this refractory Dataproc failure, that kind-of pointed to serialization errors, but which @tpoterba clearly saw wasn't due to serialization, as a test in which the HadoopFS class was explicitly serialized and deserialized succeeded. The problem appeared to be in something affecting sparkContext's ability to broadcast, as even the standard SerializableHadoopConfiguration would appear null in map-reduce operations. I therefore created a clean-slate branch from master, and have issued this here. It passes all tests, including a local reproduction of the Dataproc test, by spinning up 1 spark master, 2 workers, and passing initializing hail with master=spark-master:7077 (thanks @cseed).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6263
https://github.com/hail-is/hail/pull/6263:150,Testability,test,test,150,"Had this refractory Dataproc failure, that kind-of pointed to serialization errors, but which @tpoterba clearly saw wasn't due to serialization, as a test in which the HadoopFS class was explicitly serialized and deserialized succeeded. The problem appeared to be in something affecting sparkContext's ability to broadcast, as even the standard SerializableHadoopConfiguration would appear null in map-reduce operations. I therefore created a clean-slate branch from master, and have issued this here. It passes all tests, including a local reproduction of the Dataproc test, by spinning up 1 spark master, 2 workers, and passing initializing hail with master=spark-master:7077 (thanks @cseed).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6263
https://github.com/hail-is/hail/pull/6263:516,Testability,test,tests,516,"Had this refractory Dataproc failure, that kind-of pointed to serialization errors, but which @tpoterba clearly saw wasn't due to serialization, as a test in which the HadoopFS class was explicitly serialized and deserialized succeeded. The problem appeared to be in something affecting sparkContext's ability to broadcast, as even the standard SerializableHadoopConfiguration would appear null in map-reduce operations. I therefore created a clean-slate branch from master, and have issued this here. It passes all tests, including a local reproduction of the Dataproc test, by spinning up 1 spark master, 2 workers, and passing initializing hail with master=spark-master:7077 (thanks @cseed).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6263
https://github.com/hail-is/hail/pull/6263:570,Testability,test,test,570,"Had this refractory Dataproc failure, that kind-of pointed to serialization errors, but which @tpoterba clearly saw wasn't due to serialization, as a test in which the HadoopFS class was explicitly serialized and deserialized succeeded. The problem appeared to be in something affecting sparkContext's ability to broadcast, as even the standard SerializableHadoopConfiguration would appear null in map-reduce operations. I therefore created a clean-slate branch from master, and have issued this here. It passes all tests, including a local reproduction of the Dataproc test, by spinning up 1 spark master, 2 workers, and passing initializing hail with master=spark-master:7077 (thanks @cseed).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6263
https://github.com/hail-is/hail/pull/6263:104,Usability,clear,clearly,104,"Had this refractory Dataproc failure, that kind-of pointed to serialization errors, but which @tpoterba clearly saw wasn't due to serialization, as a test in which the HadoopFS class was explicitly serialized and deserialized succeeded. The problem appeared to be in something affecting sparkContext's ability to broadcast, as even the standard SerializableHadoopConfiguration would appear null in map-reduce operations. I therefore created a clean-slate branch from master, and have issued this here. It passes all tests, including a local reproduction of the Dataproc test, by spinning up 1 spark master, 2 workers, and passing initializing hail with master=spark-master:7077 (thanks @cseed).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6263
https://github.com/hail-is/hail/pull/6266:18,Security,expose,exposed,18,"This currently is exposed in `read_table` and `read_matrix_table` as; undocumented optional parameters `_intervals` and `_filter_intervals`; which takes a list of python `Interval`s that are used either as; a filter or a repartition. This adds an `IndexedRVDSpec` as the primary container format for; indexed data, and increments the file version to 1.1.0. One index file is written per partition. For matrix tables, the offset; to a particular key for the entries is stored in the `entries_offset`; field of the annotation that an index may contain. We use the new; `IndexSpec` to retrive the appropriate offset from the index so that we; can seek to the proper offset in the partition. When writing data with a blocked spec (like the default) we use virtual; index offsets similar to tabix. The high 48 bits are used to indicate; the file offset to the start of a block, and the low 16 bits are used to; indicate the offset from the start of the (possibly decompressed) block. cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6266
https://github.com/hail-is/hail/pull/6266:70,Usability,undo,undocumented,70,"This currently is exposed in `read_table` and `read_matrix_table` as; undocumented optional parameters `_intervals` and `_filter_intervals`; which takes a list of python `Interval`s that are used either as; a filter or a repartition. This adds an `IndexedRVDSpec` as the primary container format for; indexed data, and increments the file version to 1.1.0. One index file is written per partition. For matrix tables, the offset; to a particular key for the entries is stored in the `entries_offset`; field of the annotation that an index may contain. We use the new; `IndexSpec` to retrive the appropriate offset from the index so that we; can seek to the proper offset in the partition. When writing data with a blocked spec (like the default) we use virtual; index offsets similar to tabix. The high 48 bits are used to indicate; the file offset to the start of a block, and the low 16 bits are used to; indicate the offset from the start of the (possibly decompressed) block. cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6266
https://github.com/hail-is/hail/pull/6267:142,Availability,error,error,142,"This was causing out-of-order keys to exist and be used in #6223. This doesn't fix that issue specifically, but it will now throw the correct error (and prevent incorrect tables/matrix tables from being written out).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6267
https://github.com/hail-is/hail/pull/6268:102,Availability,Error,Error,102,"Old State Diagram:; Created -> Ready -> Complete; Cancelled. New State Diagram:; Pending -> Ready -> (Error, Running -> (Failed, Success)); Cancelled",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6268
https://github.com/hail-is/hail/pull/6269:105,Integrability,wrap,wrapper,105,Leaves RVDPartitioner.broadcast and HailContext.hadoopConfBc untouched. Backend.broadcast is just a thin wrapper around SparkContext.broadcast for the SparkBackend case.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6269
https://github.com/hail-is/hail/pull/6274:299,Testability,test,tested,299,- Added `gsa_key_file` which will mount a specified file to `/gsa-key/privateData`. Also looks at `HAIL_PIPELINE_GSA_KEY_FILE` if `gsa_key_file` is not specified.; - Added `extra_docker_run_flags`. Also looks at `HAIL_PIPELINE_EXTRA_DOCKER_RUN_FLAGS` if `extra_docker_run_flags` is not specified. I tested this by hand on my local computer. FYI: @cseed and @konradjk,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6274
https://github.com/hail-is/hail/pull/6278:220,Safety,redund,redundancy,220,"These will be helpful when we support searching based on attributes in the Batch UI. Maybe calling it attributes would be better than tags to be consistent with k8s and batch? Also, when making this PR, I don't love the redundancy of the name argument on init and the name method for Tasks. I propose getting rid of the name method. Thoughts? . Stacked on #6277",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6278
https://github.com/hail-is/hail/pull/6280:28,Deployability,patch,patch,28,"This is my original `which` patch from almost a year ago repurposed for hailctl. I have been running this locally for cloudtools. Uses `shutil.which` to find `chromium` or `chromium-browser`, falling back to the macOS default if they cannot be found.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6280
https://github.com/hail-is/hail/issues/6282:47,Modifiability,config,config,47,the parallel header reading doesn't change the config settings.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6282
https://github.com/hail-is/hail/pull/6283:82,Deployability,deploy,deployed,82,"The hail-vdc-sa-key was (temporarily) used by apiserver, which is no longer being deployed. When it comes back, it should either have its own service account (mounted in the standard location) or, if @akotlar user isolation stuff is ready, not use the Hadoop connector. FYI @konradjk when this goes I'll push a new version to Docker Hub.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6283
https://github.com/hail-is/hail/pull/6288:77,Integrability,rout,routes,77,@cseed I didn't know what to do with healthcheck. I think we either need two routes or keep it at `/healthcheck`,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6288
https://github.com/hail-is/hail/pull/6289:21,Deployability,update,updated,21,"The website will get updated when we deploy a PyPI version. This seems; like the best approach for now -- we don't want new functions hanging; around that people can't use. That's caused trouble in the past. Also, it seems that from ci to ci2, we've started deploying the docs; by full revision, rather than short revision. The makefile reflects; that.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6289
https://github.com/hail-is/hail/pull/6289:37,Deployability,deploy,deploy,37,"The website will get updated when we deploy a PyPI version. This seems; like the best approach for now -- we don't want new functions hanging; around that people can't use. That's caused trouble in the past. Also, it seems that from ci to ci2, we've started deploying the docs; by full revision, rather than short revision. The makefile reflects; that.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6289
https://github.com/hail-is/hail/pull/6289:258,Deployability,deploy,deploying,258,"The website will get updated when we deploy a PyPI version. This seems; like the best approach for now -- we don't want new functions hanging; around that people can't use. That's caused trouble in the past. Also, it seems that from ci to ci2, we've started deploying the docs; by full revision, rather than short revision. The makefile reflects; that.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6289
https://github.com/hail-is/hail/pull/6292:295,Safety,unsafe,unsafe-api-when-compiling-with-sbt,295,"I asked in an SO post how to make SBT work the right way but until then we should disable fatal `-Werror` on Javac (build.sbt didn't even have the right javac option, my bad). Pending SO question asking for help: https://stackoverflow.com/questions/56495453/how-do-i-suppress-warnings-about-the-unsafe-api-when-compiling-with-sbt. Also fix two warnings I saw in SBT but not gradle.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6292
https://github.com/hail-is/hail/pull/6293:14,Deployability,install,install,14,"This broke my install, which does not use Conda. I have no `pip`, just `pip3`. cc: @chrisvittal you represent the other major environment, does this break you?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6293
https://github.com/hail-is/hail/issues/6295:87,Availability,Error,Error,87,"See here:; https://hail.zulipchat.com/#narrow/stream/123010-Hail-0.2E2.20support/topic/Error.20when.20writing.20HailTable.2E. ```; logger.info('Read MatrixTable.'); mt = hl.read_matrix_table('path'). logger.info('Calculate median x in each group2.'); mt = mt.group_cols_by('group1', 'group2').aggregate(x = hl.median(hl.agg.collect(mt.x))). logger.info('Calculate mean x in group1.'); mt = mt.group_cols_by('group1').aggregate(x_stats = hl.agg.stats(mt.x)). logger.info('Calculate relative x.'); mt = mt.annotate_entries(x = mt.x_stats.mean); mt = mt.annotate_rows(row_sum = hl.agg.sum(mt.x)); mt = mt.select_entries(rx = mt.x/mt.row_sum). #----; logger.info('Export as HailTable.'); ht = mt.entries(); ht = ht.drop('row_sum', 'gene_id'). ht.export('path'); ```. Can write the MatrixTable before entries, but not the HailTable after",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6295
https://github.com/hail-is/hail/issues/6295:131,Testability,log,logger,131,"See here:; https://hail.zulipchat.com/#narrow/stream/123010-Hail-0.2E2.20support/topic/Error.20when.20writing.20HailTable.2E. ```; logger.info('Read MatrixTable.'); mt = hl.read_matrix_table('path'). logger.info('Calculate median x in each group2.'); mt = mt.group_cols_by('group1', 'group2').aggregate(x = hl.median(hl.agg.collect(mt.x))). logger.info('Calculate mean x in group1.'); mt = mt.group_cols_by('group1').aggregate(x_stats = hl.agg.stats(mt.x)). logger.info('Calculate relative x.'); mt = mt.annotate_entries(x = mt.x_stats.mean); mt = mt.annotate_rows(row_sum = hl.agg.sum(mt.x)); mt = mt.select_entries(rx = mt.x/mt.row_sum). #----; logger.info('Export as HailTable.'); ht = mt.entries(); ht = ht.drop('row_sum', 'gene_id'). ht.export('path'); ```. Can write the MatrixTable before entries, but not the HailTable after",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6295
https://github.com/hail-is/hail/issues/6295:200,Testability,log,logger,200,"See here:; https://hail.zulipchat.com/#narrow/stream/123010-Hail-0.2E2.20support/topic/Error.20when.20writing.20HailTable.2E. ```; logger.info('Read MatrixTable.'); mt = hl.read_matrix_table('path'). logger.info('Calculate median x in each group2.'); mt = mt.group_cols_by('group1', 'group2').aggregate(x = hl.median(hl.agg.collect(mt.x))). logger.info('Calculate mean x in group1.'); mt = mt.group_cols_by('group1').aggregate(x_stats = hl.agg.stats(mt.x)). logger.info('Calculate relative x.'); mt = mt.annotate_entries(x = mt.x_stats.mean); mt = mt.annotate_rows(row_sum = hl.agg.sum(mt.x)); mt = mt.select_entries(rx = mt.x/mt.row_sum). #----; logger.info('Export as HailTable.'); ht = mt.entries(); ht = ht.drop('row_sum', 'gene_id'). ht.export('path'); ```. Can write the MatrixTable before entries, but not the HailTable after",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6295
https://github.com/hail-is/hail/issues/6295:341,Testability,log,logger,341,"See here:; https://hail.zulipchat.com/#narrow/stream/123010-Hail-0.2E2.20support/topic/Error.20when.20writing.20HailTable.2E. ```; logger.info('Read MatrixTable.'); mt = hl.read_matrix_table('path'). logger.info('Calculate median x in each group2.'); mt = mt.group_cols_by('group1', 'group2').aggregate(x = hl.median(hl.agg.collect(mt.x))). logger.info('Calculate mean x in group1.'); mt = mt.group_cols_by('group1').aggregate(x_stats = hl.agg.stats(mt.x)). logger.info('Calculate relative x.'); mt = mt.annotate_entries(x = mt.x_stats.mean); mt = mt.annotate_rows(row_sum = hl.agg.sum(mt.x)); mt = mt.select_entries(rx = mt.x/mt.row_sum). #----; logger.info('Export as HailTable.'); ht = mt.entries(); ht = ht.drop('row_sum', 'gene_id'). ht.export('path'); ```. Can write the MatrixTable before entries, but not the HailTable after",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6295
https://github.com/hail-is/hail/issues/6295:458,Testability,log,logger,458,"See here:; https://hail.zulipchat.com/#narrow/stream/123010-Hail-0.2E2.20support/topic/Error.20when.20writing.20HailTable.2E. ```; logger.info('Read MatrixTable.'); mt = hl.read_matrix_table('path'). logger.info('Calculate median x in each group2.'); mt = mt.group_cols_by('group1', 'group2').aggregate(x = hl.median(hl.agg.collect(mt.x))). logger.info('Calculate mean x in group1.'); mt = mt.group_cols_by('group1').aggregate(x_stats = hl.agg.stats(mt.x)). logger.info('Calculate relative x.'); mt = mt.annotate_entries(x = mt.x_stats.mean); mt = mt.annotate_rows(row_sum = hl.agg.sum(mt.x)); mt = mt.select_entries(rx = mt.x/mt.row_sum). #----; logger.info('Export as HailTable.'); ht = mt.entries(); ht = ht.drop('row_sum', 'gene_id'). ht.export('path'); ```. Can write the MatrixTable before entries, but not the HailTable after",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6295
https://github.com/hail-is/hail/issues/6295:647,Testability,log,logger,647,"See here:; https://hail.zulipchat.com/#narrow/stream/123010-Hail-0.2E2.20support/topic/Error.20when.20writing.20HailTable.2E. ```; logger.info('Read MatrixTable.'); mt = hl.read_matrix_table('path'). logger.info('Calculate median x in each group2.'); mt = mt.group_cols_by('group1', 'group2').aggregate(x = hl.median(hl.agg.collect(mt.x))). logger.info('Calculate mean x in group1.'); mt = mt.group_cols_by('group1').aggregate(x_stats = hl.agg.stats(mt.x)). logger.info('Calculate relative x.'); mt = mt.annotate_entries(x = mt.x_stats.mean); mt = mt.annotate_rows(row_sum = hl.agg.sum(mt.x)); mt = mt.select_entries(rx = mt.x/mt.row_sum). #----; logger.info('Export as HailTable.'); ht = mt.entries(); ht = ht.drop('row_sum', 'gene_id'). ht.export('path'); ```. Can write the MatrixTable before entries, but not the HailTable after",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6295
https://github.com/hail-is/hail/pull/6297:15,Testability,test,tested,15,Also fixed and tested `hailctl dataproc modify`,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6297
https://github.com/hail-is/hail/issues/6299:356,Availability,error,errors,356,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: ; https://discuss.hail.is/. Please include the full Hail version and as much detail as possible. -----------------------------------------------------------------------------. I'm working through the GWAS tutorial and getting some strange errors with two different functions. On the 4th line calling hl.import_vcf('data/1kg.vcf.bgz').write('data/1kg.mt', overwrite=True) I'm getting the following error:. ```; File ""gwas_tutorial.py"", line 12, in <module>; hl.import_vcf('data/1kg.vcf.bgz').write('data/1kg.mt',overwrite=True); File ""</Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/decorator.py:decorator-gen-946>"", line 2, in write; File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/hail/typecheck/check.py"", line 561, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/hail/matrixtable.py"", line 2494, in write; Env.backend().execute(MatrixWrite(self._mir, writer)); File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/hail/backend/backend.py"", line 106, in execute; result = json.loads(Env.hail().backend.spark.SparkBackend.executeJSON(self._to_java_ir(ir))); File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1257, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/hail/utils/java.py"", line 240, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: ScalaSigParserError: Unexpected failure. Java stack trace:; org.json4s.scalap.ScalaSigParserError: Unexpected failure; 	at org.json4s.scalap.Rules$$anonfun$expect$1.apply(Rules.scala:73); 	at org.json4s.scalap.scalasig.ClassFileParser$.parse(ClassFileParser.scala:95); 	at org.json4s.reflect",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6299
https://github.com/hail-is/hail/issues/6299:514,Availability,error,error,514,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: ; https://discuss.hail.is/. Please include the full Hail version and as much detail as possible. -----------------------------------------------------------------------------. I'm working through the GWAS tutorial and getting some strange errors with two different functions. On the 4th line calling hl.import_vcf('data/1kg.vcf.bgz').write('data/1kg.mt', overwrite=True) I'm getting the following error:. ```; File ""gwas_tutorial.py"", line 12, in <module>; hl.import_vcf('data/1kg.vcf.bgz').write('data/1kg.mt',overwrite=True); File ""</Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/decorator.py:decorator-gen-946>"", line 2, in write; File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/hail/typecheck/check.py"", line 561, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/hail/matrixtable.py"", line 2494, in write; Env.backend().execute(MatrixWrite(self._mir, writer)); File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/hail/backend/backend.py"", line 106, in execute; result = json.loads(Env.hail().backend.spark.SparkBackend.executeJSON(self._to_java_ir(ir))); File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1257, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/hail/utils/java.py"", line 240, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: ScalaSigParserError: Unexpected failure. Java stack trace:; org.json4s.scalap.ScalaSigParserError: Unexpected failure; 	at org.json4s.scalap.Rules$$anonfun$expect$1.apply(Rules.scala:73); 	at org.json4s.scalap.scalasig.ClassFileParser$.parse(ClassFileParser.scala:95); 	at org.json4s.reflect",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6299
https://github.com/hail-is/hail/issues/6299:1606,Availability,Error,Error,1606,"'data/1kg.mt',overwrite=True); File ""</Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/decorator.py:decorator-gen-946>"", line 2, in write; File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/hail/typecheck/check.py"", line 561, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/hail/matrixtable.py"", line 2494, in write; Env.backend().execute(MatrixWrite(self._mir, writer)); File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/hail/backend/backend.py"", line 106, in execute; result = json.loads(Env.hail().backend.spark.SparkBackend.executeJSON(self._to_java_ir(ir))); File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1257, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/hail/utils/java.py"", line 240, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: ScalaSigParserError: Unexpected failure. Java stack trace:; org.json4s.scalap.ScalaSigParserError: Unexpected failure; 	at org.json4s.scalap.Rules$$anonfun$expect$1.apply(Rules.scala:73); 	at org.json4s.scalap.scalasig.ClassFileParser$.parse(ClassFileParser.scala:95); 	at org.json4s.reflect.ScalaSigReader$.parseClassFileFromByteCode(ScalaSigReader.scala:178); 	at org.json4s.reflect.ScalaSigReader$.findScalaSig(ScalaSigReader.scala:172); 	at org.json4s.reflect.ScalaSigReader$.findClass(ScalaSigReader.scala:53); 	at org.json4s.reflect.ScalaSigReader$.org$json4s$reflect$ScalaSigReader$$findField(ScalaSigReader.scala:100); 	at org.json4s.reflect.ScalaSigReader$.org$json4s$reflect$ScalaSigReader$$read$1(ScalaSigReader.scala:45); 	at org.json4s.reflect.ScalaSigReader$.readField(ScalaSigReader.scala:49); 	at org.json4s.reflect.Reflector$ClassDescriptorBuilder$$anonfun$3.apply(Reflector.scala:69); 	a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6299
https://github.com/hail-is/hail/issues/6299:1742,Availability,failure,failure,1742,"tor.py:decorator-gen-946>"", line 2, in write; File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/hail/typecheck/check.py"", line 561, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/hail/matrixtable.py"", line 2494, in write; Env.backend().execute(MatrixWrite(self._mir, writer)); File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/hail/backend/backend.py"", line 106, in execute; result = json.loads(Env.hail().backend.spark.SparkBackend.executeJSON(self._to_java_ir(ir))); File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1257, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/hail/utils/java.py"", line 240, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: ScalaSigParserError: Unexpected failure. Java stack trace:; org.json4s.scalap.ScalaSigParserError: Unexpected failure; 	at org.json4s.scalap.Rules$$anonfun$expect$1.apply(Rules.scala:73); 	at org.json4s.scalap.scalasig.ClassFileParser$.parse(ClassFileParser.scala:95); 	at org.json4s.reflect.ScalaSigReader$.parseClassFileFromByteCode(ScalaSigReader.scala:178); 	at org.json4s.reflect.ScalaSigReader$.findScalaSig(ScalaSigReader.scala:172); 	at org.json4s.reflect.ScalaSigReader$.findClass(ScalaSigReader.scala:53); 	at org.json4s.reflect.ScalaSigReader$.org$json4s$reflect$ScalaSigReader$$findField(ScalaSigReader.scala:100); 	at org.json4s.reflect.ScalaSigReader$.org$json4s$reflect$ScalaSigReader$$read$1(ScalaSigReader.scala:45); 	at org.json4s.reflect.ScalaSigReader$.readField(ScalaSigReader.scala:49); 	at org.json4s.reflect.Reflector$ClassDescriptorBuilder$$anonfun$3.apply(Reflector.scala:69); 	at org.json4s.reflect.Reflector$ClassDescriptorBuilder$$anonfun$3.apply(Reflector.scala:68); 	at scala.collect",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6299
https://github.com/hail-is/hail/issues/6299:1820,Availability,failure,failure,1820,"1/lib/python3.7/site-packages/hail/typecheck/check.py"", line 561, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/hail/matrixtable.py"", line 2494, in write; Env.backend().execute(MatrixWrite(self._mir, writer)); File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/hail/backend/backend.py"", line 106, in execute; result = json.loads(Env.hail().backend.spark.SparkBackend.executeJSON(self._to_java_ir(ir))); File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1257, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/hail/utils/java.py"", line 240, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: ScalaSigParserError: Unexpected failure. Java stack trace:; org.json4s.scalap.ScalaSigParserError: Unexpected failure; 	at org.json4s.scalap.Rules$$anonfun$expect$1.apply(Rules.scala:73); 	at org.json4s.scalap.scalasig.ClassFileParser$.parse(ClassFileParser.scala:95); 	at org.json4s.reflect.ScalaSigReader$.parseClassFileFromByteCode(ScalaSigReader.scala:178); 	at org.json4s.reflect.ScalaSigReader$.findScalaSig(ScalaSigReader.scala:172); 	at org.json4s.reflect.ScalaSigReader$.findClass(ScalaSigReader.scala:53); 	at org.json4s.reflect.ScalaSigReader$.org$json4s$reflect$ScalaSigReader$$findField(ScalaSigReader.scala:100); 	at org.json4s.reflect.ScalaSigReader$.org$json4s$reflect$ScalaSigReader$$read$1(ScalaSigReader.scala:45); 	at org.json4s.reflect.ScalaSigReader$.readField(ScalaSigReader.scala:49); 	at org.json4s.reflect.Reflector$ClassDescriptorBuilder$$anonfun$3.apply(Reflector.scala:69); 	at org.json4s.reflect.Reflector$ClassDescriptorBuilder$$anonfun$3.apply(Reflector.scala:68); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6299
https://github.com/hail-is/hail/issues/6299:6556,Availability,Error,Error,6556,"SparkBackend.scala:75); 	at is.hail.backend.spark.SparkBackend$.executeJSON(SparkBackend.scala:18); 	at is.hail.backend.spark.SparkBackend.executeJSON(SparkBackend.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:483); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:745). Hail version: 0.2.14-8dcb6722c72a; Error summary: ScalaSigParserError: Unexpected failure; ```; If that does execute, which it does sometimes (unclear why, I don't make any code changes), I get an error from mt = hl.read_matrix_table('data/1kg.mt'):. ```; [Stage 1:> (0 + 2) / 2]2019-06-10 14:40:22 Hail: INFO: Coerced sorted dataset; [Stage 2:> (0 + 2) / 2]2019-06-10 14:40:25 Hail: INFO: wrote matrix table with 10961 rows and 284 columns in 2 partitions to data/1kg.mt; Traceback (most recent call last):; File ""gwas_tutorial.py"", line 13, in <module>; mt = hl.read_matrix_table('data/1kg.mt'); File ""</Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/decorator.py:decorator-gen-1136>"", line 2, in read_matrix_table; File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/hail/typecheck/check.py"", line 561, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/hail/methods/impex.py"", line 1708, in read_matrix_table; return MatrixTable(MatrixRead(MatrixNativeReader(path), _drop_cols, _drop_rows)); File ""/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6299
https://github.com/hail-is/hail/issues/6299:6603,Availability,failure,failure,6603,"SparkBackend.scala:75); 	at is.hail.backend.spark.SparkBackend$.executeJSON(SparkBackend.scala:18); 	at is.hail.backend.spark.SparkBackend.executeJSON(SparkBackend.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:483); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:745). Hail version: 0.2.14-8dcb6722c72a; Error summary: ScalaSigParserError: Unexpected failure; ```; If that does execute, which it does sometimes (unclear why, I don't make any code changes), I get an error from mt = hl.read_matrix_table('data/1kg.mt'):. ```; [Stage 1:> (0 + 2) / 2]2019-06-10 14:40:22 Hail: INFO: Coerced sorted dataset; [Stage 2:> (0 + 2) / 2]2019-06-10 14:40:25 Hail: INFO: wrote matrix table with 10961 rows and 284 columns in 2 partitions to data/1kg.mt; Traceback (most recent call last):; File ""gwas_tutorial.py"", line 13, in <module>; mt = hl.read_matrix_table('data/1kg.mt'); File ""</Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/decorator.py:decorator-gen-1136>"", line 2, in read_matrix_table; File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/hail/typecheck/check.py"", line 561, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/hail/methods/impex.py"", line 1708, in read_matrix_table; return MatrixTable(MatrixRead(MatrixNativeReader(path), _drop_cols, _drop_rows)); File ""/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6299
https://github.com/hail-is/hail/issues/6299:6718,Availability,error,error,6718,"SparkBackend.scala:75); 	at is.hail.backend.spark.SparkBackend$.executeJSON(SparkBackend.scala:18); 	at is.hail.backend.spark.SparkBackend.executeJSON(SparkBackend.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:483); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:745). Hail version: 0.2.14-8dcb6722c72a; Error summary: ScalaSigParserError: Unexpected failure; ```; If that does execute, which it does sometimes (unclear why, I don't make any code changes), I get an error from mt = hl.read_matrix_table('data/1kg.mt'):. ```; [Stage 1:> (0 + 2) / 2]2019-06-10 14:40:22 Hail: INFO: Coerced sorted dataset; [Stage 2:> (0 + 2) / 2]2019-06-10 14:40:25 Hail: INFO: wrote matrix table with 10961 rows and 284 columns in 2 partitions to data/1kg.mt; Traceback (most recent call last):; File ""gwas_tutorial.py"", line 13, in <module>; mt = hl.read_matrix_table('data/1kg.mt'); File ""</Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/decorator.py:decorator-gen-1136>"", line 2, in read_matrix_table; File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/hail/typecheck/check.py"", line 561, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/hail/methods/impex.py"", line 1708, in read_matrix_table; return MatrixTable(MatrixRead(MatrixNativeReader(path), _drop_cols, _drop_rows)); File ""/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6299
https://github.com/hail-is/hail/issues/6299:8873,Availability,Error,Error,8873," typ; self._compute_type(); File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/hail/ir/matrix_ir.py"", line 40, in _compute_type; self._type = Env.backend().matrix_type(self); File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/hail/backend/backend.py"", line 121, in matrix_type; jir = self._to_java_ir(mir); File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/hail/backend/backend.py"", line 102, in _to_java_ir; ir._jir = ir.parse(r(ir), ir_map=r.jirs); File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/hail/ir/base_ir.py"", line 163, in parse; return Env.hail().expr.ir.IRParser.parse_matrix_ir(code, ref_map, ir_map); File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1257, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/hail/utils/java.py"", line 240, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: MatchError: 17 (of class java.lang.Integer). Java stack trace:; scala.MatchError: 17 (of class java.lang.Integer); 	at org.json4s.scalap.scalasig.ClassFileParser$$anonfun$37.apply(ClassFileParser.scala:119); 	at org.json4s.scalap.scalasig.ClassFileParser$$anonfun$37.apply(ClassFileParser.scala:119); 	at org.json4s.scalap.Rule$$anonfun$flatMap$1.apply(Rule.scala:33); 	at org.json4s.scalap.Rule$$anonfun$flatMap$1.apply(Rule.scala:32); 	at org.json4s.scalap.Rule$$anonfun$mapResult$1.apply(Rule.scala:43); 	at org.json4s.scalap.Rule$$anonfun$mapResult$1.apply(Rule.scala:43); 	at org.json4s.scalap.Rules$DefaultRule.apply(Rules.scala:67); 	at org.json4s.scalap.Rules$DefaultRule.apply(Rules.scala:65); 	at org.json4s.scalap.StateRules$class.rep$2(Rules.scala:137); 	at org.json4s.scalap.StateRules$$anonfun$repeatUntil$1.apply(Rules.scala:143); 	at org.json4s.scalap.StateRules$$anonfun$repeatUntil$1.app",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6299
https://github.com/hail-is/hail/issues/6299:18709,Availability,Error,Error,18709,"action$$anonfun$extract$6.apply(Extraction.scala:392); 	at org.json4s.Extraction$.customOrElse(Extraction.scala:606); 	at org.json4s.Extraction$.extract(Extraction.scala:392); 	at org.json4s.Extraction$.extract(Extraction.scala:39); 	at org.json4s.ExtractableJsonAstNode.extract(ExtractableJsonAstNode.scala:21); 	at org.json4s.jackson.Serialization$.read(Serialization.scala:50); 	at org.json4s.Serialization$class.read(Serialization.scala:30); 	at org.json4s.jackson.Serialization$.read(Serialization.scala:17); 	at is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:1117); 	at is.hail.expr.ir.IRParser$.matrix_ir(Parser.scala:1053); 	at is.hail.expr.ir.IRParser$$anonfun$parse_matrix_ir$2.apply(Parser.scala:1269); 	at is.hail.expr.ir.IRParser$$anonfun$parse_matrix_ir$2.apply(Parser.scala:1269); 	at is.hail.expr.ir.IRParser$.parse(Parser.scala:1253); 	at is.hail.expr.ir.IRParser$.parse_matrix_ir(Parser.scala:1269); 	at is.hail.expr.ir.IRParser$.parse_matrix_ir(Parser.scala:1268); 	at is.hail.expr.ir.IRParser.parse_matrix_ir(Parser.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:483); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:745). Hail version: 0.2.14-8dcb6722c72a; Error summary: MatchError: 17 (of class java.lang.Integer); ```. Any ideas on what is causing this? It seems like something in Spark itself, but I can't trace it down. I'm running Apache Spark version 2.4.1",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6299
https://github.com/hail-is/hail/issues/6299:18871,Availability,down,down,18871,"action$$anonfun$extract$6.apply(Extraction.scala:392); 	at org.json4s.Extraction$.customOrElse(Extraction.scala:606); 	at org.json4s.Extraction$.extract(Extraction.scala:392); 	at org.json4s.Extraction$.extract(Extraction.scala:39); 	at org.json4s.ExtractableJsonAstNode.extract(ExtractableJsonAstNode.scala:21); 	at org.json4s.jackson.Serialization$.read(Serialization.scala:50); 	at org.json4s.Serialization$class.read(Serialization.scala:30); 	at org.json4s.jackson.Serialization$.read(Serialization.scala:17); 	at is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:1117); 	at is.hail.expr.ir.IRParser$.matrix_ir(Parser.scala:1053); 	at is.hail.expr.ir.IRParser$$anonfun$parse_matrix_ir$2.apply(Parser.scala:1269); 	at is.hail.expr.ir.IRParser$$anonfun$parse_matrix_ir$2.apply(Parser.scala:1269); 	at is.hail.expr.ir.IRParser$.parse(Parser.scala:1253); 	at is.hail.expr.ir.IRParser$.parse_matrix_ir(Parser.scala:1269); 	at is.hail.expr.ir.IRParser$.parse_matrix_ir(Parser.scala:1268); 	at is.hail.expr.ir.IRParser.parse_matrix_ir(Parser.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:483); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:745). Hail version: 0.2.14-8dcb6722c72a; Error summary: MatchError: 17 (of class java.lang.Integer); ```. Any ideas on what is causing this? It seems like something in Spark itself, but I can't trace it down. I'm running Apache Spark version 2.4.1",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6299
https://github.com/hail-is/hail/issues/6299:881,Integrability,wrap,wrapper,881,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: ; https://discuss.hail.is/. Please include the full Hail version and as much detail as possible. -----------------------------------------------------------------------------. I'm working through the GWAS tutorial and getting some strange errors with two different functions. On the 4th line calling hl.import_vcf('data/1kg.vcf.bgz').write('data/1kg.mt', overwrite=True) I'm getting the following error:. ```; File ""gwas_tutorial.py"", line 12, in <module>; hl.import_vcf('data/1kg.vcf.bgz').write('data/1kg.mt',overwrite=True); File ""</Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/decorator.py:decorator-gen-946>"", line 2, in write; File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/hail/typecheck/check.py"", line 561, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/hail/matrixtable.py"", line 2494, in write; Env.backend().execute(MatrixWrite(self._mir, writer)); File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/hail/backend/backend.py"", line 106, in execute; result = json.loads(Env.hail().backend.spark.SparkBackend.executeJSON(self._to_java_ir(ir))); File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1257, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/hail/utils/java.py"", line 240, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: ScalaSigParserError: Unexpected failure. Java stack trace:; org.json4s.scalap.ScalaSigParserError: Unexpected failure; 	at org.json4s.scalap.Rules$$anonfun$expect$1.apply(Rules.scala:73); 	at org.json4s.scalap.scalasig.ClassFileParser$.parse(ClassFileParser.scala:95); 	at org.json4s.reflect",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6299
https://github.com/hail-is/hail/issues/6299:5197,Integrability,Wrap,WrappedMatrixWriter,5197,; 	at org.json4s.Extraction$.internalDecomposeWithBuilder(Extraction.scala:228); 	at org.json4s.Extraction$.decomposeWithBuilder(Extraction.scala:64); 	at org.json4s.Extraction$.decompose(Extraction.scala:242); 	at org.json4s.jackson.Serialization$.write(Serialization.scala:27); 	at is.hail.variant.RelationalSpec$$anonfun$write$1.apply(MatrixTable.scala:85); 	at is.hail.variant.RelationalSpec$$anonfun$write$1.apply(MatrixTable.scala:84); 	at is.hail.utils.package$.using(package.scala:594); 	at is.hail.utils.richUtils.RichHadoopConfiguration$.writeTextFile$extension(RichHadoopConfiguration.scala:281); 	at is.hail.variant.RelationalSpec.write(MatrixTable.scala:84); 	at is.hail.expr.ir.MatrixValue.writeGlobals(MatrixValue.scala:91); 	at is.hail.expr.ir.MatrixValue.is$hail$expr$ir$MatrixValue$$finalizeWrite(MatrixValue.scala:104); 	at is.hail.expr.ir.MatrixValue.write(MatrixValue.scala:184); 	at is.hail.expr.ir.MatrixNativeWriter.apply(MatrixWriter.scala:36); 	at is.hail.expr.ir.WrappedMatrixWriter.apply(MatrixWriter.scala:23); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:754); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:87); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$1.apply(CompileAndEvaluate.scala:31); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:31); 	at is.hail.backend.spark.SparkBackend$.execute(SparkBackend.scala:75); 	at is.hail.backend.spark.SparkBackend$.executeJSON(SparkBackend.scala:18); 	at is.hail.backend.spark.SparkBackend.executeJSON(SparkBackend.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:483); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.i,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6299
https://github.com/hail-is/hail/issues/6299:7368,Integrability,wrap,wrapper,7368,"79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:745). Hail version: 0.2.14-8dcb6722c72a; Error summary: ScalaSigParserError: Unexpected failure; ```; If that does execute, which it does sometimes (unclear why, I don't make any code changes), I get an error from mt = hl.read_matrix_table('data/1kg.mt'):. ```; [Stage 1:> (0 + 2) / 2]2019-06-10 14:40:22 Hail: INFO: Coerced sorted dataset; [Stage 2:> (0 + 2) / 2]2019-06-10 14:40:25 Hail: INFO: wrote matrix table with 10961 rows and 284 columns in 2 partitions to data/1kg.mt; Traceback (most recent call last):; File ""gwas_tutorial.py"", line 13, in <module>; mt = hl.read_matrix_table('data/1kg.mt'); File ""</Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/decorator.py:decorator-gen-1136>"", line 2, in read_matrix_table; File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/hail/typecheck/check.py"", line 561, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/hail/methods/impex.py"", line 1708, in read_matrix_table; return MatrixTable(MatrixRead(MatrixNativeReader(path), _drop_cols, _drop_rows)); File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/hail/matrixtable.py"", line 558, in __init__; self._type = self._mir.typ; File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/hail/ir/base_ir.py"", line 158, in typ; self._compute_type(); File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/hail/ir/matrix_ir.py"", line 40, in _compute_type; self._type = Env.backend().matrix_type(self); File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/hail/backend/backend.py"", line 121, in matrix_type; jir = self._to_java_ir(mir); File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/hail/backend/backend.py"", line 102, in _to_java_ir; ir._jir = ir.parse(r(ir), ir_map=r.jirs); File ""/Users/user",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6299
https://github.com/hail-is/hail/issues/6299:1237,Performance,load,loads,1237,"-------------------------------------------------. I'm working through the GWAS tutorial and getting some strange errors with two different functions. On the 4th line calling hl.import_vcf('data/1kg.vcf.bgz').write('data/1kg.mt', overwrite=True) I'm getting the following error:. ```; File ""gwas_tutorial.py"", line 12, in <module>; hl.import_vcf('data/1kg.vcf.bgz').write('data/1kg.mt',overwrite=True); File ""</Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/decorator.py:decorator-gen-946>"", line 2, in write; File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/hail/typecheck/check.py"", line 561, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/hail/matrixtable.py"", line 2494, in write; Env.backend().execute(MatrixWrite(self._mir, writer)); File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/hail/backend/backend.py"", line 106, in execute; result = json.loads(Env.hail().backend.spark.SparkBackend.executeJSON(self._to_java_ir(ir))); File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1257, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/hail/utils/java.py"", line 240, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: ScalaSigParserError: Unexpected failure. Java stack trace:; org.json4s.scalap.ScalaSigParserError: Unexpected failure; 	at org.json4s.scalap.Rules$$anonfun$expect$1.apply(Rules.scala:73); 	at org.json4s.scalap.scalasig.ClassFileParser$.parse(ClassFileParser.scala:95); 	at org.json4s.reflect.ScalaSigReader$.parseClassFileFromByteCode(ScalaSigReader.scala:178); 	at org.json4s.reflect.ScalaSigReader$.findScalaSig(ScalaSigReader.scala:172); 	at org.json4s.reflect.ScalaSigReader$.findClass(ScalaSigReader.scala:53); 	at org.json4s.r",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6299
https://github.com/hail-is/hail/pull/6304:82,Testability,test,tests,82,Implements a DistributedBackend and uses it to run the subset of the TableIRSuite tests that can be lowered to execute on a backend.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6304
https://github.com/hail-is/hail/pull/6306:359,Availability,error,error,359,"This adds `SpillingCollectIterator` which avoids holding more than 1000 aggregation results in memory at one time. We could do something that [listens for GC events](https://stackoverflow.com/questions/30041332/a-useful-metric-for-determining-when-the-jvm-is-about-to-get-into-memory-gc-trou) and spills data if there's high memory pressure. That seems a bit error prone and hard. How should I pipe the size limit down to TableMapRows? The only workable solution I can think of is a HailContext setting. Maybe I should bite the bullet and respond to memory pressure? Either way this should get Laurent cooking with gas. Spilling ten local files and then reading them in is probably in the noise of timings. 🎉. Master 0.2.14-4da055db5a7b; ```ipython; In [1]: %%time ; ...: ; ...: import hail as hl ; ...: ht = hl.utils.range_table(10000, n_partitions=10000) ; ...: ht = ht.annotate(rank = hl.scan.count())._force_count() ; CPU times: user 1.45 s, sys: 333 ms, total: 1.78 s; Wall time: 24.6 s; In [3]: %%time ; ...: ; ...: import hail as hl ; ...: ht = hl.utils.range_table(1000000, n_partitions=1000) ; ...: ht = ht.annotate(rank = hl.scan.count())._force_count() ; CPU times: user 6.23 ms, sys: 1.96 ms, total: 8.19 ms; Wall time: 1.33 s; ```; This branch:; ```ipython; In [2]: %%time ; ...: ; ...: import hail as hl ; ...: ht = hl.utils.range_table(10000, n_partitions=10000) ; ...: ht = ht.annotate(rank = hl.scan.count())._force_count() ; CPU times: user 1.41 s, sys: 313 ms, total: 1.72 s; Wall time: 25.2 s. In [3]: %%time ; ...: ; ...: import hail as hl ; ...: ht = hl.utils.range_table(1000000, n_partitions=1000) ; ...: ht = ht.annotate(rank = hl.scan.count())._force_count() ; ...: ; ...: ; CPU times: user 4.72 ms, sys: 1.82 ms, total: 6.53 ms; Wall time: 1.41 s; ```. ---. Minor implementation note: I did the rigamarole with `runJob` because I wasn't sure that using `synchronized` in a constructor was kosher and I'm also generally weary of Scala's constructor syntax.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6306
https://github.com/hail-is/hail/pull/6306:414,Availability,down,down,414,"This adds `SpillingCollectIterator` which avoids holding more than 1000 aggregation results in memory at one time. We could do something that [listens for GC events](https://stackoverflow.com/questions/30041332/a-useful-metric-for-determining-when-the-jvm-is-about-to-get-into-memory-gc-trou) and spills data if there's high memory pressure. That seems a bit error prone and hard. How should I pipe the size limit down to TableMapRows? The only workable solution I can think of is a HailContext setting. Maybe I should bite the bullet and respond to memory pressure? Either way this should get Laurent cooking with gas. Spilling ten local files and then reading them in is probably in the noise of timings. 🎉. Master 0.2.14-4da055db5a7b; ```ipython; In [1]: %%time ; ...: ; ...: import hail as hl ; ...: ht = hl.utils.range_table(10000, n_partitions=10000) ; ...: ht = ht.annotate(rank = hl.scan.count())._force_count() ; CPU times: user 1.45 s, sys: 333 ms, total: 1.78 s; Wall time: 24.6 s; In [3]: %%time ; ...: ; ...: import hail as hl ; ...: ht = hl.utils.range_table(1000000, n_partitions=1000) ; ...: ht = ht.annotate(rank = hl.scan.count())._force_count() ; CPU times: user 6.23 ms, sys: 1.96 ms, total: 8.19 ms; Wall time: 1.33 s; ```; This branch:; ```ipython; In [2]: %%time ; ...: ; ...: import hail as hl ; ...: ht = hl.utils.range_table(10000, n_partitions=10000) ; ...: ht = ht.annotate(rank = hl.scan.count())._force_count() ; CPU times: user 1.41 s, sys: 313 ms, total: 1.72 s; Wall time: 25.2 s. In [3]: %%time ; ...: ; ...: import hail as hl ; ...: ht = hl.utils.range_table(1000000, n_partitions=1000) ; ...: ht = ht.annotate(rank = hl.scan.count())._force_count() ; ...: ; ...: ; CPU times: user 4.72 ms, sys: 1.82 ms, total: 6.53 ms; Wall time: 1.41 s; ```. ---. Minor implementation note: I did the rigamarole with `runJob` because I wasn't sure that using `synchronized` in a constructor was kosher and I'm also generally weary of Scala's constructor syntax.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6306
https://github.com/hail-is/hail/pull/6306:1881,Integrability,synchroniz,synchronized,1881,"This adds `SpillingCollectIterator` which avoids holding more than 1000 aggregation results in memory at one time. We could do something that [listens for GC events](https://stackoverflow.com/questions/30041332/a-useful-metric-for-determining-when-the-jvm-is-about-to-get-into-memory-gc-trou) and spills data if there's high memory pressure. That seems a bit error prone and hard. How should I pipe the size limit down to TableMapRows? The only workable solution I can think of is a HailContext setting. Maybe I should bite the bullet and respond to memory pressure? Either way this should get Laurent cooking with gas. Spilling ten local files and then reading them in is probably in the noise of timings. 🎉. Master 0.2.14-4da055db5a7b; ```ipython; In [1]: %%time ; ...: ; ...: import hail as hl ; ...: ht = hl.utils.range_table(10000, n_partitions=10000) ; ...: ht = ht.annotate(rank = hl.scan.count())._force_count() ; CPU times: user 1.45 s, sys: 333 ms, total: 1.78 s; Wall time: 24.6 s; In [3]: %%time ; ...: ; ...: import hail as hl ; ...: ht = hl.utils.range_table(1000000, n_partitions=1000) ; ...: ht = ht.annotate(rank = hl.scan.count())._force_count() ; CPU times: user 6.23 ms, sys: 1.96 ms, total: 8.19 ms; Wall time: 1.33 s; ```; This branch:; ```ipython; In [2]: %%time ; ...: ; ...: import hail as hl ; ...: ht = hl.utils.range_table(10000, n_partitions=10000) ; ...: ht = ht.annotate(rank = hl.scan.count())._force_count() ; CPU times: user 1.41 s, sys: 313 ms, total: 1.72 s; Wall time: 25.2 s. In [3]: %%time ; ...: ; ...: import hail as hl ; ...: ht = hl.utils.range_table(1000000, n_partitions=1000) ; ...: ht = ht.annotate(rank = hl.scan.count())._force_count() ; ...: ; ...: ; CPU times: user 4.72 ms, sys: 1.82 ms, total: 6.53 ms; Wall time: 1.41 s; ```. ---. Minor implementation note: I did the rigamarole with `runJob` because I wasn't sure that using `synchronized` in a constructor was kosher and I'm also generally weary of Scala's constructor syntax.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6306
https://github.com/hail-is/hail/pull/6306:42,Safety,avoid,avoids,42,"This adds `SpillingCollectIterator` which avoids holding more than 1000 aggregation results in memory at one time. We could do something that [listens for GC events](https://stackoverflow.com/questions/30041332/a-useful-metric-for-determining-when-the-jvm-is-about-to-get-into-memory-gc-trou) and spills data if there's high memory pressure. That seems a bit error prone and hard. How should I pipe the size limit down to TableMapRows? The only workable solution I can think of is a HailContext setting. Maybe I should bite the bullet and respond to memory pressure? Either way this should get Laurent cooking with gas. Spilling ten local files and then reading them in is probably in the noise of timings. 🎉. Master 0.2.14-4da055db5a7b; ```ipython; In [1]: %%time ; ...: ; ...: import hail as hl ; ...: ht = hl.utils.range_table(10000, n_partitions=10000) ; ...: ht = ht.annotate(rank = hl.scan.count())._force_count() ; CPU times: user 1.45 s, sys: 333 ms, total: 1.78 s; Wall time: 24.6 s; In [3]: %%time ; ...: ; ...: import hail as hl ; ...: ht = hl.utils.range_table(1000000, n_partitions=1000) ; ...: ht = ht.annotate(rank = hl.scan.count())._force_count() ; CPU times: user 6.23 ms, sys: 1.96 ms, total: 8.19 ms; Wall time: 1.33 s; ```; This branch:; ```ipython; In [2]: %%time ; ...: ; ...: import hail as hl ; ...: ht = hl.utils.range_table(10000, n_partitions=10000) ; ...: ht = ht.annotate(rank = hl.scan.count())._force_count() ; CPU times: user 1.41 s, sys: 313 ms, total: 1.72 s; Wall time: 25.2 s. In [3]: %%time ; ...: ; ...: import hail as hl ; ...: ht = hl.utils.range_table(1000000, n_partitions=1000) ; ...: ht = ht.annotate(rank = hl.scan.count())._force_count() ; ...: ; ...: ; CPU times: user 4.72 ms, sys: 1.82 ms, total: 6.53 ms; Wall time: 1.41 s; ```. ---. Minor implementation note: I did the rigamarole with `runJob` because I wasn't sure that using `synchronized` in a constructor was kosher and I'm also generally weary of Scala's constructor syntax.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6306
https://github.com/hail-is/hail/pull/6308:71,Security,authoriz,authorized,71,"- add ci database with 1 table: authorized_shas; - only start build if authorized author or source_sha is authorized; - form on main page to add authorized sha,; - unauthorized PRs are created, but not tested, and shown as unauthorized in UI",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6308
https://github.com/hail-is/hail/pull/6308:106,Security,authoriz,authorized,106,"- add ci database with 1 table: authorized_shas; - only start build if authorized author or source_sha is authorized; - form on main page to add authorized sha,; - unauthorized PRs are created, but not tested, and shown as unauthorized in UI",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6308
https://github.com/hail-is/hail/pull/6308:145,Security,authoriz,authorized,145,"- add ci database with 1 table: authorized_shas; - only start build if authorized author or source_sha is authorized; - form on main page to add authorized sha,; - unauthorized PRs are created, but not tested, and shown as unauthorized in UI",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6308
https://github.com/hail-is/hail/pull/6308:202,Testability,test,tested,202,"- add ci database with 1 table: authorized_shas; - only start build if authorized author or source_sha is authorized; - form on main page to add authorized sha,; - unauthorized PRs are created, but not tested, and shown as unauthorized in UI",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6308
https://github.com/hail-is/hail/pull/6309:27,Testability,test,tests,27,"there aren't any scorecard tests, and running it in tests is blowing; out Github request limits. FYI @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6309
https://github.com/hail-is/hail/pull/6309:52,Testability,test,tests,52,"there aren't any scorecard tests, and running it in tests is blowing; out Github request limits. FYI @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6309
https://github.com/hail-is/hail/pull/6310:24,Energy Efficiency,monitor,monitoring,24,we're going to focus on monitoring in the cluster; already removed the upload service,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6310
https://github.com/hail-is/hail/pull/6315:33,Testability,test,test,33,"the `resource` directory is `src/test/resources`, not meant for output.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6315
https://github.com/hail-is/hail/pull/6318:36,Deployability,install,installs,36,Adds a new target to make file that installs all dependencies and developer dependencies.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6318
https://github.com/hail-is/hail/pull/6318:49,Integrability,depend,dependencies,49,Adds a new target to make file that installs all dependencies and developer dependencies.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6318
https://github.com/hail-is/hail/pull/6318:76,Integrability,depend,dependencies,76,Adds a new target to make file that installs all dependencies and developer dependencies.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6318
https://github.com/hail-is/hail/pull/6322:9,Modifiability,variab,variables,9,"- Expand variables at definition; - Fix pip definition, allow it to be overwritten; - Copy hail_version and hail_pip_version into hailctl to not break; local PYTHON_PATH settings.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6322
https://github.com/hail-is/hail/pull/6329:54,Testability,test,test,54,Currently I'm PRing so that it goes through the batch test suite.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6329
https://github.com/hail-is/hail/pull/6330:194,Testability,test,test,194,"Couples changes:; - Explicitly convert numpy types to hail types and don't only accept numpy floats. Apparently `np.array_equal` is very relaxed and will coerce between ints and booleans so the test that should have caught this now checks the types too.; - There was a subtle bug when extracting the numpy ndarray values into python. The values were always being extracted in row-major order, regardless of their previous layout in numpy. Changed it so now `row_major` can be `None`, in which case for numpy ndarrays it will use the existing ordering and will default to row major if just entering a python list. Note: row and column major (or in numpy terms, C contiguous and F contiguous) aren't the only options. It can be strided in a bunch of different permutations, but in these cases we read it out of numpy as row major.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6330
https://github.com/hail-is/hail/pull/6331:11,Deployability,deploy,deploy,11,"We need to deploy databases which don't exist. For example, there is a new ci database that needs to be created and secrets added before the create-tables script can run. cc: @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6331
https://github.com/hail-is/hail/pull/6332:126,Availability,avail,available,126,Let me know if you need the context. I don't think it's necessary and will break my interface. I didn't want the job id to be available until the whole batch is submitted and the batch id is available.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6332
https://github.com/hail-is/hail/pull/6332:191,Availability,avail,available,191,Let me know if you need the context. I don't think it's necessary and will break my interface. I didn't want the job id to be available until the whole batch is submitted and the batch id is available.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6332
https://github.com/hail-is/hail/pull/6332:84,Integrability,interface,interface,84,Let me know if you need the context. I don't think it's necessary and will break my interface. I didn't want the job id to be available until the whole batch is submitted and the batch id is available.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6332
https://github.com/hail-is/hail/pull/6333:230,Availability,error,error,230,"This adds `SpillingCollectIterator` which avoids holding more than 1000 aggregation results in memory at one time. We could do something that listens for GC events and spills data if there's high memory pressure. That seems a bit error prone and hard. How should I pipe the size limit down to TableMapRows? I decided to make it a HailContext `flag` which means its not very user-visible, but Laurent can set it for now. In C++ we can design a system that is aware of its memory usage and adjusts memory allocated to scans accordingly. Spilling ten local files and then reading them in is probably in the noise of timings. 🎉. ---. ### Implementation Notes. I had to add two new file operations to the `RichHadoopConfiguration` because I need seekable file input streams. I don't like the names. I'm not sure what to do here. Hadoop really screws us with the seek-ability on compressed streams. The implementation is rather simple, it just maintains an array of the per-partition results. The index of the array corresponds to the partition index. The sparsity of that array is controlled by how often we spill. For an operation with a huge number of partitions that are often spilled (e.g. large number of partitions, each with a lot of data), we may want to use a `Map` instead of an `Array`. The use of `ObjectOutputStream` without a try-catch-finally block is non-standard. I was having trouble seeking to individual classes when I used one ObjectOutputStream to output each partition's array. There were these ""bad header"" messages. This seems to work. I don't close the OOS because I'm going to re-use the underlying output stream on the next partition. We use O(n_spills) files. ---. ### Timings. Master 0.2.14-4da055db5a7b; ```; In [1]: %%time ; ...: ; ...: import hail as hl ; ...: ht = hl.utils.range_table(10000, n_partitions=10000) ; ...: ht = ht.annotate(rank = hl.scan.count())._force_count() ; CPU times: user 1.45 s, sys: 333 ms, total: 1.78 s; Wall time: 24.6 s; In [3]: %%time ; ...: ;",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6333
https://github.com/hail-is/hail/pull/6333:285,Availability,down,down,285,"This adds `SpillingCollectIterator` which avoids holding more than 1000 aggregation results in memory at one time. We could do something that listens for GC events and spills data if there's high memory pressure. That seems a bit error prone and hard. How should I pipe the size limit down to TableMapRows? I decided to make it a HailContext `flag` which means its not very user-visible, but Laurent can set it for now. In C++ we can design a system that is aware of its memory usage and adjusts memory allocated to scans accordingly. Spilling ten local files and then reading them in is probably in the noise of timings. 🎉. ---. ### Implementation Notes. I had to add two new file operations to the `RichHadoopConfiguration` because I need seekable file input streams. I don't like the names. I'm not sure what to do here. Hadoop really screws us with the seek-ability on compressed streams. The implementation is rather simple, it just maintains an array of the per-partition results. The index of the array corresponds to the partition index. The sparsity of that array is controlled by how often we spill. For an operation with a huge number of partitions that are often spilled (e.g. large number of partitions, each with a lot of data), we may want to use a `Map` instead of an `Array`. The use of `ObjectOutputStream` without a try-catch-finally block is non-standard. I was having trouble seeking to individual classes when I used one ObjectOutputStream to output each partition's array. There were these ""bad header"" messages. This seems to work. I don't close the OOS because I'm going to re-use the underlying output stream on the next partition. We use O(n_spills) files. ---. ### Timings. Master 0.2.14-4da055db5a7b; ```; In [1]: %%time ; ...: ; ...: import hail as hl ; ...: ht = hl.utils.range_table(10000, n_partitions=10000) ; ...: ht = ht.annotate(rank = hl.scan.count())._force_count() ; CPU times: user 1.45 s, sys: 333 ms, total: 1.78 s; Wall time: 24.6 s; In [3]: %%time ; ...: ;",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6333
https://github.com/hail-is/hail/pull/6333:503,Energy Efficiency,allocate,allocated,503,"This adds `SpillingCollectIterator` which avoids holding more than 1000 aggregation results in memory at one time. We could do something that listens for GC events and spills data if there's high memory pressure. That seems a bit error prone and hard. How should I pipe the size limit down to TableMapRows? I decided to make it a HailContext `flag` which means its not very user-visible, but Laurent can set it for now. In C++ we can design a system that is aware of its memory usage and adjusts memory allocated to scans accordingly. Spilling ten local files and then reading them in is probably in the noise of timings. 🎉. ---. ### Implementation Notes. I had to add two new file operations to the `RichHadoopConfiguration` because I need seekable file input streams. I don't like the names. I'm not sure what to do here. Hadoop really screws us with the seek-ability on compressed streams. The implementation is rather simple, it just maintains an array of the per-partition results. The index of the array corresponds to the partition index. The sparsity of that array is controlled by how often we spill. For an operation with a huge number of partitions that are often spilled (e.g. large number of partitions, each with a lot of data), we may want to use a `Map` instead of an `Array`. The use of `ObjectOutputStream` without a try-catch-finally block is non-standard. I was having trouble seeking to individual classes when I used one ObjectOutputStream to output each partition's array. There were these ""bad header"" messages. This seems to work. I don't close the OOS because I'm going to re-use the underlying output stream on the next partition. We use O(n_spills) files. ---. ### Timings. Master 0.2.14-4da055db5a7b; ```; In [1]: %%time ; ...: ; ...: import hail as hl ; ...: ht = hl.utils.range_table(10000, n_partitions=10000) ; ...: ht = ht.annotate(rank = hl.scan.count())._force_count() ; CPU times: user 1.45 s, sys: 333 ms, total: 1.78 s; Wall time: 24.6 s; In [3]: %%time ; ...: ;",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6333
https://github.com/hail-is/hail/pull/6333:1526,Integrability,message,messages,1526," scans accordingly. Spilling ten local files and then reading them in is probably in the noise of timings. 🎉. ---. ### Implementation Notes. I had to add two new file operations to the `RichHadoopConfiguration` because I need seekable file input streams. I don't like the names. I'm not sure what to do here. Hadoop really screws us with the seek-ability on compressed streams. The implementation is rather simple, it just maintains an array of the per-partition results. The index of the array corresponds to the partition index. The sparsity of that array is controlled by how often we spill. For an operation with a huge number of partitions that are often spilled (e.g. large number of partitions, each with a lot of data), we may want to use a `Map` instead of an `Array`. The use of `ObjectOutputStream` without a try-catch-finally block is non-standard. I was having trouble seeking to individual classes when I used one ObjectOutputStream to output each partition's array. There were these ""bad header"" messages. This seems to work. I don't close the OOS because I'm going to re-use the underlying output stream on the next partition. We use O(n_spills) files. ---. ### Timings. Master 0.2.14-4da055db5a7b; ```; In [1]: %%time ; ...: ; ...: import hail as hl ; ...: ht = hl.utils.range_table(10000, n_partitions=10000) ; ...: ht = ht.annotate(rank = hl.scan.count())._force_count() ; CPU times: user 1.45 s, sys: 333 ms, total: 1.78 s; Wall time: 24.6 s; In [3]: %%time ; ...: ; ...: import hail as hl ; ...: ht = hl.utils.range_table(1000000, n_partitions=1000) ; ...: ht = ht.annotate(rank = hl.scan.count())._force_count() ; CPU times: user 6.23 ms, sys: 1.96 ms, total: 8.19 ms; Wall time: 1.33 s; ```; This branch; ```; In [1]: %%time ; ...: ; ...: import hail as hl ; ...: ht = hl.utils.range_table(10000, n_partitions=10000) ; ...: ht = ht.annotate(rank = hl.scan.count())._force_count() ; CPU times: user 1.4 s, sys: 362 ms, total: 1.76 s; Wall time: 25.6 s. In [2]: %%time ; ...: ; .",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6333
https://github.com/hail-is/hail/pull/6333:42,Safety,avoid,avoids,42,"This adds `SpillingCollectIterator` which avoids holding more than 1000 aggregation results in memory at one time. We could do something that listens for GC events and spills data if there's high memory pressure. That seems a bit error prone and hard. How should I pipe the size limit down to TableMapRows? I decided to make it a HailContext `flag` which means its not very user-visible, but Laurent can set it for now. In C++ we can design a system that is aware of its memory usage and adjusts memory allocated to scans accordingly. Spilling ten local files and then reading them in is probably in the noise of timings. 🎉. ---. ### Implementation Notes. I had to add two new file operations to the `RichHadoopConfiguration` because I need seekable file input streams. I don't like the names. I'm not sure what to do here. Hadoop really screws us with the seek-ability on compressed streams. The implementation is rather simple, it just maintains an array of the per-partition results. The index of the array corresponds to the partition index. The sparsity of that array is controlled by how often we spill. For an operation with a huge number of partitions that are often spilled (e.g. large number of partitions, each with a lot of data), we may want to use a `Map` instead of an `Array`. The use of `ObjectOutputStream` without a try-catch-finally block is non-standard. I was having trouble seeking to individual classes when I used one ObjectOutputStream to output each partition's array. There were these ""bad header"" messages. This seems to work. I don't close the OOS because I'm going to re-use the underlying output stream on the next partition. We use O(n_spills) files. ---. ### Timings. Master 0.2.14-4da055db5a7b; ```; In [1]: %%time ; ...: ; ...: import hail as hl ; ...: ht = hl.utils.range_table(10000, n_partitions=10000) ; ...: ht = ht.annotate(rank = hl.scan.count())._force_count() ; CPU times: user 1.45 s, sys: 333 ms, total: 1.78 s; Wall time: 24.6 s; In [3]: %%time ; ...: ;",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6333
https://github.com/hail-is/hail/pull/6333:922,Usability,simpl,simple,922,"This adds `SpillingCollectIterator` which avoids holding more than 1000 aggregation results in memory at one time. We could do something that listens for GC events and spills data if there's high memory pressure. That seems a bit error prone and hard. How should I pipe the size limit down to TableMapRows? I decided to make it a HailContext `flag` which means its not very user-visible, but Laurent can set it for now. In C++ we can design a system that is aware of its memory usage and adjusts memory allocated to scans accordingly. Spilling ten local files and then reading them in is probably in the noise of timings. 🎉. ---. ### Implementation Notes. I had to add two new file operations to the `RichHadoopConfiguration` because I need seekable file input streams. I don't like the names. I'm not sure what to do here. Hadoop really screws us with the seek-ability on compressed streams. The implementation is rather simple, it just maintains an array of the per-partition results. The index of the array corresponds to the partition index. The sparsity of that array is controlled by how often we spill. For an operation with a huge number of partitions that are often spilled (e.g. large number of partitions, each with a lot of data), we may want to use a `Map` instead of an `Array`. The use of `ObjectOutputStream` without a try-catch-finally block is non-standard. I was having trouble seeking to individual classes when I used one ObjectOutputStream to output each partition's array. There were these ""bad header"" messages. This seems to work. I don't close the OOS because I'm going to re-use the underlying output stream on the next partition. We use O(n_spills) files. ---. ### Timings. Master 0.2.14-4da055db5a7b; ```; In [1]: %%time ; ...: ; ...: import hail as hl ; ...: ht = hl.utils.range_table(10000, n_partitions=10000) ; ...: ht = ht.annotate(rank = hl.scan.count())._force_count() ; CPU times: user 1.45 s, sys: 333 ms, total: 1.78 s; Wall time: 24.6 s; In [3]: %%time ; ...: ;",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6333
https://github.com/hail-is/hail/issues/6339:37,Availability,error,error,37,"when I tried to import_vcf, I got an error：. Hail version: 0.2.14-8dcb6722c72a; Error summary: HailException: Invalid locus 'MT:0' found. Position '0' is not within the range [1-16569] for reference genome 'GRCh37'. However, telomeres are indicated by using postion 0 or N+1 in VCF 4.2.; please tell me how can I import this VCF correctly?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6339
https://github.com/hail-is/hail/issues/6339:80,Availability,Error,Error,80,"when I tried to import_vcf, I got an error：. Hail version: 0.2.14-8dcb6722c72a; Error summary: HailException: Invalid locus 'MT:0' found. Position '0' is not within the range [1-16569] for reference genome 'GRCh37'. However, telomeres are indicated by using postion 0 or N+1 in VCF 4.2.; please tell me how can I import this VCF correctly?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6339
https://github.com/hail-is/hail/pull/6341:77,Availability,redundant,redundant,77,"I implemented `cancelled` as a join from the batch table rather than storing redundant information for each job. @akotlar The issue was that all jobs were getting set to cancelled at the same time (and thus notifying children), so always run jobs were all getting run at once neglecting the hierarchy of job dependencies. This is the purpose of having the `cancelled` flag as separate from the `Cancelled` state and is checked in `create_if_ready`. @cseed Can you look and see if this solves the cancel problem we discussed earlier? The faulty PR was this one: https://github.com/hail-is/hail/pull/6128/files",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6341
https://github.com/hail-is/hail/pull/6341:537,Availability,fault,faulty,537,"I implemented `cancelled` as a join from the batch table rather than storing redundant information for each job. @akotlar The issue was that all jobs were getting set to cancelled at the same time (and thus notifying children), so always run jobs were all getting run at once neglecting the hierarchy of job dependencies. This is the purpose of having the `cancelled` flag as separate from the `Cancelled` state and is checked in `create_if_ready`. @cseed Can you look and see if this solves the cancel problem we discussed earlier? The faulty PR was this one: https://github.com/hail-is/hail/pull/6128/files",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6341
https://github.com/hail-is/hail/pull/6341:308,Integrability,depend,dependencies,308,"I implemented `cancelled` as a join from the batch table rather than storing redundant information for each job. @akotlar The issue was that all jobs were getting set to cancelled at the same time (and thus notifying children), so always run jobs were all getting run at once neglecting the hierarchy of job dependencies. This is the purpose of having the `cancelled` flag as separate from the `Cancelled` state and is checked in `create_if_ready`. @cseed Can you look and see if this solves the cancel problem we discussed earlier? The faulty PR was this one: https://github.com/hail-is/hail/pull/6128/files",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6341
https://github.com/hail-is/hail/pull/6341:77,Safety,redund,redundant,77,"I implemented `cancelled` as a join from the batch table rather than storing redundant information for each job. @akotlar The issue was that all jobs were getting set to cancelled at the same time (and thus notifying children), so always run jobs were all getting run at once neglecting the hierarchy of job dependencies. This is the purpose of having the `cancelled` flag as separate from the `Cancelled` state and is checked in `create_if_ready`. @cseed Can you look and see if this solves the cancel problem we discussed earlier? The faulty PR was this one: https://github.com/hail-is/hail/pull/6128/files",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6341
https://github.com/hail-is/hail/issues/6342:1539,Availability,avail,available,1539,"file. ```; # hexdump /tmp/bar; 0000000 ef bb bf 73 61 6d 70 6c 65 5f 69 64 0a 66 6f 6f; 0000010 0a ; 0000011; # ipython; import hail asPython 3.7.3 (default, Mar 27 2019, 09:23:15) ; Type 'copyright', 'credits' or 'license' for more information; IPython 7.5.0 -- An enhanced Interactive Python. Type '?' for help. In [1]: import hail as hl ; hl.import_; In [2]: t = hl.import_table('/tmp/bar') ; ...: t.describe() ; ...: t = t.key_by('sample_id') ; Initializing Spark and Hail with default parameters...; using hail jar at /usr/local/lib/python3.7/site-packages/hail/hail-all-spark.jar; 19/06/13 14:08:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Running on Apache Spark version 2.4.1; SparkUI available at http://wm06b-953.broadinstitute.org:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.14-5cb00c115421; LOGGING: writing to /Users/dking/projects/hail/hail/hail-20190613-1408-0.2.14-5cb00c115421.log; 2019-06-13 14:08:15 Hail: INFO: Reading table with no type imputation; Loading column '?sample_id' as type 'str' (type not specified). ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; '﻿sample_id': str ; ----------------------------------------; Key: []; ----------------------------------------; ---------------------------------------------------------------------------; LookupError Traceback (most recent call last); <ipython-input-2-6b119cf7ec41> in <module>; 1 t = hl.import_table('/tmp/bar'); 2 t.describe(); ----> 3 t = t.key_by('sample_id'). </usr/local/lib/python3.7/site-packages/decorator.py:decorator-gen-958> in key_by(self, *keys, **named_keys). /usr/local/lib/python3.7",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6342
https://github.com/hail-is/hail/issues/6342:2591,Integrability,wrap,wrapper,2591,"sers/dking/projects/hail/hail/hail-20190613-1408-0.2.14-5cb00c115421.log; 2019-06-13 14:08:15 Hail: INFO: Reading table with no type imputation; Loading column '?sample_id' as type 'str' (type not specified). ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; '﻿sample_id': str ; ----------------------------------------; Key: []; ----------------------------------------; ---------------------------------------------------------------------------; LookupError Traceback (most recent call last); <ipython-input-2-6b119cf7ec41> in <module>; 1 t = hl.import_table('/tmp/bar'); 2 t.describe(); ----> 3 t = t.key_by('sample_id'). </usr/local/lib/python3.7/site-packages/decorator.py:decorator-gen-958> in key_by(self, *keys, **named_keys). /usr/local/lib/python3.7/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 583 def wrapper(__original_func, *args, **kwargs):; 584 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 585 return __original_func(*args_, **kwargs_); 586 ; 587 return wrapper. /usr/local/lib/python3.7/site-packages/hail/table.py in key_by(self, *keys, **named_keys); 536 Table with a new key.; 537 """"""; --> 538 key_fields, computed_keys = get_key_by_exprs(""Table.key_by"", keys, named_keys, self._row_indices); 539 ; 540 if not computed_keys:. /usr/local/lib/python3.7/site-packages/hail/utils/misc.py in get_key_by_exprs(caller, exprs, named_exprs, indices, override_protected_indices); 318 def get_key_by_exprs(caller, exprs, named_exprs, indices, override_protected_indices=None):; 319 from hail.expr.expressions import to_expr, ExpressionException, analyze; --> 320 exprs = [indices.source[e] if isinstance(e, str) else e for e in exprs]; 321 named_exprs = {k: to_expr(v) for k, v in named_exprs.items()}; 322 . /usr/local/lib/python3.7/site-packages/hail/utils/misc.py in <listcomp>(.0); 318 def get_key_by_exprs(caller, exprs, named_ex",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6342
https://github.com/hail-is/hail/issues/6342:2642,Integrability,wrap,wrapper,2642,"sers/dking/projects/hail/hail/hail-20190613-1408-0.2.14-5cb00c115421.log; 2019-06-13 14:08:15 Hail: INFO: Reading table with no type imputation; Loading column '?sample_id' as type 'str' (type not specified). ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; '﻿sample_id': str ; ----------------------------------------; Key: []; ----------------------------------------; ---------------------------------------------------------------------------; LookupError Traceback (most recent call last); <ipython-input-2-6b119cf7ec41> in <module>; 1 t = hl.import_table('/tmp/bar'); 2 t.describe(); ----> 3 t = t.key_by('sample_id'). </usr/local/lib/python3.7/site-packages/decorator.py:decorator-gen-958> in key_by(self, *keys, **named_keys). /usr/local/lib/python3.7/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 583 def wrapper(__original_func, *args, **kwargs):; 584 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 585 return __original_func(*args_, **kwargs_); 586 ; 587 return wrapper. /usr/local/lib/python3.7/site-packages/hail/table.py in key_by(self, *keys, **named_keys); 536 Table with a new key.; 537 """"""; --> 538 key_fields, computed_keys = get_key_by_exprs(""Table.key_by"", keys, named_keys, self._row_indices); 539 ; 540 if not computed_keys:. /usr/local/lib/python3.7/site-packages/hail/utils/misc.py in get_key_by_exprs(caller, exprs, named_exprs, indices, override_protected_indices); 318 def get_key_by_exprs(caller, exprs, named_exprs, indices, override_protected_indices=None):; 319 from hail.expr.expressions import to_expr, ExpressionException, analyze; --> 320 exprs = [indices.source[e] if isinstance(e, str) else e for e in exprs]; 321 named_exprs = {k: to_expr(v) for k, v in named_exprs.items()}; 322 . /usr/local/lib/python3.7/site-packages/hail/utils/misc.py in <listcomp>(.0); 318 def get_key_by_exprs(caller, exprs, named_ex",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6342
https://github.com/hail-is/hail/issues/6342:2848,Integrability,wrap,wrapper,2848,"sers/dking/projects/hail/hail/hail-20190613-1408-0.2.14-5cb00c115421.log; 2019-06-13 14:08:15 Hail: INFO: Reading table with no type imputation; Loading column '?sample_id' as type 'str' (type not specified). ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; '﻿sample_id': str ; ----------------------------------------; Key: []; ----------------------------------------; ---------------------------------------------------------------------------; LookupError Traceback (most recent call last); <ipython-input-2-6b119cf7ec41> in <module>; 1 t = hl.import_table('/tmp/bar'); 2 t.describe(); ----> 3 t = t.key_by('sample_id'). </usr/local/lib/python3.7/site-packages/decorator.py:decorator-gen-958> in key_by(self, *keys, **named_keys). /usr/local/lib/python3.7/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 583 def wrapper(__original_func, *args, **kwargs):; 584 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 585 return __original_func(*args_, **kwargs_); 586 ; 587 return wrapper. /usr/local/lib/python3.7/site-packages/hail/table.py in key_by(self, *keys, **named_keys); 536 Table with a new key.; 537 """"""; --> 538 key_fields, computed_keys = get_key_by_exprs(""Table.key_by"", keys, named_keys, self._row_indices); 539 ; 540 if not computed_keys:. /usr/local/lib/python3.7/site-packages/hail/utils/misc.py in get_key_by_exprs(caller, exprs, named_exprs, indices, override_protected_indices); 318 def get_key_by_exprs(caller, exprs, named_exprs, indices, override_protected_indices=None):; 319 from hail.expr.expressions import to_expr, ExpressionException, analyze; --> 320 exprs = [indices.source[e] if isinstance(e, str) else e for e in exprs]; 321 named_exprs = {k: to_expr(v) for k, v in named_exprs.items()}; 322 . /usr/local/lib/python3.7/site-packages/hail/utils/misc.py in <listcomp>(.0); 318 def get_key_by_exprs(caller, exprs, named_ex",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6342
https://github.com/hail-is/hail/issues/6342:814,Modifiability,enhance,enhanced,814,"See the transcript below. This is particularly confusing for users because python often elides the non-printable characters. A small wrinkle of confusion is that the UTF-8 BOM, `ef bb bf`, is converted by Java into the UTF-16 BOM, `fe ff`. This is apparently [a well known Java bug](https://stackoverflow.com/questions/1835430/byte-order-mark-screws-up-file-reading-in-java)? This looks pretty annoying to fix in Scala/Java because we'd have to muck around with Spark's `hadoopFile` infrastructure to figure out where it is actually reading from a file. ```; # hexdump /tmp/bar; 0000000 ef bb bf 73 61 6d 70 6c 65 5f 69 64 0a 66 6f 6f; 0000010 0a ; 0000011; # ipython; import hail asPython 3.7.3 (default, Mar 27 2019, 09:23:15) ; Type 'copyright', 'credits' or 'license' for more information; IPython 7.5.0 -- An enhanced Interactive Python. Type '?' for help. In [1]: import hail as hl ; hl.import_; In [2]: t = hl.import_table('/tmp/bar') ; ...: t.describe() ; ...: t = t.key_by('sample_id') ; Initializing Spark and Hail with default parameters...; using hail jar at /usr/local/lib/python3.7/site-packages/hail/hail-all-spark.jar; 19/06/13 14:08:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Running on Apache Spark version 2.4.1; SparkUI available at http://wm06b-953.broadinstitute.org:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.14-5cb00c115421; LOGGING: writing to /Users/dking/projects/hail/hail/hail-20190613-1408-0.2.14-5cb00c115421.log; 2019-06-13 14:08:15 Hail: INFO: Reading table with no type imputation; Loading column '?sample_id' as type 'str' (type not specified). ----------------------------------------; Global fields:; None; --------",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6342
https://github.com/hail-is/hail/issues/6342:1186,Performance,load,load,1186," bb bf`, is converted by Java into the UTF-16 BOM, `fe ff`. This is apparently [a well known Java bug](https://stackoverflow.com/questions/1835430/byte-order-mark-screws-up-file-reading-in-java)? This looks pretty annoying to fix in Scala/Java because we'd have to muck around with Spark's `hadoopFile` infrastructure to figure out where it is actually reading from a file. ```; # hexdump /tmp/bar; 0000000 ef bb bf 73 61 6d 70 6c 65 5f 69 64 0a 66 6f 6f; 0000010 0a ; 0000011; # ipython; import hail asPython 3.7.3 (default, Mar 27 2019, 09:23:15) ; Type 'copyright', 'credits' or 'license' for more information; IPython 7.5.0 -- An enhanced Interactive Python. Type '?' for help. In [1]: import hail as hl ; hl.import_; In [2]: t = hl.import_table('/tmp/bar') ; ...: t.describe() ; ...: t = t.key_by('sample_id') ; Initializing Spark and Hail with default parameters...; using hail jar at /usr/local/lib/python3.7/site-packages/hail/hail-all-spark.jar; 19/06/13 14:08:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Running on Apache Spark version 2.4.1; SparkUI available at http://wm06b-953.broadinstitute.org:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.14-5cb00c115421; LOGGING: writing to /Users/dking/projects/hail/hail/hail-20190613-1408-0.2.14-5cb00c115421.log; 2019-06-13 14:08:15 Hail: INFO: Reading table with no type imputation; Loading column '?sample_id' as type 'str' (type not specified). ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; '﻿sample_id': str ; ----------------------------------------; Key: []; ----------------------------------------; --------------------",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6342
https://github.com/hail-is/hail/issues/6342:1865,Performance,Load,Loading,1865,"lp. In [1]: import hail as hl ; hl.import_; In [2]: t = hl.import_table('/tmp/bar') ; ...: t.describe() ; ...: t = t.key_by('sample_id') ; Initializing Spark and Hail with default parameters...; using hail jar at /usr/local/lib/python3.7/site-packages/hail/hail-all-spark.jar; 19/06/13 14:08:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Running on Apache Spark version 2.4.1; SparkUI available at http://wm06b-953.broadinstitute.org:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.14-5cb00c115421; LOGGING: writing to /Users/dking/projects/hail/hail/hail-20190613-1408-0.2.14-5cb00c115421.log; 2019-06-13 14:08:15 Hail: INFO: Reading table with no type imputation; Loading column '?sample_id' as type 'str' (type not specified). ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; '﻿sample_id': str ; ----------------------------------------; Key: []; ----------------------------------------; ---------------------------------------------------------------------------; LookupError Traceback (most recent call last); <ipython-input-2-6b119cf7ec41> in <module>; 1 t = hl.import_table('/tmp/bar'); 2 t.describe(); ----> 3 t = t.key_by('sample_id'). </usr/local/lib/python3.7/site-packages/decorator.py:decorator-gen-958> in key_by(self, *keys, **named_keys). /usr/local/lib/python3.7/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 583 def wrapper(__original_func, *args, **kwargs):; 584 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 585 return __original_func(*args_, **kwargs_); 586 ; 587 return wrapper. /u",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6342
https://github.com/hail-is/hail/issues/6342:1376,Testability,log,log,1376,"a)? This looks pretty annoying to fix in Scala/Java because we'd have to muck around with Spark's `hadoopFile` infrastructure to figure out where it is actually reading from a file. ```; # hexdump /tmp/bar; 0000000 ef bb bf 73 61 6d 70 6c 65 5f 69 64 0a 66 6f 6f; 0000010 0a ; 0000011; # ipython; import hail asPython 3.7.3 (default, Mar 27 2019, 09:23:15) ; Type 'copyright', 'credits' or 'license' for more information; IPython 7.5.0 -- An enhanced Interactive Python. Type '?' for help. In [1]: import hail as hl ; hl.import_; In [2]: t = hl.import_table('/tmp/bar') ; ...: t.describe() ; ...: t = t.key_by('sample_id') ; Initializing Spark and Hail with default parameters...; using hail jar at /usr/local/lib/python3.7/site-packages/hail/hail-all-spark.jar; 19/06/13 14:08:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Running on Apache Spark version 2.4.1; SparkUI available at http://wm06b-953.broadinstitute.org:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.14-5cb00c115421; LOGGING: writing to /Users/dking/projects/hail/hail/hail-20190613-1408-0.2.14-5cb00c115421.log; 2019-06-13 14:08:15 Hail: INFO: Reading table with no type imputation; Loading column '?sample_id' as type 'str' (type not specified). ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; '﻿sample_id': str ; ----------------------------------------; Key: []; ----------------------------------------; ---------------------------------------------------------------------------; LookupError Traceback (most recent call last); <ipython-input-2-6b119cf7ec41> in <module>; 1 t = hl.import_table('/tmp/bar'); 2 t.desc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6342
https://github.com/hail-is/hail/issues/6342:1408,Testability,log,logging,1408," Scala/Java because we'd have to muck around with Spark's `hadoopFile` infrastructure to figure out where it is actually reading from a file. ```; # hexdump /tmp/bar; 0000000 ef bb bf 73 61 6d 70 6c 65 5f 69 64 0a 66 6f 6f; 0000010 0a ; 0000011; # ipython; import hail asPython 3.7.3 (default, Mar 27 2019, 09:23:15) ; Type 'copyright', 'credits' or 'license' for more information; IPython 7.5.0 -- An enhanced Interactive Python. Type '?' for help. In [1]: import hail as hl ; hl.import_; In [2]: t = hl.import_table('/tmp/bar') ; ...: t.describe() ; ...: t = t.key_by('sample_id') ; Initializing Spark and Hail with default parameters...; using hail jar at /usr/local/lib/python3.7/site-packages/hail/hail-all-spark.jar; 19/06/13 14:08:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Running on Apache Spark version 2.4.1; SparkUI available at http://wm06b-953.broadinstitute.org:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.14-5cb00c115421; LOGGING: writing to /Users/dking/projects/hail/hail/hail-20190613-1408-0.2.14-5cb00c115421.log; 2019-06-13 14:08:15 Hail: INFO: Reading table with no type imputation; Loading column '?sample_id' as type 'str' (type not specified). ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; '﻿sample_id': str ; ----------------------------------------; Key: []; ----------------------------------------; ---------------------------------------------------------------------------; LookupError Traceback (most recent call last); <ipython-input-2-6b119cf7ec41> in <module>; 1 t = hl.import_table('/tmp/bar'); 2 t.describe(); ----> 3 t = t.key_by('sample_id')",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6342
https://github.com/hail-is/hail/issues/6342:1698,Testability,LOG,LOGGING,1698,"5) ; Type 'copyright', 'credits' or 'license' for more information; IPython 7.5.0 -- An enhanced Interactive Python. Type '?' for help. In [1]: import hail as hl ; hl.import_; In [2]: t = hl.import_table('/tmp/bar') ; ...: t.describe() ; ...: t = t.key_by('sample_id') ; Initializing Spark and Hail with default parameters...; using hail jar at /usr/local/lib/python3.7/site-packages/hail/hail-all-spark.jar; 19/06/13 14:08:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Running on Apache Spark version 2.4.1; SparkUI available at http://wm06b-953.broadinstitute.org:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.14-5cb00c115421; LOGGING: writing to /Users/dking/projects/hail/hail/hail-20190613-1408-0.2.14-5cb00c115421.log; 2019-06-13 14:08:15 Hail: INFO: Reading table with no type imputation; Loading column '?sample_id' as type 'str' (type not specified). ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; '﻿sample_id': str ; ----------------------------------------; Key: []; ----------------------------------------; ---------------------------------------------------------------------------; LookupError Traceback (most recent call last); <ipython-input-2-6b119cf7ec41> in <module>; 1 t = hl.import_table('/tmp/bar'); 2 t.describe(); ----> 3 t = t.key_by('sample_id'). </usr/local/lib/python3.7/site-packages/decorator.py:decorator-gen-958> in key_by(self, *keys, **named_keys). /usr/local/lib/python3.7/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 583 def wrapper(__original_func, *args, **kwargs):; 584 args_, kwargs_ = check_all(__origina",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6342
https://github.com/hail-is/hail/issues/6342:1789,Testability,log,log,1789,"lp. In [1]: import hail as hl ; hl.import_; In [2]: t = hl.import_table('/tmp/bar') ; ...: t.describe() ; ...: t = t.key_by('sample_id') ; Initializing Spark and Hail with default parameters...; using hail jar at /usr/local/lib/python3.7/site-packages/hail/hail-all-spark.jar; 19/06/13 14:08:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Running on Apache Spark version 2.4.1; SparkUI available at http://wm06b-953.broadinstitute.org:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.14-5cb00c115421; LOGGING: writing to /Users/dking/projects/hail/hail/hail-20190613-1408-0.2.14-5cb00c115421.log; 2019-06-13 14:08:15 Hail: INFO: Reading table with no type imputation; Loading column '?sample_id' as type 'str' (type not specified). ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; '﻿sample_id': str ; ----------------------------------------; Key: []; ----------------------------------------; ---------------------------------------------------------------------------; LookupError Traceback (most recent call last); <ipython-input-2-6b119cf7ec41> in <module>; 1 t = hl.import_table('/tmp/bar'); 2 t.describe(); ----> 3 t = t.key_by('sample_id'). </usr/local/lib/python3.7/site-packages/decorator.py:decorator-gen-958> in key_by(self, *keys, **named_keys). /usr/local/lib/python3.7/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 583 def wrapper(__original_func, *args, **kwargs):; 584 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 585 return __original_func(*args_, **kwargs_); 586 ; 587 return wrapper. /u",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6342
https://github.com/hail-is/hail/pull/6343:386,Availability,down,down,386,"This is the most recently deployed version of monitoring.yaml. I'm not sure the best way to test it solves the problem that deployments faced. One thing to note is that StatefulSets don't guarantee that all of their constituent pods get deleted when the StatefulSet is deleted. To be sure the pods all get deleted, we'd have to either manually delete them or scale the StatefulSet size down to 0 before deleting it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6343
https://github.com/hail-is/hail/pull/6343:26,Deployability,deploy,deployed,26,"This is the most recently deployed version of monitoring.yaml. I'm not sure the best way to test it solves the problem that deployments faced. One thing to note is that StatefulSets don't guarantee that all of their constituent pods get deleted when the StatefulSet is deleted. To be sure the pods all get deleted, we'd have to either manually delete them or scale the StatefulSet size down to 0 before deleting it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6343
https://github.com/hail-is/hail/pull/6343:124,Deployability,deploy,deployments,124,"This is the most recently deployed version of monitoring.yaml. I'm not sure the best way to test it solves the problem that deployments faced. One thing to note is that StatefulSets don't guarantee that all of their constituent pods get deleted when the StatefulSet is deleted. To be sure the pods all get deleted, we'd have to either manually delete them or scale the StatefulSet size down to 0 before deleting it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6343
https://github.com/hail-is/hail/pull/6343:46,Energy Efficiency,monitor,monitoring,46,"This is the most recently deployed version of monitoring.yaml. I'm not sure the best way to test it solves the problem that deployments faced. One thing to note is that StatefulSets don't guarantee that all of their constituent pods get deleted when the StatefulSet is deleted. To be sure the pods all get deleted, we'd have to either manually delete them or scale the StatefulSet size down to 0 before deleting it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6343
https://github.com/hail-is/hail/pull/6343:92,Testability,test,test,92,"This is the most recently deployed version of monitoring.yaml. I'm not sure the best way to test it solves the problem that deployments faced. One thing to note is that StatefulSets don't guarantee that all of their constituent pods get deleted when the StatefulSet is deleted. To be sure the pods all get deleted, we'd have to either manually delete them or scale the StatefulSet size down to 0 before deleting it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6343
https://github.com/hail-is/hail/pull/6345:228,Availability,error,error,228,"This adds SpillingCollectIterator which avoids holding more than 1000 aggregation results in memory at one time. We could do something that listens for GC events and spills data if there's high memory pressure. That seems a bit error prone and hard. The number of results kept in memory is a flag on the HailContext. In C++ we can design a system that is aware of its memory usage and adjusts memory allocated to scans accordingly. #### Implementation Notes. I had to add two new file operations to `FS` and `HadoopFS` because I need seekable file input streams. When we add non-hadoop `FS`'s we'll need to address the interface issue. When we overflow our in-memory buffer, we spill to a disk file. We use O(n_partitions / mem_limit) files. We stream through the files to `scanLeft`, to compute the globally valid scan state per partition. The stream writes its results to another file which must be on a cluster-visible file system (we use `HailContext.getTemporaryFile`). Finally, each partition reads that file and seeks to its scan state. I somewhat better solution would be to eagerly scan as results come in. I leave that as future work. #### Timings. Master 0.2.14-4da055db5a7b; ```; In [1]: %%time ; ...: ; ...: import hail as hl ; ...: ht = hl.utils.range_table(10000, n_partitions=10000) ; ...: ht = ht.annotate(rank = hl.scan.count())._force_count() ; CPU times: user 1.45 s, sys: 333 ms, total: 1.78 s; Wall time: 24.6 s; In [3]: %%time ; ...: ; ...: import hail as hl ; ...: ht = hl.utils.range_table(1000000, n_partitions=1000) ; ...: ht = ht.annotate(rank = hl.scan.count())._force_count() ; CPU times: user 6.23 ms, sys: 1.96 ms, total: 8.19 ms; Wall time: 1.33 s; ```; This branch; ```; In [1]: %%time ; ...: ; ...: import hail as hl ; ...: ht = hl.utils.range_table(10000, n_partitions=10000) ; ...: ht = ht.annotate(rank = hl.scan.count())._force_count() ; CPU times: user 1.36 s, sys: 297 ms, total: 1.66 s; Wall time: 27.3 s. In [2]: %%time ; ...: ; ...: import hail as hl ; ...:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6345
https://github.com/hail-is/hail/pull/6345:400,Energy Efficiency,allocate,allocated,400,"This adds SpillingCollectIterator which avoids holding more than 1000 aggregation results in memory at one time. We could do something that listens for GC events and spills data if there's high memory pressure. That seems a bit error prone and hard. The number of results kept in memory is a flag on the HailContext. In C++ we can design a system that is aware of its memory usage and adjusts memory allocated to scans accordingly. #### Implementation Notes. I had to add two new file operations to `FS` and `HadoopFS` because I need seekable file input streams. When we add non-hadoop `FS`'s we'll need to address the interface issue. When we overflow our in-memory buffer, we spill to a disk file. We use O(n_partitions / mem_limit) files. We stream through the files to `scanLeft`, to compute the globally valid scan state per partition. The stream writes its results to another file which must be on a cluster-visible file system (we use `HailContext.getTemporaryFile`). Finally, each partition reads that file and seeks to its scan state. I somewhat better solution would be to eagerly scan as results come in. I leave that as future work. #### Timings. Master 0.2.14-4da055db5a7b; ```; In [1]: %%time ; ...: ; ...: import hail as hl ; ...: ht = hl.utils.range_table(10000, n_partitions=10000) ; ...: ht = ht.annotate(rank = hl.scan.count())._force_count() ; CPU times: user 1.45 s, sys: 333 ms, total: 1.78 s; Wall time: 24.6 s; In [3]: %%time ; ...: ; ...: import hail as hl ; ...: ht = hl.utils.range_table(1000000, n_partitions=1000) ; ...: ht = ht.annotate(rank = hl.scan.count())._force_count() ; CPU times: user 6.23 ms, sys: 1.96 ms, total: 8.19 ms; Wall time: 1.33 s; ```; This branch; ```; In [1]: %%time ; ...: ; ...: import hail as hl ; ...: ht = hl.utils.range_table(10000, n_partitions=10000) ; ...: ht = ht.annotate(rank = hl.scan.count())._force_count() ; CPU times: user 1.36 s, sys: 297 ms, total: 1.66 s; Wall time: 27.3 s. In [2]: %%time ; ...: ; ...: import hail as hl ; ...:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6345
https://github.com/hail-is/hail/pull/6345:619,Integrability,interface,interface,619,"This adds SpillingCollectIterator which avoids holding more than 1000 aggregation results in memory at one time. We could do something that listens for GC events and spills data if there's high memory pressure. That seems a bit error prone and hard. The number of results kept in memory is a flag on the HailContext. In C++ we can design a system that is aware of its memory usage and adjusts memory allocated to scans accordingly. #### Implementation Notes. I had to add two new file operations to `FS` and `HadoopFS` because I need seekable file input streams. When we add non-hadoop `FS`'s we'll need to address the interface issue. When we overflow our in-memory buffer, we spill to a disk file. We use O(n_partitions / mem_limit) files. We stream through the files to `scanLeft`, to compute the globally valid scan state per partition. The stream writes its results to another file which must be on a cluster-visible file system (we use `HailContext.getTemporaryFile`). Finally, each partition reads that file and seeks to its scan state. I somewhat better solution would be to eagerly scan as results come in. I leave that as future work. #### Timings. Master 0.2.14-4da055db5a7b; ```; In [1]: %%time ; ...: ; ...: import hail as hl ; ...: ht = hl.utils.range_table(10000, n_partitions=10000) ; ...: ht = ht.annotate(rank = hl.scan.count())._force_count() ; CPU times: user 1.45 s, sys: 333 ms, total: 1.78 s; Wall time: 24.6 s; In [3]: %%time ; ...: ; ...: import hail as hl ; ...: ht = hl.utils.range_table(1000000, n_partitions=1000) ; ...: ht = ht.annotate(rank = hl.scan.count())._force_count() ; CPU times: user 6.23 ms, sys: 1.96 ms, total: 8.19 ms; Wall time: 1.33 s; ```; This branch; ```; In [1]: %%time ; ...: ; ...: import hail as hl ; ...: ht = hl.utils.range_table(10000, n_partitions=10000) ; ...: ht = ht.annotate(rank = hl.scan.count())._force_count() ; CPU times: user 1.36 s, sys: 297 ms, total: 1.66 s; Wall time: 27.3 s. In [2]: %%time ; ...: ; ...: import hail as hl ; ...:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6345
https://github.com/hail-is/hail/pull/6345:40,Safety,avoid,avoids,40,"This adds SpillingCollectIterator which avoids holding more than 1000 aggregation results in memory at one time. We could do something that listens for GC events and spills data if there's high memory pressure. That seems a bit error prone and hard. The number of results kept in memory is a flag on the HailContext. In C++ we can design a system that is aware of its memory usage and adjusts memory allocated to scans accordingly. #### Implementation Notes. I had to add two new file operations to `FS` and `HadoopFS` because I need seekable file input streams. When we add non-hadoop `FS`'s we'll need to address the interface issue. When we overflow our in-memory buffer, we spill to a disk file. We use O(n_partitions / mem_limit) files. We stream through the files to `scanLeft`, to compute the globally valid scan state per partition. The stream writes its results to another file which must be on a cluster-visible file system (we use `HailContext.getTemporaryFile`). Finally, each partition reads that file and seeks to its scan state. I somewhat better solution would be to eagerly scan as results come in. I leave that as future work. #### Timings. Master 0.2.14-4da055db5a7b; ```; In [1]: %%time ; ...: ; ...: import hail as hl ; ...: ht = hl.utils.range_table(10000, n_partitions=10000) ; ...: ht = ht.annotate(rank = hl.scan.count())._force_count() ; CPU times: user 1.45 s, sys: 333 ms, total: 1.78 s; Wall time: 24.6 s; In [3]: %%time ; ...: ; ...: import hail as hl ; ...: ht = hl.utils.range_table(1000000, n_partitions=1000) ; ...: ht = ht.annotate(rank = hl.scan.count())._force_count() ; CPU times: user 6.23 ms, sys: 1.96 ms, total: 8.19 ms; Wall time: 1.33 s; ```; This branch; ```; In [1]: %%time ; ...: ; ...: import hail as hl ; ...: ht = hl.utils.range_table(10000, n_partitions=10000) ; ...: ht = ht.annotate(rank = hl.scan.count())._force_count() ; CPU times: user 1.36 s, sys: 297 ms, total: 1.66 s; Wall time: 27.3 s. In [2]: %%time ; ...: ; ...: import hail as hl ; ...:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6345
https://github.com/hail-is/hail/issues/6351:204,Availability,error,error,204,"In brief: . Computing an LD (X @ X.T) matrix using hail's BlockMatrices that have been normalized using the default function seems to produce values >> 1 (~1.00000001, but much larger than floating point error). This can cause problems in downstream applications. Example:. # Normalize genotypes; BlockMatrix.write_from_entry_expr(mt.GT.n_alt_alleles(), ; out_dir + out_name + ""_norm"" + ""_bm"", ; mean_impute = True, center = True, normalize = True); bm_norm = BlockMatrix.read(out_dir + out_name + ""_norm"" + ""_bm""). # LD (unadjusted); starts_and_stops = hl.linalg.utils.locus_windows(mt.locus, radius = 2.1e6, _localize = False); bm_ld = (bm_norm @ bm_norm.T); bm_ld = BlockMatrix._from_java(bm_ld._jbm.filterRowIntervalsIR(Env.backend()._to_java_ir(starts_and_stops._ir), False)); bm_ld.write(out_dir + out_name + ""_LD"" + ""_bm"", overwrite = True); bm_ld = BlockMatrix.read(out_dir + out_name + ""_LD"" + ""_bm""). # Export LD matrices; list_range = [list(range(x.start_idx, x.end_idx + 1)) for x in list_meta[0:5]]; bms = [bm_ld.filter(x,x) for x in list_range]; hl.experimental.export_block_matrices(bms, out_dir + out_name + ""_tissue"" + ""_ld""). # Example image of problem:; <img width=""594"" alt=""Screen Shot 2019-06-13 at 5 36 58 PM"" src=""https://user-images.githubusercontent.com/24594616/59470325-52676800-8e05-11e9-93fe-e48c0e06e70b.png"">. If genotypes are normalized to N(0,1), then X @ X.T should never have values larger than 1 except for floating point precision. This is anecdotal, but I never had this problem when using > 100k samples, but here I'm using ~700 samples. I'm not sure what's causing this, but I had a conversation with @liameabbott a while ago about how one should normalize these matrices. His understanding was that hail normalizes by dividing by `sqrt(sum(x^2))` whereas one may prefer to divide `sd(x)`. The example he sent me to do this is below:. # Liam's example; g = BlockMatrix.read('gs://ukbb-ldsc-dev/1000_genomes.phase_3.europeans.GT.autosomes.bm'). n = g.shape[1]; ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6351
https://github.com/hail-is/hail/issues/6351:239,Availability,down,downstream,239,"In brief: . Computing an LD (X @ X.T) matrix using hail's BlockMatrices that have been normalized using the default function seems to produce values >> 1 (~1.00000001, but much larger than floating point error). This can cause problems in downstream applications. Example:. # Normalize genotypes; BlockMatrix.write_from_entry_expr(mt.GT.n_alt_alleles(), ; out_dir + out_name + ""_norm"" + ""_bm"", ; mean_impute = True, center = True, normalize = True); bm_norm = BlockMatrix.read(out_dir + out_name + ""_norm"" + ""_bm""). # LD (unadjusted); starts_and_stops = hl.linalg.utils.locus_windows(mt.locus, radius = 2.1e6, _localize = False); bm_ld = (bm_norm @ bm_norm.T); bm_ld = BlockMatrix._from_java(bm_ld._jbm.filterRowIntervalsIR(Env.backend()._to_java_ir(starts_and_stops._ir), False)); bm_ld.write(out_dir + out_name + ""_LD"" + ""_bm"", overwrite = True); bm_ld = BlockMatrix.read(out_dir + out_name + ""_LD"" + ""_bm""). # Export LD matrices; list_range = [list(range(x.start_idx, x.end_idx + 1)) for x in list_meta[0:5]]; bms = [bm_ld.filter(x,x) for x in list_range]; hl.experimental.export_block_matrices(bms, out_dir + out_name + ""_tissue"" + ""_ld""). # Example image of problem:; <img width=""594"" alt=""Screen Shot 2019-06-13 at 5 36 58 PM"" src=""https://user-images.githubusercontent.com/24594616/59470325-52676800-8e05-11e9-93fe-e48c0e06e70b.png"">. If genotypes are normalized to N(0,1), then X @ X.T should never have values larger than 1 except for floating point precision. This is anecdotal, but I never had this problem when using > 100k samples, but here I'm using ~700 samples. I'm not sure what's causing this, but I had a conversation with @liameabbott a while ago about how one should normalize these matrices. His understanding was that hail normalizes by dividing by `sqrt(sum(x^2))` whereas one may prefer to divide `sd(x)`. The example he sent me to do this is below:. # Liam's example; g = BlockMatrix.read('gs://ukbb-ldsc-dev/1000_genomes.phase_3.europeans.GT.autosomes.bm'). n = g.shape[1]; ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6351
https://github.com/hail-is/hail/issues/6351:2335,Availability,error,error,2335,"e = True, center = True, normalize = True); bm_norm = BlockMatrix.read(out_dir + out_name + ""_norm"" + ""_bm""). # LD (unadjusted); starts_and_stops = hl.linalg.utils.locus_windows(mt.locus, radius = 2.1e6, _localize = False); bm_ld = (bm_norm @ bm_norm.T); bm_ld = BlockMatrix._from_java(bm_ld._jbm.filterRowIntervalsIR(Env.backend()._to_java_ir(starts_and_stops._ir), False)); bm_ld.write(out_dir + out_name + ""_LD"" + ""_bm"", overwrite = True); bm_ld = BlockMatrix.read(out_dir + out_name + ""_LD"" + ""_bm""). # Export LD matrices; list_range = [list(range(x.start_idx, x.end_idx + 1)) for x in list_meta[0:5]]; bms = [bm_ld.filter(x,x) for x in list_range]; hl.experimental.export_block_matrices(bms, out_dir + out_name + ""_tissue"" + ""_ld""). # Example image of problem:; <img width=""594"" alt=""Screen Shot 2019-06-13 at 5 36 58 PM"" src=""https://user-images.githubusercontent.com/24594616/59470325-52676800-8e05-11e9-93fe-e48c0e06e70b.png"">. If genotypes are normalized to N(0,1), then X @ X.T should never have values larger than 1 except for floating point precision. This is anecdotal, but I never had this problem when using > 100k samples, but here I'm using ~700 samples. I'm not sure what's causing this, but I had a conversation with @liameabbott a while ago about how one should normalize these matrices. His understanding was that hail normalizes by dividing by `sqrt(sum(x^2))` whereas one may prefer to divide `sd(x)`. The example he sent me to do this is below:. # Liam's example; g = BlockMatrix.read('gs://ukbb-ldsc-dev/1000_genomes.phase_3.europeans.GT.autosomes.bm'). n = g.shape[1]; m1 = g.sum(axis=1).cache(); m2 = (g**2).sum(axis=1).cache(). mean = m1 / n; stdev = ((m2-m1**2 / n) / (n-1)).sqrt(); g_std = ((g - mean) / stdev). g_std.write('gs://ukbb-ldsc-dev/1000_genomes.phase_3.europeans.GT_standardized.autosomes.bm', overwrite=True). I'll try this way of normalizing tomorrow to see if this is the root of the error and post back. Tagging @jbloom22 and @tpoterba 'cause why not :).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6351
https://github.com/hail-is/hail/issues/6351:2020,Performance,cache,cache,2020,"e = True, center = True, normalize = True); bm_norm = BlockMatrix.read(out_dir + out_name + ""_norm"" + ""_bm""). # LD (unadjusted); starts_and_stops = hl.linalg.utils.locus_windows(mt.locus, radius = 2.1e6, _localize = False); bm_ld = (bm_norm @ bm_norm.T); bm_ld = BlockMatrix._from_java(bm_ld._jbm.filterRowIntervalsIR(Env.backend()._to_java_ir(starts_and_stops._ir), False)); bm_ld.write(out_dir + out_name + ""_LD"" + ""_bm"", overwrite = True); bm_ld = BlockMatrix.read(out_dir + out_name + ""_LD"" + ""_bm""). # Export LD matrices; list_range = [list(range(x.start_idx, x.end_idx + 1)) for x in list_meta[0:5]]; bms = [bm_ld.filter(x,x) for x in list_range]; hl.experimental.export_block_matrices(bms, out_dir + out_name + ""_tissue"" + ""_ld""). # Example image of problem:; <img width=""594"" alt=""Screen Shot 2019-06-13 at 5 36 58 PM"" src=""https://user-images.githubusercontent.com/24594616/59470325-52676800-8e05-11e9-93fe-e48c0e06e70b.png"">. If genotypes are normalized to N(0,1), then X @ X.T should never have values larger than 1 except for floating point precision. This is anecdotal, but I never had this problem when using > 100k samples, but here I'm using ~700 samples. I'm not sure what's causing this, but I had a conversation with @liameabbott a while ago about how one should normalize these matrices. His understanding was that hail normalizes by dividing by `sqrt(sum(x^2))` whereas one may prefer to divide `sd(x)`. The example he sent me to do this is below:. # Liam's example; g = BlockMatrix.read('gs://ukbb-ldsc-dev/1000_genomes.phase_3.europeans.GT.autosomes.bm'). n = g.shape[1]; m1 = g.sum(axis=1).cache(); m2 = (g**2).sum(axis=1).cache(). mean = m1 / n; stdev = ((m2-m1**2 / n) / (n-1)).sqrt(); g_std = ((g - mean) / stdev). g_std.write('gs://ukbb-ldsc-dev/1000_genomes.phase_3.europeans.GT_standardized.autosomes.bm', overwrite=True). I'll try this way of normalizing tomorrow to see if this is the root of the error and post back. Tagging @jbloom22 and @tpoterba 'cause why not :).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6351
https://github.com/hail-is/hail/issues/6351:2053,Performance,cache,cache,2053,"e = True, center = True, normalize = True); bm_norm = BlockMatrix.read(out_dir + out_name + ""_norm"" + ""_bm""). # LD (unadjusted); starts_and_stops = hl.linalg.utils.locus_windows(mt.locus, radius = 2.1e6, _localize = False); bm_ld = (bm_norm @ bm_norm.T); bm_ld = BlockMatrix._from_java(bm_ld._jbm.filterRowIntervalsIR(Env.backend()._to_java_ir(starts_and_stops._ir), False)); bm_ld.write(out_dir + out_name + ""_LD"" + ""_bm"", overwrite = True); bm_ld = BlockMatrix.read(out_dir + out_name + ""_LD"" + ""_bm""). # Export LD matrices; list_range = [list(range(x.start_idx, x.end_idx + 1)) for x in list_meta[0:5]]; bms = [bm_ld.filter(x,x) for x in list_range]; hl.experimental.export_block_matrices(bms, out_dir + out_name + ""_tissue"" + ""_ld""). # Example image of problem:; <img width=""594"" alt=""Screen Shot 2019-06-13 at 5 36 58 PM"" src=""https://user-images.githubusercontent.com/24594616/59470325-52676800-8e05-11e9-93fe-e48c0e06e70b.png"">. If genotypes are normalized to N(0,1), then X @ X.T should never have values larger than 1 except for floating point precision. This is anecdotal, but I never had this problem when using > 100k samples, but here I'm using ~700 samples. I'm not sure what's causing this, but I had a conversation with @liameabbott a while ago about how one should normalize these matrices. His understanding was that hail normalizes by dividing by `sqrt(sum(x^2))` whereas one may prefer to divide `sd(x)`. The example he sent me to do this is below:. # Liam's example; g = BlockMatrix.read('gs://ukbb-ldsc-dev/1000_genomes.phase_3.europeans.GT.autosomes.bm'). n = g.shape[1]; m1 = g.sum(axis=1).cache(); m2 = (g**2).sum(axis=1).cache(). mean = m1 / n; stdev = ((m2-m1**2 / n) / (n-1)).sqrt(); g_std = ((g - mean) / stdev). g_std.write('gs://ukbb-ldsc-dev/1000_genomes.phase_3.europeans.GT_standardized.autosomes.bm', overwrite=True). I'll try this way of normalizing tomorrow to see if this is the root of the error and post back. Tagging @jbloom22 and @tpoterba 'cause why not :).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6351
https://github.com/hail-is/hail/pull/6355:28,Performance,optimiz,optimized,28,"This can probably be better optimized, but it is part of a larger problem with the notify children not happening atomically. Working on a better fix, but that will take some time. Figured this is an improvement.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6355
https://github.com/hail-is/hail/pull/6359:122,Safety,avoid,avoiding,122,Batch eval for tests by putting expressions in a tuple and using `Begin` for write statements. This helps dramatically by avoiding the c++ compilation time on every assert.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6359
https://github.com/hail-is/hail/pull/6359:15,Testability,test,tests,15,Batch eval for tests by putting expressions in a tuple and using `Begin` for write statements. This helps dramatically by avoiding the c++ compilation time on every assert.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6359
https://github.com/hail-is/hail/pull/6359:165,Testability,assert,assert,165,Batch eval for tests by putting expressions in a tuple and using `Begin` for write statements. This helps dramatically by avoiding the c++ compilation time on every assert.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6359
https://github.com/hail-is/hail/pull/6360:170,Integrability,interface,interface,170,"I rolled Jackie, but @tpoterba you should check you're happy with the code structure. Adds two hail experimental functions, `encode` and `decode`, which provide a python interface to the encoders and decoders in `RowStore.scala`:. ```; In [16]: import hail as hl ; ...: import hail.experimental as hle . In [17]: hle.encode(hl.literal(1)) ; Out[17]: ('Tuple[Int32]', b'\x07\x00\x00\x00\x02\x00\x00\x00 \x00\x01'). In [18]: hle.encode(hl.literal(1), 'defaultUncompressed') ; Out[18]: ('Tuple[Int32]', b'\x05\x00\x00\x00\x00\x01\x00\x00\x00'). In [19]: hle.encode(hl.literal(1), 'unblockedUncompressed') ; Out[19]: ('Tuple[Int32]', b'\x00\x01\x00\x00\x00'). In [20]: hle.encode(hl.literal([1,2,3,4,5,6])) ; Out[20]: ; ('Tuple[Array[Int32]]',; b'\x0e\x00\x00\x00\t\x00\x00\x00\x90\x00\x06\x00\x01\x02\x03\x04\x05\x06'). In [21]: hle.encode(hl.literal(1), 'defaultUncompressed') ; Out[21]: ('Tuple[Int32]', b'\x05\x00\x00\x00\x00\x01\x00\x00\x00'). In [22]: hle.encode(hl.literal(1), 'unblockedUncompressed') ; Out[22]: ('Tuple[Int32]', b'\x00\x01\x00\x00\x00'). In [23]: v = hl.struct(x=1) ; ...: hle.decode(v.dtype, *hle.encode(v)) ; Out[23]: Struct(x=1); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6360
https://github.com/hail-is/hail/pull/6361:88,Security,secur,securing,88,@cseed Can you take a look at the last commit and see if this is what you envisioned by securing the api calls with headers instead of cookies?. Stacked on #6288,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6361
https://github.com/hail-is/hail/pull/6366:182,Availability,reliab,reliably,182,"See discussion on Zulip https://hail.zulipchat.com/#narrow/stream/127527-team/topic/batch. Our worst case monthly cost moves from 40 USD to 4000 USD. However, PVCs seem to be rather reliably cleaned up now, so I am not overly concerned about this. We also have monitoring on PVC storage capacity.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6366
https://github.com/hail-is/hail/pull/6366:261,Energy Efficiency,monitor,monitoring,261,"See discussion on Zulip https://hail.zulipchat.com/#narrow/stream/127527-team/topic/batch. Our worst case monthly cost moves from 40 USD to 4000 USD. However, PVCs seem to be rather reliably cleaned up now, so I am not overly concerned about this. We also have monitoring on PVC storage capacity.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6366
https://github.com/hail-is/hail/pull/6367:15,Availability,error,error,15,"Addresses this error:; ```; ERROR | 2019-06-17 09:41:59,615 | web_protocol.py | log_exception:355 | Error handling request; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_protocol.py"", line 418, in start; resp = await task; File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_app.py"", line 458, in _handle; resp = await handler(request); File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_urldispatcher.py"", line 157, in handler_wrapper; result = await result; File ""/usr/local/lib/python3.6/dist-packages/batch/server/server.py"", line 849, in create_batch; await create_job(batch.id, userdata, job_params); File ""/usr/local/lib/python3.6/dist-packages/batch/server/server.py"", line 628, in create_job; pvc_size=pvc_size); File ""/usr/local/lib/python3.6/dist-packages/batch/server/server.py"", line 398, in create_job; await job._create_pod(); File ""/usr/local/lib/python3.6/dist-packages/batch/server/server.py"", line 190, in _create_pod; self._pvc_name = await self._create_pvc(); File ""/usr/local/lib/python3.6/dist-packages/batch/server/server.py"", line 165, in _create_pvc; await self.mark_complete(None, failed=True, failure_reason=str(err)); File ""/usr/local/lib/python3.6/dist-packages/batch/server/server.py"", line 526, in mark_complete; await self._mark_job_task_complete(task_name, pod_log, exit_code); File ""/usr/local/lib/python3.6/dist-packages/batch/server/server.py"", line 288, in _mark_job_task_complete; assert self._pod_name is not None; AssertionError; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6367
https://github.com/hail-is/hail/pull/6367:28,Availability,ERROR,ERROR,28,"Addresses this error:; ```; ERROR | 2019-06-17 09:41:59,615 | web_protocol.py | log_exception:355 | Error handling request; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_protocol.py"", line 418, in start; resp = await task; File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_app.py"", line 458, in _handle; resp = await handler(request); File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_urldispatcher.py"", line 157, in handler_wrapper; result = await result; File ""/usr/local/lib/python3.6/dist-packages/batch/server/server.py"", line 849, in create_batch; await create_job(batch.id, userdata, job_params); File ""/usr/local/lib/python3.6/dist-packages/batch/server/server.py"", line 628, in create_job; pvc_size=pvc_size); File ""/usr/local/lib/python3.6/dist-packages/batch/server/server.py"", line 398, in create_job; await job._create_pod(); File ""/usr/local/lib/python3.6/dist-packages/batch/server/server.py"", line 190, in _create_pod; self._pvc_name = await self._create_pvc(); File ""/usr/local/lib/python3.6/dist-packages/batch/server/server.py"", line 165, in _create_pvc; await self.mark_complete(None, failed=True, failure_reason=str(err)); File ""/usr/local/lib/python3.6/dist-packages/batch/server/server.py"", line 526, in mark_complete; await self._mark_job_task_complete(task_name, pod_log, exit_code); File ""/usr/local/lib/python3.6/dist-packages/batch/server/server.py"", line 288, in _mark_job_task_complete; assert self._pod_name is not None; AssertionError; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6367
https://github.com/hail-is/hail/pull/6367:100,Availability,Error,Error,100,"Addresses this error:; ```; ERROR | 2019-06-17 09:41:59,615 | web_protocol.py | log_exception:355 | Error handling request; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_protocol.py"", line 418, in start; resp = await task; File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_app.py"", line 458, in _handle; resp = await handler(request); File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_urldispatcher.py"", line 157, in handler_wrapper; result = await result; File ""/usr/local/lib/python3.6/dist-packages/batch/server/server.py"", line 849, in create_batch; await create_job(batch.id, userdata, job_params); File ""/usr/local/lib/python3.6/dist-packages/batch/server/server.py"", line 628, in create_job; pvc_size=pvc_size); File ""/usr/local/lib/python3.6/dist-packages/batch/server/server.py"", line 398, in create_job; await job._create_pod(); File ""/usr/local/lib/python3.6/dist-packages/batch/server/server.py"", line 190, in _create_pod; self._pvc_name = await self._create_pvc(); File ""/usr/local/lib/python3.6/dist-packages/batch/server/server.py"", line 165, in _create_pvc; await self.mark_complete(None, failed=True, failure_reason=str(err)); File ""/usr/local/lib/python3.6/dist-packages/batch/server/server.py"", line 526, in mark_complete; await self._mark_job_task_complete(task_name, pod_log, exit_code); File ""/usr/local/lib/python3.6/dist-packages/batch/server/server.py"", line 288, in _mark_job_task_complete; assert self._pod_name is not None; AssertionError; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6367
https://github.com/hail-is/hail/pull/6367:1478,Testability,assert,assert,1478,"Addresses this error:; ```; ERROR | 2019-06-17 09:41:59,615 | web_protocol.py | log_exception:355 | Error handling request; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_protocol.py"", line 418, in start; resp = await task; File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_app.py"", line 458, in _handle; resp = await handler(request); File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_urldispatcher.py"", line 157, in handler_wrapper; result = await result; File ""/usr/local/lib/python3.6/dist-packages/batch/server/server.py"", line 849, in create_batch; await create_job(batch.id, userdata, job_params); File ""/usr/local/lib/python3.6/dist-packages/batch/server/server.py"", line 628, in create_job; pvc_size=pvc_size); File ""/usr/local/lib/python3.6/dist-packages/batch/server/server.py"", line 398, in create_job; await job._create_pod(); File ""/usr/local/lib/python3.6/dist-packages/batch/server/server.py"", line 190, in _create_pod; self._pvc_name = await self._create_pvc(); File ""/usr/local/lib/python3.6/dist-packages/batch/server/server.py"", line 165, in _create_pvc; await self.mark_complete(None, failed=True, failure_reason=str(err)); File ""/usr/local/lib/python3.6/dist-packages/batch/server/server.py"", line 526, in mark_complete; await self._mark_job_task_complete(task_name, pod_log, exit_code); File ""/usr/local/lib/python3.6/dist-packages/batch/server/server.py"", line 288, in _mark_job_task_complete; assert self._pod_name is not None; AssertionError; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6367
https://github.com/hail-is/hail/pull/6367:1513,Testability,Assert,AssertionError,1513,"Addresses this error:; ```; ERROR | 2019-06-17 09:41:59,615 | web_protocol.py | log_exception:355 | Error handling request; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_protocol.py"", line 418, in start; resp = await task; File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_app.py"", line 458, in _handle; resp = await handler(request); File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_urldispatcher.py"", line 157, in handler_wrapper; result = await result; File ""/usr/local/lib/python3.6/dist-packages/batch/server/server.py"", line 849, in create_batch; await create_job(batch.id, userdata, job_params); File ""/usr/local/lib/python3.6/dist-packages/batch/server/server.py"", line 628, in create_job; pvc_size=pvc_size); File ""/usr/local/lib/python3.6/dist-packages/batch/server/server.py"", line 398, in create_job; await job._create_pod(); File ""/usr/local/lib/python3.6/dist-packages/batch/server/server.py"", line 190, in _create_pod; self._pvc_name = await self._create_pvc(); File ""/usr/local/lib/python3.6/dist-packages/batch/server/server.py"", line 165, in _create_pvc; await self.mark_complete(None, failed=True, failure_reason=str(err)); File ""/usr/local/lib/python3.6/dist-packages/batch/server/server.py"", line 526, in mark_complete; await self._mark_job_task_complete(task_name, pod_log, exit_code); File ""/usr/local/lib/python3.6/dist-packages/batch/server/server.py"", line 288, in _mark_job_task_complete; assert self._pod_name is not None; AssertionError; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6367
https://github.com/hail-is/hail/pull/6377:27,Deployability,deploy,deploy,27,I think we'll need to hand deploy this fix and restart ci.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6377
https://github.com/hail-is/hail/pull/6399:8,Deployability,update,update,8,Need to update the GitHub web hook once this goes in.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6399
https://github.com/hail-is/hail/pull/6405:50,Usability,simpl,simply,50,"Previously the below would display `0.GT | 1.GT` (simply the index in the entries array). ```; In [2]: import hail as hl ; ...: mt = hl.balding_nichols_model(3, 100, 100) ; ...: mt = mt.key_cols_by(sample_id = 'sample-' + hl.str(mt.sample_idx)) ; ...: mt.show(n_rows=10, n_cols=10) ; 2019-06-19 16:20:33 Hail: INFO: balding_nichols_model: generating genotypes for 3 populations, 100 samples, and 100 variants...; +---------------+------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+; | locus | alleles | sample-0.GT | sample-1.GT | sample-2.GT | sample-3.GT | sample-4.GT | sample-5.GT | sample-6.GT | sample-7.GT | sample-8.GT | sample-9.GT |; +---------------+------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+; | locus<GRCh37> | array<str> | call | call | call | call | call | call | call | call | call | call |; +---------------+------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+; | 1:1 | [""A"",""C""] | 0/0 | 0/0 | 0/1 | 1/1 | 0/1 | 0/0 | 0/1 | 0/1 | 0/1 | 0/1 |; | 1:2 | [""A"",""C""] | 1/1 | 0/1 | 0/0 | 0/1 | 1/1 | 1/1 | 0/1 | 1/1 | 0/0 | 1/1 |; | 1:3 | [""A"",""C""] | 1/1 | 0/1 | 0/0 | 1/1 | 0/0 | 1/1 | 1/1 | 0/1 | 0/0 | 1/1 |; | 1:4 | [""A"",""C""] | 0/0 | 0/1 | 0/0 | 1/1 | 0/1 | 0/0 | 0/1 | 0/0 | 0/0 | 0/1 |; | 1:5 | [""A"",""C""] | 0/1 | 0/0 | 0/0 | 0/0 | 0/1 | 0/0 | 0/1 | 0/1 | 0/0 | 1/1 |; | 1:6 | [""A"",""C""] | 0/0 | 1/1 | 1/1 | 0/1 | 0/1 | 1/1 | 0/1 | 0/1 | 1/1 | 1/1 |; | 1:7 | [""A"",""C""] | 0/0 | 0/1 | 1/1 | 0/1 | 0/1 | 0/1 | 0/0 | 0/1 | 0/0 | 0/1 |; | 1:8 | [""A"",""C""] | 0/1 | 0/1 | 1/1 | 1/1 | 0/1 | 0/1 | 1/1 | 1/1 | 0/1 | 0/1 |; | 1:9 | [""A"",""C""] | 0/0 | 0/1 | 1/1 | 1/1 | 1/1 | 0/1 | 1/1 | 1/1 | 1/1 | 1/1 |; | 1:10 | [""A"",""C""] | 0/1 | 0/1 | 0/1 | 0/0 | 0/1 | 0/0 | 0/1 | 0/1 | 0/0 | 0/1 |",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6405
https://github.com/hail-is/hail/pull/6413:116,Energy Efficiency,monitor,monitoring,116,"This PR sets up Elasticsearch, Kibana, and Fluentd on the Kubernetes cluster, allowing us to go to internal.hail.is/monitoring/kibana and look at logs from the cluster.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6413
https://github.com/hail-is/hail/pull/6413:146,Testability,log,logs,146,"This PR sets up Elasticsearch, Kibana, and Fluentd on the Kubernetes cluster, allowing us to go to internal.hail.is/monitoring/kibana and look at logs from the cluster.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6413
https://github.com/hail-is/hail/pull/6415:9,Modifiability,extend,extend,9,"Does not extend into modifying TableValue, as this affects the IR, relates to the boundary you spoke of I believe, and will affect much more besides IBD. Happy to start digging into that in this PR or the next PR if desired. cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6415
https://github.com/hail-is/hail/pull/6416:9,Modifiability,extend,extend,9,"Does not extend into modifying TableValue, as this affects the IR, relates to the boundary you spoke of I believe, and will affect much more besides IBD. Happy to start digging into that in this PR or the next PR if desired. cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6416
https://github.com/hail-is/hail/issues/6417:456,Modifiability,enhance,enhance,456,"See #6370 . > Could you open an issue, to explore changing this to a header-specified token, or randomizing the name field.; > ; > https://security.stackexchange.com/questions/211352/does-owasp-recommend-to-include-a-csrf-token-in-a-header-or-to-use-it-as-a-param; > ; > Need to take care with logging in this case.; > https://github.com/OWASP/CheatSheetSeries/blob/master/cheatsheets/Cross-Site_Request_Forgery_Prevention_Cheat_Sheet.md; > ; > To further enhance the security of this proposed design, consider randomizing the CSRF token parameter name and/or value for each request. Implementing this approach results in the generation of per-request tokens as opposed to per-session tokens.; > doing both seems identical to implementing 2 CSRF tokens",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6417
https://github.com/hail-is/hail/issues/6417:139,Security,secur,security,139,"See #6370 . > Could you open an issue, to explore changing this to a header-specified token, or randomizing the name field.; > ; > https://security.stackexchange.com/questions/211352/does-owasp-recommend-to-include-a-csrf-token-in-a-header-or-to-use-it-as-a-param; > ; > Need to take care with logging in this case.; > https://github.com/OWASP/CheatSheetSeries/blob/master/cheatsheets/Cross-Site_Request_Forgery_Prevention_Cheat_Sheet.md; > ; > To further enhance the security of this proposed design, consider randomizing the CSRF token parameter name and/or value for each request. Implementing this approach results in the generation of per-request tokens as opposed to per-session tokens.; > doing both seems identical to implementing 2 CSRF tokens",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6417
https://github.com/hail-is/hail/issues/6417:468,Security,secur,security,468,"See #6370 . > Could you open an issue, to explore changing this to a header-specified token, or randomizing the name field.; > ; > https://security.stackexchange.com/questions/211352/does-owasp-recommend-to-include-a-csrf-token-in-a-header-or-to-use-it-as-a-param; > ; > Need to take care with logging in this case.; > https://github.com/OWASP/CheatSheetSeries/blob/master/cheatsheets/Cross-Site_Request_Forgery_Prevention_Cheat_Sheet.md; > ; > To further enhance the security of this proposed design, consider randomizing the CSRF token parameter name and/or value for each request. Implementing this approach results in the generation of per-request tokens as opposed to per-session tokens.; > doing both seems identical to implementing 2 CSRF tokens",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6417
https://github.com/hail-is/hail/issues/6417:294,Testability,log,logging,294,"See #6370 . > Could you open an issue, to explore changing this to a header-specified token, or randomizing the name field.; > ; > https://security.stackexchange.com/questions/211352/does-owasp-recommend-to-include-a-csrf-token-in-a-header-or-to-use-it-as-a-param; > ; > Need to take care with logging in this case.; > https://github.com/OWASP/CheatSheetSeries/blob/master/cheatsheets/Cross-Site_Request_Forgery_Prevention_Cheat_Sheet.md; > ; > To further enhance the security of this proposed design, consider randomizing the CSRF token parameter name and/or value for each request. Implementing this approach results in the generation of per-request tokens as opposed to per-session tokens.; > doing both seems identical to implementing 2 CSRF tokens",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6417
https://github.com/hail-is/hail/pull/6418:155,Testability,test,test,155,Adds a job token to track jobs submitted by a client. I think this *should* fix the hanging issue we've been seeing on the ci with the distributed backend test.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6418
https://github.com/hail-is/hail/pull/6425:37,Availability,Error,Errors,37,"Tests not passing, not sure why yet. Errors are MatchError. Stacked on #6421, will see if I can unwind; ```sh; > Task :test; Running test: Test method bitPackedVectorCorrectWhenOffsetNotZero(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.bitPackedVectorCorrectWhenOffsetNotZero PASSED; Running test: Test method testBitPackUnpack(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testBitPackUnpack FAILED; scala.MatchError at LocalLDPruneSuite.scala:222; Running test: Test method testIsLocallyUncorrelated(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testIsLocallyUncorrelated FAILED; org.apache.spark.SparkException at LocalLDPruneSuite.scala:214; Caused by: scala.MatchError; Running test: Test method testR2(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testR2 FAILED; scala.MatchError at LocalLDPruneSuite.scala:244; Running test: Test method testRandom(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testRandom FAILED; java.lang.AssertionError at LocalLDPruneSuite.scala:323; Caused by: scala.MatchError at LocalLDPruneSuite.scala:323; ```. cc @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6425
https://github.com/hail-is/hail/pull/6425:0,Testability,Test,Tests,0,"Tests not passing, not sure why yet. Errors are MatchError. Stacked on #6421, will see if I can unwind; ```sh; > Task :test; Running test: Test method bitPackedVectorCorrectWhenOffsetNotZero(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.bitPackedVectorCorrectWhenOffsetNotZero PASSED; Running test: Test method testBitPackUnpack(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testBitPackUnpack FAILED; scala.MatchError at LocalLDPruneSuite.scala:222; Running test: Test method testIsLocallyUncorrelated(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testIsLocallyUncorrelated FAILED; org.apache.spark.SparkException at LocalLDPruneSuite.scala:214; Caused by: scala.MatchError; Running test: Test method testR2(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testR2 FAILED; scala.MatchError at LocalLDPruneSuite.scala:244; Running test: Test method testRandom(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testRandom FAILED; java.lang.AssertionError at LocalLDPruneSuite.scala:323; Caused by: scala.MatchError at LocalLDPruneSuite.scala:323; ```. cc @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6425
https://github.com/hail-is/hail/pull/6425:119,Testability,test,test,119,"Tests not passing, not sure why yet. Errors are MatchError. Stacked on #6421, will see if I can unwind; ```sh; > Task :test; Running test: Test method bitPackedVectorCorrectWhenOffsetNotZero(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.bitPackedVectorCorrectWhenOffsetNotZero PASSED; Running test: Test method testBitPackUnpack(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testBitPackUnpack FAILED; scala.MatchError at LocalLDPruneSuite.scala:222; Running test: Test method testIsLocallyUncorrelated(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testIsLocallyUncorrelated FAILED; org.apache.spark.SparkException at LocalLDPruneSuite.scala:214; Caused by: scala.MatchError; Running test: Test method testR2(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testR2 FAILED; scala.MatchError at LocalLDPruneSuite.scala:244; Running test: Test method testRandom(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testRandom FAILED; java.lang.AssertionError at LocalLDPruneSuite.scala:323; Caused by: scala.MatchError at LocalLDPruneSuite.scala:323; ```. cc @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6425
https://github.com/hail-is/hail/pull/6425:133,Testability,test,test,133,"Tests not passing, not sure why yet. Errors are MatchError. Stacked on #6421, will see if I can unwind; ```sh; > Task :test; Running test: Test method bitPackedVectorCorrectWhenOffsetNotZero(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.bitPackedVectorCorrectWhenOffsetNotZero PASSED; Running test: Test method testBitPackUnpack(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testBitPackUnpack FAILED; scala.MatchError at LocalLDPruneSuite.scala:222; Running test: Test method testIsLocallyUncorrelated(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testIsLocallyUncorrelated FAILED; org.apache.spark.SparkException at LocalLDPruneSuite.scala:214; Caused by: scala.MatchError; Running test: Test method testR2(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testR2 FAILED; scala.MatchError at LocalLDPruneSuite.scala:244; Running test: Test method testRandom(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testRandom FAILED; java.lang.AssertionError at LocalLDPruneSuite.scala:323; Caused by: scala.MatchError at LocalLDPruneSuite.scala:323; ```. cc @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6425
https://github.com/hail-is/hail/pull/6425:139,Testability,Test,Test,139,"Tests not passing, not sure why yet. Errors are MatchError. Stacked on #6421, will see if I can unwind; ```sh; > Task :test; Running test: Test method bitPackedVectorCorrectWhenOffsetNotZero(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.bitPackedVectorCorrectWhenOffsetNotZero PASSED; Running test: Test method testBitPackUnpack(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testBitPackUnpack FAILED; scala.MatchError at LocalLDPruneSuite.scala:222; Running test: Test method testIsLocallyUncorrelated(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testIsLocallyUncorrelated FAILED; org.apache.spark.SparkException at LocalLDPruneSuite.scala:214; Caused by: scala.MatchError; Running test: Test method testR2(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testR2 FAILED; scala.MatchError at LocalLDPruneSuite.scala:244; Running test: Test method testRandom(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testRandom FAILED; java.lang.AssertionError at LocalLDPruneSuite.scala:323; Caused by: scala.MatchError at LocalLDPruneSuite.scala:323; ```. cc @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6425
https://github.com/hail-is/hail/pull/6425:249,Testability,test,test,249,"Tests not passing, not sure why yet. Errors are MatchError. Stacked on #6421, will see if I can unwind; ```sh; > Task :test; Running test: Test method bitPackedVectorCorrectWhenOffsetNotZero(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.bitPackedVectorCorrectWhenOffsetNotZero PASSED; Running test: Test method testBitPackUnpack(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testBitPackUnpack FAILED; scala.MatchError at LocalLDPruneSuite.scala:222; Running test: Test method testIsLocallyUncorrelated(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testIsLocallyUncorrelated FAILED; org.apache.spark.SparkException at LocalLDPruneSuite.scala:214; Caused by: scala.MatchError; Running test: Test method testR2(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testR2 FAILED; scala.MatchError at LocalLDPruneSuite.scala:244; Running test: Test method testRandom(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testRandom FAILED; java.lang.AssertionError at LocalLDPruneSuite.scala:323; Caused by: scala.MatchError at LocalLDPruneSuite.scala:323; ```. cc @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6425
https://github.com/hail-is/hail/pull/6425:346,Testability,test,test,346,"Tests not passing, not sure why yet. Errors are MatchError. Stacked on #6421, will see if I can unwind; ```sh; > Task :test; Running test: Test method bitPackedVectorCorrectWhenOffsetNotZero(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.bitPackedVectorCorrectWhenOffsetNotZero PASSED; Running test: Test method testBitPackUnpack(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testBitPackUnpack FAILED; scala.MatchError at LocalLDPruneSuite.scala:222; Running test: Test method testIsLocallyUncorrelated(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testIsLocallyUncorrelated FAILED; org.apache.spark.SparkException at LocalLDPruneSuite.scala:214; Caused by: scala.MatchError; Running test: Test method testR2(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testR2 FAILED; scala.MatchError at LocalLDPruneSuite.scala:244; Running test: Test method testRandom(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testRandom FAILED; java.lang.AssertionError at LocalLDPruneSuite.scala:323; Caused by: scala.MatchError at LocalLDPruneSuite.scala:323; ```. cc @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6425
https://github.com/hail-is/hail/pull/6425:352,Testability,Test,Test,352,"Tests not passing, not sure why yet. Errors are MatchError. Stacked on #6421, will see if I can unwind; ```sh; > Task :test; Running test: Test method bitPackedVectorCorrectWhenOffsetNotZero(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.bitPackedVectorCorrectWhenOffsetNotZero PASSED; Running test: Test method testBitPackUnpack(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testBitPackUnpack FAILED; scala.MatchError at LocalLDPruneSuite.scala:222; Running test: Test method testIsLocallyUncorrelated(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testIsLocallyUncorrelated FAILED; org.apache.spark.SparkException at LocalLDPruneSuite.scala:214; Caused by: scala.MatchError; Running test: Test method testR2(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testR2 FAILED; scala.MatchError at LocalLDPruneSuite.scala:244; Running test: Test method testRandom(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testRandom FAILED; java.lang.AssertionError at LocalLDPruneSuite.scala:323; Caused by: scala.MatchError at LocalLDPruneSuite.scala:323; ```. cc @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6425
https://github.com/hail-is/hail/pull/6425:364,Testability,test,testBitPackUnpack,364,"Tests not passing, not sure why yet. Errors are MatchError. Stacked on #6421, will see if I can unwind; ```sh; > Task :test; Running test: Test method bitPackedVectorCorrectWhenOffsetNotZero(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.bitPackedVectorCorrectWhenOffsetNotZero PASSED; Running test: Test method testBitPackUnpack(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testBitPackUnpack FAILED; scala.MatchError at LocalLDPruneSuite.scala:222; Running test: Test method testIsLocallyUncorrelated(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testIsLocallyUncorrelated FAILED; org.apache.spark.SparkException at LocalLDPruneSuite.scala:214; Caused by: scala.MatchError; Running test: Test method testR2(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testR2 FAILED; scala.MatchError at LocalLDPruneSuite.scala:244; Running test: Test method testRandom(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testRandom FAILED; java.lang.AssertionError at LocalLDPruneSuite.scala:323; Caused by: scala.MatchError at LocalLDPruneSuite.scala:323; ```. cc @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6425
https://github.com/hail-is/hail/pull/6425:440,Testability,test,test,440,"Tests not passing, not sure why yet. Errors are MatchError. Stacked on #6421, will see if I can unwind; ```sh; > Task :test; Running test: Test method bitPackedVectorCorrectWhenOffsetNotZero(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.bitPackedVectorCorrectWhenOffsetNotZero PASSED; Running test: Test method testBitPackUnpack(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testBitPackUnpack FAILED; scala.MatchError at LocalLDPruneSuite.scala:222; Running test: Test method testIsLocallyUncorrelated(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testIsLocallyUncorrelated FAILED; org.apache.spark.SparkException at LocalLDPruneSuite.scala:214; Caused by: scala.MatchError; Running test: Test method testR2(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testR2 FAILED; scala.MatchError at LocalLDPruneSuite.scala:244; Running test: Test method testRandom(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testRandom FAILED; java.lang.AssertionError at LocalLDPruneSuite.scala:323; Caused by: scala.MatchError at LocalLDPruneSuite.scala:323; ```. cc @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6425
https://github.com/hail-is/hail/pull/6425:481,Testability,test,testBitPackUnpack,481,"Tests not passing, not sure why yet. Errors are MatchError. Stacked on #6421, will see if I can unwind; ```sh; > Task :test; Running test: Test method bitPackedVectorCorrectWhenOffsetNotZero(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.bitPackedVectorCorrectWhenOffsetNotZero PASSED; Running test: Test method testBitPackUnpack(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testBitPackUnpack FAILED; scala.MatchError at LocalLDPruneSuite.scala:222; Running test: Test method testIsLocallyUncorrelated(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testIsLocallyUncorrelated FAILED; org.apache.spark.SparkException at LocalLDPruneSuite.scala:214; Caused by: scala.MatchError; Running test: Test method testR2(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testR2 FAILED; scala.MatchError at LocalLDPruneSuite.scala:244; Running test: Test method testRandom(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testRandom FAILED; java.lang.AssertionError at LocalLDPruneSuite.scala:323; Caused by: scala.MatchError at LocalLDPruneSuite.scala:323; ```. cc @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6425
https://github.com/hail-is/hail/pull/6425:564,Testability,test,test,564,"Tests not passing, not sure why yet. Errors are MatchError. Stacked on #6421, will see if I can unwind; ```sh; > Task :test; Running test: Test method bitPackedVectorCorrectWhenOffsetNotZero(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.bitPackedVectorCorrectWhenOffsetNotZero PASSED; Running test: Test method testBitPackUnpack(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testBitPackUnpack FAILED; scala.MatchError at LocalLDPruneSuite.scala:222; Running test: Test method testIsLocallyUncorrelated(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testIsLocallyUncorrelated FAILED; org.apache.spark.SparkException at LocalLDPruneSuite.scala:214; Caused by: scala.MatchError; Running test: Test method testR2(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testR2 FAILED; scala.MatchError at LocalLDPruneSuite.scala:244; Running test: Test method testRandom(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testRandom FAILED; java.lang.AssertionError at LocalLDPruneSuite.scala:323; Caused by: scala.MatchError at LocalLDPruneSuite.scala:323; ```. cc @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6425
https://github.com/hail-is/hail/pull/6425:570,Testability,Test,Test,570,"Tests not passing, not sure why yet. Errors are MatchError. Stacked on #6421, will see if I can unwind; ```sh; > Task :test; Running test: Test method bitPackedVectorCorrectWhenOffsetNotZero(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.bitPackedVectorCorrectWhenOffsetNotZero PASSED; Running test: Test method testBitPackUnpack(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testBitPackUnpack FAILED; scala.MatchError at LocalLDPruneSuite.scala:222; Running test: Test method testIsLocallyUncorrelated(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testIsLocallyUncorrelated FAILED; org.apache.spark.SparkException at LocalLDPruneSuite.scala:214; Caused by: scala.MatchError; Running test: Test method testR2(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testR2 FAILED; scala.MatchError at LocalLDPruneSuite.scala:244; Running test: Test method testRandom(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testRandom FAILED; java.lang.AssertionError at LocalLDPruneSuite.scala:323; Caused by: scala.MatchError at LocalLDPruneSuite.scala:323; ```. cc @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6425
https://github.com/hail-is/hail/pull/6425:582,Testability,test,testIsLocallyUncorrelated,582,"Tests not passing, not sure why yet. Errors are MatchError. Stacked on #6421, will see if I can unwind; ```sh; > Task :test; Running test: Test method bitPackedVectorCorrectWhenOffsetNotZero(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.bitPackedVectorCorrectWhenOffsetNotZero PASSED; Running test: Test method testBitPackUnpack(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testBitPackUnpack FAILED; scala.MatchError at LocalLDPruneSuite.scala:222; Running test: Test method testIsLocallyUncorrelated(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testIsLocallyUncorrelated FAILED; org.apache.spark.SparkException at LocalLDPruneSuite.scala:214; Caused by: scala.MatchError; Running test: Test method testR2(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testR2 FAILED; scala.MatchError at LocalLDPruneSuite.scala:244; Running test: Test method testRandom(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testRandom FAILED; java.lang.AssertionError at LocalLDPruneSuite.scala:323; Caused by: scala.MatchError at LocalLDPruneSuite.scala:323; ```. cc @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6425
https://github.com/hail-is/hail/pull/6425:666,Testability,test,test,666,"Tests not passing, not sure why yet. Errors are MatchError. Stacked on #6421, will see if I can unwind; ```sh; > Task :test; Running test: Test method bitPackedVectorCorrectWhenOffsetNotZero(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.bitPackedVectorCorrectWhenOffsetNotZero PASSED; Running test: Test method testBitPackUnpack(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testBitPackUnpack FAILED; scala.MatchError at LocalLDPruneSuite.scala:222; Running test: Test method testIsLocallyUncorrelated(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testIsLocallyUncorrelated FAILED; org.apache.spark.SparkException at LocalLDPruneSuite.scala:214; Caused by: scala.MatchError; Running test: Test method testR2(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testR2 FAILED; scala.MatchError at LocalLDPruneSuite.scala:244; Running test: Test method testRandom(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testRandom FAILED; java.lang.AssertionError at LocalLDPruneSuite.scala:323; Caused by: scala.MatchError at LocalLDPruneSuite.scala:323; ```. cc @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6425
https://github.com/hail-is/hail/pull/6425:707,Testability,test,testIsLocallyUncorrelated,707,"Tests not passing, not sure why yet. Errors are MatchError. Stacked on #6421, will see if I can unwind; ```sh; > Task :test; Running test: Test method bitPackedVectorCorrectWhenOffsetNotZero(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.bitPackedVectorCorrectWhenOffsetNotZero PASSED; Running test: Test method testBitPackUnpack(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testBitPackUnpack FAILED; scala.MatchError at LocalLDPruneSuite.scala:222; Running test: Test method testIsLocallyUncorrelated(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testIsLocallyUncorrelated FAILED; org.apache.spark.SparkException at LocalLDPruneSuite.scala:214; Caused by: scala.MatchError; Running test: Test method testR2(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testR2 FAILED; scala.MatchError at LocalLDPruneSuite.scala:244; Running test: Test method testRandom(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testRandom FAILED; java.lang.AssertionError at LocalLDPruneSuite.scala:323; Caused by: scala.MatchError at LocalLDPruneSuite.scala:323; ```. cc @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6425
https://github.com/hail-is/hail/pull/6425:842,Testability,test,test,842,"Tests not passing, not sure why yet. Errors are MatchError. Stacked on #6421, will see if I can unwind; ```sh; > Task :test; Running test: Test method bitPackedVectorCorrectWhenOffsetNotZero(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.bitPackedVectorCorrectWhenOffsetNotZero PASSED; Running test: Test method testBitPackUnpack(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testBitPackUnpack FAILED; scala.MatchError at LocalLDPruneSuite.scala:222; Running test: Test method testIsLocallyUncorrelated(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testIsLocallyUncorrelated FAILED; org.apache.spark.SparkException at LocalLDPruneSuite.scala:214; Caused by: scala.MatchError; Running test: Test method testR2(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testR2 FAILED; scala.MatchError at LocalLDPruneSuite.scala:244; Running test: Test method testRandom(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testRandom FAILED; java.lang.AssertionError at LocalLDPruneSuite.scala:323; Caused by: scala.MatchError at LocalLDPruneSuite.scala:323; ```. cc @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6425
https://github.com/hail-is/hail/pull/6425:848,Testability,Test,Test,848,"Tests not passing, not sure why yet. Errors are MatchError. Stacked on #6421, will see if I can unwind; ```sh; > Task :test; Running test: Test method bitPackedVectorCorrectWhenOffsetNotZero(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.bitPackedVectorCorrectWhenOffsetNotZero PASSED; Running test: Test method testBitPackUnpack(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testBitPackUnpack FAILED; scala.MatchError at LocalLDPruneSuite.scala:222; Running test: Test method testIsLocallyUncorrelated(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testIsLocallyUncorrelated FAILED; org.apache.spark.SparkException at LocalLDPruneSuite.scala:214; Caused by: scala.MatchError; Running test: Test method testR2(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testR2 FAILED; scala.MatchError at LocalLDPruneSuite.scala:244; Running test: Test method testRandom(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testRandom FAILED; java.lang.AssertionError at LocalLDPruneSuite.scala:323; Caused by: scala.MatchError at LocalLDPruneSuite.scala:323; ```. cc @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6425
https://github.com/hail-is/hail/pull/6425:925,Testability,test,test,925,"Tests not passing, not sure why yet. Errors are MatchError. Stacked on #6421, will see if I can unwind; ```sh; > Task :test; Running test: Test method bitPackedVectorCorrectWhenOffsetNotZero(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.bitPackedVectorCorrectWhenOffsetNotZero PASSED; Running test: Test method testBitPackUnpack(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testBitPackUnpack FAILED; scala.MatchError at LocalLDPruneSuite.scala:222; Running test: Test method testIsLocallyUncorrelated(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testIsLocallyUncorrelated FAILED; org.apache.spark.SparkException at LocalLDPruneSuite.scala:214; Caused by: scala.MatchError; Running test: Test method testR2(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testR2 FAILED; scala.MatchError at LocalLDPruneSuite.scala:244; Running test: Test method testRandom(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testRandom FAILED; java.lang.AssertionError at LocalLDPruneSuite.scala:323; Caused by: scala.MatchError at LocalLDPruneSuite.scala:323; ```. cc @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6425
https://github.com/hail-is/hail/pull/6425:1038,Testability,test,test,1038,"Tests not passing, not sure why yet. Errors are MatchError. Stacked on #6421, will see if I can unwind; ```sh; > Task :test; Running test: Test method bitPackedVectorCorrectWhenOffsetNotZero(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.bitPackedVectorCorrectWhenOffsetNotZero PASSED; Running test: Test method testBitPackUnpack(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testBitPackUnpack FAILED; scala.MatchError at LocalLDPruneSuite.scala:222; Running test: Test method testIsLocallyUncorrelated(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testIsLocallyUncorrelated FAILED; org.apache.spark.SparkException at LocalLDPruneSuite.scala:214; Caused by: scala.MatchError; Running test: Test method testR2(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testR2 FAILED; scala.MatchError at LocalLDPruneSuite.scala:244; Running test: Test method testRandom(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testRandom FAILED; java.lang.AssertionError at LocalLDPruneSuite.scala:323; Caused by: scala.MatchError at LocalLDPruneSuite.scala:323; ```. cc @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6425
https://github.com/hail-is/hail/pull/6425:1044,Testability,Test,Test,1044,"Tests not passing, not sure why yet. Errors are MatchError. Stacked on #6421, will see if I can unwind; ```sh; > Task :test; Running test: Test method bitPackedVectorCorrectWhenOffsetNotZero(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.bitPackedVectorCorrectWhenOffsetNotZero PASSED; Running test: Test method testBitPackUnpack(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testBitPackUnpack FAILED; scala.MatchError at LocalLDPruneSuite.scala:222; Running test: Test method testIsLocallyUncorrelated(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testIsLocallyUncorrelated FAILED; org.apache.spark.SparkException at LocalLDPruneSuite.scala:214; Caused by: scala.MatchError; Running test: Test method testR2(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testR2 FAILED; scala.MatchError at LocalLDPruneSuite.scala:244; Running test: Test method testRandom(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testRandom FAILED; java.lang.AssertionError at LocalLDPruneSuite.scala:323; Caused by: scala.MatchError at LocalLDPruneSuite.scala:323; ```. cc @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6425
https://github.com/hail-is/hail/pull/6425:1056,Testability,test,testRandom,1056,"Tests not passing, not sure why yet. Errors are MatchError. Stacked on #6421, will see if I can unwind; ```sh; > Task :test; Running test: Test method bitPackedVectorCorrectWhenOffsetNotZero(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.bitPackedVectorCorrectWhenOffsetNotZero PASSED; Running test: Test method testBitPackUnpack(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testBitPackUnpack FAILED; scala.MatchError at LocalLDPruneSuite.scala:222; Running test: Test method testIsLocallyUncorrelated(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testIsLocallyUncorrelated FAILED; org.apache.spark.SparkException at LocalLDPruneSuite.scala:214; Caused by: scala.MatchError; Running test: Test method testR2(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testR2 FAILED; scala.MatchError at LocalLDPruneSuite.scala:244; Running test: Test method testRandom(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testRandom FAILED; java.lang.AssertionError at LocalLDPruneSuite.scala:323; Caused by: scala.MatchError at LocalLDPruneSuite.scala:323; ```. cc @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6425
https://github.com/hail-is/hail/pull/6425:1125,Testability,test,test,1125,"Tests not passing, not sure why yet. Errors are MatchError. Stacked on #6421, will see if I can unwind; ```sh; > Task :test; Running test: Test method bitPackedVectorCorrectWhenOffsetNotZero(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.bitPackedVectorCorrectWhenOffsetNotZero PASSED; Running test: Test method testBitPackUnpack(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testBitPackUnpack FAILED; scala.MatchError at LocalLDPruneSuite.scala:222; Running test: Test method testIsLocallyUncorrelated(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testIsLocallyUncorrelated FAILED; org.apache.spark.SparkException at LocalLDPruneSuite.scala:214; Caused by: scala.MatchError; Running test: Test method testR2(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testR2 FAILED; scala.MatchError at LocalLDPruneSuite.scala:244; Running test: Test method testRandom(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testRandom FAILED; java.lang.AssertionError at LocalLDPruneSuite.scala:323; Caused by: scala.MatchError at LocalLDPruneSuite.scala:323; ```. cc @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6425
https://github.com/hail-is/hail/pull/6425:1166,Testability,test,testRandom,1166,"Tests not passing, not sure why yet. Errors are MatchError. Stacked on #6421, will see if I can unwind; ```sh; > Task :test; Running test: Test method bitPackedVectorCorrectWhenOffsetNotZero(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.bitPackedVectorCorrectWhenOffsetNotZero PASSED; Running test: Test method testBitPackUnpack(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testBitPackUnpack FAILED; scala.MatchError at LocalLDPruneSuite.scala:222; Running test: Test method testIsLocallyUncorrelated(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testIsLocallyUncorrelated FAILED; org.apache.spark.SparkException at LocalLDPruneSuite.scala:214; Caused by: scala.MatchError; Running test: Test method testR2(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testR2 FAILED; scala.MatchError at LocalLDPruneSuite.scala:244; Running test: Test method testRandom(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testRandom FAILED; java.lang.AssertionError at LocalLDPruneSuite.scala:323; Caused by: scala.MatchError at LocalLDPruneSuite.scala:323; ```. cc @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6425
https://github.com/hail-is/hail/pull/6425:1195,Testability,Assert,AssertionError,1195,"Tests not passing, not sure why yet. Errors are MatchError. Stacked on #6421, will see if I can unwind; ```sh; > Task :test; Running test: Test method bitPackedVectorCorrectWhenOffsetNotZero(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.bitPackedVectorCorrectWhenOffsetNotZero PASSED; Running test: Test method testBitPackUnpack(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testBitPackUnpack FAILED; scala.MatchError at LocalLDPruneSuite.scala:222; Running test: Test method testIsLocallyUncorrelated(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testIsLocallyUncorrelated FAILED; org.apache.spark.SparkException at LocalLDPruneSuite.scala:214; Caused by: scala.MatchError; Running test: Test method testR2(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testR2 FAILED; scala.MatchError at LocalLDPruneSuite.scala:244; Running test: Test method testRandom(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testRandom FAILED; java.lang.AssertionError at LocalLDPruneSuite.scala:323; Caused by: scala.MatchError at LocalLDPruneSuite.scala:323; ```. cc @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6425
https://github.com/hail-is/hail/pull/6432:86,Availability,down,down,86,"Currently, on mobile browsers, the home page of [hail.is](https://hail.is/) is shrunk down to the point where it's difficult to read without zooming in. This adds a [viewport tag](https://developer.mozilla.org/en-US/docs/Mozilla/Mobile/Viewport_meta_tag) to improve that layout. ## Before:. ![before](https://user-images.githubusercontent.com/1156625/59926804-12a60f00-9409-11e9-80c5-52fe4c8ddd66.png). ## After:. ![after](https://user-images.githubusercontent.com/1156625/59926813-16d22c80-9409-11e9-958c-8553ef583be8.png)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6432
https://github.com/hail-is/hail/pull/6436:106,Deployability,update,updates,106,"These versions came from the current base image, from `pip freeze`. `pur` seems like the thing to compute updates: https://github.com/alanhamlett/pip-update-requirements. In particular, `pur -r requirements.txt` will update the requirements file with the latest version, showing what has changed. You can do `pur -o /dev/null -r requirements.txt` if you just want to see what's out of date. Here is `pur` for this change:. ```; $ pur -o /dev/null -r requirements.txt ; Updated gcsfs: 0.2.1 -> 0.2.2; Updated urllib3: 1.24.3 -> 1.25.3; ```. I'm not including updates in this PR, just pinning the current versions. I think we should have a job that regularly (weekly?) PRs an updated requirements.txt.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6436
https://github.com/hail-is/hail/pull/6436:150,Deployability,update,update-requirements,150,"These versions came from the current base image, from `pip freeze`. `pur` seems like the thing to compute updates: https://github.com/alanhamlett/pip-update-requirements. In particular, `pur -r requirements.txt` will update the requirements file with the latest version, showing what has changed. You can do `pur -o /dev/null -r requirements.txt` if you just want to see what's out of date. Here is `pur` for this change:. ```; $ pur -o /dev/null -r requirements.txt ; Updated gcsfs: 0.2.1 -> 0.2.2; Updated urllib3: 1.24.3 -> 1.25.3; ```. I'm not including updates in this PR, just pinning the current versions. I think we should have a job that regularly (weekly?) PRs an updated requirements.txt.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6436
https://github.com/hail-is/hail/pull/6436:217,Deployability,update,update,217,"These versions came from the current base image, from `pip freeze`. `pur` seems like the thing to compute updates: https://github.com/alanhamlett/pip-update-requirements. In particular, `pur -r requirements.txt` will update the requirements file with the latest version, showing what has changed. You can do `pur -o /dev/null -r requirements.txt` if you just want to see what's out of date. Here is `pur` for this change:. ```; $ pur -o /dev/null -r requirements.txt ; Updated gcsfs: 0.2.1 -> 0.2.2; Updated urllib3: 1.24.3 -> 1.25.3; ```. I'm not including updates in this PR, just pinning the current versions. I think we should have a job that regularly (weekly?) PRs an updated requirements.txt.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6436
https://github.com/hail-is/hail/pull/6436:469,Deployability,Update,Updated,469,"These versions came from the current base image, from `pip freeze`. `pur` seems like the thing to compute updates: https://github.com/alanhamlett/pip-update-requirements. In particular, `pur -r requirements.txt` will update the requirements file with the latest version, showing what has changed. You can do `pur -o /dev/null -r requirements.txt` if you just want to see what's out of date. Here is `pur` for this change:. ```; $ pur -o /dev/null -r requirements.txt ; Updated gcsfs: 0.2.1 -> 0.2.2; Updated urllib3: 1.24.3 -> 1.25.3; ```. I'm not including updates in this PR, just pinning the current versions. I think we should have a job that regularly (weekly?) PRs an updated requirements.txt.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6436
https://github.com/hail-is/hail/pull/6436:500,Deployability,Update,Updated,500,"These versions came from the current base image, from `pip freeze`. `pur` seems like the thing to compute updates: https://github.com/alanhamlett/pip-update-requirements. In particular, `pur -r requirements.txt` will update the requirements file with the latest version, showing what has changed. You can do `pur -o /dev/null -r requirements.txt` if you just want to see what's out of date. Here is `pur` for this change:. ```; $ pur -o /dev/null -r requirements.txt ; Updated gcsfs: 0.2.1 -> 0.2.2; Updated urllib3: 1.24.3 -> 1.25.3; ```. I'm not including updates in this PR, just pinning the current versions. I think we should have a job that regularly (weekly?) PRs an updated requirements.txt.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6436
https://github.com/hail-is/hail/pull/6436:558,Deployability,update,updates,558,"These versions came from the current base image, from `pip freeze`. `pur` seems like the thing to compute updates: https://github.com/alanhamlett/pip-update-requirements. In particular, `pur -r requirements.txt` will update the requirements file with the latest version, showing what has changed. You can do `pur -o /dev/null -r requirements.txt` if you just want to see what's out of date. Here is `pur` for this change:. ```; $ pur -o /dev/null -r requirements.txt ; Updated gcsfs: 0.2.1 -> 0.2.2; Updated urllib3: 1.24.3 -> 1.25.3; ```. I'm not including updates in this PR, just pinning the current versions. I think we should have a job that regularly (weekly?) PRs an updated requirements.txt.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6436
https://github.com/hail-is/hail/pull/6436:674,Deployability,update,updated,674,"These versions came from the current base image, from `pip freeze`. `pur` seems like the thing to compute updates: https://github.com/alanhamlett/pip-update-requirements. In particular, `pur -r requirements.txt` will update the requirements file with the latest version, showing what has changed. You can do `pur -o /dev/null -r requirements.txt` if you just want to see what's out of date. Here is `pur` for this change:. ```; $ pur -o /dev/null -r requirements.txt ; Updated gcsfs: 0.2.1 -> 0.2.2; Updated urllib3: 1.24.3 -> 1.25.3; ```. I'm not including updates in this PR, just pinning the current versions. I think we should have a job that regularly (weekly?) PRs an updated requirements.txt.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6436
https://github.com/hail-is/hail/pull/6438:295,Integrability,interface,interface,295,"This PR removes the JVM version of Pedigree in favor a Python based one, as a step towards removing unnecessary calls to the JVM altogether. There were already Pedigree python tests I added a few more, and the ones in Scala seem to mostly test functionality that was never exposed to the python interface anyway.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6438
https://github.com/hail-is/hail/pull/6438:273,Security,expose,exposed,273,"This PR removes the JVM version of Pedigree in favor a Python based one, as a step towards removing unnecessary calls to the JVM altogether. There were already Pedigree python tests I added a few more, and the ones in Scala seem to mostly test functionality that was never exposed to the python interface anyway.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6438
https://github.com/hail-is/hail/pull/6438:176,Testability,test,tests,176,"This PR removes the JVM version of Pedigree in favor a Python based one, as a step towards removing unnecessary calls to the JVM altogether. There were already Pedigree python tests I added a few more, and the ones in Scala seem to mostly test functionality that was never exposed to the python interface anyway.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6438
https://github.com/hail-is/hail/pull/6438:239,Testability,test,test,239,"This PR removes the JVM version of Pedigree in favor a Python based one, as a step towards removing unnecessary calls to the JVM altogether. There were already Pedigree python tests I added a few more, and the ones in Scala seem to mostly test functionality that was never exposed to the python interface anyway.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6438
https://github.com/hail-is/hail/pull/6442:82,Availability,error,error,82,* Hailtop wasn't a module.; * The import paths were fucked.; * There was a syntax error in setup.py; * No dirty trees after builds,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6442
https://github.com/hail-is/hail/pull/6443:77,Deployability,deploy,deploy,77,Reverts hail-is/hail#6376. This should just be in ci because the hailctl dev deploy should just send a request to the ci service rather than building things from a local computer.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6443
https://github.com/hail-is/hail/pull/6446:204,Energy Efficiency,efficient,efficient,204,"Adds a _partitions option to MatrixTable.write that is analogous to import_vcfs. The written table with have requseted partitioning. Elements not contained in the new partitioner will be dropped. This is efficient in the sense that it does the minimal amount of duplicate reading. In particular, if the target partitioning is a refinement of the existing partition, it will have the same cost as a normal (non-repartitioning) `read_matrix_table().write()`. @chrisvittal I think the partitions argument should take a Hail literal (e.g. Python list, dict and set, `hl.Struct`, etc.) instead of a JSON string and convert it to a string internally. Is there a reason you didn't do that with `import_vcfs`?. FYI @lfrancioli we can efficiently repartition the new callset with this.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6446
https://github.com/hail-is/hail/pull/6446:726,Energy Efficiency,efficient,efficiently,726,"Adds a _partitions option to MatrixTable.write that is analogous to import_vcfs. The written table with have requseted partitioning. Elements not contained in the new partitioner will be dropped. This is efficient in the sense that it does the minimal amount of duplicate reading. In particular, if the target partitioning is a refinement of the existing partition, it will have the same cost as a normal (non-repartitioning) `read_matrix_table().write()`. @chrisvittal I think the partitions argument should take a Hail literal (e.g. Python list, dict and set, `hl.Struct`, etc.) instead of a JSON string and convert it to a string internally. Is there a reason you didn't do that with `import_vcfs`?. FYI @lfrancioli we can efficiently repartition the new callset with this.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6446
https://github.com/hail-is/hail/pull/6447:63,Deployability,deploy,deployment,63,"Prometheus is now a StatefulSet, removing the need to delete a deployment and sleep in the monitoring Makefile. The storage has also been bumped up to 50Gi to prevent running out of storage in the future.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6447
https://github.com/hail-is/hail/pull/6447:91,Energy Efficiency,monitor,monitoring,91,"Prometheus is now a StatefulSet, removing the need to delete a deployment and sleep in the monitoring Makefile. The storage has also been bumped up to 50Gi to prevent running out of storage in the future.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6447
https://github.com/hail-is/hail/pull/6453:49,Testability,log,log,49,"I just pulled in a library specifically for json log formatting that handles escaping nicely + makes it easy to add anything else we might want to do. I've obviously not run batch locally but I've tested this code locally by running . ```from hailtop.gear import configure_logging; import logging; configure_logging(); logging.info('""Foo""'); ```. We are definitely going to want more than this eventually, but this should at least let us start to break up our logs in Kibana. And it's hard to test more complicated things without some way to run my own batch.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6453
https://github.com/hail-is/hail/pull/6453:197,Testability,test,tested,197,"I just pulled in a library specifically for json log formatting that handles escaping nicely + makes it easy to add anything else we might want to do. I've obviously not run batch locally but I've tested this code locally by running . ```from hailtop.gear import configure_logging; import logging; configure_logging(); logging.info('""Foo""'); ```. We are definitely going to want more than this eventually, but this should at least let us start to break up our logs in Kibana. And it's hard to test more complicated things without some way to run my own batch.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6453
https://github.com/hail-is/hail/pull/6453:289,Testability,log,logging,289,"I just pulled in a library specifically for json log formatting that handles escaping nicely + makes it easy to add anything else we might want to do. I've obviously not run batch locally but I've tested this code locally by running . ```from hailtop.gear import configure_logging; import logging; configure_logging(); logging.info('""Foo""'); ```. We are definitely going to want more than this eventually, but this should at least let us start to break up our logs in Kibana. And it's hard to test more complicated things without some way to run my own batch.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6453
https://github.com/hail-is/hail/pull/6453:319,Testability,log,logging,319,"I just pulled in a library specifically for json log formatting that handles escaping nicely + makes it easy to add anything else we might want to do. I've obviously not run batch locally but I've tested this code locally by running . ```from hailtop.gear import configure_logging; import logging; configure_logging(); logging.info('""Foo""'); ```. We are definitely going to want more than this eventually, but this should at least let us start to break up our logs in Kibana. And it's hard to test more complicated things without some way to run my own batch.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6453
https://github.com/hail-is/hail/pull/6453:460,Testability,log,logs,460,"I just pulled in a library specifically for json log formatting that handles escaping nicely + makes it easy to add anything else we might want to do. I've obviously not run batch locally but I've tested this code locally by running . ```from hailtop.gear import configure_logging; import logging; configure_logging(); logging.info('""Foo""'); ```. We are definitely going to want more than this eventually, but this should at least let us start to break up our logs in Kibana. And it's hard to test more complicated things without some way to run my own batch.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6453
https://github.com/hail-is/hail/pull/6453:493,Testability,test,test,493,"I just pulled in a library specifically for json log formatting that handles escaping nicely + makes it easy to add anything else we might want to do. I've obviously not run batch locally but I've tested this code locally by running . ```from hailtop.gear import configure_logging; import logging; configure_logging(); logging.info('""Foo""'); ```. We are definitely going to want more than this eventually, but this should at least let us start to break up our logs in Kibana. And it's hard to test more complicated things without some way to run my own batch.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6453
https://github.com/hail-is/hail/pull/6454:37,Testability,test,tests,37,"- unify all `alpine` images in batch tests to `alpine:3.10.0`; - ensure `alpine:3.10.0` is always fetched on nodes; - add test that creates 1000 `true` jobs; - increases [max content size in nginx](http://nginx.org/en/docs/http/ngx_http_core_module.html#client_max_body_size) to 10,000MB",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6454
https://github.com/hail-is/hail/pull/6454:122,Testability,test,test,122,"- unify all `alpine` images in batch tests to `alpine:3.10.0`; - ensure `alpine:3.10.0` is always fetched on nodes; - add test that creates 1000 `true` jobs; - increases [max content size in nginx](http://nginx.org/en/docs/http/ngx_http_core_module.html#client_max_body_size) to 10,000MB",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6454
https://github.com/hail-is/hail/issues/6458:789,Availability,avail,available,789,"Script:; ```python3; #!/usr/bin/env python3; import hail as hl; hl.init(log='/dev/null'); mt = hl.import_vcf('src/test/resources/sample.vcf'); mt.filter_rows(mt.locus < hl.Locus('1', 1)).show(); ```. Output:; ```; 2019-06-24 19:12:05 WARN Utils:66 - Your hostname, wp086-661 resolves to a loopback address: 127.0.1.1; using 10.1.8.50 instead (on interface wlp2s0); 2019-06-24 19:12:05 WARN Utils:66 - Set SPARK_LOCAL_IP if you need to bind to another address; 2019-06-24 19:12:06 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Running on Apache Spark version 2.4.0; SparkUI available at http://wp086-661.broadinstitute.org:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.16-e95038bbed35; LOGGING: writing to /dev/null; Traceback (most recent call last):; File ""/tmp/x"", line 4, in <module>; mt.filter_rows(mt.locus < hl.Locus('1', 1)).show(); File ""</home/BROAD.MIT.EDU/cvittal/.cache/hail-env/lib/python3.6/site-packages/decorator.py:decorator-gen-1000>"", line 2, in show; File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/matrixtable.py"", line 2569, in show; actual_n_cols = self.count_cols(); File ""</home/BROAD.MIT.EDU/cvittal/.cache/hail-env/lib/python3.6/site-packages/decorator.py:decorator-gen-994>"", line 2, in count_cols; File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/matrixtable.py"", line 2404, in count_cols; return Env.backend().execute(ir); File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/py",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6458
https://github.com/hail-is/hail/issues/6458:2389,Availability,Error,Error,2389,"ROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/matrixtable.py"", line 2569, in show; actual_n_cols = self.count_cols(); File ""</home/BROAD.MIT.EDU/cvittal/.cache/hail-env/lib/python3.6/site-packages/decorator.py:decorator-gen-994>"", line 2, in count_cols; File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/matrixtable.py"", line 2404, in count_cols; return Env.backend().execute(ir); File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/backend/backend.py"", line 108, in execute; result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); File ""/home/BROAD.MIT.EDU/cvittal/.local/opt/spark/spark-2.4.0-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/utils/java.py"", line 221, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: MatchError: locus<GRCh37> (of class is.hail.expr.types.virtual.TLocus). Java stack trace:; scala.MatchError: locus<GRCh37> (of class is.hail.expr.types.virtual.TLocus); 	at is.hail.expr.ir.ExtractIntervalFilters$.minimumValueByType(ExtractIntervalFilters.scala:42); 	at is.hail.expr.ir.ExtractIntervalFilters$.openInterval(ExtractIntervalFilters.scala:94); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$extractAndRewrite$6.apply(ExtractIntervalFilters.scala:205); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$extractAndRewrite$6.apply(ExtractIntervalFilters.scala:201); 	at scala.Option.flatMap(Option.scala:171); 	at is.hail.expr.ir.ExtractIntervalFilters$.extractAndRewrite(ExtractIntervalFilters.scala:201); 	at is.hail.expr.ir.ExtractIntervalFilters$.extractAndRewrite(ExtractIntervalFilters.scala:151); 	at is.hail.expr.ir.ExtractIntervalFilters$.extractPartitionFilters(ExtractIntervalF",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6458
https://github.com/hail-is/hail/issues/6458:6757,Availability,Error,Error,6757,ach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.ExtractIntervalFilters$.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.Optimize$.optimize(Optimize.scala:19); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:49); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$.optimizeIR$1(CompileAndEvaluate.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:24); 	at is.hail.backend.Backend.execute(Backend.scala:86); 	at is.hail.backend.Backend.executeJSON(Backend.scala:92); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.16-e95038bbed35; Error summary: MatchError: locus<GRCh37> (of class is.hail.expr.types.virtual.TLocus); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6458
https://github.com/hail-is/hail/issues/6458:346,Integrability,interface,interface,346,"Script:; ```python3; #!/usr/bin/env python3; import hail as hl; hl.init(log='/dev/null'); mt = hl.import_vcf('src/test/resources/sample.vcf'); mt.filter_rows(mt.locus < hl.Locus('1', 1)).show(); ```. Output:; ```; 2019-06-24 19:12:05 WARN Utils:66 - Your hostname, wp086-661 resolves to a loopback address: 127.0.1.1; using 10.1.8.50 instead (on interface wlp2s0); 2019-06-24 19:12:05 WARN Utils:66 - Set SPARK_LOCAL_IP if you need to bind to another address; 2019-06-24 19:12:06 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Running on Apache Spark version 2.4.0; SparkUI available at http://wp086-661.broadinstitute.org:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.16-e95038bbed35; LOGGING: writing to /dev/null; Traceback (most recent call last):; File ""/tmp/x"", line 4, in <module>; mt.filter_rows(mt.locus < hl.Locus('1', 1)).show(); File ""</home/BROAD.MIT.EDU/cvittal/.cache/hail-env/lib/python3.6/site-packages/decorator.py:decorator-gen-1000>"", line 2, in show; File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/matrixtable.py"", line 2569, in show; actual_n_cols = self.count_cols(); File ""</home/BROAD.MIT.EDU/cvittal/.cache/hail-env/lib/python3.6/site-packages/decorator.py:decorator-gen-994>"", line 2, in count_cols; File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/matrixtable.py"", line 2404, in count_cols; return Env.backend().execute(ir); File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/py",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6458
https://github.com/hail-is/hail/issues/6458:1332,Integrability,wrap,wrapper,1332,"p2s0); 2019-06-24 19:12:05 WARN Utils:66 - Set SPARK_LOCAL_IP if you need to bind to another address; 2019-06-24 19:12:06 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Running on Apache Spark version 2.4.0; SparkUI available at http://wp086-661.broadinstitute.org:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.16-e95038bbed35; LOGGING: writing to /dev/null; Traceback (most recent call last):; File ""/tmp/x"", line 4, in <module>; mt.filter_rows(mt.locus < hl.Locus('1', 1)).show(); File ""</home/BROAD.MIT.EDU/cvittal/.cache/hail-env/lib/python3.6/site-packages/decorator.py:decorator-gen-1000>"", line 2, in show; File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/matrixtable.py"", line 2569, in show; actual_n_cols = self.count_cols(); File ""</home/BROAD.MIT.EDU/cvittal/.cache/hail-env/lib/python3.6/site-packages/decorator.py:decorator-gen-994>"", line 2, in count_cols; File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/matrixtable.py"", line 2404, in count_cols; return Env.backend().execute(ir); File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/backend/backend.py"", line 108, in execute; result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); File ""/home/BROAD.MIT.EDU/cvittal/.local/opt/spark/spark-2.4.0-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/utils",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6458
https://github.com/hail-is/hail/issues/6458:1754,Integrability,wrap,wrapper,1754," SparkUI available at http://wp086-661.broadinstitute.org:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.16-e95038bbed35; LOGGING: writing to /dev/null; Traceback (most recent call last):; File ""/tmp/x"", line 4, in <module>; mt.filter_rows(mt.locus < hl.Locus('1', 1)).show(); File ""</home/BROAD.MIT.EDU/cvittal/.cache/hail-env/lib/python3.6/site-packages/decorator.py:decorator-gen-1000>"", line 2, in show; File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/matrixtable.py"", line 2569, in show; actual_n_cols = self.count_cols(); File ""</home/BROAD.MIT.EDU/cvittal/.cache/hail-env/lib/python3.6/site-packages/decorator.py:decorator-gen-994>"", line 2, in count_cols; File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/matrixtable.py"", line 2404, in count_cols; return Env.backend().execute(ir); File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/backend/backend.py"", line 108, in execute; result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); File ""/home/BROAD.MIT.EDU/cvittal/.local/opt/spark/spark-2.4.0-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/utils/java.py"", line 221, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: MatchError: locus<GRCh37> (of class is.hail.expr.types.virtual.TLocus). Java stack trace:; scala.MatchError: locus<GRCh37> (of class is.hail.expr.types.virtual.TLocus); 	at is.hail.expr.ir.ExtractIntervalFilters$.minimumValueByType(ExtractIntervalFilters.scala:42); 	at is.hail.expr.ir.E",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6458
https://github.com/hail-is/hail/issues/6458:4171,Integrability,Wrap,WrappedArray,4171,rite(ExtractIntervalFilters.scala:201); 	at is.hail.expr.ir.ExtractIntervalFilters$.extractAndRewrite(ExtractIntervalFilters.scala:151); 	at is.hail.expr.ir.ExtractIntervalFilters$.extractPartitionFilters(ExtractIntervalFilters.scala:249); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$apply$2.apply(ExtractIntervalFilters.scala:266); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$apply$2.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:15); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6458
https://github.com/hail-is/hail/issues/6458:4192,Integrability,Wrap,WrappedArray,4192,valFilters.scala:201); 	at is.hail.expr.ir.ExtractIntervalFilters$.extractAndRewrite(ExtractIntervalFilters.scala:151); 	at is.hail.expr.ir.ExtractIntervalFilters$.extractPartitionFilters(ExtractIntervalFilters.scala:249); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$apply$2.apply(ExtractIntervalFilters.scala:266); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$apply$2.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:15); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6458
https://github.com/hail-is/hail/issues/6458:4911,Integrability,Wrap,WrappedArray,4911,ike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.ExtractIntervalFilters$.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.Optimize$.optimize(Optimize.scala:19); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:49); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$.optimizeIR$1(CompileAndEvaluate.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:24); 	at is.hail.backend.B,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6458
https://github.com/hail-is/hail/issues/6458:4932,Integrability,Wrap,WrappedArray,4932,1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.ExtractIntervalFilters$.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.Optimize$.optimize(Optimize.scala:19); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:49); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$.optimizeIR$1(CompileAndEvaluate.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:24); 	at is.hail.backend.Backend.execute(Ba,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6458
https://github.com/hail-is/hail/issues/6458:3639,Modifiability,Rewrite,RewriteBottomUp,3639,rtual.TLocus); 	at is.hail.expr.ir.ExtractIntervalFilters$.minimumValueByType(ExtractIntervalFilters.scala:42); 	at is.hail.expr.ir.ExtractIntervalFilters$.openInterval(ExtractIntervalFilters.scala:94); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$extractAndRewrite$6.apply(ExtractIntervalFilters.scala:205); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$extractAndRewrite$6.apply(ExtractIntervalFilters.scala:201); 	at scala.Option.flatMap(Option.scala:171); 	at is.hail.expr.ir.ExtractIntervalFilters$.extractAndRewrite(ExtractIntervalFilters.scala:201); 	at is.hail.expr.ir.ExtractIntervalFilters$.extractAndRewrite(ExtractIntervalFilters.scala:151); 	at is.hail.expr.ir.ExtractIntervalFilters$.extractPartitionFilters(ExtractIntervalFilters.scala:249); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$apply$2.apply(ExtractIntervalFilters.scala:266); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$apply$2.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:15); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.Tra,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6458
https://github.com/hail-is/hail/issues/6458:3672,Modifiability,Rewrite,RewriteBottomUp,3672,ractIntervalFilters$.minimumValueByType(ExtractIntervalFilters.scala:42); 	at is.hail.expr.ir.ExtractIntervalFilters$.openInterval(ExtractIntervalFilters.scala:94); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$extractAndRewrite$6.apply(ExtractIntervalFilters.scala:205); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$extractAndRewrite$6.apply(ExtractIntervalFilters.scala:201); 	at scala.Option.flatMap(Option.scala:171); 	at is.hail.expr.ir.ExtractIntervalFilters$.extractAndRewrite(ExtractIntervalFilters.scala:201); 	at is.hail.expr.ir.ExtractIntervalFilters$.extractAndRewrite(ExtractIntervalFilters.scala:151); 	at is.hail.expr.ir.ExtractIntervalFilters$.extractPartitionFilters(ExtractIntervalFilters.scala:249); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$apply$2.apply(ExtractIntervalFilters.scala:266); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$apply$2.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:15); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(Trav,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6458
https://github.com/hail-is/hail/issues/6458:3689,Modifiability,rewrite,rewrite,3689,ractIntervalFilters$.minimumValueByType(ExtractIntervalFilters.scala:42); 	at is.hail.expr.ir.ExtractIntervalFilters$.openInterval(ExtractIntervalFilters.scala:94); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$extractAndRewrite$6.apply(ExtractIntervalFilters.scala:205); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$extractAndRewrite$6.apply(ExtractIntervalFilters.scala:201); 	at scala.Option.flatMap(Option.scala:171); 	at is.hail.expr.ir.ExtractIntervalFilters$.extractAndRewrite(ExtractIntervalFilters.scala:201); 	at is.hail.expr.ir.ExtractIntervalFilters$.extractAndRewrite(ExtractIntervalFilters.scala:151); 	at is.hail.expr.ir.ExtractIntervalFilters$.extractPartitionFilters(ExtractIntervalFilters.scala:249); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$apply$2.apply(ExtractIntervalFilters.scala:266); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$apply$2.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:15); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(Trav,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6458
https://github.com/hail-is/hail/issues/6458:3699,Modifiability,Rewrite,RewriteBottomUp,3699,ractIntervalFilters$.minimumValueByType(ExtractIntervalFilters.scala:42); 	at is.hail.expr.ir.ExtractIntervalFilters$.openInterval(ExtractIntervalFilters.scala:94); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$extractAndRewrite$6.apply(ExtractIntervalFilters.scala:205); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$extractAndRewrite$6.apply(ExtractIntervalFilters.scala:201); 	at scala.Option.flatMap(Option.scala:171); 	at is.hail.expr.ir.ExtractIntervalFilters$.extractAndRewrite(ExtractIntervalFilters.scala:201); 	at is.hail.expr.ir.ExtractIntervalFilters$.extractAndRewrite(ExtractIntervalFilters.scala:151); 	at is.hail.expr.ir.ExtractIntervalFilters$.extractPartitionFilters(ExtractIntervalFilters.scala:249); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$apply$2.apply(ExtractIntervalFilters.scala:266); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$apply$2.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:15); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(Trav,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6458
https://github.com/hail-is/hail/issues/6458:3746,Modifiability,Rewrite,RewriteBottomUp,3746,	at is.hail.expr.ir.ExtractIntervalFilters$.openInterval(ExtractIntervalFilters.scala:94); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$extractAndRewrite$6.apply(ExtractIntervalFilters.scala:205); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$extractAndRewrite$6.apply(ExtractIntervalFilters.scala:201); 	at scala.Option.flatMap(Option.scala:171); 	at is.hail.expr.ir.ExtractIntervalFilters$.extractAndRewrite(ExtractIntervalFilters.scala:201); 	at is.hail.expr.ir.ExtractIntervalFilters$.extractAndRewrite(ExtractIntervalFilters.scala:151); 	at is.hail.expr.ir.ExtractIntervalFilters$.extractPartitionFilters(ExtractIntervalFilters.scala:249); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$apply$2.apply(ExtractIntervalFilters.scala:266); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$apply$2.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:15); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6458
https://github.com/hail-is/hail/issues/6458:3779,Modifiability,Rewrite,RewriteBottomUp,3779,ctIntervalFilters$.openInterval(ExtractIntervalFilters.scala:94); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$extractAndRewrite$6.apply(ExtractIntervalFilters.scala:205); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$extractAndRewrite$6.apply(ExtractIntervalFilters.scala:201); 	at scala.Option.flatMap(Option.scala:171); 	at is.hail.expr.ir.ExtractIntervalFilters$.extractAndRewrite(ExtractIntervalFilters.scala:201); 	at is.hail.expr.ir.ExtractIntervalFilters$.extractAndRewrite(ExtractIntervalFilters.scala:151); 	at is.hail.expr.ir.ExtractIntervalFilters$.extractPartitionFilters(ExtractIntervalFilters.scala:249); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$apply$2.apply(ExtractIntervalFilters.scala:266); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$apply$2.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:15); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6458
https://github.com/hail-is/hail/issues/6458:3825,Modifiability,Rewrite,RewriteBottomUp,3825,.scala:94); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$extractAndRewrite$6.apply(ExtractIntervalFilters.scala:205); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$extractAndRewrite$6.apply(ExtractIntervalFilters.scala:201); 	at scala.Option.flatMap(Option.scala:171); 	at is.hail.expr.ir.ExtractIntervalFilters$.extractAndRewrite(ExtractIntervalFilters.scala:201); 	at is.hail.expr.ir.ExtractIntervalFilters$.extractAndRewrite(ExtractIntervalFilters.scala:151); 	at is.hail.expr.ir.ExtractIntervalFilters$.extractPartitionFilters(ExtractIntervalFilters.scala:249); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$apply$2.apply(ExtractIntervalFilters.scala:266); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$apply$2.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:15); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$cl,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6458
https://github.com/hail-is/hail/issues/6458:3858,Modifiability,Rewrite,RewriteBottomUp,3858,xpr.ir.ExtractIntervalFilters$$anonfun$extractAndRewrite$6.apply(ExtractIntervalFilters.scala:205); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$extractAndRewrite$6.apply(ExtractIntervalFilters.scala:201); 	at scala.Option.flatMap(Option.scala:171); 	at is.hail.expr.ir.ExtractIntervalFilters$.extractAndRewrite(ExtractIntervalFilters.scala:201); 	at is.hail.expr.ir.ExtractIntervalFilters$.extractAndRewrite(ExtractIntervalFilters.scala:151); 	at is.hail.expr.ir.ExtractIntervalFilters$.extractPartitionFilters(ExtractIntervalFilters.scala:249); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$apply$2.apply(ExtractIntervalFilters.scala:266); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$apply$2.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:15); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOp,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6458
https://github.com/hail-is/hail/issues/6458:4380,Modifiability,Rewrite,RewriteBottomUp,4380,tIntervalFilters.scala:249); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$apply$2.apply(ExtractIntervalFilters.scala:266); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$apply$2.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:15); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.ExtractIntervalFilters$.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.Optimize,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6458
https://github.com/hail-is/hail/issues/6458:4413,Modifiability,Rewrite,RewriteBottomUp,4413,il.expr.ir.ExtractIntervalFilters$$anonfun$apply$2.apply(ExtractIntervalFilters.scala:266); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$apply$2.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:15); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.ExtractIntervalFilters$.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.Optimize$.optimize(Optimize.scala:19); 	at is.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6458
https://github.com/hail-is/hail/issues/6458:4430,Modifiability,rewrite,rewrite,4430,il.expr.ir.ExtractIntervalFilters$$anonfun$apply$2.apply(ExtractIntervalFilters.scala:266); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$apply$2.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:15); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.ExtractIntervalFilters$.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.Optimize$.optimize(Optimize.scala:19); 	at is.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6458
https://github.com/hail-is/hail/issues/6458:4440,Modifiability,Rewrite,RewriteBottomUp,4440,il.expr.ir.ExtractIntervalFilters$$anonfun$apply$2.apply(ExtractIntervalFilters.scala:266); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$apply$2.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:15); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.ExtractIntervalFilters$.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.Optimize$.optimize(Optimize.scala:19); 	at is.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6458
https://github.com/hail-is/hail/issues/6458:4486,Modifiability,Rewrite,RewriteBottomUp,4486,ilters.scala:266); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$apply$2.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:15); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.ExtractIntervalFilters$.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.Optimize$.optimize(Optimize.scala:19); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:49); 	at is.hail.expr.ir.Comp,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6458
https://github.com/hail-is/hail/issues/6458:4519,Modifiability,Rewrite,RewriteBottomUp,4519,.hail.expr.ir.ExtractIntervalFilters$$anonfun$apply$2.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:15); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.ExtractIntervalFilters$.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.Optimize$.optimize(Optimize.scala:19); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:49); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6458
https://github.com/hail-is/hail/issues/6458:4565,Modifiability,Rewrite,RewriteBottomUp,4565,apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:15); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.ExtractIntervalFilters$.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.Optimize$.optimize(Optimize.scala:19); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:49); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at ,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6458
https://github.com/hail-is/hail/issues/6458:4598,Modifiability,Rewrite,RewriteBottomUp,4598,ers.scala:254); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:15); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.ExtractIntervalFilters$.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.Optimize$.optimize(Optimize.scala:19); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:49); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at is.hail.expr.ir.CompileA,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6458
https://github.com/hail-is/hail/issues/6458:5120,Modifiability,Rewrite,RewriteBottomUp,5120,ed.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.ExtractIntervalFilters$.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.Optimize$.optimize(Optimize.scala:19); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:49); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$.optimizeIR$1(CompileAndEvaluate.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:24); 	at is.hail.backend.Backend.execute(Backend.scala:86); 	at is.hail.backend.Backend.executeJSON(Backend.scala:92); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(Nativ,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6458
https://github.com/hail-is/hail/issues/6458:5153,Modifiability,Rewrite,RewriteBottomUp,5153,able.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.ExtractIntervalFilters$.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.Optimize$.optimize(Optimize.scala:19); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:49); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$.optimizeIR$1(CompileAndEvaluate.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:24); 	at is.hail.backend.Backend.execute(Backend.scala:86); 	at is.hail.backend.Backend.executeJSON(Backend.scala:92); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6458
https://github.com/hail-is/hail/issues/6458:5170,Modifiability,rewrite,rewrite,5170,able.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.ExtractIntervalFilters$.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.Optimize$.optimize(Optimize.scala:19); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:49); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$.optimizeIR$1(CompileAndEvaluate.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:24); 	at is.hail.backend.Backend.execute(Backend.scala:86); 	at is.hail.backend.Backend.executeJSON(Backend.scala:92); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6458
https://github.com/hail-is/hail/issues/6458:5180,Modifiability,Rewrite,RewriteBottomUp,5180,able.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.ExtractIntervalFilters$.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.Optimize$.optimize(Optimize.scala:19); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:49); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$.optimizeIR$1(CompileAndEvaluate.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:24); 	at is.hail.backend.Backend.execute(Backend.scala:86); 	at is.hail.backend.Backend.executeJSON(Backend.scala:92); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6458
https://github.com/hail-is/hail/issues/6458:5226,Modifiability,Rewrite,RewriteBottomUp,5226,on.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.ExtractIntervalFilters$.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.Optimize$.optimize(Optimize.scala:19); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:49); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$.optimizeIR$1(CompileAndEvaluate.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:24); 	at is.hail.backend.Backend.execute(Backend.scala:86); 	at is.hail.backend.Backend.executeJSON(Backend.scala:92); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessor,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6458
https://github.com/hail-is/hail/issues/6458:5249,Modifiability,Rewrite,RewriteBottomUp,5249,lass.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.ExtractIntervalFilters$.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.Optimize$.optimize(Optimize.scala:19); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:49); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$.optimizeIR$1(CompileAndEvaluate.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:24); 	at is.hail.backend.Backend.execute(Backend.scala:86); 	at is.hail.backend.Backend.executeJSON(Backend.scala:92); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at ,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6458
https://github.com/hail-is/hail/issues/6458:517,Performance,load,load,517,"Script:; ```python3; #!/usr/bin/env python3; import hail as hl; hl.init(log='/dev/null'); mt = hl.import_vcf('src/test/resources/sample.vcf'); mt.filter_rows(mt.locus < hl.Locus('1', 1)).show(); ```. Output:; ```; 2019-06-24 19:12:05 WARN Utils:66 - Your hostname, wp086-661 resolves to a loopback address: 127.0.1.1; using 10.1.8.50 instead (on interface wlp2s0); 2019-06-24 19:12:05 WARN Utils:66 - Set SPARK_LOCAL_IP if you need to bind to another address; 2019-06-24 19:12:06 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Running on Apache Spark version 2.4.0; SparkUI available at http://wp086-661.broadinstitute.org:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.16-e95038bbed35; LOGGING: writing to /dev/null; Traceback (most recent call last):; File ""/tmp/x"", line 4, in <module>; mt.filter_rows(mt.locus < hl.Locus('1', 1)).show(); File ""</home/BROAD.MIT.EDU/cvittal/.cache/hail-env/lib/python3.6/site-packages/decorator.py:decorator-gen-1000>"", line 2, in show; File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/matrixtable.py"", line 2569, in show; actual_n_cols = self.count_cols(); File ""</home/BROAD.MIT.EDU/cvittal/.cache/hail-env/lib/python3.6/site-packages/decorator.py:decorator-gen-994>"", line 2, in count_cols; File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/matrixtable.py"", line 2404, in count_cols; return Env.backend().execute(ir); File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/py",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6458
https://github.com/hail-is/hail/issues/6458:1139,Performance,cache,cache,1139,"_rows(mt.locus < hl.Locus('1', 1)).show(); ```. Output:; ```; 2019-06-24 19:12:05 WARN Utils:66 - Your hostname, wp086-661 resolves to a loopback address: 127.0.1.1; using 10.1.8.50 instead (on interface wlp2s0); 2019-06-24 19:12:05 WARN Utils:66 - Set SPARK_LOCAL_IP if you need to bind to another address; 2019-06-24 19:12:06 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Running on Apache Spark version 2.4.0; SparkUI available at http://wp086-661.broadinstitute.org:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.16-e95038bbed35; LOGGING: writing to /dev/null; Traceback (most recent call last):; File ""/tmp/x"", line 4, in <module>; mt.filter_rows(mt.locus < hl.Locus('1', 1)).show(); File ""</home/BROAD.MIT.EDU/cvittal/.cache/hail-env/lib/python3.6/site-packages/decorator.py:decorator-gen-1000>"", line 2, in show; File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/matrixtable.py"", line 2569, in show; actual_n_cols = self.count_cols(); File ""</home/BROAD.MIT.EDU/cvittal/.cache/hail-env/lib/python3.6/site-packages/decorator.py:decorator-gen-994>"", line 2, in count_cols; File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/matrixtable.py"", line 2404, in count_cols; return Env.backend().execute(ir); File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/backend/backend.py"", line 108, in execute; result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); File ""/home/BROAD.M",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6458
https://github.com/hail-is/hail/issues/6458:1556,Performance,cache,cache,1556,"g builtin-java classes where applicable; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Running on Apache Spark version 2.4.0; SparkUI available at http://wp086-661.broadinstitute.org:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.16-e95038bbed35; LOGGING: writing to /dev/null; Traceback (most recent call last):; File ""/tmp/x"", line 4, in <module>; mt.filter_rows(mt.locus < hl.Locus('1', 1)).show(); File ""</home/BROAD.MIT.EDU/cvittal/.cache/hail-env/lib/python3.6/site-packages/decorator.py:decorator-gen-1000>"", line 2, in show; File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/matrixtable.py"", line 2569, in show; actual_n_cols = self.count_cols(); File ""</home/BROAD.MIT.EDU/cvittal/.cache/hail-env/lib/python3.6/site-packages/decorator.py:decorator-gen-994>"", line 2, in count_cols; File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/matrixtable.py"", line 2404, in count_cols; return Env.backend().execute(ir); File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/backend/backend.py"", line 108, in execute; result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); File ""/home/BROAD.MIT.EDU/cvittal/.local/opt/spark/spark-2.4.0-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/utils/java.py"", line 221, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: MatchError: locus<GRCh37> (of class is.hail.expr.types.virtual.TLocus). Java ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6458
https://github.com/hail-is/hail/issues/6458:2068,Performance,load,loads,2068,"s < hl.Locus('1', 1)).show(); File ""</home/BROAD.MIT.EDU/cvittal/.cache/hail-env/lib/python3.6/site-packages/decorator.py:decorator-gen-1000>"", line 2, in show; File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/matrixtable.py"", line 2569, in show; actual_n_cols = self.count_cols(); File ""</home/BROAD.MIT.EDU/cvittal/.cache/hail-env/lib/python3.6/site-packages/decorator.py:decorator-gen-994>"", line 2, in count_cols; File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/matrixtable.py"", line 2404, in count_cols; return Env.backend().execute(ir); File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/backend/backend.py"", line 108, in execute; result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); File ""/home/BROAD.MIT.EDU/cvittal/.local/opt/spark/spark-2.4.0-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/utils/java.py"", line 221, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: MatchError: locus<GRCh37> (of class is.hail.expr.types.virtual.TLocus). Java stack trace:; scala.MatchError: locus<GRCh37> (of class is.hail.expr.types.virtual.TLocus); 	at is.hail.expr.ir.ExtractIntervalFilters$.minimumValueByType(ExtractIntervalFilters.scala:42); 	at is.hail.expr.ir.ExtractIntervalFilters$.openInterval(ExtractIntervalFilters.scala:94); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$extractAndRewrite$6.apply(ExtractIntervalFilters.scala:205); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$extractAndRewrite$6.apply(ExtractIntervalFilters.scala:201",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6458
https://github.com/hail-is/hail/issues/6458:5381,Performance,Optimiz,Optimize,5381,eBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.ExtractIntervalFilters$.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.Optimize$.optimize(Optimize.scala:19); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:49); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$.optimizeIR$1(CompileAndEvaluate.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:24); 	at is.hail.backend.Backend.execute(Backend.scala:86); 	at is.hail.backend.Backend.executeJSON(Backend.scala:92); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflect,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6458
https://github.com/hail-is/hail/issues/6458:5391,Performance,optimiz,optimize,5391,hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.ExtractIntervalFilters$.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.Optimize$.optimize(Optimize.scala:19); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:49); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$.optimizeIR$1(CompileAndEvaluate.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:24); 	at is.hail.backend.Backend.execute(Backend.scala:86); 	at is.hail.backend.Backend.executeJSON(Backend.scala:92); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.Reflection,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6458
https://github.com/hail-is/hail/issues/6458:5400,Performance,Optimiz,Optimize,5400,hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.ExtractIntervalFilters$.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.Optimize$.optimize(Optimize.scala:19); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:49); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$.optimizeIR$1(CompileAndEvaluate.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:24); 	at is.hail.backend.Backend.execute(Backend.scala:86); 	at is.hail.backend.Backend.executeJSON(Backend.scala:92); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.Reflection,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6458
https://github.com/hail-is/hail/issues/6458:5440,Performance,Optimiz,Optimize,5440,teBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.ExtractIntervalFilters$.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.Optimize$.optimize(Optimize.scala:19); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:49); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$.optimizeIR$1(CompileAndEvaluate.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:24); 	at is.hail.backend.Backend.execute(Backend.scala:86); 	at is.hail.backend.Backend.executeJSON(Backend.scala:92); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6458
https://github.com/hail-is/hail/issues/6458:5456,Performance,Optimiz,Optimize,5456,cala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.ExtractIntervalFilters$.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.Optimize$.optimize(Optimize.scala:19); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:49); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$.optimizeIR$1(CompileAndEvaluate.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:24); 	at is.hail.backend.Backend.execute(Backend.scala:86); 	at is.hail.backend.Backend.executeJSON(Backend.scala:92); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6458
https://github.com/hail-is/hail/issues/6458:5524,Performance,optimiz,optimizeIR,5524,y(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.ExtractIntervalFilters$.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.Optimize$.optimize(Optimize.scala:19); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:49); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$.optimizeIR$1(CompileAndEvaluate.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:24); 	at is.hail.backend.Backend.execute(Backend.scala:86); 	at is.hail.backend.Backend.executeJSON(Backend.scala:92); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6458
https://github.com/hail-is/hail/issues/6458:5623,Performance,optimiz,optimizeIR,5623,ala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.ExtractIntervalFilters$.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.Optimize$.optimize(Optimize.scala:19); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:49); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$.optimizeIR$1(CompileAndEvaluate.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:24); 	at is.hail.backend.Backend.execute(Backend.scala:86); 	at is.hail.backend.Backend.executeJSON(Backend.scala:92); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6458
https://github.com/hail-is/hail/issues/6458:5778,Performance,optimiz,optimizeIR,5778,; 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.ExtractIntervalFilters$.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.Optimize$.optimize(Optimize.scala:19); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:49); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$.optimizeIR$1(CompileAndEvaluate.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:24); 	at is.hail.backend.Backend.execute(Backend.scala:86); 	at is.hail.backend.Backend.executeJSON(Backend.scala:92); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.16-e95038bbed35; Error summary: MatchError: locus<GRCh,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6458
https://github.com/hail-is/hail/issues/6458:72,Testability,log,log,72,"Script:; ```python3; #!/usr/bin/env python3; import hail as hl; hl.init(log='/dev/null'); mt = hl.import_vcf('src/test/resources/sample.vcf'); mt.filter_rows(mt.locus < hl.Locus('1', 1)).show(); ```. Output:; ```; 2019-06-24 19:12:05 WARN Utils:66 - Your hostname, wp086-661 resolves to a loopback address: 127.0.1.1; using 10.1.8.50 instead (on interface wlp2s0); 2019-06-24 19:12:05 WARN Utils:66 - Set SPARK_LOCAL_IP if you need to bind to another address; 2019-06-24 19:12:06 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Running on Apache Spark version 2.4.0; SparkUI available at http://wp086-661.broadinstitute.org:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.16-e95038bbed35; LOGGING: writing to /dev/null; Traceback (most recent call last):; File ""/tmp/x"", line 4, in <module>; mt.filter_rows(mt.locus < hl.Locus('1', 1)).show(); File ""</home/BROAD.MIT.EDU/cvittal/.cache/hail-env/lib/python3.6/site-packages/decorator.py:decorator-gen-1000>"", line 2, in show; File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/matrixtable.py"", line 2569, in show; actual_n_cols = self.count_cols(); File ""</home/BROAD.MIT.EDU/cvittal/.cache/hail-env/lib/python3.6/site-packages/decorator.py:decorator-gen-994>"", line 2, in count_cols; File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/matrixtable.py"", line 2404, in count_cols; return Env.backend().execute(ir); File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/py",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6458
https://github.com/hail-is/hail/issues/6458:114,Testability,test,test,114,"Script:; ```python3; #!/usr/bin/env python3; import hail as hl; hl.init(log='/dev/null'); mt = hl.import_vcf('src/test/resources/sample.vcf'); mt.filter_rows(mt.locus < hl.Locus('1', 1)).show(); ```. Output:; ```; 2019-06-24 19:12:05 WARN Utils:66 - Your hostname, wp086-661 resolves to a loopback address: 127.0.1.1; using 10.1.8.50 instead (on interface wlp2s0); 2019-06-24 19:12:05 WARN Utils:66 - Set SPARK_LOCAL_IP if you need to bind to another address; 2019-06-24 19:12:06 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Running on Apache Spark version 2.4.0; SparkUI available at http://wp086-661.broadinstitute.org:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.16-e95038bbed35; LOGGING: writing to /dev/null; Traceback (most recent call last):; File ""/tmp/x"", line 4, in <module>; mt.filter_rows(mt.locus < hl.Locus('1', 1)).show(); File ""</home/BROAD.MIT.EDU/cvittal/.cache/hail-env/lib/python3.6/site-packages/decorator.py:decorator-gen-1000>"", line 2, in show; File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/matrixtable.py"", line 2569, in show; actual_n_cols = self.count_cols(); File ""</home/BROAD.MIT.EDU/cvittal/.cache/hail-env/lib/python3.6/site-packages/decorator.py:decorator-gen-994>"", line 2, in count_cols; File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/matrixtable.py"", line 2404, in count_cols; return Env.backend().execute(ir); File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/py",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6458
https://github.com/hail-is/hail/issues/6458:626,Testability,log,log,626,"Script:; ```python3; #!/usr/bin/env python3; import hail as hl; hl.init(log='/dev/null'); mt = hl.import_vcf('src/test/resources/sample.vcf'); mt.filter_rows(mt.locus < hl.Locus('1', 1)).show(); ```. Output:; ```; 2019-06-24 19:12:05 WARN Utils:66 - Your hostname, wp086-661 resolves to a loopback address: 127.0.1.1; using 10.1.8.50 instead (on interface wlp2s0); 2019-06-24 19:12:05 WARN Utils:66 - Set SPARK_LOCAL_IP if you need to bind to another address; 2019-06-24 19:12:06 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Running on Apache Spark version 2.4.0; SparkUI available at http://wp086-661.broadinstitute.org:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.16-e95038bbed35; LOGGING: writing to /dev/null; Traceback (most recent call last):; File ""/tmp/x"", line 4, in <module>; mt.filter_rows(mt.locus < hl.Locus('1', 1)).show(); File ""</home/BROAD.MIT.EDU/cvittal/.cache/hail-env/lib/python3.6/site-packages/decorator.py:decorator-gen-1000>"", line 2, in show; File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/matrixtable.py"", line 2569, in show; actual_n_cols = self.count_cols(); File ""</home/BROAD.MIT.EDU/cvittal/.cache/hail-env/lib/python3.6/site-packages/decorator.py:decorator-gen-994>"", line 2, in count_cols; File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/matrixtable.py"", line 2404, in count_cols; return Env.backend().execute(ir); File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/py",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6458
https://github.com/hail-is/hail/issues/6458:658,Testability,log,logging,658,"Script:; ```python3; #!/usr/bin/env python3; import hail as hl; hl.init(log='/dev/null'); mt = hl.import_vcf('src/test/resources/sample.vcf'); mt.filter_rows(mt.locus < hl.Locus('1', 1)).show(); ```. Output:; ```; 2019-06-24 19:12:05 WARN Utils:66 - Your hostname, wp086-661 resolves to a loopback address: 127.0.1.1; using 10.1.8.50 instead (on interface wlp2s0); 2019-06-24 19:12:05 WARN Utils:66 - Set SPARK_LOCAL_IP if you need to bind to another address; 2019-06-24 19:12:06 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Running on Apache Spark version 2.4.0; SparkUI available at http://wp086-661.broadinstitute.org:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.16-e95038bbed35; LOGGING: writing to /dev/null; Traceback (most recent call last):; File ""/tmp/x"", line 4, in <module>; mt.filter_rows(mt.locus < hl.Locus('1', 1)).show(); File ""</home/BROAD.MIT.EDU/cvittal/.cache/hail-env/lib/python3.6/site-packages/decorator.py:decorator-gen-1000>"", line 2, in show; File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/matrixtable.py"", line 2569, in show; actual_n_cols = self.count_cols(); File ""</home/BROAD.MIT.EDU/cvittal/.cache/hail-env/lib/python3.6/site-packages/decorator.py:decorator-gen-994>"", line 2, in count_cols; File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/matrixtable.py"", line 2404, in count_cols; return Env.backend().execute(ir); File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/py",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6458
https://github.com/hail-is/hail/issues/6458:948,Testability,LOG,LOGGING,948,"Script:; ```python3; #!/usr/bin/env python3; import hail as hl; hl.init(log='/dev/null'); mt = hl.import_vcf('src/test/resources/sample.vcf'); mt.filter_rows(mt.locus < hl.Locus('1', 1)).show(); ```. Output:; ```; 2019-06-24 19:12:05 WARN Utils:66 - Your hostname, wp086-661 resolves to a loopback address: 127.0.1.1; using 10.1.8.50 instead (on interface wlp2s0); 2019-06-24 19:12:05 WARN Utils:66 - Set SPARK_LOCAL_IP if you need to bind to another address; 2019-06-24 19:12:06 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Running on Apache Spark version 2.4.0; SparkUI available at http://wp086-661.broadinstitute.org:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.16-e95038bbed35; LOGGING: writing to /dev/null; Traceback (most recent call last):; File ""/tmp/x"", line 4, in <module>; mt.filter_rows(mt.locus < hl.Locus('1', 1)).show(); File ""</home/BROAD.MIT.EDU/cvittal/.cache/hail-env/lib/python3.6/site-packages/decorator.py:decorator-gen-1000>"", line 2, in show; File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/matrixtable.py"", line 2569, in show; actual_n_cols = self.count_cols(); File ""</home/BROAD.MIT.EDU/cvittal/.cache/hail-env/lib/python3.6/site-packages/decorator.py:decorator-gen-994>"", line 2, in count_cols; File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/matrixtable.py"", line 2404, in count_cols; return Env.backend().execute(ir); File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/py",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6458
https://github.com/hail-is/hail/pull/6462:66,Testability,test,tests,66,"Exports BGEN 1.2 with 8 bits per probability. Needs docs and more tests, not quite ready yet.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6462
https://github.com/hail-is/hail/issues/6466:2890,Availability,Toler,Tolerations,2890,"F=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747} --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8} --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20} --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinCaseCtrl=TRUE 2>&1 | tee ${__RESOURCE_FILE__749}; State: Waiting; Reason: ContainerCreating; Ready: False; Restart Count: 0; Requests:; cpu: 1; memory: 500M; Environment:; POD_IP: (v1:status.podIP); POD_NAME: batch-2554-job-4-main-cc8d4 (v1:metadata.name); Mounts:; /gsa-key from gsa-key (rw); /io from batch-2554-job-4-8vvgl (rw); /var/run/secrets/kubernetes.io/serviceaccount from default-token-8h99c (ro); Conditions:; Type Status; Initialized True ; Ready False ; ContainersReady False ; PodScheduled True ; Volumes:; gsa-key:; Type: Secret (a volume populated by a Secret); SecretName: konradk-gsa-key; Optional: false; batch-2554-job-4-8vvgl:; Type: PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace); ClaimName: batch-2554-job-4-8vvgl; ReadOnly: false; default-token-8h99c:; Type: Secret (a volume populated by a Secret); SecretName: default-token-8h99c; Optional: false; QoS Class: Burstable; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; preemptible=true; Events:; Type Reason Age From Message; ---- ------ ---- ---- -------; Warning FailedMount 2m59s (x248 over 9h) kubelet, gke-vdc-preemptible-pool-9c7148b2-4gq2 Unable to mount volumes for pod ""batch-2554-job-4-main-cc8d4_batch-pods(968b4ba5-96f6-11e9-8aa3-42010a80015f)"": timeout expired waiting for volumes to attach or mount for pod ""batch-pods""/""batch-2554-job-4-main-cc8d4"". list of unmounted volumes=[batch-2554-job-4-8vvgl]. list of unattached volumes=[gsa-key batch-2554-job-4-8vvgl default-token-8h99c]; # k get pod batch-2554-job-4-main-cc8d4 -n batch-pods -o yaml ; apiV",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466
https://github.com/hail-is/hail/issues/6466:6367,Availability,toler,tolerations,6367,"__6} --varianceRatioFile=${__RESOURCE_FILE__8}; --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20}; --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinCaseCtrl=TRUE; 2>&1 | tee ${__RESOURCE_FILE__749}; env:; - name: POD_IP; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: status.podIP; - name: POD_NAME; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: metadata.name; image: konradjk/saige:0.35.8.2.2; imagePullPolicy: IfNotPresent; name: main; resources:; requests:; cpu: ""1""; memory: 500M; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /gsa-key; name: gsa-key; - mountPath: /io; name: batch-2554-job-4-8vvgl; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: default-token-8h99c; readOnly: true; dnsPolicy: ClusterFirst; enableServiceLinks: true; nodeName: gke-vdc-preemptible-pool-9c7148b2-4gq2; priority: 500000; priorityClassName: user; restartPolicy: Never; schedulerName: default-scheduler; securityContext: {}; serviceAccount: default; serviceAccountName: default; terminationGracePeriodSeconds: 30; tolerations:; - key: preemptible; value: ""true""; - effect: NoExecute; key: node.kubernetes.io/not-ready; operator: Exists; tolerationSeconds: 300; - effect: NoExecute; key: node.kubernetes.io/unreachable; operator: Exists; tolerationSeconds: 300; volumes:; - name: gsa-key; secret:; defaultMode: 420; secretName: konradk-gsa-key; - name: batch-2554-job-4-8vvgl; persistentVolumeClaim:; claimName: batch-2554-job-4-8vvgl; - name: default-token-8h99c; secret:; defaultMode: 420; secretName: default-token-8h99c; status:; conditions:; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T03:09:04Z""; status: ""True""; type: Initialized; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T03:09:04Z""; message: 'containers with unready status: [main]'; reason: ContainersNotReady; status: ""False""; type: Ready; - lastProbeTime: null; lastTransitionTime: """,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466
https://github.com/hail-is/hail/issues/6466:6490,Availability,toler,tolerationSeconds,6490,"P; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: status.podIP; - name: POD_NAME; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: metadata.name; image: konradjk/saige:0.35.8.2.2; imagePullPolicy: IfNotPresent; name: main; resources:; requests:; cpu: ""1""; memory: 500M; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /gsa-key; name: gsa-key; - mountPath: /io; name: batch-2554-job-4-8vvgl; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: default-token-8h99c; readOnly: true; dnsPolicy: ClusterFirst; enableServiceLinks: true; nodeName: gke-vdc-preemptible-pool-9c7148b2-4gq2; priority: 500000; priorityClassName: user; restartPolicy: Never; schedulerName: default-scheduler; securityContext: {}; serviceAccount: default; serviceAccountName: default; terminationGracePeriodSeconds: 30; tolerations:; - key: preemptible; value: ""true""; - effect: NoExecute; key: node.kubernetes.io/not-ready; operator: Exists; tolerationSeconds: 300; - effect: NoExecute; key: node.kubernetes.io/unreachable; operator: Exists; tolerationSeconds: 300; volumes:; - name: gsa-key; secret:; defaultMode: 420; secretName: konradk-gsa-key; - name: batch-2554-job-4-8vvgl; persistentVolumeClaim:; claimName: batch-2554-job-4-8vvgl; - name: default-token-8h99c; secret:; defaultMode: 420; secretName: default-token-8h99c; status:; conditions:; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T03:09:04Z""; status: ""True""; type: Initialized; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T03:09:04Z""; message: 'containers with unready status: [main]'; reason: ContainersNotReady; status: ""False""; type: Ready; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T03:09:04Z""; message: 'containers with unready status: [main]'; reason: ContainersNotReady; status: ""False""; type: ContainersReady; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T03:09:04Z""; status: ""True""; type: PodScheduled; containerStatuses:; - image: konradj",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466
https://github.com/hail-is/hail/issues/6466:6590,Availability,toler,tolerationSeconds,6590,"8h99c; readOnly: true; dnsPolicy: ClusterFirst; enableServiceLinks: true; nodeName: gke-vdc-preemptible-pool-9c7148b2-4gq2; priority: 500000; priorityClassName: user; restartPolicy: Never; schedulerName: default-scheduler; securityContext: {}; serviceAccount: default; serviceAccountName: default; terminationGracePeriodSeconds: 30; tolerations:; - key: preemptible; value: ""true""; - effect: NoExecute; key: node.kubernetes.io/not-ready; operator: Exists; tolerationSeconds: 300; - effect: NoExecute; key: node.kubernetes.io/unreachable; operator: Exists; tolerationSeconds: 300; volumes:; - name: gsa-key; secret:; defaultMode: 420; secretName: konradk-gsa-key; - name: batch-2554-job-4-8vvgl; persistentVolumeClaim:; claimName: batch-2554-job-4-8vvgl; - name: default-token-8h99c; secret:; defaultMode: 420; secretName: default-token-8h99c; status:; conditions:; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T03:09:04Z""; status: ""True""; type: Initialized; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T03:09:04Z""; message: 'containers with unready status: [main]'; reason: ContainersNotReady; status: ""False""; type: Ready; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T03:09:04Z""; message: 'containers with unready status: [main]'; reason: ContainersNotReady; status: ""False""; type: ContainersReady; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T03:09:04Z""; status: ""True""; type: PodScheduled; containerStatuses:; - image: konradjk/saige:0.35.8.2.2; imageID: """"; lastState: {}; name: main; ready: false; restartCount: 0; state:; waiting:; reason: ContainerCreating; hostIP: 10.128.0.8; phase: Pending; qosClass: Burstable; startTime: ""2019-06-25T03:09:04Z""; ```; PVC in question; ```; # k describe pvc batch-2554-job-4-8vvgl -n batch-pods; Name: batch-2554-job-4-8vvgl; Namespace: batch-pods; StorageClass: batch; Status: Bound; Volume: pvc-32804669-96f6-11e9-8aa3-42010a80015f; Labels: app=batch-job; hail.is/batch-instance=cd50b95a89914efb897965a5e982a29d; Ann",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466
https://github.com/hail-is/hail/issues/6466:630,Deployability,pipeline,pipeline,630,"Batch 2554, job 4. Pod in question; ```; # k describe pod batch-2554-job-4-main-cc8d4 -n batch-pods ; Name: batch-2554-job-4-main-cc8d4; Namespace: batch-pods; Priority: 500000; PriorityClassName: user; Node: gke-vdc-preemptible-pool-9c7148b2-4gq2/10.128.0.8; Start Time: Mon, 24 Jun 2019 23:09:04 -0400; Labels: app=batch-job; hail.is/batch-instance=cd50b95a89914efb897965a5e982a29d; uuid=3bf0b121f62d4cfea15cf187a21bc0ed; Annotations: <none>; Status: Pending; IP: ; Containers:; main:; Container ID: ; Image: konradjk/saige:0.35.8.2.2; Image ID: ; Port: <none>; Host Port: <none>; Command:; /bin/bash; -c; set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19} --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747} --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8} --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20} --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinCaseCtrl=TRUE 2>&1 | tee ${__RESOURCE_FILE__749}; State: Waiting; Reas",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466
https://github.com/hail-is/hail/issues/6466:639,Deployability,pipeline,pipeline-,639,"Batch 2554, job 4. Pod in question; ```; # k describe pod batch-2554-job-4-main-cc8d4 -n batch-pods ; Name: batch-2554-job-4-main-cc8d4; Namespace: batch-pods; Priority: 500000; PriorityClassName: user; Node: gke-vdc-preemptible-pool-9c7148b2-4gq2/10.128.0.8; Start Time: Mon, 24 Jun 2019 23:09:04 -0400; Labels: app=batch-job; hail.is/batch-instance=cd50b95a89914efb897965a5e982a29d; uuid=3bf0b121f62d4cfea15cf187a21bc0ed; Annotations: <none>; Status: Pending; IP: ; Containers:; main:; Container ID: ; Image: konradjk/saige:0.35.8.2.2; Image ID: ; Port: <none>; Host Port: <none>; Command:; /bin/bash; -c; set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19} --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747} --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8} --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20} --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinCaseCtrl=TRUE 2>&1 | tee ${__RESOURCE_FILE__749}; State: Waiting; Reas",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466
https://github.com/hail-is/hail/issues/6466:698,Deployability,pipeline,pipeline,698,"Batch 2554, job 4. Pod in question; ```; # k describe pod batch-2554-job-4-main-cc8d4 -n batch-pods ; Name: batch-2554-job-4-main-cc8d4; Namespace: batch-pods; Priority: 500000; PriorityClassName: user; Node: gke-vdc-preemptible-pool-9c7148b2-4gq2/10.128.0.8; Start Time: Mon, 24 Jun 2019 23:09:04 -0400; Labels: app=batch-job; hail.is/batch-instance=cd50b95a89914efb897965a5e982a29d; uuid=3bf0b121f62d4cfea15cf187a21bc0ed; Annotations: <none>; Status: Pending; IP: ; Containers:; main:; Container ID: ; Image: konradjk/saige:0.35.8.2.2; Image ID: ; Port: <none>; Host Port: <none>; Command:; /bin/bash; -c; set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19} --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747} --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8} --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20} --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinCaseCtrl=TRUE 2>&1 | tee ${__RESOURCE_FILE__749}; State: Waiting; Reas",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466
https://github.com/hail-is/hail/issues/6466:707,Deployability,pipeline,pipeline-,707,"Batch 2554, job 4. Pod in question; ```; # k describe pod batch-2554-job-4-main-cc8d4 -n batch-pods ; Name: batch-2554-job-4-main-cc8d4; Namespace: batch-pods; Priority: 500000; PriorityClassName: user; Node: gke-vdc-preemptible-pool-9c7148b2-4gq2/10.128.0.8; Start Time: Mon, 24 Jun 2019 23:09:04 -0400; Labels: app=batch-job; hail.is/batch-instance=cd50b95a89914efb897965a5e982a29d; uuid=3bf0b121f62d4cfea15cf187a21bc0ed; Annotations: <none>; Status: Pending; IP: ; Containers:; main:; Container ID: ; Image: konradjk/saige:0.35.8.2.2; Image ID: ; Port: <none>; Host Port: <none>; Command:; /bin/bash; -c; set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19} --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747} --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8} --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20} --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinCaseCtrl=TRUE 2>&1 | tee ${__RESOURCE_FILE__749}; State: Waiting; Reas",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466
https://github.com/hail-is/hail/issues/6466:770,Deployability,pipeline,pipeline,770,"Batch 2554, job 4. Pod in question; ```; # k describe pod batch-2554-job-4-main-cc8d4 -n batch-pods ; Name: batch-2554-job-4-main-cc8d4; Namespace: batch-pods; Priority: 500000; PriorityClassName: user; Node: gke-vdc-preemptible-pool-9c7148b2-4gq2/10.128.0.8; Start Time: Mon, 24 Jun 2019 23:09:04 -0400; Labels: app=batch-job; hail.is/batch-instance=cd50b95a89914efb897965a5e982a29d; uuid=3bf0b121f62d4cfea15cf187a21bc0ed; Annotations: <none>; Status: Pending; IP: ; Containers:; main:; Container ID: ; Image: konradjk/saige:0.35.8.2.2; Image ID: ; Port: <none>; Host Port: <none>; Command:; /bin/bash; -c; set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19} --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747} --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8} --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20} --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinCaseCtrl=TRUE 2>&1 | tee ${__RESOURCE_FILE__749}; State: Waiting; Reas",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466
https://github.com/hail-is/hail/issues/6466:779,Deployability,pipeline,pipeline-,779,"Batch 2554, job 4. Pod in question; ```; # k describe pod batch-2554-job-4-main-cc8d4 -n batch-pods ; Name: batch-2554-job-4-main-cc8d4; Namespace: batch-pods; Priority: 500000; PriorityClassName: user; Node: gke-vdc-preemptible-pool-9c7148b2-4gq2/10.128.0.8; Start Time: Mon, 24 Jun 2019 23:09:04 -0400; Labels: app=batch-job; hail.is/batch-instance=cd50b95a89914efb897965a5e982a29d; uuid=3bf0b121f62d4cfea15cf187a21bc0ed; Annotations: <none>; Status: Pending; IP: ; Containers:; main:; Container ID: ; Image: konradjk/saige:0.35.8.2.2; Image ID: ; Port: <none>; Host Port: <none>; Command:; /bin/bash; -c; set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19} --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747} --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8} --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20} --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinCaseCtrl=TRUE 2>&1 | tee ${__RESOURCE_FILE__749}; State: Waiting; Reas",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466
https://github.com/hail-is/hail/issues/6466:853,Deployability,pipeline,pipeline,853,"Batch 2554, job 4. Pod in question; ```; # k describe pod batch-2554-job-4-main-cc8d4 -n batch-pods ; Name: batch-2554-job-4-main-cc8d4; Namespace: batch-pods; Priority: 500000; PriorityClassName: user; Node: gke-vdc-preemptible-pool-9c7148b2-4gq2/10.128.0.8; Start Time: Mon, 24 Jun 2019 23:09:04 -0400; Labels: app=batch-job; hail.is/batch-instance=cd50b95a89914efb897965a5e982a29d; uuid=3bf0b121f62d4cfea15cf187a21bc0ed; Annotations: <none>; Status: Pending; IP: ; Containers:; main:; Container ID: ; Image: konradjk/saige:0.35.8.2.2; Image ID: ; Port: <none>; Host Port: <none>; Command:; /bin/bash; -c; set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19} --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747} --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8} --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20} --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinCaseCtrl=TRUE 2>&1 | tee ${__RESOURCE_FILE__749}; State: Waiting; Reas",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466
https://github.com/hail-is/hail/issues/6466:862,Deployability,pipeline,pipeline-,862,"Batch 2554, job 4. Pod in question; ```; # k describe pod batch-2554-job-4-main-cc8d4 -n batch-pods ; Name: batch-2554-job-4-main-cc8d4; Namespace: batch-pods; Priority: 500000; PriorityClassName: user; Node: gke-vdc-preemptible-pool-9c7148b2-4gq2/10.128.0.8; Start Time: Mon, 24 Jun 2019 23:09:04 -0400; Labels: app=batch-job; hail.is/batch-instance=cd50b95a89914efb897965a5e982a29d; uuid=3bf0b121f62d4cfea15cf187a21bc0ed; Annotations: <none>; Status: Pending; IP: ; Containers:; main:; Container ID: ; Image: konradjk/saige:0.35.8.2.2; Image ID: ; Port: <none>; Host Port: <none>; Command:; /bin/bash; -c; set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19} --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747} --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8} --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20} --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinCaseCtrl=TRUE 2>&1 | tee ${__RESOURCE_FILE__749}; State: Waiting; Reas",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466
https://github.com/hail-is/hail/issues/6466:931,Deployability,pipeline,pipeline,931,"Batch 2554, job 4. Pod in question; ```; # k describe pod batch-2554-job-4-main-cc8d4 -n batch-pods ; Name: batch-2554-job-4-main-cc8d4; Namespace: batch-pods; Priority: 500000; PriorityClassName: user; Node: gke-vdc-preemptible-pool-9c7148b2-4gq2/10.128.0.8; Start Time: Mon, 24 Jun 2019 23:09:04 -0400; Labels: app=batch-job; hail.is/batch-instance=cd50b95a89914efb897965a5e982a29d; uuid=3bf0b121f62d4cfea15cf187a21bc0ed; Annotations: <none>; Status: Pending; IP: ; Containers:; main:; Container ID: ; Image: konradjk/saige:0.35.8.2.2; Image ID: ; Port: <none>; Host Port: <none>; Command:; /bin/bash; -c; set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19} --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747} --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8} --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20} --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinCaseCtrl=TRUE 2>&1 | tee ${__RESOURCE_FILE__749}; State: Waiting; Reas",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466
https://github.com/hail-is/hail/issues/6466:940,Deployability,pipeline,pipeline-,940,"Batch 2554, job 4. Pod in question; ```; # k describe pod batch-2554-job-4-main-cc8d4 -n batch-pods ; Name: batch-2554-job-4-main-cc8d4; Namespace: batch-pods; Priority: 500000; PriorityClassName: user; Node: gke-vdc-preemptible-pool-9c7148b2-4gq2/10.128.0.8; Start Time: Mon, 24 Jun 2019 23:09:04 -0400; Labels: app=batch-job; hail.is/batch-instance=cd50b95a89914efb897965a5e982a29d; uuid=3bf0b121f62d4cfea15cf187a21bc0ed; Annotations: <none>; Status: Pending; IP: ; Containers:; main:; Container ID: ; Image: konradjk/saige:0.35.8.2.2; Image ID: ; Port: <none>; Host Port: <none>; Command:; /bin/bash; -c; set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19} --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747} --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8} --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20} --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinCaseCtrl=TRUE 2>&1 | tee ${__RESOURCE_FILE__749}; State: Waiting; Reas",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466
https://github.com/hail-is/hail/issues/6466:1011,Deployability,pipeline,pipeline,1011,"batch-2554-job-4-main-cc8d4 -n batch-pods ; Name: batch-2554-job-4-main-cc8d4; Namespace: batch-pods; Priority: 500000; PriorityClassName: user; Node: gke-vdc-preemptible-pool-9c7148b2-4gq2/10.128.0.8; Start Time: Mon, 24 Jun 2019 23:09:04 -0400; Labels: app=batch-job; hail.is/batch-instance=cd50b95a89914efb897965a5e982a29d; uuid=3bf0b121f62d4cfea15cf187a21bc0ed; Annotations: <none>; Status: Pending; IP: ; Containers:; main:; Container ID: ; Image: konradjk/saige:0.35.8.2.2; Image ID: ; Port: <none>; Host Port: <none>; Command:; /bin/bash; -c; set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19} --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747} --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8} --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20} --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinCaseCtrl=TRUE 2>&1 | tee ${__RESOURCE_FILE__749}; State: Waiting; Reason: ContainerCreating; Ready: False; Restart Count: 0; Re",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466
https://github.com/hail-is/hail/issues/6466:1020,Deployability,pipeline,pipeline-,1020,"batch-2554-job-4-main-cc8d4 -n batch-pods ; Name: batch-2554-job-4-main-cc8d4; Namespace: batch-pods; Priority: 500000; PriorityClassName: user; Node: gke-vdc-preemptible-pool-9c7148b2-4gq2/10.128.0.8; Start Time: Mon, 24 Jun 2019 23:09:04 -0400; Labels: app=batch-job; hail.is/batch-instance=cd50b95a89914efb897965a5e982a29d; uuid=3bf0b121f62d4cfea15cf187a21bc0ed; Annotations: <none>; Status: Pending; IP: ; Containers:; main:; Container ID: ; Image: konradjk/saige:0.35.8.2.2; Image ID: ; Port: <none>; Host Port: <none>; Command:; /bin/bash; -c; set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19} --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747} --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8} --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20} --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinCaseCtrl=TRUE 2>&1 | tee ${__RESOURCE_FILE__749}; State: Waiting; Reason: ContainerCreating; Ready: False; Restart Count: 0; Re",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466
https://github.com/hail-is/hail/issues/6466:1085,Deployability,pipeline,pipeline,1085,"batch-2554-job-4-main-cc8d4 -n batch-pods ; Name: batch-2554-job-4-main-cc8d4; Namespace: batch-pods; Priority: 500000; PriorityClassName: user; Node: gke-vdc-preemptible-pool-9c7148b2-4gq2/10.128.0.8; Start Time: Mon, 24 Jun 2019 23:09:04 -0400; Labels: app=batch-job; hail.is/batch-instance=cd50b95a89914efb897965a5e982a29d; uuid=3bf0b121f62d4cfea15cf187a21bc0ed; Annotations: <none>; Status: Pending; IP: ; Containers:; main:; Container ID: ; Image: konradjk/saige:0.35.8.2.2; Image ID: ; Port: <none>; Host Port: <none>; Command:; /bin/bash; -c; set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19} --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747} --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8} --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20} --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinCaseCtrl=TRUE 2>&1 | tee ${__RESOURCE_FILE__749}; State: Waiting; Reason: ContainerCreating; Ready: False; Restart Count: 0; Re",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466
https://github.com/hail-is/hail/issues/6466:1094,Deployability,pipeline,pipeline-,1094,"batch-2554-job-4-main-cc8d4 -n batch-pods ; Name: batch-2554-job-4-main-cc8d4; Namespace: batch-pods; Priority: 500000; PriorityClassName: user; Node: gke-vdc-preemptible-pool-9c7148b2-4gq2/10.128.0.8; Start Time: Mon, 24 Jun 2019 23:09:04 -0400; Labels: app=batch-job; hail.is/batch-instance=cd50b95a89914efb897965a5e982a29d; uuid=3bf0b121f62d4cfea15cf187a21bc0ed; Annotations: <none>; Status: Pending; IP: ; Containers:; main:; Container ID: ; Image: konradjk/saige:0.35.8.2.2; Image ID: ; Port: <none>; Host Port: <none>; Command:; /bin/bash; -c; set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19} --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747} --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8} --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20} --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinCaseCtrl=TRUE 2>&1 | tee ${__RESOURCE_FILE__749}; State: Waiting; Reason: ContainerCreating; Ready: False; Restart Count: 0; Re",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466
https://github.com/hail-is/hail/issues/6466:1245,Deployability,pipeline,pipeline,1245,".8; Start Time: Mon, 24 Jun 2019 23:09:04 -0400; Labels: app=batch-job; hail.is/batch-instance=cd50b95a89914efb897965a5e982a29d; uuid=3bf0b121f62d4cfea15cf187a21bc0ed; Annotations: <none>; Status: Pending; IP: ; Containers:; main:; Container ID: ; Image: konradjk/saige:0.35.8.2.2; Image ID: ; Port: <none>; Host Port: <none>; Command:; /bin/bash; -c; set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19} --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747} --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8} --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20} --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinCaseCtrl=TRUE 2>&1 | tee ${__RESOURCE_FILE__749}; State: Waiting; Reason: ContainerCreating; Ready: False; Restart Count: 0; Requests:; cpu: 1; memory: 500M; Environment:; POD_IP: (v1:status.podIP); POD_NAME: batch-2554-job-4-main-cc8d4 (v1:metadata.name); Mounts:; /gsa-key from gsa-key (rw); /io from batch-2554-job-4-8vvgl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466
https://github.com/hail-is/hail/issues/6466:1254,Deployability,pipeline,pipeline-,1254,".8; Start Time: Mon, 24 Jun 2019 23:09:04 -0400; Labels: app=batch-job; hail.is/batch-instance=cd50b95a89914efb897965a5e982a29d; uuid=3bf0b121f62d4cfea15cf187a21bc0ed; Annotations: <none>; Status: Pending; IP: ; Containers:; main:; Container ID: ; Image: konradjk/saige:0.35.8.2.2; Image ID: ; Port: <none>; Host Port: <none>; Command:; /bin/bash; -c; set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19} --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747} --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8} --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20} --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinCaseCtrl=TRUE 2>&1 | tee ${__RESOURCE_FILE__749}; State: Waiting; Reason: ContainerCreating; Ready: False; Restart Count: 0; Requests:; cpu: 1; memory: 500M; Environment:; POD_IP: (v1:status.podIP); POD_NAME: batch-2554-job-4-main-cc8d4 (v1:metadata.name); Mounts:; /gsa-key from gsa-key (rw); /io from batch-2554-job-4-8vvgl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466
https://github.com/hail-is/hail/issues/6466:1344,Deployability,pipeline,pipeline,1344,cf187a21bc0ed; Annotations: <none>; Status: Pending; IP: ; Containers:; main:; Container ID: ; Image: konradjk/saige:0.35.8.2.2; Image ID: ; Port: <none>; Host Port: <none>; Command:; /bin/bash; -c; set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19} --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747} --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8} --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20} --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinCaseCtrl=TRUE 2>&1 | tee ${__RESOURCE_FILE__749}; State: Waiting; Reason: ContainerCreating; Ready: False; Restart Count: 0; Requests:; cpu: 1; memory: 500M; Environment:; POD_IP: (v1:status.podIP); POD_NAME: batch-2554-job-4-main-cc8d4 (v1:metadata.name); Mounts:; /gsa-key from gsa-key (rw); /io from batch-2554-job-4-8vvgl (rw); /var/run/secrets/kubernetes.io/serviceaccount from default-token-8h99c (ro); Conditions:; Type Status; Initialized True ; Ready False ; ContainersR,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466
https://github.com/hail-is/hail/issues/6466:1353,Deployability,pipeline,pipeline-,1353,cf187a21bc0ed; Annotations: <none>; Status: Pending; IP: ; Containers:; main:; Container ID: ; Image: konradjk/saige:0.35.8.2.2; Image ID: ; Port: <none>; Host Port: <none>; Command:; /bin/bash; -c; set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19} --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747} --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8} --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20} --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinCaseCtrl=TRUE 2>&1 | tee ${__RESOURCE_FILE__749}; State: Waiting; Reason: ContainerCreating; Ready: False; Restart Count: 0; Requests:; cpu: 1; memory: 500M; Environment:; POD_IP: (v1:status.podIP); POD_NAME: batch-2554-job-4-main-cc8d4 (v1:metadata.name); Mounts:; /gsa-key from gsa-key (rw); /io from batch-2554-job-4-8vvgl (rw); /var/run/secrets/kubernetes.io/serviceaccount from default-token-8h99c (ro); Conditions:; Type Status; Initialized True ; Ready False ; ContainersR,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466
https://github.com/hail-is/hail/issues/6466:1419,Deployability,pipeline,pipeline,1419,cf187a21bc0ed; Annotations: <none>; Status: Pending; IP: ; Containers:; main:; Container ID: ; Image: konradjk/saige:0.35.8.2.2; Image ID: ; Port: <none>; Host Port: <none>; Command:; /bin/bash; -c; set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19} --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747} --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8} --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20} --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinCaseCtrl=TRUE 2>&1 | tee ${__RESOURCE_FILE__749}; State: Waiting; Reason: ContainerCreating; Ready: False; Restart Count: 0; Requests:; cpu: 1; memory: 500M; Environment:; POD_IP: (v1:status.podIP); POD_NAME: batch-2554-job-4-main-cc8d4 (v1:metadata.name); Mounts:; /gsa-key from gsa-key (rw); /io from batch-2554-job-4-8vvgl (rw); /var/run/secrets/kubernetes.io/serviceaccount from default-token-8h99c (ro); Conditions:; Type Status; Initialized True ; Ready False ; ContainersR,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466
https://github.com/hail-is/hail/issues/6466:1428,Deployability,pipeline,pipeline-,1428,cf187a21bc0ed; Annotations: <none>; Status: Pending; IP: ; Containers:; main:; Container ID: ; Image: konradjk/saige:0.35.8.2.2; Image ID: ; Port: <none>; Host Port: <none>; Command:; /bin/bash; -c; set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19} --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747} --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8} --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20} --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinCaseCtrl=TRUE 2>&1 | tee ${__RESOURCE_FILE__749}; State: Waiting; Reason: ContainerCreating; Ready: False; Restart Count: 0; Requests:; cpu: 1; memory: 500M; Environment:; POD_IP: (v1:status.podIP); POD_NAME: batch-2554-job-4-main-cc8d4 (v1:metadata.name); Mounts:; /gsa-key from gsa-key (rw); /io from batch-2554-job-4-8vvgl (rw); /var/run/secrets/kubernetes.io/serviceaccount from default-token-8h99c (ro); Conditions:; Type Status; Initialized True ; Ready False ; ContainersR,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466
https://github.com/hail-is/hail/issues/6466:4117,Deployability,pipeline,pipeline,4117,"ts:; Type Reason Age From Message; ---- ------ ---- ---- -------; Warning FailedMount 2m59s (x248 over 9h) kubelet, gke-vdc-preemptible-pool-9c7148b2-4gq2 Unable to mount volumes for pod ""batch-2554-job-4-main-cc8d4_batch-pods(968b4ba5-96f6-11e9-8aa3-42010a80015f)"": timeout expired waiting for volumes to attach or mount for pod ""batch-pods""/""batch-2554-job-4-main-cc8d4"". list of unmounted volumes=[batch-2554-job-4-8vvgl]. list of unattached volumes=[gsa-key batch-2554-job-4-8vvgl default-token-8h99c]; # k get pod batch-2554-job-4-main-cc8d4 -n batch-pods -o yaml ; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-06-25T03:09:04Z""; generateName: batch-2554-job-4-main-; labels:; app: batch-job; hail.is/batch-instance: cd50b95a89914efb897965a5e982a29d; uuid: 3bf0b121f62d4cfea15cf187a21bc0ed; name: batch-2554-job-4-main-cc8d4; namespace: batch-pods; resourceVersion: ""72628848""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-2554-job-4-main-cc8d4; uid: 968b4ba5-96f6-11e9-8aa3-42010a80015f; spec:; containers:; - command:; - /bin/bash; - -c; - set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9;; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi;; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz;; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda;; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0;; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx;; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt;; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d;; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript; /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466
https://github.com/hail-is/hail/issues/6466:4126,Deployability,pipeline,pipeline-,4126,"ts:; Type Reason Age From Message; ---- ------ ---- ---- -------; Warning FailedMount 2m59s (x248 over 9h) kubelet, gke-vdc-preemptible-pool-9c7148b2-4gq2 Unable to mount volumes for pod ""batch-2554-job-4-main-cc8d4_batch-pods(968b4ba5-96f6-11e9-8aa3-42010a80015f)"": timeout expired waiting for volumes to attach or mount for pod ""batch-pods""/""batch-2554-job-4-main-cc8d4"". list of unmounted volumes=[batch-2554-job-4-8vvgl]. list of unattached volumes=[gsa-key batch-2554-job-4-8vvgl default-token-8h99c]; # k get pod batch-2554-job-4-main-cc8d4 -n batch-pods -o yaml ; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-06-25T03:09:04Z""; generateName: batch-2554-job-4-main-; labels:; app: batch-job; hail.is/batch-instance: cd50b95a89914efb897965a5e982a29d; uuid: 3bf0b121f62d4cfea15cf187a21bc0ed; name: batch-2554-job-4-main-cc8d4; namespace: batch-pods; resourceVersion: ""72628848""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-2554-job-4-main-cc8d4; uid: 968b4ba5-96f6-11e9-8aa3-42010a80015f; spec:; containers:; - command:; - /bin/bash; - -c; - set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9;; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi;; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz;; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda;; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0;; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx;; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt;; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d;; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript; /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466
https://github.com/hail-is/hail/issues/6466:4185,Deployability,pipeline,pipeline,4185,"ts:; Type Reason Age From Message; ---- ------ ---- ---- -------; Warning FailedMount 2m59s (x248 over 9h) kubelet, gke-vdc-preemptible-pool-9c7148b2-4gq2 Unable to mount volumes for pod ""batch-2554-job-4-main-cc8d4_batch-pods(968b4ba5-96f6-11e9-8aa3-42010a80015f)"": timeout expired waiting for volumes to attach or mount for pod ""batch-pods""/""batch-2554-job-4-main-cc8d4"". list of unmounted volumes=[batch-2554-job-4-8vvgl]. list of unattached volumes=[gsa-key batch-2554-job-4-8vvgl default-token-8h99c]; # k get pod batch-2554-job-4-main-cc8d4 -n batch-pods -o yaml ; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-06-25T03:09:04Z""; generateName: batch-2554-job-4-main-; labels:; app: batch-job; hail.is/batch-instance: cd50b95a89914efb897965a5e982a29d; uuid: 3bf0b121f62d4cfea15cf187a21bc0ed; name: batch-2554-job-4-main-cc8d4; namespace: batch-pods; resourceVersion: ""72628848""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-2554-job-4-main-cc8d4; uid: 968b4ba5-96f6-11e9-8aa3-42010a80015f; spec:; containers:; - command:; - /bin/bash; - -c; - set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9;; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi;; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz;; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda;; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0;; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx;; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt;; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d;; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript; /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466
https://github.com/hail-is/hail/issues/6466:4194,Deployability,pipeline,pipeline-,4194,"ts:; Type Reason Age From Message; ---- ------ ---- ---- -------; Warning FailedMount 2m59s (x248 over 9h) kubelet, gke-vdc-preemptible-pool-9c7148b2-4gq2 Unable to mount volumes for pod ""batch-2554-job-4-main-cc8d4_batch-pods(968b4ba5-96f6-11e9-8aa3-42010a80015f)"": timeout expired waiting for volumes to attach or mount for pod ""batch-pods""/""batch-2554-job-4-main-cc8d4"". list of unmounted volumes=[batch-2554-job-4-8vvgl]. list of unattached volumes=[gsa-key batch-2554-job-4-8vvgl default-token-8h99c]; # k get pod batch-2554-job-4-main-cc8d4 -n batch-pods -o yaml ; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-06-25T03:09:04Z""; generateName: batch-2554-job-4-main-; labels:; app: batch-job; hail.is/batch-instance: cd50b95a89914efb897965a5e982a29d; uuid: 3bf0b121f62d4cfea15cf187a21bc0ed; name: batch-2554-job-4-main-cc8d4; namespace: batch-pods; resourceVersion: ""72628848""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-2554-job-4-main-cc8d4; uid: 968b4ba5-96f6-11e9-8aa3-42010a80015f; spec:; containers:; - command:; - /bin/bash; - -c; - set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9;; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi;; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz;; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda;; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0;; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx;; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt;; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d;; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript; /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466
https://github.com/hail-is/hail/issues/6466:4258,Deployability,pipeline,pipeline,4258,"ts:; Type Reason Age From Message; ---- ------ ---- ---- -------; Warning FailedMount 2m59s (x248 over 9h) kubelet, gke-vdc-preemptible-pool-9c7148b2-4gq2 Unable to mount volumes for pod ""batch-2554-job-4-main-cc8d4_batch-pods(968b4ba5-96f6-11e9-8aa3-42010a80015f)"": timeout expired waiting for volumes to attach or mount for pod ""batch-pods""/""batch-2554-job-4-main-cc8d4"". list of unmounted volumes=[batch-2554-job-4-8vvgl]. list of unattached volumes=[gsa-key batch-2554-job-4-8vvgl default-token-8h99c]; # k get pod batch-2554-job-4-main-cc8d4 -n batch-pods -o yaml ; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-06-25T03:09:04Z""; generateName: batch-2554-job-4-main-; labels:; app: batch-job; hail.is/batch-instance: cd50b95a89914efb897965a5e982a29d; uuid: 3bf0b121f62d4cfea15cf187a21bc0ed; name: batch-2554-job-4-main-cc8d4; namespace: batch-pods; resourceVersion: ""72628848""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-2554-job-4-main-cc8d4; uid: 968b4ba5-96f6-11e9-8aa3-42010a80015f; spec:; containers:; - command:; - /bin/bash; - -c; - set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9;; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi;; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz;; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda;; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0;; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx;; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt;; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d;; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript; /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466
https://github.com/hail-is/hail/issues/6466:4267,Deployability,pipeline,pipeline-,4267,"ts:; Type Reason Age From Message; ---- ------ ---- ---- -------; Warning FailedMount 2m59s (x248 over 9h) kubelet, gke-vdc-preemptible-pool-9c7148b2-4gq2 Unable to mount volumes for pod ""batch-2554-job-4-main-cc8d4_batch-pods(968b4ba5-96f6-11e9-8aa3-42010a80015f)"": timeout expired waiting for volumes to attach or mount for pod ""batch-pods""/""batch-2554-job-4-main-cc8d4"". list of unmounted volumes=[batch-2554-job-4-8vvgl]. list of unattached volumes=[gsa-key batch-2554-job-4-8vvgl default-token-8h99c]; # k get pod batch-2554-job-4-main-cc8d4 -n batch-pods -o yaml ; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-06-25T03:09:04Z""; generateName: batch-2554-job-4-main-; labels:; app: batch-job; hail.is/batch-instance: cd50b95a89914efb897965a5e982a29d; uuid: 3bf0b121f62d4cfea15cf187a21bc0ed; name: batch-2554-job-4-main-cc8d4; namespace: batch-pods; resourceVersion: ""72628848""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-2554-job-4-main-cc8d4; uid: 968b4ba5-96f6-11e9-8aa3-42010a80015f; spec:; containers:; - command:; - /bin/bash; - -c; - set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9;; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi;; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz;; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda;; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0;; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx;; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt;; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d;; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript; /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466
https://github.com/hail-is/hail/issues/6466:4342,Deployability,pipeline,pipeline,4342," pod ""batch-pods""/""batch-2554-job-4-main-cc8d4"". list of unmounted volumes=[batch-2554-job-4-8vvgl]. list of unattached volumes=[gsa-key batch-2554-job-4-8vvgl default-token-8h99c]; # k get pod batch-2554-job-4-main-cc8d4 -n batch-pods -o yaml ; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-06-25T03:09:04Z""; generateName: batch-2554-job-4-main-; labels:; app: batch-job; hail.is/batch-instance: cd50b95a89914efb897965a5e982a29d; uuid: 3bf0b121f62d4cfea15cf187a21bc0ed; name: batch-2554-job-4-main-cc8d4; namespace: batch-pods; resourceVersion: ""72628848""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-2554-job-4-main-cc8d4; uid: 968b4ba5-96f6-11e9-8aa3-42010a80015f; spec:; containers:; - command:; - /bin/bash; - -c; - set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9;; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi;; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz;; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda;; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0;; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx;; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt;; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d;; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript; /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19}; --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747}; --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8}; --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20}; --sp",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466
https://github.com/hail-is/hail/issues/6466:4351,Deployability,pipeline,pipeline-,4351," pod ""batch-pods""/""batch-2554-job-4-main-cc8d4"". list of unmounted volumes=[batch-2554-job-4-8vvgl]. list of unattached volumes=[gsa-key batch-2554-job-4-8vvgl default-token-8h99c]; # k get pod batch-2554-job-4-main-cc8d4 -n batch-pods -o yaml ; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-06-25T03:09:04Z""; generateName: batch-2554-job-4-main-; labels:; app: batch-job; hail.is/batch-instance: cd50b95a89914efb897965a5e982a29d; uuid: 3bf0b121f62d4cfea15cf187a21bc0ed; name: batch-2554-job-4-main-cc8d4; namespace: batch-pods; resourceVersion: ""72628848""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-2554-job-4-main-cc8d4; uid: 968b4ba5-96f6-11e9-8aa3-42010a80015f; spec:; containers:; - command:; - /bin/bash; - -c; - set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9;; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi;; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz;; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda;; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0;; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx;; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt;; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d;; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript; /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19}; --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747}; --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8}; --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20}; --sp",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466
https://github.com/hail-is/hail/issues/6466:4421,Deployability,pipeline,pipeline,4421,"2554-job-4-8vvgl]. list of unattached volumes=[gsa-key batch-2554-job-4-8vvgl default-token-8h99c]; # k get pod batch-2554-job-4-main-cc8d4 -n batch-pods -o yaml ; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-06-25T03:09:04Z""; generateName: batch-2554-job-4-main-; labels:; app: batch-job; hail.is/batch-instance: cd50b95a89914efb897965a5e982a29d; uuid: 3bf0b121f62d4cfea15cf187a21bc0ed; name: batch-2554-job-4-main-cc8d4; namespace: batch-pods; resourceVersion: ""72628848""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-2554-job-4-main-cc8d4; uid: 968b4ba5-96f6-11e9-8aa3-42010a80015f; spec:; containers:; - command:; - /bin/bash; - -c; - set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9;; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi;; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz;; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda;; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0;; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx;; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt;; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d;; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript; /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19}; --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747}; --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8}; --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20}; --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinC",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466
https://github.com/hail-is/hail/issues/6466:4430,Deployability,pipeline,pipeline-,4430,"2554-job-4-8vvgl]. list of unattached volumes=[gsa-key batch-2554-job-4-8vvgl default-token-8h99c]; # k get pod batch-2554-job-4-main-cc8d4 -n batch-pods -o yaml ; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-06-25T03:09:04Z""; generateName: batch-2554-job-4-main-; labels:; app: batch-job; hail.is/batch-instance: cd50b95a89914efb897965a5e982a29d; uuid: 3bf0b121f62d4cfea15cf187a21bc0ed; name: batch-2554-job-4-main-cc8d4; namespace: batch-pods; resourceVersion: ""72628848""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-2554-job-4-main-cc8d4; uid: 968b4ba5-96f6-11e9-8aa3-42010a80015f; spec:; containers:; - command:; - /bin/bash; - -c; - set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9;; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi;; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz;; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda;; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0;; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx;; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt;; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d;; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript; /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19}; --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747}; --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8}; --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20}; --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinC",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466
https://github.com/hail-is/hail/issues/6466:4502,Deployability,pipeline,pipeline,4502,"-2554-job-4-main-cc8d4 -n batch-pods -o yaml ; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-06-25T03:09:04Z""; generateName: batch-2554-job-4-main-; labels:; app: batch-job; hail.is/batch-instance: cd50b95a89914efb897965a5e982a29d; uuid: 3bf0b121f62d4cfea15cf187a21bc0ed; name: batch-2554-job-4-main-cc8d4; namespace: batch-pods; resourceVersion: ""72628848""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-2554-job-4-main-cc8d4; uid: 968b4ba5-96f6-11e9-8aa3-42010a80015f; spec:; containers:; - command:; - /bin/bash; - -c; - set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9;; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi;; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz;; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda;; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0;; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx;; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt;; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d;; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript; /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19}; --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747}; --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8}; --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20}; --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinCaseCtrl=TRUE; 2>&1 | tee ${__RESOURCE_FILE__749}; env:; - name: POD_IP; valueFrom:; fieldRef:; apiVersion: v1; fieldP",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466
https://github.com/hail-is/hail/issues/6466:4511,Deployability,pipeline,pipeline-,4511,"-2554-job-4-main-cc8d4 -n batch-pods -o yaml ; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-06-25T03:09:04Z""; generateName: batch-2554-job-4-main-; labels:; app: batch-job; hail.is/batch-instance: cd50b95a89914efb897965a5e982a29d; uuid: 3bf0b121f62d4cfea15cf187a21bc0ed; name: batch-2554-job-4-main-cc8d4; namespace: batch-pods; resourceVersion: ""72628848""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-2554-job-4-main-cc8d4; uid: 968b4ba5-96f6-11e9-8aa3-42010a80015f; spec:; containers:; - command:; - /bin/bash; - -c; - set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9;; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi;; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz;; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda;; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0;; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx;; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt;; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d;; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript; /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19}; --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747}; --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8}; --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20}; --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinCaseCtrl=TRUE; 2>&1 | tee ${__RESOURCE_FILE__749}; env:; - name: POD_IP; valueFrom:; fieldRef:; apiVersion: v1; fieldP",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466
https://github.com/hail-is/hail/issues/6466:4577,Deployability,pipeline,pipeline,4577,"-2554-job-4-main-cc8d4 -n batch-pods -o yaml ; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-06-25T03:09:04Z""; generateName: batch-2554-job-4-main-; labels:; app: batch-job; hail.is/batch-instance: cd50b95a89914efb897965a5e982a29d; uuid: 3bf0b121f62d4cfea15cf187a21bc0ed; name: batch-2554-job-4-main-cc8d4; namespace: batch-pods; resourceVersion: ""72628848""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-2554-job-4-main-cc8d4; uid: 968b4ba5-96f6-11e9-8aa3-42010a80015f; spec:; containers:; - command:; - /bin/bash; - -c; - set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9;; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi;; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz;; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda;; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0;; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx;; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt;; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d;; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript; /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19}; --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747}; --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8}; --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20}; --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinCaseCtrl=TRUE; 2>&1 | tee ${__RESOURCE_FILE__749}; env:; - name: POD_IP; valueFrom:; fieldRef:; apiVersion: v1; fieldP",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466
https://github.com/hail-is/hail/issues/6466:4586,Deployability,pipeline,pipeline-,4586,"-2554-job-4-main-cc8d4 -n batch-pods -o yaml ; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-06-25T03:09:04Z""; generateName: batch-2554-job-4-main-; labels:; app: batch-job; hail.is/batch-instance: cd50b95a89914efb897965a5e982a29d; uuid: 3bf0b121f62d4cfea15cf187a21bc0ed; name: batch-2554-job-4-main-cc8d4; namespace: batch-pods; resourceVersion: ""72628848""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-2554-job-4-main-cc8d4; uid: 968b4ba5-96f6-11e9-8aa3-42010a80015f; spec:; containers:; - command:; - /bin/bash; - -c; - set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9;; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi;; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz;; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda;; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0;; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx;; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt;; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d;; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript; /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19}; --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747}; --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8}; --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20}; --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinCaseCtrl=TRUE; 2>&1 | tee ${__RESOURCE_FILE__749}; env:; - name: POD_IP; valueFrom:; fieldRef:; apiVersion: v1; fieldP",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466
https://github.com/hail-is/hail/issues/6466:4738,Deployability,pipeline,pipeline,4738,"batch-instance: cd50b95a89914efb897965a5e982a29d; uuid: 3bf0b121f62d4cfea15cf187a21bc0ed; name: batch-2554-job-4-main-cc8d4; namespace: batch-pods; resourceVersion: ""72628848""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-2554-job-4-main-cc8d4; uid: 968b4ba5-96f6-11e9-8aa3-42010a80015f; spec:; containers:; - command:; - /bin/bash; - -c; - set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9;; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi;; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz;; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda;; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0;; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx;; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt;; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d;; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript; /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19}; --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747}; --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8}; --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20}; --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinCaseCtrl=TRUE; 2>&1 | tee ${__RESOURCE_FILE__749}; env:; - name: POD_IP; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: status.podIP; - name: POD_NAME; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: metadata.name; image: konradjk/saige:0.35.8.2.2; imagePullPolicy: IfNotPresent; name: main; resources:; requests:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466
https://github.com/hail-is/hail/issues/6466:4747,Deployability,pipeline,pipeline-,4747,"batch-instance: cd50b95a89914efb897965a5e982a29d; uuid: 3bf0b121f62d4cfea15cf187a21bc0ed; name: batch-2554-job-4-main-cc8d4; namespace: batch-pods; resourceVersion: ""72628848""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-2554-job-4-main-cc8d4; uid: 968b4ba5-96f6-11e9-8aa3-42010a80015f; spec:; containers:; - command:; - /bin/bash; - -c; - set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9;; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi;; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz;; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda;; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0;; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx;; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt;; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d;; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript; /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19}; --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747}; --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8}; --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20}; --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinCaseCtrl=TRUE; 2>&1 | tee ${__RESOURCE_FILE__749}; env:; - name: POD_IP; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: status.podIP; - name: POD_NAME; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: metadata.name; image: konradjk/saige:0.35.8.2.2; imagePullPolicy: IfNotPresent; name: main; resources:; requests:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466
https://github.com/hail-is/hail/issues/6466:4838,Deployability,pipeline,pipeline,4838,"Version: ""72628848""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-2554-job-4-main-cc8d4; uid: 968b4ba5-96f6-11e9-8aa3-42010a80015f; spec:; containers:; - command:; - /bin/bash; - -c; - set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9;; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi;; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz;; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda;; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0;; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx;; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt;; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d;; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript; /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19}; --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747}; --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8}; --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20}; --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinCaseCtrl=TRUE; 2>&1 | tee ${__RESOURCE_FILE__749}; env:; - name: POD_IP; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: status.podIP; - name: POD_NAME; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: metadata.name; image: konradjk/saige:0.35.8.2.2; imagePullPolicy: IfNotPresent; name: main; resources:; requests:; cpu: ""1""; memory: 500M; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /gsa-key; name: gsa-key",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466
https://github.com/hail-is/hail/issues/6466:4847,Deployability,pipeline,pipeline-,4847,"Version: ""72628848""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-2554-job-4-main-cc8d4; uid: 968b4ba5-96f6-11e9-8aa3-42010a80015f; spec:; containers:; - command:; - /bin/bash; - -c; - set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9;; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi;; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz;; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda;; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0;; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx;; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt;; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d;; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript; /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19}; --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747}; --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8}; --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20}; --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinCaseCtrl=TRUE; 2>&1 | tee ${__RESOURCE_FILE__749}; env:; - name: POD_IP; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: status.podIP; - name: POD_NAME; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: metadata.name; image: konradjk/saige:0.35.8.2.2; imagePullPolicy: IfNotPresent; name: main; resources:; requests:; cpu: ""1""; memory: 500M; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /gsa-key; name: gsa-key",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466
https://github.com/hail-is/hail/issues/6466:4914,Deployability,pipeline,pipeline,4914,"Version: ""72628848""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-2554-job-4-main-cc8d4; uid: 968b4ba5-96f6-11e9-8aa3-42010a80015f; spec:; containers:; - command:; - /bin/bash; - -c; - set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9;; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi;; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz;; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda;; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0;; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx;; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt;; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d;; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript; /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19}; --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747}; --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8}; --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20}; --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinCaseCtrl=TRUE; 2>&1 | tee ${__RESOURCE_FILE__749}; env:; - name: POD_IP; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: status.podIP; - name: POD_NAME; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: metadata.name; image: konradjk/saige:0.35.8.2.2; imagePullPolicy: IfNotPresent; name: main; resources:; requests:; cpu: ""1""; memory: 500M; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /gsa-key; name: gsa-key",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466
https://github.com/hail-is/hail/issues/6466:4923,Deployability,pipeline,pipeline-,4923,"Version: ""72628848""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-2554-job-4-main-cc8d4; uid: 968b4ba5-96f6-11e9-8aa3-42010a80015f; spec:; containers:; - command:; - /bin/bash; - -c; - set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9;; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi;; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz;; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda;; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0;; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx;; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt;; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d;; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript; /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19}; --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747}; --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8}; --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20}; --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinCaseCtrl=TRUE; 2>&1 | tee ${__RESOURCE_FILE__749}; env:; - name: POD_IP; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: status.podIP; - name: POD_NAME; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: metadata.name; image: konradjk/saige:0.35.8.2.2; imagePullPolicy: IfNotPresent; name: main; resources:; requests:; cpu: ""1""; memory: 500M; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /gsa-key; name: gsa-key",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466
https://github.com/hail-is/hail/issues/6466:6223,Energy Efficiency,schedul,schedulerName,6223,"__6} --varianceRatioFile=${__RESOURCE_FILE__8}; --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20}; --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinCaseCtrl=TRUE; 2>&1 | tee ${__RESOURCE_FILE__749}; env:; - name: POD_IP; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: status.podIP; - name: POD_NAME; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: metadata.name; image: konradjk/saige:0.35.8.2.2; imagePullPolicy: IfNotPresent; name: main; resources:; requests:; cpu: ""1""; memory: 500M; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /gsa-key; name: gsa-key; - mountPath: /io; name: batch-2554-job-4-8vvgl; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: default-token-8h99c; readOnly: true; dnsPolicy: ClusterFirst; enableServiceLinks: true; nodeName: gke-vdc-preemptible-pool-9c7148b2-4gq2; priority: 500000; priorityClassName: user; restartPolicy: Never; schedulerName: default-scheduler; securityContext: {}; serviceAccount: default; serviceAccountName: default; terminationGracePeriodSeconds: 30; tolerations:; - key: preemptible; value: ""true""; - effect: NoExecute; key: node.kubernetes.io/not-ready; operator: Exists; tolerationSeconds: 300; - effect: NoExecute; key: node.kubernetes.io/unreachable; operator: Exists; tolerationSeconds: 300; volumes:; - name: gsa-key; secret:; defaultMode: 420; secretName: konradk-gsa-key; - name: batch-2554-job-4-8vvgl; persistentVolumeClaim:; claimName: batch-2554-job-4-8vvgl; - name: default-token-8h99c; secret:; defaultMode: 420; secretName: default-token-8h99c; status:; conditions:; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T03:09:04Z""; status: ""True""; type: Initialized; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T03:09:04Z""; message: 'containers with unready status: [main]'; reason: ContainersNotReady; status: ""False""; type: Ready; - lastProbeTime: null; lastTransitionTime: """,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466
https://github.com/hail-is/hail/issues/6466:6246,Energy Efficiency,schedul,scheduler,6246,"__6} --varianceRatioFile=${__RESOURCE_FILE__8}; --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20}; --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinCaseCtrl=TRUE; 2>&1 | tee ${__RESOURCE_FILE__749}; env:; - name: POD_IP; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: status.podIP; - name: POD_NAME; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: metadata.name; image: konradjk/saige:0.35.8.2.2; imagePullPolicy: IfNotPresent; name: main; resources:; requests:; cpu: ""1""; memory: 500M; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /gsa-key; name: gsa-key; - mountPath: /io; name: batch-2554-job-4-8vvgl; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: default-token-8h99c; readOnly: true; dnsPolicy: ClusterFirst; enableServiceLinks: true; nodeName: gke-vdc-preemptible-pool-9c7148b2-4gq2; priority: 500000; priorityClassName: user; restartPolicy: Never; schedulerName: default-scheduler; securityContext: {}; serviceAccount: default; serviceAccountName: default; terminationGracePeriodSeconds: 30; tolerations:; - key: preemptible; value: ""true""; - effect: NoExecute; key: node.kubernetes.io/not-ready; operator: Exists; tolerationSeconds: 300; - effect: NoExecute; key: node.kubernetes.io/unreachable; operator: Exists; tolerationSeconds: 300; volumes:; - name: gsa-key; secret:; defaultMode: 420; secretName: konradk-gsa-key; - name: batch-2554-job-4-8vvgl; persistentVolumeClaim:; claimName: batch-2554-job-4-8vvgl; - name: default-token-8h99c; secret:; defaultMode: 420; secretName: default-token-8h99c; status:; conditions:; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T03:09:04Z""; status: ""True""; type: Initialized; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T03:09:04Z""; message: 'containers with unready status: [main]'; reason: ContainersNotReady; status: ""False""; type: Ready; - lastProbeTime: null; lastTransitionTime: """,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466
https://github.com/hail-is/hail/issues/6466:3051,Integrability,Message,Message,3051,"me); Mounts:; /gsa-key from gsa-key (rw); /io from batch-2554-job-4-8vvgl (rw); /var/run/secrets/kubernetes.io/serviceaccount from default-token-8h99c (ro); Conditions:; Type Status; Initialized True ; Ready False ; ContainersReady False ; PodScheduled True ; Volumes:; gsa-key:; Type: Secret (a volume populated by a Secret); SecretName: konradk-gsa-key; Optional: false; batch-2554-job-4-8vvgl:; Type: PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace); ClaimName: batch-2554-job-4-8vvgl; ReadOnly: false; default-token-8h99c:; Type: Secret (a volume populated by a Secret); SecretName: default-token-8h99c; Optional: false; QoS Class: Burstable; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; preemptible=true; Events:; Type Reason Age From Message; ---- ------ ---- ---- -------; Warning FailedMount 2m59s (x248 over 9h) kubelet, gke-vdc-preemptible-pool-9c7148b2-4gq2 Unable to mount volumes for pod ""batch-2554-job-4-main-cc8d4_batch-pods(968b4ba5-96f6-11e9-8aa3-42010a80015f)"": timeout expired waiting for volumes to attach or mount for pod ""batch-pods""/""batch-2554-job-4-main-cc8d4"". list of unmounted volumes=[batch-2554-job-4-8vvgl]. list of unattached volumes=[gsa-key batch-2554-job-4-8vvgl default-token-8h99c]; # k get pod batch-2554-job-4-main-cc8d4 -n batch-pods -o yaml ; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-06-25T03:09:04Z""; generateName: batch-2554-job-4-main-; labels:; app: batch-job; hail.is/batch-instance: cd50b95a89914efb897965a5e982a29d; uuid: 3bf0b121f62d4cfea15cf187a21bc0ed; name: batch-2554-job-4-main-cc8d4; namespace: batch-pods; resourceVersion: ""72628848""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-2554-job-4-main-cc8d4; uid: 968b4ba5-96f6-11e9-8aa3-42010a80015f; spec:; containers:; - command:; - /bin/bash; - -c; - set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/i",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466
https://github.com/hail-is/hail/issues/6466:7068,Integrability,message,message,7068,"8h99c; readOnly: true; dnsPolicy: ClusterFirst; enableServiceLinks: true; nodeName: gke-vdc-preemptible-pool-9c7148b2-4gq2; priority: 500000; priorityClassName: user; restartPolicy: Never; schedulerName: default-scheduler; securityContext: {}; serviceAccount: default; serviceAccountName: default; terminationGracePeriodSeconds: 30; tolerations:; - key: preemptible; value: ""true""; - effect: NoExecute; key: node.kubernetes.io/not-ready; operator: Exists; tolerationSeconds: 300; - effect: NoExecute; key: node.kubernetes.io/unreachable; operator: Exists; tolerationSeconds: 300; volumes:; - name: gsa-key; secret:; defaultMode: 420; secretName: konradk-gsa-key; - name: batch-2554-job-4-8vvgl; persistentVolumeClaim:; claimName: batch-2554-job-4-8vvgl; - name: default-token-8h99c; secret:; defaultMode: 420; secretName: default-token-8h99c; status:; conditions:; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T03:09:04Z""; status: ""True""; type: Initialized; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T03:09:04Z""; message: 'containers with unready status: [main]'; reason: ContainersNotReady; status: ""False""; type: Ready; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T03:09:04Z""; message: 'containers with unready status: [main]'; reason: ContainersNotReady; status: ""False""; type: ContainersReady; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T03:09:04Z""; status: ""True""; type: PodScheduled; containerStatuses:; - image: konradjk/saige:0.35.8.2.2; imageID: """"; lastState: {}; name: main; ready: false; restartCount: 0; state:; waiting:; reason: ContainerCreating; hostIP: 10.128.0.8; phase: Pending; qosClass: Burstable; startTime: ""2019-06-25T03:09:04Z""; ```; PVC in question; ```; # k describe pvc batch-2554-job-4-8vvgl -n batch-pods; Name: batch-2554-job-4-8vvgl; Namespace: batch-pods; StorageClass: batch; Status: Bound; Volume: pvc-32804669-96f6-11e9-8aa3-42010a80015f; Labels: app=batch-job; hail.is/batch-instance=cd50b95a89914efb897965a5e982a29d; Ann",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466
https://github.com/hail-is/hail/issues/6466:7244,Integrability,message,message,7244,"8h99c; readOnly: true; dnsPolicy: ClusterFirst; enableServiceLinks: true; nodeName: gke-vdc-preemptible-pool-9c7148b2-4gq2; priority: 500000; priorityClassName: user; restartPolicy: Never; schedulerName: default-scheduler; securityContext: {}; serviceAccount: default; serviceAccountName: default; terminationGracePeriodSeconds: 30; tolerations:; - key: preemptible; value: ""true""; - effect: NoExecute; key: node.kubernetes.io/not-ready; operator: Exists; tolerationSeconds: 300; - effect: NoExecute; key: node.kubernetes.io/unreachable; operator: Exists; tolerationSeconds: 300; volumes:; - name: gsa-key; secret:; defaultMode: 420; secretName: konradk-gsa-key; - name: batch-2554-job-4-8vvgl; persistentVolumeClaim:; claimName: batch-2554-job-4-8vvgl; - name: default-token-8h99c; secret:; defaultMode: 420; secretName: default-token-8h99c; status:; conditions:; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T03:09:04Z""; status: ""True""; type: Initialized; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T03:09:04Z""; message: 'containers with unready status: [main]'; reason: ContainersNotReady; status: ""False""; type: Ready; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T03:09:04Z""; message: 'containers with unready status: [main]'; reason: ContainersNotReady; status: ""False""; type: ContainersReady; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T03:09:04Z""; status: ""True""; type: PodScheduled; containerStatuses:; - image: konradjk/saige:0.35.8.2.2; imageID: """"; lastState: {}; name: main; ready: false; restartCount: 0; state:; waiting:; reason: ContainerCreating; hostIP: 10.128.0.8; phase: Pending; qosClass: Burstable; startTime: ""2019-06-25T03:09:04Z""; ```; PVC in question; ```; # k describe pvc batch-2554-job-4-8vvgl -n batch-pods; Name: batch-2554-job-4-8vvgl; Namespace: batch-pods; StorageClass: batch; Status: Bound; Volume: pvc-32804669-96f6-11e9-8aa3-42010a80015f; Labels: app=batch-job; hail.is/batch-instance=cd50b95a89914efb897965a5e982a29d; Ann",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466
https://github.com/hail-is/hail/issues/6466:3292,Safety,timeout,timeout,3292,"me); Mounts:; /gsa-key from gsa-key (rw); /io from batch-2554-job-4-8vvgl (rw); /var/run/secrets/kubernetes.io/serviceaccount from default-token-8h99c (ro); Conditions:; Type Status; Initialized True ; Ready False ; ContainersReady False ; PodScheduled True ; Volumes:; gsa-key:; Type: Secret (a volume populated by a Secret); SecretName: konradk-gsa-key; Optional: false; batch-2554-job-4-8vvgl:; Type: PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace); ClaimName: batch-2554-job-4-8vvgl; ReadOnly: false; default-token-8h99c:; Type: Secret (a volume populated by a Secret); SecretName: default-token-8h99c; Optional: false; QoS Class: Burstable; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; preemptible=true; Events:; Type Reason Age From Message; ---- ------ ---- ---- -------; Warning FailedMount 2m59s (x248 over 9h) kubelet, gke-vdc-preemptible-pool-9c7148b2-4gq2 Unable to mount volumes for pod ""batch-2554-job-4-main-cc8d4_batch-pods(968b4ba5-96f6-11e9-8aa3-42010a80015f)"": timeout expired waiting for volumes to attach or mount for pod ""batch-pods""/""batch-2554-job-4-main-cc8d4"". list of unmounted volumes=[batch-2554-job-4-8vvgl]. list of unattached volumes=[gsa-key batch-2554-job-4-8vvgl default-token-8h99c]; # k get pod batch-2554-job-4-main-cc8d4 -n batch-pods -o yaml ; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-06-25T03:09:04Z""; generateName: batch-2554-job-4-main-; labels:; app: batch-job; hail.is/batch-instance: cd50b95a89914efb897965a5e982a29d; uuid: 3bf0b121f62d4cfea15cf187a21bc0ed; name: batch-2554-job-4-main-cc8d4; namespace: batch-pods; resourceVersion: ""72628848""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-2554-job-4-main-cc8d4; uid: 968b4ba5-96f6-11e9-8aa3-42010a80015f; spec:; containers:; - command:; - /bin/bash; - -c; - set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/i",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466
https://github.com/hail-is/hail/issues/6466:8593,Safety,timeout,timeout,8593,"e: 420; secretName: default-token-8h99c; status:; conditions:; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T03:09:04Z""; status: ""True""; type: Initialized; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T03:09:04Z""; message: 'containers with unready status: [main]'; reason: ContainersNotReady; status: ""False""; type: Ready; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T03:09:04Z""; message: 'containers with unready status: [main]'; reason: ContainersNotReady; status: ""False""; type: ContainersReady; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T03:09:04Z""; status: ""True""; type: PodScheduled; containerStatuses:; - image: konradjk/saige:0.35.8.2.2; imageID: """"; lastState: {}; name: main; ready: false; restartCount: 0; state:; waiting:; reason: ContainerCreating; hostIP: 10.128.0.8; phase: Pending; qosClass: Burstable; startTime: ""2019-06-25T03:09:04Z""; ```; PVC in question; ```; # k describe pvc batch-2554-job-4-8vvgl -n batch-pods; Name: batch-2554-job-4-8vvgl; Namespace: batch-pods; StorageClass: batch; Status: Bound; Volume: pvc-32804669-96f6-11e9-8aa3-42010a80015f; Labels: app=batch-job; hail.is/batch-instance=cd50b95a89914efb897965a5e982a29d; Annotations: pv.kubernetes.io/bind-completed: yes; pv.kubernetes.io/bound-by-controller: yes; volume.beta.kubernetes.io/storage-provisioner: kubernetes.io/gce-pd; Finalizers: [kubernetes.io/pvc-protection]; Capacity: 1Gi; Access Modes: RWO; VolumeMode: Filesystem; Events: <none>; Mounted By: batch-2554-job-4-main-cc8d4; ```; Events; ```; # k get events -n batch-pods --sort-by='.metadata.creationTimestamp' | grep 2554; 76s Warning FailedMount Pod Unable to mount volumes for pod ""batch-2554-job-4-main-cc8d4_batch-pods(968b4ba5-96f6-11e9-8aa3-42010a80015f)"": timeout expired waiting for volumes to attach or mount for pod ""batch-pods""/""batch-2554-job-4-main-cc8d4"". list of unmounted volumes=[batch-2554-job-4-8vvgl]. list of unattached volumes=[gsa-key batch-2554-job-4-8vvgl default-token-8h99c]; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466
https://github.com/hail-is/hail/issues/6466:6257,Security,secur,securityContext,6257,"__6} --varianceRatioFile=${__RESOURCE_FILE__8}; --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20}; --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinCaseCtrl=TRUE; 2>&1 | tee ${__RESOURCE_FILE__749}; env:; - name: POD_IP; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: status.podIP; - name: POD_NAME; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: metadata.name; image: konradjk/saige:0.35.8.2.2; imagePullPolicy: IfNotPresent; name: main; resources:; requests:; cpu: ""1""; memory: 500M; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /gsa-key; name: gsa-key; - mountPath: /io; name: batch-2554-job-4-8vvgl; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: default-token-8h99c; readOnly: true; dnsPolicy: ClusterFirst; enableServiceLinks: true; nodeName: gke-vdc-preemptible-pool-9c7148b2-4gq2; priority: 500000; priorityClassName: user; restartPolicy: Never; schedulerName: default-scheduler; securityContext: {}; serviceAccount: default; serviceAccountName: default; terminationGracePeriodSeconds: 30; tolerations:; - key: preemptible; value: ""true""; - effect: NoExecute; key: node.kubernetes.io/not-ready; operator: Exists; tolerationSeconds: 300; - effect: NoExecute; key: node.kubernetes.io/unreachable; operator: Exists; tolerationSeconds: 300; volumes:; - name: gsa-key; secret:; defaultMode: 420; secretName: konradk-gsa-key; - name: batch-2554-job-4-8vvgl; persistentVolumeClaim:; claimName: batch-2554-job-4-8vvgl; - name: default-token-8h99c; secret:; defaultMode: 420; secretName: default-token-8h99c; status:; conditions:; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T03:09:04Z""; status: ""True""; type: Initialized; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T03:09:04Z""; message: 'containers with unready status: [main]'; reason: ContainersNotReady; status: ""False""; type: Ready; - lastProbeTime: null; lastTransitionTime: """,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466
https://github.com/hail-is/hail/issues/6466:8253,Security,Access,Access,8253,"e: 420; secretName: default-token-8h99c; status:; conditions:; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T03:09:04Z""; status: ""True""; type: Initialized; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T03:09:04Z""; message: 'containers with unready status: [main]'; reason: ContainersNotReady; status: ""False""; type: Ready; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T03:09:04Z""; message: 'containers with unready status: [main]'; reason: ContainersNotReady; status: ""False""; type: ContainersReady; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T03:09:04Z""; status: ""True""; type: PodScheduled; containerStatuses:; - image: konradjk/saige:0.35.8.2.2; imageID: """"; lastState: {}; name: main; ready: false; restartCount: 0; state:; waiting:; reason: ContainerCreating; hostIP: 10.128.0.8; phase: Pending; qosClass: Burstable; startTime: ""2019-06-25T03:09:04Z""; ```; PVC in question; ```; # k describe pvc batch-2554-job-4-8vvgl -n batch-pods; Name: batch-2554-job-4-8vvgl; Namespace: batch-pods; StorageClass: batch; Status: Bound; Volume: pvc-32804669-96f6-11e9-8aa3-42010a80015f; Labels: app=batch-job; hail.is/batch-instance=cd50b95a89914efb897965a5e982a29d; Annotations: pv.kubernetes.io/bind-completed: yes; pv.kubernetes.io/bound-by-controller: yes; volume.beta.kubernetes.io/storage-provisioner: kubernetes.io/gce-pd; Finalizers: [kubernetes.io/pvc-protection]; Capacity: 1Gi; Access Modes: RWO; VolumeMode: Filesystem; Events: <none>; Mounted By: batch-2554-job-4-main-cc8d4; ```; Events; ```; # k get events -n batch-pods --sort-by='.metadata.creationTimestamp' | grep 2554; 76s Warning FailedMount Pod Unable to mount volumes for pod ""batch-2554-job-4-main-cc8d4_batch-pods(968b4ba5-96f6-11e9-8aa3-42010a80015f)"": timeout expired waiting for volumes to attach or mount for pod ""batch-pods""/""batch-2554-job-4-main-cc8d4"". list of unmounted volumes=[batch-2554-job-4-8vvgl]. list of unattached volumes=[gsa-key batch-2554-job-4-8vvgl default-token-8h99c]; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466
https://github.com/hail-is/hail/issues/6466:5816,Testability,log,log,5816,"pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d;; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript; /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19}; --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747}; --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8}; --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20}; --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinCaseCtrl=TRUE; 2>&1 | tee ${__RESOURCE_FILE__749}; env:; - name: POD_IP; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: status.podIP; - name: POD_NAME; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: metadata.name; image: konradjk/saige:0.35.8.2.2; imagePullPolicy: IfNotPresent; name: main; resources:; requests:; cpu: ""1""; memory: 500M; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /gsa-key; name: gsa-key; - mountPath: /io; name: batch-2554-job-4-8vvgl; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: default-token-8h99c; readOnly: true; dnsPolicy: ClusterFirst; enableServiceLinks: true; nodeName: gke-vdc-preemptible-pool-9c7148b2-4gq2; priority: 500000; priorityClassName: user; restartPolicy: Never; schedulerName: default-scheduler; securityContext: {}; serviceAccount: default; serviceAccountName: default; terminationGracePeriodSeconds: 30; tolerations:; - key: preemptible; value: ""true""; - effect: NoExecute; key: node.kubernetes.io/not-ready; operator: Exists; tolerationSeconds: 300; - effect: NoExecute; key: node.kubernetes.io/unreachable; operator: Exists; tolerationSeconds: 300; volumes:; - name: gsa-key; secret:; defaultMode: 420; secretName: konradk-gsa-key; - name: batch-2554-job-4-8vvgl; persistentVolumeClaim:; claimName: batch-2554-job-4-8vvgl; - name: default-token-8h99c; secret:; defaultMode: ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466
https://github.com/hail-is/hail/pull/6473:40,Testability,Test,Test,40,"Replaced by generic AggArrayPerElement. Test; ----. ```python; @benchmark; def table_aggregate_array_sum():; N = 10_000_000; M = 100; ht = hl.utils.range_table(N); ht.aggregate(hl.agg.array_sum(hl.range(0, M))); ```. Timings; -------. Master:. ```; Mean, Median: 7.71s, 7.74s; ```. Branch:. ```; Mean, Median: 4.99s, 4.99s; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6473
https://github.com/hail-is/hail/pull/6473:64,Testability,benchmark,benchmark,64,"Replaced by generic AggArrayPerElement. Test; ----. ```python; @benchmark; def table_aggregate_array_sum():; N = 10_000_000; M = 100; ht = hl.utils.range_table(N); ht.aggregate(hl.agg.array_sum(hl.range(0, M))); ```. Timings; -------. Master:. ```; Mean, Median: 7.71s, 7.74s; ```. Branch:. ```; Mean, Median: 4.99s, 4.99s; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6473
https://github.com/hail-is/hail/issues/6475:52,Integrability,message,message,52,"aiohttp.client_exceptions.ClientResponseError: 404, message='Not Found'",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6475
https://github.com/hail-is/hail/pull/6481:3,Safety,avoid,avoid,3,To avoid this:. ```sh; + sleep 360; + true; + curl -sSL http://notebook/worker-image; curl: (7) Failed to connect to notebook port 80: Connection refused; ```. Since notebook is not currently running,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6481
https://github.com/hail-is/hail/pull/6489:161,Testability,log,logs,161,@danking Feel free to reassign if you have too much to do. Requires a database change. I realize there's a lot of code duplication here with how we get/show the logs. It might be worth generalizing this in the future if there's a third thing that follows this pattern.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6489
https://github.com/hail-is/hail/issues/6490:6,Energy Efficiency,reduce,reduce,6,"- [x] reduce number of database calls when creating a pod; - [x] put the source jobs of a batch on a queue which has 16-concurrent workers sending create_pod requests to k8s; - [x] modify the server API to /create, /create_jobs *, /close (prevents holding 12M jobs in memory on batch server); - [x] record pod status JSON in database for debugging purposes",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6490
https://github.com/hail-is/hail/issues/6490:101,Performance,queue,queue,101,"- [x] reduce number of database calls when creating a pod; - [x] put the source jobs of a batch on a queue which has 16-concurrent workers sending create_pod requests to k8s; - [x] modify the server API to /create, /create_jobs *, /close (prevents holding 12M jobs in memory on batch server); - [x] record pod status JSON in database for debugging purposes",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6490
https://github.com/hail-is/hail/issues/6490:120,Performance,concurren,concurrent,120,"- [x] reduce number of database calls when creating a pod; - [x] put the source jobs of a batch on a queue which has 16-concurrent workers sending create_pod requests to k8s; - [x] modify the server API to /create, /create_jobs *, /close (prevents holding 12M jobs in memory on batch server); - [x] record pod status JSON in database for debugging purposes",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6490
https://github.com/hail-is/hail/issues/6491:128,Performance,latency,latency,128,"- [x] set resource requests on input and output pods (https://github.com/hail-is/hail/pull/6507); - [x] request metrics (req/s, latency) for Hail services (at least batch) in Prometheus/Grafana; - [x] for both REST and web endpoints; - [ ] metrics for DB requests in batch",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6491
https://github.com/hail-is/hail/issues/6492:348,Deployability,pipeline,pipeline,348,"- [ ] test order of magnitudes: 10k, 1M, 10M, 15M with `true` jobs, both flat and fan-out-fan-in dags.; - [ ] re-run Konrad's job with job count 4x max autoscaling core-count; - [ ] a dry run of Konrad's 12M job batch (perhaps with shorter individual tasks) with 1,600 cores (200 8-core nodes); - [ ] UI display 12M pods; - [ ] observe behavior of pipeline client code when creating a 12M task pipeline. core-hour = 0.01 USD/hour; core-minute ~= 0.00017 USD / hour. 12M seconds ~= 3333 hours; 12M core-seconds = 33.33 USD. Equivalent cost (33 USD) batches:; 12M 1 second jobs; 120k 100 seconds jobs; 12k 1000 second jobs. Max pods: 150,000; Max nodes: 5000; Max cores (8-core): 40,000; Max cores (64-core): 320,000",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6492
https://github.com/hail-is/hail/issues/6492:394,Deployability,pipeline,pipeline,394,"- [ ] test order of magnitudes: 10k, 1M, 10M, 15M with `true` jobs, both flat and fan-out-fan-in dags.; - [ ] re-run Konrad's job with job count 4x max autoscaling core-count; - [ ] a dry run of Konrad's 12M job batch (perhaps with shorter individual tasks) with 1,600 cores (200 8-core nodes); - [ ] UI display 12M pods; - [ ] observe behavior of pipeline client code when creating a 12M task pipeline. core-hour = 0.01 USD/hour; core-minute ~= 0.00017 USD / hour. 12M seconds ~= 3333 hours; 12M core-seconds = 33.33 USD. Equivalent cost (33 USD) batches:; 12M 1 second jobs; 120k 100 seconds jobs; 12k 1000 second jobs. Max pods: 150,000; Max nodes: 5000; Max cores (8-core): 40,000; Max cores (64-core): 320,000",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6492
https://github.com/hail-is/hail/issues/6492:6,Testability,test,test,6,"- [ ] test order of magnitudes: 10k, 1M, 10M, 15M with `true` jobs, both flat and fan-out-fan-in dags.; - [ ] re-run Konrad's job with job count 4x max autoscaling core-count; - [ ] a dry run of Konrad's 12M job batch (perhaps with shorter individual tasks) with 1,600 cores (200 8-core nodes); - [ ] UI display 12M pods; - [ ] observe behavior of pipeline client code when creating a 12M task pipeline. core-hour = 0.01 USD/hour; core-minute ~= 0.00017 USD / hour. 12M seconds ~= 3333 hours; 12M core-seconds = 33.33 USD. Equivalent cost (33 USD) batches:; 12M 1 second jobs; 120k 100 seconds jobs; 12k 1000 second jobs. Max pods: 150,000; Max nodes: 5000; Max cores (8-core): 40,000; Max cores (64-core): 320,000",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6492
https://github.com/hail-is/hail/issues/6493:159,Energy Efficiency,schedul,scheduling,159,- [ ] attributes PR (per-batch attributes); - [ ] per-job attributes; - [ ] use queue and concurrent worker pool for all k8s communication; - [ ] batch avoids scheduling more than 150k pods,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6493
https://github.com/hail-is/hail/issues/6493:80,Performance,queue,queue,80,- [ ] attributes PR (per-batch attributes); - [ ] per-job attributes; - [ ] use queue and concurrent worker pool for all k8s communication; - [ ] batch avoids scheduling more than 150k pods,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6493
https://github.com/hail-is/hail/issues/6493:90,Performance,concurren,concurrent,90,- [ ] attributes PR (per-batch attributes); - [ ] per-job attributes; - [ ] use queue and concurrent worker pool for all k8s communication; - [ ] batch avoids scheduling more than 150k pods,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6493
https://github.com/hail-is/hail/issues/6493:152,Safety,avoid,avoids,152,- [ ] attributes PR (per-batch attributes); - [ ] per-job attributes; - [ ] use queue and concurrent worker pool for all k8s communication; - [ ] batch avoids scheduling more than 150k pods,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6493
https://github.com/hail-is/hail/issues/6494:287,Availability,failure,failure,287,"- [ ] input pods: exit if the PVC is not empty; - [ ] main pods: use initContainers to delete all content other than /io/inputs; - [ ] exit pods: use pod_name in path and add a _SUCCESS file on completion; - [ ] input pods: use any input directory containing a _SUCCESS file; - [x] if a failure occurs, check the database before mark_unscheduled. e.g. if `read_log` fails because the pod doesn't exist, check if the log is already uploaded. if it is, do nothing.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6494
https://github.com/hail-is/hail/issues/6494:416,Testability,log,log,416,"- [ ] input pods: exit if the PVC is not empty; - [ ] main pods: use initContainers to delete all content other than /io/inputs; - [ ] exit pods: use pod_name in path and add a _SUCCESS file on completion; - [ ] input pods: use any input directory containing a _SUCCESS file; - [x] if a failure occurs, check the database before mark_unscheduled. e.g. if `read_log` fails because the pod doesn't exist, check if the log is already uploaded. if it is, do nothing.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6494
https://github.com/hail-is/hail/issues/6495:459,Deployability,configurat,configuration,459,"From Cotton:. I think the request/s and request latency metrics in Grafana are not actually the metrics for the Kubernetes service as we'd hoped. In particular, the reqs/s makes no sense. This post:. https://blog.freshtracks.io/a-deep-dive-into-kubernetes-metrics-part-4-the-kubernetes-api-server-72f1e1210770. indicates we should have an apiserver_request_count and apiserver_request_latencies_bucket which sound like what we want. They give ""the Prometheus configuration for getting metrics from the Kubernetes API server, even in environments where the masters are hosted for you"" (and I think our master is hosted for us in GKE). In particular, we have no analogous apiserver scrape config with ""role: endpoints"" in our setup. Here is the apiserver code with all the metrics they collect: https://github.com/kubernetes/apiserver/blob/master/pkg/endpoints/metrics/metrics.go",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6495
https://github.com/hail-is/hail/issues/6495:459,Modifiability,config,configuration,459,"From Cotton:. I think the request/s and request latency metrics in Grafana are not actually the metrics for the Kubernetes service as we'd hoped. In particular, the reqs/s makes no sense. This post:. https://blog.freshtracks.io/a-deep-dive-into-kubernetes-metrics-part-4-the-kubernetes-api-server-72f1e1210770. indicates we should have an apiserver_request_count and apiserver_request_latencies_bucket which sound like what we want. They give ""the Prometheus configuration for getting metrics from the Kubernetes API server, even in environments where the masters are hosted for you"" (and I think our master is hosted for us in GKE). In particular, we have no analogous apiserver scrape config with ""role: endpoints"" in our setup. Here is the apiserver code with all the metrics they collect: https://github.com/kubernetes/apiserver/blob/master/pkg/endpoints/metrics/metrics.go",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6495
https://github.com/hail-is/hail/issues/6495:687,Modifiability,config,config,687,"From Cotton:. I think the request/s and request latency metrics in Grafana are not actually the metrics for the Kubernetes service as we'd hoped. In particular, the reqs/s makes no sense. This post:. https://blog.freshtracks.io/a-deep-dive-into-kubernetes-metrics-part-4-the-kubernetes-api-server-72f1e1210770. indicates we should have an apiserver_request_count and apiserver_request_latencies_bucket which sound like what we want. They give ""the Prometheus configuration for getting metrics from the Kubernetes API server, even in environments where the masters are hosted for you"" (and I think our master is hosted for us in GKE). In particular, we have no analogous apiserver scrape config with ""role: endpoints"" in our setup. Here is the apiserver code with all the metrics they collect: https://github.com/kubernetes/apiserver/blob/master/pkg/endpoints/metrics/metrics.go",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6495
https://github.com/hail-is/hail/issues/6495:48,Performance,latency,latency,48,"From Cotton:. I think the request/s and request latency metrics in Grafana are not actually the metrics for the Kubernetes service as we'd hoped. In particular, the reqs/s makes no sense. This post:. https://blog.freshtracks.io/a-deep-dive-into-kubernetes-metrics-part-4-the-kubernetes-api-server-72f1e1210770. indicates we should have an apiserver_request_count and apiserver_request_latencies_bucket which sound like what we want. They give ""the Prometheus configuration for getting metrics from the Kubernetes API server, even in environments where the masters are hosted for you"" (and I think our master is hosted for us in GKE). In particular, we have no analogous apiserver scrape config with ""role: endpoints"" in our setup. Here is the apiserver code with all the metrics they collect: https://github.com/kubernetes/apiserver/blob/master/pkg/endpoints/metrics/metrics.go",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6495
https://github.com/hail-is/hail/pull/6499:67,Security,access,access,67,"Starting to break out changes from #6480. This adds the ability to access sub-regions of a region by index. @patrick-schultz I know I misnamed these---they should be child references, not parent references---I can change if you prefer, but might like to hold off until afterwards since the names of the functions are threaded through the rest of the stack :(. cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6499
https://github.com/hail-is/hail/pull/6500:25,Testability,test,tests,25,"(Changes from #6480). No tests for this right now, although I can write some---they get exercised through the staged region value aggregator stuff.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6500
https://github.com/hail-is/hail/pull/6501:89,Integrability,interface,interfaces,89,"(pulling out bits from #6480). This defines the StagedRegionValueAggregator and RVAState interfaces, implementing ArrayElementsAggregator and PrevNonNullAggregator as examples. Builds on #6500 and #6499. (for clarity, I've put all the changes from those PRs into the first commit.)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6501
https://github.com/hail-is/hail/pull/6503:73,Usability,feedback,feedback,73,First version of hailctl batch. I'm not a big batch user at this time so feedback on both code and desired functionality is welcome. @konradjk,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6503
https://github.com/hail-is/hail/pull/6505:33,Modifiability,Parameteriz,Parameterize,33,Dice came up @patrick-schultz. - Parameterize (de/en)coders by InputStream type which is either a Java input stream or an array of bytes.; - add `compileComparison` which produces the bytes of a system-specific executable for comparing two values of types `l` and `r` encoded in `codec`; - experimental python API to `compileComparison` to enable experimentation. The end goal is to run `compileComparison` and ship the bytes to the shuffle service which will use it as a comparison operation on the encoded keys.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6505
https://github.com/hail-is/hail/pull/6506:0,Testability,Benchmark,Benchmark,0,"Benchmark:; ```python; @benchmark; def per_row_stats_star_star():; mt = hl.read_matrix_table(resource('gnomad_dp_simulation.mt')); mt.annotate_rows(**hl.agg.stats(mt.x))._force_count_rows(); ```. This branch:; ```; running per_row_stats_star_star...; run 1 took 14.53s; run 2 took 16.56s; run 3 took 15.05s; Mean, Median: 15.38s, 15.05s; ```. Master:; ```; running per_row_stats_star_star...; run 1 took 31.47s; run 2 took 37.34s; run 3 took 26.67s; Mean, Median: 31.83s, 31.47s; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6506
https://github.com/hail-is/hail/pull/6506:24,Testability,benchmark,benchmark,24,"Benchmark:; ```python; @benchmark; def per_row_stats_star_star():; mt = hl.read_matrix_table(resource('gnomad_dp_simulation.mt')); mt.annotate_rows(**hl.agg.stats(mt.x))._force_count_rows(); ```. This branch:; ```; running per_row_stats_star_star...; run 1 took 14.53s; run 2 took 16.56s; run 3 took 15.05s; Mean, Median: 15.38s, 15.05s; ```. Master:; ```; running per_row_stats_star_star...; run 1 took 31.47s; run 2 took 37.34s; run 3 took 26.67s; Mean, Median: 31.83s, 31.47s; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6506
https://github.com/hail-is/hail/pull/6508:133,Integrability,message,message,133,The previous checks for `pod is None` were intended to catch this case. We're not sure why this happens. By including the pod in the message we hope to understand the cause of nameless pods.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6508
https://github.com/hail-is/hail/issues/6509:103,Performance,load,load,103,"I want a cluster pod resource waste metric. Consider one pod. if its request is 100mCPU and its actual load is 10mCPU we're ""wasting"" 90mCPU. I want to know the distribution of wasted pod CPU. I want to know the top 10 wasteful pods.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6509
https://github.com/hail-is/hail/pull/6511:24,Availability,error,errors,24,Needed to guard against errors in the k8s pod stream.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6511
https://github.com/hail-is/hail/pull/6513:82,Deployability,install,installed,82,in hailctl dataproc start; allows other hailctl commands to be used without being installed. This way you can set PYHTONPATH=$HAIL_HOME/hail/python and use the development version instead of the installed version (except for `hailctl dataproc start`),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6513
https://github.com/hail-is/hail/pull/6513:195,Deployability,install,installed,195,in hailctl dataproc start; allows other hailctl commands to be used without being installed. This way you can set PYHTONPATH=$HAIL_HOME/hail/python and use the development version instead of the installed version (except for `hailctl dataproc start`),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6513
https://github.com/hail-is/hail/pull/6516:210,Deployability,update,updates,210,"Start of an IR node for joining two streams. ## Logic; Works off the ""producer"" pattern, where a stream is an object that you can; 1. Initialize, so the head of the stream is ready to be consumed.; 2. Step, so updates the current head to the next element in the stream. A join then takes two streams and its Step steps the left stream, the right stream, or both streams until a ""valid"" state is reached. E.g. In an outer join every step is a new valid state while an inner join must loop until the match condition is met. The node must also take some combining binary functions so as not to produce runtime tuples. Optional ""left"" and ""right"" functions determine at compilation time whether an unmatched pair of values should still produce a value (e.g. in a left join you always act, just either on just the left head or both). ## State of the code base; - Added `produce` abstract method to `ArrayEmitter`. Tests will fail until that is implemented on all the array nodes.; - Inserted a small hack to the `ir.ToArray` emit case that uses the producer route; - Added a test case for `ArrayJoin` (this one passes! :)); - Have not yet incorporated missingness into producer logic and have not implemented `consume` for ArrayJoin`.; - `Binds` needs to be updated to add the appropriate bindings for the children. Right now it just adds them all.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6516
https://github.com/hail-is/hail/pull/6516:1253,Deployability,update,updated,1253,"Start of an IR node for joining two streams. ## Logic; Works off the ""producer"" pattern, where a stream is an object that you can; 1. Initialize, so the head of the stream is ready to be consumed.; 2. Step, so updates the current head to the next element in the stream. A join then takes two streams and its Step steps the left stream, the right stream, or both streams until a ""valid"" state is reached. E.g. In an outer join every step is a new valid state while an inner join must loop until the match condition is met. The node must also take some combining binary functions so as not to produce runtime tuples. Optional ""left"" and ""right"" functions determine at compilation time whether an unmatched pair of values should still produce a value (e.g. in a left join you always act, just either on just the left head or both). ## State of the code base; - Added `produce` abstract method to `ArrayEmitter`. Tests will fail until that is implemented on all the array nodes.; - Inserted a small hack to the `ir.ToArray` emit case that uses the producer route; - Added a test case for `ArrayJoin` (this one passes! :)); - Have not yet incorporated missingness into producer logic and have not implemented `consume` for ArrayJoin`.; - `Binds` needs to be updated to add the appropriate bindings for the children. Right now it just adds them all.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6516
https://github.com/hail-is/hail/pull/6516:1053,Integrability,rout,route,1053,"Start of an IR node for joining two streams. ## Logic; Works off the ""producer"" pattern, where a stream is an object that you can; 1. Initialize, so the head of the stream is ready to be consumed.; 2. Step, so updates the current head to the next element in the stream. A join then takes two streams and its Step steps the left stream, the right stream, or both streams until a ""valid"" state is reached. E.g. In an outer join every step is a new valid state while an inner join must loop until the match condition is met. The node must also take some combining binary functions so as not to produce runtime tuples. Optional ""left"" and ""right"" functions determine at compilation time whether an unmatched pair of values should still produce a value (e.g. in a left join you always act, just either on just the left head or both). ## State of the code base; - Added `produce` abstract method to `ArrayEmitter`. Tests will fail until that is implemented on all the array nodes.; - Inserted a small hack to the `ir.ToArray` emit case that uses the producer route; - Added a test case for `ArrayJoin` (this one passes! :)); - Have not yet incorporated missingness into producer logic and have not implemented `consume` for ArrayJoin`.; - `Binds` needs to be updated to add the appropriate bindings for the children. Right now it just adds them all.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6516
https://github.com/hail-is/hail/pull/6516:48,Testability,Log,Logic,48,"Start of an IR node for joining two streams. ## Logic; Works off the ""producer"" pattern, where a stream is an object that you can; 1. Initialize, so the head of the stream is ready to be consumed.; 2. Step, so updates the current head to the next element in the stream. A join then takes two streams and its Step steps the left stream, the right stream, or both streams until a ""valid"" state is reached. E.g. In an outer join every step is a new valid state while an inner join must loop until the match condition is met. The node must also take some combining binary functions so as not to produce runtime tuples. Optional ""left"" and ""right"" functions determine at compilation time whether an unmatched pair of values should still produce a value (e.g. in a left join you always act, just either on just the left head or both). ## State of the code base; - Added `produce` abstract method to `ArrayEmitter`. Tests will fail until that is implemented on all the array nodes.; - Inserted a small hack to the `ir.ToArray` emit case that uses the producer route; - Added a test case for `ArrayJoin` (this one passes! :)); - Have not yet incorporated missingness into producer logic and have not implemented `consume` for ArrayJoin`.; - `Binds` needs to be updated to add the appropriate bindings for the children. Right now it just adds them all.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6516
https://github.com/hail-is/hail/pull/6516:909,Testability,Test,Tests,909,"Start of an IR node for joining two streams. ## Logic; Works off the ""producer"" pattern, where a stream is an object that you can; 1. Initialize, so the head of the stream is ready to be consumed.; 2. Step, so updates the current head to the next element in the stream. A join then takes two streams and its Step steps the left stream, the right stream, or both streams until a ""valid"" state is reached. E.g. In an outer join every step is a new valid state while an inner join must loop until the match condition is met. The node must also take some combining binary functions so as not to produce runtime tuples. Optional ""left"" and ""right"" functions determine at compilation time whether an unmatched pair of values should still produce a value (e.g. in a left join you always act, just either on just the left head or both). ## State of the code base; - Added `produce` abstract method to `ArrayEmitter`. Tests will fail until that is implemented on all the array nodes.; - Inserted a small hack to the `ir.ToArray` emit case that uses the producer route; - Added a test case for `ArrayJoin` (this one passes! :)); - Have not yet incorporated missingness into producer logic and have not implemented `consume` for ArrayJoin`.; - `Binds` needs to be updated to add the appropriate bindings for the children. Right now it just adds them all.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6516
https://github.com/hail-is/hail/pull/6516:1070,Testability,test,test,1070,"Start of an IR node for joining two streams. ## Logic; Works off the ""producer"" pattern, where a stream is an object that you can; 1. Initialize, so the head of the stream is ready to be consumed.; 2. Step, so updates the current head to the next element in the stream. A join then takes two streams and its Step steps the left stream, the right stream, or both streams until a ""valid"" state is reached. E.g. In an outer join every step is a new valid state while an inner join must loop until the match condition is met. The node must also take some combining binary functions so as not to produce runtime tuples. Optional ""left"" and ""right"" functions determine at compilation time whether an unmatched pair of values should still produce a value (e.g. in a left join you always act, just either on just the left head or both). ## State of the code base; - Added `produce` abstract method to `ArrayEmitter`. Tests will fail until that is implemented on all the array nodes.; - Inserted a small hack to the `ir.ToArray` emit case that uses the producer route; - Added a test case for `ArrayJoin` (this one passes! :)); - Have not yet incorporated missingness into producer logic and have not implemented `consume` for ArrayJoin`.; - `Binds` needs to be updated to add the appropriate bindings for the children. Right now it just adds them all.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6516
https://github.com/hail-is/hail/pull/6516:1173,Testability,log,logic,1173,"Start of an IR node for joining two streams. ## Logic; Works off the ""producer"" pattern, where a stream is an object that you can; 1. Initialize, so the head of the stream is ready to be consumed.; 2. Step, so updates the current head to the next element in the stream. A join then takes two streams and its Step steps the left stream, the right stream, or both streams until a ""valid"" state is reached. E.g. In an outer join every step is a new valid state while an inner join must loop until the match condition is met. The node must also take some combining binary functions so as not to produce runtime tuples. Optional ""left"" and ""right"" functions determine at compilation time whether an unmatched pair of values should still produce a value (e.g. in a left join you always act, just either on just the left head or both). ## State of the code base; - Added `produce` abstract method to `ArrayEmitter`. Tests will fail until that is implemented on all the array nodes.; - Inserted a small hack to the `ir.ToArray` emit case that uses the producer route; - Added a test case for `ArrayJoin` (this one passes! :)); - Have not yet incorporated missingness into producer logic and have not implemented `consume` for ArrayJoin`.; - `Binds` needs to be updated to add the appropriate bindings for the children. Right now it just adds them all.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6516
https://github.com/hail-is/hail/pull/6518:115,Modifiability,refactor,refactor,115,these are the last of the changes from #6480; really this should be several distinct PRs and if I have a chance to refactor the tests I'll split it apart. notes to follow in the morning.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6518
https://github.com/hail-is/hail/pull/6518:128,Testability,test,tests,128,these are the last of the changes from #6480; really this should be several distinct PRs and if I have a chance to refactor the tests I'll split it apart. notes to follow in the morning.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6518
https://github.com/hail-is/hail/pull/6525:30,Testability,log,logging,30,Seeing if I can get any batch logging going in Prometheus without personal batch working.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6525
https://github.com/hail-is/hail/pull/6534:222,Testability,test,tested,222,"Third piece of #6518; staged on #6532. Add extract aggregators pass using staged aggregators. Currently uses files for communicating between workers and master. This isn't ideal, but I wanted to try to get this merged and tested as quickly as possible before starting to modify anything.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6534
https://github.com/hail-is/hail/pull/6537:161,Testability,test,test,161,"- If 'input' or 'output' task, check for existance of success file before continuing. If success file exists, just exit 0; - If 'main', use an init container to test whether a success file exists. If so, delete the file and restart the entire job (including ""input""). See #6494",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6537
https://github.com/hail-is/hail/pull/6539:36,Availability,error,error,36,Wasn't sure if this should raise an error or just continue.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6539
https://github.com/hail-is/hail/pull/6541:16,Performance,perform,performance,16,Pretty horrible performance. Most of the time is spent in the IR evaluation to annotate entries:. ```; (Let __iruid_65; (InsertFields; (Ref row); None; (`the entries! [877f12a8827e18f61222c6c8c5fb04a8]`; (ArrayMap __iruid_66; (ArrayRange; (I32 0); (I32 10); (I32 1)); (Literal Struct{} <literal value>)))); (InsertFields; (Ref __iruid_65); None; (`the entries! [877f12a8827e18f61222c6c8c5fb04a8]`; (ArrayMap __iruid_67; (ArrayRange; (I32 0); (ArrayLen; (GetField __cols; (Ref global))); (I32 1)); (InsertFields; (ArrayRef; (GetField `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`; (Ref __iruid_65)); (Ref __iruid_67)); None; (x; (ApplyBinaryPrimOp Add; (GetField col_idx; (ArrayRef; (GetField __cols; (Ref global)); (Ref __iruid_67))); (GetField row_idx; (Ref __iruid_65))))))))))); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6541
https://github.com/hail-is/hail/issues/6543:22,Availability,ERROR,ERROR,22,"```; {; ""levelname"": ""ERROR"",; ""asctime"": ""2019-07-02 13:17:00,483"",; ""filename"": ""web_protocol.py"",; ""funcNameAndLine"": ""log_exception:355"",; ""message"": ""Error handling request"",; ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_protocol.py\"", line 418, in start\n resp = await task\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_app.py\"", line 458, in _handle\n resp = await handler(request)\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_urldispatcher.py\"", line 157, in handler_wrapper\n result = await result\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 914, in create_jobs\n success = await jobs_builder.commit()\n File \""/usr/local/lib/python3.6/dist-packages/batch/database.py\"", line 161, in commit\n await cursor.executemany(self._jobs_sql, self._jobs)\n File \""/usr/local/lib/python3.6/dist-packages/aiomysql/cursors.py\"", line 283, in executemany\n self._get_db().encoding))\n File \""/usr/local/lib/python3.6/dist-packages/aiomysql/cursors.py\"", line 318, in _do_execute_many\n r = await self.execute(sql + postfix)\n File \""/usr/local/lib/python3.6/dist-packages/aiomysql/cursors.py\"", line 239, in execute\n await self._query(query)\n File \""/usr/local/lib/python3.6/dist-packages/aiomysql/cursors.py\"", line 457, in _query\n await conn.query(q)\n File \""/usr/local/lib/python3.6/dist-packages/aiomysql/connection.py\"", line 428, in query\n await self._read_query_result(unbuffered=unbuffered)\n File \""/usr/local/lib/python3.6/dist-packages/aiomysql/connection.py\"", line 622, in _read_query_result\n await result.read()\n File \""/usr/local/lib/python3.6/dist-packages/aiomysql/connection.py\"", line 1105, in read\n first_packet = await self.connection._read_packet()\n File \""/usr/local/lib/python3.6/dist-packages/aiomysql/connection.py\"", line 593, in _read_packet\n packet.check_error()\n File \""/usr/local/lib/python3.6/dist-packages/pymysql/protocol.py\"", line 220, in ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6543
https://github.com/hail-is/hail/issues/6543:155,Availability,Error,Error,155,"```; {; ""levelname"": ""ERROR"",; ""asctime"": ""2019-07-02 13:17:00,483"",; ""filename"": ""web_protocol.py"",; ""funcNameAndLine"": ""log_exception:355"",; ""message"": ""Error handling request"",; ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_protocol.py\"", line 418, in start\n resp = await task\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_app.py\"", line 458, in _handle\n resp = await handler(request)\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_urldispatcher.py\"", line 157, in handler_wrapper\n result = await result\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 914, in create_jobs\n success = await jobs_builder.commit()\n File \""/usr/local/lib/python3.6/dist-packages/batch/database.py\"", line 161, in commit\n await cursor.executemany(self._jobs_sql, self._jobs)\n File \""/usr/local/lib/python3.6/dist-packages/aiomysql/cursors.py\"", line 283, in executemany\n self._get_db().encoding))\n File \""/usr/local/lib/python3.6/dist-packages/aiomysql/cursors.py\"", line 318, in _do_execute_many\n r = await self.execute(sql + postfix)\n File \""/usr/local/lib/python3.6/dist-packages/aiomysql/cursors.py\"", line 239, in execute\n await self._query(query)\n File \""/usr/local/lib/python3.6/dist-packages/aiomysql/cursors.py\"", line 457, in _query\n await conn.query(q)\n File \""/usr/local/lib/python3.6/dist-packages/aiomysql/connection.py\"", line 428, in query\n await self._read_query_result(unbuffered=unbuffered)\n File \""/usr/local/lib/python3.6/dist-packages/aiomysql/connection.py\"", line 622, in _read_query_result\n await result.read()\n File \""/usr/local/lib/python3.6/dist-packages/aiomysql/connection.py\"", line 1105, in read\n first_packet = await self.connection._read_packet()\n File \""/usr/local/lib/python3.6/dist-packages/aiomysql/connection.py\"", line 593, in _read_packet\n packet.check_error()\n File \""/usr/local/lib/python3.6/dist-packages/pymysql/protocol.py\"", line 220, in ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6543
https://github.com/hail-is/hail/issues/6543:2162,Availability,error,errorclass,2162,"cal/lib/python3.6/dist-packages/aiomysql/cursors.py\"", line 239, in execute\n await self._query(query)\n File \""/usr/local/lib/python3.6/dist-packages/aiomysql/cursors.py\"", line 457, in _query\n await conn.query(q)\n File \""/usr/local/lib/python3.6/dist-packages/aiomysql/connection.py\"", line 428, in query\n await self._read_query_result(unbuffered=unbuffered)\n File \""/usr/local/lib/python3.6/dist-packages/aiomysql/connection.py\"", line 622, in _read_query_result\n await result.read()\n File \""/usr/local/lib/python3.6/dist-packages/aiomysql/connection.py\"", line 1105, in read\n first_packet = await self.connection._read_packet()\n File \""/usr/local/lib/python3.6/dist-packages/aiomysql/connection.py\"", line 593, in _read_packet\n packet.check_error()\n File \""/usr/local/lib/python3.6/dist-packages/pymysql/protocol.py\"", line 220, in check_error\n err.raise_mysql_exception(self._data)\n File \""/usr/local/lib/python3.6/dist-packages/pymysql/err.py\"", line 109, in raise_mysql_exception\n raise errorclass(errno, errval)\npymysql.err.OperationalError: (1213, 'Deadlock found when trying to get lock; try restarting transaction')""; }; {; ""levelname"": ""INFO"",; ""asctime"": ""2019-07-02 13:17:00,609"",; ""filename"": ""web_log.py"",; ""funcNameAndLine"": ""log:233"",; ""message"": ""10.32.4.181 [02/Jul/2019:13:17:00 +0000] \""POST /api/v1alpha/batches/278/jobs/create HTTP/1.0\"" 500 225 \""-\"" \""Python/3.7 aiohttp/3.5.4\"""",; ""remote_address"": ""10.32.4.181"",; ""request_start_time"": ""[02/Jul/2019:13:17:00 +0000]"",; ""first_request_line"": ""POST /api/v1alpha/batches/278/jobs/create HTTP/1.0"",; ""response_status"": 500,; ""response_size"": 225,; ""request_header"": {; ""Referer"": ""-"",; ""User-Agent"": ""Python/3.7 aiohttp/3.5.4""; }; }; ```; ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_protocol.py"", line 418, in start; resp = await task; File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_app.py"", line 458, in _handle; resp = await handler(request); Fil",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6543
https://github.com/hail-is/hail/issues/6543:4795,Availability,error,errorclass,4795,"/local/lib/python3.6/dist-packages/aiohttp/web_protocol.py"", line 418, in start; resp = await task; File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_app.py"", line 458, in _handle; resp = await handler(request); File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_urldispatcher.py"", line 157, in handler_wrapper; result = await result; File ""/usr/local/lib/python3.6/dist-packages/batch/batch.py"", line 914, in create_jobs; success = await jobs_builder.commit(); File ""/usr/local/lib/python3.6/dist-packages/batch/database.py"", line 161, in commit; await cursor.executemany(self._jobs_sql, self._jobs); File ""/usr/local/lib/python3.6/dist-packages/aiomysql/cursors.py"", line 283, in executemany; self._get_db().encoding)); File ""/usr/local/lib/python3.6/dist-packages/aiomysql/cursors.py"", line 318, in _do_execute_many; r = await self.execute(sql + postfix); File ""/usr/local/lib/python3.6/dist-packages/aiomysql/cursors.py"", line 239, in execute; await self._query(query); File ""/usr/local/lib/python3.6/dist-packages/aiomysql/cursors.py"", line 457, in _query; await conn.query(q); File ""/usr/local/lib/python3.6/dist-packages/aiomysql/connection.py"", line 428, in query; await self._read_query_result(unbuffered=unbuffered); File ""/usr/local/lib/python3.6/dist-packages/aiomysql/connection.py"", line 622, in _read_query_result; await result.read(); File ""/usr/local/lib/python3.6/dist-packages/aiomysql/connection.py"", line 1105, in read; first_packet = await self.connection._read_packet(); File ""/usr/local/lib/python3.6/dist-packages/aiomysql/connection.py"", line 593, in _read_packet; packet.check_error(); File ""/usr/local/lib/python3.6/dist-packages/pymysql/protocol.py"", line 220, in check_error; err.raise_mysql_exception(self._data); File ""/usr/local/lib/python3.6/dist-packages/pymysql/err.py"", line 109, in raise_mysql_exception; raise errorclass(errno, errval); pymysql.err.OperationalError: (1213, 'Deadlock found when trying to get lock; try restarting transaction'); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6543
https://github.com/hail-is/hail/issues/6543:144,Integrability,message,message,144,"```; {; ""levelname"": ""ERROR"",; ""asctime"": ""2019-07-02 13:17:00,483"",; ""filename"": ""web_protocol.py"",; ""funcNameAndLine"": ""log_exception:355"",; ""message"": ""Error handling request"",; ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_protocol.py\"", line 418, in start\n resp = await task\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_app.py\"", line 458, in _handle\n resp = await handler(request)\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_urldispatcher.py\"", line 157, in handler_wrapper\n result = await result\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 914, in create_jobs\n success = await jobs_builder.commit()\n File \""/usr/local/lib/python3.6/dist-packages/batch/database.py\"", line 161, in commit\n await cursor.executemany(self._jobs_sql, self._jobs)\n File \""/usr/local/lib/python3.6/dist-packages/aiomysql/cursors.py\"", line 283, in executemany\n self._get_db().encoding))\n File \""/usr/local/lib/python3.6/dist-packages/aiomysql/cursors.py\"", line 318, in _do_execute_many\n r = await self.execute(sql + postfix)\n File \""/usr/local/lib/python3.6/dist-packages/aiomysql/cursors.py\"", line 239, in execute\n await self._query(query)\n File \""/usr/local/lib/python3.6/dist-packages/aiomysql/cursors.py\"", line 457, in _query\n await conn.query(q)\n File \""/usr/local/lib/python3.6/dist-packages/aiomysql/connection.py\"", line 428, in query\n await self._read_query_result(unbuffered=unbuffered)\n File \""/usr/local/lib/python3.6/dist-packages/aiomysql/connection.py\"", line 622, in _read_query_result\n await result.read()\n File \""/usr/local/lib/python3.6/dist-packages/aiomysql/connection.py\"", line 1105, in read\n first_packet = await self.connection._read_packet()\n File \""/usr/local/lib/python3.6/dist-packages/aiomysql/connection.py\"", line 593, in _read_packet\n packet.check_error()\n File \""/usr/local/lib/python3.6/dist-packages/pymysql/protocol.py\"", line 220, in ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6543
https://github.com/hail-is/hail/issues/6543:1973,Integrability,protocol,protocol,1973,"many\n self._get_db().encoding))\n File \""/usr/local/lib/python3.6/dist-packages/aiomysql/cursors.py\"", line 318, in _do_execute_many\n r = await self.execute(sql + postfix)\n File \""/usr/local/lib/python3.6/dist-packages/aiomysql/cursors.py\"", line 239, in execute\n await self._query(query)\n File \""/usr/local/lib/python3.6/dist-packages/aiomysql/cursors.py\"", line 457, in _query\n await conn.query(q)\n File \""/usr/local/lib/python3.6/dist-packages/aiomysql/connection.py\"", line 428, in query\n await self._read_query_result(unbuffered=unbuffered)\n File \""/usr/local/lib/python3.6/dist-packages/aiomysql/connection.py\"", line 622, in _read_query_result\n await result.read()\n File \""/usr/local/lib/python3.6/dist-packages/aiomysql/connection.py\"", line 1105, in read\n first_packet = await self.connection._read_packet()\n File \""/usr/local/lib/python3.6/dist-packages/aiomysql/connection.py\"", line 593, in _read_packet\n packet.check_error()\n File \""/usr/local/lib/python3.6/dist-packages/pymysql/protocol.py\"", line 220, in check_error\n err.raise_mysql_exception(self._data)\n File \""/usr/local/lib/python3.6/dist-packages/pymysql/err.py\"", line 109, in raise_mysql_exception\n raise errorclass(errno, errval)\npymysql.err.OperationalError: (1213, 'Deadlock found when trying to get lock; try restarting transaction')""; }; {; ""levelname"": ""INFO"",; ""asctime"": ""2019-07-02 13:17:00,609"",; ""filename"": ""web_log.py"",; ""funcNameAndLine"": ""log:233"",; ""message"": ""10.32.4.181 [02/Jul/2019:13:17:00 +0000] \""POST /api/v1alpha/batches/278/jobs/create HTTP/1.0\"" 500 225 \""-\"" \""Python/3.7 aiohttp/3.5.4\"""",; ""remote_address"": ""10.32.4.181"",; ""request_start_time"": ""[02/Jul/2019:13:17:00 +0000]"",; ""first_request_line"": ""POST /api/v1alpha/batches/278/jobs/create HTTP/1.0"",; ""response_status"": 500,; ""response_size"": 225,; ""request_header"": {; ""Referer"": ""-"",; ""User-Agent"": ""Python/3.7 aiohttp/3.5.4""; }; }; ```; ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/dist-package",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6543
https://github.com/hail-is/hail/issues/6543:2424,Integrability,message,message,2424,"ckages/aiomysql/connection.py\"", line 428, in query\n await self._read_query_result(unbuffered=unbuffered)\n File \""/usr/local/lib/python3.6/dist-packages/aiomysql/connection.py\"", line 622, in _read_query_result\n await result.read()\n File \""/usr/local/lib/python3.6/dist-packages/aiomysql/connection.py\"", line 1105, in read\n first_packet = await self.connection._read_packet()\n File \""/usr/local/lib/python3.6/dist-packages/aiomysql/connection.py\"", line 593, in _read_packet\n packet.check_error()\n File \""/usr/local/lib/python3.6/dist-packages/pymysql/protocol.py\"", line 220, in check_error\n err.raise_mysql_exception(self._data)\n File \""/usr/local/lib/python3.6/dist-packages/pymysql/err.py\"", line 109, in raise_mysql_exception\n raise errorclass(errno, errval)\npymysql.err.OperationalError: (1213, 'Deadlock found when trying to get lock; try restarting transaction')""; }; {; ""levelname"": ""INFO"",; ""asctime"": ""2019-07-02 13:17:00,609"",; ""filename"": ""web_log.py"",; ""funcNameAndLine"": ""log:233"",; ""message"": ""10.32.4.181 [02/Jul/2019:13:17:00 +0000] \""POST /api/v1alpha/batches/278/jobs/create HTTP/1.0\"" 500 225 \""-\"" \""Python/3.7 aiohttp/3.5.4\"""",; ""remote_address"": ""10.32.4.181"",; ""request_start_time"": ""[02/Jul/2019:13:17:00 +0000]"",; ""first_request_line"": ""POST /api/v1alpha/batches/278/jobs/create HTTP/1.0"",; ""response_status"": 500,; ""response_size"": 225,; ""request_header"": {; ""Referer"": ""-"",; ""User-Agent"": ""Python/3.7 aiohttp/3.5.4""; }; }; ```; ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_protocol.py"", line 418, in start; resp = await task; File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_app.py"", line 458, in _handle; resp = await handler(request); File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_urldispatcher.py"", line 157, in handler_wrapper; result = await result; File ""/usr/local/lib/python3.6/dist-packages/batch/batch.py"", line 914, in create_jobs; success = await jobs_builder.commit(); File",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6543
https://github.com/hail-is/hail/issues/6543:4612,Integrability,protocol,protocol,4612,"/local/lib/python3.6/dist-packages/aiohttp/web_protocol.py"", line 418, in start; resp = await task; File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_app.py"", line 458, in _handle; resp = await handler(request); File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_urldispatcher.py"", line 157, in handler_wrapper; result = await result; File ""/usr/local/lib/python3.6/dist-packages/batch/batch.py"", line 914, in create_jobs; success = await jobs_builder.commit(); File ""/usr/local/lib/python3.6/dist-packages/batch/database.py"", line 161, in commit; await cursor.executemany(self._jobs_sql, self._jobs); File ""/usr/local/lib/python3.6/dist-packages/aiomysql/cursors.py"", line 283, in executemany; self._get_db().encoding)); File ""/usr/local/lib/python3.6/dist-packages/aiomysql/cursors.py"", line 318, in _do_execute_many; r = await self.execute(sql + postfix); File ""/usr/local/lib/python3.6/dist-packages/aiomysql/cursors.py"", line 239, in execute; await self._query(query); File ""/usr/local/lib/python3.6/dist-packages/aiomysql/cursors.py"", line 457, in _query; await conn.query(q); File ""/usr/local/lib/python3.6/dist-packages/aiomysql/connection.py"", line 428, in query; await self._read_query_result(unbuffered=unbuffered); File ""/usr/local/lib/python3.6/dist-packages/aiomysql/connection.py"", line 622, in _read_query_result; await result.read(); File ""/usr/local/lib/python3.6/dist-packages/aiomysql/connection.py"", line 1105, in read; first_packet = await self.connection._read_packet(); File ""/usr/local/lib/python3.6/dist-packages/aiomysql/connection.py"", line 593, in _read_packet; packet.check_error(); File ""/usr/local/lib/python3.6/dist-packages/pymysql/protocol.py"", line 220, in check_error; err.raise_mysql_exception(self._data); File ""/usr/local/lib/python3.6/dist-packages/pymysql/err.py"", line 109, in raise_mysql_exception; raise errorclass(errno, errval); pymysql.err.OperationalError: (1213, 'Deadlock found when trying to get lock; try restarting transaction'); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6543
https://github.com/hail-is/hail/issues/6543:2412,Testability,log,log,2412,"ckages/aiomysql/connection.py\"", line 428, in query\n await self._read_query_result(unbuffered=unbuffered)\n File \""/usr/local/lib/python3.6/dist-packages/aiomysql/connection.py\"", line 622, in _read_query_result\n await result.read()\n File \""/usr/local/lib/python3.6/dist-packages/aiomysql/connection.py\"", line 1105, in read\n first_packet = await self.connection._read_packet()\n File \""/usr/local/lib/python3.6/dist-packages/aiomysql/connection.py\"", line 593, in _read_packet\n packet.check_error()\n File \""/usr/local/lib/python3.6/dist-packages/pymysql/protocol.py\"", line 220, in check_error\n err.raise_mysql_exception(self._data)\n File \""/usr/local/lib/python3.6/dist-packages/pymysql/err.py\"", line 109, in raise_mysql_exception\n raise errorclass(errno, errval)\npymysql.err.OperationalError: (1213, 'Deadlock found when trying to get lock; try restarting transaction')""; }; {; ""levelname"": ""INFO"",; ""asctime"": ""2019-07-02 13:17:00,609"",; ""filename"": ""web_log.py"",; ""funcNameAndLine"": ""log:233"",; ""message"": ""10.32.4.181 [02/Jul/2019:13:17:00 +0000] \""POST /api/v1alpha/batches/278/jobs/create HTTP/1.0\"" 500 225 \""-\"" \""Python/3.7 aiohttp/3.5.4\"""",; ""remote_address"": ""10.32.4.181"",; ""request_start_time"": ""[02/Jul/2019:13:17:00 +0000]"",; ""first_request_line"": ""POST /api/v1alpha/batches/278/jobs/create HTTP/1.0"",; ""response_status"": 500,; ""response_size"": 225,; ""request_header"": {; ""Referer"": ""-"",; ""User-Agent"": ""Python/3.7 aiohttp/3.5.4""; }; }; ```; ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_protocol.py"", line 418, in start; resp = await task; File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_app.py"", line 458, in _handle; resp = await handler(request); File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_urldispatcher.py"", line 157, in handler_wrapper; result = await result; File ""/usr/local/lib/python3.6/dist-packages/batch/batch.py"", line 914, in create_jobs; success = await jobs_builder.commit(); File",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6543
https://github.com/hail-is/hail/issues/6545:4,Availability,error,errors,4,"The errors look like this:; ```; {""levelname"": ""INFO"", ""asctime"": ""2019-07-02 13:36:45,504"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1074"", ""message"": ""update job (278, 6858, 'main') with pod batch-278-job-6858-5879db""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-02 13:36:45,504"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1087"", ""message"": ""job (278, 6858, 'main') mark complete""}; File ""/usr/local/lib/python3.6/dist-packages/batch/k8s.py"", line 65, in wrapped; **kwargs),; File ""/usr/local/lib/python3.6/dist-packages/batch/blocking_to_async.py"", line 6, in blocking_to_async; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/lib/python3.6/concurrent/futures/thread.py"", line 56, in run; result = self.fn(*self.args, **self.kwargs); File ""/usr/local/lib/python3.6/dist-packages/batch/blocking_to_async.py"", line 6, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/apis/core_v1_api.py"", line 18538, in read_namespaced_pod_log; (data) = self.read_namespaced_pod_log_with_http_info(name, namespace, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/apis/core_v1_api.py"", line 18644, in read_namespaced_pod_log_with_http_info; collection_formats=collection_formats); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 334, in call_api; _return_http_data_only, collection_formats, _preload_content, _request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 168, in __call_api; _request_timeout=_request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 355, in request; headers=headers); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 231, in GET; query_params=query_params); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 222, in request; raise ApiException(http_resp=r);",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6545
https://github.com/hail-is/hail/issues/6545:2192,Availability,error,error,2192,"ad_namespaced_pod_log_with_http_info; collection_formats=collection_formats); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 334, in call_api; _return_http_data_only, collection_formats, _preload_content, _request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 168, in __call_api; _request_timeout=_request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 355, in request; headers=headers); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 231, in GET; query_params=query_params); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 222, in request; raise ApiException(http_resp=r); {""levelname"": ""INFO"", ""asctime"": ""2019-07-02 13:36:45,525"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""mark_complete:542"", ""message"": ""no logs for batch-278-job-6858-5879db due to previous error, rescheduling pod Error: (404)\nReason: Not Found\nHTTP response headers: HTTPHeaderDict({'Audit-Id': '891f2153-6a94-42ff-8fe1-edf644051234', 'Content-Type': 'application/json', 'Date': 'Tue, 02 Jul 2019 13:36:45 GMT', 'Content-Length': '218'})\nHTTP response body: {\""kind\"":\""Status\"",\""apiVersion\"":\""v1\"",\""metadata\"":{},\""status\"":\""Failure\"",\""message\"":\""pods \\\""batch-278-job-6858-5879db\\\"" not found\"",\""reason\"":\""NotFound\"",\""details\"":{\""name\"":\""batch-278-job-6858-5879db\"",\""kind\"":\""pods\""},\""code\"":404}\n\n""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-02 13:36:45,541"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""set_state:457"", ""message"": ""job (278, 6858, 'main') changed state: Running -> Ready""}; ```. Here are events that don't contain the string ""Successfully assigned batch-pods"": [events.log](https://github.com/hail-is/hail/files/3350320/events.log). There's a lot of issue with secrets getting mounted and a couple container creation failures, but nothing that obviously suggests a problem with reading logs.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6545
https://github.com/hail-is/hail/issues/6545:2216,Availability,Error,Error,2216,"ad_namespaced_pod_log_with_http_info; collection_formats=collection_formats); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 334, in call_api; _return_http_data_only, collection_formats, _preload_content, _request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 168, in __call_api; _request_timeout=_request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 355, in request; headers=headers); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 231, in GET; query_params=query_params); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 222, in request; raise ApiException(http_resp=r); {""levelname"": ""INFO"", ""asctime"": ""2019-07-02 13:36:45,525"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""mark_complete:542"", ""message"": ""no logs for batch-278-job-6858-5879db due to previous error, rescheduling pod Error: (404)\nReason: Not Found\nHTTP response headers: HTTPHeaderDict({'Audit-Id': '891f2153-6a94-42ff-8fe1-edf644051234', 'Content-Type': 'application/json', 'Date': 'Tue, 02 Jul 2019 13:36:45 GMT', 'Content-Length': '218'})\nHTTP response body: {\""kind\"":\""Status\"",\""apiVersion\"":\""v1\"",\""metadata\"":{},\""status\"":\""Failure\"",\""message\"":\""pods \\\""batch-278-job-6858-5879db\\\"" not found\"",\""reason\"":\""NotFound\"",\""details\"":{\""name\"":\""batch-278-job-6858-5879db\"",\""kind\"":\""pods\""},\""code\"":404}\n\n""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-02 13:36:45,541"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""set_state:457"", ""message"": ""job (278, 6858, 'main') changed state: Running -> Ready""}; ```. Here are events that don't contain the string ""Successfully assigned batch-pods"": [events.log](https://github.com/hail-is/hail/files/3350320/events.log). There's a lot of issue with secrets getting mounted and a couple container creation failures, but nothing that obviously suggests a problem with reading logs.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6545
https://github.com/hail-is/hail/issues/6545:2536,Availability,Failure,Failure,2536,"ad_namespaced_pod_log_with_http_info; collection_formats=collection_formats); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 334, in call_api; _return_http_data_only, collection_formats, _preload_content, _request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 168, in __call_api; _request_timeout=_request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 355, in request; headers=headers); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 231, in GET; query_params=query_params); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 222, in request; raise ApiException(http_resp=r); {""levelname"": ""INFO"", ""asctime"": ""2019-07-02 13:36:45,525"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""mark_complete:542"", ""message"": ""no logs for batch-278-job-6858-5879db due to previous error, rescheduling pod Error: (404)\nReason: Not Found\nHTTP response headers: HTTPHeaderDict({'Audit-Id': '891f2153-6a94-42ff-8fe1-edf644051234', 'Content-Type': 'application/json', 'Date': 'Tue, 02 Jul 2019 13:36:45 GMT', 'Content-Length': '218'})\nHTTP response body: {\""kind\"":\""Status\"",\""apiVersion\"":\""v1\"",\""metadata\"":{},\""status\"":\""Failure\"",\""message\"":\""pods \\\""batch-278-job-6858-5879db\\\"" not found\"",\""reason\"":\""NotFound\"",\""details\"":{\""name\"":\""batch-278-job-6858-5879db\"",\""kind\"":\""pods\""},\""code\"":404}\n\n""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-02 13:36:45,541"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""set_state:457"", ""message"": ""job (278, 6858, 'main') changed state: Running -> Ready""}; ```. Here are events that don't contain the string ""Successfully assigned batch-pods"": [events.log](https://github.com/hail-is/hail/files/3350320/events.log). There's a lot of issue with secrets getting mounted and a couple container creation failures, but nothing that obviously suggests a problem with reading logs.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6545
https://github.com/hail-is/hail/issues/6545:3161,Availability,failure,failures,3161,"ad_namespaced_pod_log_with_http_info; collection_formats=collection_formats); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 334, in call_api; _return_http_data_only, collection_formats, _preload_content, _request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 168, in __call_api; _request_timeout=_request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 355, in request; headers=headers); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 231, in GET; query_params=query_params); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 222, in request; raise ApiException(http_resp=r); {""levelname"": ""INFO"", ""asctime"": ""2019-07-02 13:36:45,525"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""mark_complete:542"", ""message"": ""no logs for batch-278-job-6858-5879db due to previous error, rescheduling pod Error: (404)\nReason: Not Found\nHTTP response headers: HTTPHeaderDict({'Audit-Id': '891f2153-6a94-42ff-8fe1-edf644051234', 'Content-Type': 'application/json', 'Date': 'Tue, 02 Jul 2019 13:36:45 GMT', 'Content-Length': '218'})\nHTTP response body: {\""kind\"":\""Status\"",\""apiVersion\"":\""v1\"",\""metadata\"":{},\""status\"":\""Failure\"",\""message\"":\""pods \\\""batch-278-job-6858-5879db\\\"" not found\"",\""reason\"":\""NotFound\"",\""details\"":{\""name\"":\""batch-278-job-6858-5879db\"",\""kind\"":\""pods\""},\""code\"":404}\n\n""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-02 13:36:45,541"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""set_state:457"", ""message"": ""job (278, 6858, 'main') changed state: Running -> Ready""}; ```. Here are events that don't contain the string ""Successfully assigned batch-pods"": [events.log](https://github.com/hail-is/hail/files/3350320/events.log). There's a lot of issue with secrets getting mounted and a couple container creation failures, but nothing that obviously suggests a problem with reading logs.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6545
https://github.com/hail-is/hail/issues/6545:176,Deployability,update,update,176,"The errors look like this:; ```; {""levelname"": ""INFO"", ""asctime"": ""2019-07-02 13:36:45,504"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1074"", ""message"": ""update job (278, 6858, 'main') with pod batch-278-job-6858-5879db""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-02 13:36:45,504"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1087"", ""message"": ""job (278, 6858, 'main') mark complete""}; File ""/usr/local/lib/python3.6/dist-packages/batch/k8s.py"", line 65, in wrapped; **kwargs),; File ""/usr/local/lib/python3.6/dist-packages/batch/blocking_to_async.py"", line 6, in blocking_to_async; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/lib/python3.6/concurrent/futures/thread.py"", line 56, in run; result = self.fn(*self.args, **self.kwargs); File ""/usr/local/lib/python3.6/dist-packages/batch/blocking_to_async.py"", line 6, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/apis/core_v1_api.py"", line 18538, in read_namespaced_pod_log; (data) = self.read_namespaced_pod_log_with_http_info(name, namespace, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/apis/core_v1_api.py"", line 18644, in read_namespaced_pod_log_with_http_info; collection_formats=collection_formats); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 334, in call_api; _return_http_data_only, collection_formats, _preload_content, _request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 168, in __call_api; _request_timeout=_request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 355, in request; headers=headers); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 231, in GET; query_params=query_params); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 222, in request; raise ApiException(http_resp=r);",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6545
https://github.com/hail-is/hail/issues/6545:165,Integrability,message,message,165,"The errors look like this:; ```; {""levelname"": ""INFO"", ""asctime"": ""2019-07-02 13:36:45,504"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1074"", ""message"": ""update job (278, 6858, 'main') with pod batch-278-job-6858-5879db""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-02 13:36:45,504"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1087"", ""message"": ""job (278, 6858, 'main') mark complete""}; File ""/usr/local/lib/python3.6/dist-packages/batch/k8s.py"", line 65, in wrapped; **kwargs),; File ""/usr/local/lib/python3.6/dist-packages/batch/blocking_to_async.py"", line 6, in blocking_to_async; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/lib/python3.6/concurrent/futures/thread.py"", line 56, in run; result = self.fn(*self.args, **self.kwargs); File ""/usr/local/lib/python3.6/dist-packages/batch/blocking_to_async.py"", line 6, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/apis/core_v1_api.py"", line 18538, in read_namespaced_pod_log; (data) = self.read_namespaced_pod_log_with_http_info(name, namespace, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/apis/core_v1_api.py"", line 18644, in read_namespaced_pod_log_with_http_info; collection_formats=collection_formats); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 334, in call_api; _return_http_data_only, collection_formats, _preload_content, _request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 168, in __call_api; _request_timeout=_request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 355, in request; headers=headers); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 231, in GET; query_params=query_params); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 222, in request; raise ApiException(http_resp=r);",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6545
https://github.com/hail-is/hail/issues/6545:377,Integrability,message,message,377,"The errors look like this:; ```; {""levelname"": ""INFO"", ""asctime"": ""2019-07-02 13:36:45,504"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1074"", ""message"": ""update job (278, 6858, 'main') with pod batch-278-job-6858-5879db""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-02 13:36:45,504"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1087"", ""message"": ""job (278, 6858, 'main') mark complete""}; File ""/usr/local/lib/python3.6/dist-packages/batch/k8s.py"", line 65, in wrapped; **kwargs),; File ""/usr/local/lib/python3.6/dist-packages/batch/blocking_to_async.py"", line 6, in blocking_to_async; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/lib/python3.6/concurrent/futures/thread.py"", line 56, in run; result = self.fn(*self.args, **self.kwargs); File ""/usr/local/lib/python3.6/dist-packages/batch/blocking_to_async.py"", line 6, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/apis/core_v1_api.py"", line 18538, in read_namespaced_pod_log; (data) = self.read_namespaced_pod_log_with_http_info(name, namespace, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/apis/core_v1_api.py"", line 18644, in read_namespaced_pod_log_with_http_info; collection_formats=collection_formats); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 334, in call_api; _return_http_data_only, collection_formats, _preload_content, _request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 168, in __call_api; _request_timeout=_request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 355, in request; headers=headers); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 231, in GET; query_params=query_params); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 222, in request; raise ApiException(http_resp=r);",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6545
https://github.com/hail-is/hail/issues/6545:501,Integrability,wrap,wrapped,501,"The errors look like this:; ```; {""levelname"": ""INFO"", ""asctime"": ""2019-07-02 13:36:45,504"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1074"", ""message"": ""update job (278, 6858, 'main') with pod batch-278-job-6858-5879db""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-02 13:36:45,504"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1087"", ""message"": ""job (278, 6858, 'main') mark complete""}; File ""/usr/local/lib/python3.6/dist-packages/batch/k8s.py"", line 65, in wrapped; **kwargs),; File ""/usr/local/lib/python3.6/dist-packages/batch/blocking_to_async.py"", line 6, in blocking_to_async; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/lib/python3.6/concurrent/futures/thread.py"", line 56, in run; result = self.fn(*self.args, **self.kwargs); File ""/usr/local/lib/python3.6/dist-packages/batch/blocking_to_async.py"", line 6, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/apis/core_v1_api.py"", line 18538, in read_namespaced_pod_log; (data) = self.read_namespaced_pod_log_with_http_info(name, namespace, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/apis/core_v1_api.py"", line 18644, in read_namespaced_pod_log_with_http_info; collection_formats=collection_formats); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 334, in call_api; _return_http_data_only, collection_formats, _preload_content, _request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 168, in __call_api; _request_timeout=_request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 355, in request; headers=headers); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 231, in GET; query_params=query_params); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 222, in request; raise ApiException(http_resp=r);",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6545
https://github.com/hail-is/hail/issues/6545:2127,Integrability,message,message,2127,"ad_namespaced_pod_log_with_http_info; collection_formats=collection_formats); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 334, in call_api; _return_http_data_only, collection_formats, _preload_content, _request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 168, in __call_api; _request_timeout=_request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 355, in request; headers=headers); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 231, in GET; query_params=query_params); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 222, in request; raise ApiException(http_resp=r); {""levelname"": ""INFO"", ""asctime"": ""2019-07-02 13:36:45,525"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""mark_complete:542"", ""message"": ""no logs for batch-278-job-6858-5879db due to previous error, rescheduling pod Error: (404)\nReason: Not Found\nHTTP response headers: HTTPHeaderDict({'Audit-Id': '891f2153-6a94-42ff-8fe1-edf644051234', 'Content-Type': 'application/json', 'Date': 'Tue, 02 Jul 2019 13:36:45 GMT', 'Content-Length': '218'})\nHTTP response body: {\""kind\"":\""Status\"",\""apiVersion\"":\""v1\"",\""metadata\"":{},\""status\"":\""Failure\"",\""message\"":\""pods \\\""batch-278-job-6858-5879db\\\"" not found\"",\""reason\"":\""NotFound\"",\""details\"":{\""name\"":\""batch-278-job-6858-5879db\"",\""kind\"":\""pods\""},\""code\"":404}\n\n""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-02 13:36:45,541"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""set_state:457"", ""message"": ""job (278, 6858, 'main') changed state: Running -> Ready""}; ```. Here are events that don't contain the string ""Successfully assigned batch-pods"": [events.log](https://github.com/hail-is/hail/files/3350320/events.log). There's a lot of issue with secrets getting mounted and a couple container creation failures, but nothing that obviously suggests a problem with reading logs.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6545
https://github.com/hail-is/hail/issues/6545:2548,Integrability,message,message,2548,"ad_namespaced_pod_log_with_http_info; collection_formats=collection_formats); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 334, in call_api; _return_http_data_only, collection_formats, _preload_content, _request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 168, in __call_api; _request_timeout=_request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 355, in request; headers=headers); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 231, in GET; query_params=query_params); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 222, in request; raise ApiException(http_resp=r); {""levelname"": ""INFO"", ""asctime"": ""2019-07-02 13:36:45,525"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""mark_complete:542"", ""message"": ""no logs for batch-278-job-6858-5879db due to previous error, rescheduling pod Error: (404)\nReason: Not Found\nHTTP response headers: HTTPHeaderDict({'Audit-Id': '891f2153-6a94-42ff-8fe1-edf644051234', 'Content-Type': 'application/json', 'Date': 'Tue, 02 Jul 2019 13:36:45 GMT', 'Content-Length': '218'})\nHTTP response body: {\""kind\"":\""Status\"",\""apiVersion\"":\""v1\"",\""metadata\"":{},\""status\"":\""Failure\"",\""message\"":\""pods \\\""batch-278-job-6858-5879db\\\"" not found\"",\""reason\"":\""NotFound\"",\""details\"":{\""name\"":\""batch-278-job-6858-5879db\"",\""kind\"":\""pods\""},\""code\"":404}\n\n""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-02 13:36:45,541"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""set_state:457"", ""message"": ""job (278, 6858, 'main') changed state: Running -> Ready""}; ```. Here are events that don't contain the string ""Successfully assigned batch-pods"": [events.log](https://github.com/hail-is/hail/files/3350320/events.log). There's a lot of issue with secrets getting mounted and a couple container creation failures, but nothing that obviously suggests a problem with reading logs.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6545
https://github.com/hail-is/hail/issues/6545:2848,Integrability,message,message,2848,"ad_namespaced_pod_log_with_http_info; collection_formats=collection_formats); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 334, in call_api; _return_http_data_only, collection_formats, _preload_content, _request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 168, in __call_api; _request_timeout=_request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 355, in request; headers=headers); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 231, in GET; query_params=query_params); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 222, in request; raise ApiException(http_resp=r); {""levelname"": ""INFO"", ""asctime"": ""2019-07-02 13:36:45,525"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""mark_complete:542"", ""message"": ""no logs for batch-278-job-6858-5879db due to previous error, rescheduling pod Error: (404)\nReason: Not Found\nHTTP response headers: HTTPHeaderDict({'Audit-Id': '891f2153-6a94-42ff-8fe1-edf644051234', 'Content-Type': 'application/json', 'Date': 'Tue, 02 Jul 2019 13:36:45 GMT', 'Content-Length': '218'})\nHTTP response body: {\""kind\"":\""Status\"",\""apiVersion\"":\""v1\"",\""metadata\"":{},\""status\"":\""Failure\"",\""message\"":\""pods \\\""batch-278-job-6858-5879db\\\"" not found\"",\""reason\"":\""NotFound\"",\""details\"":{\""name\"":\""batch-278-job-6858-5879db\"",\""kind\"":\""pods\""},\""code\"":404}\n\n""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-02 13:36:45,541"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""set_state:457"", ""message"": ""job (278, 6858, 'main') changed state: Running -> Ready""}; ```. Here are events that don't contain the string ""Successfully assigned batch-pods"": [events.log](https://github.com/hail-is/hail/files/3350320/events.log). There's a lot of issue with secrets getting mounted and a couple container creation failures, but nothing that obviously suggests a problem with reading logs.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6545
https://github.com/hail-is/hail/issues/6545:695,Performance,concurren,concurrent,695,"The errors look like this:; ```; {""levelname"": ""INFO"", ""asctime"": ""2019-07-02 13:36:45,504"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1074"", ""message"": ""update job (278, 6858, 'main') with pod batch-278-job-6858-5879db""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-02 13:36:45,504"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1087"", ""message"": ""job (278, 6858, 'main') mark complete""}; File ""/usr/local/lib/python3.6/dist-packages/batch/k8s.py"", line 65, in wrapped; **kwargs),; File ""/usr/local/lib/python3.6/dist-packages/batch/blocking_to_async.py"", line 6, in blocking_to_async; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/lib/python3.6/concurrent/futures/thread.py"", line 56, in run; result = self.fn(*self.args, **self.kwargs); File ""/usr/local/lib/python3.6/dist-packages/batch/blocking_to_async.py"", line 6, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/apis/core_v1_api.py"", line 18538, in read_namespaced_pod_log; (data) = self.read_namespaced_pod_log_with_http_info(name, namespace, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/apis/core_v1_api.py"", line 18644, in read_namespaced_pod_log_with_http_info; collection_formats=collection_formats); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 334, in call_api; _return_http_data_only, collection_formats, _preload_content, _request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 168, in __call_api; _request_timeout=_request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 355, in request; headers=headers); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 231, in GET; query_params=query_params); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 222, in request; raise ApiException(http_resp=r);",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6545
https://github.com/hail-is/hail/issues/6545:2289,Security,Audit,Audit-Id,2289,"ad_namespaced_pod_log_with_http_info; collection_formats=collection_formats); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 334, in call_api; _return_http_data_only, collection_formats, _preload_content, _request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 168, in __call_api; _request_timeout=_request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 355, in request; headers=headers); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 231, in GET; query_params=query_params); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 222, in request; raise ApiException(http_resp=r); {""levelname"": ""INFO"", ""asctime"": ""2019-07-02 13:36:45,525"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""mark_complete:542"", ""message"": ""no logs for batch-278-job-6858-5879db due to previous error, rescheduling pod Error: (404)\nReason: Not Found\nHTTP response headers: HTTPHeaderDict({'Audit-Id': '891f2153-6a94-42ff-8fe1-edf644051234', 'Content-Type': 'application/json', 'Date': 'Tue, 02 Jul 2019 13:36:45 GMT', 'Content-Length': '218'})\nHTTP response body: {\""kind\"":\""Status\"",\""apiVersion\"":\""v1\"",\""metadata\"":{},\""status\"":\""Failure\"",\""message\"":\""pods \\\""batch-278-job-6858-5879db\\\"" not found\"",\""reason\"":\""NotFound\"",\""details\"":{\""name\"":\""batch-278-job-6858-5879db\"",\""kind\"":\""pods\""},\""code\"":404}\n\n""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-02 13:36:45,541"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""set_state:457"", ""message"": ""job (278, 6858, 'main') changed state: Running -> Ready""}; ```. Here are events that don't contain the string ""Successfully assigned batch-pods"": [events.log](https://github.com/hail-is/hail/files/3350320/events.log). There's a lot of issue with secrets getting mounted and a couple container creation failures, but nothing that obviously suggests a problem with reading logs.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6545
https://github.com/hail-is/hail/issues/6545:2141,Testability,log,logs,2141,"ad_namespaced_pod_log_with_http_info; collection_formats=collection_formats); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 334, in call_api; _return_http_data_only, collection_formats, _preload_content, _request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 168, in __call_api; _request_timeout=_request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 355, in request; headers=headers); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 231, in GET; query_params=query_params); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 222, in request; raise ApiException(http_resp=r); {""levelname"": ""INFO"", ""asctime"": ""2019-07-02 13:36:45,525"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""mark_complete:542"", ""message"": ""no logs for batch-278-job-6858-5879db due to previous error, rescheduling pod Error: (404)\nReason: Not Found\nHTTP response headers: HTTPHeaderDict({'Audit-Id': '891f2153-6a94-42ff-8fe1-edf644051234', 'Content-Type': 'application/json', 'Date': 'Tue, 02 Jul 2019 13:36:45 GMT', 'Content-Length': '218'})\nHTTP response body: {\""kind\"":\""Status\"",\""apiVersion\"":\""v1\"",\""metadata\"":{},\""status\"":\""Failure\"",\""message\"":\""pods \\\""batch-278-job-6858-5879db\\\"" not found\"",\""reason\"":\""NotFound\"",\""details\"":{\""name\"":\""batch-278-job-6858-5879db\"",\""kind\"":\""pods\""},\""code\"":404}\n\n""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-02 13:36:45,541"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""set_state:457"", ""message"": ""job (278, 6858, 'main') changed state: Running -> Ready""}; ```. Here are events that don't contain the string ""Successfully assigned batch-pods"": [events.log](https://github.com/hail-is/hail/files/3350320/events.log). There's a lot of issue with secrets getting mounted and a couple container creation failures, but nothing that obviously suggests a problem with reading logs.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6545
https://github.com/hail-is/hail/issues/6545:3013,Testability,log,log,3013,"ad_namespaced_pod_log_with_http_info; collection_formats=collection_formats); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 334, in call_api; _return_http_data_only, collection_formats, _preload_content, _request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 168, in __call_api; _request_timeout=_request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 355, in request; headers=headers); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 231, in GET; query_params=query_params); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 222, in request; raise ApiException(http_resp=r); {""levelname"": ""INFO"", ""asctime"": ""2019-07-02 13:36:45,525"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""mark_complete:542"", ""message"": ""no logs for batch-278-job-6858-5879db due to previous error, rescheduling pod Error: (404)\nReason: Not Found\nHTTP response headers: HTTPHeaderDict({'Audit-Id': '891f2153-6a94-42ff-8fe1-edf644051234', 'Content-Type': 'application/json', 'Date': 'Tue, 02 Jul 2019 13:36:45 GMT', 'Content-Length': '218'})\nHTTP response body: {\""kind\"":\""Status\"",\""apiVersion\"":\""v1\"",\""metadata\"":{},\""status\"":\""Failure\"",\""message\"":\""pods \\\""batch-278-job-6858-5879db\\\"" not found\"",\""reason\"":\""NotFound\"",\""details\"":{\""name\"":\""batch-278-job-6858-5879db\"",\""kind\"":\""pods\""},\""code\"":404}\n\n""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-02 13:36:45,541"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""set_state:457"", ""message"": ""job (278, 6858, 'main') changed state: Running -> Ready""}; ```. Here are events that don't contain the string ""Successfully assigned batch-pods"": [events.log](https://github.com/hail-is/hail/files/3350320/events.log). There's a lot of issue with secrets getting mounted and a couple container creation failures, but nothing that obviously suggests a problem with reading logs.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6545
https://github.com/hail-is/hail/issues/6545:3071,Testability,log,log,3071,"ad_namespaced_pod_log_with_http_info; collection_formats=collection_formats); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 334, in call_api; _return_http_data_only, collection_formats, _preload_content, _request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 168, in __call_api; _request_timeout=_request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 355, in request; headers=headers); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 231, in GET; query_params=query_params); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 222, in request; raise ApiException(http_resp=r); {""levelname"": ""INFO"", ""asctime"": ""2019-07-02 13:36:45,525"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""mark_complete:542"", ""message"": ""no logs for batch-278-job-6858-5879db due to previous error, rescheduling pod Error: (404)\nReason: Not Found\nHTTP response headers: HTTPHeaderDict({'Audit-Id': '891f2153-6a94-42ff-8fe1-edf644051234', 'Content-Type': 'application/json', 'Date': 'Tue, 02 Jul 2019 13:36:45 GMT', 'Content-Length': '218'})\nHTTP response body: {\""kind\"":\""Status\"",\""apiVersion\"":\""v1\"",\""metadata\"":{},\""status\"":\""Failure\"",\""message\"":\""pods \\\""batch-278-job-6858-5879db\\\"" not found\"",\""reason\"":\""NotFound\"",\""details\"":{\""name\"":\""batch-278-job-6858-5879db\"",\""kind\"":\""pods\""},\""code\"":404}\n\n""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-02 13:36:45,541"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""set_state:457"", ""message"": ""job (278, 6858, 'main') changed state: Running -> Ready""}; ```. Here are events that don't contain the string ""Successfully assigned batch-pods"": [events.log](https://github.com/hail-is/hail/files/3350320/events.log). There's a lot of issue with secrets getting mounted and a couple container creation failures, but nothing that obviously suggests a problem with reading logs.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6545
https://github.com/hail-is/hail/issues/6545:3230,Testability,log,logs,3230,"ad_namespaced_pod_log_with_http_info; collection_formats=collection_formats); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 334, in call_api; _return_http_data_only, collection_formats, _preload_content, _request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 168, in __call_api; _request_timeout=_request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 355, in request; headers=headers); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 231, in GET; query_params=query_params); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 222, in request; raise ApiException(http_resp=r); {""levelname"": ""INFO"", ""asctime"": ""2019-07-02 13:36:45,525"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""mark_complete:542"", ""message"": ""no logs for batch-278-job-6858-5879db due to previous error, rescheduling pod Error: (404)\nReason: Not Found\nHTTP response headers: HTTPHeaderDict({'Audit-Id': '891f2153-6a94-42ff-8fe1-edf644051234', 'Content-Type': 'application/json', 'Date': 'Tue, 02 Jul 2019 13:36:45 GMT', 'Content-Length': '218'})\nHTTP response body: {\""kind\"":\""Status\"",\""apiVersion\"":\""v1\"",\""metadata\"":{},\""status\"":\""Failure\"",\""message\"":\""pods \\\""batch-278-job-6858-5879db\\\"" not found\"",\""reason\"":\""NotFound\"",\""details\"":{\""name\"":\""batch-278-job-6858-5879db\"",\""kind\"":\""pods\""},\""code\"":404}\n\n""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-02 13:36:45,541"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""set_state:457"", ""message"": ""job (278, 6858, 'main') changed state: Running -> Ready""}; ```. Here are events that don't contain the string ""Successfully assigned batch-pods"": [events.log](https://github.com/hail-is/hail/files/3350320/events.log). There's a lot of issue with secrets getting mounted and a couple container creation failures, but nothing that obviously suggests a problem with reading logs.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6545
https://github.com/hail-is/hail/issues/6546:279,Safety,timeout,timeout,279,"During a test that created 30,000 pods a number of pods timed out waiting for `gsa-key` or `default-token-8h99c` to mount. Example:; ```; 9m13s Warning FailedMount Pod Unable to mount volumes for pod ""batch-278-job-10258-a49a81_batch-pods(82ea5910-9ccb-11e9-ad88-42010a800049)"": timeout expired waiting for volumes to attach or mount for pod ""batch-pods""/""batch-278-job-10258-a49a81"". list of unmounted volumes=[gsa-key default-token-8h99c]. list of unattached volumes=[gsa-key default-token-8h99c]; ```. All events not containing the string ""Successfully created batch-pods"" [events.log](https://github.com/hail-is/hail/files/3350369/events.log)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6546
https://github.com/hail-is/hail/issues/6546:9,Testability,test,test,9,"During a test that created 30,000 pods a number of pods timed out waiting for `gsa-key` or `default-token-8h99c` to mount. Example:; ```; 9m13s Warning FailedMount Pod Unable to mount volumes for pod ""batch-278-job-10258-a49a81_batch-pods(82ea5910-9ccb-11e9-ad88-42010a800049)"": timeout expired waiting for volumes to attach or mount for pod ""batch-pods""/""batch-278-job-10258-a49a81"". list of unmounted volumes=[gsa-key default-token-8h99c]. list of unattached volumes=[gsa-key default-token-8h99c]; ```. All events not containing the string ""Successfully created batch-pods"" [events.log](https://github.com/hail-is/hail/files/3350369/events.log)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6546
https://github.com/hail-is/hail/issues/6546:584,Testability,log,log,584,"During a test that created 30,000 pods a number of pods timed out waiting for `gsa-key` or `default-token-8h99c` to mount. Example:; ```; 9m13s Warning FailedMount Pod Unable to mount volumes for pod ""batch-278-job-10258-a49a81_batch-pods(82ea5910-9ccb-11e9-ad88-42010a800049)"": timeout expired waiting for volumes to attach or mount for pod ""batch-pods""/""batch-278-job-10258-a49a81"". list of unmounted volumes=[gsa-key default-token-8h99c]. list of unattached volumes=[gsa-key default-token-8h99c]; ```. All events not containing the string ""Successfully created batch-pods"" [events.log](https://github.com/hail-is/hail/files/3350369/events.log)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6546
https://github.com/hail-is/hail/issues/6546:642,Testability,log,log,642,"During a test that created 30,000 pods a number of pods timed out waiting for `gsa-key` or `default-token-8h99c` to mount. Example:; ```; 9m13s Warning FailedMount Pod Unable to mount volumes for pod ""batch-278-job-10258-a49a81_batch-pods(82ea5910-9ccb-11e9-ad88-42010a800049)"": timeout expired waiting for volumes to attach or mount for pod ""batch-pods""/""batch-278-job-10258-a49a81"". list of unmounted volumes=[gsa-key default-token-8h99c]. list of unattached volumes=[gsa-key default-token-8h99c]; ```. All events not containing the string ""Successfully created batch-pods"" [events.log](https://github.com/hail-is/hail/files/3350369/events.log)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6546
https://github.com/hail-is/hail/issues/6547:523,Availability,failure,failure,523,"This test:. ```python3; p = Pipeline(backend=BatchBackend('https://batch.hail.is')); for _ in range(30000):; p.new_task().command('/bin/true'); p.run(); ```. Revealed a number of issues:; daniel king: Problems Found:; - [x] https://github.com/hail-is/hail/issues/6543 mysql can deadlock itself, requiring you to reissue the db request; - [x] https://github.com/hail-is/hail/issues/6545 of the 20760 pods that were successfully created before #6543 happened, about 800 could not get their logs due to not existing. That's a failure rate of ~4%. The number of failures continues to grow as I type this message (now up to 1280). I'm counting failures this way:; ```; k logs -l app=batch --tail=999999 | grep 'no logs for ' | sed -E 's/^.*no logs for ([^ ]+).*$/\1/' | sort -u | wc -l; ```; - the k8s request latency spiked to 3.47s max 0.6 s mean during this test and stayed elevated for 10 minutes.; - [ ] https://github.com/hail-is/hail/issues/6546 there was a lot of volume mount failures due to, apparently, the secrets, e.g.:; ```; 9m13s Warning FailedMount Pod Unable to mount volumes for pod ""batch-278-job-10258-a49a81_batch-pods(82ea5910-9ccb-11e9-ad88-42010a800049)"": timeout expired waiting for volumes to attach or mount for pod ""batch-pods""/""batch-278-job-10258-a49a81"". list of unmounted volumes=[gsa-key default-token-8h99c]. list of unattached volumes=[gsa-key default-token-8h99c]; ```; - [ ] https://github.com/hail-is/hail/issues/6548 batch takes 4 seconds to render the batch page with 20k jobs (the web browser displays it fine though), e.g. https://batch.hail.is/batches/278; - [ ] https://github.com/hail-is/hail/issues/6548 batch UI search is DOA with 20k jobs; - [ ] https://github.com/hail-is/hail/issues/6556 delete (and likely cancel) will timeout on large batches",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6547
https://github.com/hail-is/hail/issues/6547:558,Availability,failure,failures,558,"This test:. ```python3; p = Pipeline(backend=BatchBackend('https://batch.hail.is')); for _ in range(30000):; p.new_task().command('/bin/true'); p.run(); ```. Revealed a number of issues:; daniel king: Problems Found:; - [x] https://github.com/hail-is/hail/issues/6543 mysql can deadlock itself, requiring you to reissue the db request; - [x] https://github.com/hail-is/hail/issues/6545 of the 20760 pods that were successfully created before #6543 happened, about 800 could not get their logs due to not existing. That's a failure rate of ~4%. The number of failures continues to grow as I type this message (now up to 1280). I'm counting failures this way:; ```; k logs -l app=batch --tail=999999 | grep 'no logs for ' | sed -E 's/^.*no logs for ([^ ]+).*$/\1/' | sort -u | wc -l; ```; - the k8s request latency spiked to 3.47s max 0.6 s mean during this test and stayed elevated for 10 minutes.; - [ ] https://github.com/hail-is/hail/issues/6546 there was a lot of volume mount failures due to, apparently, the secrets, e.g.:; ```; 9m13s Warning FailedMount Pod Unable to mount volumes for pod ""batch-278-job-10258-a49a81_batch-pods(82ea5910-9ccb-11e9-ad88-42010a800049)"": timeout expired waiting for volumes to attach or mount for pod ""batch-pods""/""batch-278-job-10258-a49a81"". list of unmounted volumes=[gsa-key default-token-8h99c]. list of unattached volumes=[gsa-key default-token-8h99c]; ```; - [ ] https://github.com/hail-is/hail/issues/6548 batch takes 4 seconds to render the batch page with 20k jobs (the web browser displays it fine though), e.g. https://batch.hail.is/batches/278; - [ ] https://github.com/hail-is/hail/issues/6548 batch UI search is DOA with 20k jobs; - [ ] https://github.com/hail-is/hail/issues/6556 delete (and likely cancel) will timeout on large batches",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6547
https://github.com/hail-is/hail/issues/6547:639,Availability,failure,failures,639,"This test:. ```python3; p = Pipeline(backend=BatchBackend('https://batch.hail.is')); for _ in range(30000):; p.new_task().command('/bin/true'); p.run(); ```. Revealed a number of issues:; daniel king: Problems Found:; - [x] https://github.com/hail-is/hail/issues/6543 mysql can deadlock itself, requiring you to reissue the db request; - [x] https://github.com/hail-is/hail/issues/6545 of the 20760 pods that were successfully created before #6543 happened, about 800 could not get their logs due to not existing. That's a failure rate of ~4%. The number of failures continues to grow as I type this message (now up to 1280). I'm counting failures this way:; ```; k logs -l app=batch --tail=999999 | grep 'no logs for ' | sed -E 's/^.*no logs for ([^ ]+).*$/\1/' | sort -u | wc -l; ```; - the k8s request latency spiked to 3.47s max 0.6 s mean during this test and stayed elevated for 10 minutes.; - [ ] https://github.com/hail-is/hail/issues/6546 there was a lot of volume mount failures due to, apparently, the secrets, e.g.:; ```; 9m13s Warning FailedMount Pod Unable to mount volumes for pod ""batch-278-job-10258-a49a81_batch-pods(82ea5910-9ccb-11e9-ad88-42010a800049)"": timeout expired waiting for volumes to attach or mount for pod ""batch-pods""/""batch-278-job-10258-a49a81"". list of unmounted volumes=[gsa-key default-token-8h99c]. list of unattached volumes=[gsa-key default-token-8h99c]; ```; - [ ] https://github.com/hail-is/hail/issues/6548 batch takes 4 seconds to render the batch page with 20k jobs (the web browser displays it fine though), e.g. https://batch.hail.is/batches/278; - [ ] https://github.com/hail-is/hail/issues/6548 batch UI search is DOA with 20k jobs; - [ ] https://github.com/hail-is/hail/issues/6556 delete (and likely cancel) will timeout on large batches",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6547
https://github.com/hail-is/hail/issues/6547:980,Availability,failure,failures,980,"This test:. ```python3; p = Pipeline(backend=BatchBackend('https://batch.hail.is')); for _ in range(30000):; p.new_task().command('/bin/true'); p.run(); ```. Revealed a number of issues:; daniel king: Problems Found:; - [x] https://github.com/hail-is/hail/issues/6543 mysql can deadlock itself, requiring you to reissue the db request; - [x] https://github.com/hail-is/hail/issues/6545 of the 20760 pods that were successfully created before #6543 happened, about 800 could not get their logs due to not existing. That's a failure rate of ~4%. The number of failures continues to grow as I type this message (now up to 1280). I'm counting failures this way:; ```; k logs -l app=batch --tail=999999 | grep 'no logs for ' | sed -E 's/^.*no logs for ([^ ]+).*$/\1/' | sort -u | wc -l; ```; - the k8s request latency spiked to 3.47s max 0.6 s mean during this test and stayed elevated for 10 minutes.; - [ ] https://github.com/hail-is/hail/issues/6546 there was a lot of volume mount failures due to, apparently, the secrets, e.g.:; ```; 9m13s Warning FailedMount Pod Unable to mount volumes for pod ""batch-278-job-10258-a49a81_batch-pods(82ea5910-9ccb-11e9-ad88-42010a800049)"": timeout expired waiting for volumes to attach or mount for pod ""batch-pods""/""batch-278-job-10258-a49a81"". list of unmounted volumes=[gsa-key default-token-8h99c]. list of unattached volumes=[gsa-key default-token-8h99c]; ```; - [ ] https://github.com/hail-is/hail/issues/6548 batch takes 4 seconds to render the batch page with 20k jobs (the web browser displays it fine though), e.g. https://batch.hail.is/batches/278; - [ ] https://github.com/hail-is/hail/issues/6548 batch UI search is DOA with 20k jobs; - [ ] https://github.com/hail-is/hail/issues/6556 delete (and likely cancel) will timeout on large batches",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6547
https://github.com/hail-is/hail/issues/6547:28,Deployability,Pipeline,Pipeline,28,"This test:. ```python3; p = Pipeline(backend=BatchBackend('https://batch.hail.is')); for _ in range(30000):; p.new_task().command('/bin/true'); p.run(); ```. Revealed a number of issues:; daniel king: Problems Found:; - [x] https://github.com/hail-is/hail/issues/6543 mysql can deadlock itself, requiring you to reissue the db request; - [x] https://github.com/hail-is/hail/issues/6545 of the 20760 pods that were successfully created before #6543 happened, about 800 could not get their logs due to not existing. That's a failure rate of ~4%. The number of failures continues to grow as I type this message (now up to 1280). I'm counting failures this way:; ```; k logs -l app=batch --tail=999999 | grep 'no logs for ' | sed -E 's/^.*no logs for ([^ ]+).*$/\1/' | sort -u | wc -l; ```; - the k8s request latency spiked to 3.47s max 0.6 s mean during this test and stayed elevated for 10 minutes.; - [ ] https://github.com/hail-is/hail/issues/6546 there was a lot of volume mount failures due to, apparently, the secrets, e.g.:; ```; 9m13s Warning FailedMount Pod Unable to mount volumes for pod ""batch-278-job-10258-a49a81_batch-pods(82ea5910-9ccb-11e9-ad88-42010a800049)"": timeout expired waiting for volumes to attach or mount for pod ""batch-pods""/""batch-278-job-10258-a49a81"". list of unmounted volumes=[gsa-key default-token-8h99c]. list of unattached volumes=[gsa-key default-token-8h99c]; ```; - [ ] https://github.com/hail-is/hail/issues/6548 batch takes 4 seconds to render the batch page with 20k jobs (the web browser displays it fine though), e.g. https://batch.hail.is/batches/278; - [ ] https://github.com/hail-is/hail/issues/6548 batch UI search is DOA with 20k jobs; - [ ] https://github.com/hail-is/hail/issues/6556 delete (and likely cancel) will timeout on large batches",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6547
https://github.com/hail-is/hail/issues/6547:600,Integrability,message,message,600,"This test:. ```python3; p = Pipeline(backend=BatchBackend('https://batch.hail.is')); for _ in range(30000):; p.new_task().command('/bin/true'); p.run(); ```. Revealed a number of issues:; daniel king: Problems Found:; - [x] https://github.com/hail-is/hail/issues/6543 mysql can deadlock itself, requiring you to reissue the db request; - [x] https://github.com/hail-is/hail/issues/6545 of the 20760 pods that were successfully created before #6543 happened, about 800 could not get their logs due to not existing. That's a failure rate of ~4%. The number of failures continues to grow as I type this message (now up to 1280). I'm counting failures this way:; ```; k logs -l app=batch --tail=999999 | grep 'no logs for ' | sed -E 's/^.*no logs for ([^ ]+).*$/\1/' | sort -u | wc -l; ```; - the k8s request latency spiked to 3.47s max 0.6 s mean during this test and stayed elevated for 10 minutes.; - [ ] https://github.com/hail-is/hail/issues/6546 there was a lot of volume mount failures due to, apparently, the secrets, e.g.:; ```; 9m13s Warning FailedMount Pod Unable to mount volumes for pod ""batch-278-job-10258-a49a81_batch-pods(82ea5910-9ccb-11e9-ad88-42010a800049)"": timeout expired waiting for volumes to attach or mount for pod ""batch-pods""/""batch-278-job-10258-a49a81"". list of unmounted volumes=[gsa-key default-token-8h99c]. list of unattached volumes=[gsa-key default-token-8h99c]; ```; - [ ] https://github.com/hail-is/hail/issues/6548 batch takes 4 seconds to render the batch page with 20k jobs (the web browser displays it fine though), e.g. https://batch.hail.is/batches/278; - [ ] https://github.com/hail-is/hail/issues/6548 batch UI search is DOA with 20k jobs; - [ ] https://github.com/hail-is/hail/issues/6556 delete (and likely cancel) will timeout on large batches",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6547
https://github.com/hail-is/hail/issues/6547:805,Performance,latency,latency,805,"This test:. ```python3; p = Pipeline(backend=BatchBackend('https://batch.hail.is')); for _ in range(30000):; p.new_task().command('/bin/true'); p.run(); ```. Revealed a number of issues:; daniel king: Problems Found:; - [x] https://github.com/hail-is/hail/issues/6543 mysql can deadlock itself, requiring you to reissue the db request; - [x] https://github.com/hail-is/hail/issues/6545 of the 20760 pods that were successfully created before #6543 happened, about 800 could not get their logs due to not existing. That's a failure rate of ~4%. The number of failures continues to grow as I type this message (now up to 1280). I'm counting failures this way:; ```; k logs -l app=batch --tail=999999 | grep 'no logs for ' | sed -E 's/^.*no logs for ([^ ]+).*$/\1/' | sort -u | wc -l; ```; - the k8s request latency spiked to 3.47s max 0.6 s mean during this test and stayed elevated for 10 minutes.; - [ ] https://github.com/hail-is/hail/issues/6546 there was a lot of volume mount failures due to, apparently, the secrets, e.g.:; ```; 9m13s Warning FailedMount Pod Unable to mount volumes for pod ""batch-278-job-10258-a49a81_batch-pods(82ea5910-9ccb-11e9-ad88-42010a800049)"": timeout expired waiting for volumes to attach or mount for pod ""batch-pods""/""batch-278-job-10258-a49a81"". list of unmounted volumes=[gsa-key default-token-8h99c]. list of unattached volumes=[gsa-key default-token-8h99c]; ```; - [ ] https://github.com/hail-is/hail/issues/6548 batch takes 4 seconds to render the batch page with 20k jobs (the web browser displays it fine though), e.g. https://batch.hail.is/batches/278; - [ ] https://github.com/hail-is/hail/issues/6548 batch UI search is DOA with 20k jobs; - [ ] https://github.com/hail-is/hail/issues/6556 delete (and likely cancel) will timeout on large batches",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6547
https://github.com/hail-is/hail/issues/6547:1175,Safety,timeout,timeout,1175,"This test:. ```python3; p = Pipeline(backend=BatchBackend('https://batch.hail.is')); for _ in range(30000):; p.new_task().command('/bin/true'); p.run(); ```. Revealed a number of issues:; daniel king: Problems Found:; - [x] https://github.com/hail-is/hail/issues/6543 mysql can deadlock itself, requiring you to reissue the db request; - [x] https://github.com/hail-is/hail/issues/6545 of the 20760 pods that were successfully created before #6543 happened, about 800 could not get their logs due to not existing. That's a failure rate of ~4%. The number of failures continues to grow as I type this message (now up to 1280). I'm counting failures this way:; ```; k logs -l app=batch --tail=999999 | grep 'no logs for ' | sed -E 's/^.*no logs for ([^ ]+).*$/\1/' | sort -u | wc -l; ```; - the k8s request latency spiked to 3.47s max 0.6 s mean during this test and stayed elevated for 10 minutes.; - [ ] https://github.com/hail-is/hail/issues/6546 there was a lot of volume mount failures due to, apparently, the secrets, e.g.:; ```; 9m13s Warning FailedMount Pod Unable to mount volumes for pod ""batch-278-job-10258-a49a81_batch-pods(82ea5910-9ccb-11e9-ad88-42010a800049)"": timeout expired waiting for volumes to attach or mount for pod ""batch-pods""/""batch-278-job-10258-a49a81"". list of unmounted volumes=[gsa-key default-token-8h99c]. list of unattached volumes=[gsa-key default-token-8h99c]; ```; - [ ] https://github.com/hail-is/hail/issues/6548 batch takes 4 seconds to render the batch page with 20k jobs (the web browser displays it fine though), e.g. https://batch.hail.is/batches/278; - [ ] https://github.com/hail-is/hail/issues/6548 batch UI search is DOA with 20k jobs; - [ ] https://github.com/hail-is/hail/issues/6556 delete (and likely cancel) will timeout on large batches",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6547
https://github.com/hail-is/hail/issues/6547:1765,Safety,timeout,timeout,1765,"This test:. ```python3; p = Pipeline(backend=BatchBackend('https://batch.hail.is')); for _ in range(30000):; p.new_task().command('/bin/true'); p.run(); ```. Revealed a number of issues:; daniel king: Problems Found:; - [x] https://github.com/hail-is/hail/issues/6543 mysql can deadlock itself, requiring you to reissue the db request; - [x] https://github.com/hail-is/hail/issues/6545 of the 20760 pods that were successfully created before #6543 happened, about 800 could not get their logs due to not existing. That's a failure rate of ~4%. The number of failures continues to grow as I type this message (now up to 1280). I'm counting failures this way:; ```; k logs -l app=batch --tail=999999 | grep 'no logs for ' | sed -E 's/^.*no logs for ([^ ]+).*$/\1/' | sort -u | wc -l; ```; - the k8s request latency spiked to 3.47s max 0.6 s mean during this test and stayed elevated for 10 minutes.; - [ ] https://github.com/hail-is/hail/issues/6546 there was a lot of volume mount failures due to, apparently, the secrets, e.g.:; ```; 9m13s Warning FailedMount Pod Unable to mount volumes for pod ""batch-278-job-10258-a49a81_batch-pods(82ea5910-9ccb-11e9-ad88-42010a800049)"": timeout expired waiting for volumes to attach or mount for pod ""batch-pods""/""batch-278-job-10258-a49a81"". list of unmounted volumes=[gsa-key default-token-8h99c]. list of unattached volumes=[gsa-key default-token-8h99c]; ```; - [ ] https://github.com/hail-is/hail/issues/6548 batch takes 4 seconds to render the batch page with 20k jobs (the web browser displays it fine though), e.g. https://batch.hail.is/batches/278; - [ ] https://github.com/hail-is/hail/issues/6548 batch UI search is DOA with 20k jobs; - [ ] https://github.com/hail-is/hail/issues/6556 delete (and likely cancel) will timeout on large batches",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6547
https://github.com/hail-is/hail/issues/6547:5,Testability,test,test,5,"This test:. ```python3; p = Pipeline(backend=BatchBackend('https://batch.hail.is')); for _ in range(30000):; p.new_task().command('/bin/true'); p.run(); ```. Revealed a number of issues:; daniel king: Problems Found:; - [x] https://github.com/hail-is/hail/issues/6543 mysql can deadlock itself, requiring you to reissue the db request; - [x] https://github.com/hail-is/hail/issues/6545 of the 20760 pods that were successfully created before #6543 happened, about 800 could not get their logs due to not existing. That's a failure rate of ~4%. The number of failures continues to grow as I type this message (now up to 1280). I'm counting failures this way:; ```; k logs -l app=batch --tail=999999 | grep 'no logs for ' | sed -E 's/^.*no logs for ([^ ]+).*$/\1/' | sort -u | wc -l; ```; - the k8s request latency spiked to 3.47s max 0.6 s mean during this test and stayed elevated for 10 minutes.; - [ ] https://github.com/hail-is/hail/issues/6546 there was a lot of volume mount failures due to, apparently, the secrets, e.g.:; ```; 9m13s Warning FailedMount Pod Unable to mount volumes for pod ""batch-278-job-10258-a49a81_batch-pods(82ea5910-9ccb-11e9-ad88-42010a800049)"": timeout expired waiting for volumes to attach or mount for pod ""batch-pods""/""batch-278-job-10258-a49a81"". list of unmounted volumes=[gsa-key default-token-8h99c]. list of unattached volumes=[gsa-key default-token-8h99c]; ```; - [ ] https://github.com/hail-is/hail/issues/6548 batch takes 4 seconds to render the batch page with 20k jobs (the web browser displays it fine though), e.g. https://batch.hail.is/batches/278; - [ ] https://github.com/hail-is/hail/issues/6548 batch UI search is DOA with 20k jobs; - [ ] https://github.com/hail-is/hail/issues/6556 delete (and likely cancel) will timeout on large batches",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6547
https://github.com/hail-is/hail/issues/6547:488,Testability,log,logs,488,"This test:. ```python3; p = Pipeline(backend=BatchBackend('https://batch.hail.is')); for _ in range(30000):; p.new_task().command('/bin/true'); p.run(); ```. Revealed a number of issues:; daniel king: Problems Found:; - [x] https://github.com/hail-is/hail/issues/6543 mysql can deadlock itself, requiring you to reissue the db request; - [x] https://github.com/hail-is/hail/issues/6545 of the 20760 pods that were successfully created before #6543 happened, about 800 could not get their logs due to not existing. That's a failure rate of ~4%. The number of failures continues to grow as I type this message (now up to 1280). I'm counting failures this way:; ```; k logs -l app=batch --tail=999999 | grep 'no logs for ' | sed -E 's/^.*no logs for ([^ ]+).*$/\1/' | sort -u | wc -l; ```; - the k8s request latency spiked to 3.47s max 0.6 s mean during this test and stayed elevated for 10 minutes.; - [ ] https://github.com/hail-is/hail/issues/6546 there was a lot of volume mount failures due to, apparently, the secrets, e.g.:; ```; 9m13s Warning FailedMount Pod Unable to mount volumes for pod ""batch-278-job-10258-a49a81_batch-pods(82ea5910-9ccb-11e9-ad88-42010a800049)"": timeout expired waiting for volumes to attach or mount for pod ""batch-pods""/""batch-278-job-10258-a49a81"". list of unmounted volumes=[gsa-key default-token-8h99c]. list of unattached volumes=[gsa-key default-token-8h99c]; ```; - [ ] https://github.com/hail-is/hail/issues/6548 batch takes 4 seconds to render the batch page with 20k jobs (the web browser displays it fine though), e.g. https://batch.hail.is/batches/278; - [ ] https://github.com/hail-is/hail/issues/6548 batch UI search is DOA with 20k jobs; - [ ] https://github.com/hail-is/hail/issues/6556 delete (and likely cancel) will timeout on large batches",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6547
https://github.com/hail-is/hail/issues/6547:666,Testability,log,logs,666,"This test:. ```python3; p = Pipeline(backend=BatchBackend('https://batch.hail.is')); for _ in range(30000):; p.new_task().command('/bin/true'); p.run(); ```. Revealed a number of issues:; daniel king: Problems Found:; - [x] https://github.com/hail-is/hail/issues/6543 mysql can deadlock itself, requiring you to reissue the db request; - [x] https://github.com/hail-is/hail/issues/6545 of the 20760 pods that were successfully created before #6543 happened, about 800 could not get their logs due to not existing. That's a failure rate of ~4%. The number of failures continues to grow as I type this message (now up to 1280). I'm counting failures this way:; ```; k logs -l app=batch --tail=999999 | grep 'no logs for ' | sed -E 's/^.*no logs for ([^ ]+).*$/\1/' | sort -u | wc -l; ```; - the k8s request latency spiked to 3.47s max 0.6 s mean during this test and stayed elevated for 10 minutes.; - [ ] https://github.com/hail-is/hail/issues/6546 there was a lot of volume mount failures due to, apparently, the secrets, e.g.:; ```; 9m13s Warning FailedMount Pod Unable to mount volumes for pod ""batch-278-job-10258-a49a81_batch-pods(82ea5910-9ccb-11e9-ad88-42010a800049)"": timeout expired waiting for volumes to attach or mount for pod ""batch-pods""/""batch-278-job-10258-a49a81"". list of unmounted volumes=[gsa-key default-token-8h99c]. list of unattached volumes=[gsa-key default-token-8h99c]; ```; - [ ] https://github.com/hail-is/hail/issues/6548 batch takes 4 seconds to render the batch page with 20k jobs (the web browser displays it fine though), e.g. https://batch.hail.is/batches/278; - [ ] https://github.com/hail-is/hail/issues/6548 batch UI search is DOA with 20k jobs; - [ ] https://github.com/hail-is/hail/issues/6556 delete (and likely cancel) will timeout on large batches",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6547
https://github.com/hail-is/hail/issues/6547:709,Testability,log,logs,709,"This test:. ```python3; p = Pipeline(backend=BatchBackend('https://batch.hail.is')); for _ in range(30000):; p.new_task().command('/bin/true'); p.run(); ```. Revealed a number of issues:; daniel king: Problems Found:; - [x] https://github.com/hail-is/hail/issues/6543 mysql can deadlock itself, requiring you to reissue the db request; - [x] https://github.com/hail-is/hail/issues/6545 of the 20760 pods that were successfully created before #6543 happened, about 800 could not get their logs due to not existing. That's a failure rate of ~4%. The number of failures continues to grow as I type this message (now up to 1280). I'm counting failures this way:; ```; k logs -l app=batch --tail=999999 | grep 'no logs for ' | sed -E 's/^.*no logs for ([^ ]+).*$/\1/' | sort -u | wc -l; ```; - the k8s request latency spiked to 3.47s max 0.6 s mean during this test and stayed elevated for 10 minutes.; - [ ] https://github.com/hail-is/hail/issues/6546 there was a lot of volume mount failures due to, apparently, the secrets, e.g.:; ```; 9m13s Warning FailedMount Pod Unable to mount volumes for pod ""batch-278-job-10258-a49a81_batch-pods(82ea5910-9ccb-11e9-ad88-42010a800049)"": timeout expired waiting for volumes to attach or mount for pod ""batch-pods""/""batch-278-job-10258-a49a81"". list of unmounted volumes=[gsa-key default-token-8h99c]. list of unattached volumes=[gsa-key default-token-8h99c]; ```; - [ ] https://github.com/hail-is/hail/issues/6548 batch takes 4 seconds to render the batch page with 20k jobs (the web browser displays it fine though), e.g. https://batch.hail.is/batches/278; - [ ] https://github.com/hail-is/hail/issues/6548 batch UI search is DOA with 20k jobs; - [ ] https://github.com/hail-is/hail/issues/6556 delete (and likely cancel) will timeout on large batches",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6547
https://github.com/hail-is/hail/issues/6547:738,Testability,log,logs,738,"This test:. ```python3; p = Pipeline(backend=BatchBackend('https://batch.hail.is')); for _ in range(30000):; p.new_task().command('/bin/true'); p.run(); ```. Revealed a number of issues:; daniel king: Problems Found:; - [x] https://github.com/hail-is/hail/issues/6543 mysql can deadlock itself, requiring you to reissue the db request; - [x] https://github.com/hail-is/hail/issues/6545 of the 20760 pods that were successfully created before #6543 happened, about 800 could not get their logs due to not existing. That's a failure rate of ~4%. The number of failures continues to grow as I type this message (now up to 1280). I'm counting failures this way:; ```; k logs -l app=batch --tail=999999 | grep 'no logs for ' | sed -E 's/^.*no logs for ([^ ]+).*$/\1/' | sort -u | wc -l; ```; - the k8s request latency spiked to 3.47s max 0.6 s mean during this test and stayed elevated for 10 minutes.; - [ ] https://github.com/hail-is/hail/issues/6546 there was a lot of volume mount failures due to, apparently, the secrets, e.g.:; ```; 9m13s Warning FailedMount Pod Unable to mount volumes for pod ""batch-278-job-10258-a49a81_batch-pods(82ea5910-9ccb-11e9-ad88-42010a800049)"": timeout expired waiting for volumes to attach or mount for pod ""batch-pods""/""batch-278-job-10258-a49a81"". list of unmounted volumes=[gsa-key default-token-8h99c]. list of unattached volumes=[gsa-key default-token-8h99c]; ```; - [ ] https://github.com/hail-is/hail/issues/6548 batch takes 4 seconds to render the batch page with 20k jobs (the web browser displays it fine though), e.g. https://batch.hail.is/batches/278; - [ ] https://github.com/hail-is/hail/issues/6548 batch UI search is DOA with 20k jobs; - [ ] https://github.com/hail-is/hail/issues/6556 delete (and likely cancel) will timeout on large batches",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6547
https://github.com/hail-is/hail/issues/6547:856,Testability,test,test,856,"This test:. ```python3; p = Pipeline(backend=BatchBackend('https://batch.hail.is')); for _ in range(30000):; p.new_task().command('/bin/true'); p.run(); ```. Revealed a number of issues:; daniel king: Problems Found:; - [x] https://github.com/hail-is/hail/issues/6543 mysql can deadlock itself, requiring you to reissue the db request; - [x] https://github.com/hail-is/hail/issues/6545 of the 20760 pods that were successfully created before #6543 happened, about 800 could not get their logs due to not existing. That's a failure rate of ~4%. The number of failures continues to grow as I type this message (now up to 1280). I'm counting failures this way:; ```; k logs -l app=batch --tail=999999 | grep 'no logs for ' | sed -E 's/^.*no logs for ([^ ]+).*$/\1/' | sort -u | wc -l; ```; - the k8s request latency spiked to 3.47s max 0.6 s mean during this test and stayed elevated for 10 minutes.; - [ ] https://github.com/hail-is/hail/issues/6546 there was a lot of volume mount failures due to, apparently, the secrets, e.g.:; ```; 9m13s Warning FailedMount Pod Unable to mount volumes for pod ""batch-278-job-10258-a49a81_batch-pods(82ea5910-9ccb-11e9-ad88-42010a800049)"": timeout expired waiting for volumes to attach or mount for pod ""batch-pods""/""batch-278-job-10258-a49a81"". list of unmounted volumes=[gsa-key default-token-8h99c]. list of unattached volumes=[gsa-key default-token-8h99c]; ```; - [ ] https://github.com/hail-is/hail/issues/6548 batch takes 4 seconds to render the batch page with 20k jobs (the web browser displays it fine though), e.g. https://batch.hail.is/batches/278; - [ ] https://github.com/hail-is/hail/issues/6548 batch UI search is DOA with 20k jobs; - [ ] https://github.com/hail-is/hail/issues/6556 delete (and likely cancel) will timeout on large batches",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6547
https://github.com/hail-is/hail/pull/6557:128,Availability,error,error,128,- provide a default session; - made cancel not wait for all jobs to be cancelled before returning; - some log statements; - fix error catching in retry function; - add retry to batch client create jobs; - add cancel if batch is not submitted successfully,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6557
https://github.com/hail-is/hail/pull/6557:106,Testability,log,log,106,- provide a default session; - made cancel not wait for all jobs to be cancelled before returning; - some log statements; - fix error catching in retry function; - add retry to batch client create jobs; - add cancel if batch is not submitted successfully,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6557
https://github.com/hail-is/hail/pull/6559:0,Testability,Test,Tested,0,Tested and working.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6559
https://github.com/hail-is/hail/pull/6561:213,Deployability,install,install,213,"This works better with my environment and it means that, e.g., Brandon, only needs Java and python set up correctly to run: `git clone ... && cd hail/hail && make pytest`. No environment variables, no packages to install.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6561
https://github.com/hail-is/hail/pull/6561:187,Modifiability,variab,variables,187,"This works better with my environment and it means that, e.g., Brandon, only needs Java and python set up correctly to run: `git clone ... && cd hail/hail && make pytest`. No environment variables, no packages to install.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6561
https://github.com/hail-is/hail/pull/6563:94,Testability,test,test,94,"Created db.py in the experimental folder. In addition, also created a test_experimental.py to test the function as well.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6563
https://github.com/hail-is/hail/issues/6565:160,Availability,ERROR,ERROR,160,"Christina reports https://hail.zulipchat.com/#narrow/stream/123010-Hail-0.2E2.20support/topic/initialization.20action.20failed.20in.20starting.20cluster:; ```; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [d0a8142009bf49a1a51f5276576aeddb] failed with error:; Google Cloud Dataproc Agent reports job failure. If logs are available, they can be found in 'gs://dataproc-c3e3c3c1-4a54-41e4-aa06-83d5d2ce80ec-us/google-cloud-dataproc-metainfo/c174dc73-2817-4ec1-8c2d-ae4e0c4f91ae/jobs/d0a8142009bf49a1a51f5276576aeddb/driveroutput'.; Traceback (most recent call last):; File ""/Users/cchen/anaconda/envs/hail-env/bin/hailctl"", line 10, in <module>; sys.exit(main()); File ""/Users/cchen/anaconda/envs/hail-env/lib/python3.7/site-packages/hailctl/__main__.py"", line 90, in main; module(args); File ""/Users/cchen/anaconda/envs/hail-env/lib/python3.7/site-packages/hailctl/dataproc/cli.py"", line 99, in main; jmp[args.module].main(args, pass_through_args); File ""/Users/cchen/anaconda/envs/hail-env/lib/python3.7/site-packages/hailctl/dataproc/submit.py"", line 72, in main; check_call(cmd); File ""/Users/cchen/anaconda/envs/hail-env/lib/python3.7/subprocess.py"", line 347, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '['gcloud', 'dataproc', 'jobs', 'submit', 'pyspark', 'ukbb_hdpca.py', '--cluster=chen', '--files=', '--py-files=/var/folders/6h/ll2dv8t15zs9pzf4g6kjb2rrt2fc9q/T/pyscripts_2740r0cj.zip', '--properties=']' returned non-zero exit status 1.; ```; The file does not exist but there are files with the same prefix but a `.000000001` suffix or similar. Grace reports (a possibly unrelated issue) https://hail.zulipchat.com/#narrow/stream/123010-Hail-0.2E2.20support/topic/Cryptic.20array.20concordance.20error:; ```; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [873db5659acd43f7b539dcb17182959d] entered state [ERROR] while waiting for [DONE].; Traceback (most recent call last):; File ""/miniconda3/bin/hailctl"", line 10, in <module>; sys.ex",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6565
https://github.com/hail-is/hail/issues/6565:256,Availability,error,error,256,"Christina reports https://hail.zulipchat.com/#narrow/stream/123010-Hail-0.2E2.20support/topic/initialization.20action.20failed.20in.20starting.20cluster:; ```; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [d0a8142009bf49a1a51f5276576aeddb] failed with error:; Google Cloud Dataproc Agent reports job failure. If logs are available, they can be found in 'gs://dataproc-c3e3c3c1-4a54-41e4-aa06-83d5d2ce80ec-us/google-cloud-dataproc-metainfo/c174dc73-2817-4ec1-8c2d-ae4e0c4f91ae/jobs/d0a8142009bf49a1a51f5276576aeddb/driveroutput'.; Traceback (most recent call last):; File ""/Users/cchen/anaconda/envs/hail-env/bin/hailctl"", line 10, in <module>; sys.exit(main()); File ""/Users/cchen/anaconda/envs/hail-env/lib/python3.7/site-packages/hailctl/__main__.py"", line 90, in main; module(args); File ""/Users/cchen/anaconda/envs/hail-env/lib/python3.7/site-packages/hailctl/dataproc/cli.py"", line 99, in main; jmp[args.module].main(args, pass_through_args); File ""/Users/cchen/anaconda/envs/hail-env/lib/python3.7/site-packages/hailctl/dataproc/submit.py"", line 72, in main; check_call(cmd); File ""/Users/cchen/anaconda/envs/hail-env/lib/python3.7/subprocess.py"", line 347, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '['gcloud', 'dataproc', 'jobs', 'submit', 'pyspark', 'ukbb_hdpca.py', '--cluster=chen', '--files=', '--py-files=/var/folders/6h/ll2dv8t15zs9pzf4g6kjb2rrt2fc9q/T/pyscripts_2740r0cj.zip', '--properties=']' returned non-zero exit status 1.; ```; The file does not exist but there are files with the same prefix but a `.000000001` suffix or similar. Grace reports (a possibly unrelated issue) https://hail.zulipchat.com/#narrow/stream/123010-Hail-0.2E2.20support/topic/Cryptic.20array.20concordance.20error:; ```; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [873db5659acd43f7b539dcb17182959d] entered state [ERROR] while waiting for [DONE].; Traceback (most recent call last):; File ""/miniconda3/bin/hailctl"", line 10, in <module>; sys.ex",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6565
https://github.com/hail-is/hail/issues/6565:304,Availability,failure,failure,304,"Christina reports https://hail.zulipchat.com/#narrow/stream/123010-Hail-0.2E2.20support/topic/initialization.20action.20failed.20in.20starting.20cluster:; ```; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [d0a8142009bf49a1a51f5276576aeddb] failed with error:; Google Cloud Dataproc Agent reports job failure. If logs are available, they can be found in 'gs://dataproc-c3e3c3c1-4a54-41e4-aa06-83d5d2ce80ec-us/google-cloud-dataproc-metainfo/c174dc73-2817-4ec1-8c2d-ae4e0c4f91ae/jobs/d0a8142009bf49a1a51f5276576aeddb/driveroutput'.; Traceback (most recent call last):; File ""/Users/cchen/anaconda/envs/hail-env/bin/hailctl"", line 10, in <module>; sys.exit(main()); File ""/Users/cchen/anaconda/envs/hail-env/lib/python3.7/site-packages/hailctl/__main__.py"", line 90, in main; module(args); File ""/Users/cchen/anaconda/envs/hail-env/lib/python3.7/site-packages/hailctl/dataproc/cli.py"", line 99, in main; jmp[args.module].main(args, pass_through_args); File ""/Users/cchen/anaconda/envs/hail-env/lib/python3.7/site-packages/hailctl/dataproc/submit.py"", line 72, in main; check_call(cmd); File ""/Users/cchen/anaconda/envs/hail-env/lib/python3.7/subprocess.py"", line 347, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '['gcloud', 'dataproc', 'jobs', 'submit', 'pyspark', 'ukbb_hdpca.py', '--cluster=chen', '--files=', '--py-files=/var/folders/6h/ll2dv8t15zs9pzf4g6kjb2rrt2fc9q/T/pyscripts_2740r0cj.zip', '--properties=']' returned non-zero exit status 1.; ```; The file does not exist but there are files with the same prefix but a `.000000001` suffix or similar. Grace reports (a possibly unrelated issue) https://hail.zulipchat.com/#narrow/stream/123010-Hail-0.2E2.20support/topic/Cryptic.20array.20concordance.20error:; ```; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [873db5659acd43f7b539dcb17182959d] entered state [ERROR] while waiting for [DONE].; Traceback (most recent call last):; File ""/miniconda3/bin/hailctl"", line 10, in <module>; sys.ex",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6565
https://github.com/hail-is/hail/issues/6565:325,Availability,avail,available,325,"Christina reports https://hail.zulipchat.com/#narrow/stream/123010-Hail-0.2E2.20support/topic/initialization.20action.20failed.20in.20starting.20cluster:; ```; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [d0a8142009bf49a1a51f5276576aeddb] failed with error:; Google Cloud Dataproc Agent reports job failure. If logs are available, they can be found in 'gs://dataproc-c3e3c3c1-4a54-41e4-aa06-83d5d2ce80ec-us/google-cloud-dataproc-metainfo/c174dc73-2817-4ec1-8c2d-ae4e0c4f91ae/jobs/d0a8142009bf49a1a51f5276576aeddb/driveroutput'.; Traceback (most recent call last):; File ""/Users/cchen/anaconda/envs/hail-env/bin/hailctl"", line 10, in <module>; sys.exit(main()); File ""/Users/cchen/anaconda/envs/hail-env/lib/python3.7/site-packages/hailctl/__main__.py"", line 90, in main; module(args); File ""/Users/cchen/anaconda/envs/hail-env/lib/python3.7/site-packages/hailctl/dataproc/cli.py"", line 99, in main; jmp[args.module].main(args, pass_through_args); File ""/Users/cchen/anaconda/envs/hail-env/lib/python3.7/site-packages/hailctl/dataproc/submit.py"", line 72, in main; check_call(cmd); File ""/Users/cchen/anaconda/envs/hail-env/lib/python3.7/subprocess.py"", line 347, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '['gcloud', 'dataproc', 'jobs', 'submit', 'pyspark', 'ukbb_hdpca.py', '--cluster=chen', '--files=', '--py-files=/var/folders/6h/ll2dv8t15zs9pzf4g6kjb2rrt2fc9q/T/pyscripts_2740r0cj.zip', '--properties=']' returned non-zero exit status 1.; ```; The file does not exist but there are files with the same prefix but a `.000000001` suffix or similar. Grace reports (a possibly unrelated issue) https://hail.zulipchat.com/#narrow/stream/123010-Hail-0.2E2.20support/topic/Cryptic.20array.20concordance.20error:; ```; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [873db5659acd43f7b539dcb17182959d] entered state [ERROR] while waiting for [DONE].; Traceback (most recent call last):; File ""/miniconda3/bin/hailctl"", line 10, in <module>; sys.ex",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6565
https://github.com/hail-is/hail/issues/6565:1772,Availability,ERROR,ERROR,1772,"in; module(args); File ""/Users/cchen/anaconda/envs/hail-env/lib/python3.7/site-packages/hailctl/dataproc/cli.py"", line 99, in main; jmp[args.module].main(args, pass_through_args); File ""/Users/cchen/anaconda/envs/hail-env/lib/python3.7/site-packages/hailctl/dataproc/submit.py"", line 72, in main; check_call(cmd); File ""/Users/cchen/anaconda/envs/hail-env/lib/python3.7/subprocess.py"", line 347, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '['gcloud', 'dataproc', 'jobs', 'submit', 'pyspark', 'ukbb_hdpca.py', '--cluster=chen', '--files=', '--py-files=/var/folders/6h/ll2dv8t15zs9pzf4g6kjb2rrt2fc9q/T/pyscripts_2740r0cj.zip', '--properties=']' returned non-zero exit status 1.; ```; The file does not exist but there are files with the same prefix but a `.000000001` suffix or similar. Grace reports (a possibly unrelated issue) https://hail.zulipchat.com/#narrow/stream/123010-Hail-0.2E2.20support/topic/Cryptic.20array.20concordance.20error:; ```; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [873db5659acd43f7b539dcb17182959d] entered state [ERROR] while waiting for [DONE].; Traceback (most recent call last):; File ""/miniconda3/bin/hailctl"", line 10, in <module>; sys.exit(main()); File ""/miniconda3/lib/python3.7/site-packages/hailctl/__main__.py"", line 90, in main; module(args); File ""/miniconda3/lib/python3.7/site-packages/hailctl/dataproc/cli.py"", line 99, in main; jmp[args.module].main(args, pass_through_args); File ""/miniconda3/lib/python3.7/site-packages/hailctl/dataproc/submit.py"", line 72, in main; check_call(cmd); File ""/miniconda3/lib/python3.7/subprocess.py"", line 341, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '['gcloud', 'dataproc', 'jobs', 'submit', 'pyspark', '/Users/gtiao/ukbb_qc/scratch.py', '--cluster=gt1', '--files=', '--py-files=/var/folders/rn/t2xcx1ps4h96txll46qkkfsj2q8bnl/T/pyscripts_lh2k36v4.zip', '--properties=', '--', '--slack_channel', '@grace']' ret",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6565
https://github.com/hail-is/hail/issues/6565:1871,Availability,ERROR,ERROR,1871,"en/anaconda/envs/hail-env/lib/python3.7/site-packages/hailctl/dataproc/cli.py"", line 99, in main; jmp[args.module].main(args, pass_through_args); File ""/Users/cchen/anaconda/envs/hail-env/lib/python3.7/site-packages/hailctl/dataproc/submit.py"", line 72, in main; check_call(cmd); File ""/Users/cchen/anaconda/envs/hail-env/lib/python3.7/subprocess.py"", line 347, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '['gcloud', 'dataproc', 'jobs', 'submit', 'pyspark', 'ukbb_hdpca.py', '--cluster=chen', '--files=', '--py-files=/var/folders/6h/ll2dv8t15zs9pzf4g6kjb2rrt2fc9q/T/pyscripts_2740r0cj.zip', '--properties=']' returned non-zero exit status 1.; ```; The file does not exist but there are files with the same prefix but a `.000000001` suffix or similar. Grace reports (a possibly unrelated issue) https://hail.zulipchat.com/#narrow/stream/123010-Hail-0.2E2.20support/topic/Cryptic.20array.20concordance.20error:; ```; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [873db5659acd43f7b539dcb17182959d] entered state [ERROR] while waiting for [DONE].; Traceback (most recent call last):; File ""/miniconda3/bin/hailctl"", line 10, in <module>; sys.exit(main()); File ""/miniconda3/lib/python3.7/site-packages/hailctl/__main__.py"", line 90, in main; module(args); File ""/miniconda3/lib/python3.7/site-packages/hailctl/dataproc/cli.py"", line 99, in main; jmp[args.module].main(args, pass_through_args); File ""/miniconda3/lib/python3.7/site-packages/hailctl/dataproc/submit.py"", line 72, in main; check_call(cmd); File ""/miniconda3/lib/python3.7/subprocess.py"", line 341, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '['gcloud', 'dataproc', 'jobs', 'submit', 'pyspark', '/Users/gtiao/ukbb_qc/scratch.py', '--cluster=gt1', '--files=', '--py-files=/var/folders/rn/t2xcx1ps4h96txll46qkkfsj2q8bnl/T/pyscripts_lh2k36v4.zip', '--properties=', '--', '--slack_channel', '@grace']' returned non-zero exit status 1.; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6565
https://github.com/hail-is/hail/issues/6565:316,Testability,log,logs,316,"Christina reports https://hail.zulipchat.com/#narrow/stream/123010-Hail-0.2E2.20support/topic/initialization.20action.20failed.20in.20starting.20cluster:; ```; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [d0a8142009bf49a1a51f5276576aeddb] failed with error:; Google Cloud Dataproc Agent reports job failure. If logs are available, they can be found in 'gs://dataproc-c3e3c3c1-4a54-41e4-aa06-83d5d2ce80ec-us/google-cloud-dataproc-metainfo/c174dc73-2817-4ec1-8c2d-ae4e0c4f91ae/jobs/d0a8142009bf49a1a51f5276576aeddb/driveroutput'.; Traceback (most recent call last):; File ""/Users/cchen/anaconda/envs/hail-env/bin/hailctl"", line 10, in <module>; sys.exit(main()); File ""/Users/cchen/anaconda/envs/hail-env/lib/python3.7/site-packages/hailctl/__main__.py"", line 90, in main; module(args); File ""/Users/cchen/anaconda/envs/hail-env/lib/python3.7/site-packages/hailctl/dataproc/cli.py"", line 99, in main; jmp[args.module].main(args, pass_through_args); File ""/Users/cchen/anaconda/envs/hail-env/lib/python3.7/site-packages/hailctl/dataproc/submit.py"", line 72, in main; check_call(cmd); File ""/Users/cchen/anaconda/envs/hail-env/lib/python3.7/subprocess.py"", line 347, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '['gcloud', 'dataproc', 'jobs', 'submit', 'pyspark', 'ukbb_hdpca.py', '--cluster=chen', '--files=', '--py-files=/var/folders/6h/ll2dv8t15zs9pzf4g6kjb2rrt2fc9q/T/pyscripts_2740r0cj.zip', '--properties=']' returned non-zero exit status 1.; ```; The file does not exist but there are files with the same prefix but a `.000000001` suffix or similar. Grace reports (a possibly unrelated issue) https://hail.zulipchat.com/#narrow/stream/123010-Hail-0.2E2.20support/topic/Cryptic.20array.20concordance.20error:; ```; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [873db5659acd43f7b539dcb17182959d] entered state [ERROR] while waiting for [DONE].; Traceback (most recent call last):; File ""/miniconda3/bin/hailctl"", line 10, in <module>; sys.ex",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6565
https://github.com/hail-is/hail/issues/6566:51,Testability,test,test,51,This issue tracks the results of the 30k pod scale test 2,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6566
https://github.com/hail-is/hail/pull/6570:12,Testability,test,test,12,"Did a local test of prometheus_async on a tiny aiohttp server, turns out the order of the decorators does matter. The timing decorator has to be on the bottom to actually time anything successfully. Does the order of any of the other decorators matter?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6570
https://github.com/hail-is/hail/pull/6572:49,Security,expose,expose,49,"I added support for order by as well, but didn't expose it yet. We can get to that later when we add sorting to tables.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6572
https://github.com/hail-is/hail/pull/6573:5,Deployability,deploy,deployed,5,"Hand deployed (currently running). Only visible change is to router, add proxy rules from ukbb-hail.is to the ukbb-rg servers. The web site has two parts: static HTML served by nginx and a interactive, data-driven Shiny site run by R/shiny/shiny server. Servers run as stateful sets. Static HTML and ; data for Shiny were hand-populated. Currently giving them each one core. Given how much state is involved here, it's not clear how to autoscale this like we do with the other services.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6573
https://github.com/hail-is/hail/pull/6573:61,Integrability,rout,router,61,"Hand deployed (currently running). Only visible change is to router, add proxy rules from ukbb-hail.is to the ukbb-rg servers. The web site has two parts: static HTML served by nginx and a interactive, data-driven Shiny site run by R/shiny/shiny server. Servers run as stateful sets. Static HTML and ; data for Shiny were hand-populated. Currently giving them each one core. Given how much state is involved here, it's not clear how to autoscale this like we do with the other services.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6573
https://github.com/hail-is/hail/pull/6573:423,Usability,clear,clear,423,"Hand deployed (currently running). Only visible change is to router, add proxy rules from ukbb-hail.is to the ukbb-rg servers. The web site has two parts: static HTML served by nginx and a interactive, data-driven Shiny site run by R/shiny/shiny server. Servers run as stateful sets. Static HTML and ; data for Shiny were hand-populated. Currently giving them each one core. Given how much state is involved here, it's not clear how to autoscale this like we do with the other services.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6573
https://github.com/hail-is/hail/pull/6578:484,Deployability,update,update,484,"I think this is the direction we need to head with Batch. Mapping DB things to objects and then using recursive functions over the graph is not going to scale. This uses one database call to:; - set the state of every non-always-run, incomplete job in the given batch to `Cancelled`; - move the state from `Running` to `Pending` for every; - job whose parents all succeeded, and every; - always-run job whose parents all completed; - get a list of every `Ready` or `Cancelled` job; - update the batch to cancelled and closed. Then uses a loop to delete k8s resources and create pods, as appropriate for the given job. I think we can go further! We should make our k8s requests in parallel (I don't see anyway to delete a *list* of jobs in k8s [only to delete a whole namespace]) and we should avoid retrieving every column from the database. We only need a few things to `create_pod` or `delete_k8s_resources`. cc: @johnc1231",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6578
https://github.com/hail-is/hail/pull/6578:793,Safety,avoid,avoid,793,"I think this is the direction we need to head with Batch. Mapping DB things to objects and then using recursive functions over the graph is not going to scale. This uses one database call to:; - set the state of every non-always-run, incomplete job in the given batch to `Cancelled`; - move the state from `Running` to `Pending` for every; - job whose parents all succeeded, and every; - always-run job whose parents all completed; - get a list of every `Ready` or `Cancelled` job; - update the batch to cancelled and closed. Then uses a loop to delete k8s resources and create pods, as appropriate for the given job. I think we can go further! We should make our k8s requests in parallel (I don't see anyway to delete a *list* of jobs in k8s [only to delete a whole namespace]) and we should avoid retrieving every column from the database. We only need a few things to `create_pod` or `delete_k8s_resources`. cc: @johnc1231",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6578
https://github.com/hail-is/hail/pull/6579:261,Deployability,deploy,deploying,261,"Merge the Nealelab/hail-datasets repository into mainline hail. This is the set of scripts that are used to generate the datasets for the Datasets API. Next steps are to clean up these scripts (there's something called `old/`?) and to figure out a strategy for deploying new datasets when they're merged. This PR just literally merges the master branch from Nealelab/hail-datasets into hail-is/hail. cc: @liameabbott, thanks!, @GreatBrando",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6579
https://github.com/hail-is/hail/pull/6580:173,Deployability,pipeline,pipeline,173,"This is basically #6534 except with some bugfixes. Stacked on #6535. I opened a new PR because I created this branch to do some benchmarking against @chrisvittal's combiner pipeline; running `summarize` on the mt he provided (which I believe is chr22 and 10 samples wide) yields about the same runtime as the unstaged version, currently---about 30 seconds on my laptop to read/summarize/write. @patrick-schultz @tpoterba @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6580
https://github.com/hail-is/hail/pull/6580:128,Testability,benchmark,benchmarking,128,"This is basically #6534 except with some bugfixes. Stacked on #6535. I opened a new PR because I created this branch to do some benchmarking against @chrisvittal's combiner pipeline; running `summarize` on the mt he provided (which I believe is chr22 and 10 samples wide) yields about the same runtime as the unstaged version, currently---about 30 seconds on my laptop to read/summarize/write. @patrick-schultz @tpoterba @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6580
https://github.com/hail-is/hail/issues/6582:137,Testability,test,test,137,"CI needs a way to retry a PR without pushing a new commit. # Previous Discussion; I manually deleted the batch for #6561 this morning to test how CI would respond. I thought this bit would heal the situation appropriately:. https://github.com/hail-is/hail/blob/6a725fec7eca9357866b6d3ca7d89f3fef3d5deb/ci/ci/github.py#L395-L410. However, it's not working. Randomly assigned @johnc1231",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6582
https://github.com/hail-is/hail/issues/6587:5,Availability,ERROR,ERROR,5,"```; ERROR	| 2019-07-09 09:52:15,823 	| web_protocol.py 	| log_exception:355 | Error handling request; Jul 9, 2019 @ 05:52:15.824; Traceback (most recent call last):; Jul 9, 2019 @ 05:52:15.824; File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_protocol.py"", line 418, in start; Jul 9, 2019 @ 05:52:15.824; resp = await task; Jul 9, 2019 @ 05:52:15.824; File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_app.py"", line 458, in _handle; Jul 9, 2019 @ 05:52:15.824; resp = await handler(request); Jul 9, 2019 @ 05:52:15.824; File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_urldispatcher.py"", line 157, in handler_wrapper; Jul 9, 2019 @ 05:52:15.824; result = await result; Jul 9, 2019 @ 05:52:15.824; File ""/usr/local/lib/python3.6/dist-packages/aiohttp_jinja2/__init__.py"", line 91, in wrapped; Jul 9, 2019 @ 05:52:15.824; context = await coro(*args); Jul 9, 2019 @ 05:52:15.824; File ""/ci/ci.py"", line 118, in get_pr; Jul 9, 2019 @ 05:52:15.824; status = await pr.batch.status(); Jul 9, 2019 @ 05:52:15.824; File ""/usr/local/lib/python3.6/dist-packages/hailtop/batch_client/aioclient.py"", line 209, in status; Jul 9, 2019 @ 05:52:15.824; return await self._client._get(f'/api/v1alpha/batches/{self.id}'); Jul 9, 2019 @ 05:52:15.824; File ""/usr/local/lib/python3.6/dist-packages/hailtop/batch_client/aioclient.py"", line 412, in _get; Jul 9, 2019 @ 05:52:15.824; self.url + path, params=params, cookies=self._cookies, headers=self._headers); Jul 9, 2019 @ 05:52:15.824; File ""/usr/local/lib/python3.6/dist-packages/aiohttp/client.py"", line 581, in _request; Jul 9, 2019 @ 05:52:15.824; resp.raise_for_status(); Jul 9, 2019 @ 05:52:15.824; File ""/usr/local/lib/python3.6/dist-packages/aiohttp/client_reqrep.py"", line 942, in raise_for_status; Jul 9, 2019 @ 05:52:15.824; headers=self.headers); Jul 9, 2019 @ 05:52:15.824; aiohttp.client_exceptions.ClientResponseError: 404, message='Not Found'; Jul 9, 2019 @ 05:52:15.824; INFO	| 2019-07-09 09:52:15,824 	| web_log.py 	| log:233 | 1",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6587
https://github.com/hail-is/hail/issues/6587:79,Availability,Error,Error,79,"```; ERROR	| 2019-07-09 09:52:15,823 	| web_protocol.py 	| log_exception:355 | Error handling request; Jul 9, 2019 @ 05:52:15.824; Traceback (most recent call last):; Jul 9, 2019 @ 05:52:15.824; File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_protocol.py"", line 418, in start; Jul 9, 2019 @ 05:52:15.824; resp = await task; Jul 9, 2019 @ 05:52:15.824; File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_app.py"", line 458, in _handle; Jul 9, 2019 @ 05:52:15.824; resp = await handler(request); Jul 9, 2019 @ 05:52:15.824; File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_urldispatcher.py"", line 157, in handler_wrapper; Jul 9, 2019 @ 05:52:15.824; result = await result; Jul 9, 2019 @ 05:52:15.824; File ""/usr/local/lib/python3.6/dist-packages/aiohttp_jinja2/__init__.py"", line 91, in wrapped; Jul 9, 2019 @ 05:52:15.824; context = await coro(*args); Jul 9, 2019 @ 05:52:15.824; File ""/ci/ci.py"", line 118, in get_pr; Jul 9, 2019 @ 05:52:15.824; status = await pr.batch.status(); Jul 9, 2019 @ 05:52:15.824; File ""/usr/local/lib/python3.6/dist-packages/hailtop/batch_client/aioclient.py"", line 209, in status; Jul 9, 2019 @ 05:52:15.824; return await self._client._get(f'/api/v1alpha/batches/{self.id}'); Jul 9, 2019 @ 05:52:15.824; File ""/usr/local/lib/python3.6/dist-packages/hailtop/batch_client/aioclient.py"", line 412, in _get; Jul 9, 2019 @ 05:52:15.824; self.url + path, params=params, cookies=self._cookies, headers=self._headers); Jul 9, 2019 @ 05:52:15.824; File ""/usr/local/lib/python3.6/dist-packages/aiohttp/client.py"", line 581, in _request; Jul 9, 2019 @ 05:52:15.824; resp.raise_for_status(); Jul 9, 2019 @ 05:52:15.824; File ""/usr/local/lib/python3.6/dist-packages/aiohttp/client_reqrep.py"", line 942, in raise_for_status; Jul 9, 2019 @ 05:52:15.824; headers=self.headers); Jul 9, 2019 @ 05:52:15.824; aiohttp.client_exceptions.ClientResponseError: 404, message='Not Found'; Jul 9, 2019 @ 05:52:15.824; INFO	| 2019-07-09 09:52:15,824 	| web_log.py 	| log:233 | 1",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6587
https://github.com/hail-is/hail/issues/6587:807,Integrability,wrap,wrapped,807,"```; ERROR	| 2019-07-09 09:52:15,823 	| web_protocol.py 	| log_exception:355 | Error handling request; Jul 9, 2019 @ 05:52:15.824; Traceback (most recent call last):; Jul 9, 2019 @ 05:52:15.824; File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_protocol.py"", line 418, in start; Jul 9, 2019 @ 05:52:15.824; resp = await task; Jul 9, 2019 @ 05:52:15.824; File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_app.py"", line 458, in _handle; Jul 9, 2019 @ 05:52:15.824; resp = await handler(request); Jul 9, 2019 @ 05:52:15.824; File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_urldispatcher.py"", line 157, in handler_wrapper; Jul 9, 2019 @ 05:52:15.824; result = await result; Jul 9, 2019 @ 05:52:15.824; File ""/usr/local/lib/python3.6/dist-packages/aiohttp_jinja2/__init__.py"", line 91, in wrapped; Jul 9, 2019 @ 05:52:15.824; context = await coro(*args); Jul 9, 2019 @ 05:52:15.824; File ""/ci/ci.py"", line 118, in get_pr; Jul 9, 2019 @ 05:52:15.824; status = await pr.batch.status(); Jul 9, 2019 @ 05:52:15.824; File ""/usr/local/lib/python3.6/dist-packages/hailtop/batch_client/aioclient.py"", line 209, in status; Jul 9, 2019 @ 05:52:15.824; return await self._client._get(f'/api/v1alpha/batches/{self.id}'); Jul 9, 2019 @ 05:52:15.824; File ""/usr/local/lib/python3.6/dist-packages/hailtop/batch_client/aioclient.py"", line 412, in _get; Jul 9, 2019 @ 05:52:15.824; self.url + path, params=params, cookies=self._cookies, headers=self._headers); Jul 9, 2019 @ 05:52:15.824; File ""/usr/local/lib/python3.6/dist-packages/aiohttp/client.py"", line 581, in _request; Jul 9, 2019 @ 05:52:15.824; resp.raise_for_status(); Jul 9, 2019 @ 05:52:15.824; File ""/usr/local/lib/python3.6/dist-packages/aiohttp/client_reqrep.py"", line 942, in raise_for_status; Jul 9, 2019 @ 05:52:15.824; headers=self.headers); Jul 9, 2019 @ 05:52:15.824; aiohttp.client_exceptions.ClientResponseError: 404, message='Not Found'; Jul 9, 2019 @ 05:52:15.824; INFO	| 2019-07-09 09:52:15,824 	| web_log.py 	| log:233 | 1",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6587
https://github.com/hail-is/hail/issues/6587:1893,Integrability,message,message,1893,"http/web_protocol.py"", line 418, in start; Jul 9, 2019 @ 05:52:15.824; resp = await task; Jul 9, 2019 @ 05:52:15.824; File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_app.py"", line 458, in _handle; Jul 9, 2019 @ 05:52:15.824; resp = await handler(request); Jul 9, 2019 @ 05:52:15.824; File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_urldispatcher.py"", line 157, in handler_wrapper; Jul 9, 2019 @ 05:52:15.824; result = await result; Jul 9, 2019 @ 05:52:15.824; File ""/usr/local/lib/python3.6/dist-packages/aiohttp_jinja2/__init__.py"", line 91, in wrapped; Jul 9, 2019 @ 05:52:15.824; context = await coro(*args); Jul 9, 2019 @ 05:52:15.824; File ""/ci/ci.py"", line 118, in get_pr; Jul 9, 2019 @ 05:52:15.824; status = await pr.batch.status(); Jul 9, 2019 @ 05:52:15.824; File ""/usr/local/lib/python3.6/dist-packages/hailtop/batch_client/aioclient.py"", line 209, in status; Jul 9, 2019 @ 05:52:15.824; return await self._client._get(f'/api/v1alpha/batches/{self.id}'); Jul 9, 2019 @ 05:52:15.824; File ""/usr/local/lib/python3.6/dist-packages/hailtop/batch_client/aioclient.py"", line 412, in _get; Jul 9, 2019 @ 05:52:15.824; self.url + path, params=params, cookies=self._cookies, headers=self._headers); Jul 9, 2019 @ 05:52:15.824; File ""/usr/local/lib/python3.6/dist-packages/aiohttp/client.py"", line 581, in _request; Jul 9, 2019 @ 05:52:15.824; resp.raise_for_status(); Jul 9, 2019 @ 05:52:15.824; File ""/usr/local/lib/python3.6/dist-packages/aiohttp/client_reqrep.py"", line 942, in raise_for_status; Jul 9, 2019 @ 05:52:15.824; headers=self.headers); Jul 9, 2019 @ 05:52:15.824; aiohttp.client_exceptions.ClientResponseError: 404, message='Not Found'; Jul 9, 2019 @ 05:52:15.824; INFO	| 2019-07-09 09:52:15,824 	| web_log.py 	| log:233 | 10.32.17.240 [09/Jul/2019:09:52:15 +0000] ""GET /watched_branches/0/pr/6561 HTTP/1.0"" 500 315 ""https://ci.hail.is/"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.100 Safari/537.36""; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6587
https://github.com/hail-is/hail/issues/6587:1990,Testability,log,log,1990,"http/web_protocol.py"", line 418, in start; Jul 9, 2019 @ 05:52:15.824; resp = await task; Jul 9, 2019 @ 05:52:15.824; File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_app.py"", line 458, in _handle; Jul 9, 2019 @ 05:52:15.824; resp = await handler(request); Jul 9, 2019 @ 05:52:15.824; File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_urldispatcher.py"", line 157, in handler_wrapper; Jul 9, 2019 @ 05:52:15.824; result = await result; Jul 9, 2019 @ 05:52:15.824; File ""/usr/local/lib/python3.6/dist-packages/aiohttp_jinja2/__init__.py"", line 91, in wrapped; Jul 9, 2019 @ 05:52:15.824; context = await coro(*args); Jul 9, 2019 @ 05:52:15.824; File ""/ci/ci.py"", line 118, in get_pr; Jul 9, 2019 @ 05:52:15.824; status = await pr.batch.status(); Jul 9, 2019 @ 05:52:15.824; File ""/usr/local/lib/python3.6/dist-packages/hailtop/batch_client/aioclient.py"", line 209, in status; Jul 9, 2019 @ 05:52:15.824; return await self._client._get(f'/api/v1alpha/batches/{self.id}'); Jul 9, 2019 @ 05:52:15.824; File ""/usr/local/lib/python3.6/dist-packages/hailtop/batch_client/aioclient.py"", line 412, in _get; Jul 9, 2019 @ 05:52:15.824; self.url + path, params=params, cookies=self._cookies, headers=self._headers); Jul 9, 2019 @ 05:52:15.824; File ""/usr/local/lib/python3.6/dist-packages/aiohttp/client.py"", line 581, in _request; Jul 9, 2019 @ 05:52:15.824; resp.raise_for_status(); Jul 9, 2019 @ 05:52:15.824; File ""/usr/local/lib/python3.6/dist-packages/aiohttp/client_reqrep.py"", line 942, in raise_for_status; Jul 9, 2019 @ 05:52:15.824; headers=self.headers); Jul 9, 2019 @ 05:52:15.824; aiohttp.client_exceptions.ClientResponseError: 404, message='Not Found'; Jul 9, 2019 @ 05:52:15.824; INFO	| 2019-07-09 09:52:15,824 	| web_log.py 	| log:233 | 10.32.17.240 [09/Jul/2019:09:52:15 +0000] ""GET /watched_branches/0/pr/6561 HTTP/1.0"" 500 315 ""https://ci.hail.is/"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.100 Safari/537.36""; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6587
https://github.com/hail-is/hail/pull/6588:35,Testability,benchmark,benchmark,35,Sample output:; ```; $ hailctl dev benchmark compare /tmp/foo.json /tmp/foo2.json; table_annotate_many_nested_no_dependence +1.063 1.396 1.313; table_aggregate_array_sum +1.059 5.648 5.330; table_big_aggregate_compile_and_execute -1.026 12.644 12.976; table_range_force_count +1.010 6.966 6.898; -------------; Total: +1.025; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6588
https://github.com/hail-is/hail/pull/6594:49,Testability,test,tests,49,"cc @tpoterba . Isn't actually called yet, and no tests. Would like feedback on direction, then will add those pieces. . A few issues, starting from top:. 1) RelationalRef. Parser.scala `type_expr` can output a TUnion, but no PUnion exists. Current plan is to add one and the corresponding canonical type; 2) More to come.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6594
https://github.com/hail-is/hail/pull/6594:67,Usability,feedback,feedback,67,"cc @tpoterba . Isn't actually called yet, and no tests. Would like feedback on direction, then will add those pieces. . A few issues, starting from top:. 1) RelationalRef. Parser.scala `type_expr` can output a TUnion, but no PUnion exists. Current plan is to add one and the corresponding canonical type; 2) More to come.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6594
https://github.com/hail-is/hail/issues/6601:0,Testability,Log,Logic,0,"Logic is: in case when data representation of a collection of fields doesn't change, and one simply needs a subset of those values, it makes no sense to copy all of those values to a different Region, instead cast. ```scala; val rvdLP = LocalLDPrune.pruneLocal(standardizedRDD, r2Threshold, windowSize, Some(maxQueueSize)). val fieldIndicesToAdd = Array(""locus"", ""alleles"", ""mean"", ""centered_length_rec""); .map(field => bpvType.fieldIdx(field)); val sitesOnly = rvdLP.mapPartitions(; tableType.canonicalRVDType; )({ it =>; val region = Region(); val rvb = new RegionValueBuilder(region); val newRV = RegionValue(region). it.map { rv =>; region.clear(); rvb.set(region); rvb.start(tableType.canonicalPType); rvb.startStruct(); // this should be a selected fields PStruct; rvb.addFields(bpvType, rv, fieldIndicesToAdd); rvb.endStruct(); newRV.setOffset(rvb.end()); newRV; }; }); ```. With something that looked more like this. ```scala; val rvdLP = LocalLDPrune.pruneLocal(standardizedRDD, r2Threshold, windowSize, Some(maxQueueSize)). val newRvdView = rvdLP.getViewFromSelectedFields(PSelectedFields(Array(""locus"", ""alleles"", ""mean"", ""centered_length_rec"".map(field => bpvType.fieldIdx(field))); ```. presumably the implementation would not only not copy, but also not re-partition the data; cc @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6601
https://github.com/hail-is/hail/issues/6601:93,Usability,simpl,simply,93,"Logic is: in case when data representation of a collection of fields doesn't change, and one simply needs a subset of those values, it makes no sense to copy all of those values to a different Region, instead cast. ```scala; val rvdLP = LocalLDPrune.pruneLocal(standardizedRDD, r2Threshold, windowSize, Some(maxQueueSize)). val fieldIndicesToAdd = Array(""locus"", ""alleles"", ""mean"", ""centered_length_rec""); .map(field => bpvType.fieldIdx(field)); val sitesOnly = rvdLP.mapPartitions(; tableType.canonicalRVDType; )({ it =>; val region = Region(); val rvb = new RegionValueBuilder(region); val newRV = RegionValue(region). it.map { rv =>; region.clear(); rvb.set(region); rvb.start(tableType.canonicalPType); rvb.startStruct(); // this should be a selected fields PStruct; rvb.addFields(bpvType, rv, fieldIndicesToAdd); rvb.endStruct(); newRV.setOffset(rvb.end()); newRV; }; }); ```. With something that looked more like this. ```scala; val rvdLP = LocalLDPrune.pruneLocal(standardizedRDD, r2Threshold, windowSize, Some(maxQueueSize)). val newRvdView = rvdLP.getViewFromSelectedFields(PSelectedFields(Array(""locus"", ""alleles"", ""mean"", ""centered_length_rec"".map(field => bpvType.fieldIdx(field))); ```. presumably the implementation would not only not copy, but also not re-partition the data; cc @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6601
https://github.com/hail-is/hail/issues/6601:644,Usability,clear,clear,644,"Logic is: in case when data representation of a collection of fields doesn't change, and one simply needs a subset of those values, it makes no sense to copy all of those values to a different Region, instead cast. ```scala; val rvdLP = LocalLDPrune.pruneLocal(standardizedRDD, r2Threshold, windowSize, Some(maxQueueSize)). val fieldIndicesToAdd = Array(""locus"", ""alleles"", ""mean"", ""centered_length_rec""); .map(field => bpvType.fieldIdx(field)); val sitesOnly = rvdLP.mapPartitions(; tableType.canonicalRVDType; )({ it =>; val region = Region(); val rvb = new RegionValueBuilder(region); val newRV = RegionValue(region). it.map { rv =>; region.clear(); rvb.set(region); rvb.start(tableType.canonicalPType); rvb.startStruct(); // this should be a selected fields PStruct; rvb.addFields(bpvType, rv, fieldIndicesToAdd); rvb.endStruct(); newRV.setOffset(rvb.end()); newRV; }; }); ```. With something that looked more like this. ```scala; val rvdLP = LocalLDPrune.pruneLocal(standardizedRDD, r2Threshold, windowSize, Some(maxQueueSize)). val newRvdView = rvdLP.getViewFromSelectedFields(PSelectedFields(Array(""locus"", ""alleles"", ""mean"", ""centered_length_rec"".map(field => bpvType.fieldIdx(field))); ```. presumably the implementation would not only not copy, but also not re-partition the data; cc @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6601
https://github.com/hail-is/hail/pull/6606:119,Testability,test,tests,119,"right now just hard-codes three different sizes for the default block size. Not used yet, although I wrote a couple of tests; I hope to pipe this through the aggregator stuff. (broken out from #6580)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6606
https://github.com/hail-is/hail/pull/6607:46,Deployability,integrat,integrate,46,"Half finished. SQL is probably wrong. Need to integrate the job filtering with the parameters into the api calls, write tests, integrate with batch_client. I'm sure there's more to do.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6607
https://github.com/hail-is/hail/pull/6607:127,Deployability,integrat,integrate,127,"Half finished. SQL is probably wrong. Need to integrate the job filtering with the parameters into the api calls, write tests, integrate with batch_client. I'm sure there's more to do.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6607
https://github.com/hail-is/hail/pull/6607:46,Integrability,integrat,integrate,46,"Half finished. SQL is probably wrong. Need to integrate the job filtering with the parameters into the api calls, write tests, integrate with batch_client. I'm sure there's more to do.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6607
https://github.com/hail-is/hail/pull/6607:127,Integrability,integrat,integrate,127,"Half finished. SQL is probably wrong. Need to integrate the job filtering with the parameters into the api calls, write tests, integrate with batch_client. I'm sure there's more to do.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6607
https://github.com/hail-is/hail/pull/6607:120,Testability,test,tests,120,"Half finished. SQL is probably wrong. Need to integrate the job filtering with the parameters into the api calls, write tests, integrate with batch_client. I'm sure there's more to do.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6607
https://github.com/hail-is/hail/pull/6608:5,Testability,test,testing,5,wip; testing,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6608
https://github.com/hail-is/hail/issues/6616:22,Availability,fault,fault,22,"This isn't really our fault. k8s sends us a 400 when a container is in a funky state. Creating this issue so I can find it again later when I run into this. Somehow a container terminates without timing information, and the read logs request returns a 400 instead of a 404. Batch handles this fine (it treats all log read failures the same). Known issue: https://github.com/kubernetes/kubernetes/issues/59296. ```; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:39,890"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (9, 1, 'main') with pod batch-9-job-1-c8b9b2""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:39,890"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1159"", ""message"": ""job (9, 1, 'main') mark complete""}; {""levelname"": ""WARNING"", ""asctime"": ""2019-07-11 14:19:39,899"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""mark_complete:579"", ""message"": ""job (9, 1, 'main') has pod batch-9-job-1-c8b9b2 which is terminated but has no timing information. {'api_version': 'v1',\n 'kind': 'Pod',\n 'metadata': {'annotations': None,\n 'cluster_name': None,\n 'creation_timestamp': datetime.datetime(2019, 7, 11, 14, 19, 34, tzinfo=tzlocal()),\n 'deletion_grace_period_seconds': 30,\n 'deletion_timestamp': datetime.datetime(2019, 7, 11, 14, 20, 4, tzinfo=tzlocal()),\n 'finalizers': None,\n 'generate_name': None,\n 'generation': None,\n 'initializers': None,\n 'labels': {'app': 'batch-job',\n 'batch_id': '9',\n 'hail.is/batch-instance': 'ffa5abc4607849df8e5f0036e7350bcf',\n 'job_id': '1',\n 'task': 'main',\n 'user': 'test',\n 'uuid': '291b9eed73b9433c86ff1f58624cf24d'},\n 'name': 'batch-9-job-1-c8b9b2',\n 'namespace': 'pr-6604-batch-pods-cjklalqnl5u9',\n 'owner_references': None,\n 'resource_version': '86681671',\n 'self_link': '/api/v1/namespaces/pr-6604-batch-pods-cjklalqnl5u9/pods/batch-9-job-1-c8b9b2',\n 'uid': 'e878f906-a3e6-11e9-a4bb-42010a8000af'},\n 'spec': {'active_deadline_seconds': None,\n 'affinity': ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6616
https://github.com/hail-is/hail/issues/6616:322,Availability,failure,failures,322,"This isn't really our fault. k8s sends us a 400 when a container is in a funky state. Creating this issue so I can find it again later when I run into this. Somehow a container terminates without timing information, and the read logs request returns a 400 instead of a 404. Batch handles this fine (it treats all log read failures the same). Known issue: https://github.com/kubernetes/kubernetes/issues/59296. ```; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:39,890"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (9, 1, 'main') with pod batch-9-job-1-c8b9b2""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:39,890"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1159"", ""message"": ""job (9, 1, 'main') mark complete""}; {""levelname"": ""WARNING"", ""asctime"": ""2019-07-11 14:19:39,899"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""mark_complete:579"", ""message"": ""job (9, 1, 'main') has pod batch-9-job-1-c8b9b2 which is terminated but has no timing information. {'api_version': 'v1',\n 'kind': 'Pod',\n 'metadata': {'annotations': None,\n 'cluster_name': None,\n 'creation_timestamp': datetime.datetime(2019, 7, 11, 14, 19, 34, tzinfo=tzlocal()),\n 'deletion_grace_period_seconds': 30,\n 'deletion_timestamp': datetime.datetime(2019, 7, 11, 14, 20, 4, tzinfo=tzlocal()),\n 'finalizers': None,\n 'generate_name': None,\n 'generation': None,\n 'initializers': None,\n 'labels': {'app': 'batch-job',\n 'batch_id': '9',\n 'hail.is/batch-instance': 'ffa5abc4607849df8e5f0036e7350bcf',\n 'job_id': '1',\n 'task': 'main',\n 'user': 'test',\n 'uuid': '291b9eed73b9433c86ff1f58624cf24d'},\n 'name': 'batch-9-job-1-c8b9b2',\n 'namespace': 'pr-6604-batch-pods-cjklalqnl5u9',\n 'owner_references': None,\n 'resource_version': '86681671',\n 'self_link': '/api/v1/namespaces/pr-6604-batch-pods-cjklalqnl5u9/pods/batch-9-job-1-c8b9b2',\n 'uid': 'e878f906-a3e6-11e9-a4bb-42010a8000af'},\n 'spec': {'active_deadline_seconds': None,\n 'affinity': ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6616
https://github.com/hail-is/hail/issues/6616:4229,Availability,toler,tolerations,4229,"ry': '500M'}},\n 'security_context': None,\n 'stdin': None,\n 'stdin_once': None,\n 'termination_message_path': '/dev/termination-log',\n 'termination_message_policy': 'File',\n 'tty': None,\n 'volume_devices': None,\n 'volume_mounts': [{'mount_path': '/gsa-key',\n 'mount_propagation': None,\n 'name': 'gsa-key',\n 'read_only': None,\n 'sub_path': None},\n {'mount_path': '/var/run/secrets/kubernetes.io/serviceaccount',\n 'mount_propagation': None,\n 'name': 'default-token-brr2f',\n 'read_only': True,\n 'sub_path': None}],\n 'working_dir': None}],\n 'dns_config': None,\n 'dns_policy': 'ClusterFirst',\n 'enable_service_links': True,\n 'host_aliases': None,\n 'host_ipc': None,\n 'host_network': None,\n 'host_pid': None,\n 'hostname': None,\n 'image_pull_secrets': None,\n 'init_containers': None,\n 'node_name': 'gke-vdc-preemptible-pool-9c7148b2-8hq5',\n 'node_selector': None,\n 'priority': 500000,\n 'priority_class_name': 'user',\n 'readiness_gates': None,\n 'restart_policy': 'Never',\n 'runtime_class_name': None,\n 'scheduler_name': 'default-scheduler',\n 'security_context': {'fs_group': None,\n 'run_as_group': None,\n 'run_as_non_root': None,\n 'run_as_user': None,\n 'se_linux_options': None,\n 'supplemental_groups': None,\n 'sysctls': None},\n 'service_account': 'default',\n 'service_account_name': 'default',\n 'share_process_namespace': None,\n 'subdomain': None,\n 'termination_grace_period_seconds': 30,\n 'tolerations': [{'effect': None,\n 'key': 'preemptible',\n 'operator': None,\n 'toleration_seconds': None,\n 'value': 'true'},\n {'effect': 'NoExecute',\n 'key': 'node.kubernetes.io/not-ready',\n 'operator': 'Exists',\n 'toleration_seconds': 300,\n 'value': None},\n {'effect': 'NoExecute',\n 'key': 'node.kubernetes.io/unreachable',\n 'operator': 'Exists',\n 'toleration_seconds': 300,\n 'value': None}],\n 'volumes': [{'aws_elastic_block_store': None,\n 'azure_disk': None,\n 'azure_file': None,\n 'cephfs': None,\n 'cinder': None,\n 'config_map': None,\n 'downward_ap",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6616
https://github.com/hail-is/hail/issues/6616:9482,Availability,error,error,9482,"ib/python3.6/concurrent/futures/thread.py"", line 56, in run; result = self.fn(*self.args, **self.kwargs); File ""/usr/local/lib/python3.6/dist-packages/batch/blocking_to_async.py"", line 6, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/apis/core_v1_api.py"", line 18538, in read_namespaced_pod_log; (data) = self.read_namespaced_pod_log_with_http_info(name, namespace, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/apis/core_v1_api.py"", line 18644, in read_namespaced_pod_log_with_http_info; collection_formats=collection_formats); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 334, in call_api; _return_http_data_only, collection_formats, _preload_content, _request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 168, in __call_api; _request_timeout=_request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 355, in request; headers=headers); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 231, in GET; query_params=query_params); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 222, in request; raise ApiException(http_resp=r); {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:39,924"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""mark_complete:591"", ""message"": ""no logs for batch-9-job-1-c8b9b2 due to previous error, rescheduling pod Error: (400)\nReason: Bad Request\nHTTP response headers: HTTPHeaderDict({'Audit-Id': '6a744afc-154f-4b48-b4bb-51a15078d999', 'Content-Type': 'application/json', 'Date': 'Thu, 11 Jul 2019 14:19:39 GMT', 'Content-Length': '179'})\nHTTP response body: {\""kind\"":\""Status\"",\""apiVersion\"":\""v1\"",\""metadata\"":{},\""status\"":\""Failure\"",\""message\"":\""container \\\""main\\\"" in pod \\\""batch-9-job-1-c8b9b2\\\"" is terminated\"",\""reason\"":\""BadRequest\"",\""code\"":400}\n\n""}; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6616
https://github.com/hail-is/hail/issues/6616:9506,Availability,Error,Error,9506,"ib/python3.6/concurrent/futures/thread.py"", line 56, in run; result = self.fn(*self.args, **self.kwargs); File ""/usr/local/lib/python3.6/dist-packages/batch/blocking_to_async.py"", line 6, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/apis/core_v1_api.py"", line 18538, in read_namespaced_pod_log; (data) = self.read_namespaced_pod_log_with_http_info(name, namespace, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/apis/core_v1_api.py"", line 18644, in read_namespaced_pod_log_with_http_info; collection_formats=collection_formats); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 334, in call_api; _return_http_data_only, collection_formats, _preload_content, _request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 168, in __call_api; _request_timeout=_request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 355, in request; headers=headers); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 231, in GET; query_params=query_params); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 222, in request; raise ApiException(http_resp=r); {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:39,924"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""mark_complete:591"", ""message"": ""no logs for batch-9-job-1-c8b9b2 due to previous error, rescheduling pod Error: (400)\nReason: Bad Request\nHTTP response headers: HTTPHeaderDict({'Audit-Id': '6a744afc-154f-4b48-b4bb-51a15078d999', 'Content-Type': 'application/json', 'Date': 'Thu, 11 Jul 2019 14:19:39 GMT', 'Content-Length': '179'})\nHTTP response body: {\""kind\"":\""Status\"",\""apiVersion\"":\""v1\"",\""metadata\"":{},\""status\"":\""Failure\"",\""message\"":\""container \\\""main\\\"" in pod \\\""batch-9-job-1-c8b9b2\\\"" is terminated\"",\""reason\"":\""BadRequest\"",\""code\"":400}\n\n""}; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6616
https://github.com/hail-is/hail/issues/6616:9828,Availability,Failure,Failure,9828,"ib/python3.6/concurrent/futures/thread.py"", line 56, in run; result = self.fn(*self.args, **self.kwargs); File ""/usr/local/lib/python3.6/dist-packages/batch/blocking_to_async.py"", line 6, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/apis/core_v1_api.py"", line 18538, in read_namespaced_pod_log; (data) = self.read_namespaced_pod_log_with_http_info(name, namespace, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/apis/core_v1_api.py"", line 18644, in read_namespaced_pod_log_with_http_info; collection_formats=collection_formats); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 334, in call_api; _return_http_data_only, collection_formats, _preload_content, _request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 168, in __call_api; _request_timeout=_request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 355, in request; headers=headers); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 231, in GET; query_params=query_params); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 222, in request; raise ApiException(http_resp=r); {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:39,924"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""mark_complete:591"", ""message"": ""no logs for batch-9-job-1-c8b9b2 due to previous error, rescheduling pod Error: (400)\nReason: Bad Request\nHTTP response headers: HTTPHeaderDict({'Audit-Id': '6a744afc-154f-4b48-b4bb-51a15078d999', 'Content-Type': 'application/json', 'Date': 'Thu, 11 Jul 2019 14:19:39 GMT', 'Content-Length': '179'})\nHTTP response body: {\""kind\"":\""Status\"",\""apiVersion\"":\""v1\"",\""metadata\"":{},\""status\"":\""Failure\"",\""message\"":\""container \\\""main\\\"" in pod \\\""batch-9-job-1-c8b9b2\\\"" is terminated\"",\""reason\"":\""BadRequest\"",\""code\"":400}\n\n""}; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6616
https://github.com/hail-is/hail/issues/6616:558,Deployability,update,update,558,"This isn't really our fault. k8s sends us a 400 when a container is in a funky state. Creating this issue so I can find it again later when I run into this. Somehow a container terminates without timing information, and the read logs request returns a 400 instead of a 404. Batch handles this fine (it treats all log read failures the same). Known issue: https://github.com/kubernetes/kubernetes/issues/59296. ```; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:39,890"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (9, 1, 'main') with pod batch-9-job-1-c8b9b2""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:39,890"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1159"", ""message"": ""job (9, 1, 'main') mark complete""}; {""levelname"": ""WARNING"", ""asctime"": ""2019-07-11 14:19:39,899"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""mark_complete:579"", ""message"": ""job (9, 1, 'main') has pod batch-9-job-1-c8b9b2 which is terminated but has no timing information. {'api_version': 'v1',\n 'kind': 'Pod',\n 'metadata': {'annotations': None,\n 'cluster_name': None,\n 'creation_timestamp': datetime.datetime(2019, 7, 11, 14, 19, 34, tzinfo=tzlocal()),\n 'deletion_grace_period_seconds': 30,\n 'deletion_timestamp': datetime.datetime(2019, 7, 11, 14, 20, 4, tzinfo=tzlocal()),\n 'finalizers': None,\n 'generate_name': None,\n 'generation': None,\n 'initializers': None,\n 'labels': {'app': 'batch-job',\n 'batch_id': '9',\n 'hail.is/batch-instance': 'ffa5abc4607849df8e5f0036e7350bcf',\n 'job_id': '1',\n 'task': 'main',\n 'user': 'test',\n 'uuid': '291b9eed73b9433c86ff1f58624cf24d'},\n 'name': 'batch-9-job-1-c8b9b2',\n 'namespace': 'pr-6604-batch-pods-cjklalqnl5u9',\n 'owner_references': None,\n 'resource_version': '86681671',\n 'self_link': '/api/v1/namespaces/pr-6604-batch-pods-cjklalqnl5u9/pods/batch-9-job-1-c8b9b2',\n 'uid': 'e878f906-a3e6-11e9-a4bb-42010a8000af'},\n 'spec': {'active_deadline_seconds': None,\n 'affinity': ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6616
https://github.com/hail-is/hail/issues/6616:3853,Energy Efficiency,schedul,scheduler,3853,"ry': '500M'}},\n 'security_context': None,\n 'stdin': None,\n 'stdin_once': None,\n 'termination_message_path': '/dev/termination-log',\n 'termination_message_policy': 'File',\n 'tty': None,\n 'volume_devices': None,\n 'volume_mounts': [{'mount_path': '/gsa-key',\n 'mount_propagation': None,\n 'name': 'gsa-key',\n 'read_only': None,\n 'sub_path': None},\n {'mount_path': '/var/run/secrets/kubernetes.io/serviceaccount',\n 'mount_propagation': None,\n 'name': 'default-token-brr2f',\n 'read_only': True,\n 'sub_path': None}],\n 'working_dir': None}],\n 'dns_config': None,\n 'dns_policy': 'ClusterFirst',\n 'enable_service_links': True,\n 'host_aliases': None,\n 'host_ipc': None,\n 'host_network': None,\n 'host_pid': None,\n 'hostname': None,\n 'image_pull_secrets': None,\n 'init_containers': None,\n 'node_name': 'gke-vdc-preemptible-pool-9c7148b2-8hq5',\n 'node_selector': None,\n 'priority': 500000,\n 'priority_class_name': 'user',\n 'readiness_gates': None,\n 'restart_policy': 'Never',\n 'runtime_class_name': None,\n 'scheduler_name': 'default-scheduler',\n 'security_context': {'fs_group': None,\n 'run_as_group': None,\n 'run_as_non_root': None,\n 'run_as_user': None,\n 'se_linux_options': None,\n 'supplemental_groups': None,\n 'sysctls': None},\n 'service_account': 'default',\n 'service_account_name': 'default',\n 'share_process_namespace': None,\n 'subdomain': None,\n 'termination_grace_period_seconds': 30,\n 'tolerations': [{'effect': None,\n 'key': 'preemptible',\n 'operator': None,\n 'toleration_seconds': None,\n 'value': 'true'},\n {'effect': 'NoExecute',\n 'key': 'node.kubernetes.io/not-ready',\n 'operator': 'Exists',\n 'toleration_seconds': 300,\n 'value': None},\n {'effect': 'NoExecute',\n 'key': 'node.kubernetes.io/unreachable',\n 'operator': 'Exists',\n 'toleration_seconds': 300,\n 'value': None}],\n 'volumes': [{'aws_elastic_block_store': None,\n 'azure_disk': None,\n 'azure_file': None,\n 'cephfs': None,\n 'cinder': None,\n 'config_map': None,\n 'downward_ap",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6616
https://github.com/hail-is/hail/issues/6616:547,Integrability,message,message,547,"This isn't really our fault. k8s sends us a 400 when a container is in a funky state. Creating this issue so I can find it again later when I run into this. Somehow a container terminates without timing information, and the read logs request returns a 400 instead of a 404. Batch handles this fine (it treats all log read failures the same). Known issue: https://github.com/kubernetes/kubernetes/issues/59296. ```; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:39,890"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (9, 1, 'main') with pod batch-9-job-1-c8b9b2""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:39,890"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1159"", ""message"": ""job (9, 1, 'main') mark complete""}; {""levelname"": ""WARNING"", ""asctime"": ""2019-07-11 14:19:39,899"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""mark_complete:579"", ""message"": ""job (9, 1, 'main') has pod batch-9-job-1-c8b9b2 which is terminated but has no timing information. {'api_version': 'v1',\n 'kind': 'Pod',\n 'metadata': {'annotations': None,\n 'cluster_name': None,\n 'creation_timestamp': datetime.datetime(2019, 7, 11, 14, 19, 34, tzinfo=tzlocal()),\n 'deletion_grace_period_seconds': 30,\n 'deletion_timestamp': datetime.datetime(2019, 7, 11, 14, 20, 4, tzinfo=tzlocal()),\n 'finalizers': None,\n 'generate_name': None,\n 'generation': None,\n 'initializers': None,\n 'labels': {'app': 'batch-job',\n 'batch_id': '9',\n 'hail.is/batch-instance': 'ffa5abc4607849df8e5f0036e7350bcf',\n 'job_id': '1',\n 'task': 'main',\n 'user': 'test',\n 'uuid': '291b9eed73b9433c86ff1f58624cf24d'},\n 'name': 'batch-9-job-1-c8b9b2',\n 'namespace': 'pr-6604-batch-pods-cjklalqnl5u9',\n 'owner_references': None,\n 'resource_version': '86681671',\n 'self_link': '/api/v1/namespaces/pr-6604-batch-pods-cjklalqnl5u9/pods/batch-9-job-1-c8b9b2',\n 'uid': 'e878f906-a3e6-11e9-a4bb-42010a8000af'},\n 'spec': {'active_deadline_seconds': None,\n 'affinity': ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6616
https://github.com/hail-is/hail/issues/6616:749,Integrability,message,message,749,"This isn't really our fault. k8s sends us a 400 when a container is in a funky state. Creating this issue so I can find it again later when I run into this. Somehow a container terminates without timing information, and the read logs request returns a 400 instead of a 404. Batch handles this fine (it treats all log read failures the same). Known issue: https://github.com/kubernetes/kubernetes/issues/59296. ```; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:39,890"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (9, 1, 'main') with pod batch-9-job-1-c8b9b2""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:39,890"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1159"", ""message"": ""job (9, 1, 'main') mark complete""}; {""levelname"": ""WARNING"", ""asctime"": ""2019-07-11 14:19:39,899"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""mark_complete:579"", ""message"": ""job (9, 1, 'main') has pod batch-9-job-1-c8b9b2 which is terminated but has no timing information. {'api_version': 'v1',\n 'kind': 'Pod',\n 'metadata': {'annotations': None,\n 'cluster_name': None,\n 'creation_timestamp': datetime.datetime(2019, 7, 11, 14, 19, 34, tzinfo=tzlocal()),\n 'deletion_grace_period_seconds': 30,\n 'deletion_timestamp': datetime.datetime(2019, 7, 11, 14, 20, 4, tzinfo=tzlocal()),\n 'finalizers': None,\n 'generate_name': None,\n 'generation': None,\n 'initializers': None,\n 'labels': {'app': 'batch-job',\n 'batch_id': '9',\n 'hail.is/batch-instance': 'ffa5abc4607849df8e5f0036e7350bcf',\n 'job_id': '1',\n 'task': 'main',\n 'user': 'test',\n 'uuid': '291b9eed73b9433c86ff1f58624cf24d'},\n 'name': 'batch-9-job-1-c8b9b2',\n 'namespace': 'pr-6604-batch-pods-cjklalqnl5u9',\n 'owner_references': None,\n 'resource_version': '86681671',\n 'self_link': '/api/v1/namespaces/pr-6604-batch-pods-cjklalqnl5u9/pods/batch-9-job-1-c8b9b2',\n 'uid': 'e878f906-a3e6-11e9-a4bb-42010a8000af'},\n 'spec': {'active_deadline_seconds': None,\n 'affinity': ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6616
https://github.com/hail-is/hail/issues/6616:924,Integrability,message,message,924,"This isn't really our fault. k8s sends us a 400 when a container is in a funky state. Creating this issue so I can find it again later when I run into this. Somehow a container terminates without timing information, and the read logs request returns a 400 instead of a 404. Batch handles this fine (it treats all log read failures the same). Known issue: https://github.com/kubernetes/kubernetes/issues/59296. ```; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:39,890"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (9, 1, 'main') with pod batch-9-job-1-c8b9b2""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:39,890"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1159"", ""message"": ""job (9, 1, 'main') mark complete""}; {""levelname"": ""WARNING"", ""asctime"": ""2019-07-11 14:19:39,899"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""mark_complete:579"", ""message"": ""job (9, 1, 'main') has pod batch-9-job-1-c8b9b2 which is terminated but has no timing information. {'api_version': 'v1',\n 'kind': 'Pod',\n 'metadata': {'annotations': None,\n 'cluster_name': None,\n 'creation_timestamp': datetime.datetime(2019, 7, 11, 14, 19, 34, tzinfo=tzlocal()),\n 'deletion_grace_period_seconds': 30,\n 'deletion_timestamp': datetime.datetime(2019, 7, 11, 14, 20, 4, tzinfo=tzlocal()),\n 'finalizers': None,\n 'generate_name': None,\n 'generation': None,\n 'initializers': None,\n 'labels': {'app': 'batch-job',\n 'batch_id': '9',\n 'hail.is/batch-instance': 'ffa5abc4607849df8e5f0036e7350bcf',\n 'job_id': '1',\n 'task': 'main',\n 'user': 'test',\n 'uuid': '291b9eed73b9433c86ff1f58624cf24d'},\n 'name': 'batch-9-job-1-c8b9b2',\n 'namespace': 'pr-6604-batch-pods-cjklalqnl5u9',\n 'owner_references': None,\n 'resource_version': '86681671',\n 'self_link': '/api/v1/namespaces/pr-6604-batch-pods-cjklalqnl5u9/pods/batch-9-job-1-c8b9b2',\n 'uid': 'e878f906-a3e6-11e9-a4bb-42010a8000af'},\n 'spec': {'active_deadline_seconds': None,\n 'affinity': ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6616
https://github.com/hail-is/hail/issues/6616:6230,Integrability,message,message,6230,"n 'secret_name': 'test-gsa-key'},\n 'storageos': None,\n 'vsphere_volume': None},\n {'aws_elastic_block_store': None,\n 'azure_disk': None,\n 'azure_file': None,\n 'cephfs': None,\n 'cinder': None,\n 'config_map': None,\n 'downward_api': None,\n 'empty_dir': None,\n 'fc': None,\n 'flex_volume': None,\n 'flocker': None,\n 'gce_persistent_disk': None,\n 'git_repo': None,\n 'glusterfs': None,\n 'host_path': None,\n 'iscsi': None,\n 'name': 'default-token-brr2f',\n 'nfs': None,\n 'persistent_volume_claim': None,\n 'photon_persistent_disk': None,\n 'portworx_volume': None,\n 'projected': None,\n 'quobyte': None,\n 'rbd': None,\n 'scale_io': None,\n 'secret': {'default_mode': 420,\n 'items': None,\n 'optional': None,\n 'secret_name': 'default-token-brr2f'},\n 'storageos': None,\n 'vsphere_volume': None}]},\n 'status': {'conditions': [{'last_probe_time': None,\n 'last_transition_time': datetime.datetime(2019, 7, 11, 14, 19, 34, tzinfo=tzlocal()),\n 'message': None,\n 'reason': None,\n 'status': 'True',\n 'type': 'Initialized'},\n {'last_probe_time': None,\n 'last_transition_time': datetime.datetime(2019, 7, 11, 14, 19, 34, tzinfo=tzlocal()),\n 'message': 'containers with unready status: [main]',\n 'reason': 'ContainersNotReady',\n 'status': 'False',\n 'type': 'Ready'},\n {'last_probe_time': None,\n 'last_transition_time': datetime.datetime(2019, 7, 11, 14, 19, 34, tzinfo=tzlocal()),\n 'message': 'containers with unready status: [main]',\n 'reason': 'ContainersNotReady',\n 'status': 'False',\n 'type': 'ContainersReady'},\n {'last_probe_time': None,\n 'last_transition_time': datetime.datetime(2019, 7, 11, 14, 19, 34, tzinfo=tzlocal()),\n 'message': None,\n 'reason': None,\n 'status': 'True',\n 'type': 'PodScheduled'}],\n 'container_statuses': [{'container_id': None,\n 'image': 'alpine',\n 'image_id': '',\n 'last_state': {'running': None,\n 'terminated': None,\n 'waiting': None},\n 'name': 'main',\n 'ready': False,\n 'restart_count': 0,\n 'state': {'running': None,\n 'terminat",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6616
https://github.com/hail-is/hail/issues/6616:6429,Integrability,message,message,6429,"downward_api': None,\n 'empty_dir': None,\n 'fc': None,\n 'flex_volume': None,\n 'flocker': None,\n 'gce_persistent_disk': None,\n 'git_repo': None,\n 'glusterfs': None,\n 'host_path': None,\n 'iscsi': None,\n 'name': 'default-token-brr2f',\n 'nfs': None,\n 'persistent_volume_claim': None,\n 'photon_persistent_disk': None,\n 'portworx_volume': None,\n 'projected': None,\n 'quobyte': None,\n 'rbd': None,\n 'scale_io': None,\n 'secret': {'default_mode': 420,\n 'items': None,\n 'optional': None,\n 'secret_name': 'default-token-brr2f'},\n 'storageos': None,\n 'vsphere_volume': None}]},\n 'status': {'conditions': [{'last_probe_time': None,\n 'last_transition_time': datetime.datetime(2019, 7, 11, 14, 19, 34, tzinfo=tzlocal()),\n 'message': None,\n 'reason': None,\n 'status': 'True',\n 'type': 'Initialized'},\n {'last_probe_time': None,\n 'last_transition_time': datetime.datetime(2019, 7, 11, 14, 19, 34, tzinfo=tzlocal()),\n 'message': 'containers with unready status: [main]',\n 'reason': 'ContainersNotReady',\n 'status': 'False',\n 'type': 'Ready'},\n {'last_probe_time': None,\n 'last_transition_time': datetime.datetime(2019, 7, 11, 14, 19, 34, tzinfo=tzlocal()),\n 'message': 'containers with unready status: [main]',\n 'reason': 'ContainersNotReady',\n 'status': 'False',\n 'type': 'ContainersReady'},\n {'last_probe_time': None,\n 'last_transition_time': datetime.datetime(2019, 7, 11, 14, 19, 34, tzinfo=tzlocal()),\n 'message': None,\n 'reason': None,\n 'status': 'True',\n 'type': 'PodScheduled'}],\n 'container_statuses': [{'container_id': None,\n 'image': 'alpine',\n 'image_id': '',\n 'last_state': {'running': None,\n 'terminated': None,\n 'waiting': None},\n 'name': 'main',\n 'ready': False,\n 'restart_count': 0,\n 'state': {'running': None,\n 'terminated': {'container_id': None,\n 'exit_code': 0,\n 'finished_at': None,\n 'message': None,\n 'reason': None,\n 'signal': None,\n 'started_at': None},\n 'waiting': None}}],\n 'host_ip': '10.128.0.56',\n 'init_container_statuse",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6616
https://github.com/hail-is/hail/issues/6616:6675,Integrability,message,message,6675,"one,\n 'persistent_volume_claim': None,\n 'photon_persistent_disk': None,\n 'portworx_volume': None,\n 'projected': None,\n 'quobyte': None,\n 'rbd': None,\n 'scale_io': None,\n 'secret': {'default_mode': 420,\n 'items': None,\n 'optional': None,\n 'secret_name': 'default-token-brr2f'},\n 'storageos': None,\n 'vsphere_volume': None}]},\n 'status': {'conditions': [{'last_probe_time': None,\n 'last_transition_time': datetime.datetime(2019, 7, 11, 14, 19, 34, tzinfo=tzlocal()),\n 'message': None,\n 'reason': None,\n 'status': 'True',\n 'type': 'Initialized'},\n {'last_probe_time': None,\n 'last_transition_time': datetime.datetime(2019, 7, 11, 14, 19, 34, tzinfo=tzlocal()),\n 'message': 'containers with unready status: [main]',\n 'reason': 'ContainersNotReady',\n 'status': 'False',\n 'type': 'Ready'},\n {'last_probe_time': None,\n 'last_transition_time': datetime.datetime(2019, 7, 11, 14, 19, 34, tzinfo=tzlocal()),\n 'message': 'containers with unready status: [main]',\n 'reason': 'ContainersNotReady',\n 'status': 'False',\n 'type': 'ContainersReady'},\n {'last_probe_time': None,\n 'last_transition_time': datetime.datetime(2019, 7, 11, 14, 19, 34, tzinfo=tzlocal()),\n 'message': None,\n 'reason': None,\n 'status': 'True',\n 'type': 'PodScheduled'}],\n 'container_statuses': [{'container_id': None,\n 'image': 'alpine',\n 'image_id': '',\n 'last_state': {'running': None,\n 'terminated': None,\n 'waiting': None},\n 'name': 'main',\n 'ready': False,\n 'restart_count': 0,\n 'state': {'running': None,\n 'terminated': {'container_id': None,\n 'exit_code': 0,\n 'finished_at': None,\n 'message': None,\n 'reason': None,\n 'signal': None,\n 'started_at': None},\n 'waiting': None}}],\n 'host_ip': '10.128.0.56',\n 'init_container_statuses': None,\n 'message': None,\n 'nominated_node_name': None,\n 'phase': 'Pending',\n 'pod_ip': None,\n 'qos_class': 'Burstable',\n 'reason': None,\n 'start_time': datetime.datetime(2019, 7, 11, 14, 19, 34, tzinfo=tzlocal())}}""}; File ""/usr/local/lib/py",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6616
https://github.com/hail-is/hail/issues/6616:6931,Integrability,message,message,6931,"tetime.datetime(2019, 7, 11, 14, 19, 34, tzinfo=tzlocal()),\n 'message': None,\n 'reason': None,\n 'status': 'True',\n 'type': 'Initialized'},\n {'last_probe_time': None,\n 'last_transition_time': datetime.datetime(2019, 7, 11, 14, 19, 34, tzinfo=tzlocal()),\n 'message': 'containers with unready status: [main]',\n 'reason': 'ContainersNotReady',\n 'status': 'False',\n 'type': 'Ready'},\n {'last_probe_time': None,\n 'last_transition_time': datetime.datetime(2019, 7, 11, 14, 19, 34, tzinfo=tzlocal()),\n 'message': 'containers with unready status: [main]',\n 'reason': 'ContainersNotReady',\n 'status': 'False',\n 'type': 'ContainersReady'},\n {'last_probe_time': None,\n 'last_transition_time': datetime.datetime(2019, 7, 11, 14, 19, 34, tzinfo=tzlocal()),\n 'message': None,\n 'reason': None,\n 'status': 'True',\n 'type': 'PodScheduled'}],\n 'container_statuses': [{'container_id': None,\n 'image': 'alpine',\n 'image_id': '',\n 'last_state': {'running': None,\n 'terminated': None,\n 'waiting': None},\n 'name': 'main',\n 'ready': False,\n 'restart_count': 0,\n 'state': {'running': None,\n 'terminated': {'container_id': None,\n 'exit_code': 0,\n 'finished_at': None,\n 'message': None,\n 'reason': None,\n 'signal': None,\n 'started_at': None},\n 'waiting': None}}],\n 'host_ip': '10.128.0.56',\n 'init_container_statuses': None,\n 'message': None,\n 'nominated_node_name': None,\n 'phase': 'Pending',\n 'pod_ip': None,\n 'qos_class': 'Burstable',\n 'reason': None,\n 'start_time': datetime.datetime(2019, 7, 11, 14, 19, 34, tzinfo=tzlocal())}}""}; File ""/usr/local/lib/python3.6/dist-packages/batch/k8s.py"", line 65, in wrapped; **kwargs),; File ""/usr/local/lib/python3.6/dist-packages/batch/blocking_to_async.py"", line 6, in blocking_to_async; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/lib/python3.6/concurrent/futures/thread.py"", line 56, in run; result = self.fn(*self.args, **self.kwargs); File ""/usr/local/lib/python3.6/dist-packages/batch/blocking_to_async.py"", line 6, in ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6616
https://github.com/hail-is/hail/issues/6616:7346,Integrability,message,message,7346,"tetime.datetime(2019, 7, 11, 14, 19, 34, tzinfo=tzlocal()),\n 'message': None,\n 'reason': None,\n 'status': 'True',\n 'type': 'Initialized'},\n {'last_probe_time': None,\n 'last_transition_time': datetime.datetime(2019, 7, 11, 14, 19, 34, tzinfo=tzlocal()),\n 'message': 'containers with unready status: [main]',\n 'reason': 'ContainersNotReady',\n 'status': 'False',\n 'type': 'Ready'},\n {'last_probe_time': None,\n 'last_transition_time': datetime.datetime(2019, 7, 11, 14, 19, 34, tzinfo=tzlocal()),\n 'message': 'containers with unready status: [main]',\n 'reason': 'ContainersNotReady',\n 'status': 'False',\n 'type': 'ContainersReady'},\n {'last_probe_time': None,\n 'last_transition_time': datetime.datetime(2019, 7, 11, 14, 19, 34, tzinfo=tzlocal()),\n 'message': None,\n 'reason': None,\n 'status': 'True',\n 'type': 'PodScheduled'}],\n 'container_statuses': [{'container_id': None,\n 'image': 'alpine',\n 'image_id': '',\n 'last_state': {'running': None,\n 'terminated': None,\n 'waiting': None},\n 'name': 'main',\n 'ready': False,\n 'restart_count': 0,\n 'state': {'running': None,\n 'terminated': {'container_id': None,\n 'exit_code': 0,\n 'finished_at': None,\n 'message': None,\n 'reason': None,\n 'signal': None,\n 'started_at': None},\n 'waiting': None}}],\n 'host_ip': '10.128.0.56',\n 'init_container_statuses': None,\n 'message': None,\n 'nominated_node_name': None,\n 'phase': 'Pending',\n 'pod_ip': None,\n 'qos_class': 'Burstable',\n 'reason': None,\n 'start_time': datetime.datetime(2019, 7, 11, 14, 19, 34, tzinfo=tzlocal())}}""}; File ""/usr/local/lib/python3.6/dist-packages/batch/k8s.py"", line 65, in wrapped; **kwargs),; File ""/usr/local/lib/python3.6/dist-packages/batch/blocking_to_async.py"", line 6, in blocking_to_async; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/lib/python3.6/concurrent/futures/thread.py"", line 56, in run; result = self.fn(*self.args, **self.kwargs); File ""/usr/local/lib/python3.6/dist-packages/batch/blocking_to_async.py"", line 6, in ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6616
https://github.com/hail-is/hail/issues/6616:7509,Integrability,message,message,7509,"robe_time': None,\n 'last_transition_time': datetime.datetime(2019, 7, 11, 14, 19, 34, tzinfo=tzlocal()),\n 'message': 'containers with unready status: [main]',\n 'reason': 'ContainersNotReady',\n 'status': 'False',\n 'type': 'ContainersReady'},\n {'last_probe_time': None,\n 'last_transition_time': datetime.datetime(2019, 7, 11, 14, 19, 34, tzinfo=tzlocal()),\n 'message': None,\n 'reason': None,\n 'status': 'True',\n 'type': 'PodScheduled'}],\n 'container_statuses': [{'container_id': None,\n 'image': 'alpine',\n 'image_id': '',\n 'last_state': {'running': None,\n 'terminated': None,\n 'waiting': None},\n 'name': 'main',\n 'ready': False,\n 'restart_count': 0,\n 'state': {'running': None,\n 'terminated': {'container_id': None,\n 'exit_code': 0,\n 'finished_at': None,\n 'message': None,\n 'reason': None,\n 'signal': None,\n 'started_at': None},\n 'waiting': None}}],\n 'host_ip': '10.128.0.56',\n 'init_container_statuses': None,\n 'message': None,\n 'nominated_node_name': None,\n 'phase': 'Pending',\n 'pod_ip': None,\n 'qos_class': 'Burstable',\n 'reason': None,\n 'start_time': datetime.datetime(2019, 7, 11, 14, 19, 34, tzinfo=tzlocal())}}""}; File ""/usr/local/lib/python3.6/dist-packages/batch/k8s.py"", line 65, in wrapped; **kwargs),; File ""/usr/local/lib/python3.6/dist-packages/batch/blocking_to_async.py"", line 6, in blocking_to_async; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/lib/python3.6/concurrent/futures/thread.py"", line 56, in run; result = self.fn(*self.args, **self.kwargs); File ""/usr/local/lib/python3.6/dist-packages/batch/blocking_to_async.py"", line 6, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/apis/core_v1_api.py"", line 18538, in read_namespaced_pod_log; (data) = self.read_namespaced_pod_log_with_http_info(name, namespace, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/apis/core_v1_api.py"", line 18644, in read_namespaced_pod_log_with_http_info;",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6616
https://github.com/hail-is/hail/issues/6616:7796,Integrability,wrap,wrapped,7796,"n {'last_probe_time': None,\n 'last_transition_time': datetime.datetime(2019, 7, 11, 14, 19, 34, tzinfo=tzlocal()),\n 'message': None,\n 'reason': None,\n 'status': 'True',\n 'type': 'PodScheduled'}],\n 'container_statuses': [{'container_id': None,\n 'image': 'alpine',\n 'image_id': '',\n 'last_state': {'running': None,\n 'terminated': None,\n 'waiting': None},\n 'name': 'main',\n 'ready': False,\n 'restart_count': 0,\n 'state': {'running': None,\n 'terminated': {'container_id': None,\n 'exit_code': 0,\n 'finished_at': None,\n 'message': None,\n 'reason': None,\n 'signal': None,\n 'started_at': None},\n 'waiting': None}}],\n 'host_ip': '10.128.0.56',\n 'init_container_statuses': None,\n 'message': None,\n 'nominated_node_name': None,\n 'phase': 'Pending',\n 'pod_ip': None,\n 'qos_class': 'Burstable',\n 'reason': None,\n 'start_time': datetime.datetime(2019, 7, 11, 14, 19, 34, tzinfo=tzlocal())}}""}; File ""/usr/local/lib/python3.6/dist-packages/batch/k8s.py"", line 65, in wrapped; **kwargs),; File ""/usr/local/lib/python3.6/dist-packages/batch/blocking_to_async.py"", line 6, in blocking_to_async; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/lib/python3.6/concurrent/futures/thread.py"", line 56, in run; result = self.fn(*self.args, **self.kwargs); File ""/usr/local/lib/python3.6/dist-packages/batch/blocking_to_async.py"", line 6, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/apis/core_v1_api.py"", line 18538, in read_namespaced_pod_log; (data) = self.read_namespaced_pod_log_with_http_info(name, namespace, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/apis/core_v1_api.py"", line 18644, in read_namespaced_pod_log_with_http_info; collection_formats=collection_formats); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 334, in call_api; _return_http_data_only, collection_formats, _preload_content, _request_timeout); File ""/usr/local/lib/p",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6616
https://github.com/hail-is/hail/issues/6616:9422,Integrability,message,message,9422,"ib/python3.6/concurrent/futures/thread.py"", line 56, in run; result = self.fn(*self.args, **self.kwargs); File ""/usr/local/lib/python3.6/dist-packages/batch/blocking_to_async.py"", line 6, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/apis/core_v1_api.py"", line 18538, in read_namespaced_pod_log; (data) = self.read_namespaced_pod_log_with_http_info(name, namespace, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/apis/core_v1_api.py"", line 18644, in read_namespaced_pod_log_with_http_info; collection_formats=collection_formats); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 334, in call_api; _return_http_data_only, collection_formats, _preload_content, _request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 168, in __call_api; _request_timeout=_request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 355, in request; headers=headers); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 231, in GET; query_params=query_params); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 222, in request; raise ApiException(http_resp=r); {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:39,924"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""mark_complete:591"", ""message"": ""no logs for batch-9-job-1-c8b9b2 due to previous error, rescheduling pod Error: (400)\nReason: Bad Request\nHTTP response headers: HTTPHeaderDict({'Audit-Id': '6a744afc-154f-4b48-b4bb-51a15078d999', 'Content-Type': 'application/json', 'Date': 'Thu, 11 Jul 2019 14:19:39 GMT', 'Content-Length': '179'})\nHTTP response body: {\""kind\"":\""Status\"",\""apiVersion\"":\""v1\"",\""metadata\"":{},\""status\"":\""Failure\"",\""message\"":\""container \\\""main\\\"" in pod \\\""batch-9-job-1-c8b9b2\\\"" is terminated\"",\""reason\"":\""BadRequest\"",\""code\"":400}\n\n""}; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6616
https://github.com/hail-is/hail/issues/6616:9840,Integrability,message,message,9840,"ib/python3.6/concurrent/futures/thread.py"", line 56, in run; result = self.fn(*self.args, **self.kwargs); File ""/usr/local/lib/python3.6/dist-packages/batch/blocking_to_async.py"", line 6, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/apis/core_v1_api.py"", line 18538, in read_namespaced_pod_log; (data) = self.read_namespaced_pod_log_with_http_info(name, namespace, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/apis/core_v1_api.py"", line 18644, in read_namespaced_pod_log_with_http_info; collection_formats=collection_formats); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 334, in call_api; _return_http_data_only, collection_formats, _preload_content, _request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 168, in __call_api; _request_timeout=_request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 355, in request; headers=headers); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 231, in GET; query_params=query_params); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 222, in request; raise ApiException(http_resp=r); {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:39,924"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""mark_complete:591"", ""message"": ""no logs for batch-9-job-1-c8b9b2 due to previous error, rescheduling pod Error: (400)\nReason: Bad Request\nHTTP response headers: HTTPHeaderDict({'Audit-Id': '6a744afc-154f-4b48-b4bb-51a15078d999', 'Content-Type': 'application/json', 'Date': 'Thu, 11 Jul 2019 14:19:39 GMT', 'Content-Length': '179'})\nHTTP response body: {\""kind\"":\""Status\"",\""apiVersion\"":\""v1\"",\""metadata\"":{},\""status\"":\""Failure\"",\""message\"":\""container \\\""main\\\"" in pod \\\""batch-9-job-1-c8b9b2\\\"" is terminated\"",\""reason\"":\""BadRequest\"",\""code\"":400}\n\n""}; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6616
https://github.com/hail-is/hail/issues/6616:7990,Performance,concurren,concurrent,7990,"eduled'}],\n 'container_statuses': [{'container_id': None,\n 'image': 'alpine',\n 'image_id': '',\n 'last_state': {'running': None,\n 'terminated': None,\n 'waiting': None},\n 'name': 'main',\n 'ready': False,\n 'restart_count': 0,\n 'state': {'running': None,\n 'terminated': {'container_id': None,\n 'exit_code': 0,\n 'finished_at': None,\n 'message': None,\n 'reason': None,\n 'signal': None,\n 'started_at': None},\n 'waiting': None}}],\n 'host_ip': '10.128.0.56',\n 'init_container_statuses': None,\n 'message': None,\n 'nominated_node_name': None,\n 'phase': 'Pending',\n 'pod_ip': None,\n 'qos_class': 'Burstable',\n 'reason': None,\n 'start_time': datetime.datetime(2019, 7, 11, 14, 19, 34, tzinfo=tzlocal())}}""}; File ""/usr/local/lib/python3.6/dist-packages/batch/k8s.py"", line 65, in wrapped; **kwargs),; File ""/usr/local/lib/python3.6/dist-packages/batch/blocking_to_async.py"", line 6, in blocking_to_async; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/lib/python3.6/concurrent/futures/thread.py"", line 56, in run; result = self.fn(*self.args, **self.kwargs); File ""/usr/local/lib/python3.6/dist-packages/batch/blocking_to_async.py"", line 6, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/apis/core_v1_api.py"", line 18538, in read_namespaced_pod_log; (data) = self.read_namespaced_pod_log_with_http_info(name, namespace, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/apis/core_v1_api.py"", line 18644, in read_namespaced_pod_log_with_http_info; collection_formats=collection_formats); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 334, in call_api; _return_http_data_only, collection_formats, _preload_content, _request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 168, in __call_api; _request_timeout=_request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_clie",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6616
https://github.com/hail-is/hail/issues/6616:9581,Security,Audit,Audit-Id,9581,"ib/python3.6/concurrent/futures/thread.py"", line 56, in run; result = self.fn(*self.args, **self.kwargs); File ""/usr/local/lib/python3.6/dist-packages/batch/blocking_to_async.py"", line 6, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/apis/core_v1_api.py"", line 18538, in read_namespaced_pod_log; (data) = self.read_namespaced_pod_log_with_http_info(name, namespace, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/apis/core_v1_api.py"", line 18644, in read_namespaced_pod_log_with_http_info; collection_formats=collection_formats); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 334, in call_api; _return_http_data_only, collection_formats, _preload_content, _request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 168, in __call_api; _request_timeout=_request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 355, in request; headers=headers); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 231, in GET; query_params=query_params); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 222, in request; raise ApiException(http_resp=r); {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:39,924"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""mark_complete:591"", ""message"": ""no logs for batch-9-job-1-c8b9b2 due to previous error, rescheduling pod Error: (400)\nReason: Bad Request\nHTTP response headers: HTTPHeaderDict({'Audit-Id': '6a744afc-154f-4b48-b4bb-51a15078d999', 'Content-Type': 'application/json', 'Date': 'Thu, 11 Jul 2019 14:19:39 GMT', 'Content-Length': '179'})\nHTTP response body: {\""kind\"":\""Status\"",\""apiVersion\"":\""v1\"",\""metadata\"":{},\""status\"":\""Failure\"",\""message\"":\""container \\\""main\\\"" in pod \\\""batch-9-job-1-c8b9b2\\\"" is terminated\"",\""reason\"":\""BadRequest\"",\""code\"":400}\n\n""}; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6616
https://github.com/hail-is/hail/issues/6616:229,Testability,log,logs,229,"This isn't really our fault. k8s sends us a 400 when a container is in a funky state. Creating this issue so I can find it again later when I run into this. Somehow a container terminates without timing information, and the read logs request returns a 400 instead of a 404. Batch handles this fine (it treats all log read failures the same). Known issue: https://github.com/kubernetes/kubernetes/issues/59296. ```; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:39,890"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (9, 1, 'main') with pod batch-9-job-1-c8b9b2""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:39,890"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1159"", ""message"": ""job (9, 1, 'main') mark complete""}; {""levelname"": ""WARNING"", ""asctime"": ""2019-07-11 14:19:39,899"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""mark_complete:579"", ""message"": ""job (9, 1, 'main') has pod batch-9-job-1-c8b9b2 which is terminated but has no timing information. {'api_version': 'v1',\n 'kind': 'Pod',\n 'metadata': {'annotations': None,\n 'cluster_name': None,\n 'creation_timestamp': datetime.datetime(2019, 7, 11, 14, 19, 34, tzinfo=tzlocal()),\n 'deletion_grace_period_seconds': 30,\n 'deletion_timestamp': datetime.datetime(2019, 7, 11, 14, 20, 4, tzinfo=tzlocal()),\n 'finalizers': None,\n 'generate_name': None,\n 'generation': None,\n 'initializers': None,\n 'labels': {'app': 'batch-job',\n 'batch_id': '9',\n 'hail.is/batch-instance': 'ffa5abc4607849df8e5f0036e7350bcf',\n 'job_id': '1',\n 'task': 'main',\n 'user': 'test',\n 'uuid': '291b9eed73b9433c86ff1f58624cf24d'},\n 'name': 'batch-9-job-1-c8b9b2',\n 'namespace': 'pr-6604-batch-pods-cjklalqnl5u9',\n 'owner_references': None,\n 'resource_version': '86681671',\n 'self_link': '/api/v1/namespaces/pr-6604-batch-pods-cjklalqnl5u9/pods/batch-9-job-1-c8b9b2',\n 'uid': 'e878f906-a3e6-11e9-a4bb-42010a8000af'},\n 'spec': {'active_deadline_seconds': None,\n 'affinity': ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6616
https://github.com/hail-is/hail/issues/6616:313,Testability,log,log,313,"This isn't really our fault. k8s sends us a 400 when a container is in a funky state. Creating this issue so I can find it again later when I run into this. Somehow a container terminates without timing information, and the read logs request returns a 400 instead of a 404. Batch handles this fine (it treats all log read failures the same). Known issue: https://github.com/kubernetes/kubernetes/issues/59296. ```; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:39,890"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (9, 1, 'main') with pod batch-9-job-1-c8b9b2""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:39,890"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1159"", ""message"": ""job (9, 1, 'main') mark complete""}; {""levelname"": ""WARNING"", ""asctime"": ""2019-07-11 14:19:39,899"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""mark_complete:579"", ""message"": ""job (9, 1, 'main') has pod batch-9-job-1-c8b9b2 which is terminated but has no timing information. {'api_version': 'v1',\n 'kind': 'Pod',\n 'metadata': {'annotations': None,\n 'cluster_name': None,\n 'creation_timestamp': datetime.datetime(2019, 7, 11, 14, 19, 34, tzinfo=tzlocal()),\n 'deletion_grace_period_seconds': 30,\n 'deletion_timestamp': datetime.datetime(2019, 7, 11, 14, 20, 4, tzinfo=tzlocal()),\n 'finalizers': None,\n 'generate_name': None,\n 'generation': None,\n 'initializers': None,\n 'labels': {'app': 'batch-job',\n 'batch_id': '9',\n 'hail.is/batch-instance': 'ffa5abc4607849df8e5f0036e7350bcf',\n 'job_id': '1',\n 'task': 'main',\n 'user': 'test',\n 'uuid': '291b9eed73b9433c86ff1f58624cf24d'},\n 'name': 'batch-9-job-1-c8b9b2',\n 'namespace': 'pr-6604-batch-pods-cjklalqnl5u9',\n 'owner_references': None,\n 'resource_version': '86681671',\n 'self_link': '/api/v1/namespaces/pr-6604-batch-pods-cjklalqnl5u9/pods/batch-9-job-1-c8b9b2',\n 'uid': 'e878f906-a3e6-11e9-a4bb-42010a8000af'},\n 'spec': {'active_deadline_seconds': None,\n 'affinity': ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6616
https://github.com/hail-is/hail/issues/6616:1598,Testability,test,test,1598,""", ""funcNameAndLine"": ""mark_complete:579"", ""message"": ""job (9, 1, 'main') has pod batch-9-job-1-c8b9b2 which is terminated but has no timing information. {'api_version': 'v1',\n 'kind': 'Pod',\n 'metadata': {'annotations': None,\n 'cluster_name': None,\n 'creation_timestamp': datetime.datetime(2019, 7, 11, 14, 19, 34, tzinfo=tzlocal()),\n 'deletion_grace_period_seconds': 30,\n 'deletion_timestamp': datetime.datetime(2019, 7, 11, 14, 20, 4, tzinfo=tzlocal()),\n 'finalizers': None,\n 'generate_name': None,\n 'generation': None,\n 'initializers': None,\n 'labels': {'app': 'batch-job',\n 'batch_id': '9',\n 'hail.is/batch-instance': 'ffa5abc4607849df8e5f0036e7350bcf',\n 'job_id': '1',\n 'task': 'main',\n 'user': 'test',\n 'uuid': '291b9eed73b9433c86ff1f58624cf24d'},\n 'name': 'batch-9-job-1-c8b9b2',\n 'namespace': 'pr-6604-batch-pods-cjklalqnl5u9',\n 'owner_references': None,\n 'resource_version': '86681671',\n 'self_link': '/api/v1/namespaces/pr-6604-batch-pods-cjklalqnl5u9/pods/batch-9-job-1-c8b9b2',\n 'uid': 'e878f906-a3e6-11e9-a4bb-42010a8000af'},\n 'spec': {'active_deadline_seconds': None,\n 'affinity': None,\n 'automount_service_account_token': None,\n 'containers': [{'args': None,\n 'command': ['sleep', '30'],\n 'env': [{'name': 'POD_IP',\n 'value': None,\n 'value_from': {'config_map_key_ref': None,\n 'field_ref': {'api_version': 'v1',\n 'field_path': 'status.podIP'},\n 'resource_field_ref': None,\n 'secret_key_ref': None}},\n {'name': 'POD_NAME',\n 'value': None,\n 'value_from': {'config_map_key_ref': None,\n 'field_ref': {'api_version': 'v1',\n 'field_path': 'metadata.name'},\n 'resource_field_ref': None,\n 'secret_key_ref': None}}],\n 'env_from': None,\n 'image': 'alpine',\n 'image_pull_policy': 'Always',\n 'lifecycle': None,\n 'liveness_probe': None,\n 'name': 'main',\n 'ports': None,\n 'readiness_probe': None,\n 'resources': {'limits': None,\n 'requests': {'cpu': '100m',\n 'memory': '500M'}},\n 'security_context': None,\n 'stdin': None,\n 'stdin_once': None,\",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6616
https://github.com/hail-is/hail/issues/6616:2928,Testability,log,log,2928,"4-batch-pods-cjklalqnl5u9/pods/batch-9-job-1-c8b9b2',\n 'uid': 'e878f906-a3e6-11e9-a4bb-42010a8000af'},\n 'spec': {'active_deadline_seconds': None,\n 'affinity': None,\n 'automount_service_account_token': None,\n 'containers': [{'args': None,\n 'command': ['sleep', '30'],\n 'env': [{'name': 'POD_IP',\n 'value': None,\n 'value_from': {'config_map_key_ref': None,\n 'field_ref': {'api_version': 'v1',\n 'field_path': 'status.podIP'},\n 'resource_field_ref': None,\n 'secret_key_ref': None}},\n {'name': 'POD_NAME',\n 'value': None,\n 'value_from': {'config_map_key_ref': None,\n 'field_ref': {'api_version': 'v1',\n 'field_path': 'metadata.name'},\n 'resource_field_ref': None,\n 'secret_key_ref': None}}],\n 'env_from': None,\n 'image': 'alpine',\n 'image_pull_policy': 'Always',\n 'lifecycle': None,\n 'liveness_probe': None,\n 'name': 'main',\n 'ports': None,\n 'readiness_probe': None,\n 'resources': {'limits': None,\n 'requests': {'cpu': '100m',\n 'memory': '500M'}},\n 'security_context': None,\n 'stdin': None,\n 'stdin_once': None,\n 'termination_message_path': '/dev/termination-log',\n 'termination_message_policy': 'File',\n 'tty': None,\n 'volume_devices': None,\n 'volume_mounts': [{'mount_path': '/gsa-key',\n 'mount_propagation': None,\n 'name': 'gsa-key',\n 'read_only': None,\n 'sub_path': None},\n {'mount_path': '/var/run/secrets/kubernetes.io/serviceaccount',\n 'mount_propagation': None,\n 'name': 'default-token-brr2f',\n 'read_only': True,\n 'sub_path': None}],\n 'working_dir': None}],\n 'dns_config': None,\n 'dns_policy': 'ClusterFirst',\n 'enable_service_links': True,\n 'host_aliases': None,\n 'host_ipc': None,\n 'host_network': None,\n 'host_pid': None,\n 'hostname': None,\n 'image_pull_secrets': None,\n 'init_containers': None,\n 'node_name': 'gke-vdc-preemptible-pool-9c7148b2-8hq5',\n 'node_selector': None,\n 'priority': 500000,\n 'priority_class_name': 'user',\n 'readiness_gates': None,\n 'restart_policy': 'Never',\n 'runtime_class_name': None,\n 'scheduler_nam",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6616
https://github.com/hail-is/hail/issues/6616:5291,Testability,test,test-gsa-key,5291,"effect': 'NoExecute',\n 'key': 'node.kubernetes.io/not-ready',\n 'operator': 'Exists',\n 'toleration_seconds': 300,\n 'value': None},\n {'effect': 'NoExecute',\n 'key': 'node.kubernetes.io/unreachable',\n 'operator': 'Exists',\n 'toleration_seconds': 300,\n 'value': None}],\n 'volumes': [{'aws_elastic_block_store': None,\n 'azure_disk': None,\n 'azure_file': None,\n 'cephfs': None,\n 'cinder': None,\n 'config_map': None,\n 'downward_api': None,\n 'empty_dir': None,\n 'fc': None,\n 'flex_volume': None,\n 'flocker': None,\n 'gce_persistent_disk': None,\n 'git_repo': None,\n 'glusterfs': None,\n 'host_path': None,\n 'iscsi': None,\n 'name': 'gsa-key',\n 'nfs': None,\n 'persistent_volume_claim': None,\n 'photon_persistent_disk': None,\n 'portworx_volume': None,\n 'projected': None,\n 'quobyte': None,\n 'rbd': None,\n 'scale_io': None,\n 'secret': {'default_mode': 420,\n 'items': None,\n 'optional': None,\n 'secret_name': 'test-gsa-key'},\n 'storageos': None,\n 'vsphere_volume': None},\n {'aws_elastic_block_store': None,\n 'azure_disk': None,\n 'azure_file': None,\n 'cephfs': None,\n 'cinder': None,\n 'config_map': None,\n 'downward_api': None,\n 'empty_dir': None,\n 'fc': None,\n 'flex_volume': None,\n 'flocker': None,\n 'gce_persistent_disk': None,\n 'git_repo': None,\n 'glusterfs': None,\n 'host_path': None,\n 'iscsi': None,\n 'name': 'default-token-brr2f',\n 'nfs': None,\n 'persistent_volume_claim': None,\n 'photon_persistent_disk': None,\n 'portworx_volume': None,\n 'projected': None,\n 'quobyte': None,\n 'rbd': None,\n 'scale_io': None,\n 'secret': {'default_mode': 420,\n 'items': None,\n 'optional': None,\n 'secret_name': 'default-token-brr2f'},\n 'storageos': None,\n 'vsphere_volume': None}]},\n 'status': {'conditions': [{'last_probe_time': None,\n 'last_transition_time': datetime.datetime(2019, 7, 11, 14, 19, 34, tzinfo=tzlocal()),\n 'message': None,\n 'reason': None,\n 'status': 'True',\n 'type': 'Initialized'},\n {'last_probe_time': None,\n 'last_transition_tim",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6616
https://github.com/hail-is/hail/issues/6616:9436,Testability,log,logs,9436,"ib/python3.6/concurrent/futures/thread.py"", line 56, in run; result = self.fn(*self.args, **self.kwargs); File ""/usr/local/lib/python3.6/dist-packages/batch/blocking_to_async.py"", line 6, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/apis/core_v1_api.py"", line 18538, in read_namespaced_pod_log; (data) = self.read_namespaced_pod_log_with_http_info(name, namespace, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/apis/core_v1_api.py"", line 18644, in read_namespaced_pod_log_with_http_info; collection_formats=collection_formats); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 334, in call_api; _return_http_data_only, collection_formats, _preload_content, _request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 168, in __call_api; _request_timeout=_request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 355, in request; headers=headers); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 231, in GET; query_params=query_params); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 222, in request; raise ApiException(http_resp=r); {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:39,924"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""mark_complete:591"", ""message"": ""no logs for batch-9-job-1-c8b9b2 due to previous error, rescheduling pod Error: (400)\nReason: Bad Request\nHTTP response headers: HTTPHeaderDict({'Audit-Id': '6a744afc-154f-4b48-b4bb-51a15078d999', 'Content-Type': 'application/json', 'Date': 'Thu, 11 Jul 2019 14:19:39 GMT', 'Content-Length': '179'})\nHTTP response body: {\""kind\"":\""Status\"",\""apiVersion\"":\""v1\"",\""metadata\"":{},\""status\"":\""Failure\"",\""message\"":\""container \\\""main\\\"" in pod \\\""batch-9-job-1-c8b9b2\\\"" is terminated\"",\""reason\"":\""BadRequest\"",\""code\"":400}\n\n""}; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6616
https://github.com/hail-is/hail/issues/6617:8920,Availability,toler,tolerations,8920,"ry': '500M'}},\n 'security_context': None,\n 'stdin': None,\n 'stdin_once': None,\n 'termination_message_path': '/dev/termination-log',\n 'termination_message_policy': 'File',\n 'tty': None,\n 'volume_devices': None,\n 'volume_mounts': [{'mount_path': '/gsa-key',\n 'mount_propagation': None,\n 'name': 'gsa-key',\n 'read_only': None,\n 'sub_path': None},\n {'mount_path': '/var/run/secrets/kubernetes.io/serviceaccount',\n 'mount_propagation': None,\n 'name': 'default-token-brr2f',\n 'read_only': True,\n 'sub_path': None}],\n 'working_dir': None}],\n 'dns_config': None,\n 'dns_policy': 'ClusterFirst',\n 'enable_service_links': True,\n 'host_aliases': None,\n 'host_ipc': None,\n 'host_network': None,\n 'host_pid': None,\n 'hostname': None,\n 'image_pull_secrets': None,\n 'init_containers': None,\n 'node_name': 'gke-vdc-preemptible-pool-9c7148b2-8hq5',\n 'node_selector': None,\n 'priority': 500000,\n 'priority_class_name': 'user',\n 'readiness_gates': None,\n 'restart_policy': 'Never',\n 'runtime_class_name': None,\n 'scheduler_name': 'default-scheduler',\n 'security_context': {'fs_group': None,\n 'run_as_group': None,\n 'run_as_non_root': None,\n 'run_as_user': None,\n 'se_linux_options': None,\n 'supplemental_groups': None,\n 'sysctls': None},\n 'service_account': 'default',\n 'service_account_name': 'default',\n 'share_process_namespace': None,\n 'subdomain': None,\n 'termination_grace_period_seconds': 30,\n 'tolerations': [{'effect': None,\n 'key': 'preemptible',\n 'operator': None,\n 'toleration_seconds': None,\n 'value': 'true'},\n {'effect': 'NoExecute',\n 'key': 'node.kubernetes.io/not-ready',\n 'operator': 'Exists',\n 'toleration_seconds': 300,\n 'value': None},\n {'effect': 'NoExecute',\n 'key': 'node.kubernetes.io/unreachable',\n 'operator': 'Exists',\n 'toleration_seconds': 300,\n 'value': None}],\n 'volumes': [{'aws_elastic_block_store': None,\n 'azure_disk': None,\n 'azure_file': None,\n 'cephfs': None,\n 'cinder': None,\n 'config_map': None,\n 'downward_ap",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:14173,Availability,error,error,14173,"ient.py"", line 334, in call_api; _return_http_data_only, collection_formats, _preload_content, _request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 168, in __call_api; _request_timeout=_request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 355, in request; headers=headers); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 231, in GET; query_params=query_params); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 222, in request; raise ApiException(http_resp=r); {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:39,924"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""mark_complete:591"", ""message"": ""no logs for batch-9-job-1-c8b9b2 due to previous error, rescheduling pod Error: (400)\nReason: Bad Request\nHTTP response headers: HTTPHeaderDict({'Audit-Id': '6a744afc-154f-4b48-b4bb-51a15078d999', 'Content-Type': 'application/json', 'Date': 'Thu, 11 Jul 2019 14:19:39 GMT', 'Content-Length': '179'})\nHTTP response body: {\""kind\"":\""Status\"",\""apiVersion\"":\""v1\"",\""metadata\"":{},\""status\"":\""Failure\"",\""message\"":\""container \\\""main\\\"" in pod \\\""batch-9-job-1-c8b9b2\\\"" is terminated\"",\""reason\"":\""BadRequest\"",\""code\"":400}\n\n""}; ```. And finally, this k8s refresh loop sequence repeats until CI kills the tests due to a timeout. ```; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,070"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_state:1261"", ""message"": ""started k8s state refresh""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,085"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_pods:1210"", ""message"": ""k8s had 3 pods""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,088"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (11, 1, 'output') with pod batch-11-job-1-4f1118""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,090"", ""fil",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:14197,Availability,Error,Error,14197,"ient.py"", line 334, in call_api; _return_http_data_only, collection_formats, _preload_content, _request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 168, in __call_api; _request_timeout=_request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 355, in request; headers=headers); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 231, in GET; query_params=query_params); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 222, in request; raise ApiException(http_resp=r); {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:39,924"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""mark_complete:591"", ""message"": ""no logs for batch-9-job-1-c8b9b2 due to previous error, rescheduling pod Error: (400)\nReason: Bad Request\nHTTP response headers: HTTPHeaderDict({'Audit-Id': '6a744afc-154f-4b48-b4bb-51a15078d999', 'Content-Type': 'application/json', 'Date': 'Thu, 11 Jul 2019 14:19:39 GMT', 'Content-Length': '179'})\nHTTP response body: {\""kind\"":\""Status\"",\""apiVersion\"":\""v1\"",\""metadata\"":{},\""status\"":\""Failure\"",\""message\"":\""container \\\""main\\\"" in pod \\\""batch-9-job-1-c8b9b2\\\"" is terminated\"",\""reason\"":\""BadRequest\"",\""code\"":400}\n\n""}; ```. And finally, this k8s refresh loop sequence repeats until CI kills the tests due to a timeout. ```; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,070"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_state:1261"", ""message"": ""started k8s state refresh""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,085"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_pods:1210"", ""message"": ""k8s had 3 pods""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,088"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (11, 1, 'output') with pod batch-11-job-1-4f1118""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,090"", ""fil",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:14519,Availability,Failure,Failure,14519,"ient.py"", line 334, in call_api; _return_http_data_only, collection_formats, _preload_content, _request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 168, in __call_api; _request_timeout=_request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 355, in request; headers=headers); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 231, in GET; query_params=query_params); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 222, in request; raise ApiException(http_resp=r); {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:39,924"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""mark_complete:591"", ""message"": ""no logs for batch-9-job-1-c8b9b2 due to previous error, rescheduling pod Error: (400)\nReason: Bad Request\nHTTP response headers: HTTPHeaderDict({'Audit-Id': '6a744afc-154f-4b48-b4bb-51a15078d999', 'Content-Type': 'application/json', 'Date': 'Thu, 11 Jul 2019 14:19:39 GMT', 'Content-Length': '179'})\nHTTP response body: {\""kind\"":\""Status\"",\""apiVersion\"":\""v1\"",\""metadata\"":{},\""status\"":\""Failure\"",\""message\"":\""container \\\""main\\\"" in pod \\\""batch-9-job-1-c8b9b2\\\"" is terminated\"",\""reason\"":\""BadRequest\"",\""code\"":400}\n\n""}; ```. And finally, this k8s refresh loop sequence repeats until CI kills the tests due to a timeout. ```; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,070"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_state:1261"", ""message"": ""started k8s state refresh""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,085"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_pods:1210"", ""message"": ""k8s had 3 pods""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,088"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (11, 1, 'output') with pod batch-11-job-1-4f1118""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,090"", ""fil",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:18023,Availability,error,error,18023," ```; root@test-batch:/# curl -H ""Authorization: Bearer $(cat test-jwt/jwt)"" batch.pr-6604-default-ov5tgx24rrou/api/v1alpha/batches/9; {""id"": 9, ""state"": ""running"", ""complete"": false, ""closed"": 1, ""jobs"": [{""batch_id"": 9, ""job_id"": 1, ""state"": ""Running""}; ```. The batch is somehow in the state ""running"" and the only job is also in the; state ""Running"". Only two lines of code transition to the state 'Running'. They; both appear in the suffix of `Job._create_pod`:. ```python; pod, err = await app['k8s'].create_pod(body=pod_template); if err is not None:; if err.status == 409:; log.info(f'pod already exists for job {self.full_id}'); n_updated = await db.jobs.update_record(*self.id, compare_items={'state': self._state}, state='Running'); if n_updated == 0:; log.warning(f'changing the state for job {self.full_id} failed due to the expected state {self._state} not in db'); return; traceback.print_tb(err.__traceback__); log.info(f'pod creation failed for job {self.full_id} '; f'with the following error: {err}'); return. n_updated = await db.jobs.update_record(*self.id, compare_items={'state': self._state}, state='Running'); if n_updated == 0:; log.warning(f'changing the state for job {self.full_id} failed due to the expected state {self._state} not in db'); ```. For either of these database updates to succeed, the thread of control must have; thought the `_state` was `Cancelled` or we moved through some intermediate; state. We continue under the assumption that we went directly to `Running`. Who calls `_create_pod`?. - `start_pod`, but it checks that the state is in `Ready`; - `mark_complete`, but that's only if there's a ""next task"", this job has only; one task. That leaves `create_if_ready` and `mark_unscheduled`. `create_if_ready` is only; called by methods that are triggered when a parent with children finishes. We; have no parent-child relationships here. By process of elimination, `mark_unscheduled` must be the culprit. But how?; `mark_unscheduled` is called when a p",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:1518,Deployability,PATCH,PATCH,1518,"ber 9. There is one job, its number is 1. Here's a snippet of the logs around when batch 9 is created:. ```; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,933"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""create_jobs:986"", ""message"": ""created 1 jobs for batch 9""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,934"", ""filename"": ""web_log.py"", ""funcNameAndLine"": ""log:233"", ""message"": ""10.32.4.199 [11/Jul/2019:14:19:34 +0000] \""POST /api/v1alpha/batches/9/jobs/create HTTP/1.1\"" 200 158 \""-\"" \""Python/3.6 aiohttp/3.5.4\"""", ""remote_address"": ""10.32.4.199"", ""request_start_time"": ""[11/Jul/2019:14:19:34 +0000]"", ""first_request_line"": ""POST /api/v1alpha/batches/9/jobs/create HTTP/1.1"", ""response_status"": 200, ""response_size"": 158, ""request_header"": {""Referer"": ""-"", ""User-Agent"": ""Python/3.6 aiohttp/3.5.4""}}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,945"", ""filename"": ""web_log.py"", ""funcNameAndLine"": ""log:233"", ""message"": ""10.32.4.199 [11/Jul/2019:14:19:34 +0000] \""PATCH /api/v1alpha/batches/9/close HTTP/1.1\"" 200 158 \""-\"" \""Python/3.6 aiohttp/3.5.4\"""", ""remote_address"": ""10.32.4.199"", ""request_start_time"": ""[11/Jul/2019:14:19:34 +0000]"", ""first_request_line"": ""PATCH /api/v1alpha/batches/9/close HTTP/1.1"", ""response_status"": 200, ""response_size"": 158, ""request_header"": {""Referer"": ""-"", ""User-Agent"": ""Python/3.6 aiohttp/3.5.4""}}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,957"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""cancel:862"", ""message"": ""batch 9 cancelled""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,958"", ""filename"": ""web_log.py"", ""funcNameAndLine"": ""log:233"", ""message"": ""10.32.4.199 [11/Jul/2019:14:19:34 +0000] \""PATCH /api/v1alpha/batches/9/cancel HTTP/1.1\"" 200 158 \""-\"" \""Python/3.6 aiohttp/3.5.4\"""", ""remote_address"": ""10.32.4.199"", ""request_start_time"": ""[11/Jul/2019:14:19:34 +0000]"", ""first_request_line"": ""PATCH /api/v1alpha/batches/9/cancel HTTP/1.1"", ""response_status"": 200, ""response_size"": 158, ""request_header""",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:1719,Deployability,PATCH,PATCH,1719,"e"": ""batch.py"", ""funcNameAndLine"": ""create_jobs:986"", ""message"": ""created 1 jobs for batch 9""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,934"", ""filename"": ""web_log.py"", ""funcNameAndLine"": ""log:233"", ""message"": ""10.32.4.199 [11/Jul/2019:14:19:34 +0000] \""POST /api/v1alpha/batches/9/jobs/create HTTP/1.1\"" 200 158 \""-\"" \""Python/3.6 aiohttp/3.5.4\"""", ""remote_address"": ""10.32.4.199"", ""request_start_time"": ""[11/Jul/2019:14:19:34 +0000]"", ""first_request_line"": ""POST /api/v1alpha/batches/9/jobs/create HTTP/1.1"", ""response_status"": 200, ""response_size"": 158, ""request_header"": {""Referer"": ""-"", ""User-Agent"": ""Python/3.6 aiohttp/3.5.4""}}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,945"", ""filename"": ""web_log.py"", ""funcNameAndLine"": ""log:233"", ""message"": ""10.32.4.199 [11/Jul/2019:14:19:34 +0000] \""PATCH /api/v1alpha/batches/9/close HTTP/1.1\"" 200 158 \""-\"" \""Python/3.6 aiohttp/3.5.4\"""", ""remote_address"": ""10.32.4.199"", ""request_start_time"": ""[11/Jul/2019:14:19:34 +0000]"", ""first_request_line"": ""PATCH /api/v1alpha/batches/9/close HTTP/1.1"", ""response_status"": 200, ""response_size"": 158, ""request_header"": {""Referer"": ""-"", ""User-Agent"": ""Python/3.6 aiohttp/3.5.4""}}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,957"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""cancel:862"", ""message"": ""batch 9 cancelled""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,958"", ""filename"": ""web_log.py"", ""funcNameAndLine"": ""log:233"", ""message"": ""10.32.4.199 [11/Jul/2019:14:19:34 +0000] \""PATCH /api/v1alpha/batches/9/cancel HTTP/1.1\"" 200 158 \""-\"" \""Python/3.6 aiohttp/3.5.4\"""", ""remote_address"": ""10.32.4.199"", ""request_start_time"": ""[11/Jul/2019:14:19:34 +0000]"", ""first_request_line"": ""PATCH /api/v1alpha/batches/9/cancel HTTP/1.1"", ""response_status"": 200, ""response_size"": 158, ""request_header"": {""Referer"": ""-"", ""User-Agent"": ""Python/3.6 aiohttp/3.5.4""}}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,967"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:2211,Deployability,PATCH,PATCH,2211,"/1.1"", ""response_status"": 200, ""response_size"": 158, ""request_header"": {""Referer"": ""-"", ""User-Agent"": ""Python/3.6 aiohttp/3.5.4""}}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,945"", ""filename"": ""web_log.py"", ""funcNameAndLine"": ""log:233"", ""message"": ""10.32.4.199 [11/Jul/2019:14:19:34 +0000] \""PATCH /api/v1alpha/batches/9/close HTTP/1.1\"" 200 158 \""-\"" \""Python/3.6 aiohttp/3.5.4\"""", ""remote_address"": ""10.32.4.199"", ""request_start_time"": ""[11/Jul/2019:14:19:34 +0000]"", ""first_request_line"": ""PATCH /api/v1alpha/batches/9/close HTTP/1.1"", ""response_status"": 200, ""response_size"": 158, ""request_header"": {""Referer"": ""-"", ""User-Agent"": ""Python/3.6 aiohttp/3.5.4""}}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,957"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""cancel:862"", ""message"": ""batch 9 cancelled""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,958"", ""filename"": ""web_log.py"", ""funcNameAndLine"": ""log:233"", ""message"": ""10.32.4.199 [11/Jul/2019:14:19:34 +0000] \""PATCH /api/v1alpha/batches/9/cancel HTTP/1.1\"" 200 158 \""-\"" \""Python/3.6 aiohttp/3.5.4\"""", ""remote_address"": ""10.32.4.199"", ""request_start_time"": ""[11/Jul/2019:14:19:34 +0000]"", ""first_request_line"": ""PATCH /api/v1alpha/batches/9/cancel HTTP/1.1"", ""response_status"": 200, ""response_size"": 158, ""request_header"": {""Referer"": ""-"", ""User-Agent"": ""Python/3.6 aiohttp/3.5.4""}}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,967"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (9, 1, 'main') with pod batch-9-job-1-c8b9b2""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,969"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""set_state:501"", ""message"": ""job (9, 1, 'main') changed state: Ready -> Cancelled""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,974"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""_delete_pvc:251"", ""message"": ""deleting persistent volume claim batch-9-job-1-c8b9b2""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:1",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:2413,Deployability,PATCH,PATCH,2413,"4:19:34,945"", ""filename"": ""web_log.py"", ""funcNameAndLine"": ""log:233"", ""message"": ""10.32.4.199 [11/Jul/2019:14:19:34 +0000] \""PATCH /api/v1alpha/batches/9/close HTTP/1.1\"" 200 158 \""-\"" \""Python/3.6 aiohttp/3.5.4\"""", ""remote_address"": ""10.32.4.199"", ""request_start_time"": ""[11/Jul/2019:14:19:34 +0000]"", ""first_request_line"": ""PATCH /api/v1alpha/batches/9/close HTTP/1.1"", ""response_status"": 200, ""response_size"": 158, ""request_header"": {""Referer"": ""-"", ""User-Agent"": ""Python/3.6 aiohttp/3.5.4""}}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,957"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""cancel:862"", ""message"": ""batch 9 cancelled""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,958"", ""filename"": ""web_log.py"", ""funcNameAndLine"": ""log:233"", ""message"": ""10.32.4.199 [11/Jul/2019:14:19:34 +0000] \""PATCH /api/v1alpha/batches/9/cancel HTTP/1.1\"" 200 158 \""-\"" \""Python/3.6 aiohttp/3.5.4\"""", ""remote_address"": ""10.32.4.199"", ""request_start_time"": ""[11/Jul/2019:14:19:34 +0000]"", ""first_request_line"": ""PATCH /api/v1alpha/batches/9/cancel HTTP/1.1"", ""response_status"": 200, ""response_size"": 158, ""request_header"": {""Referer"": ""-"", ""User-Agent"": ""Python/3.6 aiohttp/3.5.4""}}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,967"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (9, 1, 'main') with pod batch-9-job-1-c8b9b2""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,969"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""set_state:501"", ""message"": ""job (9, 1, 'main') changed state: Ready -> Cancelled""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,974"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""_delete_pvc:251"", ""message"": ""deleting persistent volume claim batch-9-job-1-c8b9b2""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,976"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (9, 1, 'main') with pod batch-9-job-1-c8b9b2""}; {""levelname"": ""INFO"", ""as",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:2728,Deployability,update,update,2728,"""response_status"": 200, ""response_size"": 158, ""request_header"": {""Referer"": ""-"", ""User-Agent"": ""Python/3.6 aiohttp/3.5.4""}}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,957"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""cancel:862"", ""message"": ""batch 9 cancelled""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,958"", ""filename"": ""web_log.py"", ""funcNameAndLine"": ""log:233"", ""message"": ""10.32.4.199 [11/Jul/2019:14:19:34 +0000] \""PATCH /api/v1alpha/batches/9/cancel HTTP/1.1\"" 200 158 \""-\"" \""Python/3.6 aiohttp/3.5.4\"""", ""remote_address"": ""10.32.4.199"", ""request_start_time"": ""[11/Jul/2019:14:19:34 +0000]"", ""first_request_line"": ""PATCH /api/v1alpha/batches/9/cancel HTTP/1.1"", ""response_status"": 200, ""response_size"": 158, ""request_header"": {""Referer"": ""-"", ""User-Agent"": ""Python/3.6 aiohttp/3.5.4""}}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,967"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (9, 1, 'main') with pod batch-9-job-1-c8b9b2""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,969"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""set_state:501"", ""message"": ""job (9, 1, 'main') changed state: Ready -> Cancelled""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,974"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""_delete_pvc:251"", ""message"": ""deleting persistent volume claim batch-9-job-1-c8b9b2""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,976"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (9, 1, 'main') with pod batch-9-job-1-c8b9b2""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,977"", ""filename"": ""web_log.py"", ""funcNameAndLine"": ""log:233"", ""message"": ""10.32.4.199 [11/Jul/2019:14:19:34 +0000] \""GET /api/v1alpha/batches/9 HTTP/1.1\"" 200 279 \""-\"" \""Python/3.6 aiohttp/3.5.4\"""", ""remote_address"": ""10.32.4.199"", ""request_start_time"": ""[11/Jul/2019:14:19:34 +0000]"", ""first_request_line"": ""GET /api/v1alpha/batches/9 HTTP/1",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:3309,Deployability,update,update,3309,"art_time"": ""[11/Jul/2019:14:19:34 +0000]"", ""first_request_line"": ""PATCH /api/v1alpha/batches/9/cancel HTTP/1.1"", ""response_status"": 200, ""response_size"": 158, ""request_header"": {""Referer"": ""-"", ""User-Agent"": ""Python/3.6 aiohttp/3.5.4""}}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,967"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (9, 1, 'main') with pod batch-9-job-1-c8b9b2""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,969"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""set_state:501"", ""message"": ""job (9, 1, 'main') changed state: Ready -> Cancelled""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,974"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""_delete_pvc:251"", ""message"": ""deleting persistent volume claim batch-9-job-1-c8b9b2""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,976"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (9, 1, 'main') with pod batch-9-job-1-c8b9b2""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,977"", ""filename"": ""web_log.py"", ""funcNameAndLine"": ""log:233"", ""message"": ""10.32.4.199 [11/Jul/2019:14:19:34 +0000] \""GET /api/v1alpha/batches/9 HTTP/1.1\"" 200 279 \""-\"" \""Python/3.6 aiohttp/3.5.4\"""", ""remote_address"": ""10.32.4.199"", ""request_start_time"": ""[11/Jul/2019:14:19:34 +0000]"", ""first_request_line"": ""GET /api/v1alpha/batches/9 HTTP/1.1"", ""response_status"": 200, ""response_size"": 279, ""request_header"": {""Referer"": ""-"", ""User-Agent"": ""Python/3.6 aiohttp/3.5.4""}}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,985"", ""filename"": ""web_log.py"", ""funcNameAndLine"": ""log:233"", ""message"": ""10.32.4.199 [11/Jul/2019:14:19:34 +0000] \""GET /api/v1alpha/batches/9 HTTP/1.1\"" 200 279 \""-\"" \""Python/3.6 aiohttp/3.5.4\"""", ""remote_address"": ""10.32.4.199"", ""request_start_time"": ""[11/Jul/2019:14:19:34 +0000]"", ""first_request_line"": ""GET /api/v1alpha/batches/9 HTTP/1.1"", ""response_status"": 200, ""response_size"": 279, ""req",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:4565,Deployability,update,update,4565,"9 HTTP/1.1\"" 200 279 \""-\"" \""Python/3.6 aiohttp/3.5.4\"""", ""remote_address"": ""10.32.4.199"", ""request_start_time"": ""[11/Jul/2019:14:19:34 +0000]"", ""first_request_line"": ""GET /api/v1alpha/batches/9 HTTP/1.1"", ""response_status"": 200, ""response_size"": 279, ""request_header"": {""Referer"": ""-"", ""User-Agent"": ""Python/3.6 aiohttp/3.5.4""}}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,985"", ""filename"": ""web_log.py"", ""funcNameAndLine"": ""log:233"", ""message"": ""10.32.4.199 [11/Jul/2019:14:19:34 +0000] \""GET /api/v1alpha/batches/9 HTTP/1.1\"" 200 279 \""-\"" \""Python/3.6 aiohttp/3.5.4\"""", ""remote_address"": ""10.32.4.199"", ""request_start_time"": ""[11/Jul/2019:14:19:34 +0000]"", ""first_request_line"": ""GET /api/v1alpha/batches/9 HTTP/1.1"", ""response_status"": 200, ""response_size"": 279, ""request_header"": {""Referer"": ""-"", ""User-Agent"": ""Python/3.6 aiohttp/3.5.4""}}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,991"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (9, 1, 'main') with pod batch-9-job-1-c8b9b2""}; ```. Recall that pod creation happens in the background, so the `batches/9/create` and `batches/9/close` endpoints return to the client before pods are necessarily created. See batch.py:866, `Batch.close`. Likewise, the `batches/9/cancel` endpoint returns before the individual jobs are cancelled. There are now at most three concurrent threads of control interacting with the database and k8s. Eventually this sequence of log messages appears three times in quick succession. ```; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:39,890"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (9, 1, 'main') with pod batch-9-job-1-c8b9b2""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:39,890"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1159"", ""message"": ""job (9, 1, 'main') mark complete""}; {""levelname"": ""WARNING"", ""asctime"": ""2019-07-11 14:19:39,899"", ""filename"": ""ba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:5249,Deployability,update,update,5249,"HTTP/1.1"", ""response_status"": 200, ""response_size"": 279, ""request_header"": {""Referer"": ""-"", ""User-Agent"": ""Python/3.6 aiohttp/3.5.4""}}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,991"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (9, 1, 'main') with pod batch-9-job-1-c8b9b2""}; ```. Recall that pod creation happens in the background, so the `batches/9/create` and `batches/9/close` endpoints return to the client before pods are necessarily created. See batch.py:866, `Batch.close`. Likewise, the `batches/9/cancel` endpoint returns before the individual jobs are cancelled. There are now at most three concurrent threads of control interacting with the database and k8s. Eventually this sequence of log messages appears three times in quick succession. ```; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:39,890"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (9, 1, 'main') with pod batch-9-job-1-c8b9b2""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:39,890"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1159"", ""message"": ""job (9, 1, 'main') mark complete""}; {""levelname"": ""WARNING"", ""asctime"": ""2019-07-11 14:19:39,899"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""mark_complete:579"", ""message"": ""job (9, 1, 'main') has pod batch-9-job-1-c8b9b2 which is terminated but has no timing information. {'api_version': 'v1',\n 'kind': 'Pod',\n 'metadata': {'annotations': None,\n 'cluster_name': None,\n 'creation_timestamp': datetime.datetime(2019, 7, 11, 14, 19, 34, tzinfo=tzlocal()),\n 'deletion_grace_period_seconds': 30,\n 'deletion_timestamp': datetime.datetime(2019, 7, 11, 14, 20, 4, tzinfo=tzlocal()),\n 'finalizers': None,\n 'generate_name': None,\n 'generation': None,\n 'initializers': None,\n 'labels': {'app': 'batch-job',\n 'batch_id': '9',\n 'hail.is/batch-instance': 'ffa5abc4607849df8e5f0036e7350bcf',\n 'job_id': '1',\n 'task': 'main',\n 'user'",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:15241,Deployability,update,update,15241,"': '6a744afc-154f-4b48-b4bb-51a15078d999', 'Content-Type': 'application/json', 'Date': 'Thu, 11 Jul 2019 14:19:39 GMT', 'Content-Length': '179'})\nHTTP response body: {\""kind\"":\""Status\"",\""apiVersion\"":\""v1\"",\""metadata\"":{},\""status\"":\""Failure\"",\""message\"":\""container \\\""main\\\"" in pod \\\""batch-9-job-1-c8b9b2\\\"" is terminated\"",\""reason\"":\""BadRequest\"",\""code\"":400}\n\n""}; ```. And finally, this k8s refresh loop sequence repeats until CI kills the tests due to a timeout. ```; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,070"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_state:1261"", ""message"": ""started k8s state refresh""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,085"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_pods:1210"", ""message"": ""k8s had 3 pods""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,088"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (11, 1, 'output') with pod batch-11-job-1-4f1118""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,090"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (11, 2, 'output') with pod batch-11-job-2-ad1587""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,093"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (11, 3, 'output') with pod batch-11-job-3-d826dd""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,093"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_pods:1221"", ""message"": ""restarting ready and running jobs with pods not seen in k8s""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,093"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_pods:1225"", ""message"": ""restarting job (9, 1, 'main')""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,093"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (9, 1, 'main') with pod None""}; {""",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:15447,Deployability,update,update,15447,"1\"",\""metadata\"":{},\""status\"":\""Failure\"",\""message\"":\""container \\\""main\\\"" in pod \\\""batch-9-job-1-c8b9b2\\\"" is terminated\"",\""reason\"":\""BadRequest\"",\""code\"":400}\n\n""}; ```. And finally, this k8s refresh loop sequence repeats until CI kills the tests due to a timeout. ```; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,070"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_state:1261"", ""message"": ""started k8s state refresh""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,085"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_pods:1210"", ""message"": ""k8s had 3 pods""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,088"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (11, 1, 'output') with pod batch-11-job-1-4f1118""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,090"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (11, 2, 'output') with pod batch-11-job-2-ad1587""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,093"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (11, 3, 'output') with pod batch-11-job-3-d826dd""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,093"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_pods:1221"", ""message"": ""restarting ready and running jobs with pods not seen in k8s""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,093"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_pods:1225"", ""message"": ""restarting job (9, 1, 'main')""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,093"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (9, 1, 'main') with pod None""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,093"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1148"", ""message"": ""job (9, 1, 'main') mark unscheduled""}; {""levelname"": ""INFO"", ""asc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:15653,Deployability,update,update,15653,"refresh loop sequence repeats until CI kills the tests due to a timeout. ```; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,070"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_state:1261"", ""message"": ""started k8s state refresh""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,085"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_pods:1210"", ""message"": ""k8s had 3 pods""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,088"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (11, 1, 'output') with pod batch-11-job-1-4f1118""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,090"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (11, 2, 'output') with pod batch-11-job-2-ad1587""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,093"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (11, 3, 'output') with pod batch-11-job-3-d826dd""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,093"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_pods:1221"", ""message"": ""restarting ready and running jobs with pods not seen in k8s""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,093"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_pods:1225"", ""message"": ""restarting job (9, 1, 'main')""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,093"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (9, 1, 'main') with pod None""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,093"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1148"", ""message"": ""job (9, 1, 'main') mark unscheduled""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,106"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_pvc:1237"", ""message"": ""k8s had 3 pvcs""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,113"", ""filename"": ""ba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:16235,Deployability,update,update,16235,"put') with pod batch-11-job-1-4f1118""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,090"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (11, 2, 'output') with pod batch-11-job-2-ad1587""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,093"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (11, 3, 'output') with pod batch-11-job-3-d826dd""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,093"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_pods:1221"", ""message"": ""restarting ready and running jobs with pods not seen in k8s""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,093"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_pods:1225"", ""message"": ""restarting job (9, 1, 'main')""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,093"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (9, 1, 'main') with pod None""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,093"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1148"", ""message"": ""job (9, 1, 'main') mark unscheduled""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,106"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_pvc:1237"", ""message"": ""k8s had 3 pvcs""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,113"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_state:1264"", ""message"": ""k8s state refresh complete""}; ```. Because job 1's batch is cancelled, we never restart the job/pod. However, the batch is stuck in an incomplete state. Before this PR's namespace was destroyed, I SSH'ed to the test_batch pod and looked at the batch's status:. ```; root@test-batch:/# curl -H ""Authorization: Bearer $(cat test-jwt/jwt)"" batch.pr-6604-default-ov5tgx24rrou/api/v1alpha/batches/9; {""id"": 9, ""state"": ""running"", ""complete"": false, ""closed"": 1, ""jobs"": [{""batch_id"": 9, ""job_id"": 1, ""state"": ""R",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:18323,Deployability,update,updates,18323,"ly two lines of code transition to the state 'Running'. They; both appear in the suffix of `Job._create_pod`:. ```python; pod, err = await app['k8s'].create_pod(body=pod_template); if err is not None:; if err.status == 409:; log.info(f'pod already exists for job {self.full_id}'); n_updated = await db.jobs.update_record(*self.id, compare_items={'state': self._state}, state='Running'); if n_updated == 0:; log.warning(f'changing the state for job {self.full_id} failed due to the expected state {self._state} not in db'); return; traceback.print_tb(err.__traceback__); log.info(f'pod creation failed for job {self.full_id} '; f'with the following error: {err}'); return. n_updated = await db.jobs.update_record(*self.id, compare_items={'state': self._state}, state='Running'); if n_updated == 0:; log.warning(f'changing the state for job {self.full_id} failed due to the expected state {self._state} not in db'); ```. For either of these database updates to succeed, the thread of control must have; thought the `_state` was `Cancelled` or we moved through some intermediate; state. We continue under the assumption that we went directly to `Running`. Who calls `_create_pod`?. - `start_pod`, but it checks that the state is in `Ready`; - `mark_complete`, but that's only if there's a ""next task"", this job has only; one task. That leaves `create_if_ready` and `mark_unscheduled`. `create_if_ready` is only; called by methods that are triggered when a parent with children finishes. We; have no parent-child relationships here. By process of elimination, `mark_unscheduled` must be the culprit. But how?; `mark_unscheduled` is called when a pod is evicted or by the k8s update loop if; there exists no pod. In those cases a message a special log is printed. That log; appears later (because the pod is missing) but it does not appear during the; initial sequence of events. Let's set that aside and focus on the other path by; which `mark_unscheduled` is called: `mark_complete` when the pod log cann",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:19046,Deployability,update,update,19046,"n failed for job {self.full_id} '; f'with the following error: {err}'); return. n_updated = await db.jobs.update_record(*self.id, compare_items={'state': self._state}, state='Running'); if n_updated == 0:; log.warning(f'changing the state for job {self.full_id} failed due to the expected state {self._state} not in db'); ```. For either of these database updates to succeed, the thread of control must have; thought the `_state` was `Cancelled` or we moved through some intermediate; state. We continue under the assumption that we went directly to `Running`. Who calls `_create_pod`?. - `start_pod`, but it checks that the state is in `Ready`; - `mark_complete`, but that's only if there's a ""next task"", this job has only; one task. That leaves `create_if_ready` and `mark_unscheduled`. `create_if_ready` is only; called by methods that are triggered when a parent with children finishes. We; have no parent-child relationships here. By process of elimination, `mark_unscheduled` must be the culprit. But how?; `mark_unscheduled` is called when a pod is evicted or by the k8s update loop if; there exists no pod. In those cases a message a special log is printed. That log; appears later (because the pod is missing) but it does not appear during the; initial sequence of events. Let's set that aside and focus on the other path by; which `mark_unscheduled` is called: `mark_complete` when the pod log cannot be; retrieved. Proposed sequence of events:. - pod is created; - k8s sends an event that the pod is terminated (but without timing information); - we load a Job object from the db. the job is Pending, the batch is cancelled; - we enter `update_job_with_pod` then `mark_complete`; - we fail to retrieve the logs; - we try to mark unscheduled, but the batch is cancelled so we do nothing.; ...; - we realize a Pending pod was never created, we try to mark_unscheduled, but; the batch is cancelled, so we do nothing; ...; - we realize a Pending pod was never created, we try ... ad infinitum",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:8544,Energy Efficiency,schedul,scheduler,8544,"ry': '500M'}},\n 'security_context': None,\n 'stdin': None,\n 'stdin_once': None,\n 'termination_message_path': '/dev/termination-log',\n 'termination_message_policy': 'File',\n 'tty': None,\n 'volume_devices': None,\n 'volume_mounts': [{'mount_path': '/gsa-key',\n 'mount_propagation': None,\n 'name': 'gsa-key',\n 'read_only': None,\n 'sub_path': None},\n {'mount_path': '/var/run/secrets/kubernetes.io/serviceaccount',\n 'mount_propagation': None,\n 'name': 'default-token-brr2f',\n 'read_only': True,\n 'sub_path': None}],\n 'working_dir': None}],\n 'dns_config': None,\n 'dns_policy': 'ClusterFirst',\n 'enable_service_links': True,\n 'host_aliases': None,\n 'host_ipc': None,\n 'host_network': None,\n 'host_pid': None,\n 'hostname': None,\n 'image_pull_secrets': None,\n 'init_containers': None,\n 'node_name': 'gke-vdc-preemptible-pool-9c7148b2-8hq5',\n 'node_selector': None,\n 'priority': 500000,\n 'priority_class_name': 'user',\n 'readiness_gates': None,\n 'restart_policy': 'Never',\n 'runtime_class_name': None,\n 'scheduler_name': 'default-scheduler',\n 'security_context': {'fs_group': None,\n 'run_as_group': None,\n 'run_as_non_root': None,\n 'run_as_user': None,\n 'se_linux_options': None,\n 'supplemental_groups': None,\n 'sysctls': None},\n 'service_account': 'default',\n 'service_account_name': 'default',\n 'share_process_namespace': None,\n 'subdomain': None,\n 'termination_grace_period_seconds': 30,\n 'tolerations': [{'effect': None,\n 'key': 'preemptible',\n 'operator': None,\n 'toleration_seconds': None,\n 'value': 'true'},\n {'effect': 'NoExecute',\n 'key': 'node.kubernetes.io/not-ready',\n 'operator': 'Exists',\n 'toleration_seconds': 300,\n 'value': None},\n {'effect': 'NoExecute',\n 'key': 'node.kubernetes.io/unreachable',\n 'operator': 'Exists',\n 'toleration_seconds': 300,\n 'value': None}],\n 'volumes': [{'aws_elastic_block_store': None,\n 'azure_disk': None,\n 'azure_file': None,\n 'cephfs': None,\n 'cinder': None,\n 'config_map': None,\n 'downward_ap",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:753,Integrability,message,message,753,"[the logs for PR 6604, batch 2414, job 78, `test_batch`](https://github.com/hail-is/hail/files/3384019/job-78-log.txt). `test_batch` has been flaky lately. This is one particular failing PR. There seems to be an issue where the batch created by the following script hangs forever:. ```python; b4 = self.client.create_batch(); b4.create_job('alpine', ['sleep', '30']); b4 = b4.submit(); b4.cancel(); b4.wait(); b4s = b4.status(); assert b4s['complete'] and b4s['state'] == 'cancelled', b4s; ```. This batch is assigned number 9. There is one job, its number is 1. Here's a snippet of the logs around when batch 9 is created:. ```; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,933"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""create_jobs:986"", ""message"": ""created 1 jobs for batch 9""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,934"", ""filename"": ""web_log.py"", ""funcNameAndLine"": ""log:233"", ""message"": ""10.32.4.199 [11/Jul/2019:14:19:34 +0000] \""POST /api/v1alpha/batches/9/jobs/create HTTP/1.1\"" 200 158 \""-\"" \""Python/3.6 aiohttp/3.5.4\"""", ""remote_address"": ""10.32.4.199"", ""request_start_time"": ""[11/Jul/2019:14:19:34 +0000]"", ""first_request_line"": ""POST /api/v1alpha/batches/9/jobs/create HTTP/1.1"", ""response_status"": 200, ""response_size"": 158, ""request_header"": {""Referer"": ""-"", ""User-Agent"": ""Python/3.6 aiohttp/3.5.4""}}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,945"", ""filename"": ""web_log.py"", ""funcNameAndLine"": ""log:233"", ""message"": ""10.32.4.199 [11/Jul/2019:14:19:34 +0000] \""PATCH /api/v1alpha/batches/9/close HTTP/1.1\"" 200 158 \""-\"" \""Python/3.6 aiohttp/3.5.4\"""", ""remote_address"": ""10.32.4.199"", ""request_start_time"": ""[11/Jul/2019:14:19:34 +0000]"", ""first_request_line"": ""PATCH /api/v1alpha/batches/9/close HTTP/1.1"", ""response_status"": 200, ""response_size"": 158, ""request_header"": {""Referer"": ""-"", ""User-Agent"": ""Python/3.6 aiohttp/3.5.4""}}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,957"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""cancel:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:911,Integrability,message,message,911,"[the logs for PR 6604, batch 2414, job 78, `test_batch`](https://github.com/hail-is/hail/files/3384019/job-78-log.txt). `test_batch` has been flaky lately. This is one particular failing PR. There seems to be an issue where the batch created by the following script hangs forever:. ```python; b4 = self.client.create_batch(); b4.create_job('alpine', ['sleep', '30']); b4 = b4.submit(); b4.cancel(); b4.wait(); b4s = b4.status(); assert b4s['complete'] and b4s['state'] == 'cancelled', b4s; ```. This batch is assigned number 9. There is one job, its number is 1. Here's a snippet of the logs around when batch 9 is created:. ```; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,933"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""create_jobs:986"", ""message"": ""created 1 jobs for batch 9""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,934"", ""filename"": ""web_log.py"", ""funcNameAndLine"": ""log:233"", ""message"": ""10.32.4.199 [11/Jul/2019:14:19:34 +0000] \""POST /api/v1alpha/batches/9/jobs/create HTTP/1.1\"" 200 158 \""-\"" \""Python/3.6 aiohttp/3.5.4\"""", ""remote_address"": ""10.32.4.199"", ""request_start_time"": ""[11/Jul/2019:14:19:34 +0000]"", ""first_request_line"": ""POST /api/v1alpha/batches/9/jobs/create HTTP/1.1"", ""response_status"": 200, ""response_size"": 158, ""request_header"": {""Referer"": ""-"", ""User-Agent"": ""Python/3.6 aiohttp/3.5.4""}}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,945"", ""filename"": ""web_log.py"", ""funcNameAndLine"": ""log:233"", ""message"": ""10.32.4.199 [11/Jul/2019:14:19:34 +0000] \""PATCH /api/v1alpha/batches/9/close HTTP/1.1\"" 200 158 \""-\"" \""Python/3.6 aiohttp/3.5.4\"""", ""remote_address"": ""10.32.4.199"", ""request_start_time"": ""[11/Jul/2019:14:19:34 +0000]"", ""first_request_line"": ""PATCH /api/v1alpha/batches/9/close HTTP/1.1"", ""response_status"": 200, ""response_size"": 158, ""request_header"": {""Referer"": ""-"", ""User-Agent"": ""Python/3.6 aiohttp/3.5.4""}}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,957"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""cancel:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:1464,Integrability,message,message,1464,"nd b4s['state'] == 'cancelled', b4s; ```. This batch is assigned number 9. There is one job, its number is 1. Here's a snippet of the logs around when batch 9 is created:. ```; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,933"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""create_jobs:986"", ""message"": ""created 1 jobs for batch 9""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,934"", ""filename"": ""web_log.py"", ""funcNameAndLine"": ""log:233"", ""message"": ""10.32.4.199 [11/Jul/2019:14:19:34 +0000] \""POST /api/v1alpha/batches/9/jobs/create HTTP/1.1\"" 200 158 \""-\"" \""Python/3.6 aiohttp/3.5.4\"""", ""remote_address"": ""10.32.4.199"", ""request_start_time"": ""[11/Jul/2019:14:19:34 +0000]"", ""first_request_line"": ""POST /api/v1alpha/batches/9/jobs/create HTTP/1.1"", ""response_status"": 200, ""response_size"": 158, ""request_header"": {""Referer"": ""-"", ""User-Agent"": ""Python/3.6 aiohttp/3.5.4""}}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,945"", ""filename"": ""web_log.py"", ""funcNameAndLine"": ""log:233"", ""message"": ""10.32.4.199 [11/Jul/2019:14:19:34 +0000] \""PATCH /api/v1alpha/batches/9/close HTTP/1.1\"" 200 158 \""-\"" \""Python/3.6 aiohttp/3.5.4\"""", ""remote_address"": ""10.32.4.199"", ""request_start_time"": ""[11/Jul/2019:14:19:34 +0000]"", ""first_request_line"": ""PATCH /api/v1alpha/batches/9/close HTTP/1.1"", ""response_status"": 200, ""response_size"": 158, ""request_header"": {""Referer"": ""-"", ""User-Agent"": ""Python/3.6 aiohttp/3.5.4""}}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,957"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""cancel:862"", ""message"": ""batch 9 cancelled""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,958"", ""filename"": ""web_log.py"", ""funcNameAndLine"": ""log:233"", ""message"": ""10.32.4.199 [11/Jul/2019:14:19:34 +0000] \""PATCH /api/v1alpha/batches/9/cancel HTTP/1.1\"" 200 158 \""-\"" \""Python/3.6 aiohttp/3.5.4\"""", ""remote_address"": ""10.32.4.199"", ""request_start_time"": ""[11/Jul/2019:14:19:34 +0000]"", ""first_request_line"": ""PATCH /api/v1alpha/batches/9/cancel HTTP",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:2008,Integrability,message,message,2008,"ohttp/3.5.4\"""", ""remote_address"": ""10.32.4.199"", ""request_start_time"": ""[11/Jul/2019:14:19:34 +0000]"", ""first_request_line"": ""POST /api/v1alpha/batches/9/jobs/create HTTP/1.1"", ""response_status"": 200, ""response_size"": 158, ""request_header"": {""Referer"": ""-"", ""User-Agent"": ""Python/3.6 aiohttp/3.5.4""}}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,945"", ""filename"": ""web_log.py"", ""funcNameAndLine"": ""log:233"", ""message"": ""10.32.4.199 [11/Jul/2019:14:19:34 +0000] \""PATCH /api/v1alpha/batches/9/close HTTP/1.1\"" 200 158 \""-\"" \""Python/3.6 aiohttp/3.5.4\"""", ""remote_address"": ""10.32.4.199"", ""request_start_time"": ""[11/Jul/2019:14:19:34 +0000]"", ""first_request_line"": ""PATCH /api/v1alpha/batches/9/close HTTP/1.1"", ""response_status"": 200, ""response_size"": 158, ""request_header"": {""Referer"": ""-"", ""User-Agent"": ""Python/3.6 aiohttp/3.5.4""}}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,957"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""cancel:862"", ""message"": ""batch 9 cancelled""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,958"", ""filename"": ""web_log.py"", ""funcNameAndLine"": ""log:233"", ""message"": ""10.32.4.199 [11/Jul/2019:14:19:34 +0000] \""PATCH /api/v1alpha/batches/9/cancel HTTP/1.1\"" 200 158 \""-\"" \""Python/3.6 aiohttp/3.5.4\"""", ""remote_address"": ""10.32.4.199"", ""request_start_time"": ""[11/Jul/2019:14:19:34 +0000]"", ""first_request_line"": ""PATCH /api/v1alpha/batches/9/cancel HTTP/1.1"", ""response_status"": 200, ""response_size"": 158, ""request_header"": {""Referer"": ""-"", ""User-Agent"": ""Python/3.6 aiohttp/3.5.4""}}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,967"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (9, 1, 'main') with pod batch-9-job-1-c8b9b2""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,969"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""set_state:501"", ""message"": ""job (9, 1, 'main') changed state: Ready -> Cancelled""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,974"", ""filename""",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:2157,Integrability,message,message,2157,", ""first_request_line"": ""POST /api/v1alpha/batches/9/jobs/create HTTP/1.1"", ""response_status"": 200, ""response_size"": 158, ""request_header"": {""Referer"": ""-"", ""User-Agent"": ""Python/3.6 aiohttp/3.5.4""}}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,945"", ""filename"": ""web_log.py"", ""funcNameAndLine"": ""log:233"", ""message"": ""10.32.4.199 [11/Jul/2019:14:19:34 +0000] \""PATCH /api/v1alpha/batches/9/close HTTP/1.1\"" 200 158 \""-\"" \""Python/3.6 aiohttp/3.5.4\"""", ""remote_address"": ""10.32.4.199"", ""request_start_time"": ""[11/Jul/2019:14:19:34 +0000]"", ""first_request_line"": ""PATCH /api/v1alpha/batches/9/close HTTP/1.1"", ""response_status"": 200, ""response_size"": 158, ""request_header"": {""Referer"": ""-"", ""User-Agent"": ""Python/3.6 aiohttp/3.5.4""}}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,957"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""cancel:862"", ""message"": ""batch 9 cancelled""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,958"", ""filename"": ""web_log.py"", ""funcNameAndLine"": ""log:233"", ""message"": ""10.32.4.199 [11/Jul/2019:14:19:34 +0000] \""PATCH /api/v1alpha/batches/9/cancel HTTP/1.1\"" 200 158 \""-\"" \""Python/3.6 aiohttp/3.5.4\"""", ""remote_address"": ""10.32.4.199"", ""request_start_time"": ""[11/Jul/2019:14:19:34 +0000]"", ""first_request_line"": ""PATCH /api/v1alpha/batches/9/cancel HTTP/1.1"", ""response_status"": 200, ""response_size"": 158, ""request_header"": {""Referer"": ""-"", ""User-Agent"": ""Python/3.6 aiohttp/3.5.4""}}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,967"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (9, 1, 'main') with pod batch-9-job-1-c8b9b2""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,969"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""set_state:501"", ""message"": ""job (9, 1, 'main') changed state: Ready -> Cancelled""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,974"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""_delete_pvc:251"", ""message"": ""deleting persistent volume claim batc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:2717,Integrability,message,message,2717,"""response_status"": 200, ""response_size"": 158, ""request_header"": {""Referer"": ""-"", ""User-Agent"": ""Python/3.6 aiohttp/3.5.4""}}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,957"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""cancel:862"", ""message"": ""batch 9 cancelled""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,958"", ""filename"": ""web_log.py"", ""funcNameAndLine"": ""log:233"", ""message"": ""10.32.4.199 [11/Jul/2019:14:19:34 +0000] \""PATCH /api/v1alpha/batches/9/cancel HTTP/1.1\"" 200 158 \""-\"" \""Python/3.6 aiohttp/3.5.4\"""", ""remote_address"": ""10.32.4.199"", ""request_start_time"": ""[11/Jul/2019:14:19:34 +0000]"", ""first_request_line"": ""PATCH /api/v1alpha/batches/9/cancel HTTP/1.1"", ""response_status"": 200, ""response_size"": 158, ""request_header"": {""Referer"": ""-"", ""User-Agent"": ""Python/3.6 aiohttp/3.5.4""}}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,967"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (9, 1, 'main') with pod batch-9-job-1-c8b9b2""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,969"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""set_state:501"", ""message"": ""job (9, 1, 'main') changed state: Ready -> Cancelled""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,974"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""_delete_pvc:251"", ""message"": ""deleting persistent volume claim batch-9-job-1-c8b9b2""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,976"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (9, 1, 'main') with pod batch-9-job-1-c8b9b2""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,977"", ""filename"": ""web_log.py"", ""funcNameAndLine"": ""log:233"", ""message"": ""10.32.4.199 [11/Jul/2019:14:19:34 +0000] \""GET /api/v1alpha/batches/9 HTTP/1.1\"" 200 279 \""-\"" \""Python/3.6 aiohttp/3.5.4\"""", ""remote_address"": ""10.32.4.199"", ""request_start_time"": ""[11/Jul/2019:14:19:34 +0000]"", ""first_request_line"": ""GET /api/v1alpha/batches/9 HTTP/1",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:2908,Integrability,message,message,2908,": ""batch.py"", ""funcNameAndLine"": ""cancel:862"", ""message"": ""batch 9 cancelled""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,958"", ""filename"": ""web_log.py"", ""funcNameAndLine"": ""log:233"", ""message"": ""10.32.4.199 [11/Jul/2019:14:19:34 +0000] \""PATCH /api/v1alpha/batches/9/cancel HTTP/1.1\"" 200 158 \""-\"" \""Python/3.6 aiohttp/3.5.4\"""", ""remote_address"": ""10.32.4.199"", ""request_start_time"": ""[11/Jul/2019:14:19:34 +0000]"", ""first_request_line"": ""PATCH /api/v1alpha/batches/9/cancel HTTP/1.1"", ""response_status"": 200, ""response_size"": 158, ""request_header"": {""Referer"": ""-"", ""User-Agent"": ""Python/3.6 aiohttp/3.5.4""}}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,967"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (9, 1, 'main') with pod batch-9-job-1-c8b9b2""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,969"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""set_state:501"", ""message"": ""job (9, 1, 'main') changed state: Ready -> Cancelled""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,974"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""_delete_pvc:251"", ""message"": ""deleting persistent volume claim batch-9-job-1-c8b9b2""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,976"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (9, 1, 'main') with pod batch-9-job-1-c8b9b2""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,977"", ""filename"": ""web_log.py"", ""funcNameAndLine"": ""log:233"", ""message"": ""10.32.4.199 [11/Jul/2019:14:19:34 +0000] \""GET /api/v1alpha/batches/9 HTTP/1.1\"" 200 279 \""-\"" \""Python/3.6 aiohttp/3.5.4\"""", ""remote_address"": ""10.32.4.199"", ""request_start_time"": ""[11/Jul/2019:14:19:34 +0000]"", ""first_request_line"": ""GET /api/v1alpha/batches/9 HTTP/1.1"", ""response_status"": 200, ""response_size"": 279, ""request_header"": {""Referer"": ""-"", ""User-Agent"": ""Python/3.6 aiohttp/3.5.4""}}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,985"", ""file",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:3098,Integrability,message,message,3098,":233"", ""message"": ""10.32.4.199 [11/Jul/2019:14:19:34 +0000] \""PATCH /api/v1alpha/batches/9/cancel HTTP/1.1\"" 200 158 \""-\"" \""Python/3.6 aiohttp/3.5.4\"""", ""remote_address"": ""10.32.4.199"", ""request_start_time"": ""[11/Jul/2019:14:19:34 +0000]"", ""first_request_line"": ""PATCH /api/v1alpha/batches/9/cancel HTTP/1.1"", ""response_status"": 200, ""response_size"": 158, ""request_header"": {""Referer"": ""-"", ""User-Agent"": ""Python/3.6 aiohttp/3.5.4""}}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,967"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (9, 1, 'main') with pod batch-9-job-1-c8b9b2""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,969"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""set_state:501"", ""message"": ""job (9, 1, 'main') changed state: Ready -> Cancelled""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,974"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""_delete_pvc:251"", ""message"": ""deleting persistent volume claim batch-9-job-1-c8b9b2""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,976"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (9, 1, 'main') with pod batch-9-job-1-c8b9b2""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,977"", ""filename"": ""web_log.py"", ""funcNameAndLine"": ""log:233"", ""message"": ""10.32.4.199 [11/Jul/2019:14:19:34 +0000] \""GET /api/v1alpha/batches/9 HTTP/1.1\"" 200 279 \""-\"" \""Python/3.6 aiohttp/3.5.4\"""", ""remote_address"": ""10.32.4.199"", ""request_start_time"": ""[11/Jul/2019:14:19:34 +0000]"", ""first_request_line"": ""GET /api/v1alpha/batches/9 HTTP/1.1"", ""response_status"": 200, ""response_size"": 279, ""request_header"": {""Referer"": ""-"", ""User-Agent"": ""Python/3.6 aiohttp/3.5.4""}}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,985"", ""filename"": ""web_log.py"", ""funcNameAndLine"": ""log:233"", ""message"": ""10.32.4.199 [11/Jul/2019:14:19:34 +0000] \""GET /api/v1alpha/batches/9 HTTP/1.1\"" 200 279 \""-\"" \""Python/3.6 aiohttp/3.5.4\"""", """,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:3298,Integrability,message,message,3298,"art_time"": ""[11/Jul/2019:14:19:34 +0000]"", ""first_request_line"": ""PATCH /api/v1alpha/batches/9/cancel HTTP/1.1"", ""response_status"": 200, ""response_size"": 158, ""request_header"": {""Referer"": ""-"", ""User-Agent"": ""Python/3.6 aiohttp/3.5.4""}}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,967"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (9, 1, 'main') with pod batch-9-job-1-c8b9b2""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,969"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""set_state:501"", ""message"": ""job (9, 1, 'main') changed state: Ready -> Cancelled""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,974"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""_delete_pvc:251"", ""message"": ""deleting persistent volume claim batch-9-job-1-c8b9b2""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,976"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (9, 1, 'main') with pod batch-9-job-1-c8b9b2""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,977"", ""filename"": ""web_log.py"", ""funcNameAndLine"": ""log:233"", ""message"": ""10.32.4.199 [11/Jul/2019:14:19:34 +0000] \""GET /api/v1alpha/batches/9 HTTP/1.1\"" 200 279 \""-\"" \""Python/3.6 aiohttp/3.5.4\"""", ""remote_address"": ""10.32.4.199"", ""request_start_time"": ""[11/Jul/2019:14:19:34 +0000]"", ""first_request_line"": ""GET /api/v1alpha/batches/9 HTTP/1.1"", ""response_status"": 200, ""response_size"": 279, ""request_header"": {""Referer"": ""-"", ""User-Agent"": ""Python/3.6 aiohttp/3.5.4""}}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,985"", ""filename"": ""web_log.py"", ""funcNameAndLine"": ""log:233"", ""message"": ""10.32.4.199 [11/Jul/2019:14:19:34 +0000] \""GET /api/v1alpha/batches/9 HTTP/1.1\"" 200 279 \""-\"" \""Python/3.6 aiohttp/3.5.4\"""", ""remote_address"": ""10.32.4.199"", ""request_start_time"": ""[11/Jul/2019:14:19:34 +0000]"", ""first_request_line"": ""GET /api/v1alpha/batches/9 HTTP/1.1"", ""response_status"": 200, ""response_size"": 279, ""req",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:3485,Integrability,message,message,3485,"us"": 200, ""response_size"": 158, ""request_header"": {""Referer"": ""-"", ""User-Agent"": ""Python/3.6 aiohttp/3.5.4""}}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,967"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (9, 1, 'main') with pod batch-9-job-1-c8b9b2""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,969"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""set_state:501"", ""message"": ""job (9, 1, 'main') changed state: Ready -> Cancelled""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,974"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""_delete_pvc:251"", ""message"": ""deleting persistent volume claim batch-9-job-1-c8b9b2""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,976"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (9, 1, 'main') with pod batch-9-job-1-c8b9b2""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,977"", ""filename"": ""web_log.py"", ""funcNameAndLine"": ""log:233"", ""message"": ""10.32.4.199 [11/Jul/2019:14:19:34 +0000] \""GET /api/v1alpha/batches/9 HTTP/1.1\"" 200 279 \""-\"" \""Python/3.6 aiohttp/3.5.4\"""", ""remote_address"": ""10.32.4.199"", ""request_start_time"": ""[11/Jul/2019:14:19:34 +0000]"", ""first_request_line"": ""GET /api/v1alpha/batches/9 HTTP/1.1"", ""response_status"": 200, ""response_size"": 279, ""request_header"": {""Referer"": ""-"", ""User-Agent"": ""Python/3.6 aiohttp/3.5.4""}}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,985"", ""filename"": ""web_log.py"", ""funcNameAndLine"": ""log:233"", ""message"": ""10.32.4.199 [11/Jul/2019:14:19:34 +0000] \""GET /api/v1alpha/batches/9 HTTP/1.1\"" 200 279 \""-\"" \""Python/3.6 aiohttp/3.5.4\"""", ""remote_address"": ""10.32.4.199"", ""request_start_time"": ""[11/Jul/2019:14:19:34 +0000]"", ""first_request_line"": ""GET /api/v1alpha/batches/9 HTTP/1.1"", ""response_status"": 200, ""response_size"": 279, ""request_header"": {""Referer"": ""-"", ""User-Agent"": ""Python/3.6 aiohttp/3.5.4""}}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:3",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:4012,Integrability,message,message,4012,"time"": ""2019-07-11 14:19:34,974"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""_delete_pvc:251"", ""message"": ""deleting persistent volume claim batch-9-job-1-c8b9b2""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,976"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (9, 1, 'main') with pod batch-9-job-1-c8b9b2""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,977"", ""filename"": ""web_log.py"", ""funcNameAndLine"": ""log:233"", ""message"": ""10.32.4.199 [11/Jul/2019:14:19:34 +0000] \""GET /api/v1alpha/batches/9 HTTP/1.1\"" 200 279 \""-\"" \""Python/3.6 aiohttp/3.5.4\"""", ""remote_address"": ""10.32.4.199"", ""request_start_time"": ""[11/Jul/2019:14:19:34 +0000]"", ""first_request_line"": ""GET /api/v1alpha/batches/9 HTTP/1.1"", ""response_status"": 200, ""response_size"": 279, ""request_header"": {""Referer"": ""-"", ""User-Agent"": ""Python/3.6 aiohttp/3.5.4""}}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,985"", ""filename"": ""web_log.py"", ""funcNameAndLine"": ""log:233"", ""message"": ""10.32.4.199 [11/Jul/2019:14:19:34 +0000] \""GET /api/v1alpha/batches/9 HTTP/1.1\"" 200 279 \""-\"" \""Python/3.6 aiohttp/3.5.4\"""", ""remote_address"": ""10.32.4.199"", ""request_start_time"": ""[11/Jul/2019:14:19:34 +0000]"", ""first_request_line"": ""GET /api/v1alpha/batches/9 HTTP/1.1"", ""response_status"": 200, ""response_size"": 279, ""request_header"": {""Referer"": ""-"", ""User-Agent"": ""Python/3.6 aiohttp/3.5.4""}}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,991"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (9, 1, 'main') with pod batch-9-job-1-c8b9b2""}; ```. Recall that pod creation happens in the background, so the `batches/9/create` and `batches/9/close` endpoints return to the client before pods are necessarily created. See batch.py:866, `Batch.close`. Likewise, the `batches/9/cancel` endpoint returns before the individual jobs are cancelled. There are now at most three concurrent threads of control interacting with the ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:4554,Integrability,message,message,4554,"9 HTTP/1.1\"" 200 279 \""-\"" \""Python/3.6 aiohttp/3.5.4\"""", ""remote_address"": ""10.32.4.199"", ""request_start_time"": ""[11/Jul/2019:14:19:34 +0000]"", ""first_request_line"": ""GET /api/v1alpha/batches/9 HTTP/1.1"", ""response_status"": 200, ""response_size"": 279, ""request_header"": {""Referer"": ""-"", ""User-Agent"": ""Python/3.6 aiohttp/3.5.4""}}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,985"", ""filename"": ""web_log.py"", ""funcNameAndLine"": ""log:233"", ""message"": ""10.32.4.199 [11/Jul/2019:14:19:34 +0000] \""GET /api/v1alpha/batches/9 HTTP/1.1\"" 200 279 \""-\"" \""Python/3.6 aiohttp/3.5.4\"""", ""remote_address"": ""10.32.4.199"", ""request_start_time"": ""[11/Jul/2019:14:19:34 +0000]"", ""first_request_line"": ""GET /api/v1alpha/batches/9 HTTP/1.1"", ""response_status"": 200, ""response_size"": 279, ""request_header"": {""Referer"": ""-"", ""User-Agent"": ""Python/3.6 aiohttp/3.5.4""}}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,991"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (9, 1, 'main') with pod batch-9-job-1-c8b9b2""}; ```. Recall that pod creation happens in the background, so the `batches/9/create` and `batches/9/close` endpoints return to the client before pods are necessarily created. See batch.py:866, `Batch.close`. Likewise, the `batches/9/cancel` endpoint returns before the individual jobs are cancelled. There are now at most three concurrent threads of control interacting with the database and k8s. Eventually this sequence of log messages appears three times in quick succession. ```; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:39,890"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (9, 1, 'main') with pod batch-9-job-1-c8b9b2""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:39,890"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1159"", ""message"": ""job (9, 1, 'main') mark complete""}; {""levelname"": ""WARNING"", ""asctime"": ""2019-07-11 14:19:39,899"", ""filename"": ""ba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:5051,Integrability,message,messages,5051,"000] \""GET /api/v1alpha/batches/9 HTTP/1.1\"" 200 279 \""-\"" \""Python/3.6 aiohttp/3.5.4\"""", ""remote_address"": ""10.32.4.199"", ""request_start_time"": ""[11/Jul/2019:14:19:34 +0000]"", ""first_request_line"": ""GET /api/v1alpha/batches/9 HTTP/1.1"", ""response_status"": 200, ""response_size"": 279, ""request_header"": {""Referer"": ""-"", ""User-Agent"": ""Python/3.6 aiohttp/3.5.4""}}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,991"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (9, 1, 'main') with pod batch-9-job-1-c8b9b2""}; ```. Recall that pod creation happens in the background, so the `batches/9/create` and `batches/9/close` endpoints return to the client before pods are necessarily created. See batch.py:866, `Batch.close`. Likewise, the `batches/9/cancel` endpoint returns before the individual jobs are cancelled. There are now at most three concurrent threads of control interacting with the database and k8s. Eventually this sequence of log messages appears three times in quick succession. ```; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:39,890"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (9, 1, 'main') with pod batch-9-job-1-c8b9b2""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:39,890"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1159"", ""message"": ""job (9, 1, 'main') mark complete""}; {""levelname"": ""WARNING"", ""asctime"": ""2019-07-11 14:19:39,899"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""mark_complete:579"", ""message"": ""job (9, 1, 'main') has pod batch-9-job-1-c8b9b2 which is terminated but has no timing information. {'api_version': 'v1',\n 'kind': 'Pod',\n 'metadata': {'annotations': None,\n 'cluster_name': None,\n 'creation_timestamp': datetime.datetime(2019, 7, 11, 14, 19, 34, tzinfo=tzlocal()),\n 'deletion_grace_period_seconds': 30,\n 'deletion_timestamp': datetime.datetime(2019, 7, 11, 14, 20, 4, tzinfo=tzlocal()),\n 'finalizers': None,\n '",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:5238,Integrability,message,message,5238,"HTTP/1.1"", ""response_status"": 200, ""response_size"": 279, ""request_header"": {""Referer"": ""-"", ""User-Agent"": ""Python/3.6 aiohttp/3.5.4""}}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,991"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (9, 1, 'main') with pod batch-9-job-1-c8b9b2""}; ```. Recall that pod creation happens in the background, so the `batches/9/create` and `batches/9/close` endpoints return to the client before pods are necessarily created. See batch.py:866, `Batch.close`. Likewise, the `batches/9/cancel` endpoint returns before the individual jobs are cancelled. There are now at most three concurrent threads of control interacting with the database and k8s. Eventually this sequence of log messages appears three times in quick succession. ```; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:39,890"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (9, 1, 'main') with pod batch-9-job-1-c8b9b2""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:39,890"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1159"", ""message"": ""job (9, 1, 'main') mark complete""}; {""levelname"": ""WARNING"", ""asctime"": ""2019-07-11 14:19:39,899"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""mark_complete:579"", ""message"": ""job (9, 1, 'main') has pod batch-9-job-1-c8b9b2 which is terminated but has no timing information. {'api_version': 'v1',\n 'kind': 'Pod',\n 'metadata': {'annotations': None,\n 'cluster_name': None,\n 'creation_timestamp': datetime.datetime(2019, 7, 11, 14, 19, 34, tzinfo=tzlocal()),\n 'deletion_grace_period_seconds': 30,\n 'deletion_timestamp': datetime.datetime(2019, 7, 11, 14, 20, 4, tzinfo=tzlocal()),\n 'finalizers': None,\n 'generate_name': None,\n 'generation': None,\n 'initializers': None,\n 'labels': {'app': 'batch-job',\n 'batch_id': '9',\n 'hail.is/batch-instance': 'ffa5abc4607849df8e5f0036e7350bcf',\n 'job_id': '1',\n 'task': 'main',\n 'user'",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:5440,Integrability,message,message,5440,"1"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (9, 1, 'main') with pod batch-9-job-1-c8b9b2""}; ```. Recall that pod creation happens in the background, so the `batches/9/create` and `batches/9/close` endpoints return to the client before pods are necessarily created. See batch.py:866, `Batch.close`. Likewise, the `batches/9/cancel` endpoint returns before the individual jobs are cancelled. There are now at most three concurrent threads of control interacting with the database and k8s. Eventually this sequence of log messages appears three times in quick succession. ```; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:39,890"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (9, 1, 'main') with pod batch-9-job-1-c8b9b2""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:39,890"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1159"", ""message"": ""job (9, 1, 'main') mark complete""}; {""levelname"": ""WARNING"", ""asctime"": ""2019-07-11 14:19:39,899"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""mark_complete:579"", ""message"": ""job (9, 1, 'main') has pod batch-9-job-1-c8b9b2 which is terminated but has no timing information. {'api_version': 'v1',\n 'kind': 'Pod',\n 'metadata': {'annotations': None,\n 'cluster_name': None,\n 'creation_timestamp': datetime.datetime(2019, 7, 11, 14, 19, 34, tzinfo=tzlocal()),\n 'deletion_grace_period_seconds': 30,\n 'deletion_timestamp': datetime.datetime(2019, 7, 11, 14, 20, 4, tzinfo=tzlocal()),\n 'finalizers': None,\n 'generate_name': None,\n 'generation': None,\n 'initializers': None,\n 'labels': {'app': 'batch-job',\n 'batch_id': '9',\n 'hail.is/batch-instance': 'ffa5abc4607849df8e5f0036e7350bcf',\n 'job_id': '1',\n 'task': 'main',\n 'user': 'test',\n 'uuid': '291b9eed73b9433c86ff1f58624cf24d'},\n 'name': 'batch-9-job-1-c8b9b2',\n 'namespace': 'pr-6604-batch-pods-cjklalqnl5u9',\n 'owner_references': None,\n 'resource_version': '",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:5615,Integrability,message,message,5615,"reation happens in the background, so the `batches/9/create` and `batches/9/close` endpoints return to the client before pods are necessarily created. See batch.py:866, `Batch.close`. Likewise, the `batches/9/cancel` endpoint returns before the individual jobs are cancelled. There are now at most three concurrent threads of control interacting with the database and k8s. Eventually this sequence of log messages appears three times in quick succession. ```; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:39,890"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (9, 1, 'main') with pod batch-9-job-1-c8b9b2""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:39,890"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1159"", ""message"": ""job (9, 1, 'main') mark complete""}; {""levelname"": ""WARNING"", ""asctime"": ""2019-07-11 14:19:39,899"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""mark_complete:579"", ""message"": ""job (9, 1, 'main') has pod batch-9-job-1-c8b9b2 which is terminated but has no timing information. {'api_version': 'v1',\n 'kind': 'Pod',\n 'metadata': {'annotations': None,\n 'cluster_name': None,\n 'creation_timestamp': datetime.datetime(2019, 7, 11, 14, 19, 34, tzinfo=tzlocal()),\n 'deletion_grace_period_seconds': 30,\n 'deletion_timestamp': datetime.datetime(2019, 7, 11, 14, 20, 4, tzinfo=tzlocal()),\n 'finalizers': None,\n 'generate_name': None,\n 'generation': None,\n 'initializers': None,\n 'labels': {'app': 'batch-job',\n 'batch_id': '9',\n 'hail.is/batch-instance': 'ffa5abc4607849df8e5f0036e7350bcf',\n 'job_id': '1',\n 'task': 'main',\n 'user': 'test',\n 'uuid': '291b9eed73b9433c86ff1f58624cf24d'},\n 'name': 'batch-9-job-1-c8b9b2',\n 'namespace': 'pr-6604-batch-pods-cjklalqnl5u9',\n 'owner_references': None,\n 'resource_version': '86681671',\n 'self_link': '/api/v1/namespaces/pr-6604-batch-pods-cjklalqnl5u9/pods/batch-9-job-1-c8b9b2',\n 'uid': 'e878f906-a3e6-11e9-a4bb-42010a8000af'},\n 'spec': {'a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:10921,Integrability,message,message,10921,"n 'secret_name': 'test-gsa-key'},\n 'storageos': None,\n 'vsphere_volume': None},\n {'aws_elastic_block_store': None,\n 'azure_disk': None,\n 'azure_file': None,\n 'cephfs': None,\n 'cinder': None,\n 'config_map': None,\n 'downward_api': None,\n 'empty_dir': None,\n 'fc': None,\n 'flex_volume': None,\n 'flocker': None,\n 'gce_persistent_disk': None,\n 'git_repo': None,\n 'glusterfs': None,\n 'host_path': None,\n 'iscsi': None,\n 'name': 'default-token-brr2f',\n 'nfs': None,\n 'persistent_volume_claim': None,\n 'photon_persistent_disk': None,\n 'portworx_volume': None,\n 'projected': None,\n 'quobyte': None,\n 'rbd': None,\n 'scale_io': None,\n 'secret': {'default_mode': 420,\n 'items': None,\n 'optional': None,\n 'secret_name': 'default-token-brr2f'},\n 'storageos': None,\n 'vsphere_volume': None}]},\n 'status': {'conditions': [{'last_probe_time': None,\n 'last_transition_time': datetime.datetime(2019, 7, 11, 14, 19, 34, tzinfo=tzlocal()),\n 'message': None,\n 'reason': None,\n 'status': 'True',\n 'type': 'Initialized'},\n {'last_probe_time': None,\n 'last_transition_time': datetime.datetime(2019, 7, 11, 14, 19, 34, tzinfo=tzlocal()),\n 'message': 'containers with unready status: [main]',\n 'reason': 'ContainersNotReady',\n 'status': 'False',\n 'type': 'Ready'},\n {'last_probe_time': None,\n 'last_transition_time': datetime.datetime(2019, 7, 11, 14, 19, 34, tzinfo=tzlocal()),\n 'message': 'containers with unready status: [main]',\n 'reason': 'ContainersNotReady',\n 'status': 'False',\n 'type': 'ContainersReady'},\n {'last_probe_time': None,\n 'last_transition_time': datetime.datetime(2019, 7, 11, 14, 19, 34, tzinfo=tzlocal()),\n 'message': None,\n 'reason': None,\n 'status': 'True',\n 'type': 'PodScheduled'}],\n 'container_statuses': [{'container_id': None,\n 'image': 'alpine',\n 'image_id': '',\n 'last_state': {'running': None,\n 'terminated': None,\n 'waiting': None},\n 'name': 'main',\n 'ready': False,\n 'restart_count': 0,\n 'state': {'running': None,\n 'terminat",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:11120,Integrability,message,message,11120,"downward_api': None,\n 'empty_dir': None,\n 'fc': None,\n 'flex_volume': None,\n 'flocker': None,\n 'gce_persistent_disk': None,\n 'git_repo': None,\n 'glusterfs': None,\n 'host_path': None,\n 'iscsi': None,\n 'name': 'default-token-brr2f',\n 'nfs': None,\n 'persistent_volume_claim': None,\n 'photon_persistent_disk': None,\n 'portworx_volume': None,\n 'projected': None,\n 'quobyte': None,\n 'rbd': None,\n 'scale_io': None,\n 'secret': {'default_mode': 420,\n 'items': None,\n 'optional': None,\n 'secret_name': 'default-token-brr2f'},\n 'storageos': None,\n 'vsphere_volume': None}]},\n 'status': {'conditions': [{'last_probe_time': None,\n 'last_transition_time': datetime.datetime(2019, 7, 11, 14, 19, 34, tzinfo=tzlocal()),\n 'message': None,\n 'reason': None,\n 'status': 'True',\n 'type': 'Initialized'},\n {'last_probe_time': None,\n 'last_transition_time': datetime.datetime(2019, 7, 11, 14, 19, 34, tzinfo=tzlocal()),\n 'message': 'containers with unready status: [main]',\n 'reason': 'ContainersNotReady',\n 'status': 'False',\n 'type': 'Ready'},\n {'last_probe_time': None,\n 'last_transition_time': datetime.datetime(2019, 7, 11, 14, 19, 34, tzinfo=tzlocal()),\n 'message': 'containers with unready status: [main]',\n 'reason': 'ContainersNotReady',\n 'status': 'False',\n 'type': 'ContainersReady'},\n {'last_probe_time': None,\n 'last_transition_time': datetime.datetime(2019, 7, 11, 14, 19, 34, tzinfo=tzlocal()),\n 'message': None,\n 'reason': None,\n 'status': 'True',\n 'type': 'PodScheduled'}],\n 'container_statuses': [{'container_id': None,\n 'image': 'alpine',\n 'image_id': '',\n 'last_state': {'running': None,\n 'terminated': None,\n 'waiting': None},\n 'name': 'main',\n 'ready': False,\n 'restart_count': 0,\n 'state': {'running': None,\n 'terminated': {'container_id': None,\n 'exit_code': 0,\n 'finished_at': None,\n 'message': None,\n 'reason': None,\n 'signal': None,\n 'started_at': None},\n 'waiting': None}}],\n 'host_ip': '10.128.0.56',\n 'init_container_statuse",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:11366,Integrability,message,message,11366,"one,\n 'persistent_volume_claim': None,\n 'photon_persistent_disk': None,\n 'portworx_volume': None,\n 'projected': None,\n 'quobyte': None,\n 'rbd': None,\n 'scale_io': None,\n 'secret': {'default_mode': 420,\n 'items': None,\n 'optional': None,\n 'secret_name': 'default-token-brr2f'},\n 'storageos': None,\n 'vsphere_volume': None}]},\n 'status': {'conditions': [{'last_probe_time': None,\n 'last_transition_time': datetime.datetime(2019, 7, 11, 14, 19, 34, tzinfo=tzlocal()),\n 'message': None,\n 'reason': None,\n 'status': 'True',\n 'type': 'Initialized'},\n {'last_probe_time': None,\n 'last_transition_time': datetime.datetime(2019, 7, 11, 14, 19, 34, tzinfo=tzlocal()),\n 'message': 'containers with unready status: [main]',\n 'reason': 'ContainersNotReady',\n 'status': 'False',\n 'type': 'Ready'},\n {'last_probe_time': None,\n 'last_transition_time': datetime.datetime(2019, 7, 11, 14, 19, 34, tzinfo=tzlocal()),\n 'message': 'containers with unready status: [main]',\n 'reason': 'ContainersNotReady',\n 'status': 'False',\n 'type': 'ContainersReady'},\n {'last_probe_time': None,\n 'last_transition_time': datetime.datetime(2019, 7, 11, 14, 19, 34, tzinfo=tzlocal()),\n 'message': None,\n 'reason': None,\n 'status': 'True',\n 'type': 'PodScheduled'}],\n 'container_statuses': [{'container_id': None,\n 'image': 'alpine',\n 'image_id': '',\n 'last_state': {'running': None,\n 'terminated': None,\n 'waiting': None},\n 'name': 'main',\n 'ready': False,\n 'restart_count': 0,\n 'state': {'running': None,\n 'terminated': {'container_id': None,\n 'exit_code': 0,\n 'finished_at': None,\n 'message': None,\n 'reason': None,\n 'signal': None,\n 'started_at': None},\n 'waiting': None}}],\n 'host_ip': '10.128.0.56',\n 'init_container_statuses': None,\n 'message': None,\n 'nominated_node_name': None,\n 'phase': 'Pending',\n 'pod_ip': None,\n 'qos_class': 'Burstable',\n 'reason': None,\n 'start_time': datetime.datetime(2019, 7, 11, 14, 19, 34, tzinfo=tzlocal())}}""}; File ""/usr/local/lib/py",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:11622,Integrability,message,message,11622,"tetime.datetime(2019, 7, 11, 14, 19, 34, tzinfo=tzlocal()),\n 'message': None,\n 'reason': None,\n 'status': 'True',\n 'type': 'Initialized'},\n {'last_probe_time': None,\n 'last_transition_time': datetime.datetime(2019, 7, 11, 14, 19, 34, tzinfo=tzlocal()),\n 'message': 'containers with unready status: [main]',\n 'reason': 'ContainersNotReady',\n 'status': 'False',\n 'type': 'Ready'},\n {'last_probe_time': None,\n 'last_transition_time': datetime.datetime(2019, 7, 11, 14, 19, 34, tzinfo=tzlocal()),\n 'message': 'containers with unready status: [main]',\n 'reason': 'ContainersNotReady',\n 'status': 'False',\n 'type': 'ContainersReady'},\n {'last_probe_time': None,\n 'last_transition_time': datetime.datetime(2019, 7, 11, 14, 19, 34, tzinfo=tzlocal()),\n 'message': None,\n 'reason': None,\n 'status': 'True',\n 'type': 'PodScheduled'}],\n 'container_statuses': [{'container_id': None,\n 'image': 'alpine',\n 'image_id': '',\n 'last_state': {'running': None,\n 'terminated': None,\n 'waiting': None},\n 'name': 'main',\n 'ready': False,\n 'restart_count': 0,\n 'state': {'running': None,\n 'terminated': {'container_id': None,\n 'exit_code': 0,\n 'finished_at': None,\n 'message': None,\n 'reason': None,\n 'signal': None,\n 'started_at': None},\n 'waiting': None}}],\n 'host_ip': '10.128.0.56',\n 'init_container_statuses': None,\n 'message': None,\n 'nominated_node_name': None,\n 'phase': 'Pending',\n 'pod_ip': None,\n 'qos_class': 'Burstable',\n 'reason': None,\n 'start_time': datetime.datetime(2019, 7, 11, 14, 19, 34, tzinfo=tzlocal())}}""}; File ""/usr/local/lib/python3.6/dist-packages/batch/k8s.py"", line 65, in wrapped; **kwargs),; File ""/usr/local/lib/python3.6/dist-packages/batch/blocking_to_async.py"", line 6, in blocking_to_async; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/lib/python3.6/concurrent/futures/thread.py"", line 56, in run; result = self.fn(*self.args, **self.kwargs); File ""/usr/local/lib/python3.6/dist-packages/batch/blocking_to_async.py"", line 6, in ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:12037,Integrability,message,message,12037,"tetime.datetime(2019, 7, 11, 14, 19, 34, tzinfo=tzlocal()),\n 'message': None,\n 'reason': None,\n 'status': 'True',\n 'type': 'Initialized'},\n {'last_probe_time': None,\n 'last_transition_time': datetime.datetime(2019, 7, 11, 14, 19, 34, tzinfo=tzlocal()),\n 'message': 'containers with unready status: [main]',\n 'reason': 'ContainersNotReady',\n 'status': 'False',\n 'type': 'Ready'},\n {'last_probe_time': None,\n 'last_transition_time': datetime.datetime(2019, 7, 11, 14, 19, 34, tzinfo=tzlocal()),\n 'message': 'containers with unready status: [main]',\n 'reason': 'ContainersNotReady',\n 'status': 'False',\n 'type': 'ContainersReady'},\n {'last_probe_time': None,\n 'last_transition_time': datetime.datetime(2019, 7, 11, 14, 19, 34, tzinfo=tzlocal()),\n 'message': None,\n 'reason': None,\n 'status': 'True',\n 'type': 'PodScheduled'}],\n 'container_statuses': [{'container_id': None,\n 'image': 'alpine',\n 'image_id': '',\n 'last_state': {'running': None,\n 'terminated': None,\n 'waiting': None},\n 'name': 'main',\n 'ready': False,\n 'restart_count': 0,\n 'state': {'running': None,\n 'terminated': {'container_id': None,\n 'exit_code': 0,\n 'finished_at': None,\n 'message': None,\n 'reason': None,\n 'signal': None,\n 'started_at': None},\n 'waiting': None}}],\n 'host_ip': '10.128.0.56',\n 'init_container_statuses': None,\n 'message': None,\n 'nominated_node_name': None,\n 'phase': 'Pending',\n 'pod_ip': None,\n 'qos_class': 'Burstable',\n 'reason': None,\n 'start_time': datetime.datetime(2019, 7, 11, 14, 19, 34, tzinfo=tzlocal())}}""}; File ""/usr/local/lib/python3.6/dist-packages/batch/k8s.py"", line 65, in wrapped; **kwargs),; File ""/usr/local/lib/python3.6/dist-packages/batch/blocking_to_async.py"", line 6, in blocking_to_async; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/lib/python3.6/concurrent/futures/thread.py"", line 56, in run; result = self.fn(*self.args, **self.kwargs); File ""/usr/local/lib/python3.6/dist-packages/batch/blocking_to_async.py"", line 6, in ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:12200,Integrability,message,message,12200,"robe_time': None,\n 'last_transition_time': datetime.datetime(2019, 7, 11, 14, 19, 34, tzinfo=tzlocal()),\n 'message': 'containers with unready status: [main]',\n 'reason': 'ContainersNotReady',\n 'status': 'False',\n 'type': 'ContainersReady'},\n {'last_probe_time': None,\n 'last_transition_time': datetime.datetime(2019, 7, 11, 14, 19, 34, tzinfo=tzlocal()),\n 'message': None,\n 'reason': None,\n 'status': 'True',\n 'type': 'PodScheduled'}],\n 'container_statuses': [{'container_id': None,\n 'image': 'alpine',\n 'image_id': '',\n 'last_state': {'running': None,\n 'terminated': None,\n 'waiting': None},\n 'name': 'main',\n 'ready': False,\n 'restart_count': 0,\n 'state': {'running': None,\n 'terminated': {'container_id': None,\n 'exit_code': 0,\n 'finished_at': None,\n 'message': None,\n 'reason': None,\n 'signal': None,\n 'started_at': None},\n 'waiting': None}}],\n 'host_ip': '10.128.0.56',\n 'init_container_statuses': None,\n 'message': None,\n 'nominated_node_name': None,\n 'phase': 'Pending',\n 'pod_ip': None,\n 'qos_class': 'Burstable',\n 'reason': None,\n 'start_time': datetime.datetime(2019, 7, 11, 14, 19, 34, tzinfo=tzlocal())}}""}; File ""/usr/local/lib/python3.6/dist-packages/batch/k8s.py"", line 65, in wrapped; **kwargs),; File ""/usr/local/lib/python3.6/dist-packages/batch/blocking_to_async.py"", line 6, in blocking_to_async; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/lib/python3.6/concurrent/futures/thread.py"", line 56, in run; result = self.fn(*self.args, **self.kwargs); File ""/usr/local/lib/python3.6/dist-packages/batch/blocking_to_async.py"", line 6, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/apis/core_v1_api.py"", line 18538, in read_namespaced_pod_log; (data) = self.read_namespaced_pod_log_with_http_info(name, namespace, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/apis/core_v1_api.py"", line 18644, in read_namespaced_pod_log_with_http_info;",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:12487,Integrability,wrap,wrapped,12487,"n {'last_probe_time': None,\n 'last_transition_time': datetime.datetime(2019, 7, 11, 14, 19, 34, tzinfo=tzlocal()),\n 'message': None,\n 'reason': None,\n 'status': 'True',\n 'type': 'PodScheduled'}],\n 'container_statuses': [{'container_id': None,\n 'image': 'alpine',\n 'image_id': '',\n 'last_state': {'running': None,\n 'terminated': None,\n 'waiting': None},\n 'name': 'main',\n 'ready': False,\n 'restart_count': 0,\n 'state': {'running': None,\n 'terminated': {'container_id': None,\n 'exit_code': 0,\n 'finished_at': None,\n 'message': None,\n 'reason': None,\n 'signal': None,\n 'started_at': None},\n 'waiting': None}}],\n 'host_ip': '10.128.0.56',\n 'init_container_statuses': None,\n 'message': None,\n 'nominated_node_name': None,\n 'phase': 'Pending',\n 'pod_ip': None,\n 'qos_class': 'Burstable',\n 'reason': None,\n 'start_time': datetime.datetime(2019, 7, 11, 14, 19, 34, tzinfo=tzlocal())}}""}; File ""/usr/local/lib/python3.6/dist-packages/batch/k8s.py"", line 65, in wrapped; **kwargs),; File ""/usr/local/lib/python3.6/dist-packages/batch/blocking_to_async.py"", line 6, in blocking_to_async; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/lib/python3.6/concurrent/futures/thread.py"", line 56, in run; result = self.fn(*self.args, **self.kwargs); File ""/usr/local/lib/python3.6/dist-packages/batch/blocking_to_async.py"", line 6, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/apis/core_v1_api.py"", line 18538, in read_namespaced_pod_log; (data) = self.read_namespaced_pod_log_with_http_info(name, namespace, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/apis/core_v1_api.py"", line 18644, in read_namespaced_pod_log_with_http_info; collection_formats=collection_formats); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 334, in call_api; _return_http_data_only, collection_formats, _preload_content, _request_timeout); File ""/usr/local/lib/p",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:14113,Integrability,message,message,14113,"ient.py"", line 334, in call_api; _return_http_data_only, collection_formats, _preload_content, _request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 168, in __call_api; _request_timeout=_request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 355, in request; headers=headers); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 231, in GET; query_params=query_params); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 222, in request; raise ApiException(http_resp=r); {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:39,924"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""mark_complete:591"", ""message"": ""no logs for batch-9-job-1-c8b9b2 due to previous error, rescheduling pod Error: (400)\nReason: Bad Request\nHTTP response headers: HTTPHeaderDict({'Audit-Id': '6a744afc-154f-4b48-b4bb-51a15078d999', 'Content-Type': 'application/json', 'Date': 'Thu, 11 Jul 2019 14:19:39 GMT', 'Content-Length': '179'})\nHTTP response body: {\""kind\"":\""Status\"",\""apiVersion\"":\""v1\"",\""metadata\"":{},\""status\"":\""Failure\"",\""message\"":\""container \\\""main\\\"" in pod \\\""batch-9-job-1-c8b9b2\\\"" is terminated\"",\""reason\"":\""BadRequest\"",\""code\"":400}\n\n""}; ```. And finally, this k8s refresh loop sequence repeats until CI kills the tests due to a timeout. ```; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,070"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_state:1261"", ""message"": ""started k8s state refresh""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,085"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_pods:1210"", ""message"": ""k8s had 3 pods""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,088"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (11, 1, 'output') with pod batch-11-job-1-4f1118""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,090"", ""fil",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:14531,Integrability,message,message,14531,"ient.py"", line 334, in call_api; _return_http_data_only, collection_formats, _preload_content, _request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 168, in __call_api; _request_timeout=_request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 355, in request; headers=headers); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 231, in GET; query_params=query_params); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 222, in request; raise ApiException(http_resp=r); {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:39,924"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""mark_complete:591"", ""message"": ""no logs for batch-9-job-1-c8b9b2 due to previous error, rescheduling pod Error: (400)\nReason: Bad Request\nHTTP response headers: HTTPHeaderDict({'Audit-Id': '6a744afc-154f-4b48-b4bb-51a15078d999', 'Content-Type': 'application/json', 'Date': 'Thu, 11 Jul 2019 14:19:39 GMT', 'Content-Length': '179'})\nHTTP response body: {\""kind\"":\""Status\"",\""apiVersion\"":\""v1\"",\""metadata\"":{},\""status\"":\""Failure\"",\""message\"":\""container \\\""main\\\"" in pod \\\""batch-9-job-1-c8b9b2\\\"" is terminated\"",\""reason\"":\""BadRequest\"",\""code\"":400}\n\n""}; ```. And finally, this k8s refresh loop sequence repeats until CI kills the tests due to a timeout. ```; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,070"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_state:1261"", ""message"": ""started k8s state refresh""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,085"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_pods:1210"", ""message"": ""k8s had 3 pods""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,088"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (11, 1, 'output') with pod batch-11-job-1-4f1118""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,090"", ""fil",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:14900,Integrability,message,message,14900,"ine 222, in request; raise ApiException(http_resp=r); {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:39,924"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""mark_complete:591"", ""message"": ""no logs for batch-9-job-1-c8b9b2 due to previous error, rescheduling pod Error: (400)\nReason: Bad Request\nHTTP response headers: HTTPHeaderDict({'Audit-Id': '6a744afc-154f-4b48-b4bb-51a15078d999', 'Content-Type': 'application/json', 'Date': 'Thu, 11 Jul 2019 14:19:39 GMT', 'Content-Length': '179'})\nHTTP response body: {\""kind\"":\""Status\"",\""apiVersion\"":\""v1\"",\""metadata\"":{},\""status\"":\""Failure\"",\""message\"":\""container \\\""main\\\"" in pod \\\""batch-9-job-1-c8b9b2\\\"" is terminated\"",\""reason\"":\""BadRequest\"",\""code\"":400}\n\n""}; ```. And finally, this k8s refresh loop sequence repeats until CI kills the tests due to a timeout. ```; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,070"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_state:1261"", ""message"": ""started k8s state refresh""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,085"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_pods:1210"", ""message"": ""k8s had 3 pods""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,088"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (11, 1, 'output') with pod batch-11-job-1-4f1118""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,090"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (11, 2, 'output') with pod batch-11-job-2-ad1587""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,093"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (11, 3, 'output') with pod batch-11-job-3-d826dd""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,093"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_pods:1221"", ""message"": ""restarting ready and running jobs with pods not seen in k8s""}; {""levelname"": """,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:15069,Integrability,message,message,15069,"omplete:591"", ""message"": ""no logs for batch-9-job-1-c8b9b2 due to previous error, rescheduling pod Error: (400)\nReason: Bad Request\nHTTP response headers: HTTPHeaderDict({'Audit-Id': '6a744afc-154f-4b48-b4bb-51a15078d999', 'Content-Type': 'application/json', 'Date': 'Thu, 11 Jul 2019 14:19:39 GMT', 'Content-Length': '179'})\nHTTP response body: {\""kind\"":\""Status\"",\""apiVersion\"":\""v1\"",\""metadata\"":{},\""status\"":\""Failure\"",\""message\"":\""container \\\""main\\\"" in pod \\\""batch-9-job-1-c8b9b2\\\"" is terminated\"",\""reason\"":\""BadRequest\"",\""code\"":400}\n\n""}; ```. And finally, this k8s refresh loop sequence repeats until CI kills the tests due to a timeout. ```; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,070"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_state:1261"", ""message"": ""started k8s state refresh""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,085"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_pods:1210"", ""message"": ""k8s had 3 pods""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,088"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (11, 1, 'output') with pod batch-11-job-1-4f1118""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,090"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (11, 2, 'output') with pod batch-11-job-2-ad1587""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,093"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (11, 3, 'output') with pod batch-11-job-3-d826dd""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,093"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_pods:1221"", ""message"": ""restarting ready and running jobs with pods not seen in k8s""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,093"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_pods:1225"", ""message"": ""restarting job (9, 1, 'main')""}; {""leve",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:15230,Integrability,message,message,15230,"': '6a744afc-154f-4b48-b4bb-51a15078d999', 'Content-Type': 'application/json', 'Date': 'Thu, 11 Jul 2019 14:19:39 GMT', 'Content-Length': '179'})\nHTTP response body: {\""kind\"":\""Status\"",\""apiVersion\"":\""v1\"",\""metadata\"":{},\""status\"":\""Failure\"",\""message\"":\""container \\\""main\\\"" in pod \\\""batch-9-job-1-c8b9b2\\\"" is terminated\"",\""reason\"":\""BadRequest\"",\""code\"":400}\n\n""}; ```. And finally, this k8s refresh loop sequence repeats until CI kills the tests due to a timeout. ```; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,070"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_state:1261"", ""message"": ""started k8s state refresh""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,085"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_pods:1210"", ""message"": ""k8s had 3 pods""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,088"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (11, 1, 'output') with pod batch-11-job-1-4f1118""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,090"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (11, 2, 'output') with pod batch-11-job-2-ad1587""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,093"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (11, 3, 'output') with pod batch-11-job-3-d826dd""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,093"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_pods:1221"", ""message"": ""restarting ready and running jobs with pods not seen in k8s""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,093"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_pods:1225"", ""message"": ""restarting job (9, 1, 'main')""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,093"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (9, 1, 'main') with pod None""}; {""",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:15436,Integrability,message,message,15436,"1\"",\""metadata\"":{},\""status\"":\""Failure\"",\""message\"":\""container \\\""main\\\"" in pod \\\""batch-9-job-1-c8b9b2\\\"" is terminated\"",\""reason\"":\""BadRequest\"",\""code\"":400}\n\n""}; ```. And finally, this k8s refresh loop sequence repeats until CI kills the tests due to a timeout. ```; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,070"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_state:1261"", ""message"": ""started k8s state refresh""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,085"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_pods:1210"", ""message"": ""k8s had 3 pods""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,088"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (11, 1, 'output') with pod batch-11-job-1-4f1118""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,090"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (11, 2, 'output') with pod batch-11-job-2-ad1587""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,093"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (11, 3, 'output') with pod batch-11-job-3-d826dd""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,093"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_pods:1221"", ""message"": ""restarting ready and running jobs with pods not seen in k8s""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,093"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_pods:1225"", ""message"": ""restarting job (9, 1, 'main')""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,093"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (9, 1, 'main') with pod None""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,093"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1148"", ""message"": ""job (9, 1, 'main') mark unscheduled""}; {""levelname"": ""INFO"", ""asc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:15642,Integrability,message,message,15642,"refresh loop sequence repeats until CI kills the tests due to a timeout. ```; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,070"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_state:1261"", ""message"": ""started k8s state refresh""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,085"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_pods:1210"", ""message"": ""k8s had 3 pods""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,088"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (11, 1, 'output') with pod batch-11-job-1-4f1118""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,090"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (11, 2, 'output') with pod batch-11-job-2-ad1587""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,093"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (11, 3, 'output') with pod batch-11-job-3-d826dd""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,093"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_pods:1221"", ""message"": ""restarting ready and running jobs with pods not seen in k8s""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,093"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_pods:1225"", ""message"": ""restarting job (9, 1, 'main')""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,093"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (9, 1, 'main') with pod None""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,093"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1148"", ""message"": ""job (9, 1, 'main') mark unscheduled""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,106"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_pvc:1237"", ""message"": ""k8s had 3 pvcs""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,113"", ""filename"": ""ba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:15845,Integrability,message,message,15845,""", ""message"": ""started k8s state refresh""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,085"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_pods:1210"", ""message"": ""k8s had 3 pods""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,088"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (11, 1, 'output') with pod batch-11-job-1-4f1118""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,090"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (11, 2, 'output') with pod batch-11-job-2-ad1587""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,093"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (11, 3, 'output') with pod batch-11-job-3-d826dd""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,093"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_pods:1221"", ""message"": ""restarting ready and running jobs with pods not seen in k8s""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,093"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_pods:1225"", ""message"": ""restarting job (9, 1, 'main')""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,093"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (9, 1, 'main') with pod None""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,093"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1148"", ""message"": ""job (9, 1, 'main') mark unscheduled""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,106"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_pvc:1237"", ""message"": ""k8s had 3 pvcs""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,113"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_state:1264"", ""message"": ""k8s state refresh complete""}; ```. Because job 1's batch is cancelled, we never restart the job/pod. However, the batch is stuck in an inco",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
