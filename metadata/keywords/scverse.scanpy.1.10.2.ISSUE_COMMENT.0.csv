id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/scverse/scanpy/issues/11#issuecomment-285361766:138,Availability,reliab,reliable,138,"No, there is no such way in DPT. We had good experience with manually choosing it. In our opinion, no one really came up with a sound and reliable statistical way of detecting the number of branching points, independent of the underlying algorithm. The best attempts to solve the problem though might be found within [Monocle 2](http://biorxiv.org/content/early/2017/02/21/110668) or [K-Branches](http://biorxiv.org/content/early/2016/12/15/094532).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/11#issuecomment-285361766
https://github.com/scverse/scanpy/issues/11#issuecomment-285361766:166,Safety,detect,detecting,166,"No, there is no such way in DPT. We had good experience with manually choosing it. In our opinion, no one really came up with a sound and reliable statistical way of detecting the number of branching points, independent of the underlying algorithm. The best attempts to solve the problem though might be found within [Monocle 2](http://biorxiv.org/content/early/2017/02/21/110668) or [K-Branches](http://biorxiv.org/content/early/2016/12/15/094532).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/11#issuecomment-285361766
https://github.com/scverse/scanpy/issues/11#issuecomment-302212941:15,Usability,feedback,feedback,15,"Thanks for the feedback! It astonishes me a bit, though, that Monocle 2 doesn't work well on that example. For the Paul et al, Cell (2015), it should give very nice results ([link to preprocessing](https://github.com/theislab/scanpy/blob/a2a330fa4640fdd4847fb48970a743242936e1df/scanpy/examples/builtin.py#L183-L199) / [link to plots](https://github.com/theislab/scanpy_usage/blob/master/EXAMPLES.md#paul15)), as they show in the recent preprint I reference above.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/11#issuecomment-302212941
https://github.com/scverse/scanpy/issues/19#issuecomment-380415001:91,Energy Efficiency,adapt,adapting,91,"As we don't have any extensions anymore we can close this for now. Also, many people start adapting numba. In the context of Scanpy this should usually be enough... let's talk again in case we need it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/19#issuecomment-380415001
https://github.com/scverse/scanpy/issues/19#issuecomment-380415001:91,Modifiability,adapt,adapting,91,"As we don't have any extensions anymore we can close this for now. Also, many people start adapting numba. In the context of Scanpy this should usually be enough... let's talk again in case we need it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/19#issuecomment-380415001
https://github.com/scverse/scanpy/pull/20#issuecomment-304808803:101,Deployability,integrat,integrating,101,"Hey Phil! I'm still a bit hesitant to adopt this option. As discussed before, I'm mainly planning on integrating c++ extensions. Then doing everything via Cython seems overhead. Even the latest Cython distributions recommends **not** doing it. See [here](http://cython.readthedocs.io/en/latest/src/reference/compilation.html) and search for *distributing cython modules*. They provide a solution very similar to the one fom stackoverflow that we have adopted right now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/20#issuecomment-304808803
https://github.com/scverse/scanpy/pull/20#issuecomment-304808803:101,Integrability,integrat,integrating,101,"Hey Phil! I'm still a bit hesitant to adopt this option. As discussed before, I'm mainly planning on integrating c++ extensions. Then doing everything via Cython seems overhead. Even the latest Cython distributions recommends **not** doing it. See [here](http://cython.readthedocs.io/en/latest/src/reference/compilation.html) and search for *distributing cython modules*. They provide a solution very similar to the one fom stackoverflow that we have adopted right now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/20#issuecomment-304808803
https://github.com/scverse/scanpy/issues/25#issuecomment-309980879:867,Energy Efficiency,reduce,reduce,867,"No, there is no way within Scanpy. I'll talk to Philipp about the find_sigmas function... and get back to you. My personal opinion is that in a wide range of values, the qualitative (significant) results should be independent of the value of k. The default value of k=30, meaning that we construct a k-nearest neighbor graph in which each cell is connected with 30 neighbors, yields good results on all data sets (>10) that I worked with so far. If you have very little noise, for example, by selecting only very few highly variable genes in the preprocessing, you might obtain a more ""pronounced structure"" by reducing k (I'd recommend at least 3, though). Also with very noisy data, k=30 should be high enough to average out noise effects. To summarize, k=30 is a conservative choice that in my experience does the job for everything. In some cases, it pays off to reduce the value.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/25#issuecomment-309980879
https://github.com/scverse/scanpy/issues/25#issuecomment-309980879:524,Modifiability,variab,variable,524,"No, there is no way within Scanpy. I'll talk to Philipp about the find_sigmas function... and get back to you. My personal opinion is that in a wide range of values, the qualitative (significant) results should be independent of the value of k. The default value of k=30, meaning that we construct a k-nearest neighbor graph in which each cell is connected with 30 neighbors, yields good results on all data sets (>10) that I worked with so far. If you have very little noise, for example, by selecting only very few highly variable genes in the preprocessing, you might obtain a more ""pronounced structure"" by reducing k (I'd recommend at least 3, though). Also with very noisy data, k=30 should be high enough to average out noise effects. To summarize, k=30 is a conservative choice that in my experience does the job for everything. In some cases, it pays off to reduce the value.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/25#issuecomment-309980879
https://github.com/scverse/scanpy/issues/25#issuecomment-310073367:143,Usability,simpl,simply,143,"No, not right now. You have to do this with numpy or pandas. adata is just a collection of numpy arrays and a dict (`print(adata)`) So you can simply do this manually. If you explain in a more detailed way what you want, I can probably also quickly implement it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/25#issuecomment-310073367
https://github.com/scverse/scanpy/issues/25#issuecomment-310275631:26,Integrability,rout,route,26,"Just in general: the main route to ""improving the results"" is to make sure that your preprocessing yields a meaningful tSNE for the standard parameters. Then also DPT will perfectly do its job.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/25#issuecomment-310275631
https://github.com/scverse/scanpy/issues/25#issuecomment-310611678:258,Usability,simpl,simply,258,"The tSNE looks ""good"" if parts of the data that you expect to be connected actually look connected in the tSNE and do not cluster apart. In your case, it looks a bit too clustered, but seeing the DiffMaps, everything turns out to be fine. The second example simply doesn't seem to have a branching.; PS: There will soon be a new default tSNE that will ensure that the correspondence between DiffMap and tSNE is better. Still, of course, both visualization methods focus on different aspects of the data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/25#issuecomment-310611678
https://github.com/scverse/scanpy/issues/25#issuecomment-313218328:90,Availability,error,error,90,"Is there any particular reason why DPT fails when you don't have enough genes? I get this error when I select 23 genes for use. ```; scipy.sparse.linalg.eigen.arpack.arpack.ArpackError: ARPACK error 3:; No shifts could be applied during a cycle of the Implicitly restarted Arnoldi iteration.; One possibility is to increase the size of NCV relative to NEV.; ```. with a lot of warnings. Also, is there any particular reason why on some datasets, you can't find significant genes via filter_gene_dispersion? Does that mean that dataset is bad?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/25#issuecomment-313218328
https://github.com/scverse/scanpy/issues/25#issuecomment-313218328:193,Availability,error,error,193,"Is there any particular reason why DPT fails when you don't have enough genes? I get this error when I select 23 genes for use. ```; scipy.sparse.linalg.eigen.arpack.arpack.ArpackError: ARPACK error 3:; No shifts could be applied during a cycle of the Implicitly restarted Arnoldi iteration.; One possibility is to increase the size of NCV relative to NEV.; ```. with a lot of warnings. Also, is there any particular reason why on some datasets, you can't find significant genes via filter_gene_dispersion? Does that mean that dataset is bad?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/25#issuecomment-313218328
https://github.com/scverse/scanpy/issues/25#issuecomment-313320910:156,Deployability,toggle,toggleswitch,156,"No, there should not be any reason that is associated with a small number of genes per se. In the moignard15 example, everything works for 40 genes; in the toggleswitch, everything works for 2 genes. Does your PCA look meaningful? Try supplying a very small number of PCs to DPT (`n_pcs=3` or so). If you do not find significant genes with `filter_genes_dispersion`, you have to adapt the parameters [e.g. set `min_disp` to a lower value](https://github.com/theislab/scanpy/blob/2cea8341e28eb8d0658f62d010631f77465e16d7/scanpy/preprocessing/simple.py#L132-L177). See the example [here](https://github.com/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb). Alternatively, you can simply select the `n_top_genes` highest variabale genes by setting `flavor` to `'cell_ranger'`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/25#issuecomment-313320910
https://github.com/scverse/scanpy/issues/25#issuecomment-313320910:379,Energy Efficiency,adapt,adapt,379,"No, there should not be any reason that is associated with a small number of genes per se. In the moignard15 example, everything works for 40 genes; in the toggleswitch, everything works for 2 genes. Does your PCA look meaningful? Try supplying a very small number of PCs to DPT (`n_pcs=3` or so). If you do not find significant genes with `filter_genes_dispersion`, you have to adapt the parameters [e.g. set `min_disp` to a lower value](https://github.com/theislab/scanpy/blob/2cea8341e28eb8d0658f62d010631f77465e16d7/scanpy/preprocessing/simple.py#L132-L177). See the example [here](https://github.com/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb). Alternatively, you can simply select the `n_top_genes` highest variabale genes by setting `flavor` to `'cell_ranger'`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/25#issuecomment-313320910
https://github.com/scverse/scanpy/issues/25#issuecomment-313320910:379,Modifiability,adapt,adapt,379,"No, there should not be any reason that is associated with a small number of genes per se. In the moignard15 example, everything works for 40 genes; in the toggleswitch, everything works for 2 genes. Does your PCA look meaningful? Try supplying a very small number of PCs to DPT (`n_pcs=3` or so). If you do not find significant genes with `filter_genes_dispersion`, you have to adapt the parameters [e.g. set `min_disp` to a lower value](https://github.com/theislab/scanpy/blob/2cea8341e28eb8d0658f62d010631f77465e16d7/scanpy/preprocessing/simple.py#L132-L177). See the example [here](https://github.com/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb). Alternatively, you can simply select the `n_top_genes` highest variabale genes by setting `flavor` to `'cell_ranger'`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/25#issuecomment-313320910
https://github.com/scverse/scanpy/issues/25#issuecomment-313320910:731,Modifiability,variab,variabale,731,"No, there should not be any reason that is associated with a small number of genes per se. In the moignard15 example, everything works for 40 genes; in the toggleswitch, everything works for 2 genes. Does your PCA look meaningful? Try supplying a very small number of PCs to DPT (`n_pcs=3` or so). If you do not find significant genes with `filter_genes_dispersion`, you have to adapt the parameters [e.g. set `min_disp` to a lower value](https://github.com/theislab/scanpy/blob/2cea8341e28eb8d0658f62d010631f77465e16d7/scanpy/preprocessing/simple.py#L132-L177). See the example [here](https://github.com/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb). Alternatively, you can simply select the `n_top_genes` highest variabale genes by setting `flavor` to `'cell_ranger'`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/25#issuecomment-313320910
https://github.com/scverse/scanpy/issues/25#issuecomment-313320910:541,Usability,simpl,simple,541,"No, there should not be any reason that is associated with a small number of genes per se. In the moignard15 example, everything works for 40 genes; in the toggleswitch, everything works for 2 genes. Does your PCA look meaningful? Try supplying a very small number of PCs to DPT (`n_pcs=3` or so). If you do not find significant genes with `filter_genes_dispersion`, you have to adapt the parameters [e.g. set `min_disp` to a lower value](https://github.com/theislab/scanpy/blob/2cea8341e28eb8d0658f62d010631f77465e16d7/scanpy/preprocessing/simple.py#L132-L177). See the example [here](https://github.com/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb). Alternatively, you can simply select the `n_top_genes` highest variabale genes by setting `flavor` to `'cell_ranger'`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/25#issuecomment-313320910
https://github.com/scverse/scanpy/issues/25#issuecomment-313320910:691,Usability,simpl,simply,691,"No, there should not be any reason that is associated with a small number of genes per se. In the moignard15 example, everything works for 40 genes; in the toggleswitch, everything works for 2 genes. Does your PCA look meaningful? Try supplying a very small number of PCs to DPT (`n_pcs=3` or so). If you do not find significant genes with `filter_genes_dispersion`, you have to adapt the parameters [e.g. set `min_disp` to a lower value](https://github.com/theislab/scanpy/blob/2cea8341e28eb8d0658f62d010631f77465e16d7/scanpy/preprocessing/simple.py#L132-L177). See the example [here](https://github.com/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb). Alternatively, you can simply select the `n_top_genes` highest variabale genes by setting `flavor` to `'cell_ranger'`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/25#issuecomment-313320910
https://github.com/scverse/scanpy/issues/29#issuecomment-321767948:81,Usability,simpl,simply,81,"PS: If you have a conda environment for Python 3, you do not need to use `pip3`; simply use `pip`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/29#issuecomment-321767948
https://github.com/scverse/scanpy/issues/29#issuecomment-321769746:96,Security,Hash,Hashem,96,"Hi Alex,; Thanks for your very prompt reply. The problem is solved.; Looking forward using it.; Hashem",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/29#issuecomment-321769746
https://github.com/scverse/scanpy/issues/29#issuecomment-321782798:151,Deployability,install,installable,151,"hi, sorry for the confusion. i removed the comments not relevant to the issue. for the record:. - continuum inc. is responsible for providing packages installable with `conda`, we can’t influence that IIRC. they will probably eventually include scanpy.; - python comes with `pip` by default, and `pip` can be used to install all packages on [the python package index](https://pypi.python.org); - the only advantage of anaconda is that it comes with preinstalled packages, but installing scanpy via `pip` will always be just as fast as it will eventually be with `conda`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/29#issuecomment-321782798
https://github.com/scverse/scanpy/issues/29#issuecomment-321782798:317,Deployability,install,install,317,"hi, sorry for the confusion. i removed the comments not relevant to the issue. for the record:. - continuum inc. is responsible for providing packages installable with `conda`, we can’t influence that IIRC. they will probably eventually include scanpy.; - python comes with `pip` by default, and `pip` can be used to install all packages on [the python package index](https://pypi.python.org); - the only advantage of anaconda is that it comes with preinstalled packages, but installing scanpy via `pip` will always be just as fast as it will eventually be with `conda`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/29#issuecomment-321782798
https://github.com/scverse/scanpy/issues/29#issuecomment-321782798:476,Deployability,install,installing,476,"hi, sorry for the confusion. i removed the comments not relevant to the issue. for the record:. - continuum inc. is responsible for providing packages installable with `conda`, we can’t influence that IIRC. they will probably eventually include scanpy.; - python comes with `pip` by default, and `pip` can be used to install all packages on [the python package index](https://pypi.python.org); - the only advantage of anaconda is that it comes with preinstalled packages, but installing scanpy via `pip` will always be just as fast as it will eventually be with `conda`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/29#issuecomment-321782798
https://github.com/scverse/scanpy/issues/32#issuecomment-324116498:63,Deployability,install,install,63,"Hi Sarah,; thanks for the note and sorry about that; would you install a stable release from PyPi in the meanwhile `pip install scanpy`? I'm currently rewriting quite substantial parts and yes, this is clearly a bug I caused on the weekend; testing will also be more extensive in the future so that this stuff does happen anymore. This kind of stuff will also not happen on master branch in the future; but this rewriting goes along with building some [documentation](https://scanpy.readthedocs.io) and this builds from master... ; Cheers,; Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/32#issuecomment-324116498
https://github.com/scverse/scanpy/issues/32#issuecomment-324116498:80,Deployability,release,release,80,"Hi Sarah,; thanks for the note and sorry about that; would you install a stable release from PyPi in the meanwhile `pip install scanpy`? I'm currently rewriting quite substantial parts and yes, this is clearly a bug I caused on the weekend; testing will also be more extensive in the future so that this stuff does happen anymore. This kind of stuff will also not happen on master branch in the future; but this rewriting goes along with building some [documentation](https://scanpy.readthedocs.io) and this builds from master... ; Cheers,; Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/32#issuecomment-324116498
https://github.com/scverse/scanpy/issues/32#issuecomment-324116498:120,Deployability,install,install,120,"Hi Sarah,; thanks for the note and sorry about that; would you install a stable release from PyPi in the meanwhile `pip install scanpy`? I'm currently rewriting quite substantial parts and yes, this is clearly a bug I caused on the weekend; testing will also be more extensive in the future so that this stuff does happen anymore. This kind of stuff will also not happen on master branch in the future; but this rewriting goes along with building some [documentation](https://scanpy.readthedocs.io) and this builds from master... ; Cheers,; Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/32#issuecomment-324116498
https://github.com/scverse/scanpy/issues/32#issuecomment-324116498:241,Testability,test,testing,241,"Hi Sarah,; thanks for the note and sorry about that; would you install a stable release from PyPi in the meanwhile `pip install scanpy`? I'm currently rewriting quite substantial parts and yes, this is clearly a bug I caused on the weekend; testing will also be more extensive in the future so that this stuff does happen anymore. This kind of stuff will also not happen on master branch in the future; but this rewriting goes along with building some [documentation](https://scanpy.readthedocs.io) and this builds from master... ; Cheers,; Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/32#issuecomment-324116498
https://github.com/scverse/scanpy/issues/32#issuecomment-324116498:202,Usability,clear,clearly,202,"Hi Sarah,; thanks for the note and sorry about that; would you install a stable release from PyPi in the meanwhile `pip install scanpy`? I'm currently rewriting quite substantial parts and yes, this is clearly a bug I caused on the weekend; testing will also be more extensive in the future so that this stuff does happen anymore. This kind of stuff will also not happen on master branch in the future; but this rewriting goes along with building some [documentation](https://scanpy.readthedocs.io) and this builds from master... ; Cheers,; Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/32#issuecomment-324116498
https://github.com/scverse/scanpy/issues/32#issuecomment-324379251:58,Deployability,install,install,58,Could you pull the current version (0.2.7) from github or install it via pip? Everything should be back to normal now.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/32#issuecomment-324379251
https://github.com/scverse/scanpy/issues/34#issuecomment-324336195:70,Availability,error,error,70,Ok I think this is a bug - notebooks paul15.ipynb fails with the same error - see Section ; ### Using a preprocessing recipe. adata = paul15_raw(); sc.pp.recipe_zheng17(adata),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/34#issuecomment-324336195
https://github.com/scverse/scanpy/issues/34#issuecomment-324337710:241,Testability,test,tests,241,"Hi Alexis,; sorry about that. I made substantial changes to set up and build the [docs](https://scanpy.readthedocs.io) remotely just in the past days and for that had to experiment on the master branch. I'm fixing everything tonight running tests on all example notebooks. I will also incude more tests in the future so that stuff like this doesn't happen anymore.; Cheers,; Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/34#issuecomment-324337710
https://github.com/scverse/scanpy/issues/34#issuecomment-324337710:297,Testability,test,tests,297,"Hi Alexis,; sorry about that. I made substantial changes to set up and build the [docs](https://scanpy.readthedocs.io) remotely just in the past days and for that had to experiment on the master branch. I'm fixing everything tonight running tests on all example notebooks. I will also incude more tests in the future so that stuff like this doesn't happen anymore.; Cheers,; Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/34#issuecomment-324337710
https://github.com/scverse/scanpy/issues/34#issuecomment-324338365:44,Deployability,release,release,44,"I let you know as soon as there is a stable release back on github. 0.2.5 should be stable [as well](https://github.com/theislab/scanpy_usage), but has some other drawbacks. Things are still progressing fast.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/34#issuecomment-324338365
https://github.com/scverse/scanpy/issues/34#issuecomment-324348777:44,Testability,test,tests,44,You could use the notebooks as part of your tests - see [GPflow notebook tests](https://github.com/GPflow/GPflow/blob/master/testing/test_notebooks.py) as an example.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/34#issuecomment-324348777
https://github.com/scverse/scanpy/issues/34#issuecomment-324348777:73,Testability,test,tests,73,You could use the notebooks as part of your tests - see [GPflow notebook tests](https://github.com/GPflow/GPflow/blob/master/testing/test_notebooks.py) as an example.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/34#issuecomment-324348777
https://github.com/scverse/scanpy/issues/34#issuecomment-324348777:125,Testability,test,testing,125,You could use the notebooks as part of your tests - see [GPflow notebook tests](https://github.com/GPflow/GPflow/blob/master/testing/test_notebooks.py) as an example.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/34#issuecomment-324348777
https://github.com/scverse/scanpy/issues/34#issuecomment-324376016:297,Testability,test,testable,297,"Thanks for the suggestions and that concrete implementation! :smile: @flying-sheep has been suggesting this for quite some time already, but has not yet found the time to implement it. The difficulty will be to replace the visual inspection of all the plots from version to version with something testable.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/34#issuecomment-324376016
https://github.com/scverse/scanpy/issues/34#issuecomment-324378094:333,Safety,avoid,avoid,333,"Regarding your other bug: `scanpy.plotting` used to have the attribute and `scanpy.api.plotting` would simply import the module. To make the [overview of the API](https://scanpy.readthedocs.io/en/latest/api.html) work, I had to introduce a [dummy module](https://github.com/theislab/scanpy/blob/master/scanpy/api/pl.py). In order to avoid duplication, I removed all exports from `scanpy.plotting.__init__`. I readded it to fix the bug on the development branch, but I need to think of a better solution.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/34#issuecomment-324378094
https://github.com/scverse/scanpy/issues/34#issuecomment-324378094:103,Usability,simpl,simply,103,"Regarding your other bug: `scanpy.plotting` used to have the attribute and `scanpy.api.plotting` would simply import the module. To make the [overview of the API](https://scanpy.readthedocs.io/en/latest/api.html) work, I had to introduce a [dummy module](https://github.com/theislab/scanpy/blob/master/scanpy/api/pl.py). In order to avoid duplication, I removed all exports from `scanpy.plotting.__init__`. I readded it to fix the bug on the development branch, but I need to think of a better solution.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/34#issuecomment-324378094
https://github.com/scverse/scanpy/issues/34#issuecomment-324469115:30,Availability,error,error,30,"OK; now I have more time. The error thrown at ; ```; 207 df['dispersion_norm'] = (df['dispersion'].values # use values here as index differs; --> 208 - disp_mean_bin[df['mean_bin']].values) \; 209 / disp_std_bin[df['mean_bin']].values; ```; astonishes me. The line has been working for me on pandas 0.19.2 and 0.20.3 and for others for other versions for many months already. Do you have an old pandas version? The line should work as `disp_mean_bin` has been computed from `disp_grouped = df.groupby('mean_bin')['dispersion']` [here](https://github.com/theislab/scanpy/blob/65503d34d6b9d0a1d23e831d6daeba86856b3eee/scanpy/preprocessing/simple.py#L215); i.e., the Series 'mean_bin' was used to initialize the index of `disp_mean_bin`. Hence, you should be able to index with 'mean_bin'.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/34#issuecomment-324469115
https://github.com/scverse/scanpy/issues/34#issuecomment-324469115:637,Usability,simpl,simple,637,"OK; now I have more time. The error thrown at ; ```; 207 df['dispersion_norm'] = (df['dispersion'].values # use values here as index differs; --> 208 - disp_mean_bin[df['mean_bin']].values) \; 209 / disp_std_bin[df['mean_bin']].values; ```; astonishes me. The line has been working for me on pandas 0.19.2 and 0.20.3 and for others for other versions for many months already. Do you have an old pandas version? The line should work as `disp_mean_bin` has been computed from `disp_grouped = df.groupby('mean_bin')['dispersion']` [here](https://github.com/theislab/scanpy/blob/65503d34d6b9d0a1d23e831d6daeba86856b3eee/scanpy/preprocessing/simple.py#L215); i.e., the Series 'mean_bin' was used to initialize the index of `disp_mean_bin`. Hence, you should be able to index with 'mean_bin'.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/34#issuecomment-324469115
https://github.com/scverse/scanpy/pull/38#issuecomment-335447670:136,Testability,test,test,136,"Phil, thanks for this! I'm slowly finding time again to deal with these things. I looked through it and it's a very nice solution. I'll test it these days and merge it into master. :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/38#issuecomment-335447670
https://github.com/scverse/scanpy/issues/39#issuecomment-333509613:306,Deployability,install,installing,306,"Hi and sorry for the very late response!. 1. Hm, this seems to be related to your matplolib version and I've never seen this before. The code for the plotting function is [here](https://github.com/theislab/scanpy/blob/a17e9f4bac124547fec1c373da8d12b679c84bcc/scanpy/plotting/preprocessing.py#L11-L46). Try installing matplotlib 2.0.0. 2. The warning can be ignored, in my experience. Soon, we'll catch that case explicitly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/39#issuecomment-333509613
https://github.com/scverse/scanpy/issues/40#issuecomment-333528844:64,Deployability,release,release,64,"Mmh, very strange. Graph abstraction will be in the next Scanpy release and is not stable yet... Are you simply running the [minimal example](https://github.com/theislab/graph_abstraction/blob/master/minimal_examples/minimal_examples.ipynb)? Maybe reread and reload your data? At some point a few months ago, the format for AnnData files changed. Also, the master branch on Github doesn't have all tests on all notebooks yet, I'd recommend to wait until the release that is scheduled for the next week. Cheers,; alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/40#issuecomment-333528844
https://github.com/scverse/scanpy/issues/40#issuecomment-333528844:458,Deployability,release,release,458,"Mmh, very strange. Graph abstraction will be in the next Scanpy release and is not stable yet... Are you simply running the [minimal example](https://github.com/theislab/graph_abstraction/blob/master/minimal_examples/minimal_examples.ipynb)? Maybe reread and reload your data? At some point a few months ago, the format for AnnData files changed. Also, the master branch on Github doesn't have all tests on all notebooks yet, I'd recommend to wait until the release that is scheduled for the next week. Cheers,; alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/40#issuecomment-333528844
https://github.com/scverse/scanpy/issues/40#issuecomment-333528844:474,Energy Efficiency,schedul,scheduled,474,"Mmh, very strange. Graph abstraction will be in the next Scanpy release and is not stable yet... Are you simply running the [minimal example](https://github.com/theislab/graph_abstraction/blob/master/minimal_examples/minimal_examples.ipynb)? Maybe reread and reload your data? At some point a few months ago, the format for AnnData files changed. Also, the master branch on Github doesn't have all tests on all notebooks yet, I'd recommend to wait until the release that is scheduled for the next week. Cheers,; alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/40#issuecomment-333528844
https://github.com/scverse/scanpy/issues/40#issuecomment-333528844:398,Testability,test,tests,398,"Mmh, very strange. Graph abstraction will be in the next Scanpy release and is not stable yet... Are you simply running the [minimal example](https://github.com/theislab/graph_abstraction/blob/master/minimal_examples/minimal_examples.ipynb)? Maybe reread and reload your data? At some point a few months ago, the format for AnnData files changed. Also, the master branch on Github doesn't have all tests on all notebooks yet, I'd recommend to wait until the release that is scheduled for the next week. Cheers,; alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/40#issuecomment-333528844
https://github.com/scverse/scanpy/issues/40#issuecomment-333528844:105,Usability,simpl,simply,105,"Mmh, very strange. Graph abstraction will be in the next Scanpy release and is not stable yet... Are you simply running the [minimal example](https://github.com/theislab/graph_abstraction/blob/master/minimal_examples/minimal_examples.ipynb)? Maybe reread and reload your data? At some point a few months ago, the format for AnnData files changed. Also, the master branch on Github doesn't have all tests on all notebooks yet, I'd recommend to wait until the release that is scheduled for the next week. Cheers,; alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/40#issuecomment-333528844
https://github.com/scverse/scanpy/issues/41#issuecomment-347357609:103,Performance,load,load,103,"We're still hesitant about making AnnData more complex, for these reasons:; * It is not inefficient to load multiple versions of the full data into AnnData.; * It is not straightforward to determine the point of the preprocessing at which one would want to save a version of the raw data (probably after filtering out cells and taking the logarithm, but this might change in the future).; As the second point implies that some manual intervention would be necessary, anyway, we tend to leave it to the user to keep track of one, two or more versions of the data; each with annotations that can easily be exchanged. Specifically, would you be happy to proceed as in differential expression tests, see e.g., https://github.com/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb? You keep track of two versions of the data, one for doing all the machine learning inference and another one for doing statistics and plotting. Using the linked example: for plotting, you would simply need to add the visualization basis to the AnnData that stores the raw data. Then you call `sc.pl.tsne`.; ```; adata_corrected = sc.read('pbmc3k_corrected'); adata_raw = sc.read('pbmc3k_filtered_raw_log'); adata_raw.smpm['X_tsne'] = adata_corrected.smpm['X_tsne']; adata_raw.smpm['X_pca'] = adata_corrected.smpm['X_pca']; sc.pl.tsne(adata_raw, color='NKG7'); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/41#issuecomment-347357609
https://github.com/scverse/scanpy/issues/41#issuecomment-347357609:339,Testability,log,logarithm,339,"We're still hesitant about making AnnData more complex, for these reasons:; * It is not inefficient to load multiple versions of the full data into AnnData.; * It is not straightforward to determine the point of the preprocessing at which one would want to save a version of the raw data (probably after filtering out cells and taking the logarithm, but this might change in the future).; As the second point implies that some manual intervention would be necessary, anyway, we tend to leave it to the user to keep track of one, two or more versions of the data; each with annotations that can easily be exchanged. Specifically, would you be happy to proceed as in differential expression tests, see e.g., https://github.com/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb? You keep track of two versions of the data, one for doing all the machine learning inference and another one for doing statistics and plotting. Using the linked example: for plotting, you would simply need to add the visualization basis to the AnnData that stores the raw data. Then you call `sc.pl.tsne`.; ```; adata_corrected = sc.read('pbmc3k_corrected'); adata_raw = sc.read('pbmc3k_filtered_raw_log'); adata_raw.smpm['X_tsne'] = adata_corrected.smpm['X_tsne']; adata_raw.smpm['X_pca'] = adata_corrected.smpm['X_pca']; sc.pl.tsne(adata_raw, color='NKG7'); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/41#issuecomment-347357609
https://github.com/scverse/scanpy/issues/41#issuecomment-347357609:689,Testability,test,tests,689,"We're still hesitant about making AnnData more complex, for these reasons:; * It is not inefficient to load multiple versions of the full data into AnnData.; * It is not straightforward to determine the point of the preprocessing at which one would want to save a version of the raw data (probably after filtering out cells and taking the logarithm, but this might change in the future).; As the second point implies that some manual intervention would be necessary, anyway, we tend to leave it to the user to keep track of one, two or more versions of the data; each with annotations that can easily be exchanged. Specifically, would you be happy to proceed as in differential expression tests, see e.g., https://github.com/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb? You keep track of two versions of the data, one for doing all the machine learning inference and another one for doing statistics and plotting. Using the linked example: for plotting, you would simply need to add the visualization basis to the AnnData that stores the raw data. Then you call `sc.pl.tsne`.; ```; adata_corrected = sc.read('pbmc3k_corrected'); adata_raw = sc.read('pbmc3k_filtered_raw_log'); adata_raw.smpm['X_tsne'] = adata_corrected.smpm['X_tsne']; adata_raw.smpm['X_pca'] = adata_corrected.smpm['X_pca']; sc.pl.tsne(adata_raw, color='NKG7'); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/41#issuecomment-347357609
https://github.com/scverse/scanpy/issues/41#issuecomment-347357609:861,Usability,learn,learning,861,"We're still hesitant about making AnnData more complex, for these reasons:; * It is not inefficient to load multiple versions of the full data into AnnData.; * It is not straightforward to determine the point of the preprocessing at which one would want to save a version of the raw data (probably after filtering out cells and taking the logarithm, but this might change in the future).; As the second point implies that some manual intervention would be necessary, anyway, we tend to leave it to the user to keep track of one, two or more versions of the data; each with annotations that can easily be exchanged. Specifically, would you be happy to proceed as in differential expression tests, see e.g., https://github.com/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb? You keep track of two versions of the data, one for doing all the machine learning inference and another one for doing statistics and plotting. Using the linked example: for plotting, you would simply need to add the visualization basis to the AnnData that stores the raw data. Then you call `sc.pl.tsne`.; ```; adata_corrected = sc.read('pbmc3k_corrected'); adata_raw = sc.read('pbmc3k_filtered_raw_log'); adata_raw.smpm['X_tsne'] = adata_corrected.smpm['X_tsne']; adata_raw.smpm['X_pca'] = adata_corrected.smpm['X_pca']; sc.pl.tsne(adata_raw, color='NKG7'); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/41#issuecomment-347357609
https://github.com/scverse/scanpy/issues/41#issuecomment-347357609:981,Usability,simpl,simply,981,"We're still hesitant about making AnnData more complex, for these reasons:; * It is not inefficient to load multiple versions of the full data into AnnData.; * It is not straightforward to determine the point of the preprocessing at which one would want to save a version of the raw data (probably after filtering out cells and taking the logarithm, but this might change in the future).; As the second point implies that some manual intervention would be necessary, anyway, we tend to leave it to the user to keep track of one, two or more versions of the data; each with annotations that can easily be exchanged. Specifically, would you be happy to proceed as in differential expression tests, see e.g., https://github.com/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb? You keep track of two versions of the data, one for doing all the machine learning inference and another one for doing statistics and plotting. Using the linked example: for plotting, you would simply need to add the visualization basis to the AnnData that stores the raw data. Then you call `sc.pl.tsne`.; ```; adata_corrected = sc.read('pbmc3k_corrected'); adata_raw = sc.read('pbmc3k_filtered_raw_log'); adata_raw.smpm['X_tsne'] = adata_corrected.smpm['X_tsne']; adata_raw.smpm['X_pca'] = adata_corrected.smpm['X_pca']; sc.pl.tsne(adata_raw, color='NKG7'); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/41#issuecomment-347357609
https://github.com/scverse/scanpy/issues/41#issuecomment-353766971:61,Performance,scalab,scalable,61,"Finally, we could solve this elegantly without sacrificing a scalable design, as shown in the [tutorial](https://github.com/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb). Also, Scanpy is accepted in Genome Biology and will soon be published. Merry Christmas! :); Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/41#issuecomment-353766971
https://github.com/scverse/scanpy/issues/41#issuecomment-395336870:8,Modifiability,extend,extend,8,Can you extend scanpy functions so that I can show gene expression level on plot generated by sc.pl.diffmap? just like that monocle2 does.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/41#issuecomment-395336870
https://github.com/scverse/scanpy/issues/41#issuecomment-395337961:108,Modifiability,variab,variables,108,"And, in which step should I execute MNN batch effect correction ? Is it still necessary to regress out some variables ( n_counts, percent_mito et al.,) when I execute MNN ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/41#issuecomment-395337961
https://github.com/scverse/scanpy/issues/43#issuecomment-342897102:184,Availability,error,error,184,It turned out that this is definitely something wrong with my system setup. After I circumvented the bug above by clearing `README.rst` I found another package that spits out the same error (`louvain`).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/43#issuecomment-342897102
https://github.com/scverse/scanpy/issues/43#issuecomment-342897102:114,Usability,clear,clearing,114,It turned out that this is definitely something wrong with my system setup. After I circumvented the bug above by clearing `README.rst` I found another package that spits out the same error (`louvain`).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/43#issuecomment-342897102
https://github.com/scverse/scanpy/issues/43#issuecomment-343252579:188,Availability,error,error,188,"So actually, I run a test on a fresh docker image (with this [Dockerfile](https://gist.github.com/pwl/005c781cbe19f5e961b59366f738caaf)) and it still fails to install scanpy with the same error. I had some success with changing the default python encoding to utf-8 as shown in the Dockerfile but it only works when calling python3 directly and not for pip3. However, it worked with python2. I guess python3 is not supported by scanpy, is that correct?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/43#issuecomment-343252579
https://github.com/scverse/scanpy/issues/43#issuecomment-343252579:159,Deployability,install,install,159,"So actually, I run a test on a fresh docker image (with this [Dockerfile](https://gist.github.com/pwl/005c781cbe19f5e961b59366f738caaf)) and it still fails to install scanpy with the same error. I had some success with changing the default python encoding to utf-8 as shown in the Dockerfile but it only works when calling python3 directly and not for pip3. However, it worked with python2. I guess python3 is not supported by scanpy, is that correct?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/43#issuecomment-343252579
https://github.com/scverse/scanpy/issues/43#issuecomment-343252579:21,Testability,test,test,21,"So actually, I run a test on a fresh docker image (with this [Dockerfile](https://gist.github.com/pwl/005c781cbe19f5e961b59366f738caaf)) and it still fails to install scanpy with the same error. I had some success with changing the default python encoding to utf-8 as shown in the Dockerfile but it only works when calling python3 directly and not for pip3. However, it worked with python2. I guess python3 is not supported by scanpy, is that correct?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/43#issuecomment-343252579
https://github.com/scverse/scanpy/issues/43#issuecomment-343484072:323,Availability,down,down,323,"it should definitely work. on a properly configured system (including docker images), the encoding should be UTF-8. you’re right, we should probably do it. the only reason we didn’t yet is that we open quite a few files in the codebase, and if one of those open calls expects UTF-8, it’ll break again, but this time deeper down and harder to reproduce.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/43#issuecomment-343484072
https://github.com/scverse/scanpy/issues/43#issuecomment-343484072:41,Modifiability,config,configured,41,"it should definitely work. on a properly configured system (including docker images), the encoding should be UTF-8. you’re right, we should probably do it. the only reason we didn’t yet is that we open quite a few files in the codebase, and if one of those open calls expects UTF-8, it’ll break again, but this time deeper down and harder to reproduce.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/43#issuecomment-343484072
https://github.com/scverse/scanpy/issues/43#issuecomment-343491172:250,Modifiability,config,configured,250,"How about [changing the encoding globally](https://stackoverflow.com/questions/2276200/changing-default-encoding-of-python#17628350)? Would that break anything? Also, there is the same bug with the package you rely on, `louvain`. As for the properly configured system, ubuntu:17.10 is the most generic and recent system I can think of, shouldn't it be properly configured out of the box? If not, is there a way to configure the system so that the encoding is globally set to utf-8?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/43#issuecomment-343491172
https://github.com/scverse/scanpy/issues/43#issuecomment-343491172:361,Modifiability,config,configured,361,"How about [changing the encoding globally](https://stackoverflow.com/questions/2276200/changing-default-encoding-of-python#17628350)? Would that break anything? Also, there is the same bug with the package you rely on, `louvain`. As for the properly configured system, ubuntu:17.10 is the most generic and recent system I can think of, shouldn't it be properly configured out of the box? If not, is there a way to configure the system so that the encoding is globally set to utf-8?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/43#issuecomment-343491172
https://github.com/scverse/scanpy/issues/43#issuecomment-343491172:414,Modifiability,config,configure,414,"How about [changing the encoding globally](https://stackoverflow.com/questions/2276200/changing-default-encoding-of-python#17628350)? Would that break anything? Also, there is the same bug with the package you rely on, `louvain`. As for the properly configured system, ubuntu:17.10 is the most generic and recent system I can think of, shouldn't it be properly configured out of the box? If not, is there a way to configure the system so that the encoding is globally set to utf-8?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/43#issuecomment-343491172
https://github.com/scverse/scanpy/issues/43#issuecomment-344015034:7,Usability,learn,learned,7,i just learned that OSX sends its locale per default when connecting to a server. so is it a local ubuntu or on a server?. what does `locale` (executed from a terminal) return?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/43#issuecomment-344015034
https://github.com/scverse/scanpy/issues/43#issuecomment-344235559:26,Availability,error,error,26,"I managed to get past the error by adding; ```; RUN locale-gen en_US.UTF-8; ENV LC_ALL en_US.UTF-8; ```; to the [Dockerfile](https://gist.github.com/pwl/a26726fda94ac7f4cbfb57e4fe98bf28). Before that the default locale was set to `POSIX`, which caused all of these problems. This is a weird choice of defaults as clearly python code doesn't work as expected. Thanks for helping out @flying-sheep!. EDIT: just to clarify, this dockerfile is not an example of how to install scanpy, it's just a demonstration of how to circumvent the issues with locales. In particular, several libraries are missing and scanpy does not complete the installation. Feel free to update this Dockerfile or add one to the scanpy repository.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/43#issuecomment-344235559
https://github.com/scverse/scanpy/issues/43#issuecomment-344235559:465,Deployability,install,install,465,"I managed to get past the error by adding; ```; RUN locale-gen en_US.UTF-8; ENV LC_ALL en_US.UTF-8; ```; to the [Dockerfile](https://gist.github.com/pwl/a26726fda94ac7f4cbfb57e4fe98bf28). Before that the default locale was set to `POSIX`, which caused all of these problems. This is a weird choice of defaults as clearly python code doesn't work as expected. Thanks for helping out @flying-sheep!. EDIT: just to clarify, this dockerfile is not an example of how to install scanpy, it's just a demonstration of how to circumvent the issues with locales. In particular, several libraries are missing and scanpy does not complete the installation. Feel free to update this Dockerfile or add one to the scanpy repository.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/43#issuecomment-344235559
https://github.com/scverse/scanpy/issues/43#issuecomment-344235559:631,Deployability,install,installation,631,"I managed to get past the error by adding; ```; RUN locale-gen en_US.UTF-8; ENV LC_ALL en_US.UTF-8; ```; to the [Dockerfile](https://gist.github.com/pwl/a26726fda94ac7f4cbfb57e4fe98bf28). Before that the default locale was set to `POSIX`, which caused all of these problems. This is a weird choice of defaults as clearly python code doesn't work as expected. Thanks for helping out @flying-sheep!. EDIT: just to clarify, this dockerfile is not an example of how to install scanpy, it's just a demonstration of how to circumvent the issues with locales. In particular, several libraries are missing and scanpy does not complete the installation. Feel free to update this Dockerfile or add one to the scanpy repository.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/43#issuecomment-344235559
https://github.com/scverse/scanpy/issues/43#issuecomment-344235559:658,Deployability,update,update,658,"I managed to get past the error by adding; ```; RUN locale-gen en_US.UTF-8; ENV LC_ALL en_US.UTF-8; ```; to the [Dockerfile](https://gist.github.com/pwl/a26726fda94ac7f4cbfb57e4fe98bf28). Before that the default locale was set to `POSIX`, which caused all of these problems. This is a weird choice of defaults as clearly python code doesn't work as expected. Thanks for helping out @flying-sheep!. EDIT: just to clarify, this dockerfile is not an example of how to install scanpy, it's just a demonstration of how to circumvent the issues with locales. In particular, several libraries are missing and scanpy does not complete the installation. Feel free to update this Dockerfile or add one to the scanpy repository.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/43#issuecomment-344235559
https://github.com/scverse/scanpy/issues/43#issuecomment-344235559:313,Usability,clear,clearly,313,"I managed to get past the error by adding; ```; RUN locale-gen en_US.UTF-8; ENV LC_ALL en_US.UTF-8; ```; to the [Dockerfile](https://gist.github.com/pwl/a26726fda94ac7f4cbfb57e4fe98bf28). Before that the default locale was set to `POSIX`, which caused all of these problems. This is a weird choice of defaults as clearly python code doesn't work as expected. Thanks for helping out @flying-sheep!. EDIT: just to clarify, this dockerfile is not an example of how to install scanpy, it's just a demonstration of how to circumvent the issues with locales. In particular, several libraries are missing and scanpy does not complete the installation. Feel free to update this Dockerfile or add one to the scanpy repository.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/43#issuecomment-344235559
https://github.com/scverse/scanpy/issues/43#issuecomment-344265736:464,Deployability,install,installing,464,"Thanks for the info!. I was about to rant that this is weird and broken, but locales are literally the first section in the Ubuntu image docs, so I can’t blame them (too much): https://hub.docker.com/_/ubuntu. They probably use `POSIX` as `C.UTF-8` isn’t standard. ([blame the C standard consortium](https://github.com/mpv-player/mpv/commit/1e70e82baa9193f6f027338b0fab0f5078971fbe)). They recommend `ENV LANG C.UTF-8` though, maybe that works for you. (otherwise installing `locales` and your instructions work too)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/43#issuecomment-344265736
https://github.com/scverse/scanpy/issues/43#issuecomment-344278619:148,Deployability,install,install,148,"@falexwolf @pwl i used this one, works fine: https://gist.github.com/flying-sheep/0e003ae3398dd543638955a55c031c8d. i wonder why you didn’t have to install the dev packages though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/43#issuecomment-344278619
https://github.com/scverse/scanpy/issues/43#issuecomment-344294567:61,Deployability,install,install,61,"I somehow missed the documentation section, my bad. I didn't install the dev packages because I only needed a minimal setup to recreate the bug, scanpy actually fails to install without the dev packages, as expected, but that comes later on, after the initial bug.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/43#issuecomment-344294567
https://github.com/scverse/scanpy/issues/43#issuecomment-344294567:170,Deployability,install,install,170,"I somehow missed the documentation section, my bad. I didn't install the dev packages because I only needed a minimal setup to recreate the bug, scanpy actually fails to install without the dev packages, as expected, but that comes later on, after the initial bug.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/43#issuecomment-344294567
https://github.com/scverse/scanpy/issues/43#issuecomment-344299361:110,Energy Efficiency,adapt,adapting,110,"don’t worry, i think they should really default to a better locale: many people will get their Dockerfiles by adapting existing ones instead of finding that specific doc site, i think.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/43#issuecomment-344299361
https://github.com/scverse/scanpy/issues/43#issuecomment-344299361:110,Modifiability,adapt,adapting,110,"don’t worry, i think they should really default to a better locale: many people will get their Dockerfiles by adapting existing ones instead of finding that specific doc site, i think.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/43#issuecomment-344299361
https://github.com/scverse/scanpy/issues/44#issuecomment-344124406:34,Deployability,Release,Release,34,"Hi!; Sorry for the late response. Release 0.3 comes today or tomorrow, with many improvements.; Is the following OK for you?; From http://scanpy.readthedocs.io/en/latest/api/scanpy.api.AnnData.html; ```; >>> adata1 = AnnData(np.array([[1, 2, 3], [4, 5, 6]]),; >>> {'smp_names': ['s1', 's2'],; >>> 'anno1': ['c1', 'c2']},; >>> {'var_names': ['a', 'b', 'c']}); >>> adata2 = AnnData(np.array([[1, 2, 3], [4, 5, 6]]),; >>> {'smp_names': ['s3', 's4'],; >>> 'anno1': ['c3', 'c4']},; >>> {'var_names': ['b', 'c', 'd']}); >>> adata3 = AnnData(np.array([[1, 2, 3], [4, 5, 6]]),; >>> {'smp_names': ['s5', 's6'],; >>> 'anno2': ['d3', 'd4']},; >>> {'var_names': ['b', 'c', 'd']}); >>>; >>> adata = adata1.concatenate([adata2, adata3]); >>> adata.X; [[ 2. 3.]; [ 5. 6.]; [ 1. 2.]; [ 4. 5.]; [ 1. 2.]; [ 4. 5.]]; >>> adata.smp; anno1 anno2 batch; s1 c1 NaN 0; s2 c2 NaN 0; s3 c3 NaN 1; s4 c4 NaN 1; s5 NaN d3 2; s6 NaN d4 2; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/44#issuecomment-344124406
https://github.com/scverse/scanpy/issues/45#issuecomment-356611882:309,Availability,down,downstream,309,"Hi all!. I just wanted to jump in with @sophietr and say that implementing a cell cycle classification function like Seurat's [CellCycleScoring](https://github.com/satijalab/seurat/blob/master/R/scoring.R) function would be a nice addition to the preprocessing options. Would be valuable to keep an eye on in downstream exploration and could then be easily regressed out if needed. Also, do you guys have any opinions about the inclusion of imputation/smoothing strategies? I've been messing around with including it in analysis pipelines, but still haven't really settled on when to include them. If there's interest, [MAGIC](https://github.com/pkathail/magic) seems like a great option and is currently implemented in Python.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/45#issuecomment-356611882
https://github.com/scverse/scanpy/issues/45#issuecomment-356611882:529,Deployability,pipeline,pipelines,529,"Hi all!. I just wanted to jump in with @sophietr and say that implementing a cell cycle classification function like Seurat's [CellCycleScoring](https://github.com/satijalab/seurat/blob/master/R/scoring.R) function would be a nice addition to the preprocessing options. Would be valuable to keep an eye on in downstream exploration and could then be easily regressed out if needed. Also, do you guys have any opinions about the inclusion of imputation/smoothing strategies? I've been messing around with including it in analysis pipelines, but still haven't really settled on when to include them. If there's interest, [MAGIC](https://github.com/pkathail/magic) seems like a great option and is currently implemented in Python.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/45#issuecomment-356611882
https://github.com/scverse/scanpy/issues/45#issuecomment-363250398:517,Testability,test,test,517,"@dawe a cell cycle scoring function would be great! everything that's a bit more extensive and non-standard should go into [sc.tl](https://github.com/theislab/scanpy/tree/master/scanpy/tools), everything that's really just simple preprocessing and stats with a few lines can go to [sc.pp](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/simple.py). usually, there should be a plotting function in sc.pl that presents a canonical visualization of the annotation added in with the tool... writing a test for your function would also be great ;)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/45#issuecomment-363250398
https://github.com/scverse/scanpy/issues/45#issuecomment-363250398:223,Usability,simpl,simple,223,"@dawe a cell cycle scoring function would be great! everything that's a bit more extensive and non-standard should go into [sc.tl](https://github.com/theislab/scanpy/tree/master/scanpy/tools), everything that's really just simple preprocessing and stats with a few lines can go to [sc.pp](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/simple.py). usually, there should be a plotting function in sc.pl that presents a canonical visualization of the annotation added in with the tool... writing a test for your function would also be great ;)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/45#issuecomment-363250398
https://github.com/scverse/scanpy/issues/45#issuecomment-363250398:357,Usability,simpl,simple,357,"@dawe a cell cycle scoring function would be great! everything that's a bit more extensive and non-standard should go into [sc.tl](https://github.com/theislab/scanpy/tree/master/scanpy/tools), everything that's really just simple preprocessing and stats with a few lines can go to [sc.pp](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/simple.py). usually, there should be a plotting function in sc.pl that presents a canonical visualization of the annotation added in with the tool... writing a test for your function would also be great ;)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/45#issuecomment-363250398
https://github.com/scverse/scanpy/issues/45#issuecomment-363458980:140,Testability,test,tests,140,"@falexwolf I have the functions in my scanpy branch, right now. It seems to be properly working (take a look, if you want to). I'll add the tests as soon as possibile (now getting back to ""ordinary work"")",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/45#issuecomment-363458980
https://github.com/scverse/scanpy/issues/45#issuecomment-367378135:130,Performance,optimiz,optimization,130,@flying-sheep can you cite a reference for scImpute and countae outperforming MAGIC? I'd be curious to learn which hyperparameter optimization methods and performance measures were used in the benchmark.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/45#issuecomment-367378135
https://github.com/scverse/scanpy/issues/45#issuecomment-367378135:155,Performance,perform,performance,155,@flying-sheep can you cite a reference for scImpute and countae outperforming MAGIC? I'd be curious to learn which hyperparameter optimization methods and performance measures were used in the benchmark.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/45#issuecomment-367378135
https://github.com/scverse/scanpy/issues/45#issuecomment-367378135:193,Testability,benchmark,benchmark,193,@flying-sheep can you cite a reference for scImpute and countae outperforming MAGIC? I'd be curious to learn which hyperparameter optimization methods and performance measures were used in the benchmark.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/45#issuecomment-367378135
https://github.com/scverse/scanpy/issues/45#issuecomment-367378135:103,Usability,learn,learn,103,@flying-sheep can you cite a reference for scImpute and countae outperforming MAGIC? I'd be curious to learn which hyperparameter optimization methods and performance measures were used in the benchmark.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/45#issuecomment-367378135
https://github.com/scverse/scanpy/issues/45#issuecomment-367680111:148,Availability,down,downstream,148,@flying-sheep @gokceneraslan great! I agree it's hard to compare these algorithms as the performance of an imputation strategy often depends on the downstream use case. I'm looking forward to checking out the countae preprint. I find the [scVI](https://github.com/YosefLab/scVI) benchmark of imputation methods to be useful for now.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/45#issuecomment-367680111
https://github.com/scverse/scanpy/issues/45#issuecomment-367680111:133,Integrability,depend,depends,133,@flying-sheep @gokceneraslan great! I agree it's hard to compare these algorithms as the performance of an imputation strategy often depends on the downstream use case. I'm looking forward to checking out the countae preprint. I find the [scVI](https://github.com/YosefLab/scVI) benchmark of imputation methods to be useful for now.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/45#issuecomment-367680111
https://github.com/scverse/scanpy/issues/45#issuecomment-367680111:89,Performance,perform,performance,89,@flying-sheep @gokceneraslan great! I agree it's hard to compare these algorithms as the performance of an imputation strategy often depends on the downstream use case. I'm looking forward to checking out the countae preprint. I find the [scVI](https://github.com/YosefLab/scVI) benchmark of imputation methods to be useful for now.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/45#issuecomment-367680111
https://github.com/scverse/scanpy/issues/45#issuecomment-367680111:279,Testability,benchmark,benchmark,279,@flying-sheep @gokceneraslan great! I agree it's hard to compare these algorithms as the performance of an imputation strategy often depends on the downstream use case. I'm looking forward to checking out the countae preprint. I find the [scVI](https://github.com/YosefLab/scVI) benchmark of imputation methods to be useful for now.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/45#issuecomment-367680111
https://github.com/scverse/scanpy/issues/47#issuecomment-344400517:254,Deployability,update,updated,254,"Sorry about that bug and thanks for reporting it. It only occurred with the `copy` parameter set, which is why no one noticed it till now. The bug is fixed: https://github.com/theislab/scanpy/commit/f6a41f140a646c350ab12d8bd6aeff7499df069e. The docs are updated, there's now an example: http://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.normalize_per_cell.html. I wrote a test that checks that this doesn't break again in the future: https://github.com/theislab/scanpy/blob/f6a41f140a646c350ab12d8bd6aeff7499df069e/scanpy/tests/preprocessing.py#L11-L31. There will be a new release 0.3 with many improvements tomorrow. Cheers,; Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/47#issuecomment-344400517
https://github.com/scverse/scanpy/issues/47#issuecomment-344400517:581,Deployability,release,release,581,"Sorry about that bug and thanks for reporting it. It only occurred with the `copy` parameter set, which is why no one noticed it till now. The bug is fixed: https://github.com/theislab/scanpy/commit/f6a41f140a646c350ab12d8bd6aeff7499df069e. The docs are updated, there's now an example: http://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.normalize_per_cell.html. I wrote a test that checks that this doesn't break again in the future: https://github.com/theislab/scanpy/blob/f6a41f140a646c350ab12d8bd6aeff7499df069e/scanpy/tests/preprocessing.py#L11-L31. There will be a new release 0.3 with many improvements tomorrow. Cheers,; Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/47#issuecomment-344400517
https://github.com/scverse/scanpy/issues/47#issuecomment-344400517:379,Testability,test,test,379,"Sorry about that bug and thanks for reporting it. It only occurred with the `copy` parameter set, which is why no one noticed it till now. The bug is fixed: https://github.com/theislab/scanpy/commit/f6a41f140a646c350ab12d8bd6aeff7499df069e. The docs are updated, there's now an example: http://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.normalize_per_cell.html. I wrote a test that checks that this doesn't break again in the future: https://github.com/theislab/scanpy/blob/f6a41f140a646c350ab12d8bd6aeff7499df069e/scanpy/tests/preprocessing.py#L11-L31. There will be a new release 0.3 with many improvements tomorrow. Cheers,; Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/47#issuecomment-344400517
https://github.com/scverse/scanpy/issues/47#issuecomment-344400517:529,Testability,test,tests,529,"Sorry about that bug and thanks for reporting it. It only occurred with the `copy` parameter set, which is why no one noticed it till now. The bug is fixed: https://github.com/theislab/scanpy/commit/f6a41f140a646c350ab12d8bd6aeff7499df069e. The docs are updated, there's now an example: http://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.normalize_per_cell.html. I wrote a test that checks that this doesn't break again in the future: https://github.com/theislab/scanpy/blob/f6a41f140a646c350ab12d8bd6aeff7499df069e/scanpy/tests/preprocessing.py#L11-L31. There will be a new release 0.3 with many improvements tomorrow. Cheers,; Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/47#issuecomment-344400517
https://github.com/scverse/scanpy/issues/47#issuecomment-344538383:71,Deployability,update,update,71,"Hi Alex,. Thanks for fixing this promptly. I will eagerly wait for the update.; I have another query related to usage of this function but I'll create a new issue for that. Parashar",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/47#issuecomment-344538383
https://github.com/scverse/scanpy/issues/48#issuecomment-347354902:823,Energy Efficiency,adapt,adapted,823,"I'm very sorry for having forgotten about this issue... Of course, `sc.pp.normalize_per_cell()` stores the total counts per cell *prior* to normalization as *n_counts*. See the examples here https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.normalize_per_cell.html. Performing the normalization removes the effect of having different total counts per cell by scaling each gene with the total counts. But one might want more: if there is still some correlation of a gene with *n_counts* *after* normalization, one concludes that the simple scaling done in normalization has *not* fully removed the effect of *n_counts* on that particular gene. Hence, using `sc.pp.regress_out`, one performs an additional gene-wise correction. I have to admit that I have not investigated how necessary this is. As you know, this is adapted from the Seurat tutorial - I guess the authors of Seurat found it useful in some cases to fully remove the effect of *n_counts* on each single gene.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/48#issuecomment-347354902
https://github.com/scverse/scanpy/issues/48#issuecomment-347354902:823,Modifiability,adapt,adapted,823,"I'm very sorry for having forgotten about this issue... Of course, `sc.pp.normalize_per_cell()` stores the total counts per cell *prior* to normalization as *n_counts*. See the examples here https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.normalize_per_cell.html. Performing the normalization removes the effect of having different total counts per cell by scaling each gene with the total counts. But one might want more: if there is still some correlation of a gene with *n_counts* *after* normalization, one concludes that the simple scaling done in normalization has *not* fully removed the effect of *n_counts* on that particular gene. Hence, using `sc.pp.regress_out`, one performs an additional gene-wise correction. I have to admit that I have not investigated how necessary this is. As you know, this is adapted from the Seurat tutorial - I guess the authors of Seurat found it useful in some cases to fully remove the effect of *n_counts* on each single gene.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/48#issuecomment-347354902
https://github.com/scverse/scanpy/issues/48#issuecomment-347354902:274,Performance,Perform,Performing,274,"I'm very sorry for having forgotten about this issue... Of course, `sc.pp.normalize_per_cell()` stores the total counts per cell *prior* to normalization as *n_counts*. See the examples here https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.normalize_per_cell.html. Performing the normalization removes the effect of having different total counts per cell by scaling each gene with the total counts. But one might want more: if there is still some correlation of a gene with *n_counts* *after* normalization, one concludes that the simple scaling done in normalization has *not* fully removed the effect of *n_counts* on that particular gene. Hence, using `sc.pp.regress_out`, one performs an additional gene-wise correction. I have to admit that I have not investigated how necessary this is. As you know, this is adapted from the Seurat tutorial - I guess the authors of Seurat found it useful in some cases to fully remove the effect of *n_counts* on each single gene.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/48#issuecomment-347354902
https://github.com/scverse/scanpy/issues/48#issuecomment-347354902:689,Performance,perform,performs,689,"I'm very sorry for having forgotten about this issue... Of course, `sc.pp.normalize_per_cell()` stores the total counts per cell *prior* to normalization as *n_counts*. See the examples here https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.normalize_per_cell.html. Performing the normalization removes the effect of having different total counts per cell by scaling each gene with the total counts. But one might want more: if there is still some correlation of a gene with *n_counts* *after* normalization, one concludes that the simple scaling done in normalization has *not* fully removed the effect of *n_counts* on that particular gene. Hence, using `sc.pp.regress_out`, one performs an additional gene-wise correction. I have to admit that I have not investigated how necessary this is. As you know, this is adapted from the Seurat tutorial - I guess the authors of Seurat found it useful in some cases to fully remove the effect of *n_counts* on each single gene.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/48#issuecomment-347354902
https://github.com/scverse/scanpy/issues/48#issuecomment-347354902:540,Usability,simpl,simple,540,"I'm very sorry for having forgotten about this issue... Of course, `sc.pp.normalize_per_cell()` stores the total counts per cell *prior* to normalization as *n_counts*. See the examples here https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.normalize_per_cell.html. Performing the normalization removes the effect of having different total counts per cell by scaling each gene with the total counts. But one might want more: if there is still some correlation of a gene with *n_counts* *after* normalization, one concludes that the simple scaling done in normalization has *not* fully removed the effect of *n_counts* on that particular gene. Hence, using `sc.pp.regress_out`, one performs an additional gene-wise correction. I have to admit that I have not investigated how necessary this is. As you know, this is adapted from the Seurat tutorial - I guess the authors of Seurat found it useful in some cases to fully remove the effect of *n_counts* on each single gene.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/48#issuecomment-347354902
https://github.com/scverse/scanpy/issues/49#issuecomment-345195949:23,Deployability,install,install,23,"I have just managed to install successfully (kind of, more details on things going wrong to come in the other repository) with pip without actually doing any of this, so I'm not sure what was actually going on here. One way or the other, this seems to have gone away now somehow on its own.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/49#issuecomment-345195949
https://github.com/scverse/scanpy/issues/50#issuecomment-346303221:104,Performance,cache,cache,104,"i think the reason we do this is for project-specific caching, not temporary files. using the dedicated cache dir is of course preferable to using the working dir, since the OS knows about them (and can clean them once prompted or necessary), and preferable to a tempdir, as they survive restarts. we should use <code>cache_dir = Path([appdirs](https://pypi.python.org/pypi/appdirs/1.4.3).user_cache_dir('scanpy', 'F. Alex Wolf'))</code>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/50#issuecomment-346303221
https://github.com/scverse/scanpy/issues/50#issuecomment-346318918:55,Deployability,configurat,configuration,55,"The reason for this directory is just project-specific configuration. Here, https://github.com/theislab/scanpy/commit/7a57fd4cf140dc4b2ffca7ef0651a355c74f0122, I removed the creation of this directory. Nonetheless, it's true that Scanpy, when you tell it to cache a file, it wants to create a directory (by default './write/') for it. Tell me if this is a problem for you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/50#issuecomment-346318918
https://github.com/scverse/scanpy/issues/50#issuecomment-346318918:55,Modifiability,config,configuration,55,"The reason for this directory is just project-specific configuration. Here, https://github.com/theislab/scanpy/commit/7a57fd4cf140dc4b2ffca7ef0651a355c74f0122, I removed the creation of this directory. Nonetheless, it's true that Scanpy, when you tell it to cache a file, it wants to create a directory (by default './write/') for it. Tell me if this is a problem for you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/50#issuecomment-346318918
https://github.com/scverse/scanpy/issues/50#issuecomment-346318918:258,Performance,cache,cache,258,"The reason for this directory is just project-specific configuration. Here, https://github.com/theislab/scanpy/commit/7a57fd4cf140dc4b2ffca7ef0651a355c74f0122, I removed the creation of this directory. Nonetheless, it's true that Scanpy, when you tell it to cache a file, it wants to create a directory (by default './write/') for it. Tell me if this is a problem for you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/50#issuecomment-346318918
https://github.com/scverse/scanpy/issues/50#issuecomment-346320991:70,Performance,cache,cache,70,"ah, yes, i confused `.scanpy` and `.write`. i think caching in a real cache directory instead of `./.write` would be better in any case.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/50#issuecomment-346320991
https://github.com/scverse/scanpy/issues/50#issuecomment-346321453:140,Modifiability,config,config,140,"It's `'./write/'`, so it's not a hidden directory - i guess it wouldn't be a good idea to save large files in a hidden fashion; whereas the config was hidden in `'.scanpy/'` - but the latter is not really needed anymore and I could simply remove it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/50#issuecomment-346321453
https://github.com/scverse/scanpy/issues/50#issuecomment-346321453:232,Usability,simpl,simply,232,"It's `'./write/'`, so it's not a hidden directory - i guess it wouldn't be a good idea to save large files in a hidden fashion; whereas the config was hidden in `'.scanpy/'` - but the latter is not really needed anymore and I could simply remove it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/50#issuecomment-346321453
https://github.com/scverse/scanpy/issues/50#issuecomment-346761583:37,Safety,avoid,avoid,37,I like the idea.; You could probably avoid the context manager but it's ok I think.-,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/50#issuecomment-346761583
https://github.com/scverse/scanpy/issues/50#issuecomment-346763343:62,Integrability,wrap,wrapping,62,how would you avoid the context manager?. it’s either that or wrapping try/catch around every single use of the `writedir`.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/50#issuecomment-346763343
https://github.com/scverse/scanpy/issues/50#issuecomment-346763343:14,Safety,avoid,avoid,14,how would you avoid the context manager?. it’s either that or wrapping try/catch around every single use of the `writedir`.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/50#issuecomment-346763343
https://github.com/scverse/scanpy/issues/50#issuecomment-346776672:219,Performance,load,loaded,219,"OK, will talk to Philipp about this in Person... This only concerns speeding up the reading of slow (e.g., text-based) data file formats. This might also be relevant for this discussion: foreseeing the use of partially loaded data into memory, files for backing AnnData remain something the user has to actively interact with. With the creation of an AnnData object, she/he would then have the option to create a corresponding ""backing-file"", which is internally used by AnnData to load needed parts into memory and leave parts that are not needed on the disk. At any time when there is no active write or read to the file, the file stores the current state of AnnData. I felt that both cache files and ""backing files"" should happen in a project-specific './write' directory - that is, at a location where an inexperienced user directly ""sees"" what happens and how this affects disk space. One could think about renaming the ""data"" subdirectory to something like ""data_cache"" or so to make evident that this only stores cache files, which can simply be deleted, and everything else stores ""AnnData backing files"" = ""result files"" or exported files... But I agree true cache files might be better placed in a tmp directory. As said, will discuss this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/50#issuecomment-346776672
https://github.com/scverse/scanpy/issues/50#issuecomment-346776672:482,Performance,load,load,482,"OK, will talk to Philipp about this in Person... This only concerns speeding up the reading of slow (e.g., text-based) data file formats. This might also be relevant for this discussion: foreseeing the use of partially loaded data into memory, files for backing AnnData remain something the user has to actively interact with. With the creation of an AnnData object, she/he would then have the option to create a corresponding ""backing-file"", which is internally used by AnnData to load needed parts into memory and leave parts that are not needed on the disk. At any time when there is no active write or read to the file, the file stores the current state of AnnData. I felt that both cache files and ""backing files"" should happen in a project-specific './write' directory - that is, at a location where an inexperienced user directly ""sees"" what happens and how this affects disk space. One could think about renaming the ""data"" subdirectory to something like ""data_cache"" or so to make evident that this only stores cache files, which can simply be deleted, and everything else stores ""AnnData backing files"" = ""result files"" or exported files... But I agree true cache files might be better placed in a tmp directory. As said, will discuss this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/50#issuecomment-346776672
https://github.com/scverse/scanpy/issues/50#issuecomment-346776672:687,Performance,cache,cache,687,"OK, will talk to Philipp about this in Person... This only concerns speeding up the reading of slow (e.g., text-based) data file formats. This might also be relevant for this discussion: foreseeing the use of partially loaded data into memory, files for backing AnnData remain something the user has to actively interact with. With the creation of an AnnData object, she/he would then have the option to create a corresponding ""backing-file"", which is internally used by AnnData to load needed parts into memory and leave parts that are not needed on the disk. At any time when there is no active write or read to the file, the file stores the current state of AnnData. I felt that both cache files and ""backing files"" should happen in a project-specific './write' directory - that is, at a location where an inexperienced user directly ""sees"" what happens and how this affects disk space. One could think about renaming the ""data"" subdirectory to something like ""data_cache"" or so to make evident that this only stores cache files, which can simply be deleted, and everything else stores ""AnnData backing files"" = ""result files"" or exported files... But I agree true cache files might be better placed in a tmp directory. As said, will discuss this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/50#issuecomment-346776672
https://github.com/scverse/scanpy/issues/50#issuecomment-346776672:1020,Performance,cache,cache,1020,"OK, will talk to Philipp about this in Person... This only concerns speeding up the reading of slow (e.g., text-based) data file formats. This might also be relevant for this discussion: foreseeing the use of partially loaded data into memory, files for backing AnnData remain something the user has to actively interact with. With the creation of an AnnData object, she/he would then have the option to create a corresponding ""backing-file"", which is internally used by AnnData to load needed parts into memory and leave parts that are not needed on the disk. At any time when there is no active write or read to the file, the file stores the current state of AnnData. I felt that both cache files and ""backing files"" should happen in a project-specific './write' directory - that is, at a location where an inexperienced user directly ""sees"" what happens and how this affects disk space. One could think about renaming the ""data"" subdirectory to something like ""data_cache"" or so to make evident that this only stores cache files, which can simply be deleted, and everything else stores ""AnnData backing files"" = ""result files"" or exported files... But I agree true cache files might be better placed in a tmp directory. As said, will discuss this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/50#issuecomment-346776672
https://github.com/scverse/scanpy/issues/50#issuecomment-346776672:1168,Performance,cache,cache,1168,"OK, will talk to Philipp about this in Person... This only concerns speeding up the reading of slow (e.g., text-based) data file formats. This might also be relevant for this discussion: foreseeing the use of partially loaded data into memory, files for backing AnnData remain something the user has to actively interact with. With the creation of an AnnData object, she/he would then have the option to create a corresponding ""backing-file"", which is internally used by AnnData to load needed parts into memory and leave parts that are not needed on the disk. At any time when there is no active write or read to the file, the file stores the current state of AnnData. I felt that both cache files and ""backing files"" should happen in a project-specific './write' directory - that is, at a location where an inexperienced user directly ""sees"" what happens and how this affects disk space. One could think about renaming the ""data"" subdirectory to something like ""data_cache"" or so to make evident that this only stores cache files, which can simply be deleted, and everything else stores ""AnnData backing files"" = ""result files"" or exported files... But I agree true cache files might be better placed in a tmp directory. As said, will discuss this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/50#issuecomment-346776672
https://github.com/scverse/scanpy/issues/50#issuecomment-346776672:1043,Usability,simpl,simply,1043,"OK, will talk to Philipp about this in Person... This only concerns speeding up the reading of slow (e.g., text-based) data file formats. This might also be relevant for this discussion: foreseeing the use of partially loaded data into memory, files for backing AnnData remain something the user has to actively interact with. With the creation of an AnnData object, she/he would then have the option to create a corresponding ""backing-file"", which is internally used by AnnData to load needed parts into memory and leave parts that are not needed on the disk. At any time when there is no active write or read to the file, the file stores the current state of AnnData. I felt that both cache files and ""backing files"" should happen in a project-specific './write' directory - that is, at a location where an inexperienced user directly ""sees"" what happens and how this affects disk space. One could think about renaming the ""data"" subdirectory to something like ""data_cache"" or so to make evident that this only stores cache files, which can simply be deleted, and everything else stores ""AnnData backing files"" = ""result files"" or exported files... But I agree true cache files might be better placed in a tmp directory. As said, will discuss this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/50#issuecomment-346776672
https://github.com/scverse/scanpy/issues/50#issuecomment-346781457:815,Availability,reboot,reboot,815,"sure, we’ll talk in 10 days or so, after my holidays 😄. except if you want to earlier, then we can skype or so. > One could think about renaming the ""data"" subdirectory to something like ""data_cache"" or so to make evident that this only stores cache files, which can simply be deleted, and everything else stores ""AnnData backing files"" = ""result files"" or exported files... definitely. > But I agree true cache files might be better placed in a tmp directory. no, as i said: cache directory, not temp directory. both have (overridable) standard locations on all OSs. e.g. on linux:. - `$TMPDIR` or `/tmp/`: temporary means that the files are only to be read during the same function/script execution, and deleted after. temp files forgotten by the application that created them are deleted after `$TMPTIME` and on reboot. (on linux now usually because `/tmp/` is a ramdisk and RAM contents don’t survive a reboot). ```py; # python gives you a context manager that deletes the file after its block; with tempfile.TemporaryFile() as fp:; use(fp); # fp and the file are gone now; ```. - `$XDG_CACHE_HOME` or `~/.cache/`: cache files are permanent until the user or OS cleans up or the application decides it no longer needs them (i think e.g. browsers clear out the parts of their cache periodically). since scanpy has a notion of a project directory, putting the cache there is OK as well. the advantage is visibility, but that only works if the user knows what the directory/ies are for. using `cache` in the name of the cache directory would certainly help to signify that the stuff can be safely deleted.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/50#issuecomment-346781457
https://github.com/scverse/scanpy/issues/50#issuecomment-346781457:907,Availability,reboot,reboot,907,"sure, we’ll talk in 10 days or so, after my holidays 😄. except if you want to earlier, then we can skype or so. > One could think about renaming the ""data"" subdirectory to something like ""data_cache"" or so to make evident that this only stores cache files, which can simply be deleted, and everything else stores ""AnnData backing files"" = ""result files"" or exported files... definitely. > But I agree true cache files might be better placed in a tmp directory. no, as i said: cache directory, not temp directory. both have (overridable) standard locations on all OSs. e.g. on linux:. - `$TMPDIR` or `/tmp/`: temporary means that the files are only to be read during the same function/script execution, and deleted after. temp files forgotten by the application that created them are deleted after `$TMPTIME` and on reboot. (on linux now usually because `/tmp/` is a ramdisk and RAM contents don’t survive a reboot). ```py; # python gives you a context manager that deletes the file after its block; with tempfile.TemporaryFile() as fp:; use(fp); # fp and the file are gone now; ```. - `$XDG_CACHE_HOME` or `~/.cache/`: cache files are permanent until the user or OS cleans up or the application decides it no longer needs them (i think e.g. browsers clear out the parts of their cache periodically). since scanpy has a notion of a project directory, putting the cache there is OK as well. the advantage is visibility, but that only works if the user knows what the directory/ies are for. using `cache` in the name of the cache directory would certainly help to signify that the stuff can be safely deleted.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/50#issuecomment-346781457
https://github.com/scverse/scanpy/issues/50#issuecomment-346781457:244,Performance,cache,cache,244,"sure, we’ll talk in 10 days or so, after my holidays 😄. except if you want to earlier, then we can skype or so. > One could think about renaming the ""data"" subdirectory to something like ""data_cache"" or so to make evident that this only stores cache files, which can simply be deleted, and everything else stores ""AnnData backing files"" = ""result files"" or exported files... definitely. > But I agree true cache files might be better placed in a tmp directory. no, as i said: cache directory, not temp directory. both have (overridable) standard locations on all OSs. e.g. on linux:. - `$TMPDIR` or `/tmp/`: temporary means that the files are only to be read during the same function/script execution, and deleted after. temp files forgotten by the application that created them are deleted after `$TMPTIME` and on reboot. (on linux now usually because `/tmp/` is a ramdisk and RAM contents don’t survive a reboot). ```py; # python gives you a context manager that deletes the file after its block; with tempfile.TemporaryFile() as fp:; use(fp); # fp and the file are gone now; ```. - `$XDG_CACHE_HOME` or `~/.cache/`: cache files are permanent until the user or OS cleans up or the application decides it no longer needs them (i think e.g. browsers clear out the parts of their cache periodically). since scanpy has a notion of a project directory, putting the cache there is OK as well. the advantage is visibility, but that only works if the user knows what the directory/ies are for. using `cache` in the name of the cache directory would certainly help to signify that the stuff can be safely deleted.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/50#issuecomment-346781457
https://github.com/scverse/scanpy/issues/50#issuecomment-346781457:406,Performance,cache,cache,406,"sure, we’ll talk in 10 days or so, after my holidays 😄. except if you want to earlier, then we can skype or so. > One could think about renaming the ""data"" subdirectory to something like ""data_cache"" or so to make evident that this only stores cache files, which can simply be deleted, and everything else stores ""AnnData backing files"" = ""result files"" or exported files... definitely. > But I agree true cache files might be better placed in a tmp directory. no, as i said: cache directory, not temp directory. both have (overridable) standard locations on all OSs. e.g. on linux:. - `$TMPDIR` or `/tmp/`: temporary means that the files are only to be read during the same function/script execution, and deleted after. temp files forgotten by the application that created them are deleted after `$TMPTIME` and on reboot. (on linux now usually because `/tmp/` is a ramdisk and RAM contents don’t survive a reboot). ```py; # python gives you a context manager that deletes the file after its block; with tempfile.TemporaryFile() as fp:; use(fp); # fp and the file are gone now; ```. - `$XDG_CACHE_HOME` or `~/.cache/`: cache files are permanent until the user or OS cleans up or the application decides it no longer needs them (i think e.g. browsers clear out the parts of their cache periodically). since scanpy has a notion of a project directory, putting the cache there is OK as well. the advantage is visibility, but that only works if the user knows what the directory/ies are for. using `cache` in the name of the cache directory would certainly help to signify that the stuff can be safely deleted.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/50#issuecomment-346781457
https://github.com/scverse/scanpy/issues/50#issuecomment-346781457:476,Performance,cache,cache,476,"sure, we’ll talk in 10 days or so, after my holidays 😄. except if you want to earlier, then we can skype or so. > One could think about renaming the ""data"" subdirectory to something like ""data_cache"" or so to make evident that this only stores cache files, which can simply be deleted, and everything else stores ""AnnData backing files"" = ""result files"" or exported files... definitely. > But I agree true cache files might be better placed in a tmp directory. no, as i said: cache directory, not temp directory. both have (overridable) standard locations on all OSs. e.g. on linux:. - `$TMPDIR` or `/tmp/`: temporary means that the files are only to be read during the same function/script execution, and deleted after. temp files forgotten by the application that created them are deleted after `$TMPTIME` and on reboot. (on linux now usually because `/tmp/` is a ramdisk and RAM contents don’t survive a reboot). ```py; # python gives you a context manager that deletes the file after its block; with tempfile.TemporaryFile() as fp:; use(fp); # fp and the file are gone now; ```. - `$XDG_CACHE_HOME` or `~/.cache/`: cache files are permanent until the user or OS cleans up or the application decides it no longer needs them (i think e.g. browsers clear out the parts of their cache periodically). since scanpy has a notion of a project directory, putting the cache there is OK as well. the advantage is visibility, but that only works if the user knows what the directory/ies are for. using `cache` in the name of the cache directory would certainly help to signify that the stuff can be safely deleted.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/50#issuecomment-346781457
https://github.com/scverse/scanpy/issues/50#issuecomment-346781457:1110,Performance,cache,cache,1110,"sure, we’ll talk in 10 days or so, after my holidays 😄. except if you want to earlier, then we can skype or so. > One could think about renaming the ""data"" subdirectory to something like ""data_cache"" or so to make evident that this only stores cache files, which can simply be deleted, and everything else stores ""AnnData backing files"" = ""result files"" or exported files... definitely. > But I agree true cache files might be better placed in a tmp directory. no, as i said: cache directory, not temp directory. both have (overridable) standard locations on all OSs. e.g. on linux:. - `$TMPDIR` or `/tmp/`: temporary means that the files are only to be read during the same function/script execution, and deleted after. temp files forgotten by the application that created them are deleted after `$TMPTIME` and on reboot. (on linux now usually because `/tmp/` is a ramdisk and RAM contents don’t survive a reboot). ```py; # python gives you a context manager that deletes the file after its block; with tempfile.TemporaryFile() as fp:; use(fp); # fp and the file are gone now; ```. - `$XDG_CACHE_HOME` or `~/.cache/`: cache files are permanent until the user or OS cleans up or the application decides it no longer needs them (i think e.g. browsers clear out the parts of their cache periodically). since scanpy has a notion of a project directory, putting the cache there is OK as well. the advantage is visibility, but that only works if the user knows what the directory/ies are for. using `cache` in the name of the cache directory would certainly help to signify that the stuff can be safely deleted.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/50#issuecomment-346781457
https://github.com/scverse/scanpy/issues/50#issuecomment-346781457:1119,Performance,cache,cache,1119,"sure, we’ll talk in 10 days or so, after my holidays 😄. except if you want to earlier, then we can skype or so. > One could think about renaming the ""data"" subdirectory to something like ""data_cache"" or so to make evident that this only stores cache files, which can simply be deleted, and everything else stores ""AnnData backing files"" = ""result files"" or exported files... definitely. > But I agree true cache files might be better placed in a tmp directory. no, as i said: cache directory, not temp directory. both have (overridable) standard locations on all OSs. e.g. on linux:. - `$TMPDIR` or `/tmp/`: temporary means that the files are only to be read during the same function/script execution, and deleted after. temp files forgotten by the application that created them are deleted after `$TMPTIME` and on reboot. (on linux now usually because `/tmp/` is a ramdisk and RAM contents don’t survive a reboot). ```py; # python gives you a context manager that deletes the file after its block; with tempfile.TemporaryFile() as fp:; use(fp); # fp and the file are gone now; ```. - `$XDG_CACHE_HOME` or `~/.cache/`: cache files are permanent until the user or OS cleans up or the application decides it no longer needs them (i think e.g. browsers clear out the parts of their cache periodically). since scanpy has a notion of a project directory, putting the cache there is OK as well. the advantage is visibility, but that only works if the user knows what the directory/ies are for. using `cache` in the name of the cache directory would certainly help to signify that the stuff can be safely deleted.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/50#issuecomment-346781457
https://github.com/scverse/scanpy/issues/50#issuecomment-346781457:1279,Performance,cache,cache,1279,"sure, we’ll talk in 10 days or so, after my holidays 😄. except if you want to earlier, then we can skype or so. > One could think about renaming the ""data"" subdirectory to something like ""data_cache"" or so to make evident that this only stores cache files, which can simply be deleted, and everything else stores ""AnnData backing files"" = ""result files"" or exported files... definitely. > But I agree true cache files might be better placed in a tmp directory. no, as i said: cache directory, not temp directory. both have (overridable) standard locations on all OSs. e.g. on linux:. - `$TMPDIR` or `/tmp/`: temporary means that the files are only to be read during the same function/script execution, and deleted after. temp files forgotten by the application that created them are deleted after `$TMPTIME` and on reboot. (on linux now usually because `/tmp/` is a ramdisk and RAM contents don’t survive a reboot). ```py; # python gives you a context manager that deletes the file after its block; with tempfile.TemporaryFile() as fp:; use(fp); # fp and the file are gone now; ```. - `$XDG_CACHE_HOME` or `~/.cache/`: cache files are permanent until the user or OS cleans up or the application decides it no longer needs them (i think e.g. browsers clear out the parts of their cache periodically). since scanpy has a notion of a project directory, putting the cache there is OK as well. the advantage is visibility, but that only works if the user knows what the directory/ies are for. using `cache` in the name of the cache directory would certainly help to signify that the stuff can be safely deleted.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/50#issuecomment-346781457
https://github.com/scverse/scanpy/issues/50#issuecomment-346781457:1362,Performance,cache,cache,1362,"sure, we’ll talk in 10 days or so, after my holidays 😄. except if you want to earlier, then we can skype or so. > One could think about renaming the ""data"" subdirectory to something like ""data_cache"" or so to make evident that this only stores cache files, which can simply be deleted, and everything else stores ""AnnData backing files"" = ""result files"" or exported files... definitely. > But I agree true cache files might be better placed in a tmp directory. no, as i said: cache directory, not temp directory. both have (overridable) standard locations on all OSs. e.g. on linux:. - `$TMPDIR` or `/tmp/`: temporary means that the files are only to be read during the same function/script execution, and deleted after. temp files forgotten by the application that created them are deleted after `$TMPTIME` and on reboot. (on linux now usually because `/tmp/` is a ramdisk and RAM contents don’t survive a reboot). ```py; # python gives you a context manager that deletes the file after its block; with tempfile.TemporaryFile() as fp:; use(fp); # fp and the file are gone now; ```. - `$XDG_CACHE_HOME` or `~/.cache/`: cache files are permanent until the user or OS cleans up or the application decides it no longer needs them (i think e.g. browsers clear out the parts of their cache periodically). since scanpy has a notion of a project directory, putting the cache there is OK as well. the advantage is visibility, but that only works if the user knows what the directory/ies are for. using `cache` in the name of the cache directory would certainly help to signify that the stuff can be safely deleted.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/50#issuecomment-346781457
https://github.com/scverse/scanpy/issues/50#issuecomment-346781457:1495,Performance,cache,cache,1495,"sure, we’ll talk in 10 days or so, after my holidays 😄. except if you want to earlier, then we can skype or so. > One could think about renaming the ""data"" subdirectory to something like ""data_cache"" or so to make evident that this only stores cache files, which can simply be deleted, and everything else stores ""AnnData backing files"" = ""result files"" or exported files... definitely. > But I agree true cache files might be better placed in a tmp directory. no, as i said: cache directory, not temp directory. both have (overridable) standard locations on all OSs. e.g. on linux:. - `$TMPDIR` or `/tmp/`: temporary means that the files are only to be read during the same function/script execution, and deleted after. temp files forgotten by the application that created them are deleted after `$TMPTIME` and on reboot. (on linux now usually because `/tmp/` is a ramdisk and RAM contents don’t survive a reboot). ```py; # python gives you a context manager that deletes the file after its block; with tempfile.TemporaryFile() as fp:; use(fp); # fp and the file are gone now; ```. - `$XDG_CACHE_HOME` or `~/.cache/`: cache files are permanent until the user or OS cleans up or the application decides it no longer needs them (i think e.g. browsers clear out the parts of their cache periodically). since scanpy has a notion of a project directory, putting the cache there is OK as well. the advantage is visibility, but that only works if the user knows what the directory/ies are for. using `cache` in the name of the cache directory would certainly help to signify that the stuff can be safely deleted.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/50#issuecomment-346781457
https://github.com/scverse/scanpy/issues/50#issuecomment-346781457:1521,Performance,cache,cache,1521,"sure, we’ll talk in 10 days or so, after my holidays 😄. except if you want to earlier, then we can skype or so. > One could think about renaming the ""data"" subdirectory to something like ""data_cache"" or so to make evident that this only stores cache files, which can simply be deleted, and everything else stores ""AnnData backing files"" = ""result files"" or exported files... definitely. > But I agree true cache files might be better placed in a tmp directory. no, as i said: cache directory, not temp directory. both have (overridable) standard locations on all OSs. e.g. on linux:. - `$TMPDIR` or `/tmp/`: temporary means that the files are only to be read during the same function/script execution, and deleted after. temp files forgotten by the application that created them are deleted after `$TMPTIME` and on reboot. (on linux now usually because `/tmp/` is a ramdisk and RAM contents don’t survive a reboot). ```py; # python gives you a context manager that deletes the file after its block; with tempfile.TemporaryFile() as fp:; use(fp); # fp and the file are gone now; ```. - `$XDG_CACHE_HOME` or `~/.cache/`: cache files are permanent until the user or OS cleans up or the application decides it no longer needs them (i think e.g. browsers clear out the parts of their cache periodically). since scanpy has a notion of a project directory, putting the cache there is OK as well. the advantage is visibility, but that only works if the user knows what the directory/ies are for. using `cache` in the name of the cache directory would certainly help to signify that the stuff can be safely deleted.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/50#issuecomment-346781457
https://github.com/scverse/scanpy/issues/50#issuecomment-346781457:1591,Safety,safe,safely,1591,"sure, we’ll talk in 10 days or so, after my holidays 😄. except if you want to earlier, then we can skype or so. > One could think about renaming the ""data"" subdirectory to something like ""data_cache"" or so to make evident that this only stores cache files, which can simply be deleted, and everything else stores ""AnnData backing files"" = ""result files"" or exported files... definitely. > But I agree true cache files might be better placed in a tmp directory. no, as i said: cache directory, not temp directory. both have (overridable) standard locations on all OSs. e.g. on linux:. - `$TMPDIR` or `/tmp/`: temporary means that the files are only to be read during the same function/script execution, and deleted after. temp files forgotten by the application that created them are deleted after `$TMPTIME` and on reboot. (on linux now usually because `/tmp/` is a ramdisk and RAM contents don’t survive a reboot). ```py; # python gives you a context manager that deletes the file after its block; with tempfile.TemporaryFile() as fp:; use(fp); # fp and the file are gone now; ```. - `$XDG_CACHE_HOME` or `~/.cache/`: cache files are permanent until the user or OS cleans up or the application decides it no longer needs them (i think e.g. browsers clear out the parts of their cache periodically). since scanpy has a notion of a project directory, putting the cache there is OK as well. the advantage is visibility, but that only works if the user knows what the directory/ies are for. using `cache` in the name of the cache directory would certainly help to signify that the stuff can be safely deleted.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/50#issuecomment-346781457
https://github.com/scverse/scanpy/issues/50#issuecomment-346781457:267,Usability,simpl,simply,267,"sure, we’ll talk in 10 days or so, after my holidays 😄. except if you want to earlier, then we can skype or so. > One could think about renaming the ""data"" subdirectory to something like ""data_cache"" or so to make evident that this only stores cache files, which can simply be deleted, and everything else stores ""AnnData backing files"" = ""result files"" or exported files... definitely. > But I agree true cache files might be better placed in a tmp directory. no, as i said: cache directory, not temp directory. both have (overridable) standard locations on all OSs. e.g. on linux:. - `$TMPDIR` or `/tmp/`: temporary means that the files are only to be read during the same function/script execution, and deleted after. temp files forgotten by the application that created them are deleted after `$TMPTIME` and on reboot. (on linux now usually because `/tmp/` is a ramdisk and RAM contents don’t survive a reboot). ```py; # python gives you a context manager that deletes the file after its block; with tempfile.TemporaryFile() as fp:; use(fp); # fp and the file are gone now; ```. - `$XDG_CACHE_HOME` or `~/.cache/`: cache files are permanent until the user or OS cleans up or the application decides it no longer needs them (i think e.g. browsers clear out the parts of their cache periodically). since scanpy has a notion of a project directory, putting the cache there is OK as well. the advantage is visibility, but that only works if the user knows what the directory/ies are for. using `cache` in the name of the cache directory would certainly help to signify that the stuff can be safely deleted.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/50#issuecomment-346781457
https://github.com/scverse/scanpy/issues/50#issuecomment-346781457:1250,Usability,clear,clear,1250,"sure, we’ll talk in 10 days or so, after my holidays 😄. except if you want to earlier, then we can skype or so. > One could think about renaming the ""data"" subdirectory to something like ""data_cache"" or so to make evident that this only stores cache files, which can simply be deleted, and everything else stores ""AnnData backing files"" = ""result files"" or exported files... definitely. > But I agree true cache files might be better placed in a tmp directory. no, as i said: cache directory, not temp directory. both have (overridable) standard locations on all OSs. e.g. on linux:. - `$TMPDIR` or `/tmp/`: temporary means that the files are only to be read during the same function/script execution, and deleted after. temp files forgotten by the application that created them are deleted after `$TMPTIME` and on reboot. (on linux now usually because `/tmp/` is a ramdisk and RAM contents don’t survive a reboot). ```py; # python gives you a context manager that deletes the file after its block; with tempfile.TemporaryFile() as fp:; use(fp); # fp and the file are gone now; ```. - `$XDG_CACHE_HOME` or `~/.cache/`: cache files are permanent until the user or OS cleans up or the application decides it no longer needs them (i think e.g. browsers clear out the parts of their cache periodically). since scanpy has a notion of a project directory, putting the cache there is OK as well. the advantage is visibility, but that only works if the user knows what the directory/ies are for. using `cache` in the name of the cache directory would certainly help to signify that the stuff can be safely deleted.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/50#issuecomment-346781457
https://github.com/scverse/scanpy/issues/52#issuecomment-348018857:78,Deployability,integrat,integration,78,"Sorry about this; `sc.tl.sim` used to be a separate tool in the beginning and integration into Scanpy was erroneous. For the past months I've only used to produce the two reference datasets linked below. All of the problems you mentioned are fixed in Scanpy 0.3.2. Take a look at:; https://github.com/theislab/scanpy_usage/tree/master/170430_krumsiek11. Cheers,; Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/52#issuecomment-348018857
https://github.com/scverse/scanpy/issues/52#issuecomment-348018857:78,Integrability,integrat,integration,78,"Sorry about this; `sc.tl.sim` used to be a separate tool in the beginning and integration into Scanpy was erroneous. For the past months I've only used to produce the two reference datasets linked below. All of the problems you mentioned are fixed in Scanpy 0.3.2. Take a look at:; https://github.com/theislab/scanpy_usage/tree/master/170430_krumsiek11. Cheers,; Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/52#issuecomment-348018857
https://github.com/scverse/scanpy/issues/52#issuecomment-348160053:573,Usability,clear,clear,573,"Thanks for the quick reaction. While the parameters seem to be respected in Scnapy 0.3.2, there is still some weird caching issue. Everything works OK when the ```write``` directory is empty, but when running multiple simulations in a row, e.g.:. ```; adam_krumsiek11_2 = sc.tl.sim('krumsiek11.txt', nrRealizations=1); sc.pl.sim(adam_krumsiek11_2). adam_krumsiek11_2 = sc.tl.sim('krumsiek11.txt', nrRealizations=2); sc.pl.sim(adam_krumsiek11_2); ```. I sometime get the same result (and both calls report reading from the very same simulation result file). However, when I clear the ```write``` directory between the calls to ```sc.tl.sim```, the results are as expected. This problem occurs only for certain parameters (for example, varying seed this way works as expected - I get two different figures).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/52#issuecomment-348160053
https://github.com/scverse/scanpy/issues/55#issuecomment-354371241:400,Usability,simpl,simply,400,"Hi Davide,. thank you! Currently, we use the default behavior of pandas concatenate: see [here](https://github.com/theislab/anndata/blob/562954b43a9b8faa969e0ec01707bc56cbc021b0/anndata/base.py#L1371-L1400). I'll not be able to fix this during the next days. @flying-sheep, could you have a look and maybe figure out a meaningful option or meaningful default to circumvent this? It should be easy to simply pass an option to DataFrame.concat(). @dawe Thanks again for your pull request. I also put a new anndata version that incorporates it on PyPI. Cheers, ; Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/55#issuecomment-354371241
https://github.com/scverse/scanpy/issues/55#issuecomment-354442003:90,Availability,error,error,90,What would be a useful default?. I would assume: Drop identical observations and throw an error if observations with the same ID but different data exist.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/55#issuecomment-354442003
https://github.com/scverse/scanpy/issues/55#issuecomment-354903240:1188,Modifiability,variab,variable,1188,"@dawe What if you use the 'batch' field in `adata.obs` together with the index that might contain duplicates to get the uniqueness that your're missing. If that's not enough. I would do the same thing as [pandas.concat](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.concat.html), namely generating a multi-index. This would be just a one-line edit for `AnnData.concatenate()` and we should do this; we simply need to make sure that the indexing behavior of AnnData as a whole remains consistent with the following. @flying-sheep, could you do this? ; ```; import pandas as pd; s1 = pd.DataFrame({'v1': ['a', 'b'], 'v2': [2, 3]}); s2 = pd.DataFrame({'v1': ['c', 'd'], 'v2': [3, 4]}); s = pd.concat([s1, s2], keys=['s1', 's2']); print('... the concatenated annotations'); print(s); print('... all observations corresponding to ""s1""'); print(s.loc['s1']); print('... a single observation'); print(s.loc['s1', 0]); print('... values of a single observation'); # this is what we do not want in AnnData, only the behavior of the next line; # that is, a multi-index should be indexed with lists or tuples; print(s.loc['s1', 0].values); print('... single observation and a single variable'); print(s.loc[['s1', 0], 'v1']); print('... single observation and a single variable'); print(s.loc[('s1', 0), 'v1']); ```; gives; ```; ... the concatenated annotations; v1 v2; s1 0 a 2; 1 b 3; s2 0 c 3; 1 d 4; ... all observations corresponding to ""s1""; v1 v2; 0 a 2; 1 b 3; ... a single observation; v1 a; v2 2; Name: (s1, 0), dtype: object; ... values of a single observation; ['a' 2]; ... single observation and a single variable; s1 0 a; 1 b; Name: v1, dtype: object; ... single observation and a single variable; a; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/55#issuecomment-354903240
https://github.com/scverse/scanpy/issues/55#issuecomment-354903240:1274,Modifiability,variab,variable,1274,"@dawe What if you use the 'batch' field in `adata.obs` together with the index that might contain duplicates to get the uniqueness that your're missing. If that's not enough. I would do the same thing as [pandas.concat](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.concat.html), namely generating a multi-index. This would be just a one-line edit for `AnnData.concatenate()` and we should do this; we simply need to make sure that the indexing behavior of AnnData as a whole remains consistent with the following. @flying-sheep, could you do this? ; ```; import pandas as pd; s1 = pd.DataFrame({'v1': ['a', 'b'], 'v2': [2, 3]}); s2 = pd.DataFrame({'v1': ['c', 'd'], 'v2': [3, 4]}); s = pd.concat([s1, s2], keys=['s1', 's2']); print('... the concatenated annotations'); print(s); print('... all observations corresponding to ""s1""'); print(s.loc['s1']); print('... a single observation'); print(s.loc['s1', 0]); print('... values of a single observation'); # this is what we do not want in AnnData, only the behavior of the next line; # that is, a multi-index should be indexed with lists or tuples; print(s.loc['s1', 0].values); print('... single observation and a single variable'); print(s.loc[['s1', 0], 'v1']); print('... single observation and a single variable'); print(s.loc[('s1', 0), 'v1']); ```; gives; ```; ... the concatenated annotations; v1 v2; s1 0 a 2; 1 b 3; s2 0 c 3; 1 d 4; ... all observations corresponding to ""s1""; v1 v2; 0 a 2; 1 b 3; ... a single observation; v1 a; v2 2; Name: (s1, 0), dtype: object; ... values of a single observation; ['a' 2]; ... single observation and a single variable; s1 0 a; 1 b; Name: v1, dtype: object; ... single observation and a single variable; a; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/55#issuecomment-354903240
https://github.com/scverse/scanpy/issues/55#issuecomment-354903240:1623,Modifiability,variab,variable,1623,"@dawe What if you use the 'batch' field in `adata.obs` together with the index that might contain duplicates to get the uniqueness that your're missing. If that's not enough. I would do the same thing as [pandas.concat](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.concat.html), namely generating a multi-index. This would be just a one-line edit for `AnnData.concatenate()` and we should do this; we simply need to make sure that the indexing behavior of AnnData as a whole remains consistent with the following. @flying-sheep, could you do this? ; ```; import pandas as pd; s1 = pd.DataFrame({'v1': ['a', 'b'], 'v2': [2, 3]}); s2 = pd.DataFrame({'v1': ['c', 'd'], 'v2': [3, 4]}); s = pd.concat([s1, s2], keys=['s1', 's2']); print('... the concatenated annotations'); print(s); print('... all observations corresponding to ""s1""'); print(s.loc['s1']); print('... a single observation'); print(s.loc['s1', 0]); print('... values of a single observation'); # this is what we do not want in AnnData, only the behavior of the next line; # that is, a multi-index should be indexed with lists or tuples; print(s.loc['s1', 0].values); print('... single observation and a single variable'); print(s.loc[['s1', 0], 'v1']); print('... single observation and a single variable'); print(s.loc[('s1', 0), 'v1']); ```; gives; ```; ... the concatenated annotations; v1 v2; s1 0 a 2; 1 b 3; s2 0 c 3; 1 d 4; ... all observations corresponding to ""s1""; v1 v2; 0 a 2; 1 b 3; ... a single observation; v1 a; v2 2; Name: (s1, 0), dtype: object; ... values of a single observation; ['a' 2]; ... single observation and a single variable; s1 0 a; 1 b; Name: v1, dtype: object; ... single observation and a single variable; a; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/55#issuecomment-354903240
https://github.com/scverse/scanpy/issues/55#issuecomment-354903240:1707,Modifiability,variab,variable,1707,"@dawe What if you use the 'batch' field in `adata.obs` together with the index that might contain duplicates to get the uniqueness that your're missing. If that's not enough. I would do the same thing as [pandas.concat](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.concat.html), namely generating a multi-index. This would be just a one-line edit for `AnnData.concatenate()` and we should do this; we simply need to make sure that the indexing behavior of AnnData as a whole remains consistent with the following. @flying-sheep, could you do this? ; ```; import pandas as pd; s1 = pd.DataFrame({'v1': ['a', 'b'], 'v2': [2, 3]}); s2 = pd.DataFrame({'v1': ['c', 'd'], 'v2': [3, 4]}); s = pd.concat([s1, s2], keys=['s1', 's2']); print('... the concatenated annotations'); print(s); print('... all observations corresponding to ""s1""'); print(s.loc['s1']); print('... a single observation'); print(s.loc['s1', 0]); print('... values of a single observation'); # this is what we do not want in AnnData, only the behavior of the next line; # that is, a multi-index should be indexed with lists or tuples; print(s.loc['s1', 0].values); print('... single observation and a single variable'); print(s.loc[['s1', 0], 'v1']); print('... single observation and a single variable'); print(s.loc[('s1', 0), 'v1']); ```; gives; ```; ... the concatenated annotations; v1 v2; s1 0 a 2; 1 b 3; s2 0 c 3; 1 d 4; ... all observations corresponding to ""s1""; v1 v2; 0 a 2; 1 b 3; ... a single observation; v1 a; v2 2; Name: (s1, 0), dtype: object; ... values of a single observation; ['a' 2]; ... single observation and a single variable; s1 0 a; 1 b; Name: v1, dtype: object; ... single observation and a single variable; a; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/55#issuecomment-354903240
https://github.com/scverse/scanpy/issues/55#issuecomment-354903240:418,Usability,simpl,simply,418,"@dawe What if you use the 'batch' field in `adata.obs` together with the index that might contain duplicates to get the uniqueness that your're missing. If that's not enough. I would do the same thing as [pandas.concat](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.concat.html), namely generating a multi-index. This would be just a one-line edit for `AnnData.concatenate()` and we should do this; we simply need to make sure that the indexing behavior of AnnData as a whole remains consistent with the following. @flying-sheep, could you do this? ; ```; import pandas as pd; s1 = pd.DataFrame({'v1': ['a', 'b'], 'v2': [2, 3]}); s2 = pd.DataFrame({'v1': ['c', 'd'], 'v2': [3, 4]}); s = pd.concat([s1, s2], keys=['s1', 's2']); print('... the concatenated annotations'); print(s); print('... all observations corresponding to ""s1""'); print(s.loc['s1']); print('... a single observation'); print(s.loc['s1', 0]); print('... values of a single observation'); # this is what we do not want in AnnData, only the behavior of the next line; # that is, a multi-index should be indexed with lists or tuples; print(s.loc['s1', 0].values); print('... single observation and a single variable'); print(s.loc[['s1', 0], 'v1']); print('... single observation and a single variable'); print(s.loc[('s1', 0), 'v1']); ```; gives; ```; ... the concatenated annotations; v1 v2; s1 0 a 2; 1 b 3; s2 0 c 3; 1 d 4; ... all observations corresponding to ""s1""; v1 v2; 0 a 2; 1 b 3; ... a single observation; v1 a; v2 2; Name: (s1, 0), dtype: object; ... values of a single observation; ['a' 2]; ... single observation and a single variable; s1 0 a; 1 b; Name: v1, dtype: object; ... single observation and a single variable; a; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/55#issuecomment-354903240
https://github.com/scverse/scanpy/issues/55#issuecomment-364922530:60,Deployability,release,release,60,"So, this is solved in anndata 0.5 and scanpy 0.4.3. See the release notes (https://scanpy.readthedocs.io) and https://github.com/theislab/anndata/commit/63500075e926f202e856bd04ec673df55bbd2460 and the [example](http://anndata.readthedocs.io/en/latest/anndata.AnnData.concatenate.html). Hope this is a meaningful default. If you pass `index_unique=None`, then it keeps the previous indices including duplicates.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/55#issuecomment-364922530
https://github.com/scverse/scanpy/issues/56#issuecomment-354681849:197,Deployability,install,installed,197,"Dear Bo, sorry for the late response. I just became the father of twins a few days ago and couldn't respond earlier. If you still need an answer, I will look into this tomorrow. If you have `h5ls` installed on the command line, it would be great to show me the output of running `h5ls your_file.h5`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/56#issuecomment-354681849
https://github.com/scverse/scanpy/issues/56#issuecomment-354906745:394,Deployability,pipeline,pipelines,394,"Thanks for the wishes! :). If it's not much work for you: could you paste your workaround here? In my tests, the reading of old AnnData backing files worked fine, but I only tested from version to version... 0.2.8 is already quite old for the speed with which Scanpy evolves, so I probably missed something. In principle, Scanpy should be fully backward compatible; several people have written pipelines and stored files that still have to run with more recent versions of Scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/56#issuecomment-354906745
https://github.com/scverse/scanpy/issues/56#issuecomment-354906745:267,Modifiability,evolve,evolves,267,"Thanks for the wishes! :). If it's not much work for you: could you paste your workaround here? In my tests, the reading of old AnnData backing files worked fine, but I only tested from version to version... 0.2.8 is already quite old for the speed with which Scanpy evolves, so I probably missed something. In principle, Scanpy should be fully backward compatible; several people have written pipelines and stored files that still have to run with more recent versions of Scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/56#issuecomment-354906745
https://github.com/scverse/scanpy/issues/56#issuecomment-354906745:102,Testability,test,tests,102,"Thanks for the wishes! :). If it's not much work for you: could you paste your workaround here? In my tests, the reading of old AnnData backing files worked fine, but I only tested from version to version... 0.2.8 is already quite old for the speed with which Scanpy evolves, so I probably missed something. In principle, Scanpy should be fully backward compatible; several people have written pipelines and stored files that still have to run with more recent versions of Scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/56#issuecomment-354906745
https://github.com/scverse/scanpy/issues/56#issuecomment-354906745:174,Testability,test,tested,174,"Thanks for the wishes! :). If it's not much work for you: could you paste your workaround here? In my tests, the reading of old AnnData backing files worked fine, but I only tested from version to version... 0.2.8 is already quite old for the speed with which Scanpy evolves, so I probably missed something. In principle, Scanpy should be fully backward compatible; several people have written pipelines and stored files that still have to run with more recent versions of Scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/56#issuecomment-354906745
https://github.com/scverse/scanpy/issues/59#issuecomment-354904560:42,Deployability,install,install,42,"Sounds like a great idea! Currently, `pip install scanpy` avoids installing the C++ dependencies, which give some users trouble: `louvain` and `python-igraph`. I think that these should still remain optional dependencies: users should be able to do some basic analysis with plotting without having to install C++ dependencies. They can then continue to install optional dependencies, if they like.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/59#issuecomment-354904560
https://github.com/scverse/scanpy/issues/59#issuecomment-354904560:65,Deployability,install,installing,65,"Sounds like a great idea! Currently, `pip install scanpy` avoids installing the C++ dependencies, which give some users trouble: `louvain` and `python-igraph`. I think that these should still remain optional dependencies: users should be able to do some basic analysis with plotting without having to install C++ dependencies. They can then continue to install optional dependencies, if they like.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/59#issuecomment-354904560
https://github.com/scverse/scanpy/issues/59#issuecomment-354904560:301,Deployability,install,install,301,"Sounds like a great idea! Currently, `pip install scanpy` avoids installing the C++ dependencies, which give some users trouble: `louvain` and `python-igraph`. I think that these should still remain optional dependencies: users should be able to do some basic analysis with plotting without having to install C++ dependencies. They can then continue to install optional dependencies, if they like.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/59#issuecomment-354904560
https://github.com/scverse/scanpy/issues/59#issuecomment-354904560:353,Deployability,install,install,353,"Sounds like a great idea! Currently, `pip install scanpy` avoids installing the C++ dependencies, which give some users trouble: `louvain` and `python-igraph`. I think that these should still remain optional dependencies: users should be able to do some basic analysis with plotting without having to install C++ dependencies. They can then continue to install optional dependencies, if they like.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/59#issuecomment-354904560
https://github.com/scverse/scanpy/issues/59#issuecomment-354904560:84,Integrability,depend,dependencies,84,"Sounds like a great idea! Currently, `pip install scanpy` avoids installing the C++ dependencies, which give some users trouble: `louvain` and `python-igraph`. I think that these should still remain optional dependencies: users should be able to do some basic analysis with plotting without having to install C++ dependencies. They can then continue to install optional dependencies, if they like.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/59#issuecomment-354904560
https://github.com/scverse/scanpy/issues/59#issuecomment-354904560:208,Integrability,depend,dependencies,208,"Sounds like a great idea! Currently, `pip install scanpy` avoids installing the C++ dependencies, which give some users trouble: `louvain` and `python-igraph`. I think that these should still remain optional dependencies: users should be able to do some basic analysis with plotting without having to install C++ dependencies. They can then continue to install optional dependencies, if they like.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/59#issuecomment-354904560
https://github.com/scverse/scanpy/issues/59#issuecomment-354904560:313,Integrability,depend,dependencies,313,"Sounds like a great idea! Currently, `pip install scanpy` avoids installing the C++ dependencies, which give some users trouble: `louvain` and `python-igraph`. I think that these should still remain optional dependencies: users should be able to do some basic analysis with plotting without having to install C++ dependencies. They can then continue to install optional dependencies, if they like.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/59#issuecomment-354904560
https://github.com/scverse/scanpy/issues/59#issuecomment-354904560:370,Integrability,depend,dependencies,370,"Sounds like a great idea! Currently, `pip install scanpy` avoids installing the C++ dependencies, which give some users trouble: `louvain` and `python-igraph`. I think that these should still remain optional dependencies: users should be able to do some basic analysis with plotting without having to install C++ dependencies. They can then continue to install optional dependencies, if they like.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/59#issuecomment-354904560
https://github.com/scverse/scanpy/issues/59#issuecomment-354904560:58,Safety,avoid,avoids,58,"Sounds like a great idea! Currently, `pip install scanpy` avoids installing the C++ dependencies, which give some users trouble: `louvain` and `python-igraph`. I think that these should still remain optional dependencies: users should be able to do some basic analysis with plotting without having to install C++ dependencies. They can then continue to install optional dependencies, if they like.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/59#issuecomment-354904560
https://github.com/scverse/scanpy/issues/59#issuecomment-355115416:24,Deployability,install,installations,24,"so you would want three installations, right?. - full (by manually installing all optional dependencies); - uncomplicated but limited; - super barebones. since `scanpy` is already the second version, we don’t lose anything this way. in the future maybe we can achieve that `scanpy` becomes the full installation (once the C++ dependencies start shipping wheels)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/59#issuecomment-355115416
https://github.com/scverse/scanpy/issues/59#issuecomment-355115416:67,Deployability,install,installing,67,"so you would want three installations, right?. - full (by manually installing all optional dependencies); - uncomplicated but limited; - super barebones. since `scanpy` is already the second version, we don’t lose anything this way. in the future maybe we can achieve that `scanpy` becomes the full installation (once the C++ dependencies start shipping wheels)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/59#issuecomment-355115416
https://github.com/scverse/scanpy/issues/59#issuecomment-355115416:299,Deployability,install,installation,299,"so you would want three installations, right?. - full (by manually installing all optional dependencies); - uncomplicated but limited; - super barebones. since `scanpy` is already the second version, we don’t lose anything this way. in the future maybe we can achieve that `scanpy` becomes the full installation (once the C++ dependencies start shipping wheels)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/59#issuecomment-355115416
https://github.com/scverse/scanpy/issues/59#issuecomment-355115416:91,Integrability,depend,dependencies,91,"so you would want three installations, right?. - full (by manually installing all optional dependencies); - uncomplicated but limited; - super barebones. since `scanpy` is already the second version, we don’t lose anything this way. in the future maybe we can achieve that `scanpy` becomes the full installation (once the C++ dependencies start shipping wheels)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/59#issuecomment-355115416
https://github.com/scverse/scanpy/issues/59#issuecomment-355115416:326,Integrability,depend,dependencies,326,"so you would want three installations, right?. - full (by manually installing all optional dependencies); - uncomplicated but limited; - super barebones. since `scanpy` is already the second version, we don’t lose anything this way. in the future maybe we can achieve that `scanpy` becomes the full installation (once the C++ dependencies start shipping wheels)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/59#issuecomment-355115416
https://github.com/scverse/scanpy/issues/59#issuecomment-355144559:187,Deployability,install,installation,187,"Yes, I think that would be the best solution for the time during which we rely on packages which do not ship proper wheels... . I agree that in the future, `scanpy` could become the full installation. Why not `scanpy-core`, `scanpy`, `scanpy-full`? I don't think it will bother anyone if we stop supporting `scanpy-full` at some point and only use `scanpy`. Given how Scanpy is set up and used, I could also imagine that, upon growing, it will become in some parts even more a thin wrapper for packages that should be optionally installed (it is already a thin wrapper for `igraph`, `louvain` and `MulticoreTSNE`, where Scanpy simply makes the usage more convenient by unifying visualization etc. and efficient by reusing input parameters that have previously been computed and used in other parts of Scanpy - right now, essentially all the preprocessing, the neighborhood relations and graph stuff). . What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/59#issuecomment-355144559
https://github.com/scverse/scanpy/issues/59#issuecomment-355144559:529,Deployability,install,installed,529,"Yes, I think that would be the best solution for the time during which we rely on packages which do not ship proper wheels... . I agree that in the future, `scanpy` could become the full installation. Why not `scanpy-core`, `scanpy`, `scanpy-full`? I don't think it will bother anyone if we stop supporting `scanpy-full` at some point and only use `scanpy`. Given how Scanpy is set up and used, I could also imagine that, upon growing, it will become in some parts even more a thin wrapper for packages that should be optionally installed (it is already a thin wrapper for `igraph`, `louvain` and `MulticoreTSNE`, where Scanpy simply makes the usage more convenient by unifying visualization etc. and efficient by reusing input parameters that have previously been computed and used in other parts of Scanpy - right now, essentially all the preprocessing, the neighborhood relations and graph stuff). . What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/59#issuecomment-355144559
https://github.com/scverse/scanpy/issues/59#issuecomment-355144559:701,Energy Efficiency,efficient,efficient,701,"Yes, I think that would be the best solution for the time during which we rely on packages which do not ship proper wheels... . I agree that in the future, `scanpy` could become the full installation. Why not `scanpy-core`, `scanpy`, `scanpy-full`? I don't think it will bother anyone if we stop supporting `scanpy-full` at some point and only use `scanpy`. Given how Scanpy is set up and used, I could also imagine that, upon growing, it will become in some parts even more a thin wrapper for packages that should be optionally installed (it is already a thin wrapper for `igraph`, `louvain` and `MulticoreTSNE`, where Scanpy simply makes the usage more convenient by unifying visualization etc. and efficient by reusing input parameters that have previously been computed and used in other parts of Scanpy - right now, essentially all the preprocessing, the neighborhood relations and graph stuff). . What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/59#issuecomment-355144559
https://github.com/scverse/scanpy/issues/59#issuecomment-355144559:482,Integrability,wrap,wrapper,482,"Yes, I think that would be the best solution for the time during which we rely on packages which do not ship proper wheels... . I agree that in the future, `scanpy` could become the full installation. Why not `scanpy-core`, `scanpy`, `scanpy-full`? I don't think it will bother anyone if we stop supporting `scanpy-full` at some point and only use `scanpy`. Given how Scanpy is set up and used, I could also imagine that, upon growing, it will become in some parts even more a thin wrapper for packages that should be optionally installed (it is already a thin wrapper for `igraph`, `louvain` and `MulticoreTSNE`, where Scanpy simply makes the usage more convenient by unifying visualization etc. and efficient by reusing input parameters that have previously been computed and used in other parts of Scanpy - right now, essentially all the preprocessing, the neighborhood relations and graph stuff). . What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/59#issuecomment-355144559
https://github.com/scverse/scanpy/issues/59#issuecomment-355144559:561,Integrability,wrap,wrapper,561,"Yes, I think that would be the best solution for the time during which we rely on packages which do not ship proper wheels... . I agree that in the future, `scanpy` could become the full installation. Why not `scanpy-core`, `scanpy`, `scanpy-full`? I don't think it will bother anyone if we stop supporting `scanpy-full` at some point and only use `scanpy`. Given how Scanpy is set up and used, I could also imagine that, upon growing, it will become in some parts even more a thin wrapper for packages that should be optionally installed (it is already a thin wrapper for `igraph`, `louvain` and `MulticoreTSNE`, where Scanpy simply makes the usage more convenient by unifying visualization etc. and efficient by reusing input parameters that have previously been computed and used in other parts of Scanpy - right now, essentially all the preprocessing, the neighborhood relations and graph stuff). . What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/59#issuecomment-355144559
https://github.com/scverse/scanpy/issues/59#issuecomment-355144559:627,Usability,simpl,simply,627,"Yes, I think that would be the best solution for the time during which we rely on packages which do not ship proper wheels... . I agree that in the future, `scanpy` could become the full installation. Why not `scanpy-core`, `scanpy`, `scanpy-full`? I don't think it will bother anyone if we stop supporting `scanpy-full` at some point and only use `scanpy`. Given how Scanpy is set up and used, I could also imagine that, upon growing, it will become in some parts even more a thin wrapper for packages that should be optionally installed (it is already a thin wrapper for `igraph`, `louvain` and `MulticoreTSNE`, where Scanpy simply makes the usage more convenient by unifying visualization etc. and efficient by reusing input parameters that have previously been computed and used in other parts of Scanpy - right now, essentially all the preprocessing, the neighborhood relations and graph stuff). . What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/59#issuecomment-355144559
https://github.com/scverse/scanpy/issues/59#issuecomment-355146641:69,Integrability,depend,depend,69,"sure!. as long as the last version of `scanpy-full` does nothing but depend on `scanpy`, nobody will suffer any consequences if it becomes obsolete one day.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/59#issuecomment-355146641
https://github.com/scverse/scanpy/pull/60#issuecomment-355011043:72,Testability,test,tests,72,"Great, thank you! I wasn't completely happy with how Tobias wrote these tests on pickled files - they never actually passed on travis, it's much better that now, they pass! :smile: :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/60#issuecomment-355011043
https://github.com/scverse/scanpy/issues/61#issuecomment-355082458:1003,Availability,down,down,1003,"yes, I know, that's non-ideal... the sparseness issue is circumvented by only returning top-scoring genes... I see that you make suggestions for how the user can get dataframes but I tend to say that he shouldn't have to do some extra work for this. i think we should continue to return a table with groups vs. top-scoring genes. this is also what all others (Seurat, Pagoda, ...) do and what, I guess, feels most intuitive. a sparse object is likely to confuse users. if we start changing this, we should also talk to @mbuttner, who has written a function for transforming the recarrays to a single dataframe to write them to a csv or xls file and send it out to collaborators... we should also talk to @tcallies, who worked a lot on `rank_genes_groups`; ; our current workflow often involves showing collaborators tables of marker genes for different cell groups. these can get quite long as, e.g., transcription factors are not much differentially expressed, hence not top-scoring and appear further down the tabular. the tabular therefore has to be easily inspectable. currently, you can quickly turn a single rearray into a dataframe as shown [here](https://github.com/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb). `rank_genes_groups` returns a recarray for historical reasons: there is a simple hdf5-backing via the recarray. these days, since the hdf5-backing of categorical data types within anndata works well, we could think about returning a dataframe directly. i guess this would be the way to go requiring only minor modifactions in that the hdf5-backing also accepts dataframes in `.uns` and not only in `.obs` and `.var`. very generally: I think that it would be a decent convention to only allow strings to denote groups/categories. this was also the convetion before using dataframes for the annotation. now we use the category dtype of pandas, which - in contrast to R - allows arbitrary data types for denoting categories. I don't see much advantage of this flexibi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/61#issuecomment-355082458
https://github.com/scverse/scanpy/issues/61#issuecomment-355082458:414,Usability,intuit,intuitive,414,"yes, I know, that's non-ideal... the sparseness issue is circumvented by only returning top-scoring genes... I see that you make suggestions for how the user can get dataframes but I tend to say that he shouldn't have to do some extra work for this. i think we should continue to return a table with groups vs. top-scoring genes. this is also what all others (Seurat, Pagoda, ...) do and what, I guess, feels most intuitive. a sparse object is likely to confuse users. if we start changing this, we should also talk to @mbuttner, who has written a function for transforming the recarrays to a single dataframe to write them to a csv or xls file and send it out to collaborators... we should also talk to @tcallies, who worked a lot on `rank_genes_groups`; ; our current workflow often involves showing collaborators tables of marker genes for different cell groups. these can get quite long as, e.g., transcription factors are not much differentially expressed, hence not top-scoring and appear further down the tabular. the tabular therefore has to be easily inspectable. currently, you can quickly turn a single rearray into a dataframe as shown [here](https://github.com/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb). `rank_genes_groups` returns a recarray for historical reasons: there is a simple hdf5-backing via the recarray. these days, since the hdf5-backing of categorical data types within anndata works well, we could think about returning a dataframe directly. i guess this would be the way to go requiring only minor modifactions in that the hdf5-backing also accepts dataframes in `.uns` and not only in `.obs` and `.var`. very generally: I think that it would be a decent convention to only allow strings to denote groups/categories. this was also the convetion before using dataframes for the annotation. now we use the category dtype of pandas, which - in contrast to R - allows arbitrary data types for denoting categories. I don't see much advantage of this flexibi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/61#issuecomment-355082458
https://github.com/scverse/scanpy/issues/61#issuecomment-355082458:1311,Usability,simpl,simple,1311,"he user can get dataframes but I tend to say that he shouldn't have to do some extra work for this. i think we should continue to return a table with groups vs. top-scoring genes. this is also what all others (Seurat, Pagoda, ...) do and what, I guess, feels most intuitive. a sparse object is likely to confuse users. if we start changing this, we should also talk to @mbuttner, who has written a function for transforming the recarrays to a single dataframe to write them to a csv or xls file and send it out to collaborators... we should also talk to @tcallies, who worked a lot on `rank_genes_groups`; ; our current workflow often involves showing collaborators tables of marker genes for different cell groups. these can get quite long as, e.g., transcription factors are not much differentially expressed, hence not top-scoring and appear further down the tabular. the tabular therefore has to be easily inspectable. currently, you can quickly turn a single rearray into a dataframe as shown [here](https://github.com/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb). `rank_genes_groups` returns a recarray for historical reasons: there is a simple hdf5-backing via the recarray. these days, since the hdf5-backing of categorical data types within anndata works well, we could think about returning a dataframe directly. i guess this would be the way to go requiring only minor modifactions in that the hdf5-backing also accepts dataframes in `.uns` and not only in `.obs` and `.var`. very generally: I think that it would be a decent convention to only allow strings to denote groups/categories. this was also the convetion before using dataframes for the annotation. now we use the category dtype of pandas, which - in contrast to R - allows arbitrary data types for denoting categories. I don't see much advantage of this flexibility but we should probably stick with it. hence, another argument for simply using a dataframe and putting it in the unstructured annotation `.uns`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/61#issuecomment-355082458
https://github.com/scverse/scanpy/issues/61#issuecomment-355082458:2072,Usability,simpl,simply,2072,"he user can get dataframes but I tend to say that he shouldn't have to do some extra work for this. i think we should continue to return a table with groups vs. top-scoring genes. this is also what all others (Seurat, Pagoda, ...) do and what, I guess, feels most intuitive. a sparse object is likely to confuse users. if we start changing this, we should also talk to @mbuttner, who has written a function for transforming the recarrays to a single dataframe to write them to a csv or xls file and send it out to collaborators... we should also talk to @tcallies, who worked a lot on `rank_genes_groups`; ; our current workflow often involves showing collaborators tables of marker genes for different cell groups. these can get quite long as, e.g., transcription factors are not much differentially expressed, hence not top-scoring and appear further down the tabular. the tabular therefore has to be easily inspectable. currently, you can quickly turn a single rearray into a dataframe as shown [here](https://github.com/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb). `rank_genes_groups` returns a recarray for historical reasons: there is a simple hdf5-backing via the recarray. these days, since the hdf5-backing of categorical data types within anndata works well, we could think about returning a dataframe directly. i guess this would be the way to go requiring only minor modifactions in that the hdf5-backing also accepts dataframes in `.uns` and not only in `.obs` and `.var`. very generally: I think that it would be a decent convention to only allow strings to denote groups/categories. this was also the convetion before using dataframes for the annotation. now we use the category dtype of pandas, which - in contrast to R - allows arbitrary data types for denoting categories. I don't see much advantage of this flexibility but we should probably stick with it. hence, another argument for simply using a dataframe and putting it in the unstructured annotation `.uns`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/61#issuecomment-355082458
https://github.com/scverse/scanpy/issues/61#issuecomment-1874144812:146,Testability,test,tests,146,"As an intermediate solution, we could. 1. implement https://github.com/scverse/anndata/issues/679; 2. write recarrays as dataframes; 3. make sure tests run successfully on anndata objects where `adata.uns[""rank_genes_groups_filtered""][""names""]` has been converted into a DataFrame",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/61#issuecomment-1874144812
https://github.com/scverse/scanpy/issues/62#issuecomment-355731449:112,Deployability,install,install,112,"Sorry about this bug in AnnData views, which have only recently been introduced. Is fixed in anndata 0.4.4 `pip install anndata --upgrade` and on the master branch: https://github.com/theislab/anndata/commit/ba9b3eed381ce427920ec67e13331d5423a5d9b3.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/62#issuecomment-355731449
https://github.com/scverse/scanpy/issues/62#issuecomment-355731449:130,Deployability,upgrade,upgrade,130,"Sorry about this bug in AnnData views, which have only recently been introduced. Is fixed in anndata 0.4.4 `pip install anndata --upgrade` and on the master branch: https://github.com/theislab/anndata/commit/ba9b3eed381ce427920ec67e13331d5423a5d9b3.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/62#issuecomment-355731449
https://github.com/scverse/scanpy/issues/63#issuecomment-355768104:119,Deployability,release,release,119,thank you! the bug was fixed in https://github.com/theislab/scanpy/commit/a4baaaf6c29b8da4f3d9026552719039d2600ca9 and release 0.4.1: `pip install scanpy --upgrade`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/63#issuecomment-355768104
https://github.com/scverse/scanpy/issues/63#issuecomment-355768104:139,Deployability,install,install,139,thank you! the bug was fixed in https://github.com/theislab/scanpy/commit/a4baaaf6c29b8da4f3d9026552719039d2600ca9 and release 0.4.1: `pip install scanpy --upgrade`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/63#issuecomment-355768104
https://github.com/scverse/scanpy/issues/63#issuecomment-355768104:156,Deployability,upgrade,upgrade,156,thank you! the bug was fixed in https://github.com/theislab/scanpy/commit/a4baaaf6c29b8da4f3d9026552719039d2600ca9 and release 0.4.1: `pip install scanpy --upgrade`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/63#issuecomment-355768104
https://github.com/scverse/scanpy/issues/65#issuecomment-356956056:411,Deployability,release,release,411,"Hi Jorvis! This should be very easy. Use the text file reader:; ```; adata = sc.read_text(filename).transpose(); ```; or use the general purpose reader that writes cache files automatically; ```; adata = sc.read(filename, ext='txt').transpose() # 'tab', 'data', 'tsv' mean the same; ```; see the [API docs](https://scanpy.readthedocs.io/en/latest/api/index.html). The 'tsv' file ending is not yet in the latest release, I just commited that: https://github.com/theislab/scanpy/commit/884c5f8a6a39c43aef27c7398ec9c195b977a3d3. Hope this helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/65#issuecomment-356956056
https://github.com/scverse/scanpy/issues/65#issuecomment-356956056:164,Performance,cache,cache,164,"Hi Jorvis! This should be very easy. Use the text file reader:; ```; adata = sc.read_text(filename).transpose(); ```; or use the general purpose reader that writes cache files automatically; ```; adata = sc.read(filename, ext='txt').transpose() # 'tab', 'data', 'tsv' mean the same; ```; see the [API docs](https://scanpy.readthedocs.io/en/latest/api/index.html). The 'tsv' file ending is not yet in the latest release, I just commited that: https://github.com/theislab/scanpy/commit/884c5f8a6a39c43aef27c7398ec9c195b977a3d3. Hope this helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/65#issuecomment-356956056
https://github.com/scverse/scanpy/pull/68#issuecomment-357785692:131,Usability,simpl,simple,131,"Awesome, Gokcen, thank you! :grin:. Also, adding an export utility for Gephi was on the list already before. Cool that you found a simple solution for this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/68#issuecomment-357785692
https://github.com/scverse/scanpy/pull/68#issuecomment-357787075:618,Deployability,install,install,618,"> Awesome, Gokcen, thank you! 😁; >. Thank you!; ; > Also, adding an export utility for Gephi was on the list already before. Cool that you found a simple solution for this.; > . Ah ok, didn't know that. Here is what I used so far for gephi:. ```python; # python-igraph from master branch is required; # see https://github.com/igraph/python-igraph/issues/115; from igraph.remote.gephi import GephiConnection, GephiGraphStreamer. sc.tl.draw_graph(adata); # would be also nice have access to igraph object right after sc.tl.draw_graph; g = sc.utils.get_igraph_from_adjacency(adata.uns['data_graph_norm_weights']). # then install latest Gephi and the streaming plugin:; # https://gephi.org/plugins/#/plugin/graphstreaming; # and start the Gephi master server; streamer = GephiGraphStreamer(); conn = GephiConnection(workspace=1). # igraph cannot serialize numpy float32 to json, so it must be converted to float64; g.es['weight'] = [float(x) for x in g.es['weight']]; g.vs['groups'] = adata.obs['louvain_groups'].tolist(); streamer.post(g, conn); ```. Here is the Yifan Hu layout for 3K PBMC:. ![image](https://user-images.githubusercontent.com/1140359/34961174-384c5658-fa0c-11e7-8597-db4e77cbf4e3.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/68#issuecomment-357787075
https://github.com/scverse/scanpy/pull/68#issuecomment-357787075:657,Modifiability,plugin,plugin,657,"> Awesome, Gokcen, thank you! 😁; >. Thank you!; ; > Also, adding an export utility for Gephi was on the list already before. Cool that you found a simple solution for this.; > . Ah ok, didn't know that. Here is what I used so far for gephi:. ```python; # python-igraph from master branch is required; # see https://github.com/igraph/python-igraph/issues/115; from igraph.remote.gephi import GephiConnection, GephiGraphStreamer. sc.tl.draw_graph(adata); # would be also nice have access to igraph object right after sc.tl.draw_graph; g = sc.utils.get_igraph_from_adjacency(adata.uns['data_graph_norm_weights']). # then install latest Gephi and the streaming plugin:; # https://gephi.org/plugins/#/plugin/graphstreaming; # and start the Gephi master server; streamer = GephiGraphStreamer(); conn = GephiConnection(workspace=1). # igraph cannot serialize numpy float32 to json, so it must be converted to float64; g.es['weight'] = [float(x) for x in g.es['weight']]; g.vs['groups'] = adata.obs['louvain_groups'].tolist(); streamer.post(g, conn); ```. Here is the Yifan Hu layout for 3K PBMC:. ![image](https://user-images.githubusercontent.com/1140359/34961174-384c5658-fa0c-11e7-8597-db4e77cbf4e3.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/68#issuecomment-357787075
https://github.com/scverse/scanpy/pull/68#issuecomment-357787075:686,Modifiability,plugin,plugins,686,"> Awesome, Gokcen, thank you! 😁; >. Thank you!; ; > Also, adding an export utility for Gephi was on the list already before. Cool that you found a simple solution for this.; > . Ah ok, didn't know that. Here is what I used so far for gephi:. ```python; # python-igraph from master branch is required; # see https://github.com/igraph/python-igraph/issues/115; from igraph.remote.gephi import GephiConnection, GephiGraphStreamer. sc.tl.draw_graph(adata); # would be also nice have access to igraph object right after sc.tl.draw_graph; g = sc.utils.get_igraph_from_adjacency(adata.uns['data_graph_norm_weights']). # then install latest Gephi and the streaming plugin:; # https://gephi.org/plugins/#/plugin/graphstreaming; # and start the Gephi master server; streamer = GephiGraphStreamer(); conn = GephiConnection(workspace=1). # igraph cannot serialize numpy float32 to json, so it must be converted to float64; g.es['weight'] = [float(x) for x in g.es['weight']]; g.vs['groups'] = adata.obs['louvain_groups'].tolist(); streamer.post(g, conn); ```. Here is the Yifan Hu layout for 3K PBMC:. ![image](https://user-images.githubusercontent.com/1140359/34961174-384c5658-fa0c-11e7-8597-db4e77cbf4e3.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/68#issuecomment-357787075
https://github.com/scverse/scanpy/pull/68#issuecomment-357787075:696,Modifiability,plugin,plugin,696,"> Awesome, Gokcen, thank you! 😁; >. Thank you!; ; > Also, adding an export utility for Gephi was on the list already before. Cool that you found a simple solution for this.; > . Ah ok, didn't know that. Here is what I used so far for gephi:. ```python; # python-igraph from master branch is required; # see https://github.com/igraph/python-igraph/issues/115; from igraph.remote.gephi import GephiConnection, GephiGraphStreamer. sc.tl.draw_graph(adata); # would be also nice have access to igraph object right after sc.tl.draw_graph; g = sc.utils.get_igraph_from_adjacency(adata.uns['data_graph_norm_weights']). # then install latest Gephi and the streaming plugin:; # https://gephi.org/plugins/#/plugin/graphstreaming; # and start the Gephi master server; streamer = GephiGraphStreamer(); conn = GephiConnection(workspace=1). # igraph cannot serialize numpy float32 to json, so it must be converted to float64; g.es['weight'] = [float(x) for x in g.es['weight']]; g.vs['groups'] = adata.obs['louvain_groups'].tolist(); streamer.post(g, conn); ```. Here is the Yifan Hu layout for 3K PBMC:. ![image](https://user-images.githubusercontent.com/1140359/34961174-384c5658-fa0c-11e7-8597-db4e77cbf4e3.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/68#issuecomment-357787075
https://github.com/scverse/scanpy/pull/68#issuecomment-357787075:479,Security,access,access,479,"> Awesome, Gokcen, thank you! 😁; >. Thank you!; ; > Also, adding an export utility for Gephi was on the list already before. Cool that you found a simple solution for this.; > . Ah ok, didn't know that. Here is what I used so far for gephi:. ```python; # python-igraph from master branch is required; # see https://github.com/igraph/python-igraph/issues/115; from igraph.remote.gephi import GephiConnection, GephiGraphStreamer. sc.tl.draw_graph(adata); # would be also nice have access to igraph object right after sc.tl.draw_graph; g = sc.utils.get_igraph_from_adjacency(adata.uns['data_graph_norm_weights']). # then install latest Gephi and the streaming plugin:; # https://gephi.org/plugins/#/plugin/graphstreaming; # and start the Gephi master server; streamer = GephiGraphStreamer(); conn = GephiConnection(workspace=1). # igraph cannot serialize numpy float32 to json, so it must be converted to float64; g.es['weight'] = [float(x) for x in g.es['weight']]; g.vs['groups'] = adata.obs['louvain_groups'].tolist(); streamer.post(g, conn); ```. Here is the Yifan Hu layout for 3K PBMC:. ![image](https://user-images.githubusercontent.com/1140359/34961174-384c5658-fa0c-11e7-8597-db4e77cbf4e3.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/68#issuecomment-357787075
https://github.com/scverse/scanpy/pull/68#issuecomment-357787075:147,Usability,simpl,simple,147,"> Awesome, Gokcen, thank you! 😁; >. Thank you!; ; > Also, adding an export utility for Gephi was on the list already before. Cool that you found a simple solution for this.; > . Ah ok, didn't know that. Here is what I used so far for gephi:. ```python; # python-igraph from master branch is required; # see https://github.com/igraph/python-igraph/issues/115; from igraph.remote.gephi import GephiConnection, GephiGraphStreamer. sc.tl.draw_graph(adata); # would be also nice have access to igraph object right after sc.tl.draw_graph; g = sc.utils.get_igraph_from_adjacency(adata.uns['data_graph_norm_weights']). # then install latest Gephi and the streaming plugin:; # https://gephi.org/plugins/#/plugin/graphstreaming; # and start the Gephi master server; streamer = GephiGraphStreamer(); conn = GephiConnection(workspace=1). # igraph cannot serialize numpy float32 to json, so it must be converted to float64; g.es['weight'] = [float(x) for x in g.es['weight']]; g.vs['groups'] = adata.obs['louvain_groups'].tolist(); streamer.post(g, conn); ```. Here is the Yifan Hu layout for 3K PBMC:. ![image](https://user-images.githubusercontent.com/1140359/34961174-384c5658-fa0c-11e7-8597-db4e77cbf4e3.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/68#issuecomment-357787075
https://github.com/scverse/scanpy/issues/69#issuecomment-358298098:459,Deployability,update,update,459,"Hi Benedikt!. Yes, we could make this automatic. Let me briefly think whether this will have any unwanted side effects. There are some advantages of letting the user fully control the annotation dataframes. PS: you could also call `mammary.obs['CellType'].cat.remove_unused_categories(inplace=True)`.; PPS: The mouse atlas example has been written by a first-time-python user, according to what he stated. It can be written a lot more elegantly. We will soon update it...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/69#issuecomment-358298098
https://github.com/scverse/scanpy/issues/69#issuecomment-364919294:63,Deployability,release,release,63,This has been implemented in anndata 0.5 and scanpy 0.4.3. See release notes (https://scanpy.readthedocs.io). See https://github.com/theislab/anndata/commit/8cabf9c86a38d6db88c664e2ea28e3fb29bdf99e and a few fixes after that.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/69#issuecomment-364919294
https://github.com/scverse/scanpy/issues/70#issuecomment-358711733:120,Performance,load,loaded-from-tabular-data,120,I also [put this on StackOverflow](https://stackoverflow.com/questions/48326579/unable-to-iterate-over-pandas-dataframe-loaded-from-tabular-data),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/70#issuecomment-358711733
https://github.com/scverse/scanpy/issues/71#issuecomment-359410549:219,Modifiability,variab,variables,219,"`n_genes_user` is supposed to limit the length of the returned tables. however, one still needs to search all genes `n_genes` (`== X.shape[1]`) in order to get the top-scoring ones. this is the rationale behind the two variables and the naming",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/71#issuecomment-359410549
https://github.com/scverse/scanpy/issues/72#issuecomment-361891662:38,Availability,avail,available,38,"Dear both, . correlation matrices are available now. Following our usual split into tools and plotting, you can call . `sc.tl.correlation_matrix(adata,name_list, n_genes=20, annotation_key=None, method='pearson')`. for correlation matrix calculation. ; I have left out a few parameters because I wrote the function actually to conveniently plot results from DE testing, but the basic functionality is the following: . _adata_ is the usual AnnData object you are working with. ; _name_list_ is a string containing gene names and should be specified. ; _n_genes_ cuts the name_list if the number specified is smaller then the length of the list, so set this high enough if you want to work with large data ; _annotation_key_ allows you to specify a string that works as the key in the AnnData object where results are stored. By default, the key is ""Correlation_matrix"". The method basically wraps the pd.DataFrame.corr method, which allows you to specify the correlation method ('pearson', 'spearman', 'kendall'). . I use it for smaller data so it has not been optimized for performance (yet), but I tested the method for 3k cells and 600 genes and ended up with a runtime of ~8 seconds. I hope that is conveniently fast enough for you (if not let us know). . After calling the tool, you can plot correlation matrices (using a wrapper for seaborn heatmap) by calling. `sc.pl.correlation_matrix(adata, annotation_key=None)`. This function searches basically only the AnnData annotation (again, if no key specified, ""Correlation_matrix"" is the default). Hope this does the job!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/72#issuecomment-361891662
https://github.com/scverse/scanpy/issues/72#issuecomment-361891662:890,Integrability,wrap,wraps,890,"Dear both, . correlation matrices are available now. Following our usual split into tools and plotting, you can call . `sc.tl.correlation_matrix(adata,name_list, n_genes=20, annotation_key=None, method='pearson')`. for correlation matrix calculation. ; I have left out a few parameters because I wrote the function actually to conveniently plot results from DE testing, but the basic functionality is the following: . _adata_ is the usual AnnData object you are working with. ; _name_list_ is a string containing gene names and should be specified. ; _n_genes_ cuts the name_list if the number specified is smaller then the length of the list, so set this high enough if you want to work with large data ; _annotation_key_ allows you to specify a string that works as the key in the AnnData object where results are stored. By default, the key is ""Correlation_matrix"". The method basically wraps the pd.DataFrame.corr method, which allows you to specify the correlation method ('pearson', 'spearman', 'kendall'). . I use it for smaller data so it has not been optimized for performance (yet), but I tested the method for 3k cells and 600 genes and ended up with a runtime of ~8 seconds. I hope that is conveniently fast enough for you (if not let us know). . After calling the tool, you can plot correlation matrices (using a wrapper for seaborn heatmap) by calling. `sc.pl.correlation_matrix(adata, annotation_key=None)`. This function searches basically only the AnnData annotation (again, if no key specified, ""Correlation_matrix"" is the default). Hope this does the job!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/72#issuecomment-361891662
https://github.com/scverse/scanpy/issues/72#issuecomment-361891662:1326,Integrability,wrap,wrapper,1326,"Dear both, . correlation matrices are available now. Following our usual split into tools and plotting, you can call . `sc.tl.correlation_matrix(adata,name_list, n_genes=20, annotation_key=None, method='pearson')`. for correlation matrix calculation. ; I have left out a few parameters because I wrote the function actually to conveniently plot results from DE testing, but the basic functionality is the following: . _adata_ is the usual AnnData object you are working with. ; _name_list_ is a string containing gene names and should be specified. ; _n_genes_ cuts the name_list if the number specified is smaller then the length of the list, so set this high enough if you want to work with large data ; _annotation_key_ allows you to specify a string that works as the key in the AnnData object where results are stored. By default, the key is ""Correlation_matrix"". The method basically wraps the pd.DataFrame.corr method, which allows you to specify the correlation method ('pearson', 'spearman', 'kendall'). . I use it for smaller data so it has not been optimized for performance (yet), but I tested the method for 3k cells and 600 genes and ended up with a runtime of ~8 seconds. I hope that is conveniently fast enough for you (if not let us know). . After calling the tool, you can plot correlation matrices (using a wrapper for seaborn heatmap) by calling. `sc.pl.correlation_matrix(adata, annotation_key=None)`. This function searches basically only the AnnData annotation (again, if no key specified, ""Correlation_matrix"" is the default). Hope this does the job!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/72#issuecomment-361891662
https://github.com/scverse/scanpy/issues/72#issuecomment-361891662:1060,Performance,optimiz,optimized,1060,"Dear both, . correlation matrices are available now. Following our usual split into tools and plotting, you can call . `sc.tl.correlation_matrix(adata,name_list, n_genes=20, annotation_key=None, method='pearson')`. for correlation matrix calculation. ; I have left out a few parameters because I wrote the function actually to conveniently plot results from DE testing, but the basic functionality is the following: . _adata_ is the usual AnnData object you are working with. ; _name_list_ is a string containing gene names and should be specified. ; _n_genes_ cuts the name_list if the number specified is smaller then the length of the list, so set this high enough if you want to work with large data ; _annotation_key_ allows you to specify a string that works as the key in the AnnData object where results are stored. By default, the key is ""Correlation_matrix"". The method basically wraps the pd.DataFrame.corr method, which allows you to specify the correlation method ('pearson', 'spearman', 'kendall'). . I use it for smaller data so it has not been optimized for performance (yet), but I tested the method for 3k cells and 600 genes and ended up with a runtime of ~8 seconds. I hope that is conveniently fast enough for you (if not let us know). . After calling the tool, you can plot correlation matrices (using a wrapper for seaborn heatmap) by calling. `sc.pl.correlation_matrix(adata, annotation_key=None)`. This function searches basically only the AnnData annotation (again, if no key specified, ""Correlation_matrix"" is the default). Hope this does the job!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/72#issuecomment-361891662
https://github.com/scverse/scanpy/issues/72#issuecomment-361891662:1074,Performance,perform,performance,1074,"Dear both, . correlation matrices are available now. Following our usual split into tools and plotting, you can call . `sc.tl.correlation_matrix(adata,name_list, n_genes=20, annotation_key=None, method='pearson')`. for correlation matrix calculation. ; I have left out a few parameters because I wrote the function actually to conveniently plot results from DE testing, but the basic functionality is the following: . _adata_ is the usual AnnData object you are working with. ; _name_list_ is a string containing gene names and should be specified. ; _n_genes_ cuts the name_list if the number specified is smaller then the length of the list, so set this high enough if you want to work with large data ; _annotation_key_ allows you to specify a string that works as the key in the AnnData object where results are stored. By default, the key is ""Correlation_matrix"". The method basically wraps the pd.DataFrame.corr method, which allows you to specify the correlation method ('pearson', 'spearman', 'kendall'). . I use it for smaller data so it has not been optimized for performance (yet), but I tested the method for 3k cells and 600 genes and ended up with a runtime of ~8 seconds. I hope that is conveniently fast enough for you (if not let us know). . After calling the tool, you can plot correlation matrices (using a wrapper for seaborn heatmap) by calling. `sc.pl.correlation_matrix(adata, annotation_key=None)`. This function searches basically only the AnnData annotation (again, if no key specified, ""Correlation_matrix"" is the default). Hope this does the job!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/72#issuecomment-361891662
https://github.com/scverse/scanpy/issues/72#issuecomment-361891662:361,Testability,test,testing,361,"Dear both, . correlation matrices are available now. Following our usual split into tools and plotting, you can call . `sc.tl.correlation_matrix(adata,name_list, n_genes=20, annotation_key=None, method='pearson')`. for correlation matrix calculation. ; I have left out a few parameters because I wrote the function actually to conveniently plot results from DE testing, but the basic functionality is the following: . _adata_ is the usual AnnData object you are working with. ; _name_list_ is a string containing gene names and should be specified. ; _n_genes_ cuts the name_list if the number specified is smaller then the length of the list, so set this high enough if you want to work with large data ; _annotation_key_ allows you to specify a string that works as the key in the AnnData object where results are stored. By default, the key is ""Correlation_matrix"". The method basically wraps the pd.DataFrame.corr method, which allows you to specify the correlation method ('pearson', 'spearman', 'kendall'). . I use it for smaller data so it has not been optimized for performance (yet), but I tested the method for 3k cells and 600 genes and ended up with a runtime of ~8 seconds. I hope that is conveniently fast enough for you (if not let us know). . After calling the tool, you can plot correlation matrices (using a wrapper for seaborn heatmap) by calling. `sc.pl.correlation_matrix(adata, annotation_key=None)`. This function searches basically only the AnnData annotation (again, if no key specified, ""Correlation_matrix"" is the default). Hope this does the job!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/72#issuecomment-361891662
https://github.com/scverse/scanpy/issues/72#issuecomment-361891662:1099,Testability,test,tested,1099,"Dear both, . correlation matrices are available now. Following our usual split into tools and plotting, you can call . `sc.tl.correlation_matrix(adata,name_list, n_genes=20, annotation_key=None, method='pearson')`. for correlation matrix calculation. ; I have left out a few parameters because I wrote the function actually to conveniently plot results from DE testing, but the basic functionality is the following: . _adata_ is the usual AnnData object you are working with. ; _name_list_ is a string containing gene names and should be specified. ; _n_genes_ cuts the name_list if the number specified is smaller then the length of the list, so set this high enough if you want to work with large data ; _annotation_key_ allows you to specify a string that works as the key in the AnnData object where results are stored. By default, the key is ""Correlation_matrix"". The method basically wraps the pd.DataFrame.corr method, which allows you to specify the correlation method ('pearson', 'spearman', 'kendall'). . I use it for smaller data so it has not been optimized for performance (yet), but I tested the method for 3k cells and 600 genes and ended up with a runtime of ~8 seconds. I hope that is conveniently fast enough for you (if not let us know). . After calling the tool, you can plot correlation matrices (using a wrapper for seaborn heatmap) by calling. `sc.pl.correlation_matrix(adata, annotation_key=None)`. This function searches basically only the AnnData annotation (again, if no key specified, ""Correlation_matrix"" is the default). Hope this does the job!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/72#issuecomment-361891662
https://github.com/scverse/scanpy/issues/72#issuecomment-361899845:17,Deployability,update,update,17,I will certainly update my new stuff today at least once (probably more often ) and change the name / add the documentation ; and then let you know as soon as the name has changed,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/72#issuecomment-361899845
https://github.com/scverse/scanpy/issues/72#issuecomment-362374369:54,Availability,avail,available,54,"That sounds right, yes. Looking forward to this being available.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/72#issuecomment-362374369
https://github.com/scverse/scanpy/issues/72#issuecomment-363210986:4,Deployability,update,updates,4,Any updates here? I'd love to add this to an analysis tool UI I'm working on (and presenting at a conference this weekend). Very happy to promote scanpy there.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/72#issuecomment-363210986
https://github.com/scverse/scanpy/issues/72#issuecomment-399897165:121,Usability,simpl,simple,121,"Unfortunately, all of this discussion here was not really further pursued, I have to admit. In principle, these are very simple things. However, I'm a bit afraid of offering a canonical function as I fear that there are also a lot of bad ways of visualizing gene correlation plots and I don't feel capable of judging this. If no one else wants to make a pull request for that (maybe using what @tcallies already did, but I fear it's not really serving the purpose of the discussion here: [here](https://github.com/theislab/scanpy/blob/8e06ff6ecfab892240b58d2206e461685216a926/scanpy/tools/top_genes.py), [here](https://github.com/theislab/scanpy/blob/8e06ff6ecfab892240b58d2206e461685216a926/scanpy/plotting/top_genes_visual.py)) it would be cool if someone sent me an example case, which clearly shows what you want. Maybe @jorvis, you can send images for the examples you have in mind?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/72#issuecomment-399897165
https://github.com/scverse/scanpy/issues/72#issuecomment-399897165:789,Usability,clear,clearly,789,"Unfortunately, all of this discussion here was not really further pursued, I have to admit. In principle, these are very simple things. However, I'm a bit afraid of offering a canonical function as I fear that there are also a lot of bad ways of visualizing gene correlation plots and I don't feel capable of judging this. If no one else wants to make a pull request for that (maybe using what @tcallies already did, but I fear it's not really serving the purpose of the discussion here: [here](https://github.com/theislab/scanpy/blob/8e06ff6ecfab892240b58d2206e461685216a926/scanpy/tools/top_genes.py), [here](https://github.com/theislab/scanpy/blob/8e06ff6ecfab892240b58d2206e461685216a926/scanpy/plotting/top_genes_visual.py)) it would be cool if someone sent me an example case, which clearly shows what you want. Maybe @jorvis, you can send images for the examples you have in mind?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/72#issuecomment-399897165
https://github.com/scverse/scanpy/issues/72#issuecomment-1322433329:5,Availability,error,error,5,"same error, seconded -- is there an alternative approach built in?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/72#issuecomment-1322433329
https://github.com/scverse/scanpy/issues/72#issuecomment-2271053765:4,Deployability,update,updates,4,Any updates on this? Has `correlation_matrix()` been removed?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/72#issuecomment-2271053765
https://github.com/scverse/scanpy/issues/73#issuecomment-361946553:128,Deployability,pipeline,pipeline,128,"Great info about the figdir, thank you. I think it would be good to be able to access/save both plots, yes. I want this sort of pipeline script to be able to generate the same output for inspection as if the user were running the commands within Jupyter, and both display there. I don't think it's a critical thing though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/73#issuecomment-361946553
https://github.com/scverse/scanpy/issues/73#issuecomment-361946553:79,Security,access,access,79,"Great info about the figdir, thank you. I think it would be good to be able to access/save both plots, yes. I want this sort of pipeline script to be able to generate the same output for inspection as if the user were running the commands within Jupyter, and both display there. I don't think it's a critical thing though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/73#issuecomment-361946553
https://github.com/scverse/scanpy/issues/74#issuecomment-363800572:222,Availability,error,errors,222,"Thanks! It works! I used your first suggestion. I'm mostly an R user, but in the past I worked on python. I haven't used it for ages and the first thing of R I'm really missing is the help. Here, I can google commands and errors for standard libraries but for example, in the case of new tools, I can just rely on few examples or tutorials.. Or it would be nice for example, also have a list of all the functions in scanpy, with explanation of inputs, outputs and explanation of them. Your documentation is really helpful and well-structured, but I feel a bit limited by that aspect. ; Cheers, ; Elisabetta",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/74#issuecomment-363800572
https://github.com/scverse/scanpy/issues/74#issuecomment-363820657:564,Deployability,release,release,564,"I suspect Elisabetta's concern was more about the general differences in documentation standards between Python packages and R packages (at least within genomics). It can definitely make the transition from R to Python more difficult (I can attest, as I'm going through it right now :P). R packages just tend to come with very thorough vignettes, documenting how the package is supposed to be used end-to-end, highlighting special use-cases, etc. I think [Seurat](http://satijalab.org/seurat/get_started.html), [Monocle](http://cole-trapnell-lab.github.io/monocle-release/), or really any Bioconductor package (eg. [Scater](https://bioconductor.org/packages/release/bioc/vignettes/scater/inst/doc/vignette-intro.html)) are great examples. Jupyter notebooks are fantastic for helping get an idea of how to use a package, but sometimes the documentation within them is lacking, making it hard to understand what's going on at each step without having to dive into source code. I find Scanpy's straight forward to follow though. I really appreciate the modular design of Scanpy. At least if you're familiar with single-cell analysis, you can recognize the steps quite easily and just string together the appropriate functions for your analysis pipeline. Some other packages are bit trickier. For example, I found Velocyto (which I love btw) incredibly hard to navigate. There are functions that are used in some analysis notebooks but not all of them, so it becomes unclear what the standard analysis pipeline with it should be. Obviously you can dive into the paper, understand the statistical guts of the method, go through all the source code and see what's there, but then it starts becoming a barrier to newer users adapting it. . Anyway, this is just a general thing I also noticed moving from R to Python. Not saying that it's something that necessarily needs changing, but it does create a little bit of friction to newer users, so it may be worth thinking about as a community. I suspect this wi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/74#issuecomment-363820657
https://github.com/scverse/scanpy/issues/74#issuecomment-363820657:658,Deployability,release,release,658,"I suspect Elisabetta's concern was more about the general differences in documentation standards between Python packages and R packages (at least within genomics). It can definitely make the transition from R to Python more difficult (I can attest, as I'm going through it right now :P). R packages just tend to come with very thorough vignettes, documenting how the package is supposed to be used end-to-end, highlighting special use-cases, etc. I think [Seurat](http://satijalab.org/seurat/get_started.html), [Monocle](http://cole-trapnell-lab.github.io/monocle-release/), or really any Bioconductor package (eg. [Scater](https://bioconductor.org/packages/release/bioc/vignettes/scater/inst/doc/vignette-intro.html)) are great examples. Jupyter notebooks are fantastic for helping get an idea of how to use a package, but sometimes the documentation within them is lacking, making it hard to understand what's going on at each step without having to dive into source code. I find Scanpy's straight forward to follow though. I really appreciate the modular design of Scanpy. At least if you're familiar with single-cell analysis, you can recognize the steps quite easily and just string together the appropriate functions for your analysis pipeline. Some other packages are bit trickier. For example, I found Velocyto (which I love btw) incredibly hard to navigate. There are functions that are used in some analysis notebooks but not all of them, so it becomes unclear what the standard analysis pipeline with it should be. Obviously you can dive into the paper, understand the statistical guts of the method, go through all the source code and see what's there, but then it starts becoming a barrier to newer users adapting it. . Anyway, this is just a general thing I also noticed moving from R to Python. Not saying that it's something that necessarily needs changing, but it does create a little bit of friction to newer users, so it may be worth thinking about as a community. I suspect this wi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/74#issuecomment-363820657
https://github.com/scverse/scanpy/issues/74#issuecomment-363820657:1241,Deployability,pipeline,pipeline,1241,". It can definitely make the transition from R to Python more difficult (I can attest, as I'm going through it right now :P). R packages just tend to come with very thorough vignettes, documenting how the package is supposed to be used end-to-end, highlighting special use-cases, etc. I think [Seurat](http://satijalab.org/seurat/get_started.html), [Monocle](http://cole-trapnell-lab.github.io/monocle-release/), or really any Bioconductor package (eg. [Scater](https://bioconductor.org/packages/release/bioc/vignettes/scater/inst/doc/vignette-intro.html)) are great examples. Jupyter notebooks are fantastic for helping get an idea of how to use a package, but sometimes the documentation within them is lacking, making it hard to understand what's going on at each step without having to dive into source code. I find Scanpy's straight forward to follow though. I really appreciate the modular design of Scanpy. At least if you're familiar with single-cell analysis, you can recognize the steps quite easily and just string together the appropriate functions for your analysis pipeline. Some other packages are bit trickier. For example, I found Velocyto (which I love btw) incredibly hard to navigate. There are functions that are used in some analysis notebooks but not all of them, so it becomes unclear what the standard analysis pipeline with it should be. Obviously you can dive into the paper, understand the statistical guts of the method, go through all the source code and see what's there, but then it starts becoming a barrier to newer users adapting it. . Anyway, this is just a general thing I also noticed moving from R to Python. Not saying that it's something that necessarily needs changing, but it does create a little bit of friction to newer users, so it may be worth thinking about as a community. I suspect this will improve as the genomics user-base of Python increases, and as all these packages have more time to develop. Also, I understand that this is more of a ""communit",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/74#issuecomment-363820657
https://github.com/scverse/scanpy/issues/74#issuecomment-363820657:1498,Deployability,pipeline,pipeline,1498,"I'm going through it right now :P). R packages just tend to come with very thorough vignettes, documenting how the package is supposed to be used end-to-end, highlighting special use-cases, etc. I think [Seurat](http://satijalab.org/seurat/get_started.html), [Monocle](http://cole-trapnell-lab.github.io/monocle-release/), or really any Bioconductor package (eg. [Scater](https://bioconductor.org/packages/release/bioc/vignettes/scater/inst/doc/vignette-intro.html)) are great examples. Jupyter notebooks are fantastic for helping get an idea of how to use a package, but sometimes the documentation within them is lacking, making it hard to understand what's going on at each step without having to dive into source code. I find Scanpy's straight forward to follow though. I really appreciate the modular design of Scanpy. At least if you're familiar with single-cell analysis, you can recognize the steps quite easily and just string together the appropriate functions for your analysis pipeline. Some other packages are bit trickier. For example, I found Velocyto (which I love btw) incredibly hard to navigate. There are functions that are used in some analysis notebooks but not all of them, so it becomes unclear what the standard analysis pipeline with it should be. Obviously you can dive into the paper, understand the statistical guts of the method, go through all the source code and see what's there, but then it starts becoming a barrier to newer users adapting it. . Anyway, this is just a general thing I also noticed moving from R to Python. Not saying that it's something that necessarily needs changing, but it does create a little bit of friction to newer users, so it may be worth thinking about as a community. I suspect this will improve as the genomics user-base of Python increases, and as all these packages have more time to develop. Also, I understand that this is more of a ""community"" chat and may not belong in the scanpy/issues page anymore, so feel free to close it ;)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/74#issuecomment-363820657
https://github.com/scverse/scanpy/issues/74#issuecomment-363820657:1718,Energy Efficiency,adapt,adapting,1718,"I'm going through it right now :P). R packages just tend to come with very thorough vignettes, documenting how the package is supposed to be used end-to-end, highlighting special use-cases, etc. I think [Seurat](http://satijalab.org/seurat/get_started.html), [Monocle](http://cole-trapnell-lab.github.io/monocle-release/), or really any Bioconductor package (eg. [Scater](https://bioconductor.org/packages/release/bioc/vignettes/scater/inst/doc/vignette-intro.html)) are great examples. Jupyter notebooks are fantastic for helping get an idea of how to use a package, but sometimes the documentation within them is lacking, making it hard to understand what's going on at each step without having to dive into source code. I find Scanpy's straight forward to follow though. I really appreciate the modular design of Scanpy. At least if you're familiar with single-cell analysis, you can recognize the steps quite easily and just string together the appropriate functions for your analysis pipeline. Some other packages are bit trickier. For example, I found Velocyto (which I love btw) incredibly hard to navigate. There are functions that are used in some analysis notebooks but not all of them, so it becomes unclear what the standard analysis pipeline with it should be. Obviously you can dive into the paper, understand the statistical guts of the method, go through all the source code and see what's there, but then it starts becoming a barrier to newer users adapting it. . Anyway, this is just a general thing I also noticed moving from R to Python. Not saying that it's something that necessarily needs changing, but it does create a little bit of friction to newer users, so it may be worth thinking about as a community. I suspect this will improve as the genomics user-base of Python increases, and as all these packages have more time to develop. Also, I understand that this is more of a ""community"" chat and may not belong in the scanpy/issues page anymore, so feel free to close it ;)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/74#issuecomment-363820657
https://github.com/scverse/scanpy/issues/74#issuecomment-363820657:1718,Modifiability,adapt,adapting,1718,"I'm going through it right now :P). R packages just tend to come with very thorough vignettes, documenting how the package is supposed to be used end-to-end, highlighting special use-cases, etc. I think [Seurat](http://satijalab.org/seurat/get_started.html), [Monocle](http://cole-trapnell-lab.github.io/monocle-release/), or really any Bioconductor package (eg. [Scater](https://bioconductor.org/packages/release/bioc/vignettes/scater/inst/doc/vignette-intro.html)) are great examples. Jupyter notebooks are fantastic for helping get an idea of how to use a package, but sometimes the documentation within them is lacking, making it hard to understand what's going on at each step without having to dive into source code. I find Scanpy's straight forward to follow though. I really appreciate the modular design of Scanpy. At least if you're familiar with single-cell analysis, you can recognize the steps quite easily and just string together the appropriate functions for your analysis pipeline. Some other packages are bit trickier. For example, I found Velocyto (which I love btw) incredibly hard to navigate. There are functions that are used in some analysis notebooks but not all of them, so it becomes unclear what the standard analysis pipeline with it should be. Obviously you can dive into the paper, understand the statistical guts of the method, go through all the source code and see what's there, but then it starts becoming a barrier to newer users adapting it. . Anyway, this is just a general thing I also noticed moving from R to Python. Not saying that it's something that necessarily needs changing, but it does create a little bit of friction to newer users, so it may be worth thinking about as a community. I suspect this will improve as the genomics user-base of Python increases, and as all these packages have more time to develop. Also, I understand that this is more of a ""community"" chat and may not belong in the scanpy/issues page anymore, so feel free to close it ;)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/74#issuecomment-363820657
https://github.com/scverse/scanpy/issues/74#issuecomment-364055498:756,Availability,down,download,756,"Thank you for these thoughts!. I guess the high documentation quality in R stems from the Bioconductor project, which really set some standards. Nothing like this exists in Python - everyone just does what he or she wants. There are few people thinking about setting up something similar to Bioconductor for Python - but this will likely take some time... With Scanpy, we try to provide documentation at the Standards of the big packages: numpy, scipy, statsmodels, seaborn, scikit-learn, h5py, pytables, etc. There are many more and all of them have great docs. I think, with Scanpy, one can still do a lot better. Tuturials tend to be too short. Also, there should be a properly rendered html output of the notebooks - with a button where you can simply download it and then run it yourself to start playing around with it. Hope we will have this in a couple of weeks. And yes, other packages maybe just need to take time. But I'd guess that this will get much better soon...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/74#issuecomment-364055498
https://github.com/scverse/scanpy/issues/74#issuecomment-364055498:482,Usability,learn,learn,482,"Thank you for these thoughts!. I guess the high documentation quality in R stems from the Bioconductor project, which really set some standards. Nothing like this exists in Python - everyone just does what he or she wants. There are few people thinking about setting up something similar to Bioconductor for Python - but this will likely take some time... With Scanpy, we try to provide documentation at the Standards of the big packages: numpy, scipy, statsmodels, seaborn, scikit-learn, h5py, pytables, etc. There are many more and all of them have great docs. I think, with Scanpy, one can still do a lot better. Tuturials tend to be too short. Also, there should be a properly rendered html output of the notebooks - with a button where you can simply download it and then run it yourself to start playing around with it. Hope we will have this in a couple of weeks. And yes, other packages maybe just need to take time. But I'd guess that this will get much better soon...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/74#issuecomment-364055498
https://github.com/scverse/scanpy/issues/74#issuecomment-364055498:749,Usability,simpl,simply,749,"Thank you for these thoughts!. I guess the high documentation quality in R stems from the Bioconductor project, which really set some standards. Nothing like this exists in Python - everyone just does what he or she wants. There are few people thinking about setting up something similar to Bioconductor for Python - but this will likely take some time... With Scanpy, we try to provide documentation at the Standards of the big packages: numpy, scipy, statsmodels, seaborn, scikit-learn, h5py, pytables, etc. There are many more and all of them have great docs. I think, with Scanpy, one can still do a lot better. Tuturials tend to be too short. Also, there should be a properly rendered html output of the notebooks - with a button where you can simply download it and then run it yourself to start playing around with it. Hope we will have this in a couple of weeks. And yes, other packages maybe just need to take time. But I'd guess that this will get much better soon...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/74#issuecomment-364055498
https://github.com/scverse/scanpy/issues/74#issuecomment-364060125:675,Deployability,Install,Installation,675,"OK, so we want to build and upload the notebooks. With a bit of convincing, we can get readthedocs to do that for us. But (for a good reason) `scanpy_usage` is a different repo. This means:. 1. Changing something there should trigger a rebuild, not changing the scanpy repo; 2. We probably need to put them on https://scanpy_usage.readthedocs.io; 3. We can convince sphinx to create an index entry for the docs in the navigation sidebar that leads to the scanpy_usage site. Then it would look like this:. https://scanpy.readthedocs.io:. - Examples → Converted to a fake index entry that is a link to https://scanpy_usage.readthedocs.io; - Basic Usage → …/basic_usage.html; - Installation → …/installation.html; - API → …/api/index.html; - References → …/references.html. https://scanpy_usage.readthedocs.io:. - Examples → /index.html; - Basic Usage → Fake index extry to https://scanpy.readthedocs.io/…/basic_usage.html; - Installation → Fake index extry to https://scanpy.readthedocs.io/…/installation.html; - API → Fake index extry to https://scanpy.readthedocs.io/…/api/index.html; - References → Fake index extry to https://scanpy.readthedocs.io/…/references.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/74#issuecomment-364060125
https://github.com/scverse/scanpy/issues/74#issuecomment-364060125:692,Deployability,install,installation,692,"OK, so we want to build and upload the notebooks. With a bit of convincing, we can get readthedocs to do that for us. But (for a good reason) `scanpy_usage` is a different repo. This means:. 1. Changing something there should trigger a rebuild, not changing the scanpy repo; 2. We probably need to put them on https://scanpy_usage.readthedocs.io; 3. We can convince sphinx to create an index entry for the docs in the navigation sidebar that leads to the scanpy_usage site. Then it would look like this:. https://scanpy.readthedocs.io:. - Examples → Converted to a fake index entry that is a link to https://scanpy_usage.readthedocs.io; - Basic Usage → …/basic_usage.html; - Installation → …/installation.html; - API → …/api/index.html; - References → …/references.html. https://scanpy_usage.readthedocs.io:. - Examples → /index.html; - Basic Usage → Fake index extry to https://scanpy.readthedocs.io/…/basic_usage.html; - Installation → Fake index extry to https://scanpy.readthedocs.io/…/installation.html; - API → Fake index extry to https://scanpy.readthedocs.io/…/api/index.html; - References → Fake index extry to https://scanpy.readthedocs.io/…/references.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/74#issuecomment-364060125
https://github.com/scverse/scanpy/issues/74#issuecomment-364060125:923,Deployability,Install,Installation,923,"OK, so we want to build and upload the notebooks. With a bit of convincing, we can get readthedocs to do that for us. But (for a good reason) `scanpy_usage` is a different repo. This means:. 1. Changing something there should trigger a rebuild, not changing the scanpy repo; 2. We probably need to put them on https://scanpy_usage.readthedocs.io; 3. We can convince sphinx to create an index entry for the docs in the navigation sidebar that leads to the scanpy_usage site. Then it would look like this:. https://scanpy.readthedocs.io:. - Examples → Converted to a fake index entry that is a link to https://scanpy_usage.readthedocs.io; - Basic Usage → …/basic_usage.html; - Installation → …/installation.html; - API → …/api/index.html; - References → …/references.html. https://scanpy_usage.readthedocs.io:. - Examples → /index.html; - Basic Usage → Fake index extry to https://scanpy.readthedocs.io/…/basic_usage.html; - Installation → Fake index extry to https://scanpy.readthedocs.io/…/installation.html; - API → Fake index extry to https://scanpy.readthedocs.io/…/api/index.html; - References → Fake index extry to https://scanpy.readthedocs.io/…/references.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/74#issuecomment-364060125
https://github.com/scverse/scanpy/issues/74#issuecomment-364060125:990,Deployability,install,installation,990,"OK, so we want to build and upload the notebooks. With a bit of convincing, we can get readthedocs to do that for us. But (for a good reason) `scanpy_usage` is a different repo. This means:. 1. Changing something there should trigger a rebuild, not changing the scanpy repo; 2. We probably need to put them on https://scanpy_usage.readthedocs.io; 3. We can convince sphinx to create an index entry for the docs in the navigation sidebar that leads to the scanpy_usage site. Then it would look like this:. https://scanpy.readthedocs.io:. - Examples → Converted to a fake index entry that is a link to https://scanpy_usage.readthedocs.io; - Basic Usage → …/basic_usage.html; - Installation → …/installation.html; - API → …/api/index.html; - References → …/references.html. https://scanpy_usage.readthedocs.io:. - Examples → /index.html; - Basic Usage → Fake index extry to https://scanpy.readthedocs.io/…/basic_usage.html; - Installation → Fake index extry to https://scanpy.readthedocs.io/…/installation.html; - API → Fake index extry to https://scanpy.readthedocs.io/…/api/index.html; - References → Fake index extry to https://scanpy.readthedocs.io/…/references.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/74#issuecomment-364060125
https://github.com/scverse/scanpy/issues/74#issuecomment-364063478:99,Availability,down,download,99,"Yes, one could think about doing it that way. I had in mind slowly transitioning to notebooks that download data and run through automatically. One can build docs with them https://nbsphinx.readthedocs.io and possibly use them for testing. In these notebooks, there won't be any images... so it would be fine to add them to the scanpy repo. It's essentially the same thing as in the numpy etc. tutorials... only that not writing this in .rst but in notebook form gives the user the neat feature of being able to download an executable notebook. For now, everything is built via https://nbviewer.jupyter.org/. Maybe you haven't yet realized the new layout of https://scanpy.readthedocs.io/en/latest/examples.html... But this is still too manual... No hurry with these things, though...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/74#issuecomment-364063478
https://github.com/scverse/scanpy/issues/74#issuecomment-364063478:512,Availability,down,download,512,"Yes, one could think about doing it that way. I had in mind slowly transitioning to notebooks that download data and run through automatically. One can build docs with them https://nbsphinx.readthedocs.io and possibly use them for testing. In these notebooks, there won't be any images... so it would be fine to add them to the scanpy repo. It's essentially the same thing as in the numpy etc. tutorials... only that not writing this in .rst but in notebook form gives the user the neat feature of being able to download an executable notebook. For now, everything is built via https://nbviewer.jupyter.org/. Maybe you haven't yet realized the new layout of https://scanpy.readthedocs.io/en/latest/examples.html... But this is still too manual... No hurry with these things, though...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/74#issuecomment-364063478
https://github.com/scverse/scanpy/issues/74#issuecomment-364063478:231,Testability,test,testing,231,"Yes, one could think about doing it that way. I had in mind slowly transitioning to notebooks that download data and run through automatically. One can build docs with them https://nbsphinx.readthedocs.io and possibly use them for testing. In these notebooks, there won't be any images... so it would be fine to add them to the scanpy repo. It's essentially the same thing as in the numpy etc. tutorials... only that not writing this in .rst but in notebook form gives the user the neat feature of being able to download an executable notebook. For now, everything is built via https://nbviewer.jupyter.org/. Maybe you haven't yet realized the new layout of https://scanpy.readthedocs.io/en/latest/examples.html... But this is still too manual... No hurry with these things, though...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/74#issuecomment-364063478
https://github.com/scverse/scanpy/pull/76#issuecomment-363587569:838,Testability,test,test,838,"Hi Davide!. Thank you very much for this! Sorry that I tend to be late these days, have two 6 week old baby twins to take care of... I'm happy to merge this and I'll add you to the author list! I hope it is ok if I rename the module and the top-level function to `score_gene_lists` and the second top-level function to `score_cell_cylce_genes`? Simply `score` is a bit generic... there might be many other scores in the future and then people will get confused. It's also good if both start with `score` so that auto-lookup gives you directly these suggestions? . Also, do you have a notebook with an example? It would be cool to see this at work. You could push this to a new subdirectory in `scanpy_usage`: https://github.com/theislab/scanpy_usage. I just sent you a collaborator invitation. From the example, we can then mayb design a test that goes a bit more into detail. Would be cool to benchmark with Seurat, for example. Also, one could think about providing a default list of genes, right? In particular for the cell cycle, it would be nice to directly call the function with default parameters - one can then still add user-specified lists. Do you want to provide such a list? I also sent you collaborator invitation for scanpy - maybe only temporarily if we get too many people at some point - so that you can quickly add this, if you like. Cheers,; Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/76#issuecomment-363587569
https://github.com/scverse/scanpy/pull/76#issuecomment-363587569:894,Testability,benchmark,benchmark,894,"Hi Davide!. Thank you very much for this! Sorry that I tend to be late these days, have two 6 week old baby twins to take care of... I'm happy to merge this and I'll add you to the author list! I hope it is ok if I rename the module and the top-level function to `score_gene_lists` and the second top-level function to `score_cell_cylce_genes`? Simply `score` is a bit generic... there might be many other scores in the future and then people will get confused. It's also good if both start with `score` so that auto-lookup gives you directly these suggestions? . Also, do you have a notebook with an example? It would be cool to see this at work. You could push this to a new subdirectory in `scanpy_usage`: https://github.com/theislab/scanpy_usage. I just sent you a collaborator invitation. From the example, we can then mayb design a test that goes a bit more into detail. Would be cool to benchmark with Seurat, for example. Also, one could think about providing a default list of genes, right? In particular for the cell cycle, it would be nice to directly call the function with default parameters - one can then still add user-specified lists. Do you want to provide such a list? I also sent you collaborator invitation for scanpy - maybe only temporarily if we get too many people at some point - so that you can quickly add this, if you like. Cheers,; Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/76#issuecomment-363587569
https://github.com/scverse/scanpy/pull/76#issuecomment-363587569:345,Usability,Simpl,Simply,345,"Hi Davide!. Thank you very much for this! Sorry that I tend to be late these days, have two 6 week old baby twins to take care of... I'm happy to merge this and I'll add you to the author list! I hope it is ok if I rename the module and the top-level function to `score_gene_lists` and the second top-level function to `score_cell_cylce_genes`? Simply `score` is a bit generic... there might be many other scores in the future and then people will get confused. It's also good if both start with `score` so that auto-lookup gives you directly these suggestions? . Also, do you have a notebook with an example? It would be cool to see this at work. You could push this to a new subdirectory in `scanpy_usage`: https://github.com/theislab/scanpy_usage. I just sent you a collaborator invitation. From the example, we can then mayb design a test that goes a bit more into detail. Would be cool to benchmark with Seurat, for example. Also, one could think about providing a default list of genes, right? In particular for the cell cycle, it would be nice to directly call the function with default parameters - one can then still add user-specified lists. Do you want to provide such a list? I also sent you collaborator invitation for scanpy - maybe only temporarily if we get too many people at some point - so that you can quickly add this, if you like. Cheers,; Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/76#issuecomment-363587569
https://github.com/scverse/scanpy/pull/76#issuecomment-363733820:662,Testability,benchmark,benchmarking,662,"@dawe ; - the coding style is the [official python coding style](https://www.python.org/dev/peps/pep-0008/), we should all stick to that. in particular: [white spaces](https://www.python.org/dev/peps/pep-0008/#whitespace-in-expressions-and-statements) around operators but **not** around optional keyword arguments; - thank you for a notebook!; - gene list: why not add it as an attribute of your module? or make a class `GeneLists` with a few gene lists in your model? of course, these will not be comprehensive, but might provide a good starting point; - in a few instances, I had to go through the reverse R engineering myself - in particular, if it comes to benchmarking code to floating point precision, it's really a hassle to dig out all the hidden different conventions... but I think it really pays off in the sense that it provides a lot more confidence in code and methods if several tools provide the same result on basic things - even across languages",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/76#issuecomment-363733820
https://github.com/scverse/scanpy/issues/77#issuecomment-363822066:123,Usability,simpl,simple,123,"There's my initial attempt, but it fails with:. ```pytb; File ""/usr/local/lib/python3.6/dist-packages/scanpy/preprocessing/simple.py"", line 169, in filter_genes; adata.var['n_cells'] = number; File ""/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py"", line 2519, in __setitem__; self._set_item(key, value); File ""/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py"", line 2585, in _set_item; value = self._sanitize_column(key, value); File ""/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py"", line 2760, in _sanitize_column; value = _sanitize_index(value, self.index, copy=False); File ""/usr/local/lib/python3.6/dist-packages/pandas/core/series.py"", line 3121, in _sanitize_index; raise ValueError('Length of values does not match length of ' 'index'); ValueError: Length of values does not match length of index; ```. If it isn't obvious to you what's wrong I'll return to it after my conference this weekend. Every hour is critical right now. :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/77#issuecomment-363822066
https://github.com/scverse/scanpy/issues/1#issuecomment-277506990:90,Energy Efficiency,efficient,efficient,90,"Good! So, I'd really like to jump in and work on ann_matrix as well, if you think this is efficient. Of course, I don't want to mess up what you had in mind.; 1. yes, that's important - can i help?; 2. that's easy, simply put it in smp as a multicolumn object; 3. should be very easy as well, maybe recarray can directly be written with a single key, if not, one has to make the separation between str and float columns -> shall I attack that? see [this](https://github.com/theislab/scanpy/commit/ac79f8991953bf7f4ae33f243b384560c131a8f9#L650-L669) for how it was done with the ddata using its 'rowcat' attribute. should be straightforwardly adapted, right?*; ---; *sorry, I simply forgot to add readwrite.py on thursday night, which caused master to be non-working since then, of course. with readwrite.py added, master now works just fine. I guess the only change you made to utils.py was adding the AnnData.from_dict(...) in the function read()? so one could use readwrite.py from master within ann_matrix. or just create readwrite.py again by cutting out everything related to reading/writing from utils and pasting it into the new module readwrite.py.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1#issuecomment-277506990
https://github.com/scverse/scanpy/issues/1#issuecomment-277506990:642,Energy Efficiency,adapt,adapted,642,"Good! So, I'd really like to jump in and work on ann_matrix as well, if you think this is efficient. Of course, I don't want to mess up what you had in mind.; 1. yes, that's important - can i help?; 2. that's easy, simply put it in smp as a multicolumn object; 3. should be very easy as well, maybe recarray can directly be written with a single key, if not, one has to make the separation between str and float columns -> shall I attack that? see [this](https://github.com/theislab/scanpy/commit/ac79f8991953bf7f4ae33f243b384560c131a8f9#L650-L669) for how it was done with the ddata using its 'rowcat' attribute. should be straightforwardly adapted, right?*; ---; *sorry, I simply forgot to add readwrite.py on thursday night, which caused master to be non-working since then, of course. with readwrite.py added, master now works just fine. I guess the only change you made to utils.py was adding the AnnData.from_dict(...) in the function read()? so one could use readwrite.py from master within ann_matrix. or just create readwrite.py again by cutting out everything related to reading/writing from utils and pasting it into the new module readwrite.py.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1#issuecomment-277506990
https://github.com/scverse/scanpy/issues/1#issuecomment-277506990:642,Modifiability,adapt,adapted,642,"Good! So, I'd really like to jump in and work on ann_matrix as well, if you think this is efficient. Of course, I don't want to mess up what you had in mind.; 1. yes, that's important - can i help?; 2. that's easy, simply put it in smp as a multicolumn object; 3. should be very easy as well, maybe recarray can directly be written with a single key, if not, one has to make the separation between str and float columns -> shall I attack that? see [this](https://github.com/theislab/scanpy/commit/ac79f8991953bf7f4ae33f243b384560c131a8f9#L650-L669) for how it was done with the ddata using its 'rowcat' attribute. should be straightforwardly adapted, right?*; ---; *sorry, I simply forgot to add readwrite.py on thursday night, which caused master to be non-working since then, of course. with readwrite.py added, master now works just fine. I guess the only change you made to utils.py was adding the AnnData.from_dict(...) in the function read()? so one could use readwrite.py from master within ann_matrix. or just create readwrite.py again by cutting out everything related to reading/writing from utils and pasting it into the new module readwrite.py.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1#issuecomment-277506990
https://github.com/scverse/scanpy/issues/1#issuecomment-277506990:431,Security,attack,attack,431,"Good! So, I'd really like to jump in and work on ann_matrix as well, if you think this is efficient. Of course, I don't want to mess up what you had in mind.; 1. yes, that's important - can i help?; 2. that's easy, simply put it in smp as a multicolumn object; 3. should be very easy as well, maybe recarray can directly be written with a single key, if not, one has to make the separation between str and float columns -> shall I attack that? see [this](https://github.com/theislab/scanpy/commit/ac79f8991953bf7f4ae33f243b384560c131a8f9#L650-L669) for how it was done with the ddata using its 'rowcat' attribute. should be straightforwardly adapted, right?*; ---; *sorry, I simply forgot to add readwrite.py on thursday night, which caused master to be non-working since then, of course. with readwrite.py added, master now works just fine. I guess the only change you made to utils.py was adding the AnnData.from_dict(...) in the function read()? so one could use readwrite.py from master within ann_matrix. or just create readwrite.py again by cutting out everything related to reading/writing from utils and pasting it into the new module readwrite.py.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1#issuecomment-277506990
https://github.com/scverse/scanpy/issues/1#issuecomment-277506990:215,Usability,simpl,simply,215,"Good! So, I'd really like to jump in and work on ann_matrix as well, if you think this is efficient. Of course, I don't want to mess up what you had in mind.; 1. yes, that's important - can i help?; 2. that's easy, simply put it in smp as a multicolumn object; 3. should be very easy as well, maybe recarray can directly be written with a single key, if not, one has to make the separation between str and float columns -> shall I attack that? see [this](https://github.com/theislab/scanpy/commit/ac79f8991953bf7f4ae33f243b384560c131a8f9#L650-L669) for how it was done with the ddata using its 'rowcat' attribute. should be straightforwardly adapted, right?*; ---; *sorry, I simply forgot to add readwrite.py on thursday night, which caused master to be non-working since then, of course. with readwrite.py added, master now works just fine. I guess the only change you made to utils.py was adding the AnnData.from_dict(...) in the function read()? so one could use readwrite.py from master within ann_matrix. or just create readwrite.py again by cutting out everything related to reading/writing from utils and pasting it into the new module readwrite.py.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1#issuecomment-277506990
https://github.com/scverse/scanpy/issues/1#issuecomment-277506990:675,Usability,simpl,simply,675,"Good! So, I'd really like to jump in and work on ann_matrix as well, if you think this is efficient. Of course, I don't want to mess up what you had in mind.; 1. yes, that's important - can i help?; 2. that's easy, simply put it in smp as a multicolumn object; 3. should be very easy as well, maybe recarray can directly be written with a single key, if not, one has to make the separation between str and float columns -> shall I attack that? see [this](https://github.com/theislab/scanpy/commit/ac79f8991953bf7f4ae33f243b384560c131a8f9#L650-L669) for how it was done with the ddata using its 'rowcat' attribute. should be straightforwardly adapted, right?*; ---; *sorry, I simply forgot to add readwrite.py on thursday night, which caused master to be non-working since then, of course. with readwrite.py added, master now works just fine. I guess the only change you made to utils.py was adding the AnnData.from_dict(...) in the function read()? so one could use readwrite.py from master within ann_matrix. or just create readwrite.py again by cutting out everything related to reading/writing from utils and pasting it into the new module readwrite.py.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1#issuecomment-277506990
https://github.com/scverse/scanpy/pull/2#issuecomment-278282236:216,Deployability,continuous,continuous,216,"no, not great, but it work's and one should now be much better settled for the future with AnnData. for example, the gene plots and different subgroups work. if you have a good suggestion for a default color map for continuous and categorial columns in smp, I'm very happy to adapt it. :). https://github.com/falexwolf/collab_alex/blob/master/scanpy/examples/maehr17.md. or here directly in the main readme. https://github.com/theislab/scanpy#moignard15",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2#issuecomment-278282236
https://github.com/scverse/scanpy/pull/2#issuecomment-278282236:276,Energy Efficiency,adapt,adapt,276,"no, not great, but it work's and one should now be much better settled for the future with AnnData. for example, the gene plots and different subgroups work. if you have a good suggestion for a default color map for continuous and categorial columns in smp, I'm very happy to adapt it. :). https://github.com/falexwolf/collab_alex/blob/master/scanpy/examples/maehr17.md. or here directly in the main readme. https://github.com/theislab/scanpy#moignard15",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2#issuecomment-278282236
https://github.com/scverse/scanpy/pull/2#issuecomment-278282236:276,Modifiability,adapt,adapt,276,"no, not great, but it work's and one should now be much better settled for the future with AnnData. for example, the gene plots and different subgroups work. if you have a good suggestion for a default color map for continuous and categorial columns in smp, I'm very happy to adapt it. :). https://github.com/falexwolf/collab_alex/blob/master/scanpy/examples/maehr17.md. or here directly in the main readme. https://github.com/theislab/scanpy#moignard15",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2#issuecomment-278282236
https://github.com/scverse/scanpy/issues/3#issuecomment-278339544:223,Deployability,continuous,continuous,223,"haha with ease. you can observe the inhomogeneous contrast distribution with C2 and C10 there: the colors are indistinguishable dark blue while C7 and C8 go from snot green all the way to orange. that would be horrible for continuous data, but merely makes C2 and C10 indistinguishable for categorical colors and unnecessarily reduces contrast there (as it’s a color map and no palette).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3#issuecomment-278339544
https://github.com/scverse/scanpy/issues/3#issuecomment-278339544:167,Energy Efficiency,green,green,167,"haha with ease. you can observe the inhomogeneous contrast distribution with C2 and C10 there: the colors are indistinguishable dark blue while C7 and C8 go from snot green all the way to orange. that would be horrible for continuous data, but merely makes C2 and C10 indistinguishable for categorical colors and unnecessarily reduces contrast there (as it’s a color map and no palette).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3#issuecomment-278339544
https://github.com/scverse/scanpy/issues/3#issuecomment-278339544:327,Energy Efficiency,reduce,reduces,327,"haha with ease. you can observe the inhomogeneous contrast distribution with C2 and C10 there: the colors are indistinguishable dark blue while C7 and C8 go from snot green all the way to orange. that would be horrible for continuous data, but merely makes C2 and C10 indistinguishable for categorical colors and unnecessarily reduces contrast there (as it’s a color map and no palette).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3#issuecomment-278339544
https://github.com/scverse/scanpy/issues/3#issuecomment-278376182:45,Deployability,integrat,integrating,45,"looks good, let's take one these. but before integrating it, the scatter plot of dpt should invoke `plotting.plot_tool` as well. then it's going to be just a one line change.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3#issuecomment-278376182
https://github.com/scverse/scanpy/issues/3#issuecomment-278376182:45,Integrability,integrat,integrating,45,"looks good, let's take one these. but before integrating it, the scatter plot of dpt should invoke `plotting.plot_tool` as well. then it's going to be just a one line change.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3#issuecomment-278376182
https://github.com/scverse/scanpy/issues/4#issuecomment-278579015:417,Deployability,update,update,417,"i guess we could go this route:. ```py; mean_filter = 0.01; cv_filter = 2; nr_pcs = 50. # row normalize ; adata = adata.smp_norm(max_fraction=0.05, mult_with_mean=True); # filter out genes with mean expression < 0.1 and coefficient of variance < ; # cvFilter ; adata = adata.filter_var_cv(mean_filter, cv_filter); # compute zscore of filtered matrix ; Xz = zscore(adata.X); # PCA ; Xpca = pca(Xz, nr_comps=nr_pcs); # update dictionary; adata['Xpca'] = Xpca; sett.m(0, 'Xpca has shape', Xpca.shape[0], 'x', Xpca.shape[1]); print(adata.X); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/4#issuecomment-278579015
https://github.com/scverse/scanpy/issues/4#issuecomment-278579015:25,Integrability,rout,route,25,"i guess we could go this route:. ```py; mean_filter = 0.01; cv_filter = 2; nr_pcs = 50. # row normalize ; adata = adata.smp_norm(max_fraction=0.05, mult_with_mean=True); # filter out genes with mean expression < 0.1 and coefficient of variance < ; # cvFilter ; adata = adata.filter_var_cv(mean_filter, cv_filter); # compute zscore of filtered matrix ; Xz = zscore(adata.X); # PCA ; Xpca = pca(Xz, nr_comps=nr_pcs); # update dictionary; adata['Xpca'] = Xpca; sett.m(0, 'Xpca has shape', Xpca.shape[0], 'x', Xpca.shape[1]); print(adata.X); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/4#issuecomment-278579015
https://github.com/scverse/scanpy/issues/4#issuecomment-278581479:461,Availability,error,errors,461,"Thanks for the suggestion! If putting it inside the class, I'd rather go for a subclass of AnnData. But I'd prefer to have the sc.pp namespace for all preprocessing methods. I expect that a lot of different methods could still come. If you always have to wonder whether this might be something that is already in AnnData or just in the sc.pp or applies to a data matrix X, it's hard to keep track. If everything just applies to X, it's easy. You still can make errors, like I did above, something like `adata.var = adata.var[gene_filter]` should work, right, whereas the `adata.var_names = adata.var_names[gene_filter]` from above will not work and should throw a sensible error... I'll again have a look. Please, for now, don't do anything. :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/4#issuecomment-278581479
https://github.com/scverse/scanpy/issues/4#issuecomment-278581479:673,Availability,error,error,673,"Thanks for the suggestion! If putting it inside the class, I'd rather go for a subclass of AnnData. But I'd prefer to have the sc.pp namespace for all preprocessing methods. I expect that a lot of different methods could still come. If you always have to wonder whether this might be something that is already in AnnData or just in the sc.pp or applies to a data matrix X, it's hard to keep track. If everything just applies to X, it's easy. You still can make errors, like I did above, something like `adata.var = adata.var[gene_filter]` should work, right, whereas the `adata.var_names = adata.var_names[gene_filter]` from above will not work and should throw a sensible error... I'll again have a look. Please, for now, don't do anything. :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/4#issuecomment-278581479
https://github.com/scverse/scanpy/issues/7#issuecomment-281458534:374,Deployability,update,updated,374,"Hi Pawel, sorry for the confusion, yes, we just did a major revision. The package is still in the testing phase even though everything should work fine. Any comments from your side would be greatly appreciated!. Packaging will start soon. Development will happen on a development branch from now on. The notebooks are currently being migrated to another repo, links will be updated tomorrow or day after tomorrow.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/7#issuecomment-281458534
https://github.com/scverse/scanpy/issues/7#issuecomment-281458534:98,Testability,test,testing,98,"Hi Pawel, sorry for the confusion, yes, we just did a major revision. The package is still in the testing phase even though everything should work fine. Any comments from your side would be greatly appreciated!. Packaging will start soon. Development will happen on a development branch from now on. The notebooks are currently being migrated to another repo, links will be updated tomorrow or day after tomorrow.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/7#issuecomment-281458534
https://github.com/scverse/scanpy/issues/7#issuecomment-284341109:41,Testability,test,test,41,"Thanks! Hopefully I'll find some time to test it out this week, I'll get back to you once I do that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/7#issuecomment-284341109
https://github.com/scverse/scanpy/issues/7#issuecomment-284343715:1043,Availability,error,error,1043,"So I just tried to install the package from the master branch by running. ```; pip install git+https://github.com/theislab/scanpy.git; ```; (by the way, you can update the Installation section of the README.md because the above line is equivalent to cloning the repository and running pip install on that.) The installation failed with; ```; Collecting git+https://github.com/theislab/scanpy.git; Cloning https://github.com/theislab/scanpy.git to /tmp/pip-203inirx-build; Complete output from command python setup.py egg_info:; running egg_info; creating pip-egg-info/scanpy.egg-info; writing pip-egg-info/scanpy.egg-info/PKG-INFO; writing dependency_links to pip-egg-info/scanpy.egg-info/dependency_links.txt; writing entry points to pip-egg-info/scanpy.egg-info/entry_points.txt; writing requirements to pip-egg-info/scanpy.egg-info/requires.txt; writing top-level names to pip-egg-info/scanpy.egg-info/top_level.txt; writing manifest file 'pip-egg-info/scanpy.egg-info/SOURCES.txt'; warning: manifest_maker: standard file '-c' not found; ; error: package directory 'scanpy/exs' does not exist; ; ----------------------------------------; Command ""python setup.py egg_info"" failed with error code 1 in /tmp/pip-203inirx-build/; The command '/bin/sh -c pip install --upgrade --no-cache-dir git+https://github.com/theislab/scanpy.git' returned a non-zero code: 1; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/7#issuecomment-284343715
https://github.com/scverse/scanpy/issues/7#issuecomment-284343715:1188,Availability,error,error,1188,"So I just tried to install the package from the master branch by running. ```; pip install git+https://github.com/theislab/scanpy.git; ```; (by the way, you can update the Installation section of the README.md because the above line is equivalent to cloning the repository and running pip install on that.) The installation failed with; ```; Collecting git+https://github.com/theislab/scanpy.git; Cloning https://github.com/theislab/scanpy.git to /tmp/pip-203inirx-build; Complete output from command python setup.py egg_info:; running egg_info; creating pip-egg-info/scanpy.egg-info; writing pip-egg-info/scanpy.egg-info/PKG-INFO; writing dependency_links to pip-egg-info/scanpy.egg-info/dependency_links.txt; writing entry points to pip-egg-info/scanpy.egg-info/entry_points.txt; writing requirements to pip-egg-info/scanpy.egg-info/requires.txt; writing top-level names to pip-egg-info/scanpy.egg-info/top_level.txt; writing manifest file 'pip-egg-info/scanpy.egg-info/SOURCES.txt'; warning: manifest_maker: standard file '-c' not found; ; error: package directory 'scanpy/exs' does not exist; ; ----------------------------------------; Command ""python setup.py egg_info"" failed with error code 1 in /tmp/pip-203inirx-build/; The command '/bin/sh -c pip install --upgrade --no-cache-dir git+https://github.com/theislab/scanpy.git' returned a non-zero code: 1; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/7#issuecomment-284343715
https://github.com/scverse/scanpy/issues/7#issuecomment-284343715:19,Deployability,install,install,19,"So I just tried to install the package from the master branch by running. ```; pip install git+https://github.com/theislab/scanpy.git; ```; (by the way, you can update the Installation section of the README.md because the above line is equivalent to cloning the repository and running pip install on that.) The installation failed with; ```; Collecting git+https://github.com/theislab/scanpy.git; Cloning https://github.com/theislab/scanpy.git to /tmp/pip-203inirx-build; Complete output from command python setup.py egg_info:; running egg_info; creating pip-egg-info/scanpy.egg-info; writing pip-egg-info/scanpy.egg-info/PKG-INFO; writing dependency_links to pip-egg-info/scanpy.egg-info/dependency_links.txt; writing entry points to pip-egg-info/scanpy.egg-info/entry_points.txt; writing requirements to pip-egg-info/scanpy.egg-info/requires.txt; writing top-level names to pip-egg-info/scanpy.egg-info/top_level.txt; writing manifest file 'pip-egg-info/scanpy.egg-info/SOURCES.txt'; warning: manifest_maker: standard file '-c' not found; ; error: package directory 'scanpy/exs' does not exist; ; ----------------------------------------; Command ""python setup.py egg_info"" failed with error code 1 in /tmp/pip-203inirx-build/; The command '/bin/sh -c pip install --upgrade --no-cache-dir git+https://github.com/theislab/scanpy.git' returned a non-zero code: 1; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/7#issuecomment-284343715
https://github.com/scverse/scanpy/issues/7#issuecomment-284343715:83,Deployability,install,install,83,"So I just tried to install the package from the master branch by running. ```; pip install git+https://github.com/theislab/scanpy.git; ```; (by the way, you can update the Installation section of the README.md because the above line is equivalent to cloning the repository and running pip install on that.) The installation failed with; ```; Collecting git+https://github.com/theislab/scanpy.git; Cloning https://github.com/theislab/scanpy.git to /tmp/pip-203inirx-build; Complete output from command python setup.py egg_info:; running egg_info; creating pip-egg-info/scanpy.egg-info; writing pip-egg-info/scanpy.egg-info/PKG-INFO; writing dependency_links to pip-egg-info/scanpy.egg-info/dependency_links.txt; writing entry points to pip-egg-info/scanpy.egg-info/entry_points.txt; writing requirements to pip-egg-info/scanpy.egg-info/requires.txt; writing top-level names to pip-egg-info/scanpy.egg-info/top_level.txt; writing manifest file 'pip-egg-info/scanpy.egg-info/SOURCES.txt'; warning: manifest_maker: standard file '-c' not found; ; error: package directory 'scanpy/exs' does not exist; ; ----------------------------------------; Command ""python setup.py egg_info"" failed with error code 1 in /tmp/pip-203inirx-build/; The command '/bin/sh -c pip install --upgrade --no-cache-dir git+https://github.com/theislab/scanpy.git' returned a non-zero code: 1; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/7#issuecomment-284343715
https://github.com/scverse/scanpy/issues/7#issuecomment-284343715:161,Deployability,update,update,161,"So I just tried to install the package from the master branch by running. ```; pip install git+https://github.com/theislab/scanpy.git; ```; (by the way, you can update the Installation section of the README.md because the above line is equivalent to cloning the repository and running pip install on that.) The installation failed with; ```; Collecting git+https://github.com/theislab/scanpy.git; Cloning https://github.com/theislab/scanpy.git to /tmp/pip-203inirx-build; Complete output from command python setup.py egg_info:; running egg_info; creating pip-egg-info/scanpy.egg-info; writing pip-egg-info/scanpy.egg-info/PKG-INFO; writing dependency_links to pip-egg-info/scanpy.egg-info/dependency_links.txt; writing entry points to pip-egg-info/scanpy.egg-info/entry_points.txt; writing requirements to pip-egg-info/scanpy.egg-info/requires.txt; writing top-level names to pip-egg-info/scanpy.egg-info/top_level.txt; writing manifest file 'pip-egg-info/scanpy.egg-info/SOURCES.txt'; warning: manifest_maker: standard file '-c' not found; ; error: package directory 'scanpy/exs' does not exist; ; ----------------------------------------; Command ""python setup.py egg_info"" failed with error code 1 in /tmp/pip-203inirx-build/; The command '/bin/sh -c pip install --upgrade --no-cache-dir git+https://github.com/theislab/scanpy.git' returned a non-zero code: 1; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/7#issuecomment-284343715
https://github.com/scverse/scanpy/issues/7#issuecomment-284343715:172,Deployability,Install,Installation,172,"So I just tried to install the package from the master branch by running. ```; pip install git+https://github.com/theislab/scanpy.git; ```; (by the way, you can update the Installation section of the README.md because the above line is equivalent to cloning the repository and running pip install on that.) The installation failed with; ```; Collecting git+https://github.com/theislab/scanpy.git; Cloning https://github.com/theislab/scanpy.git to /tmp/pip-203inirx-build; Complete output from command python setup.py egg_info:; running egg_info; creating pip-egg-info/scanpy.egg-info; writing pip-egg-info/scanpy.egg-info/PKG-INFO; writing dependency_links to pip-egg-info/scanpy.egg-info/dependency_links.txt; writing entry points to pip-egg-info/scanpy.egg-info/entry_points.txt; writing requirements to pip-egg-info/scanpy.egg-info/requires.txt; writing top-level names to pip-egg-info/scanpy.egg-info/top_level.txt; writing manifest file 'pip-egg-info/scanpy.egg-info/SOURCES.txt'; warning: manifest_maker: standard file '-c' not found; ; error: package directory 'scanpy/exs' does not exist; ; ----------------------------------------; Command ""python setup.py egg_info"" failed with error code 1 in /tmp/pip-203inirx-build/; The command '/bin/sh -c pip install --upgrade --no-cache-dir git+https://github.com/theislab/scanpy.git' returned a non-zero code: 1; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/7#issuecomment-284343715
https://github.com/scverse/scanpy/issues/7#issuecomment-284343715:289,Deployability,install,install,289,"So I just tried to install the package from the master branch by running. ```; pip install git+https://github.com/theislab/scanpy.git; ```; (by the way, you can update the Installation section of the README.md because the above line is equivalent to cloning the repository and running pip install on that.) The installation failed with; ```; Collecting git+https://github.com/theislab/scanpy.git; Cloning https://github.com/theislab/scanpy.git to /tmp/pip-203inirx-build; Complete output from command python setup.py egg_info:; running egg_info; creating pip-egg-info/scanpy.egg-info; writing pip-egg-info/scanpy.egg-info/PKG-INFO; writing dependency_links to pip-egg-info/scanpy.egg-info/dependency_links.txt; writing entry points to pip-egg-info/scanpy.egg-info/entry_points.txt; writing requirements to pip-egg-info/scanpy.egg-info/requires.txt; writing top-level names to pip-egg-info/scanpy.egg-info/top_level.txt; writing manifest file 'pip-egg-info/scanpy.egg-info/SOURCES.txt'; warning: manifest_maker: standard file '-c' not found; ; error: package directory 'scanpy/exs' does not exist; ; ----------------------------------------; Command ""python setup.py egg_info"" failed with error code 1 in /tmp/pip-203inirx-build/; The command '/bin/sh -c pip install --upgrade --no-cache-dir git+https://github.com/theislab/scanpy.git' returned a non-zero code: 1; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/7#issuecomment-284343715
https://github.com/scverse/scanpy/issues/7#issuecomment-284343715:311,Deployability,install,installation,311,"So I just tried to install the package from the master branch by running. ```; pip install git+https://github.com/theislab/scanpy.git; ```; (by the way, you can update the Installation section of the README.md because the above line is equivalent to cloning the repository and running pip install on that.) The installation failed with; ```; Collecting git+https://github.com/theislab/scanpy.git; Cloning https://github.com/theislab/scanpy.git to /tmp/pip-203inirx-build; Complete output from command python setup.py egg_info:; running egg_info; creating pip-egg-info/scanpy.egg-info; writing pip-egg-info/scanpy.egg-info/PKG-INFO; writing dependency_links to pip-egg-info/scanpy.egg-info/dependency_links.txt; writing entry points to pip-egg-info/scanpy.egg-info/entry_points.txt; writing requirements to pip-egg-info/scanpy.egg-info/requires.txt; writing top-level names to pip-egg-info/scanpy.egg-info/top_level.txt; writing manifest file 'pip-egg-info/scanpy.egg-info/SOURCES.txt'; warning: manifest_maker: standard file '-c' not found; ; error: package directory 'scanpy/exs' does not exist; ; ----------------------------------------; Command ""python setup.py egg_info"" failed with error code 1 in /tmp/pip-203inirx-build/; The command '/bin/sh -c pip install --upgrade --no-cache-dir git+https://github.com/theislab/scanpy.git' returned a non-zero code: 1; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/7#issuecomment-284343715
https://github.com/scverse/scanpy/issues/7#issuecomment-284343715:1258,Deployability,install,install,1258,"So I just tried to install the package from the master branch by running. ```; pip install git+https://github.com/theislab/scanpy.git; ```; (by the way, you can update the Installation section of the README.md because the above line is equivalent to cloning the repository and running pip install on that.) The installation failed with; ```; Collecting git+https://github.com/theislab/scanpy.git; Cloning https://github.com/theislab/scanpy.git to /tmp/pip-203inirx-build; Complete output from command python setup.py egg_info:; running egg_info; creating pip-egg-info/scanpy.egg-info; writing pip-egg-info/scanpy.egg-info/PKG-INFO; writing dependency_links to pip-egg-info/scanpy.egg-info/dependency_links.txt; writing entry points to pip-egg-info/scanpy.egg-info/entry_points.txt; writing requirements to pip-egg-info/scanpy.egg-info/requires.txt; writing top-level names to pip-egg-info/scanpy.egg-info/top_level.txt; writing manifest file 'pip-egg-info/scanpy.egg-info/SOURCES.txt'; warning: manifest_maker: standard file '-c' not found; ; error: package directory 'scanpy/exs' does not exist; ; ----------------------------------------; Command ""python setup.py egg_info"" failed with error code 1 in /tmp/pip-203inirx-build/; The command '/bin/sh -c pip install --upgrade --no-cache-dir git+https://github.com/theislab/scanpy.git' returned a non-zero code: 1; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/7#issuecomment-284343715
https://github.com/scverse/scanpy/issues/7#issuecomment-284343715:1268,Deployability,upgrade,upgrade,1268,"So I just tried to install the package from the master branch by running. ```; pip install git+https://github.com/theislab/scanpy.git; ```; (by the way, you can update the Installation section of the README.md because the above line is equivalent to cloning the repository and running pip install on that.) The installation failed with; ```; Collecting git+https://github.com/theislab/scanpy.git; Cloning https://github.com/theislab/scanpy.git to /tmp/pip-203inirx-build; Complete output from command python setup.py egg_info:; running egg_info; creating pip-egg-info/scanpy.egg-info; writing pip-egg-info/scanpy.egg-info/PKG-INFO; writing dependency_links to pip-egg-info/scanpy.egg-info/dependency_links.txt; writing entry points to pip-egg-info/scanpy.egg-info/entry_points.txt; writing requirements to pip-egg-info/scanpy.egg-info/requires.txt; writing top-level names to pip-egg-info/scanpy.egg-info/top_level.txt; writing manifest file 'pip-egg-info/scanpy.egg-info/SOURCES.txt'; warning: manifest_maker: standard file '-c' not found; ; error: package directory 'scanpy/exs' does not exist; ; ----------------------------------------; Command ""python setup.py egg_info"" failed with error code 1 in /tmp/pip-203inirx-build/; The command '/bin/sh -c pip install --upgrade --no-cache-dir git+https://github.com/theislab/scanpy.git' returned a non-zero code: 1; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/7#issuecomment-284343715
https://github.com/scverse/scanpy/issues/7#issuecomment-284343715:1281,Performance,cache,cache-dir,1281,"So I just tried to install the package from the master branch by running. ```; pip install git+https://github.com/theislab/scanpy.git; ```; (by the way, you can update the Installation section of the README.md because the above line is equivalent to cloning the repository and running pip install on that.) The installation failed with; ```; Collecting git+https://github.com/theislab/scanpy.git; Cloning https://github.com/theislab/scanpy.git to /tmp/pip-203inirx-build; Complete output from command python setup.py egg_info:; running egg_info; creating pip-egg-info/scanpy.egg-info; writing pip-egg-info/scanpy.egg-info/PKG-INFO; writing dependency_links to pip-egg-info/scanpy.egg-info/dependency_links.txt; writing entry points to pip-egg-info/scanpy.egg-info/entry_points.txt; writing requirements to pip-egg-info/scanpy.egg-info/requires.txt; writing top-level names to pip-egg-info/scanpy.egg-info/top_level.txt; writing manifest file 'pip-egg-info/scanpy.egg-info/SOURCES.txt'; warning: manifest_maker: standard file '-c' not found; ; error: package directory 'scanpy/exs' does not exist; ; ----------------------------------------; Command ""python setup.py egg_info"" failed with error code 1 in /tmp/pip-203inirx-build/; The command '/bin/sh -c pip install --upgrade --no-cache-dir git+https://github.com/theislab/scanpy.git' returned a non-zero code: 1; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/7#issuecomment-284343715
https://github.com/scverse/scanpy/issues/7#issuecomment-284355075:75,Deployability,install,install,75,"Sorry for that, fixed this bug. I think there is no way to make ; ```; pip install git+https://github.com/theislab/scanpy.git; ```; work if you want to install using symbolic links (to your local clone of the git repo). Until versioning starts, it's good to be able to type `git pull` in the local clone and by that automatically update the `scanpy` installation without having to do anything else. But you're right, this is should just be a temporary solution.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/7#issuecomment-284355075
https://github.com/scverse/scanpy/issues/7#issuecomment-284355075:152,Deployability,install,install,152,"Sorry for that, fixed this bug. I think there is no way to make ; ```; pip install git+https://github.com/theislab/scanpy.git; ```; work if you want to install using symbolic links (to your local clone of the git repo). Until versioning starts, it's good to be able to type `git pull` in the local clone and by that automatically update the `scanpy` installation without having to do anything else. But you're right, this is should just be a temporary solution.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/7#issuecomment-284355075
https://github.com/scverse/scanpy/issues/7#issuecomment-284355075:330,Deployability,update,update,330,"Sorry for that, fixed this bug. I think there is no way to make ; ```; pip install git+https://github.com/theislab/scanpy.git; ```; work if you want to install using symbolic links (to your local clone of the git repo). Until versioning starts, it's good to be able to type `git pull` in the local clone and by that automatically update the `scanpy` installation without having to do anything else. But you're right, this is should just be a temporary solution.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/7#issuecomment-284355075
https://github.com/scverse/scanpy/issues/7#issuecomment-284355075:350,Deployability,install,installation,350,"Sorry for that, fixed this bug. I think there is no way to make ; ```; pip install git+https://github.com/theislab/scanpy.git; ```; work if you want to install using symbolic links (to your local clone of the git repo). Until versioning starts, it's good to be able to type `git pull` in the local clone and by that automatically update the `scanpy` installation without having to do anything else. But you're right, this is should just be a temporary solution.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/7#issuecomment-284355075
https://github.com/scverse/scanpy/issues/15#issuecomment-298314807:13,Deployability,release,release,13,"0.0 is not a release, it's just a dummy tag. if you upgrade now, the version will be `0.0+216.g2d10bdd`, where `0.0` marks the initial commit, `+216` one is 216 commits later, and `g2d10bdd` marks the installed commit",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/15#issuecomment-298314807
https://github.com/scverse/scanpy/issues/15#issuecomment-298314807:52,Deployability,upgrade,upgrade,52,"0.0 is not a release, it's just a dummy tag. if you upgrade now, the version will be `0.0+216.g2d10bdd`, where `0.0` marks the initial commit, `+216` one is 216 commits later, and `g2d10bdd` marks the installed commit",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/15#issuecomment-298314807
https://github.com/scverse/scanpy/issues/15#issuecomment-298314807:201,Deployability,install,installed,201,"0.0 is not a release, it's just a dummy tag. if you upgrade now, the version will be `0.0+216.g2d10bdd`, where `0.0` marks the initial commit, `+216` one is 216 commits later, and `g2d10bdd` marks the installed commit",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/15#issuecomment-298314807
https://github.com/scverse/scanpy/issues/15#issuecomment-298314896:40,Deployability,release,released,40,"as soon as `0.1` is ready, this will be released and the version of the release will simply be `0.1`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/15#issuecomment-298314896
https://github.com/scverse/scanpy/issues/15#issuecomment-298314896:72,Deployability,release,release,72,"as soon as `0.1` is ready, this will be released and the version of the release will simply be `0.1`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/15#issuecomment-298314896
https://github.com/scverse/scanpy/issues/15#issuecomment-298314896:85,Usability,simpl,simply,85,"as soon as `0.1` is ready, this will be released and the version of the release will simply be `0.1`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/15#issuecomment-298314896
https://github.com/scverse/scanpy/issues/15#issuecomment-298315116:86,Usability,clear,clear,86,"later development versions will then again show `0.1+NUMCOMMITS.gHASH` and it will be clear for the user which kind of version she/he uses. i think it makes sense, do you?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/15#issuecomment-298315116
https://github.com/scverse/scanpy/issues/16#issuecomment-298663054:643,Modifiability,variab,variable,643,"I removed all this automatic setting of backends etc. . Currently ""is_interactive"" is only used to choose different progress bars (tqdm behaves very differently on the command line, in jupyter and then, unfortunately again differently in Rodeo) and to decide on whether a `total wall time` should be output when leaving the session. It's now left to the user to choose the matplotlib backend. If she/he logs in via ssh without setting an -X tunnel, the default interactive backend will simply fail. But that's left to the user now, no longer output of, which seemed to annoy you (I can understand that); ```; ... WARNING: did not find DISPLAY variable needed for interactive plotting; --> try ssh with `-X` or `-Y`; setting `sett.savefigs = True`; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/16#issuecomment-298663054
https://github.com/scverse/scanpy/issues/16#issuecomment-298663054:403,Testability,log,logs,403,"I removed all this automatic setting of backends etc. . Currently ""is_interactive"" is only used to choose different progress bars (tqdm behaves very differently on the command line, in jupyter and then, unfortunately again differently in Rodeo) and to decide on whether a `total wall time` should be output when leaving the session. It's now left to the user to choose the matplotlib backend. If she/he logs in via ssh without setting an -X tunnel, the default interactive backend will simply fail. But that's left to the user now, no longer output of, which seemed to annoy you (I can understand that); ```; ... WARNING: did not find DISPLAY variable needed for interactive plotting; --> try ssh with `-X` or `-Y`; setting `sett.savefigs = True`; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/16#issuecomment-298663054
https://github.com/scverse/scanpy/issues/16#issuecomment-298663054:116,Usability,progress bar,progress bars,116,"I removed all this automatic setting of backends etc. . Currently ""is_interactive"" is only used to choose different progress bars (tqdm behaves very differently on the command line, in jupyter and then, unfortunately again differently in Rodeo) and to decide on whether a `total wall time` should be output when leaving the session. It's now left to the user to choose the matplotlib backend. If she/he logs in via ssh without setting an -X tunnel, the default interactive backend will simply fail. But that's left to the user now, no longer output of, which seemed to annoy you (I can understand that); ```; ... WARNING: did not find DISPLAY variable needed for interactive plotting; --> try ssh with `-X` or `-Y`; setting `sett.savefigs = True`; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/16#issuecomment-298663054
https://github.com/scverse/scanpy/issues/16#issuecomment-298663054:486,Usability,simpl,simply,486,"I removed all this automatic setting of backends etc. . Currently ""is_interactive"" is only used to choose different progress bars (tqdm behaves very differently on the command line, in jupyter and then, unfortunately again differently in Rodeo) and to decide on whether a `total wall time` should be output when leaving the session. It's now left to the user to choose the matplotlib backend. If she/he logs in via ssh without setting an -X tunnel, the default interactive backend will simply fail. But that's left to the user now, no longer output of, which seemed to annoy you (I can understand that); ```; ... WARNING: did not find DISPLAY variable needed for interactive plotting; --> try ssh with `-X` or `-Y`; setting `sett.savefigs = True`; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/16#issuecomment-298663054
https://github.com/scverse/scanpy/issues/16#issuecomment-298757002:137,Safety,detect,detect,137,[here’s code](https://github.com/flying-sheep/smart-progress/blob/1091a0a9cc2d7a6304f992d13cb718d5150a64c6/smart_progress.py#L12-L21) to detect if we’re in a notebook or not. we could use that to decide if we want to use `tqdm_notebook` or `tqdm`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/16#issuecomment-298757002
https://github.com/scverse/scanpy/issues/16#issuecomment-298884393:15,Deployability,release,release,15,Btw: I plan to release version 0.1 later today. Together with benchmarks and many examples for the 10x datasets. Any objections?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/16#issuecomment-298884393
https://github.com/scverse/scanpy/issues/16#issuecomment-298884393:62,Testability,benchmark,benchmarks,62,Btw: I plan to release version 0.1 later today. Together with benchmarks and many examples for the 10x datasets. Any objections?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/16#issuecomment-298884393
https://github.com/scverse/scanpy/issues/16#issuecomment-298897726:245,Deployability,release,release,245,"protip: when wanting to link to lines, press the <kbd>y</kbd> key to change the url from `…/master/…` to `…/<sha1sum>/…`. that way the copied URL will stay valid and not break once the file is modified on the master branch. no objections to the release!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/16#issuecomment-298897726
https://github.com/scverse/scanpy/issues/22#issuecomment-307050792:170,Deployability,install,installing,170,"I readded cython to the requirements: https://github.com/theislab/scanpy/commit/2ae826b71c1eefa16b165d4ff85de9f76fc9e62d. I thought it would be unnecessary when directly installing from the .c files, but evidently, it is not. Anyhow, this should fix it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/22#issuecomment-307050792
https://github.com/scverse/scanpy/issues/24#issuecomment-308729587:202,Deployability,release,release,202,"Thanks! And sorry, this was something I had already fixed locally, but not yet pushed. Now it's up (https://github.com/theislab/scanpy/commit/320fa421aa2e1dcb15d047c629416b8640eb1635). The first stable release is not far away.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/24#issuecomment-308729587
https://github.com/scverse/scanpy/issues/26#issuecomment-312623579:187,Availability,robust,robust,187,"Hi ShuhuaGao,. thanks for your input! Monocle 2 has many more options for preprocessing, that's right. I believe though that you should get along with the limited options of Scanpy for a robust pseudotime and branching inference using DPT; simply because DPT is very robust. Nonetheless I have to admit that I've not worked with an extensive number of data types. From this experience, my understanding is the following. * for RNA-Seq data, you should normalize and extract highly-variable genes. this is most simply done by using the procedure of cell ranger [`sc.pp.recipe_zheng17`](https://github.com/theislab/scanpy/blob/373dc325bdc24754dd658bc06b818987de6d568c/scanpy/preprocessing/recipes.py#L59-L78) (example [here](https://github.com/theislab/scanpy_usage/tree/master/170503_zheng17)) or, if you want more control, the Seurat workflow (example [here](https://github.com/theislab/scanpy_usage/tree/master/170505_seurat)); * for qPCR, a simple log-normalization ([sc.pp.log1p](https://github.com/theislab/scanpy/blob/373dc325bdc24754dd658bc06b818987de6d568c/scanpy/preprocessing/simple.py#L280-L298)) should suffice (see example [here](https://github.com/theislab/scanpy_usage/tree/master/170501_moignard15)); you might though consider ""normalizing per cell / UMI correction"", one of the steps done in RNA-seq part ([`sc.pp.normalize_per_cell`](https://github.com/theislab/scanpy/blob/373dc325bdc24754dd658bc06b818987de6d568c/scanpy/preprocessing/simple.py#L405-L452)). Ask if you have further questions. 😄",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/26#issuecomment-312623579
https://github.com/scverse/scanpy/issues/26#issuecomment-312623579:267,Availability,robust,robust,267,"Hi ShuhuaGao,. thanks for your input! Monocle 2 has many more options for preprocessing, that's right. I believe though that you should get along with the limited options of Scanpy for a robust pseudotime and branching inference using DPT; simply because DPT is very robust. Nonetheless I have to admit that I've not worked with an extensive number of data types. From this experience, my understanding is the following. * for RNA-Seq data, you should normalize and extract highly-variable genes. this is most simply done by using the procedure of cell ranger [`sc.pp.recipe_zheng17`](https://github.com/theislab/scanpy/blob/373dc325bdc24754dd658bc06b818987de6d568c/scanpy/preprocessing/recipes.py#L59-L78) (example [here](https://github.com/theislab/scanpy_usage/tree/master/170503_zheng17)) or, if you want more control, the Seurat workflow (example [here](https://github.com/theislab/scanpy_usage/tree/master/170505_seurat)); * for qPCR, a simple log-normalization ([sc.pp.log1p](https://github.com/theislab/scanpy/blob/373dc325bdc24754dd658bc06b818987de6d568c/scanpy/preprocessing/simple.py#L280-L298)) should suffice (see example [here](https://github.com/theislab/scanpy_usage/tree/master/170501_moignard15)); you might though consider ""normalizing per cell / UMI correction"", one of the steps done in RNA-seq part ([`sc.pp.normalize_per_cell`](https://github.com/theislab/scanpy/blob/373dc325bdc24754dd658bc06b818987de6d568c/scanpy/preprocessing/simple.py#L405-L452)). Ask if you have further questions. 😄",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/26#issuecomment-312623579
https://github.com/scverse/scanpy/issues/26#issuecomment-312623579:481,Modifiability,variab,variable,481,"Hi ShuhuaGao,. thanks for your input! Monocle 2 has many more options for preprocessing, that's right. I believe though that you should get along with the limited options of Scanpy for a robust pseudotime and branching inference using DPT; simply because DPT is very robust. Nonetheless I have to admit that I've not worked with an extensive number of data types. From this experience, my understanding is the following. * for RNA-Seq data, you should normalize and extract highly-variable genes. this is most simply done by using the procedure of cell ranger [`sc.pp.recipe_zheng17`](https://github.com/theislab/scanpy/blob/373dc325bdc24754dd658bc06b818987de6d568c/scanpy/preprocessing/recipes.py#L59-L78) (example [here](https://github.com/theislab/scanpy_usage/tree/master/170503_zheng17)) or, if you want more control, the Seurat workflow (example [here](https://github.com/theislab/scanpy_usage/tree/master/170505_seurat)); * for qPCR, a simple log-normalization ([sc.pp.log1p](https://github.com/theislab/scanpy/blob/373dc325bdc24754dd658bc06b818987de6d568c/scanpy/preprocessing/simple.py#L280-L298)) should suffice (see example [here](https://github.com/theislab/scanpy_usage/tree/master/170501_moignard15)); you might though consider ""normalizing per cell / UMI correction"", one of the steps done in RNA-seq part ([`sc.pp.normalize_per_cell`](https://github.com/theislab/scanpy/blob/373dc325bdc24754dd658bc06b818987de6d568c/scanpy/preprocessing/simple.py#L405-L452)). Ask if you have further questions. 😄",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/26#issuecomment-312623579
https://github.com/scverse/scanpy/issues/26#issuecomment-312623579:950,Testability,log,log-normalization,950,"Hi ShuhuaGao,. thanks for your input! Monocle 2 has many more options for preprocessing, that's right. I believe though that you should get along with the limited options of Scanpy for a robust pseudotime and branching inference using DPT; simply because DPT is very robust. Nonetheless I have to admit that I've not worked with an extensive number of data types. From this experience, my understanding is the following. * for RNA-Seq data, you should normalize and extract highly-variable genes. this is most simply done by using the procedure of cell ranger [`sc.pp.recipe_zheng17`](https://github.com/theislab/scanpy/blob/373dc325bdc24754dd658bc06b818987de6d568c/scanpy/preprocessing/recipes.py#L59-L78) (example [here](https://github.com/theislab/scanpy_usage/tree/master/170503_zheng17)) or, if you want more control, the Seurat workflow (example [here](https://github.com/theislab/scanpy_usage/tree/master/170505_seurat)); * for qPCR, a simple log-normalization ([sc.pp.log1p](https://github.com/theislab/scanpy/blob/373dc325bdc24754dd658bc06b818987de6d568c/scanpy/preprocessing/simple.py#L280-L298)) should suffice (see example [here](https://github.com/theislab/scanpy_usage/tree/master/170501_moignard15)); you might though consider ""normalizing per cell / UMI correction"", one of the steps done in RNA-seq part ([`sc.pp.normalize_per_cell`](https://github.com/theislab/scanpy/blob/373dc325bdc24754dd658bc06b818987de6d568c/scanpy/preprocessing/simple.py#L405-L452)). Ask if you have further questions. 😄",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/26#issuecomment-312623579
https://github.com/scverse/scanpy/issues/26#issuecomment-312623579:240,Usability,simpl,simply,240,"Hi ShuhuaGao,. thanks for your input! Monocle 2 has many more options for preprocessing, that's right. I believe though that you should get along with the limited options of Scanpy for a robust pseudotime and branching inference using DPT; simply because DPT is very robust. Nonetheless I have to admit that I've not worked with an extensive number of data types. From this experience, my understanding is the following. * for RNA-Seq data, you should normalize and extract highly-variable genes. this is most simply done by using the procedure of cell ranger [`sc.pp.recipe_zheng17`](https://github.com/theislab/scanpy/blob/373dc325bdc24754dd658bc06b818987de6d568c/scanpy/preprocessing/recipes.py#L59-L78) (example [here](https://github.com/theislab/scanpy_usage/tree/master/170503_zheng17)) or, if you want more control, the Seurat workflow (example [here](https://github.com/theislab/scanpy_usage/tree/master/170505_seurat)); * for qPCR, a simple log-normalization ([sc.pp.log1p](https://github.com/theislab/scanpy/blob/373dc325bdc24754dd658bc06b818987de6d568c/scanpy/preprocessing/simple.py#L280-L298)) should suffice (see example [here](https://github.com/theislab/scanpy_usage/tree/master/170501_moignard15)); you might though consider ""normalizing per cell / UMI correction"", one of the steps done in RNA-seq part ([`sc.pp.normalize_per_cell`](https://github.com/theislab/scanpy/blob/373dc325bdc24754dd658bc06b818987de6d568c/scanpy/preprocessing/simple.py#L405-L452)). Ask if you have further questions. 😄",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/26#issuecomment-312623579
https://github.com/scverse/scanpy/issues/26#issuecomment-312623579:510,Usability,simpl,simply,510,"Hi ShuhuaGao,. thanks for your input! Monocle 2 has many more options for preprocessing, that's right. I believe though that you should get along with the limited options of Scanpy for a robust pseudotime and branching inference using DPT; simply because DPT is very robust. Nonetheless I have to admit that I've not worked with an extensive number of data types. From this experience, my understanding is the following. * for RNA-Seq data, you should normalize and extract highly-variable genes. this is most simply done by using the procedure of cell ranger [`sc.pp.recipe_zheng17`](https://github.com/theislab/scanpy/blob/373dc325bdc24754dd658bc06b818987de6d568c/scanpy/preprocessing/recipes.py#L59-L78) (example [here](https://github.com/theislab/scanpy_usage/tree/master/170503_zheng17)) or, if you want more control, the Seurat workflow (example [here](https://github.com/theislab/scanpy_usage/tree/master/170505_seurat)); * for qPCR, a simple log-normalization ([sc.pp.log1p](https://github.com/theislab/scanpy/blob/373dc325bdc24754dd658bc06b818987de6d568c/scanpy/preprocessing/simple.py#L280-L298)) should suffice (see example [here](https://github.com/theislab/scanpy_usage/tree/master/170501_moignard15)); you might though consider ""normalizing per cell / UMI correction"", one of the steps done in RNA-seq part ([`sc.pp.normalize_per_cell`](https://github.com/theislab/scanpy/blob/373dc325bdc24754dd658bc06b818987de6d568c/scanpy/preprocessing/simple.py#L405-L452)). Ask if you have further questions. 😄",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/26#issuecomment-312623579
https://github.com/scverse/scanpy/issues/26#issuecomment-312623579:943,Usability,simpl,simple,943,"Hi ShuhuaGao,. thanks for your input! Monocle 2 has many more options for preprocessing, that's right. I believe though that you should get along with the limited options of Scanpy for a robust pseudotime and branching inference using DPT; simply because DPT is very robust. Nonetheless I have to admit that I've not worked with an extensive number of data types. From this experience, my understanding is the following. * for RNA-Seq data, you should normalize and extract highly-variable genes. this is most simply done by using the procedure of cell ranger [`sc.pp.recipe_zheng17`](https://github.com/theislab/scanpy/blob/373dc325bdc24754dd658bc06b818987de6d568c/scanpy/preprocessing/recipes.py#L59-L78) (example [here](https://github.com/theislab/scanpy_usage/tree/master/170503_zheng17)) or, if you want more control, the Seurat workflow (example [here](https://github.com/theislab/scanpy_usage/tree/master/170505_seurat)); * for qPCR, a simple log-normalization ([sc.pp.log1p](https://github.com/theislab/scanpy/blob/373dc325bdc24754dd658bc06b818987de6d568c/scanpy/preprocessing/simple.py#L280-L298)) should suffice (see example [here](https://github.com/theislab/scanpy_usage/tree/master/170501_moignard15)); you might though consider ""normalizing per cell / UMI correction"", one of the steps done in RNA-seq part ([`sc.pp.normalize_per_cell`](https://github.com/theislab/scanpy/blob/373dc325bdc24754dd658bc06b818987de6d568c/scanpy/preprocessing/simple.py#L405-L452)). Ask if you have further questions. 😄",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/26#issuecomment-312623579
https://github.com/scverse/scanpy/issues/26#issuecomment-312623579:1085,Usability,simpl,simple,1085,"Hi ShuhuaGao,. thanks for your input! Monocle 2 has many more options for preprocessing, that's right. I believe though that you should get along with the limited options of Scanpy for a robust pseudotime and branching inference using DPT; simply because DPT is very robust. Nonetheless I have to admit that I've not worked with an extensive number of data types. From this experience, my understanding is the following. * for RNA-Seq data, you should normalize and extract highly-variable genes. this is most simply done by using the procedure of cell ranger [`sc.pp.recipe_zheng17`](https://github.com/theislab/scanpy/blob/373dc325bdc24754dd658bc06b818987de6d568c/scanpy/preprocessing/recipes.py#L59-L78) (example [here](https://github.com/theislab/scanpy_usage/tree/master/170503_zheng17)) or, if you want more control, the Seurat workflow (example [here](https://github.com/theislab/scanpy_usage/tree/master/170505_seurat)); * for qPCR, a simple log-normalization ([sc.pp.log1p](https://github.com/theislab/scanpy/blob/373dc325bdc24754dd658bc06b818987de6d568c/scanpy/preprocessing/simple.py#L280-L298)) should suffice (see example [here](https://github.com/theislab/scanpy_usage/tree/master/170501_moignard15)); you might though consider ""normalizing per cell / UMI correction"", one of the steps done in RNA-seq part ([`sc.pp.normalize_per_cell`](https://github.com/theislab/scanpy/blob/373dc325bdc24754dd658bc06b818987de6d568c/scanpy/preprocessing/simple.py#L405-L452)). Ask if you have further questions. 😄",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/26#issuecomment-312623579
https://github.com/scverse/scanpy/issues/26#issuecomment-312623579:1453,Usability,simpl,simple,1453,"Hi ShuhuaGao,. thanks for your input! Monocle 2 has many more options for preprocessing, that's right. I believe though that you should get along with the limited options of Scanpy for a robust pseudotime and branching inference using DPT; simply because DPT is very robust. Nonetheless I have to admit that I've not worked with an extensive number of data types. From this experience, my understanding is the following. * for RNA-Seq data, you should normalize and extract highly-variable genes. this is most simply done by using the procedure of cell ranger [`sc.pp.recipe_zheng17`](https://github.com/theislab/scanpy/blob/373dc325bdc24754dd658bc06b818987de6d568c/scanpy/preprocessing/recipes.py#L59-L78) (example [here](https://github.com/theislab/scanpy_usage/tree/master/170503_zheng17)) or, if you want more control, the Seurat workflow (example [here](https://github.com/theislab/scanpy_usage/tree/master/170505_seurat)); * for qPCR, a simple log-normalization ([sc.pp.log1p](https://github.com/theislab/scanpy/blob/373dc325bdc24754dd658bc06b818987de6d568c/scanpy/preprocessing/simple.py#L280-L298)) should suffice (see example [here](https://github.com/theislab/scanpy_usage/tree/master/170501_moignard15)); you might though consider ""normalizing per cell / UMI correction"", one of the steps done in RNA-seq part ([`sc.pp.normalize_per_cell`](https://github.com/theislab/scanpy/blob/373dc325bdc24754dd658bc06b818987de6d568c/scanpy/preprocessing/simple.py#L405-L452)). Ask if you have further questions. 😄",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/26#issuecomment-312623579
https://github.com/scverse/scanpy/issues/26#issuecomment-312650646:927,Availability,avail,available,927,"Hi, Alex,. Many thanks for your quick reply. I just saw your reply as it is almost 10PM in Singapore now. It is understandable to perform quality control, in-cell normalization and to extract the highly variable genes for ordering. I got your point. For your reply about qPCR, do we need a log normalization? I think a log transform is only required for RNA-Seq data to get a non-skewed normal distribution. As for qPCR data, the delta_Ct value is actually already in a log scale. In the example you have mentioned, there is no call of sc.pp.log1p, either. Instead, we just read the data by ; `adata = sc.read(filename, sheet='dCt_values.txt', backup_url=backup_url)`; and no more processing is applied. As can be found from the original paper, the so-called dCt_value is just defined as HK_Ct - Ct, where HK_Ct is the mean Ct of 4 housing keeping genes on a cell-wise basis. . Besides, in many cases, there may be no UMI data available. In such a case, the normalization per cell for RNA-Seq is actually to compute the FPKM/TPM to compensate for the sequencing depth, right? Usually, the RNA-Seq data in FPKM form is already provided in publications. And then we work on this data to find the highly variable genes. (Just personal understanding. I am new to this field from mechatronics engineering.). Anyway, thanks again for your help. I noticed that there are no examples for pseudo-time ordering with RNA-Seq data. Maybe I can provide one in the near future, as I am working on gene network modeling based on the pseudo-time information.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/26#issuecomment-312650646
https://github.com/scverse/scanpy/issues/26#issuecomment-312650646:203,Modifiability,variab,variable,203,"Hi, Alex,. Many thanks for your quick reply. I just saw your reply as it is almost 10PM in Singapore now. It is understandable to perform quality control, in-cell normalization and to extract the highly variable genes for ordering. I got your point. For your reply about qPCR, do we need a log normalization? I think a log transform is only required for RNA-Seq data to get a non-skewed normal distribution. As for qPCR data, the delta_Ct value is actually already in a log scale. In the example you have mentioned, there is no call of sc.pp.log1p, either. Instead, we just read the data by ; `adata = sc.read(filename, sheet='dCt_values.txt', backup_url=backup_url)`; and no more processing is applied. As can be found from the original paper, the so-called dCt_value is just defined as HK_Ct - Ct, where HK_Ct is the mean Ct of 4 housing keeping genes on a cell-wise basis. . Besides, in many cases, there may be no UMI data available. In such a case, the normalization per cell for RNA-Seq is actually to compute the FPKM/TPM to compensate for the sequencing depth, right? Usually, the RNA-Seq data in FPKM form is already provided in publications. And then we work on this data to find the highly variable genes. (Just personal understanding. I am new to this field from mechatronics engineering.). Anyway, thanks again for your help. I noticed that there are no examples for pseudo-time ordering with RNA-Seq data. Maybe I can provide one in the near future, as I am working on gene network modeling based on the pseudo-time information.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/26#issuecomment-312650646
https://github.com/scverse/scanpy/issues/26#issuecomment-312650646:1201,Modifiability,variab,variable,1201,"Hi, Alex,. Many thanks for your quick reply. I just saw your reply as it is almost 10PM in Singapore now. It is understandable to perform quality control, in-cell normalization and to extract the highly variable genes for ordering. I got your point. For your reply about qPCR, do we need a log normalization? I think a log transform is only required for RNA-Seq data to get a non-skewed normal distribution. As for qPCR data, the delta_Ct value is actually already in a log scale. In the example you have mentioned, there is no call of sc.pp.log1p, either. Instead, we just read the data by ; `adata = sc.read(filename, sheet='dCt_values.txt', backup_url=backup_url)`; and no more processing is applied. As can be found from the original paper, the so-called dCt_value is just defined as HK_Ct - Ct, where HK_Ct is the mean Ct of 4 housing keeping genes on a cell-wise basis. . Besides, in many cases, there may be no UMI data available. In such a case, the normalization per cell for RNA-Seq is actually to compute the FPKM/TPM to compensate for the sequencing depth, right? Usually, the RNA-Seq data in FPKM form is already provided in publications. And then we work on this data to find the highly variable genes. (Just personal understanding. I am new to this field from mechatronics engineering.). Anyway, thanks again for your help. I noticed that there are no examples for pseudo-time ordering with RNA-Seq data. Maybe I can provide one in the near future, as I am working on gene network modeling based on the pseudo-time information.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/26#issuecomment-312650646
https://github.com/scverse/scanpy/issues/26#issuecomment-312650646:130,Performance,perform,perform,130,"Hi, Alex,. Many thanks for your quick reply. I just saw your reply as it is almost 10PM in Singapore now. It is understandable to perform quality control, in-cell normalization and to extract the highly variable genes for ordering. I got your point. For your reply about qPCR, do we need a log normalization? I think a log transform is only required for RNA-Seq data to get a non-skewed normal distribution. As for qPCR data, the delta_Ct value is actually already in a log scale. In the example you have mentioned, there is no call of sc.pp.log1p, either. Instead, we just read the data by ; `adata = sc.read(filename, sheet='dCt_values.txt', backup_url=backup_url)`; and no more processing is applied. As can be found from the original paper, the so-called dCt_value is just defined as HK_Ct - Ct, where HK_Ct is the mean Ct of 4 housing keeping genes on a cell-wise basis. . Besides, in many cases, there may be no UMI data available. In such a case, the normalization per cell for RNA-Seq is actually to compute the FPKM/TPM to compensate for the sequencing depth, right? Usually, the RNA-Seq data in FPKM form is already provided in publications. And then we work on this data to find the highly variable genes. (Just personal understanding. I am new to this field from mechatronics engineering.). Anyway, thanks again for your help. I noticed that there are no examples for pseudo-time ordering with RNA-Seq data. Maybe I can provide one in the near future, as I am working on gene network modeling based on the pseudo-time information.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/26#issuecomment-312650646
https://github.com/scverse/scanpy/issues/26#issuecomment-312650646:290,Testability,log,log,290,"Hi, Alex,. Many thanks for your quick reply. I just saw your reply as it is almost 10PM in Singapore now. It is understandable to perform quality control, in-cell normalization and to extract the highly variable genes for ordering. I got your point. For your reply about qPCR, do we need a log normalization? I think a log transform is only required for RNA-Seq data to get a non-skewed normal distribution. As for qPCR data, the delta_Ct value is actually already in a log scale. In the example you have mentioned, there is no call of sc.pp.log1p, either. Instead, we just read the data by ; `adata = sc.read(filename, sheet='dCt_values.txt', backup_url=backup_url)`; and no more processing is applied. As can be found from the original paper, the so-called dCt_value is just defined as HK_Ct - Ct, where HK_Ct is the mean Ct of 4 housing keeping genes on a cell-wise basis. . Besides, in many cases, there may be no UMI data available. In such a case, the normalization per cell for RNA-Seq is actually to compute the FPKM/TPM to compensate for the sequencing depth, right? Usually, the RNA-Seq data in FPKM form is already provided in publications. And then we work on this data to find the highly variable genes. (Just personal understanding. I am new to this field from mechatronics engineering.). Anyway, thanks again for your help. I noticed that there are no examples for pseudo-time ordering with RNA-Seq data. Maybe I can provide one in the near future, as I am working on gene network modeling based on the pseudo-time information.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/26#issuecomment-312650646
https://github.com/scverse/scanpy/issues/26#issuecomment-312650646:319,Testability,log,log,319,"Hi, Alex,. Many thanks for your quick reply. I just saw your reply as it is almost 10PM in Singapore now. It is understandable to perform quality control, in-cell normalization and to extract the highly variable genes for ordering. I got your point. For your reply about qPCR, do we need a log normalization? I think a log transform is only required for RNA-Seq data to get a non-skewed normal distribution. As for qPCR data, the delta_Ct value is actually already in a log scale. In the example you have mentioned, there is no call of sc.pp.log1p, either. Instead, we just read the data by ; `adata = sc.read(filename, sheet='dCt_values.txt', backup_url=backup_url)`; and no more processing is applied. As can be found from the original paper, the so-called dCt_value is just defined as HK_Ct - Ct, where HK_Ct is the mean Ct of 4 housing keeping genes on a cell-wise basis. . Besides, in many cases, there may be no UMI data available. In such a case, the normalization per cell for RNA-Seq is actually to compute the FPKM/TPM to compensate for the sequencing depth, right? Usually, the RNA-Seq data in FPKM form is already provided in publications. And then we work on this data to find the highly variable genes. (Just personal understanding. I am new to this field from mechatronics engineering.). Anyway, thanks again for your help. I noticed that there are no examples for pseudo-time ordering with RNA-Seq data. Maybe I can provide one in the near future, as I am working on gene network modeling based on the pseudo-time information.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/26#issuecomment-312650646
https://github.com/scverse/scanpy/issues/26#issuecomment-312650646:470,Testability,log,log,470,"Hi, Alex,. Many thanks for your quick reply. I just saw your reply as it is almost 10PM in Singapore now. It is understandable to perform quality control, in-cell normalization and to extract the highly variable genes for ordering. I got your point. For your reply about qPCR, do we need a log normalization? I think a log transform is only required for RNA-Seq data to get a non-skewed normal distribution. As for qPCR data, the delta_Ct value is actually already in a log scale. In the example you have mentioned, there is no call of sc.pp.log1p, either. Instead, we just read the data by ; `adata = sc.read(filename, sheet='dCt_values.txt', backup_url=backup_url)`; and no more processing is applied. As can be found from the original paper, the so-called dCt_value is just defined as HK_Ct - Ct, where HK_Ct is the mean Ct of 4 housing keeping genes on a cell-wise basis. . Besides, in many cases, there may be no UMI data available. In such a case, the normalization per cell for RNA-Seq is actually to compute the FPKM/TPM to compensate for the sequencing depth, right? Usually, the RNA-Seq data in FPKM form is already provided in publications. And then we work on this data to find the highly variable genes. (Just personal understanding. I am new to this field from mechatronics engineering.). Anyway, thanks again for your help. I noticed that there are no examples for pseudo-time ordering with RNA-Seq data. Maybe I can provide one in the near future, as I am working on gene network modeling based on the pseudo-time information.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/26#issuecomment-312650646
https://github.com/scverse/scanpy/issues/26#issuecomment-312654301:80,Testability,log,log,80,"Hi!. Everything that you write makes sense: if the qPCR values are already on a log scale, you shouldn't log-transform them anymore / if the RNA-seq data is already in FPKM form, you do not need to do account for UMI correction ... Regarding the pseudotime example for RNA-seq data: [here](https://github.com/theislab/scanpy_usage/blob/master/170502_paul15/paul15.ipynb) is a public one. But it would be nice to have more!. Thanks for your input!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/26#issuecomment-312654301
https://github.com/scverse/scanpy/issues/26#issuecomment-312654301:105,Testability,log,log-transform,105,"Hi!. Everything that you write makes sense: if the qPCR values are already on a log scale, you shouldn't log-transform them anymore / if the RNA-seq data is already in FPKM form, you do not need to do account for UMI correction ... Regarding the pseudotime example for RNA-seq data: [here](https://github.com/theislab/scanpy_usage/blob/master/170502_paul15/paul15.ipynb) is a public one. But it would be nice to have more!. Thanks for your input!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/26#issuecomment-312654301
https://github.com/scverse/scanpy/issues/26#issuecomment-312780478:58,Usability,feedback,feedback,58,Thanks for your reply. I will try that and may given more feedback. Cheers!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/26#issuecomment-312780478
https://github.com/scverse/scanpy/issues/27#issuecomment-314053618:270,Availability,down,download,270,"About dpt_order in the Moignard example, if we plot the pseudotime vs. dpt-order at last manually like follows, ; `plt.figure(figsize=(8,8)); plt.plot(adata.smp['dpt_order'], adata.smp['dpt_pseudotime']); plt.xlabel('dpt-order'); plt.ylabel('pseudotime')`; we'll get; ![download 1](https://user-images.githubusercontent.com/20141984/28011291-687c20e0-6594-11e7-8068-472b4ab4d8a8.png); which is totally chaotic, unlike the one we get through `sc.pl.dpt`. Anyway, did I misunderstand the concept of dpt-order or is there a bug about this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/27#issuecomment-314053618
https://github.com/scverse/scanpy/issues/27#issuecomment-314744802:125,Usability,simpl,simply,125,"Ok, now I see why you are confused! The x-axis in the figure above is somehow confusingly labeled 'dpt_order' even though it simply corresponds to `range(adata.n_smps)`, as evident from the code snippet I posted above; posting it again with the weird axis labeling that confuses you; ```; import matplotlib.pyplot as pl; pl.plot(range(adata.n_smps), adata.smp['dpt_pseudotime'][adata.smp['dpt_order']]); pl.xlabel('dpt_order'); pl.show(); ```. I definitely have to change this naming. Still it is meaningful that `adata.smp['dpt_order']` is an index vector that ""generates"" the order. OK, very soon, I could do the following. I'll rename the index array that generates the order `adata.smp['dpt_generate_order']` and the order you see in the plot `adata.smp['dpt_ordering_id']`, if you think this is better. So, one probably fast computation of this is the following; ```; ordering_id = np.zeros(adata.n_smps, dtype=int); for count, idx in adata.smp['dpt_generate_order']:; ordering_id[idx] = count; ```. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/27#issuecomment-314744802
https://github.com/scverse/scanpy/issues/27#issuecomment-314766836:81,Availability,robust,robustly,81,"Well, I think it is better to be consistent with the paper [Diffusion pseudotime robustly reconstructs lineage branching](https://www.nature.com/nmeth/journal/v13/n10/full/nmeth.3971.html) since this paper first introduces the diffusion pseudotime concept. In Figure 1 (c) of this paper, there is a _DPT order_. It seems the dpt order in this paper is just a global rank for each individual cell according to their pseudotime. Therefore, I suggest that the adata.smp['dpt_order'] and the one in the figure should have the same meaning, though IMHO dpt_order only matters for cells on the same branch. If we extract cells by their dpt_group, then the dpt_order is still applicable even though it is not continuous now. . In short, I think a dpt_order defined as the global rank by pseudotime like the one in the original paper is more understandable. By the way, if there are multiple branches in the diffusion map, is there some way to assign the cells to a certain branch? That is to say, can we provide an _adata.smp['branch']_ field?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/27#issuecomment-314766836
https://github.com/scverse/scanpy/issues/27#issuecomment-314766836:702,Deployability,continuous,continuous,702,"Well, I think it is better to be consistent with the paper [Diffusion pseudotime robustly reconstructs lineage branching](https://www.nature.com/nmeth/journal/v13/n10/full/nmeth.3971.html) since this paper first introduces the diffusion pseudotime concept. In Figure 1 (c) of this paper, there is a _DPT order_. It seems the dpt order in this paper is just a global rank for each individual cell according to their pseudotime. Therefore, I suggest that the adata.smp['dpt_order'] and the one in the figure should have the same meaning, though IMHO dpt_order only matters for cells on the same branch. If we extract cells by their dpt_group, then the dpt_order is still applicable even though it is not continuous now. . In short, I think a dpt_order defined as the global rank by pseudotime like the one in the original paper is more understandable. By the way, if there are multiple branches in the diffusion map, is there some way to assign the cells to a certain branch? That is to say, can we provide an _adata.smp['branch']_ field?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/27#issuecomment-314766836
https://github.com/scverse/scanpy/issues/30#issuecomment-322022113:38,Integrability,interface,interface,38,"Hi!; Sorry for that, the command-line interface got a bit behind. I fixed everything, simply pull again.; Cheers,; Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/30#issuecomment-322022113
https://github.com/scverse/scanpy/issues/30#issuecomment-322022113:86,Usability,simpl,simply,86,"Hi!; Sorry for that, the command-line interface got a bit behind. I fixed everything, simply pull again.; Cheers,; Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/30#issuecomment-322022113
https://github.com/scverse/scanpy/issues/33#issuecomment-324471700:38,Safety,detect,detects,38,"Hi Jiping!. I know that sometimes DPT detects groups with no cells in it; you can try setting the obscure option `allow_kendall_tau_shift` to `False`; sometimes this helps. But the problem goes deeper [see at the very end [here](https://github.com/theislab/scanpy_usage/blob/master/170502_paul15/paul15.ipynb) and [there](https://github.com/theislab/scanpy_usage/blob/master/170430_krumsiek11/krumsiek11.ipynb) how branching groups are sometimes not meaningfully chosen). We're almost done with a method that combines the merits of DPT with conventional clustering that resolves this problem. No, it doesn't mean that there is no branching signature in your data; but it is certainly not a strong one; in many ""easy"" cases, DPT works perfectly. You could also try to make the branching more proncounced by changing the preprocessing. Hope that helps,; Alex. Btw: We started to set up a documentation at https://scanpy.readthedocs.io.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/33#issuecomment-324471700
https://github.com/scverse/scanpy/issues/33#issuecomment-324831221:96,Availability,down,downstream,96,"Hi Alex!; Before, I filtered gene with `min_mean` and `min_disp`, and left about 1300 genes for downstream analysis. Maybe the dataset is highly similar, so I reduce the gene number and choose the top 200 highly variable genes and it run without error. ; Thanks a lot,; Jiping",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/33#issuecomment-324831221
https://github.com/scverse/scanpy/issues/33#issuecomment-324831221:246,Availability,error,error,246,"Hi Alex!; Before, I filtered gene with `min_mean` and `min_disp`, and left about 1300 genes for downstream analysis. Maybe the dataset is highly similar, so I reduce the gene number and choose the top 200 highly variable genes and it run without error. ; Thanks a lot,; Jiping",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/33#issuecomment-324831221
https://github.com/scverse/scanpy/issues/33#issuecomment-324831221:159,Energy Efficiency,reduce,reduce,159,"Hi Alex!; Before, I filtered gene with `min_mean` and `min_disp`, and left about 1300 genes for downstream analysis. Maybe the dataset is highly similar, so I reduce the gene number and choose the top 200 highly variable genes and it run without error. ; Thanks a lot,; Jiping",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/33#issuecomment-324831221
https://github.com/scverse/scanpy/issues/33#issuecomment-324831221:212,Modifiability,variab,variable,212,"Hi Alex!; Before, I filtered gene with `min_mean` and `min_disp`, and left about 1300 genes for downstream analysis. Maybe the dataset is highly similar, so I reduce the gene number and choose the top 200 highly variable genes and it run without error. ; Thanks a lot,; Jiping",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/33#issuecomment-324831221
https://github.com/scverse/scanpy/issues/35#issuecomment-324463747:291,Availability,error,error,291,"That is evidently a problem of [psutil](https://pypi.python.org/pypi/psutil); do you have an old version of it? I have tested with 5.2.2 and 5.1.2. Earlier, psutil seemed to have had a [different convention](https://stackoverflow.com/questions/31216835/python-psutil-psutil-get-process-list-error). But both is not related to Scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/35#issuecomment-324463747
https://github.com/scverse/scanpy/issues/35#issuecomment-324463747:119,Testability,test,tested,119,"That is evidently a problem of [psutil](https://pypi.python.org/pypi/psutil); do you have an old version of it? I have tested with 5.2.2 and 5.1.2. Earlier, psutil seemed to have had a [different convention](https://stackoverflow.com/questions/31216835/python-psutil-psutil-get-process-list-error). But both is not related to Scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/35#issuecomment-324463747
https://github.com/scverse/scanpy/issues/35#issuecomment-324465215:18,Deployability,install,install,18,"Does running `pip install psutil --upgrade` help? If yes, I will update the requirements with a minimal version of psutil so that others don't run into the same problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/35#issuecomment-324465215
https://github.com/scverse/scanpy/issues/35#issuecomment-324465215:35,Deployability,upgrade,upgrade,35,"Does running `pip install psutil --upgrade` help? If yes, I will update the requirements with a minimal version of psutil so that others don't run into the same problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/35#issuecomment-324465215
https://github.com/scverse/scanpy/issues/35#issuecomment-324465215:65,Deployability,update,update,65,"Does running `pip install psutil --upgrade` help? If yes, I will update the requirements with a minimal version of psutil so that others don't run into the same problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/35#issuecomment-324465215
https://github.com/scverse/scanpy/issues/35#issuecomment-324476821:258,Availability,error,error,258,"Hm, I researched a bit more. psutil doesn't seem to cause problems and also, this has not been a problem within Scanpy for any user up to now. If you start a terminal with `python` and type; ```; import psutil; psutil.process_iter(); ```; does this throw an error? I'd really like to know what's going on. If you want a quick fix; you can simply comment out line 773 in your file `/ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/readwrite.py`; this should cause no problem for your applications.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/35#issuecomment-324476821
https://github.com/scverse/scanpy/issues/35#issuecomment-324476821:392,Security,hash,hashem,392,"Hm, I researched a bit more. psutil doesn't seem to cause problems and also, this has not been a problem within Scanpy for any user up to now. If you start a terminal with `python` and type; ```; import psutil; psutil.process_iter(); ```; does this throw an error? I'd really like to know what's going on. If you want a quick fix; you can simply comment out line 773 in your file `/ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/readwrite.py`; this should cause no problem for your applications.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/35#issuecomment-324476821
https://github.com/scverse/scanpy/issues/35#issuecomment-324476821:339,Usability,simpl,simply,339,"Hm, I researched a bit more. psutil doesn't seem to cause problems and also, this has not been a problem within Scanpy for any user up to now. If you start a terminal with `python` and type; ```; import psutil; psutil.process_iter(); ```; does this throw an error? I'd really like to know what's going on. If you want a quick fix; you can simply comment out line 773 in your file `/ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/readwrite.py`; this should cause no problem for your applications.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/35#issuecomment-324476821
https://github.com/scverse/scanpy/issues/35#issuecomment-324587457:208,Availability,error,error,208,"Hi Alex, ; The psutil issue by updating it has apparently gone away, however later on when I call `sc.tl.louvain(adata_corrected, n_neighbors=10, resolution=1.3, recompute_graph=True)`; , I get the following error. The igraph I am using is V 0.1.11.; Many thanks; Hashem; `DeprecationWarning Traceback (most recent call last); <ipython-input-20-fb44185f2d28> in <module>(); 1 ; ----> 2 sc.tl.louvain(adata_corrected, n_neighbors=10, resolution=1.3, recompute_graph=True); 3 . /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/tools/louvain.py in louvain(adata, n_neighbors, resolution, n_pcs, random_state, flavor, directed, recompute_pca, recompute_distances, recompute_graph, n_dcs, n_jobs, copy); 78 directed = False; 79 if not directed: logg.m(' using the undirected graph', v=4); ---> 80 g = utils.get_igraph_from_adjacency(adjacency, directed=directed); 81 if flavor == 'vtraag':; 82 import louvain. /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/utils.py in get_igraph_from_adjacency(adjacency, directed); 41 def get_igraph_from_adjacency(adjacency, directed=None):; 42 """"""Get igraph graph from adjacency matrix.""""""; ---> 43 import igraph as ig; 44 sources, targets = adjacency.nonzero(); 45 weights = adjacency[sources, targets]. /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/igraph/__init__.py in <module>(); 6 __license__ = ""MIT""; 7 ; ----> 8 raise DeprecationWarning(""To avoid name collision with the igraph project, ""; 9 ""this visualization library has been renamed to ""; 10 ""'jgraph'. Please upgrade when convenient.""). DeprecationWarning: To avoid name collision with the igraph project, this visualization library has been renamed to 'jgraph'. Please upgrade when convenient.`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/35#issuecomment-324587457
https://github.com/scverse/scanpy/issues/35#issuecomment-324587457:1550,Deployability,upgrade,upgrade,1550,"Hi Alex, ; The psutil issue by updating it has apparently gone away, however later on when I call `sc.tl.louvain(adata_corrected, n_neighbors=10, resolution=1.3, recompute_graph=True)`; , I get the following error. The igraph I am using is V 0.1.11.; Many thanks; Hashem; `DeprecationWarning Traceback (most recent call last); <ipython-input-20-fb44185f2d28> in <module>(); 1 ; ----> 2 sc.tl.louvain(adata_corrected, n_neighbors=10, resolution=1.3, recompute_graph=True); 3 . /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/tools/louvain.py in louvain(adata, n_neighbors, resolution, n_pcs, random_state, flavor, directed, recompute_pca, recompute_distances, recompute_graph, n_dcs, n_jobs, copy); 78 directed = False; 79 if not directed: logg.m(' using the undirected graph', v=4); ---> 80 g = utils.get_igraph_from_adjacency(adjacency, directed=directed); 81 if flavor == 'vtraag':; 82 import louvain. /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/utils.py in get_igraph_from_adjacency(adjacency, directed); 41 def get_igraph_from_adjacency(adjacency, directed=None):; 42 """"""Get igraph graph from adjacency matrix.""""""; ---> 43 import igraph as ig; 44 sources, targets = adjacency.nonzero(); 45 weights = adjacency[sources, targets]. /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/igraph/__init__.py in <module>(); 6 __license__ = ""MIT""; 7 ; ----> 8 raise DeprecationWarning(""To avoid name collision with the igraph project, ""; 9 ""this visualization library has been renamed to ""; 10 ""'jgraph'. Please upgrade when convenient.""). DeprecationWarning: To avoid name collision with the igraph project, this visualization library has been renamed to 'jgraph'. Please upgrade when convenient.`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/35#issuecomment-324587457
https://github.com/scverse/scanpy/issues/35#issuecomment-324587457:1711,Deployability,upgrade,upgrade,1711,"Hi Alex, ; The psutil issue by updating it has apparently gone away, however later on when I call `sc.tl.louvain(adata_corrected, n_neighbors=10, resolution=1.3, recompute_graph=True)`; , I get the following error. The igraph I am using is V 0.1.11.; Many thanks; Hashem; `DeprecationWarning Traceback (most recent call last); <ipython-input-20-fb44185f2d28> in <module>(); 1 ; ----> 2 sc.tl.louvain(adata_corrected, n_neighbors=10, resolution=1.3, recompute_graph=True); 3 . /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/tools/louvain.py in louvain(adata, n_neighbors, resolution, n_pcs, random_state, flavor, directed, recompute_pca, recompute_distances, recompute_graph, n_dcs, n_jobs, copy); 78 directed = False; 79 if not directed: logg.m(' using the undirected graph', v=4); ---> 80 g = utils.get_igraph_from_adjacency(adjacency, directed=directed); 81 if flavor == 'vtraag':; 82 import louvain. /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/utils.py in get_igraph_from_adjacency(adjacency, directed); 41 def get_igraph_from_adjacency(adjacency, directed=None):; 42 """"""Get igraph graph from adjacency matrix.""""""; ---> 43 import igraph as ig; 44 sources, targets = adjacency.nonzero(); 45 weights = adjacency[sources, targets]. /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/igraph/__init__.py in <module>(); 6 __license__ = ""MIT""; 7 ; ----> 8 raise DeprecationWarning(""To avoid name collision with the igraph project, ""; 9 ""this visualization library has been renamed to ""; 10 ""'jgraph'. Please upgrade when convenient.""). DeprecationWarning: To avoid name collision with the igraph project, this visualization library has been renamed to 'jgraph'. Please upgrade when convenient.`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/35#issuecomment-324587457
https://github.com/scverse/scanpy/issues/35#issuecomment-324587457:1427,Safety,avoid,avoid,1427,"Hi Alex, ; The psutil issue by updating it has apparently gone away, however later on when I call `sc.tl.louvain(adata_corrected, n_neighbors=10, resolution=1.3, recompute_graph=True)`; , I get the following error. The igraph I am using is V 0.1.11.; Many thanks; Hashem; `DeprecationWarning Traceback (most recent call last); <ipython-input-20-fb44185f2d28> in <module>(); 1 ; ----> 2 sc.tl.louvain(adata_corrected, n_neighbors=10, resolution=1.3, recompute_graph=True); 3 . /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/tools/louvain.py in louvain(adata, n_neighbors, resolution, n_pcs, random_state, flavor, directed, recompute_pca, recompute_distances, recompute_graph, n_dcs, n_jobs, copy); 78 directed = False; 79 if not directed: logg.m(' using the undirected graph', v=4); ---> 80 g = utils.get_igraph_from_adjacency(adjacency, directed=directed); 81 if flavor == 'vtraag':; 82 import louvain. /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/utils.py in get_igraph_from_adjacency(adjacency, directed); 41 def get_igraph_from_adjacency(adjacency, directed=None):; 42 """"""Get igraph graph from adjacency matrix.""""""; ---> 43 import igraph as ig; 44 sources, targets = adjacency.nonzero(); 45 weights = adjacency[sources, targets]. /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/igraph/__init__.py in <module>(); 6 __license__ = ""MIT""; 7 ; ----> 8 raise DeprecationWarning(""To avoid name collision with the igraph project, ""; 9 ""this visualization library has been renamed to ""; 10 ""'jgraph'. Please upgrade when convenient.""). DeprecationWarning: To avoid name collision with the igraph project, this visualization library has been renamed to 'jgraph'. Please upgrade when convenient.`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/35#issuecomment-324587457
https://github.com/scverse/scanpy/issues/35#issuecomment-324587457:1601,Safety,avoid,avoid,1601,"Hi Alex, ; The psutil issue by updating it has apparently gone away, however later on when I call `sc.tl.louvain(adata_corrected, n_neighbors=10, resolution=1.3, recompute_graph=True)`; , I get the following error. The igraph I am using is V 0.1.11.; Many thanks; Hashem; `DeprecationWarning Traceback (most recent call last); <ipython-input-20-fb44185f2d28> in <module>(); 1 ; ----> 2 sc.tl.louvain(adata_corrected, n_neighbors=10, resolution=1.3, recompute_graph=True); 3 . /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/tools/louvain.py in louvain(adata, n_neighbors, resolution, n_pcs, random_state, flavor, directed, recompute_pca, recompute_distances, recompute_graph, n_dcs, n_jobs, copy); 78 directed = False; 79 if not directed: logg.m(' using the undirected graph', v=4); ---> 80 g = utils.get_igraph_from_adjacency(adjacency, directed=directed); 81 if flavor == 'vtraag':; 82 import louvain. /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/utils.py in get_igraph_from_adjacency(adjacency, directed); 41 def get_igraph_from_adjacency(adjacency, directed=None):; 42 """"""Get igraph graph from adjacency matrix.""""""; ---> 43 import igraph as ig; 44 sources, targets = adjacency.nonzero(); 45 weights = adjacency[sources, targets]. /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/igraph/__init__.py in <module>(); 6 __license__ = ""MIT""; 7 ; ----> 8 raise DeprecationWarning(""To avoid name collision with the igraph project, ""; 9 ""this visualization library has been renamed to ""; 10 ""'jgraph'. Please upgrade when convenient.""). DeprecationWarning: To avoid name collision with the igraph project, this visualization library has been renamed to 'jgraph'. Please upgrade when convenient.`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/35#issuecomment-324587457
https://github.com/scverse/scanpy/issues/35#issuecomment-324587457:264,Security,Hash,Hashem,264,"Hi Alex, ; The psutil issue by updating it has apparently gone away, however later on when I call `sc.tl.louvain(adata_corrected, n_neighbors=10, resolution=1.3, recompute_graph=True)`; , I get the following error. The igraph I am using is V 0.1.11.; Many thanks; Hashem; `DeprecationWarning Traceback (most recent call last); <ipython-input-20-fb44185f2d28> in <module>(); 1 ; ----> 2 sc.tl.louvain(adata_corrected, n_neighbors=10, resolution=1.3, recompute_graph=True); 3 . /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/tools/louvain.py in louvain(adata, n_neighbors, resolution, n_pcs, random_state, flavor, directed, recompute_pca, recompute_distances, recompute_graph, n_dcs, n_jobs, copy); 78 directed = False; 79 if not directed: logg.m(' using the undirected graph', v=4); ---> 80 g = utils.get_igraph_from_adjacency(adjacency, directed=directed); 81 if flavor == 'vtraag':; 82 import louvain. /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/utils.py in get_igraph_from_adjacency(adjacency, directed); 41 def get_igraph_from_adjacency(adjacency, directed=None):; 42 """"""Get igraph graph from adjacency matrix.""""""; ---> 43 import igraph as ig; 44 sources, targets = adjacency.nonzero(); 45 weights = adjacency[sources, targets]. /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/igraph/__init__.py in <module>(); 6 __license__ = ""MIT""; 7 ; ----> 8 raise DeprecationWarning(""To avoid name collision with the igraph project, ""; 9 ""this visualization library has been renamed to ""; 10 ""'jgraph'. Please upgrade when convenient.""). DeprecationWarning: To avoid name collision with the igraph project, this visualization library has been renamed to 'jgraph'. Please upgrade when convenient.`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/35#issuecomment-324587457
https://github.com/scverse/scanpy/issues/35#issuecomment-324587457:487,Security,hash,hashem,487,"Hi Alex, ; The psutil issue by updating it has apparently gone away, however later on when I call `sc.tl.louvain(adata_corrected, n_neighbors=10, resolution=1.3, recompute_graph=True)`; , I get the following error. The igraph I am using is V 0.1.11.; Many thanks; Hashem; `DeprecationWarning Traceback (most recent call last); <ipython-input-20-fb44185f2d28> in <module>(); 1 ; ----> 2 sc.tl.louvain(adata_corrected, n_neighbors=10, resolution=1.3, recompute_graph=True); 3 . /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/tools/louvain.py in louvain(adata, n_neighbors, resolution, n_pcs, random_state, flavor, directed, recompute_pca, recompute_distances, recompute_graph, n_dcs, n_jobs, copy); 78 directed = False; 79 if not directed: logg.m(' using the undirected graph', v=4); ---> 80 g = utils.get_igraph_from_adjacency(adjacency, directed=directed); 81 if flavor == 'vtraag':; 82 import louvain. /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/utils.py in get_igraph_from_adjacency(adjacency, directed); 41 def get_igraph_from_adjacency(adjacency, directed=None):; 42 """"""Get igraph graph from adjacency matrix.""""""; ---> 43 import igraph as ig; 44 sources, targets = adjacency.nonzero(); 45 weights = adjacency[sources, targets]. /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/igraph/__init__.py in <module>(); 6 __license__ = ""MIT""; 7 ; ----> 8 raise DeprecationWarning(""To avoid name collision with the igraph project, ""; 9 ""this visualization library has been renamed to ""; 10 ""'jgraph'. Please upgrade when convenient.""). DeprecationWarning: To avoid name collision with the igraph project, this visualization library has been renamed to 'jgraph'. Please upgrade when convenient.`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/35#issuecomment-324587457
https://github.com/scverse/scanpy/issues/35#issuecomment-324587457:932,Security,hash,hashem,932,"Hi Alex, ; The psutil issue by updating it has apparently gone away, however later on when I call `sc.tl.louvain(adata_corrected, n_neighbors=10, resolution=1.3, recompute_graph=True)`; , I get the following error. The igraph I am using is V 0.1.11.; Many thanks; Hashem; `DeprecationWarning Traceback (most recent call last); <ipython-input-20-fb44185f2d28> in <module>(); 1 ; ----> 2 sc.tl.louvain(adata_corrected, n_neighbors=10, resolution=1.3, recompute_graph=True); 3 . /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/tools/louvain.py in louvain(adata, n_neighbors, resolution, n_pcs, random_state, flavor, directed, recompute_pca, recompute_distances, recompute_graph, n_dcs, n_jobs, copy); 78 directed = False; 79 if not directed: logg.m(' using the undirected graph', v=4); ---> 80 g = utils.get_igraph_from_adjacency(adjacency, directed=directed); 81 if flavor == 'vtraag':; 82 import louvain. /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/utils.py in get_igraph_from_adjacency(adjacency, directed); 41 def get_igraph_from_adjacency(adjacency, directed=None):; 42 """"""Get igraph graph from adjacency matrix.""""""; ---> 43 import igraph as ig; 44 sources, targets = adjacency.nonzero(); 45 weights = adjacency[sources, targets]. /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/igraph/__init__.py in <module>(); 6 __license__ = ""MIT""; 7 ; ----> 8 raise DeprecationWarning(""To avoid name collision with the igraph project, ""; 9 ""this visualization library has been renamed to ""; 10 ""'jgraph'. Please upgrade when convenient.""). DeprecationWarning: To avoid name collision with the igraph project, this visualization library has been renamed to 'jgraph'. Please upgrade when convenient.`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/35#issuecomment-324587457
https://github.com/scverse/scanpy/issues/35#issuecomment-324587457:1282,Security,hash,hashem,1282,"Hi Alex, ; The psutil issue by updating it has apparently gone away, however later on when I call `sc.tl.louvain(adata_corrected, n_neighbors=10, resolution=1.3, recompute_graph=True)`; , I get the following error. The igraph I am using is V 0.1.11.; Many thanks; Hashem; `DeprecationWarning Traceback (most recent call last); <ipython-input-20-fb44185f2d28> in <module>(); 1 ; ----> 2 sc.tl.louvain(adata_corrected, n_neighbors=10, resolution=1.3, recompute_graph=True); 3 . /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/tools/louvain.py in louvain(adata, n_neighbors, resolution, n_pcs, random_state, flavor, directed, recompute_pca, recompute_distances, recompute_graph, n_dcs, n_jobs, copy); 78 directed = False; 79 if not directed: logg.m(' using the undirected graph', v=4); ---> 80 g = utils.get_igraph_from_adjacency(adjacency, directed=directed); 81 if flavor == 'vtraag':; 82 import louvain. /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/utils.py in get_igraph_from_adjacency(adjacency, directed); 41 def get_igraph_from_adjacency(adjacency, directed=None):; 42 """"""Get igraph graph from adjacency matrix.""""""; ---> 43 import igraph as ig; 44 sources, targets = adjacency.nonzero(); 45 weights = adjacency[sources, targets]. /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/igraph/__init__.py in <module>(); 6 __license__ = ""MIT""; 7 ; ----> 8 raise DeprecationWarning(""To avoid name collision with the igraph project, ""; 9 ""this visualization library has been renamed to ""; 10 ""'jgraph'. Please upgrade when convenient.""). DeprecationWarning: To avoid name collision with the igraph project, this visualization library has been renamed to 'jgraph'. Please upgrade when convenient.`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/35#issuecomment-324587457
https://github.com/scverse/scanpy/issues/35#issuecomment-324587457:756,Testability,log,logg,756,"Hi Alex, ; The psutil issue by updating it has apparently gone away, however later on when I call `sc.tl.louvain(adata_corrected, n_neighbors=10, resolution=1.3, recompute_graph=True)`; , I get the following error. The igraph I am using is V 0.1.11.; Many thanks; Hashem; `DeprecationWarning Traceback (most recent call last); <ipython-input-20-fb44185f2d28> in <module>(); 1 ; ----> 2 sc.tl.louvain(adata_corrected, n_neighbors=10, resolution=1.3, recompute_graph=True); 3 . /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/tools/louvain.py in louvain(adata, n_neighbors, resolution, n_pcs, random_state, flavor, directed, recompute_pca, recompute_distances, recompute_graph, n_dcs, n_jobs, copy); 78 directed = False; 79 if not directed: logg.m(' using the undirected graph', v=4); ---> 80 g = utils.get_igraph_from_adjacency(adjacency, directed=directed); 81 if flavor == 'vtraag':; 82 import louvain. /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/utils.py in get_igraph_from_adjacency(adjacency, directed); 41 def get_igraph_from_adjacency(adjacency, directed=None):; 42 """"""Get igraph graph from adjacency matrix.""""""; ---> 43 import igraph as ig; 44 sources, targets = adjacency.nonzero(); 45 weights = adjacency[sources, targets]. /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/igraph/__init__.py in <module>(); 6 __license__ = ""MIT""; 7 ; ----> 8 raise DeprecationWarning(""To avoid name collision with the igraph project, ""; 9 ""this visualization library has been renamed to ""; 10 ""'jgraph'. Please upgrade when convenient.""). DeprecationWarning: To avoid name collision with the igraph project, this visualization library has been renamed to 'jgraph'. Please upgrade when convenient.`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/35#issuecomment-324587457
https://github.com/scverse/scanpy/issues/35#issuecomment-324589126:16,Deployability,configurat,configuration,16,This is again a configuration problem. You don't have http://igraph.org/python/ installed. That's a library with thousands of users and citations. It has a very powerful and fast C++ core that allows treating dataset sizes with a million cells. I realize that I misspecified this in Scanpy's automatic installation in the requirements file. I just updated this and will push it to the master branch. You simply need to type `pip install python-igraph` and then everything should work.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/35#issuecomment-324589126
https://github.com/scverse/scanpy/issues/35#issuecomment-324589126:80,Deployability,install,installed,80,This is again a configuration problem. You don't have http://igraph.org/python/ installed. That's a library with thousands of users and citations. It has a very powerful and fast C++ core that allows treating dataset sizes with a million cells. I realize that I misspecified this in Scanpy's automatic installation in the requirements file. I just updated this and will push it to the master branch. You simply need to type `pip install python-igraph` and then everything should work.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/35#issuecomment-324589126
https://github.com/scverse/scanpy/issues/35#issuecomment-324589126:302,Deployability,install,installation,302,This is again a configuration problem. You don't have http://igraph.org/python/ installed. That's a library with thousands of users and citations. It has a very powerful and fast C++ core that allows treating dataset sizes with a million cells. I realize that I misspecified this in Scanpy's automatic installation in the requirements file. I just updated this and will push it to the master branch. You simply need to type `pip install python-igraph` and then everything should work.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/35#issuecomment-324589126
https://github.com/scverse/scanpy/issues/35#issuecomment-324589126:348,Deployability,update,updated,348,This is again a configuration problem. You don't have http://igraph.org/python/ installed. That's a library with thousands of users and citations. It has a very powerful and fast C++ core that allows treating dataset sizes with a million cells. I realize that I misspecified this in Scanpy's automatic installation in the requirements file. I just updated this and will push it to the master branch. You simply need to type `pip install python-igraph` and then everything should work.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/35#issuecomment-324589126
https://github.com/scverse/scanpy/issues/35#issuecomment-324589126:429,Deployability,install,install,429,This is again a configuration problem. You don't have http://igraph.org/python/ installed. That's a library with thousands of users and citations. It has a very powerful and fast C++ core that allows treating dataset sizes with a million cells. I realize that I misspecified this in Scanpy's automatic installation in the requirements file. I just updated this and will push it to the master branch. You simply need to type `pip install python-igraph` and then everything should work.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/35#issuecomment-324589126
https://github.com/scverse/scanpy/issues/35#issuecomment-324589126:161,Energy Efficiency,power,powerful,161,This is again a configuration problem. You don't have http://igraph.org/python/ installed. That's a library with thousands of users and citations. It has a very powerful and fast C++ core that allows treating dataset sizes with a million cells. I realize that I misspecified this in Scanpy's automatic installation in the requirements file. I just updated this and will push it to the master branch. You simply need to type `pip install python-igraph` and then everything should work.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/35#issuecomment-324589126
https://github.com/scverse/scanpy/issues/35#issuecomment-324589126:16,Modifiability,config,configuration,16,This is again a configuration problem. You don't have http://igraph.org/python/ installed. That's a library with thousands of users and citations. It has a very powerful and fast C++ core that allows treating dataset sizes with a million cells. I realize that I misspecified this in Scanpy's automatic installation in the requirements file. I just updated this and will push it to the master branch. You simply need to type `pip install python-igraph` and then everything should work.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/35#issuecomment-324589126
https://github.com/scverse/scanpy/issues/35#issuecomment-324589126:404,Usability,simpl,simply,404,This is again a configuration problem. You don't have http://igraph.org/python/ installed. That's a library with thousands of users and citations. It has a very powerful and fast C++ core that allows treating dataset sizes with a million cells. I realize that I misspecified this in Scanpy's automatic installation in the requirements file. I just updated this and will push it to the master branch. You simply need to type `pip install python-igraph` and then everything should work.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/35#issuecomment-324589126
https://github.com/scverse/scanpy/issues/35#issuecomment-324590937:55,Deployability,install,installed,55,I think I have the most recent version of python-graph installed ( python-igraph 0.7.1.post6 ).; pip install python-igraph doesnt solve the issue since it is already updated.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/35#issuecomment-324590937
https://github.com/scverse/scanpy/issues/35#issuecomment-324590937:101,Deployability,install,install,101,I think I have the most recent version of python-graph installed ( python-igraph 0.7.1.post6 ).; pip install python-igraph doesnt solve the issue since it is already updated.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/35#issuecomment-324590937
https://github.com/scverse/scanpy/issues/35#issuecomment-324590937:166,Deployability,update,updated,166,I think I have the most recent version of python-graph installed ( python-igraph 0.7.1.post6 ).; pip install python-igraph doesnt solve the issue since it is already updated.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/35#issuecomment-324590937
https://github.com/scverse/scanpy/issues/35#issuecomment-324615579:88,Deployability,install,installing,88,"I think this was happening because of confusion between igraph and python-igraph, by un-installing both and reinstalling just python-igraph, the issue was solved. Interesting enough installation with conda didnt work, only installation with pip worked.; thanks again.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/35#issuecomment-324615579
https://github.com/scverse/scanpy/issues/35#issuecomment-324615579:182,Deployability,install,installation,182,"I think this was happening because of confusion between igraph and python-igraph, by un-installing both and reinstalling just python-igraph, the issue was solved. Interesting enough installation with conda didnt work, only installation with pip worked.; thanks again.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/35#issuecomment-324615579
https://github.com/scverse/scanpy/issues/35#issuecomment-324615579:223,Deployability,install,installation,223,"I think this was happening because of confusion between igraph and python-igraph, by un-installing both and reinstalling just python-igraph, the issue was solved. Interesting enough installation with conda didnt work, only installation with pip worked.; thanks again.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/35#issuecomment-324615579
https://github.com/scverse/scanpy/issues/35#issuecomment-324638985:345,Availability,mainten,maintenance,345,"Sorry for being offline for a few hours and great that you could resolve it. In the next version, this confusion will not appear again. Even though, obviously, any Scanpy release reproduces all examples on https://github.com/theislab/scanpy_usage, we're still at an early stage in the package. Things are progressing very fast and structure and maintenance of the package are becoming more and more professional. Also, soon, many new features and examples where other Python packages are used will be added. Thank you for a bit of patience at this stage.; Alex. PS: We now have an initial version of the documentation: https://scanpy.readthedocs.io.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/35#issuecomment-324638985
https://github.com/scverse/scanpy/issues/35#issuecomment-324638985:171,Deployability,release,release,171,"Sorry for being offline for a few hours and great that you could resolve it. In the next version, this confusion will not appear again. Even though, obviously, any Scanpy release reproduces all examples on https://github.com/theislab/scanpy_usage, we're still at an early stage in the package. Things are progressing very fast and structure and maintenance of the package are becoming more and more professional. Also, soon, many new features and examples where other Python packages are used will be added. Thank you for a bit of patience at this stage.; Alex. PS: We now have an initial version of the documentation: https://scanpy.readthedocs.io.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/35#issuecomment-324638985
https://github.com/scverse/scanpy/issues/35#issuecomment-324641466:254,Deployability,configurat,configuration,254,No worries and thank you for usually very prompt suggestions.; The idea that scanpy can handle many cells efficiently is great and therefore I have been trying it in a computing cluster (and not my local machine) for the future usage. This in turn makes configuration just a bit more difficult. ; Looking forward to a more stable version with more added function.; Thank you; Hashem,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/35#issuecomment-324641466
https://github.com/scverse/scanpy/issues/35#issuecomment-324641466:106,Energy Efficiency,efficient,efficiently,106,No worries and thank you for usually very prompt suggestions.; The idea that scanpy can handle many cells efficiently is great and therefore I have been trying it in a computing cluster (and not my local machine) for the future usage. This in turn makes configuration just a bit more difficult. ; Looking forward to a more stable version with more added function.; Thank you; Hashem,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/35#issuecomment-324641466
https://github.com/scverse/scanpy/issues/35#issuecomment-324641466:254,Modifiability,config,configuration,254,No worries and thank you for usually very prompt suggestions.; The idea that scanpy can handle many cells efficiently is great and therefore I have been trying it in a computing cluster (and not my local machine) for the future usage. This in turn makes configuration just a bit more difficult. ; Looking forward to a more stable version with more added function.; Thank you; Hashem,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/35#issuecomment-324641466
https://github.com/scverse/scanpy/issues/35#issuecomment-324641466:376,Security,Hash,Hashem,376,No worries and thank you for usually very prompt suggestions.; The idea that scanpy can handle many cells efficiently is great and therefore I have been trying it in a computing cluster (and not my local machine) for the future usage. This in turn makes configuration just a bit more difficult. ; Looking forward to a more stable version with more added function.; Thank you; Hashem,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/35#issuecomment-324641466
https://github.com/scverse/scanpy/pull/80#issuecomment-364153382:114,Availability,error,error,114,"Thank you! So you say it doesn’t work, but I see a green checkmark. Would you mind adding a test that exposes the error?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/80#issuecomment-364153382
https://github.com/scverse/scanpy/pull/80#issuecomment-364153382:51,Energy Efficiency,green,green,51,"Thank you! So you say it doesn’t work, but I see a green checkmark. Would you mind adding a test that exposes the error?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/80#issuecomment-364153382
https://github.com/scverse/scanpy/pull/80#issuecomment-364153382:102,Security,expose,exposes,102,"Thank you! So you say it doesn’t work, but I see a green checkmark. Would you mind adding a test that exposes the error?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/80#issuecomment-364153382
https://github.com/scverse/scanpy/pull/80#issuecomment-364153382:92,Testability,test,test,92,"Thank you! So you say it doesn’t work, but I see a green checkmark. Would you mind adding a test that exposes the error?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/80#issuecomment-364153382
https://github.com/scverse/scanpy/pull/80#issuecomment-364154163:59,Testability,test,testing,59,No problem - is there a standard input dataset you use for testing? Otherwise I can just use one I have on-hand.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/80#issuecomment-364154163
https://github.com/scverse/scanpy/pull/80#issuecomment-364159311:159,Availability,down,downloaded,159,"We have a few built in: https://github.com/theislab/scanpy/blob/master/scanpy/datasets/builtin.py. But AFAIK there’s no scRNA-Seq ones that aren’t dynamically downloaded. I think for tests we should add a small built-in one, and make sure it doesn’t end up in the binary wheels when building.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/80#issuecomment-364159311
https://github.com/scverse/scanpy/pull/80#issuecomment-364159311:183,Testability,test,tests,183,"We have a few built in: https://github.com/theislab/scanpy/blob/master/scanpy/datasets/builtin.py. But AFAIK there’s no scRNA-Seq ones that aren’t dynamically downloaded. I think for tests we should add a small built-in one, and make sure it doesn’t end up in the binary wheels when building.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/80#issuecomment-364159311
https://github.com/scverse/scanpy/pull/80#issuecomment-364331145:12,Availability,down,downloaded,12,"`paul15` is downloaded automatically, very practical.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/80#issuecomment-364331145
https://github.com/scverse/scanpy/pull/80#issuecomment-364331847:48,Availability,down,downloaded,48,"Ah, I didn't know others are also automatically downloaded, very nice. paul15 uses `sc.utils.check_presence_download()`, while others use `check_datafile_present_and_download()` via `readwrite.read()`, though. Is this intentional?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/80#issuecomment-364331847
https://github.com/scverse/scanpy/pull/80#issuecomment-364372580:14,Availability,down,downloaded,14,"> `paul15` is downloaded automatically, very practical. Yeah, it’s really cool for interactive use, but not for automated testing / continuous integration I think.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/80#issuecomment-364372580
https://github.com/scverse/scanpy/pull/80#issuecomment-364372580:132,Deployability,continuous,continuous,132,"> `paul15` is downloaded automatically, very practical. Yeah, it’s really cool for interactive use, but not for automated testing / continuous integration I think.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/80#issuecomment-364372580
https://github.com/scverse/scanpy/pull/80#issuecomment-364372580:143,Deployability,integrat,integration,143,"> `paul15` is downloaded automatically, very practical. Yeah, it’s really cool for interactive use, but not for automated testing / continuous integration I think.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/80#issuecomment-364372580
https://github.com/scverse/scanpy/pull/80#issuecomment-364372580:143,Integrability,integrat,integration,143,"> `paul15` is downloaded automatically, very practical. Yeah, it’s really cool for interactive use, but not for automated testing / continuous integration I think.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/80#issuecomment-364372580
https://github.com/scverse/scanpy/pull/80#issuecomment-364372580:122,Testability,test,testing,122,"> `paul15` is downloaded automatically, very practical. Yeah, it’s really cool for interactive use, but not for automated testing / continuous integration I think.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/80#issuecomment-364372580
https://github.com/scverse/scanpy/pull/80#issuecomment-364468317:1133,Availability,error,error,1133,"Here's [test code](https://gist.github.com/jorvis/da877d89fd159b2fb7dfba26705f7ceb) and my output is:. ```pytb; Initial shape: 737280x28002; After min_genes: 5128x28002; After max_genes: 1431x28002; Traceback (most recent call last):; File ""/tmp/test_cell_and_gene_filter.py"", line 22, in <module>; sc.pp.filter_genes(adata, min_cells=3); File ""/home/jorvis/git/scanpy/scanpy/preprocessing/simple.py"", line 152, in filter_genes; adata.var['n_cells'] = number; File ""/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py"", line 2519, in __setitem__; self._set_item(key, value); File ""/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py"", line 2585, in _set_item; value = self._sanitize_column(key, value); File ""/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py"", line 2760, in _sanitize_column; value = _sanitize_index(value, self.index, copy=False); File ""/usr/local/lib/python3.6/dist-packages/pandas/core/series.py"", line 3121, in _sanitize_index; raise ValueError('Length of values does not match length of ' 'index'); ValueError: Length of values does not match length of index; ```. Note that this same error displays on both of the following lines:. ```python; sc.pp.filter_genes(adata, min_cells=3); sc.pp.filter_genes(adata, max_cells=1000); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/80#issuecomment-364468317
https://github.com/scverse/scanpy/pull/80#issuecomment-364468317:8,Testability,test,test,8,"Here's [test code](https://gist.github.com/jorvis/da877d89fd159b2fb7dfba26705f7ceb) and my output is:. ```pytb; Initial shape: 737280x28002; After min_genes: 5128x28002; After max_genes: 1431x28002; Traceback (most recent call last):; File ""/tmp/test_cell_and_gene_filter.py"", line 22, in <module>; sc.pp.filter_genes(adata, min_cells=3); File ""/home/jorvis/git/scanpy/scanpy/preprocessing/simple.py"", line 152, in filter_genes; adata.var['n_cells'] = number; File ""/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py"", line 2519, in __setitem__; self._set_item(key, value); File ""/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py"", line 2585, in _set_item; value = self._sanitize_column(key, value); File ""/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py"", line 2760, in _sanitize_column; value = _sanitize_index(value, self.index, copy=False); File ""/usr/local/lib/python3.6/dist-packages/pandas/core/series.py"", line 3121, in _sanitize_index; raise ValueError('Length of values does not match length of ' 'index'); ValueError: Length of values does not match length of index; ```. Note that this same error displays on both of the following lines:. ```python; sc.pp.filter_genes(adata, min_cells=3); sc.pp.filter_genes(adata, max_cells=1000); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/80#issuecomment-364468317
https://github.com/scverse/scanpy/pull/80#issuecomment-364468317:390,Usability,simpl,simple,390,"Here's [test code](https://gist.github.com/jorvis/da877d89fd159b2fb7dfba26705f7ceb) and my output is:. ```pytb; Initial shape: 737280x28002; After min_genes: 5128x28002; After max_genes: 1431x28002; Traceback (most recent call last):; File ""/tmp/test_cell_and_gene_filter.py"", line 22, in <module>; sc.pp.filter_genes(adata, min_cells=3); File ""/home/jorvis/git/scanpy/scanpy/preprocessing/simple.py"", line 152, in filter_genes; adata.var['n_cells'] = number; File ""/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py"", line 2519, in __setitem__; self._set_item(key, value); File ""/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py"", line 2585, in _set_item; value = self._sanitize_column(key, value); File ""/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py"", line 2760, in _sanitize_column; value = _sanitize_index(value, self.index, copy=False); File ""/usr/local/lib/python3.6/dist-packages/pandas/core/series.py"", line 3121, in _sanitize_index; raise ValueError('Length of values does not match length of ' 'index'); ValueError: Length of values does not match length of index; ```. Note that this same error displays on both of the following lines:. ```python; sc.pp.filter_genes(adata, min_cells=3); sc.pp.filter_genes(adata, max_cells=1000); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/80#issuecomment-364468317
https://github.com/scverse/scanpy/pull/80#issuecomment-367706813:70,Availability,failure,failures,70,"Hmmm, you got it to successfully run? The last thing I had posted was failures in testing (see above)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/80#issuecomment-367706813
https://github.com/scverse/scanpy/pull/80#issuecomment-367706813:82,Testability,test,testing,82,"Hmmm, you got it to successfully run? The last thing I had posted was failures in testing (see above)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/80#issuecomment-367706813
https://github.com/scverse/scanpy/pull/80#issuecomment-368081444:22,Usability,clear,clear,22,"Oh, damn... It wasn't clear to me that you were actually saying that this is broke... Sorry, I went over a few pull requests too quickly. Anyways, so the master branch was broke for a day. It's resolved in https://github.com/theislab/scanpy/commit/96890730972162aa531c3289b38ad728a7585c85. The next pull request goes smoother... :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/80#issuecomment-368081444
https://github.com/scverse/scanpy/pull/82#issuecomment-364915958:177,Deployability,release,release,177,"Dear Davide, again sorry for the late response... I'll, of course, merge this and you can directly push such small things on the master... . In the meanwhile, I've made another release (see notes on https://scanpy.readthedocs.io). It would be cool to further work on the gene scoring with the small amendments (default gene list class, a small notebook with a use case) to get this in a new release. Also, in this context. I know think that `score_genes` instead of `score_gene_lists` would be better. The `_lists` does not contain actual information and is self-understood. It would also fit nicely with other functions (`cluster_genes`, e.g.). Until all of this is fixed, I removed this stuff from the API docs... Have a good start into the week!; Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/82#issuecomment-364915958
https://github.com/scverse/scanpy/pull/82#issuecomment-364915958:391,Deployability,release,release,391,"Dear Davide, again sorry for the late response... I'll, of course, merge this and you can directly push such small things on the master... . In the meanwhile, I've made another release (see notes on https://scanpy.readthedocs.io). It would be cool to further work on the gene scoring with the small amendments (default gene list class, a small notebook with a use case) to get this in a new release. Also, in this context. I know think that `score_genes` instead of `score_gene_lists` would be better. The `_lists` does not contain actual information and is self-understood. It would also fit nicely with other functions (`cluster_genes`, e.g.). Until all of this is fixed, I removed this stuff from the API docs... Have a good start into the week!; Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/82#issuecomment-364915958
https://github.com/scverse/scanpy/pull/82#issuecomment-368930312:36,Availability,error,error,36,"Hi, . I just stumbled upon the same error, maybe due to the installation method.; Anyway, I got it fixed but since my package was outdated and it's been a while since I used python maybe the next point as already been solved:. -typo issue in the notebook examples (phase instead of Phase); at In [8] when you call the pca.scatter function with color 'phase'. > ValueError: ""phase"" is invalid! specify valid sample annotation, one of ['n_genes', 'percent_mito', 'n_counts', 'dropouts', 'complexity', 'S_score', 'G2M_score', 'Phase', 'X_diffmap0', 'louvain_groups']. Very nice package and notebooks BTW,; Raphaël",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/82#issuecomment-368930312
https://github.com/scverse/scanpy/pull/82#issuecomment-368930312:60,Deployability,install,installation,60,"Hi, . I just stumbled upon the same error, maybe due to the installation method.; Anyway, I got it fixed but since my package was outdated and it's been a while since I used python maybe the next point as already been solved:. -typo issue in the notebook examples (phase instead of Phase); at In [8] when you call the pca.scatter function with color 'phase'. > ValueError: ""phase"" is invalid! specify valid sample annotation, one of ['n_genes', 'percent_mito', 'n_counts', 'dropouts', 'complexity', 'S_score', 'G2M_score', 'Phase', 'X_diffmap0', 'louvain_groups']. Very nice package and notebooks BTW,; Raphaël",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/82#issuecomment-368930312
https://github.com/scverse/scanpy/pull/82#issuecomment-368964431:75,Availability,error,error,75,"Hi Raphaël!. Thanks!. What do you mean with ""I just stumbled upon the same error, maybe due to the installation method."" - which error?. Regarding the typo: Hm, are you running Scanpy 0.4.4; if you run an early version, this was 'Phase' with a captical 'P'; since 0.4.3+7 it's 'phase'; like all the other annotations. Cheers,; Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/82#issuecomment-368964431
https://github.com/scverse/scanpy/pull/82#issuecomment-368964431:129,Availability,error,error,129,"Hi Raphaël!. Thanks!. What do you mean with ""I just stumbled upon the same error, maybe due to the installation method."" - which error?. Regarding the typo: Hm, are you running Scanpy 0.4.4; if you run an early version, this was 'Phase' with a captical 'P'; since 0.4.3+7 it's 'phase'; like all the other annotations. Cheers,; Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/82#issuecomment-368964431
https://github.com/scverse/scanpy/pull/82#issuecomment-368964431:99,Deployability,install,installation,99,"Hi Raphaël!. Thanks!. What do you mean with ""I just stumbled upon the same error, maybe due to the installation method."" - which error?. Regarding the typo: Hm, are you running Scanpy 0.4.4; if you run an early version, this was 'Phase' with a captical 'P'; since 0.4.3+7 it's 'phase'; like all the other annotations. Cheers,; Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/82#issuecomment-368964431
https://github.com/scverse/scanpy/pull/83#issuecomment-364916493:94,Modifiability,variab,variable,94,"Just push things like this on the master branch. :wink: And, consider using `'a string with a variable: {}'.format(variable)` for formatting strings. :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/83#issuecomment-364916493
https://github.com/scverse/scanpy/pull/83#issuecomment-364916493:115,Modifiability,variab,variable,115,"Just push things like this on the master branch. :wink: And, consider using `'a string with a variable: {}'.format(variable)` for formatting strings. :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/83#issuecomment-364916493
https://github.com/scverse/scanpy/issues/85#issuecomment-365873899:239,Security,expose,expose,239,"Hi Joshua, can you upload an example dataset somewhere? So that I can reproduce the figure above? I'm confident that I can speed this up...; PS: Still consolidating all the gene correlation stuff... Everything works, but we do not want to expose things to the user that have not been checked 3 times... in particular the conventions need to be intuitive etc.; PPS: The new cell cycle example could interest you:; https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.score_genes.html; https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.score_genes_cell_cycle.html; Both link to the notebook in the ""Examples"" section; https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/85#issuecomment-365873899
https://github.com/scverse/scanpy/issues/85#issuecomment-365873899:344,Usability,intuit,intuitive,344,"Hi Joshua, can you upload an example dataset somewhere? So that I can reproduce the figure above? I'm confident that I can speed this up...; PS: Still consolidating all the gene correlation stuff... Everything works, but we do not want to expose things to the user that have not been checked 3 times... in particular the conventions need to be intuitive etc.; PPS: The new cell cycle example could interest you:; https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.score_genes.html; https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.score_genes_cell_cycle.html; Both link to the notebook in the ""Examples"" section; https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/85#issuecomment-365873899
https://github.com/scverse/scanpy/issues/85#issuecomment-367770658:179,Usability,learn,learn,179,"Thinking about this more I think ideally we wouldn't really on the obs column name ""--"" convention and rather place and filter this based on data put in .uns, right? Just have to learn how to do that ....",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/85#issuecomment-367770658
https://github.com/scverse/scanpy/issues/85#issuecomment-370141359:762,Energy Efficiency,efficient,efficient,762,"Hi Joshua,. ok, first of all, you should store the group id as a field in the annotation of observations/cells (`.obs`). Taking your AnnData, you'd do the following:; ```; adata.obs['mygroups'] = [name.split('--')[0] for name in adata.obs_names]; ```; You can then do ; ```; sc.pl.violin(adata, 'mygene', group_by='mygroups'); ```; as in the [standard example](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb), box [32]. Instead of 'mygene', you can use any annotation key; for instance gene scores, as produced by `sc.pp.score_genes`. Or averages over genes `adata.obs['my_gene_set_avg'] = adata[:, gene_set].X.mean(axis=1)`. Does this help or did I misunderstand something?. PS: As a principle rule for writing efficient python code: never loop over more than a 100 items... absolutely never use nested loops. :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/85#issuecomment-370141359
https://github.com/scverse/scanpy/issues/85#issuecomment-370200511:76,Deployability,integrat,integrated,76,"Thanks, that's all very helpful. I'll work on getting these recommendations integrated. One quick question, is this still the most efficient way to get the data for one gene for the violin plot?. selected = adata[:, adata.var_names.isin([gene.gene_symbol,])]. And before all the data was just in relational tables but, of course, the scale was a lot less. EDIT: I just re-read and saw that it seems you can pass the gene name to the violin() call itself. Beautiful magic.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/85#issuecomment-370200511
https://github.com/scverse/scanpy/issues/85#issuecomment-370200511:131,Energy Efficiency,efficient,efficient,131,"Thanks, that's all very helpful. I'll work on getting these recommendations integrated. One quick question, is this still the most efficient way to get the data for one gene for the violin plot?. selected = adata[:, adata.var_names.isin([gene.gene_symbol,])]. And before all the data was just in relational tables but, of course, the scale was a lot less. EDIT: I just re-read and saw that it seems you can pass the gene name to the violin() call itself. Beautiful magic.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/85#issuecomment-370200511
https://github.com/scverse/scanpy/issues/85#issuecomment-370200511:76,Integrability,integrat,integrated,76,"Thanks, that's all very helpful. I'll work on getting these recommendations integrated. One quick question, is this still the most efficient way to get the data for one gene for the violin plot?. selected = adata[:, adata.var_names.isin([gene.gene_symbol,])]. And before all the data was just in relational tables but, of course, the scale was a lot less. EDIT: I just re-read and saw that it seems you can pass the gene name to the violin() call itself. Beautiful magic.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/85#issuecomment-370200511
https://github.com/scverse/scanpy/issues/85#issuecomment-370355474:9,Usability,simpl,simply,9,"Yes, you simply pass a single gene name to the violin plot... :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/85#issuecomment-370355474
https://github.com/scverse/scanpy/issues/85#issuecomment-370859955:165,Security,access,access,165,"I wanted to report success here. I had to change your references to var_names above to obs_names but got it working after that. Now I just need to figure out how to access the plot customization methods so I can clean it up. The labels are all blurred together, for example.; ![screenshot from 2018-03-06 11-16-16](https://user-images.githubusercontent.com/330899/37047580-01c7b8e2-2131-11e8-9bb7-d060bfc6d0d6.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/85#issuecomment-370859955
https://github.com/scverse/scanpy/issues/85#issuecomment-371026156:51,Availability,failure,failures,51,"I was following the documentation and kept getting failures when I tried to pass additional args to violin() via kwargs, and found that kwargs was only [added to violin 6 days ago](https://github.com/theislab/scanpy/blame/master/scanpy/plotting/anndata.py#L347). Yay (and thanks) for the updates)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/85#issuecomment-371026156
https://github.com/scverse/scanpy/issues/85#issuecomment-371026156:288,Deployability,update,updates,288,"I was following the documentation and kept getting failures when I tried to pass additional args to violin() via kwargs, and found that kwargs was only [added to violin 6 days ago](https://github.com/theislab/scanpy/blame/master/scanpy/plotting/anndata.py#L347). Yay (and thanks) for the updates)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/85#issuecomment-371026156
https://github.com/scverse/scanpy/pull/87#issuecomment-366189008:58,Usability,learn,learn,58,"Thank you! I'll rename the result according to the scikit-learn convention, though. If they call it `explained_variance_`, we should also call it that way - we can add in the docs, that it equals the eigenvalues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/87#issuecomment-366189008
https://github.com/scverse/scanpy/issues/88#issuecomment-366284420:92,Availability,error,error,92,Never mind... I just can't seem to read the difference between 'on_data' and 'on data'. The error didn't really help I guess.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/88#issuecomment-366284420
https://github.com/scverse/scanpy/issues/88#issuecomment-366287782:33,Availability,error,error,33,:smile: Now this raises a proper error message: https://github.com/theislab/scanpy/commit/2490bec27c1c37e1388cb1da44369c81e176df6c,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/88#issuecomment-366287782
https://github.com/scverse/scanpy/issues/88#issuecomment-366287782:39,Integrability,message,message,39,:smile: Now this raises a proper error message: https://github.com/theislab/scanpy/commit/2490bec27c1c37e1388cb1da44369c81e176df6c,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/88#issuecomment-366287782
https://github.com/scverse/scanpy/issues/88#issuecomment-366300686:19,Availability,error,error,19,"No, the matplotlib error message was really confusing... the 'on data' and 'right margin' locations are scanpy features and should be in the error message... wanted to do this anyways. :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/88#issuecomment-366300686
https://github.com/scverse/scanpy/issues/88#issuecomment-366300686:141,Availability,error,error,141,"No, the matplotlib error message was really confusing... the 'on data' and 'right margin' locations are scanpy features and should be in the error message... wanted to do this anyways. :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/88#issuecomment-366300686
https://github.com/scverse/scanpy/issues/88#issuecomment-366300686:25,Integrability,message,message,25,"No, the matplotlib error message was really confusing... the 'on data' and 'right margin' locations are scanpy features and should be in the error message... wanted to do this anyways. :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/88#issuecomment-366300686
https://github.com/scverse/scanpy/issues/88#issuecomment-366300686:147,Integrability,message,message,147,"No, the matplotlib error message was really confusing... the 'on data' and 'right margin' locations are scanpy features and should be in the error message... wanted to do this anyways. :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/88#issuecomment-366300686
https://github.com/scverse/scanpy/issues/90#issuecomment-367380705:10,Deployability,upgrade,upgrade,10,"Oh! I did upgrade pip and all the packages needed by scanpy, but didn't have the idea of doing:. pip3 install --upgrade setuptools. This fixed it! Many thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/90#issuecomment-367380705
https://github.com/scverse/scanpy/issues/90#issuecomment-367380705:102,Deployability,install,install,102,"Oh! I did upgrade pip and all the packages needed by scanpy, but didn't have the idea of doing:. pip3 install --upgrade setuptools. This fixed it! Many thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/90#issuecomment-367380705
https://github.com/scverse/scanpy/issues/90#issuecomment-367380705:112,Deployability,upgrade,upgrade,112,"Oh! I did upgrade pip and all the packages needed by scanpy, but didn't have the idea of doing:. pip3 install --upgrade setuptools. This fixed it! Many thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/90#issuecomment-367380705
https://github.com/scverse/scanpy/pull/93#issuecomment-367725475:124,Modifiability,variab,variable,124,"Also `groups_names` was undefined in the same function, I changed it to `groups_order` but I'm not sure if it's the correct variable.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/93#issuecomment-367725475
https://github.com/scverse/scanpy/issues/94#issuecomment-369132536:357,Availability,error,error,357,"Hm, strange; how did you manage to obtain integer cluster names? Did you manually rename them? `adata.obs['louvain_groups'].cat.categories` should always be strings. If you use integers, you should also pass `reference=18` and not `reference='18'`. But the use of integers is strongly discouraged and probably not stable... I only could reproduce a similar error when running `sc.tl.rank_genes_groups(adata, 'louvain_groups', groups=['0'], reference='9')` in the [clustering example notebook](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb):. ```pytb; ValueError: reference = 9 needs to be one of group_by = ['0', '1', '2', '3', '4', '5', '6', '7'].; ```. PS: The error is raised by the following. ```py; if (reference != 'rest' ; and reference not in set(adata.obs[group_by].cat.categories)): ; raise ValueError('reference = {} needs to be one of group_by = {}.' ; .format(reference, ; adata.obs[group_by].cat.categories.tolist())); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/94#issuecomment-369132536
https://github.com/scverse/scanpy/issues/94#issuecomment-369132536:714,Availability,error,error,714,"Hm, strange; how did you manage to obtain integer cluster names? Did you manually rename them? `adata.obs['louvain_groups'].cat.categories` should always be strings. If you use integers, you should also pass `reference=18` and not `reference='18'`. But the use of integers is strongly discouraged and probably not stable... I only could reproduce a similar error when running `sc.tl.rank_genes_groups(adata, 'louvain_groups', groups=['0'], reference='9')` in the [clustering example notebook](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb):. ```pytb; ValueError: reference = 9 needs to be one of group_by = ['0', '1', '2', '3', '4', '5', '6', '7'].; ```. PS: The error is raised by the following. ```py; if (reference != 'rest' ; and reference not in set(adata.obs[group_by].cat.categories)): ; raise ValueError('reference = {} needs to be one of group_by = {}.' ; .format(reference, ; adata.obs[group_by].cat.categories.tolist())); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/94#issuecomment-369132536
https://github.com/scverse/scanpy/issues/94#issuecomment-370140266:606,Deployability,install,install,606,"This is very strange... Do you have this issue also in the [standard tutorial](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb)? Of course, the louvain function produces string-named categories, see [here](https://github.com/theislab/scanpy/blob/5299c6caaec6402513f1e0442186350787177d2c/scanpy/tools/louvain.py#L135) and has always done so. I'm puzzled that the `.astype('U')`, where the `'U'` stands for unicode-string, seems to have no effect in your version of Scanpy. Do you use the most recent version (0.4.4) and recent dependencies? If not, run `pip install --upgrade scanpy`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/94#issuecomment-370140266
https://github.com/scverse/scanpy/issues/94#issuecomment-370140266:616,Deployability,upgrade,upgrade,616,"This is very strange... Do you have this issue also in the [standard tutorial](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb)? Of course, the louvain function produces string-named categories, see [here](https://github.com/theislab/scanpy/blob/5299c6caaec6402513f1e0442186350787177d2c/scanpy/tools/louvain.py#L135) and has always done so. I'm puzzled that the `.astype('U')`, where the `'U'` stands for unicode-string, seems to have no effect in your version of Scanpy. Do you use the most recent version (0.4.4) and recent dependencies? If not, run `pip install --upgrade scanpy`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/94#issuecomment-370140266
https://github.com/scverse/scanpy/issues/94#issuecomment-370140266:575,Integrability,depend,dependencies,575,"This is very strange... Do you have this issue also in the [standard tutorial](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb)? Of course, the louvain function produces string-named categories, see [here](https://github.com/theislab/scanpy/blob/5299c6caaec6402513f1e0442186350787177d2c/scanpy/tools/louvain.py#L135) and has always done so. I'm puzzled that the `.astype('U')`, where the `'U'` stands for unicode-string, seems to have no effect in your version of Scanpy. Do you use the most recent version (0.4.4) and recent dependencies? If not, run `pip install --upgrade scanpy`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/94#issuecomment-370140266
https://github.com/scverse/scanpy/issues/94#issuecomment-370147087:82,Usability,simpl,simply,82,Thank you for this; maybe there was one version where this was inconsistent and I simply don't remember... @AnatoleKing have you used version 0.4.4?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/94#issuecomment-370147087
https://github.com/scverse/scanpy/issues/94#issuecomment-370292754:44,Deployability,update,update,44,"@falexwolf , I used '0.4.2' version, I will update to the latest.; Thanks, scanpy is powerful tool for single cell RNASeq data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/94#issuecomment-370292754
https://github.com/scverse/scanpy/issues/94#issuecomment-370292754:85,Energy Efficiency,power,powerful,85,"@falexwolf , I used '0.4.2' version, I will update to the latest.; Thanks, scanpy is powerful tool for single cell RNASeq data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/94#issuecomment-370292754
https://github.com/scverse/scanpy/issues/95#issuecomment-369860454:324,Deployability,update,updated,324,"Hi Davide,. I like the preprint and the blog post. I agree that differential expression testing deserves a classification perspective. Coincidentally, we (with @tcallies) were also working on a little paper that makes this point but used neither logistic regression nor TCCs as covariates... unfortunately, we still haven't updated our benchmarks, but I'd assume that what Lior Pachter does works best. :smile:. Anyways, yes, we should include it at some point but let's still collect some experience... Until then, people can use your two-line workaround. :wink:. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/95#issuecomment-369860454
https://github.com/scverse/scanpy/issues/95#issuecomment-369860454:88,Testability,test,testing,88,"Hi Davide,. I like the preprint and the blog post. I agree that differential expression testing deserves a classification perspective. Coincidentally, we (with @tcallies) were also working on a little paper that makes this point but used neither logistic regression nor TCCs as covariates... unfortunately, we still haven't updated our benchmarks, but I'd assume that what Lior Pachter does works best. :smile:. Anyways, yes, we should include it at some point but let's still collect some experience... Until then, people can use your two-line workaround. :wink:. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/95#issuecomment-369860454
https://github.com/scverse/scanpy/issues/95#issuecomment-369860454:246,Testability,log,logistic,246,"Hi Davide,. I like the preprint and the blog post. I agree that differential expression testing deserves a classification perspective. Coincidentally, we (with @tcallies) were also working on a little paper that makes this point but used neither logistic regression nor TCCs as covariates... unfortunately, we still haven't updated our benchmarks, but I'd assume that what Lior Pachter does works best. :smile:. Anyways, yes, we should include it at some point but let's still collect some experience... Until then, people can use your two-line workaround. :wink:. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/95#issuecomment-369860454
https://github.com/scverse/scanpy/issues/95#issuecomment-369860454:336,Testability,benchmark,benchmarks,336,"Hi Davide,. I like the preprint and the blog post. I agree that differential expression testing deserves a classification perspective. Coincidentally, we (with @tcallies) were also working on a little paper that makes this point but used neither logistic regression nor TCCs as covariates... unfortunately, we still haven't updated our benchmarks, but I'd assume that what Lior Pachter does works best. :smile:. Anyways, yes, we should include it at some point but let's still collect some experience... Until then, people can use your two-line workaround. :wink:. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/95#issuecomment-369860454
https://github.com/scverse/scanpy/issues/95#issuecomment-369863247:110,Integrability,wrap,wrapper,110,"Agreed. I don’t think we should rush and include everything into scanpy, especially when it would be a simple wrapper of something existing.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/95#issuecomment-369863247
https://github.com/scverse/scanpy/issues/95#issuecomment-369863247:103,Usability,simpl,simple,103,"Agreed. I don’t think we should rush and include everything into scanpy, especially when it would be a simple wrapper of something existing.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/95#issuecomment-369863247
https://github.com/scverse/scanpy/issues/96#issuecomment-370139940:479,Usability,clear,clearer,479,"Hi!. Thanks for reaching out!. We have an option to compute connectivity based on minimum distance, right. The default choice, however, is based on edge-statistics (actual inter-edges between clusters vs. expected number of edges in random connections). Currently, I'm working on the revision of the algorithm. The option for minimum distance will disappear and everything will become much cleaner. I'm also trying to improve the statistical model for connectivity and provide a clearer option for its significance threshold. Right now, the only relevant option in the whole AGA [given the single-cell graph is computed and clustered] is `tree_based_confidence=True`; if you set this to `False`, the significance value for edges to appear will be much lower and you'll get a much sparser abstracted graph. However, this graph is sometimes too sparse. If `tree_based_confidence=True`, as per default, this works fine on very connected datasets, but sometimes gives results that are too dense on disconnected datasets. For now, you could simply try setting `tree_based_confidence=False` and see whether this is satisfying. If not; probably too sparse, it would be great if you could try the new AGA version in a couple of days. Also, I'd be very happy to run the method on your data and look at specific issues. You can also approach me via email... Cheers,; Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/96#issuecomment-370139940
https://github.com/scverse/scanpy/issues/96#issuecomment-370139940:1036,Usability,simpl,simply,1036,"Hi!. Thanks for reaching out!. We have an option to compute connectivity based on minimum distance, right. The default choice, however, is based on edge-statistics (actual inter-edges between clusters vs. expected number of edges in random connections). Currently, I'm working on the revision of the algorithm. The option for minimum distance will disappear and everything will become much cleaner. I'm also trying to improve the statistical model for connectivity and provide a clearer option for its significance threshold. Right now, the only relevant option in the whole AGA [given the single-cell graph is computed and clustered] is `tree_based_confidence=True`; if you set this to `False`, the significance value for edges to appear will be much lower and you'll get a much sparser abstracted graph. However, this graph is sometimes too sparse. If `tree_based_confidence=True`, as per default, this works fine on very connected datasets, but sometimes gives results that are too dense on disconnected datasets. For now, you could simply try setting `tree_based_confidence=False` and see whether this is satisfying. If not; probably too sparse, it would be great if you could try the new AGA version in a couple of days. Also, I'd be very happy to run the method on your data and look at specific issues. You can also approach me via email... Cheers,; Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/96#issuecomment-370139940
https://github.com/scverse/scanpy/issues/96#issuecomment-393690042:590,Performance,perform,perform,590,"Hi, just to confirm that I tried the new PAGA functions a while ago and the results look very good. (sorry for the delay of the response. I meant to respond to the thread much earlier but got busy doing other stuff.). Now I'm wondering about how to interpret the graph connectivities. An undirected graph does not imply whether two connected clusters are sequential (e.g. progenitors -> newborn neurons -> mature neurons) or on different branches but highly correlated (e.g. neuron subtype 1 vs. neuron subtype 2). . Do you think it's possible to use RNA velocity (http://velocyto.org/) to perform quantitative interference on the directionality of the edges? I have the velocity data but not sure how to mathematically infer edge directions. Maybe I should open a new issue on this or approach you via email? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/96#issuecomment-393690042
https://github.com/scverse/scanpy/issues/96#issuecomment-393738263:42,Usability,simpl,simply,42,"Hi! Good to read! :smile:. The PAGA edges simply mean that clusters are topologically connected - in the single-cell graph, there is a significant number of inter-cluster-edges, above noise-level. They absolutely don't have an orientation. Regarding velocyto: yes, it's possible to use it to orient the edges in PAGA. You can get that functionality following [this](https://github.com/falexwolf/paga/blob/master/planaria/planaria_paga_velocyto.ipynb); however, until this becomes really well-documented etc. this will still take a while... the model behind this will also be subject to change, I guess... Get Scanpy 1.1 for this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/96#issuecomment-393738263
https://github.com/scverse/scanpy/issues/96#issuecomment-393819773:161,Usability,clear,clearer,161,"If you're planning to look into the code: There will be a new version of PAGA in Scanpy 1.2, which will feature two connectivity models... The code will be much clearer. We'll also see whether we can upload an extensive revision of the preprint - unfortunately, the review process at the journal took ages and coming up with the revision, too. All of this should happen in the next days.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/96#issuecomment-393819773
https://github.com/scverse/scanpy/issues/96#issuecomment-393970757:140,Deployability,update,updates,140,"This what I'm currently compiling. While [Scanpy 1.1](http://scanpy.readthedocs.io/en/latest/#version-1-1-may-31-2018) concerned some basic updates and more general features, Scanpy 1.2 will be about PAGA. No worries, everything is backward compatible... but PAGA will have many more cool features and in addition, also feature a second, better model. I will release it this weekend.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/96#issuecomment-393970757
https://github.com/scverse/scanpy/issues/96#issuecomment-393970757:359,Deployability,release,release,359,"This what I'm currently compiling. While [Scanpy 1.1](http://scanpy.readthedocs.io/en/latest/#version-1-1-may-31-2018) concerned some basic updates and more general features, Scanpy 1.2 will be about PAGA. No worries, everything is backward compatible... but PAGA will have many more cool features and in addition, also feature a second, better model. I will release it this weekend.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/96#issuecomment-393970757
https://github.com/scverse/scanpy/issues/96#issuecomment-393972623:94,Availability,avail,available,94,"The current model is stable and has been successfully used in many instances. It will also be available in Scanpy 1.2. In addition, there will be another model. General improvements only regard the ease of use of PAGA and are model-independent anyways.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/96#issuecomment-393972623
https://github.com/scverse/scanpy/issues/97#issuecomment-370144822:463,Availability,redundant,redundant,463,"Advantage of networkx is that it's easily installed... But yes, we should remove it in the future. I think with anaconda, one gets all the igraph and louvain stuff to work very easily without compiling. Without using Grohlke's binaries... One just needs to document this probably. At the latest when igraph and louvain are easily installed, networkx can be removed... PS: I'm currently preparing scanpy 1.0; there will be some slight changes to make the API less redundant... So for now, please no big changes...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97#issuecomment-370144822
https://github.com/scverse/scanpy/issues/97#issuecomment-370144822:42,Deployability,install,installed,42,"Advantage of networkx is that it's easily installed... But yes, we should remove it in the future. I think with anaconda, one gets all the igraph and louvain stuff to work very easily without compiling. Without using Grohlke's binaries... One just needs to document this probably. At the latest when igraph and louvain are easily installed, networkx can be removed... PS: I'm currently preparing scanpy 1.0; there will be some slight changes to make the API less redundant... So for now, please no big changes...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97#issuecomment-370144822
https://github.com/scverse/scanpy/issues/97#issuecomment-370144822:330,Deployability,install,installed,330,"Advantage of networkx is that it's easily installed... But yes, we should remove it in the future. I think with anaconda, one gets all the igraph and louvain stuff to work very easily without compiling. Without using Grohlke's binaries... One just needs to document this probably. At the latest when igraph and louvain are easily installed, networkx can be removed... PS: I'm currently preparing scanpy 1.0; there will be some slight changes to make the API less redundant... So for now, please no big changes...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97#issuecomment-370144822
https://github.com/scverse/scanpy/issues/97#issuecomment-370144822:463,Safety,redund,redundant,463,"Advantage of networkx is that it's easily installed... But yes, we should remove it in the future. I think with anaconda, one gets all the igraph and louvain stuff to work very easily without compiling. Without using Grohlke's binaries... One just needs to document this probably. At the latest when igraph and louvain are easily installed, networkx can be removed... PS: I'm currently preparing scanpy 1.0; there will be some slight changes to make the API less redundant... So for now, please no big changes...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97#issuecomment-370144822
https://github.com/scverse/scanpy/issues/97#issuecomment-370161196:161,Deployability,install,installation,161,Graph_tool library would be even better and it also implements SBM and many other things that may be useful in graph analysis of single cells. Unfortunately its installation is a painful experience.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97#issuecomment-370161196
https://github.com/scverse/scanpy/issues/97#issuecomment-370355231:61,Deployability,install,installation,61,"Yes, graph_tool is nice and I'm also using it; but yes, it's installation is even worse than igraph... hence, no option for a dependency...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97#issuecomment-370355231
https://github.com/scverse/scanpy/issues/97#issuecomment-370355231:126,Integrability,depend,dependency,126,"Yes, graph_tool is nice and I'm also using it; but yes, it's installation is even worse than igraph... hence, no option for a dependency...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97#issuecomment-370355231
https://github.com/scverse/scanpy/issues/97#issuecomment-370393215:731,Performance,perform,performant,731,"@dawe In all benchmarks that I did maybe a bit less than a year ago, the `python-louvain` didn't seem to produce satisfying results... But maybe I did something stupid. I'll reevaluate this, thanks, Davide! PS: One can easily switch between implementations; simply pass `flavor='taynaud'` to `sc.tl.louvain` and you'll use `python-louvain`. See [here](https://github.com/theislab/scanpy/blob/5299c6caaec6402513f1e0442186350787177d2c/scanpy/tools/louvain.py#L118-L125). However, I removed this from the docs as I was not so satisfied with it... @flying-sheep the only thing where `igraph` is used in Scanpy is for graph drawing, where it's incredibly faster than `networkx` (completely forget about `networkx` in this respect); the performant `louvain` implementation is due to the `louvain` package, which simply uses `igraph`'s graph data structures",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97#issuecomment-370393215
https://github.com/scverse/scanpy/issues/97#issuecomment-370393215:13,Testability,benchmark,benchmarks,13,"@dawe In all benchmarks that I did maybe a bit less than a year ago, the `python-louvain` didn't seem to produce satisfying results... But maybe I did something stupid. I'll reevaluate this, thanks, Davide! PS: One can easily switch between implementations; simply pass `flavor='taynaud'` to `sc.tl.louvain` and you'll use `python-louvain`. See [here](https://github.com/theislab/scanpy/blob/5299c6caaec6402513f1e0442186350787177d2c/scanpy/tools/louvain.py#L118-L125). However, I removed this from the docs as I was not so satisfied with it... @flying-sheep the only thing where `igraph` is used in Scanpy is for graph drawing, where it's incredibly faster than `networkx` (completely forget about `networkx` in this respect); the performant `louvain` implementation is due to the `louvain` package, which simply uses `igraph`'s graph data structures",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97#issuecomment-370393215
https://github.com/scverse/scanpy/issues/97#issuecomment-370393215:258,Usability,simpl,simply,258,"@dawe In all benchmarks that I did maybe a bit less than a year ago, the `python-louvain` didn't seem to produce satisfying results... But maybe I did something stupid. I'll reevaluate this, thanks, Davide! PS: One can easily switch between implementations; simply pass `flavor='taynaud'` to `sc.tl.louvain` and you'll use `python-louvain`. See [here](https://github.com/theislab/scanpy/blob/5299c6caaec6402513f1e0442186350787177d2c/scanpy/tools/louvain.py#L118-L125). However, I removed this from the docs as I was not so satisfied with it... @flying-sheep the only thing where `igraph` is used in Scanpy is for graph drawing, where it's incredibly faster than `networkx` (completely forget about `networkx` in this respect); the performant `louvain` implementation is due to the `louvain` package, which simply uses `igraph`'s graph data structures",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97#issuecomment-370393215
https://github.com/scverse/scanpy/issues/97#issuecomment-370393215:806,Usability,simpl,simply,806,"@dawe In all benchmarks that I did maybe a bit less than a year ago, the `python-louvain` didn't seem to produce satisfying results... But maybe I did something stupid. I'll reevaluate this, thanks, Davide! PS: One can easily switch between implementations; simply pass `flavor='taynaud'` to `sc.tl.louvain` and you'll use `python-louvain`. See [here](https://github.com/theislab/scanpy/blob/5299c6caaec6402513f1e0442186350787177d2c/scanpy/tools/louvain.py#L118-L125). However, I removed this from the docs as I was not so satisfied with it... @flying-sheep the only thing where `igraph` is used in Scanpy is for graph drawing, where it's incredibly faster than `networkx` (completely forget about `networkx` in this respect); the performant `louvain` implementation is due to the `louvain` package, which simply uses `igraph`'s graph data structures",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97#issuecomment-370393215
https://github.com/scverse/scanpy/issues/97#issuecomment-440373090:131,Availability,avail,available,131,"Hello again, some months later. Unfortunately I haven’t used scanpy for a while and now I’m back I found that only two flavors are available for Louvain processing, `igraph` and `vtraag`. The latter allows for resolution parameter but relies on `python-louvain`, right? the reason for my “but” is that I’m having issues with that module on OSX, which I understand it is not a specific scanpy issue. I’ll ask again: what’s wrong with `networkx` and `community` modules? I’m asking because flavor `taynaud` is no more available",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97#issuecomment-440373090
https://github.com/scverse/scanpy/issues/97#issuecomment-440373090:516,Availability,avail,available,516,"Hello again, some months later. Unfortunately I haven’t used scanpy for a while and now I’m back I found that only two flavors are available for Louvain processing, `igraph` and `vtraag`. The latter allows for resolution parameter but relies on `python-louvain`, right? the reason for my “but” is that I’m having issues with that module on OSX, which I understand it is not a specific scanpy issue. I’ll ask again: what’s wrong with `networkx` and `community` modules? I’m asking because flavor `taynaud` is no more available",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97#issuecomment-440373090
https://github.com/scverse/scanpy/issues/97#issuecomment-440389136:428,Deployability,install,installed,428,"I think `flavor='taynaud'` should [still work](https://github.com/theislab/scanpy/blob/a9ee39f5152d167f1aeb784ffbdd0a6e3dc1409a/scanpy/tools/louvain.py#L142), but is really only meant as a last resort. AFAIK the networkx modules don't even provide a resolution parameter, but most importantly, they don't scale. I think I even was satisfied with convergence of the results and comparisons with, e.g., Seurat. Scanpy can even be installed using bioconda, so there should be no problem with igraph installation these days. Did you checkout https://scanpy.readthedocs.io/en/latest/installation.html?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97#issuecomment-440389136
https://github.com/scverse/scanpy/issues/97#issuecomment-440389136:496,Deployability,install,installation,496,"I think `flavor='taynaud'` should [still work](https://github.com/theislab/scanpy/blob/a9ee39f5152d167f1aeb784ffbdd0a6e3dc1409a/scanpy/tools/louvain.py#L142), but is really only meant as a last resort. AFAIK the networkx modules don't even provide a resolution parameter, but most importantly, they don't scale. I think I even was satisfied with convergence of the results and comparisons with, e.g., Seurat. Scanpy can even be installed using bioconda, so there should be no problem with igraph installation these days. Did you checkout https://scanpy.readthedocs.io/en/latest/installation.html?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97#issuecomment-440389136
https://github.com/scverse/scanpy/issues/97#issuecomment-440389136:578,Deployability,install,installation,578,"I think `flavor='taynaud'` should [still work](https://github.com/theislab/scanpy/blob/a9ee39f5152d167f1aeb784ffbdd0a6e3dc1409a/scanpy/tools/louvain.py#L142), but is really only meant as a last resort. AFAIK the networkx modules don't even provide a resolution parameter, but most importantly, they don't scale. I think I even was satisfied with convergence of the results and comparisons with, e.g., Seurat. Scanpy can even be installed using bioconda, so there should be no problem with igraph installation these days. Did you checkout https://scanpy.readthedocs.io/en/latest/installation.html?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97#issuecomment-440389136
https://github.com/scverse/scanpy/issues/97#issuecomment-440443151:166,Deployability,install,installation,166,"I’m quite sure it has a resolution parameter, but at this point I’m also quite sure I’m messing up with modules and dependencies, both in this thread and on my local installation... about conda, I guess I’m one of the last around who hasn’t adopted it yet",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97#issuecomment-440443151
https://github.com/scverse/scanpy/issues/97#issuecomment-440443151:116,Integrability,depend,dependencies,116,"I’m quite sure it has a resolution parameter, but at this point I’m also quite sure I’m messing up with modules and dependencies, both in this thread and on my local installation... about conda, I guess I’m one of the last around who hasn’t adopted it yet",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97#issuecomment-440443151
https://github.com/scverse/scanpy/pull/100#issuecomment-371090054:113,Availability,down,downsampling,113,"thank you very much! one thing that we could consider is renaming this to ""downsample_counts""; for some people, ""downsampling observations"" is an alias to ""subsampling observations"" and for these the function name is not descriptive enough. what do you think? can I make this change?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/100#issuecomment-371090054
https://github.com/scverse/scanpy/pull/100#issuecomment-371099043:33,Availability,error,error,33,"Also, you may want to change the error that is shown if a non-AnnData object is passed to the function. I put it as TypeError, because I didn't find how you have previously called that error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/100#issuecomment-371099043
https://github.com/scverse/scanpy/pull/100#issuecomment-371099043:185,Availability,error,error,185,"Also, you may want to change the error that is shown if a non-AnnData object is passed to the function. I put it as TypeError, because I didn't find how you have previously called that error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/100#issuecomment-371099043
https://github.com/scverse/scanpy/issues/101#issuecomment-371489944:259,Deployability,release,released,259,"Yes, I know about an issue that is probably related to that: At some point in `add_or_update_graph_in_adata`, numpy takes more cores than it's supposed to - that's the only instance in the whole of Scanpy. Otherwise it's well-behaved. When Scanpy 1.0 will be released in the next few days, this will be resolved. Do you think this will do the job for you?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/101#issuecomment-371489944
https://github.com/scverse/scanpy/issues/102#issuecomment-566126409:414,Availability,error,error,414,"I am also experiencing this issue. Running the following code:; ```; import pandas as pd; import scanpy as sc; import anndata. print(pd.__version__); print(sc.__version__); print(anndata.__version__); adata = sc.datasets.pbmc68k_reduced(); adata.obs[""single_cat""] = 1; adata.obs['single_cat'] = pd.Categorical(adata.obs['single_cat']); adata.write('/tmp/adata.h5ad'); sc.read('/tmp/adata.h5ad'); ```. Returns this error message:; ```; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-5-adde38d13544> in <module>; ----> 1 sc.read('/tmp/adata.h5ad'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 95 filename, backed=backed, sheet=sheet, ext=ext,; 96 delimiter=delimiter, first_column_names=first_column_names,; ---> 97 backup_url=backup_url, cache=cache, **kwargs,; 98 ); 99 # generate filename and read to dict. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 497 if ext in {'h5', 'h5ad'}:; 498 if sheet is None:; --> 499 return read_h5ad(filename, backed=backed); 500 else:; 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size); 445 else:; 446 # load everything into memory; --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size); 448 X = constructor_args[0]; 449 dtype = None. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size); 500 if not backed:; 501 f.close(); --> 502 return AnnData._args_from_dict(d); 503 ; 504 . /usr/local/anaconda3/envs/d",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/102#issuecomment-566126409
https://github.com/scverse/scanpy/issues/102#issuecomment-566126409:420,Integrability,message,message,420,"I am also experiencing this issue. Running the following code:; ```; import pandas as pd; import scanpy as sc; import anndata. print(pd.__version__); print(sc.__version__); print(anndata.__version__); adata = sc.datasets.pbmc68k_reduced(); adata.obs[""single_cat""] = 1; adata.obs['single_cat'] = pd.Categorical(adata.obs['single_cat']); adata.write('/tmp/adata.h5ad'); sc.read('/tmp/adata.h5ad'); ```. Returns this error message:; ```; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-5-adde38d13544> in <module>; ----> 1 sc.read('/tmp/adata.h5ad'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 95 filename, backed=backed, sheet=sheet, ext=ext,; 96 delimiter=delimiter, first_column_names=first_column_names,; ---> 97 backup_url=backup_url, cache=cache, **kwargs,; 98 ); 99 # generate filename and read to dict. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 497 if ext in {'h5', 'h5ad'}:; 498 if sheet is None:; --> 499 return read_h5ad(filename, backed=backed); 500 else:; 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size); 445 else:; 446 # load everything into memory; --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size); 448 X = constructor_args[0]; 449 dtype = None. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size); 500 if not backed:; 501 f.close(); --> 502 return AnnData._args_from_dict(d); 503 ; 504 . /usr/local/anaconda3/envs/d",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/102#issuecomment-566126409
https://github.com/scverse/scanpy/issues/102#issuecomment-566126409:800,Performance,cache,cache,800,"I am also experiencing this issue. Running the following code:; ```; import pandas as pd; import scanpy as sc; import anndata. print(pd.__version__); print(sc.__version__); print(anndata.__version__); adata = sc.datasets.pbmc68k_reduced(); adata.obs[""single_cat""] = 1; adata.obs['single_cat'] = pd.Categorical(adata.obs['single_cat']); adata.write('/tmp/adata.h5ad'); sc.read('/tmp/adata.h5ad'); ```. Returns this error message:; ```; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-5-adde38d13544> in <module>; ----> 1 sc.read('/tmp/adata.h5ad'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 95 filename, backed=backed, sheet=sheet, ext=ext,; 96 delimiter=delimiter, first_column_names=first_column_names,; ---> 97 backup_url=backup_url, cache=cache, **kwargs,; 98 ); 99 # generate filename and read to dict. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 497 if ext in {'h5', 'h5ad'}:; 498 if sheet is None:; --> 499 return read_h5ad(filename, backed=backed); 500 else:; 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size); 445 else:; 446 # load everything into memory; --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size); 448 X = constructor_args[0]; 449 dtype = None. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size); 500 if not backed:; 501 f.close(); --> 502 return AnnData._args_from_dict(d); 503 ; 504 . /usr/local/anaconda3/envs/d",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/102#issuecomment-566126409
https://github.com/scverse/scanpy/issues/102#issuecomment-566126409:964,Performance,cache,cache,964,"I am also experiencing this issue. Running the following code:; ```; import pandas as pd; import scanpy as sc; import anndata. print(pd.__version__); print(sc.__version__); print(anndata.__version__); adata = sc.datasets.pbmc68k_reduced(); adata.obs[""single_cat""] = 1; adata.obs['single_cat'] = pd.Categorical(adata.obs['single_cat']); adata.write('/tmp/adata.h5ad'); sc.read('/tmp/adata.h5ad'); ```. Returns this error message:; ```; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-5-adde38d13544> in <module>; ----> 1 sc.read('/tmp/adata.h5ad'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 95 filename, backed=backed, sheet=sheet, ext=ext,; 96 delimiter=delimiter, first_column_names=first_column_names,; ---> 97 backup_url=backup_url, cache=cache, **kwargs,; 98 ); 99 # generate filename and read to dict. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 497 if ext in {'h5', 'h5ad'}:; 498 if sheet is None:; --> 499 return read_h5ad(filename, backed=backed); 500 else:; 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size); 445 else:; 446 # load everything into memory; --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size); 448 X = constructor_args[0]; 449 dtype = None. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size); 500 if not backed:; 501 f.close(); --> 502 return AnnData._args_from_dict(d); 503 ; 504 . /usr/local/anaconda3/envs/d",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/102#issuecomment-566126409
https://github.com/scverse/scanpy/issues/102#issuecomment-566126409:970,Performance,cache,cache,970,"I am also experiencing this issue. Running the following code:; ```; import pandas as pd; import scanpy as sc; import anndata. print(pd.__version__); print(sc.__version__); print(anndata.__version__); adata = sc.datasets.pbmc68k_reduced(); adata.obs[""single_cat""] = 1; adata.obs['single_cat'] = pd.Categorical(adata.obs['single_cat']); adata.write('/tmp/adata.h5ad'); sc.read('/tmp/adata.h5ad'); ```. Returns this error message:; ```; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-5-adde38d13544> in <module>; ----> 1 sc.read('/tmp/adata.h5ad'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 95 filename, backed=backed, sheet=sheet, ext=ext,; 96 delimiter=delimiter, first_column_names=first_column_names,; ---> 97 backup_url=backup_url, cache=cache, **kwargs,; 98 ); 99 # generate filename and read to dict. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 497 if ext in {'h5', 'h5ad'}:; 498 if sheet is None:; --> 499 return read_h5ad(filename, backed=backed); 500 else:; 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size); 445 else:; 446 # load everything into memory; --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size); 448 X = constructor_args[0]; 449 dtype = None. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size); 500 if not backed:; 501 f.close(); --> 502 return AnnData._args_from_dict(d); 503 ; 504 . /usr/local/anaconda3/envs/d",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/102#issuecomment-566126409
https://github.com/scverse/scanpy/issues/102#issuecomment-566126409:1199,Performance,cache,cache,1199,"adata.obs[""single_cat""] = 1; adata.obs['single_cat'] = pd.Categorical(adata.obs['single_cat']); adata.write('/tmp/adata.h5ad'); sc.read('/tmp/adata.h5ad'); ```. Returns this error message:; ```; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-5-adde38d13544> in <module>; ----> 1 sc.read('/tmp/adata.h5ad'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 95 filename, backed=backed, sheet=sheet, ext=ext,; 96 delimiter=delimiter, first_column_names=first_column_names,; ---> 97 backup_url=backup_url, cache=cache, **kwargs,; 98 ); 99 # generate filename and read to dict. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 497 if ext in {'h5', 'h5ad'}:; 498 if sheet is None:; --> 499 return read_h5ad(filename, backed=backed); 500 else:; 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size); 445 else:; 446 # load everything into memory; --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size); 448 X = constructor_args[0]; 449 dtype = None. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size); 500 if not backed:; 501 f.close(); --> 502 return AnnData._args_from_dict(d); 503 ; 504 . /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/core/anndata.py in _args_from_dict(ddata); 2182 d_true_keys[ann][k_stripped] = pd.Categorical.from_codes(; 2183 codes=d_true_keys[ann][k_stripped].values,; -> 2184 categories=v,; 2185 ); 2186 k_to",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/102#issuecomment-566126409
https://github.com/scverse/scanpy/issues/102#issuecomment-566126409:1569,Performance,load,load,1569,"mp/adata.h5ad'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 95 filename, backed=backed, sheet=sheet, ext=ext,; 96 delimiter=delimiter, first_column_names=first_column_names,; ---> 97 backup_url=backup_url, cache=cache, **kwargs,; 98 ); 99 # generate filename and read to dict. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 497 if ext in {'h5', 'h5ad'}:; 498 if sheet is None:; --> 499 return read_h5ad(filename, backed=backed); 500 else:; 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size); 445 else:; 446 # load everything into memory; --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size); 448 X = constructor_args[0]; 449 dtype = None. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size); 500 if not backed:; 501 f.close(); --> 502 return AnnData._args_from_dict(d); 503 ; 504 . /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/core/anndata.py in _args_from_dict(ddata); 2182 d_true_keys[ann][k_stripped] = pd.Categorical.from_codes(; 2183 codes=d_true_keys[ann][k_stripped].values,; -> 2184 categories=v,; 2185 ); 2186 k_to_delete.append(k). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in from_codes(cls, codes, categories, ordered, dtype); 638 dtype = CategoricalDtype._from_values_or_dtype(categories=categories,; 639 ordered=ordered,; --> 640 dtype=dtype); 641 if dtype.categories is None:; 642 msg = (""The categories must be provided in 'categorie",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/102#issuecomment-566126409
https://github.com/scverse/scanpy/issues/102#issuecomment-566126409:1361,Testability,log,logg,1361,"adata.obs[""single_cat""] = 1; adata.obs['single_cat'] = pd.Categorical(adata.obs['single_cat']); adata.write('/tmp/adata.h5ad'); sc.read('/tmp/adata.h5ad'); ```. Returns this error message:; ```; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-5-adde38d13544> in <module>; ----> 1 sc.read('/tmp/adata.h5ad'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 95 filename, backed=backed, sheet=sheet, ext=ext,; 96 delimiter=delimiter, first_column_names=first_column_names,; ---> 97 backup_url=backup_url, cache=cache, **kwargs,; 98 ); 99 # generate filename and read to dict. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 497 if ext in {'h5', 'h5ad'}:; 498 if sheet is None:; --> 499 return read_h5ad(filename, backed=backed); 500 else:; 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size); 445 else:; 446 # load everything into memory; --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size); 448 X = constructor_args[0]; 449 dtype = None. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size); 500 if not backed:; 501 f.close(); --> 502 return AnnData._args_from_dict(d); 503 ; 504 . /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/core/anndata.py in _args_from_dict(ddata); 2182 d_true_keys[ann][k_stripped] = pd.Categorical.from_codes(; 2183 codes=d_true_keys[ann][k_stripped].values,; -> 2184 categories=v,; 2185 ); 2186 k_to",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/102#issuecomment-566126409
https://github.com/scverse/scanpy/issues/103#issuecomment-373161702:732,Modifiability,extend,extend,732,"Hi Dustin!. Thank you for providing a template. You can easily read that template in with two additional lines of code:; ```; import scanpy.api as sc; import pandas as pd; adata = sc.read_excel('./data/base_template.xlsx', sheet='expression').T; adata.obs = pd.read_excel('./data/base_template.xlsx', sheet_name='observations', index_col='observations'); adata.var = pd.read_excel('./data/base_template.xlsx', sheet_name='genes', index_col='gene_symbol'); ```; I'm a bit hesitant to put all of this into `sc.read_excel`... it would add 4 new parameters and many people might not have organized their excel files this way? Or am I wrong with the last assumption? If it's something very common todo, I can add the two other lines and extend `read_excel`. What do you think? Are the two further lines too much additional code?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/103#issuecomment-373161702
https://github.com/scverse/scanpy/pull/104#issuecomment-374291211:44,Testability,log,logarithm,44,"Thank you! :smile: There is also taking the logarithm of the means involved and I'll add this for clarifcation, but of course you're right... there's no logarithm of the variance taken.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/104#issuecomment-374291211
https://github.com/scverse/scanpy/pull/104#issuecomment-374291211:153,Testability,log,logarithm,153,"Thank you! :smile: There is also taking the logarithm of the means involved and I'll add this for clarifcation, but of course you're right... there's no logarithm of the variance taken.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/104#issuecomment-374291211
https://github.com/scverse/scanpy/issues/106#issuecomment-378689095:65,Deployability,rolling,rolling,65,@falexwolf - feedback here would be appreciated. We are weary of rolling our own solution when a standard may be in place or planned.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/106#issuecomment-378689095
https://github.com/scverse/scanpy/issues/106#issuecomment-378689095:13,Usability,feedback,feedback,13,@falexwolf - feedback here would be appreciated. We are weary of rolling our own solution when a standard may be in place or planned.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/106#issuecomment-378689095
https://github.com/scverse/scanpy/issues/106#issuecomment-378912055:139,Deployability,toggle,toggleswitch,139,"Thank you for the reminder, Joshua! :smile:. How about doing this?; ```; import scanpy.api as sc; import pandas as pd; adata = sc.datasets.toggleswitch(); adata.obs['replicate'] = 0; adata.obs['replicate'].loc[100:] = 1; df = pd.DataFrame(adata.X) # does not allocate new memory if X is an array, so this efficient; df['replicate'] = adata.obs['replicate'].values # if not using assign, no copy is made; df_grouped = df.groupby('replicate'); print(df_grouped.mean()); print(df_grouped.std()); ```; outputs; ```; 0 1; replicate ; 0 0.510177 0.135317; 1 0.152043 0.439836; 0 1; replicate ; 0 0.293965 0.162549; 1 0.153663 0.271669; ```; Of course, you can add this stuff as unstructured annotation to an AnnData... Does it answer your question?. PS: Visualize this is using ideas e.g. from https://stackoverflow.com/questions/46186784/handling-replicate-data-in-pandas",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/106#issuecomment-378912055
https://github.com/scverse/scanpy/issues/106#issuecomment-378912055:259,Energy Efficiency,allocate,allocate,259,"Thank you for the reminder, Joshua! :smile:. How about doing this?; ```; import scanpy.api as sc; import pandas as pd; adata = sc.datasets.toggleswitch(); adata.obs['replicate'] = 0; adata.obs['replicate'].loc[100:] = 1; df = pd.DataFrame(adata.X) # does not allocate new memory if X is an array, so this efficient; df['replicate'] = adata.obs['replicate'].values # if not using assign, no copy is made; df_grouped = df.groupby('replicate'); print(df_grouped.mean()); print(df_grouped.std()); ```; outputs; ```; 0 1; replicate ; 0 0.510177 0.135317; 1 0.152043 0.439836; 0 1; replicate ; 0 0.293965 0.162549; 1 0.153663 0.271669; ```; Of course, you can add this stuff as unstructured annotation to an AnnData... Does it answer your question?. PS: Visualize this is using ideas e.g. from https://stackoverflow.com/questions/46186784/handling-replicate-data-in-pandas",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/106#issuecomment-378912055
https://github.com/scverse/scanpy/issues/106#issuecomment-378912055:305,Energy Efficiency,efficient,efficient,305,"Thank you for the reminder, Joshua! :smile:. How about doing this?; ```; import scanpy.api as sc; import pandas as pd; adata = sc.datasets.toggleswitch(); adata.obs['replicate'] = 0; adata.obs['replicate'].loc[100:] = 1; df = pd.DataFrame(adata.X) # does not allocate new memory if X is an array, so this efficient; df['replicate'] = adata.obs['replicate'].values # if not using assign, no copy is made; df_grouped = df.groupby('replicate'); print(df_grouped.mean()); print(df_grouped.std()); ```; outputs; ```; 0 1; replicate ; 0 0.510177 0.135317; 1 0.152043 0.439836; 0 1; replicate ; 0 0.293965 0.162549; 1 0.153663 0.271669; ```; Of course, you can add this stuff as unstructured annotation to an AnnData... Does it answer your question?. PS: Visualize this is using ideas e.g. from https://stackoverflow.com/questions/46186784/handling-replicate-data-in-pandas",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/106#issuecomment-378912055
https://github.com/scverse/scanpy/issues/106#issuecomment-380582298:338,Integrability,interface,interfaces,338,"I think it shows us some strategy, yes. I can't claim to understand what each line you did there does but I have a notebook open now and am trying to step through it. I think part of the issue here is we're wanting to store aggregate values like means within adata so that they don't have to be recomputed every time they are needed. Web interfaces are being driven off these files, so rapid access is preferred over storage concerns. . If, in the example of datasets with technical replicates, we almost always are only interested in the mean values across replicates. So if our input is like this:. ![screenshot from 2018-04-11 15-11-07](https://user-images.githubusercontent.com/330899/38640690-9b7c41a8-3d9a-11e8-9b40-b9763a3df422.png). Simple demo with 4 genes, 2 conditions, and 3 replicates of each condition. Does one make this exact matrix adata.X? . ![screenshot from 2018-04-11 15-16-11](https://user-images.githubusercontent.com/330899/38640934-4a25562c-3d9b-11e8-82eb-eb7811463fa4.png). Or should the means be calculated and stored as adata.X with individual replicates stored as separate matrices of the same size, like adata.uns['rep1'], adata.uns['rep2'], ... etc. This is specifically the part I was asking about regarding conventions, as it seems there must already be a convention for storing technical replicates and their aggregate values (mean, stddev, etc.). I think the examples given with values like 0 and 1 are a bit confusing because I can't tell if you're using that as a proxy for column names or are they index positions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/106#issuecomment-380582298
https://github.com/scverse/scanpy/issues/106#issuecomment-380582298:392,Security,access,access,392,"I think it shows us some strategy, yes. I can't claim to understand what each line you did there does but I have a notebook open now and am trying to step through it. I think part of the issue here is we're wanting to store aggregate values like means within adata so that they don't have to be recomputed every time they are needed. Web interfaces are being driven off these files, so rapid access is preferred over storage concerns. . If, in the example of datasets with technical replicates, we almost always are only interested in the mean values across replicates. So if our input is like this:. ![screenshot from 2018-04-11 15-11-07](https://user-images.githubusercontent.com/330899/38640690-9b7c41a8-3d9a-11e8-9b40-b9763a3df422.png). Simple demo with 4 genes, 2 conditions, and 3 replicates of each condition. Does one make this exact matrix adata.X? . ![screenshot from 2018-04-11 15-16-11](https://user-images.githubusercontent.com/330899/38640934-4a25562c-3d9b-11e8-82eb-eb7811463fa4.png). Or should the means be calculated and stored as adata.X with individual replicates stored as separate matrices of the same size, like adata.uns['rep1'], adata.uns['rep2'], ... etc. This is specifically the part I was asking about regarding conventions, as it seems there must already be a convention for storing technical replicates and their aggregate values (mean, stddev, etc.). I think the examples given with values like 0 and 1 are a bit confusing because I can't tell if you're using that as a proxy for column names or are they index positions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/106#issuecomment-380582298
https://github.com/scverse/scanpy/issues/106#issuecomment-380582298:741,Usability,Simpl,Simple,741,"I think it shows us some strategy, yes. I can't claim to understand what each line you did there does but I have a notebook open now and am trying to step through it. I think part of the issue here is we're wanting to store aggregate values like means within adata so that they don't have to be recomputed every time they are needed. Web interfaces are being driven off these files, so rapid access is preferred over storage concerns. . If, in the example of datasets with technical replicates, we almost always are only interested in the mean values across replicates. So if our input is like this:. ![screenshot from 2018-04-11 15-11-07](https://user-images.githubusercontent.com/330899/38640690-9b7c41a8-3d9a-11e8-9b40-b9763a3df422.png). Simple demo with 4 genes, 2 conditions, and 3 replicates of each condition. Does one make this exact matrix adata.X? . ![screenshot from 2018-04-11 15-16-11](https://user-images.githubusercontent.com/330899/38640934-4a25562c-3d9b-11e8-82eb-eb7811463fa4.png). Or should the means be calculated and stored as adata.X with individual replicates stored as separate matrices of the same size, like adata.uns['rep1'], adata.uns['rep2'], ... etc. This is specifically the part I was asking about regarding conventions, as it seems there must already be a convention for storing technical replicates and their aggregate values (mean, stddev, etc.). I think the examples given with values like 0 and 1 are a bit confusing because I can't tell if you're using that as a proxy for column names or are they index positions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/106#issuecomment-380582298
https://github.com/scverse/scanpy/issues/107#issuecomment-374180584:144,Usability,learn,learning-based,144,"Hi David,; it's currently not a focus, at least for me... Our general perspective is to replace all of the manual preprocessing with some ""deep learning-based preprocessing""... We will soon have something on this... If it works, more advanced preprocessing becomes obsolete, I guess. If it doesn't, we'll definitely add more advanced stuff... Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/107#issuecomment-374180584
https://github.com/scverse/scanpy/issues/108#issuecomment-374805935:293,Deployability,configurat,configuration,293,"Fixed in https://github.com/theislab/scanpy/commit/57161ec444eef7815e159037c6944ddcc75572d9. However, the version1 branch is not stable yet... another day or two... What made me believe that seaborn is still doing strange things, is this... one call to `seaborn.set_style` messes up the whole configuration... That's a bug, isn't it?; <img width=""207"" alt=""image"" src=""https://user-images.githubusercontent.com/16916678/37690247-05c2686e-2caa-11e8-8dc2-7365a90f8748.png"">",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/108#issuecomment-374805935
https://github.com/scverse/scanpy/issues/108#issuecomment-374805935:293,Modifiability,config,configuration,293,"Fixed in https://github.com/theislab/scanpy/commit/57161ec444eef7815e159037c6944ddcc75572d9. However, the version1 branch is not stable yet... another day or two... What made me believe that seaborn is still doing strange things, is this... one call to `seaborn.set_style` messes up the whole configuration... That's a bug, isn't it?; <img width=""207"" alt=""image"" src=""https://user-images.githubusercontent.com/16916678/37690247-05c2686e-2caa-11e8-8dc2-7365a90f8748.png"">",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/108#issuecomment-374805935
https://github.com/scverse/scanpy/issues/108#issuecomment-375036470:64,Safety,avoid,avoid,64,"Mm I think I you're supposed to use a context (`with ... :`) to avoid changing things globally. Anyway my current workaround of doing `from scanpy.api import preprocessing as scpp` is not too cumbersome, so should be okay until 1.0 hits :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/108#issuecomment-375036470
https://github.com/scverse/scanpy/issues/109#issuecomment-375856560:734,Availability,error,error,734,"Dear Olivia,; as I understand, you get a; ```; KeyError: 'Wfdc18'; ```; when calling; ```; adata1 = adata[:, filter_result.gene_subset]; adata1.var.ix['Wfdc18']; ```; Right? So, 'Wfdc18' is no longer `adata1.var_names`. You can also check by typing; ```; print('Wfdc18' in adata1.var_names); ```; which should print `False`. Regarding plotting: as stated in the [basic tutorial](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb) in box [22], you have to pass `use_raw=False` if you want to access `adata.X` in plotting if `adata.raw` has been set, otherwise it assumes that you want to plot the raw data.; ```; sc.pl.pca(adata1, color='Wfdc18', use_raw=False); ```; which will throw an error after filtering if 'Wfdc18' is no longer there. Does this help and explain what you observe?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/109#issuecomment-375856560
https://github.com/scverse/scanpy/issues/109#issuecomment-375856560:538,Security,access,access,538,"Dear Olivia,; as I understand, you get a; ```; KeyError: 'Wfdc18'; ```; when calling; ```; adata1 = adata[:, filter_result.gene_subset]; adata1.var.ix['Wfdc18']; ```; Right? So, 'Wfdc18' is no longer `adata1.var_names`. You can also check by typing; ```; print('Wfdc18' in adata1.var_names); ```; which should print `False`. Regarding plotting: as stated in the [basic tutorial](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb) in box [22], you have to pass `use_raw=False` if you want to access `adata.X` in plotting if `adata.raw` has been set, otherwise it assumes that you want to plot the raw data.; ```; sc.pl.pca(adata1, color='Wfdc18', use_raw=False); ```; which will throw an error after filtering if 'Wfdc18' is no longer there. Does this help and explain what you observe?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/109#issuecomment-375856560
https://github.com/scverse/scanpy/issues/110#issuecomment-376174066:203,Deployability,release,release,203,Sorry about this and thanks for pointing it out! I'm currently intensively working on the revision of the method... a lot will become better. What you mention probably got lost on the way. I'm hoping to release a new version within a week.; Alex,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/110#issuecomment-376174066
https://github.com/scverse/scanpy/pull/114#issuecomment-378183576:91,Availability,robust,robust,91,No problem. I think increasing the test coverage should be prioritised to make scanpy more robust.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/114#issuecomment-378183576
https://github.com/scverse/scanpy/pull/114#issuecomment-378183576:35,Testability,test,test,35,No problem. I think increasing the test coverage should be prioritised to make scanpy more robust.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/114#issuecomment-378183576
https://github.com/scverse/scanpy/pull/114#issuecomment-378184419:172,Testability,test,tests,172,"Yes... you're of course right. For now, I added the call of diffmaps to a notebook... When graph abstraction is finished, I'll think about systematically adding a lot more tests...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/114#issuecomment-378184419
https://github.com/scverse/scanpy/pull/116#issuecomment-378952904:66,Integrability,interface,interface,66,Switching README.rst to markdown would also look nice on new pypi interface e.g. https://pypi.org/project/markdown-description-example/,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/116#issuecomment-378952904
https://github.com/scverse/scanpy/pull/119#issuecomment-379778109:295,Modifiability,rewrite,rewrite,295,"Cool, very interesting! :smile: Greatest advantage at first sight for me: `scanpy.api.AnnData` is now `anndata.AnnData`. Also, you don't seem to have to mingle around with autodoc anymore, which seems a good thing... The simpler docstrings are also nice... but it's going to be a lot of work to rewrite all the docstrings... also, there might be some danger of introducing bugs as one needs to rewrite the function headers. Hm, ... I'm a bit hesitant to just do this right away... Let's discuss! :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/119#issuecomment-379778109
https://github.com/scverse/scanpy/pull/119#issuecomment-379778109:394,Modifiability,rewrite,rewrite,394,"Cool, very interesting! :smile: Greatest advantage at first sight for me: `scanpy.api.AnnData` is now `anndata.AnnData`. Also, you don't seem to have to mingle around with autodoc anymore, which seems a good thing... The simpler docstrings are also nice... but it's going to be a lot of work to rewrite all the docstrings... also, there might be some danger of introducing bugs as one needs to rewrite the function headers. Hm, ... I'm a bit hesitant to just do this right away... Let's discuss! :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/119#issuecomment-379778109
https://github.com/scverse/scanpy/pull/119#issuecomment-379778109:221,Usability,simpl,simpler,221,"Cool, very interesting! :smile: Greatest advantage at first sight for me: `scanpy.api.AnnData` is now `anndata.AnnData`. Also, you don't seem to have to mingle around with autodoc anymore, which seems a good thing... The simpler docstrings are also nice... but it's going to be a lot of work to rewrite all the docstrings... also, there might be some danger of introducing bugs as one needs to rewrite the function headers. Hm, ... I'm a bit hesitant to just do this right away... Let's discuss! :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/119#issuecomment-379778109
https://github.com/scverse/scanpy/pull/119#issuecomment-379833230:871,Modifiability,rewrite,rewrite,871,"> Greatest advantage at first sight for me: scanpy.api.AnnData is now anndata.AnnData. to be fair, this was a simple consequence of adding intersphinx and would have been possible without the rest. > Also, you don't seem to have to mingle around with autodoc anymore, which seems a good thing... yes, but by now i added another, less invasive hack to get the parameter doc style the way you want them. it would be nice to have numpydoc-style parameter rendering as a separate extension or sphinx option. :skull_and_crossbones: the hack is also not finished, as in its current form, it’ll break docstrings with indentation (code blocks, lists, …). optimally the hack would be rewritten as a sphinx extension that can be loaded after the others. (it’s only a hack because it piggypacks on another extension just to ensure it runs last). > it's going to be a lot of work to rewrite all the docstrings... We don’t lose anything if we do it gradually: Unconverted Docstrings just render as they do now. > there might be some danger of introducing bugs as one needs to rewrite the function headers. i don’t think it’s possible to get bugs this way: we’re just adding type annotations, we don’t change the defaults or the order or anything.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/119#issuecomment-379833230
https://github.com/scverse/scanpy/pull/119#issuecomment-379833230:1063,Modifiability,rewrite,rewrite,1063,"> Greatest advantage at first sight for me: scanpy.api.AnnData is now anndata.AnnData. to be fair, this was a simple consequence of adding intersphinx and would have been possible without the rest. > Also, you don't seem to have to mingle around with autodoc anymore, which seems a good thing... yes, but by now i added another, less invasive hack to get the parameter doc style the way you want them. it would be nice to have numpydoc-style parameter rendering as a separate extension or sphinx option. :skull_and_crossbones: the hack is also not finished, as in its current form, it’ll break docstrings with indentation (code blocks, lists, …). optimally the hack would be rewritten as a sphinx extension that can be loaded after the others. (it’s only a hack because it piggypacks on another extension just to ensure it runs last). > it's going to be a lot of work to rewrite all the docstrings... We don’t lose anything if we do it gradually: Unconverted Docstrings just render as they do now. > there might be some danger of introducing bugs as one needs to rewrite the function headers. i don’t think it’s possible to get bugs this way: we’re just adding type annotations, we don’t change the defaults or the order or anything.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/119#issuecomment-379833230
https://github.com/scverse/scanpy/pull/119#issuecomment-379833230:719,Performance,load,loaded,719,"> Greatest advantage at first sight for me: scanpy.api.AnnData is now anndata.AnnData. to be fair, this was a simple consequence of adding intersphinx and would have been possible without the rest. > Also, you don't seem to have to mingle around with autodoc anymore, which seems a good thing... yes, but by now i added another, less invasive hack to get the parameter doc style the way you want them. it would be nice to have numpydoc-style parameter rendering as a separate extension or sphinx option. :skull_and_crossbones: the hack is also not finished, as in its current form, it’ll break docstrings with indentation (code blocks, lists, …). optimally the hack would be rewritten as a sphinx extension that can be loaded after the others. (it’s only a hack because it piggypacks on another extension just to ensure it runs last). > it's going to be a lot of work to rewrite all the docstrings... We don’t lose anything if we do it gradually: Unconverted Docstrings just render as they do now. > there might be some danger of introducing bugs as one needs to rewrite the function headers. i don’t think it’s possible to get bugs this way: we’re just adding type annotations, we don’t change the defaults or the order or anything.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/119#issuecomment-379833230
https://github.com/scverse/scanpy/pull/119#issuecomment-379833230:110,Usability,simpl,simple,110,"> Greatest advantage at first sight for me: scanpy.api.AnnData is now anndata.AnnData. to be fair, this was a simple consequence of adding intersphinx and would have been possible without the rest. > Also, you don't seem to have to mingle around with autodoc anymore, which seems a good thing... yes, but by now i added another, less invasive hack to get the parameter doc style the way you want them. it would be nice to have numpydoc-style parameter rendering as a separate extension or sphinx option. :skull_and_crossbones: the hack is also not finished, as in its current form, it’ll break docstrings with indentation (code blocks, lists, …). optimally the hack would be rewritten as a sphinx extension that can be loaded after the others. (it’s only a hack because it piggypacks on another extension just to ensure it runs last). > it's going to be a lot of work to rewrite all the docstrings... We don’t lose anything if we do it gradually: Unconverted Docstrings just render as they do now. > there might be some danger of introducing bugs as one needs to rewrite the function headers. i don’t think it’s possible to get bugs this way: we’re just adding type annotations, we don’t change the defaults or the order or anything.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/119#issuecomment-379833230
https://github.com/scverse/scanpy/pull/119#issuecomment-380066080:763,Usability,simpl,simply,763,"Hey! I just wanted to find you in your office... We should discuss and look at this in person... Non-working indentation, for instance, would be a serious problem... I'd suggest waiting a little bit longer before you write more hacks... in principle, I was satisfied with the docs except for referencing `scanpy.api.AnnData`... I still don't see the big advantage of using type annotation outside of the docstrings... As they are right now, they look very good when rendered as html and they look good as plain text when invoked within a Jupyter notebook... Of course, you usually have the better arguments on such questions in the end, but I'm not fully convinced at this stage. So, let's discuss in person. :smile:. PS: Evidently, scanpy's style for docstrings simply imitates numpy, pandas, seaborn, scikit-learn. I'm not sure whether one should break these conventions... Also, you imagine how much work it was - many iterations over the past 12 months - to get all the type annotations, the ""optional"" keyword and the default value into the docstrings... as I'm busy like crazy with other stuff, I'm again hesitant to make such big changes... :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/119#issuecomment-380066080
https://github.com/scverse/scanpy/pull/119#issuecomment-380066080:810,Usability,learn,learn,810,"Hey! I just wanted to find you in your office... We should discuss and look at this in person... Non-working indentation, for instance, would be a serious problem... I'd suggest waiting a little bit longer before you write more hacks... in principle, I was satisfied with the docs except for referencing `scanpy.api.AnnData`... I still don't see the big advantage of using type annotation outside of the docstrings... As they are right now, they look very good when rendered as html and they look good as plain text when invoked within a Jupyter notebook... Of course, you usually have the better arguments on such questions in the end, but I'm not fully convinced at this stage. So, let's discuss in person. :smile:. PS: Evidently, scanpy's style for docstrings simply imitates numpy, pandas, seaborn, scikit-learn. I'm not sure whether one should break these conventions... Also, you imagine how much work it was - many iterations over the past 12 months - to get all the type annotations, the ""optional"" keyword and the default value into the docstrings... as I'm busy like crazy with other stuff, I'm again hesitant to make such big changes... :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/119#issuecomment-380066080
https://github.com/scverse/scanpy/pull/119#issuecomment-380095294:947,Usability,simpl,simply,947,"> Hey! I just wanted to find you in your office... We should discuss and look at this in person... Sure! I’m at my gradma’s place right now, but I’ll be back tomorrow or thursday. > Non-working indentation, for instance, would be a serious problem... Yaya, that’s a very temporary hack because I just wanted a working version, nothing that stays. > in principle, I was satisfied with the docs except for referencing scanpy.api.AnnData... I still don't see the big advantage of using type annotation outside of the docstrings... The big one is that many of the bugs we had in the past and that we’ll have in the future can be prevented if your IDE/editor tells you “you can’t pass that thing here, wrong type.”. And by using type annotations in the code, it’s impossible to have them wrong (to forget a comma or so) and break the type format. I’ve seen quite some commits by you just fixing such a thing. > Evidently, scanpy's style for docstrings simply imitates numpy, pandas, seaborn, scikit-learn. I'm not sure whether one should break these conventions... We don’t break them. Typeless parameter annotations are still in numpy style 😉",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/119#issuecomment-380095294
https://github.com/scverse/scanpy/pull/119#issuecomment-380095294:994,Usability,learn,learn,994,"> Hey! I just wanted to find you in your office... We should discuss and look at this in person... Sure! I’m at my gradma’s place right now, but I’ll be back tomorrow or thursday. > Non-working indentation, for instance, would be a serious problem... Yaya, that’s a very temporary hack because I just wanted a working version, nothing that stays. > in principle, I was satisfied with the docs except for referencing scanpy.api.AnnData... I still don't see the big advantage of using type annotation outside of the docstrings... The big one is that many of the bugs we had in the past and that we’ll have in the future can be prevented if your IDE/editor tells you “you can’t pass that thing here, wrong type.”. And by using type annotations in the code, it’s impossible to have them wrong (to forget a comma or so) and break the type format. I’ve seen quite some commits by you just fixing such a thing. > Evidently, scanpy's style for docstrings simply imitates numpy, pandas, seaborn, scikit-learn. I'm not sure whether one should break these conventions... We don’t break them. Typeless parameter annotations are still in numpy style 😉",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/119#issuecomment-380095294
https://github.com/scverse/scanpy/issues/120#issuecomment-380016607:523,Usability,simpl,simple-pseudotime,523,Hi! I added a more extensive explanation in the [docs](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.diffmap.html#scanpy.api.tl.diffmap). . The line in the code where the sigmas are calculated is [here](https://github.com/theislab/scanpy/blob/e78062a0e4f02888cab080f8ed2571ff7764efc7/scanpy/neighbors/__init__.py#L725). There is no explicit way of changing this parameter... you can only implicitly change it via the number of neighbors. See the [examples](https://scanpy.readthedocs.io/en/latest/examples.html#simple-pseudotime). Does this help?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/120#issuecomment-380016607
https://github.com/scverse/scanpy/issues/120#issuecomment-380258000:339,Testability,test,tests,339,"Thanks for the quick response. My understanding is that diffusion maps are extremely sensitive to sigma, so I'm curious why you don't allow direct control of this parameter?. I also notice the following comment in your source code:; ""choose sigma, the heuristic here doesn't seem to make much of a difference"". Does this mean that in your tests, varying sigma doesn't make much of a difference? If so, I'm curious why this is because in my own tests using the destiny package, choice of sigma matters quite a bit.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/120#issuecomment-380258000
https://github.com/scverse/scanpy/issues/120#issuecomment-380258000:444,Testability,test,tests,444,"Thanks for the quick response. My understanding is that diffusion maps are extremely sensitive to sigma, so I'm curious why you don't allow direct control of this parameter?. I also notice the following comment in your source code:; ""choose sigma, the heuristic here doesn't seem to make much of a difference"". Does this mean that in your tests, varying sigma doesn't make much of a difference? If so, I'm curious why this is because in my own tests using the destiny package, choice of sigma matters quite a bit.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/120#issuecomment-380258000
https://github.com/scverse/scanpy/issues/120#issuecomment-380350482:795,Usability,clear,clear,795,"The ""choice"" mentioned in the source code regards how to compute sigma from the distances of the k nearest neighbors. In the default setting (sparse knn graph), this uses the median. In the completely different original dense and smooth setting of Coifman and Laleh's first diffmap paper, this uses the distance to the k-th neighbor divided by 4... Both of theses choices are completely ad hoc... The only goal that they should achieve is give you some smoothing already *among* the k nearest neighbors, and not just beyond (there are no further neighbors in the sparse setting). In destiny, some additional flexibility is allowed. You can choose the number of nearest neighbors and the sigma. Even though in principle it's ok to have both parameters, from the interpretation view it's not very clear. Both parameters affect the size of your effective local neighborhoods. In the dense setting, varying sigma makes a huge difference in destiny as it controls the neighborhood size alone, you can achieve the same by varying `n_neighbors` in scanpy. In the sparse setting, sigma does not influence the result a lot any more, if you don't choose pathological values, but `k` (called `n_neighbors` in scanpy) dramatically determines the size of the local neighborhood. So Scanpy tries to not burden the user with a parameter sigma that might take some thinking to understand and destroys consistency between the sparse-dense setting. Having said that, I'd generally recommend using umap or draw_graph for the things that you did with diffmap up to now... It's so much easier to have everything in 2 dimensions... Does this help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/120#issuecomment-380350482
https://github.com/scverse/scanpy/issues/121#issuecomment-381527583:3,Availability,error,error,3,"An error jumped out when normalizing, turns out I had a bad dataset mixed in and after filter there were no cell left, hence the error. Thanks!!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/121#issuecomment-381527583
https://github.com/scverse/scanpy/issues/121#issuecomment-381527583:129,Availability,error,error,129,"An error jumped out when normalizing, turns out I had a bad dataset mixed in and after filter there were no cell left, hence the error. Thanks!!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/121#issuecomment-381527583
https://github.com/scverse/scanpy/issues/122#issuecomment-381551334:147,Availability,avail,available,147,"Hey!. We do it as they do in Seurat. See [here](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb), available through the [examples page](http://scanpy.readthedocs.io/en/latest/examples.html):; ```; adata = adata[adata.obs['percent_mito'] < 0.05, :]; ```; in Box 8. Does this help you or do you need more?. Best,; lex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/122#issuecomment-381551334
https://github.com/scverse/scanpy/issues/123#issuecomment-381650708:277,Security,access,access,277,"Seems like an ""old"" X_diffmap has only length 2056. But that should not be the case, of course, as that would be invalid. I just learned about a bug in the storage of the graph `.uns['neighbors']['connectivities']` that appears when you do subsetting on an AnnData and want to access the original object. That could explain what is happening... I'll submit a bug for that in the next couple of hours.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/123#issuecomment-381650708
https://github.com/scverse/scanpy/issues/123#issuecomment-381650708:129,Usability,learn,learned,129,"Seems like an ""old"" X_diffmap has only length 2056. But that should not be the case, of course, as that would be invalid. I just learned about a bug in the storage of the graph `.uns['neighbors']['connectivities']` that appears when you do subsetting on an AnnData and want to access the original object. That could explain what is happening... I'll submit a bug for that in the next couple of hours.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/123#issuecomment-381650708
https://github.com/scverse/scanpy/issues/123#issuecomment-381746375:77,Deployability,install,install,77,"If it's what I suspect it to be, it should be fixed with anndata 0.5.9. `pip install anndata==0.5.9`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/123#issuecomment-381746375
https://github.com/scverse/scanpy/pull/125#issuecomment-381973984:37,Deployability,install,installed,37,The build environment doesn't have R installed so the checks failed....,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-381973984
https://github.com/scverse/scanpy/pull/125#issuecomment-381980880:62,Deployability,install,install,62,"One of the aims of scanpy is to be self-contained and easy-to-install for users and also to be easy to maintain by the developers. Heavy dependencies like louvain and python-igraph are already troublesome, expecting users to have rpy2 + proper R installation + Bioconductor + scran would risk smooth user experience and easy maintainability. I was wondering whether it makes sense to have a community-maintained `scanpy-contrib` or `scanpy-extensions` repository (and python package) similar to https://github.com/keras-team/keras-contrib ? There are also couple of things I have in mind like `sc.pl.netsne(adata, anotheradata)` for embedding unseen samples via parametric tSNE, or `sc.tl.simlr` and `sc.pl.simlr` for [SIMLR](https://github.com/BatzoglouLabSU/SIMLR) via RPy2 bridge... . These are popular requests for Scanpy and people expect the same convenient API and an easy integration with AnnData objects. However, they will probably not be included in the mainstream Scanpy because of the reasons I mentioned above. What do you think @falexwolf and @flying-sheep ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-381980880
https://github.com/scverse/scanpy/pull/125#issuecomment-381980880:246,Deployability,install,installation,246,"One of the aims of scanpy is to be self-contained and easy-to-install for users and also to be easy to maintain by the developers. Heavy dependencies like louvain and python-igraph are already troublesome, expecting users to have rpy2 + proper R installation + Bioconductor + scran would risk smooth user experience and easy maintainability. I was wondering whether it makes sense to have a community-maintained `scanpy-contrib` or `scanpy-extensions` repository (and python package) similar to https://github.com/keras-team/keras-contrib ? There are also couple of things I have in mind like `sc.pl.netsne(adata, anotheradata)` for embedding unseen samples via parametric tSNE, or `sc.tl.simlr` and `sc.pl.simlr` for [SIMLR](https://github.com/BatzoglouLabSU/SIMLR) via RPy2 bridge... . These are popular requests for Scanpy and people expect the same convenient API and an easy integration with AnnData objects. However, they will probably not be included in the mainstream Scanpy because of the reasons I mentioned above. What do you think @falexwolf and @flying-sheep ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-381980880
https://github.com/scverse/scanpy/pull/125#issuecomment-381980880:880,Deployability,integrat,integration,880,"One of the aims of scanpy is to be self-contained and easy-to-install for users and also to be easy to maintain by the developers. Heavy dependencies like louvain and python-igraph are already troublesome, expecting users to have rpy2 + proper R installation + Bioconductor + scran would risk smooth user experience and easy maintainability. I was wondering whether it makes sense to have a community-maintained `scanpy-contrib` or `scanpy-extensions` repository (and python package) similar to https://github.com/keras-team/keras-contrib ? There are also couple of things I have in mind like `sc.pl.netsne(adata, anotheradata)` for embedding unseen samples via parametric tSNE, or `sc.tl.simlr` and `sc.pl.simlr` for [SIMLR](https://github.com/BatzoglouLabSU/SIMLR) via RPy2 bridge... . These are popular requests for Scanpy and people expect the same convenient API and an easy integration with AnnData objects. However, they will probably not be included in the mainstream Scanpy because of the reasons I mentioned above. What do you think @falexwolf and @flying-sheep ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-381980880
https://github.com/scverse/scanpy/pull/125#issuecomment-381980880:137,Integrability,depend,dependencies,137,"One of the aims of scanpy is to be self-contained and easy-to-install for users and also to be easy to maintain by the developers. Heavy dependencies like louvain and python-igraph are already troublesome, expecting users to have rpy2 + proper R installation + Bioconductor + scran would risk smooth user experience and easy maintainability. I was wondering whether it makes sense to have a community-maintained `scanpy-contrib` or `scanpy-extensions` repository (and python package) similar to https://github.com/keras-team/keras-contrib ? There are also couple of things I have in mind like `sc.pl.netsne(adata, anotheradata)` for embedding unseen samples via parametric tSNE, or `sc.tl.simlr` and `sc.pl.simlr` for [SIMLR](https://github.com/BatzoglouLabSU/SIMLR) via RPy2 bridge... . These are popular requests for Scanpy and people expect the same convenient API and an easy integration with AnnData objects. However, they will probably not be included in the mainstream Scanpy because of the reasons I mentioned above. What do you think @falexwolf and @flying-sheep ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-381980880
https://github.com/scverse/scanpy/pull/125#issuecomment-381980880:776,Integrability,bridg,bridge,776,"One of the aims of scanpy is to be self-contained and easy-to-install for users and also to be easy to maintain by the developers. Heavy dependencies like louvain and python-igraph are already troublesome, expecting users to have rpy2 + proper R installation + Bioconductor + scran would risk smooth user experience and easy maintainability. I was wondering whether it makes sense to have a community-maintained `scanpy-contrib` or `scanpy-extensions` repository (and python package) similar to https://github.com/keras-team/keras-contrib ? There are also couple of things I have in mind like `sc.pl.netsne(adata, anotheradata)` for embedding unseen samples via parametric tSNE, or `sc.tl.simlr` and `sc.pl.simlr` for [SIMLR](https://github.com/BatzoglouLabSU/SIMLR) via RPy2 bridge... . These are popular requests for Scanpy and people expect the same convenient API and an easy integration with AnnData objects. However, they will probably not be included in the mainstream Scanpy because of the reasons I mentioned above. What do you think @falexwolf and @flying-sheep ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-381980880
https://github.com/scverse/scanpy/pull/125#issuecomment-381980880:880,Integrability,integrat,integration,880,"One of the aims of scanpy is to be self-contained and easy-to-install for users and also to be easy to maintain by the developers. Heavy dependencies like louvain and python-igraph are already troublesome, expecting users to have rpy2 + proper R installation + Bioconductor + scran would risk smooth user experience and easy maintainability. I was wondering whether it makes sense to have a community-maintained `scanpy-contrib` or `scanpy-extensions` repository (and python package) similar to https://github.com/keras-team/keras-contrib ? There are also couple of things I have in mind like `sc.pl.netsne(adata, anotheradata)` for embedding unseen samples via parametric tSNE, or `sc.tl.simlr` and `sc.pl.simlr` for [SIMLR](https://github.com/BatzoglouLabSU/SIMLR) via RPy2 bridge... . These are popular requests for Scanpy and people expect the same convenient API and an easy integration with AnnData objects. However, they will probably not be included in the mainstream Scanpy because of the reasons I mentioned above. What do you think @falexwolf and @flying-sheep ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-381980880
https://github.com/scverse/scanpy/pull/125#issuecomment-381980880:325,Modifiability,maintainab,maintainability,325,"One of the aims of scanpy is to be self-contained and easy-to-install for users and also to be easy to maintain by the developers. Heavy dependencies like louvain and python-igraph are already troublesome, expecting users to have rpy2 + proper R installation + Bioconductor + scran would risk smooth user experience and easy maintainability. I was wondering whether it makes sense to have a community-maintained `scanpy-contrib` or `scanpy-extensions` repository (and python package) similar to https://github.com/keras-team/keras-contrib ? There are also couple of things I have in mind like `sc.pl.netsne(adata, anotheradata)` for embedding unseen samples via parametric tSNE, or `sc.tl.simlr` and `sc.pl.simlr` for [SIMLR](https://github.com/BatzoglouLabSU/SIMLR) via RPy2 bridge... . These are popular requests for Scanpy and people expect the same convenient API and an easy integration with AnnData objects. However, they will probably not be included in the mainstream Scanpy because of the reasons I mentioned above. What do you think @falexwolf and @flying-sheep ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-381980880
https://github.com/scverse/scanpy/pull/125#issuecomment-381980880:288,Safety,risk,risk,288,"One of the aims of scanpy is to be self-contained and easy-to-install for users and also to be easy to maintain by the developers. Heavy dependencies like louvain and python-igraph are already troublesome, expecting users to have rpy2 + proper R installation + Bioconductor + scran would risk smooth user experience and easy maintainability. I was wondering whether it makes sense to have a community-maintained `scanpy-contrib` or `scanpy-extensions` repository (and python package) similar to https://github.com/keras-team/keras-contrib ? There are also couple of things I have in mind like `sc.pl.netsne(adata, anotheradata)` for embedding unseen samples via parametric tSNE, or `sc.tl.simlr` and `sc.pl.simlr` for [SIMLR](https://github.com/BatzoglouLabSU/SIMLR) via RPy2 bridge... . These are popular requests for Scanpy and people expect the same convenient API and an easy integration with AnnData objects. However, they will probably not be included in the mainstream Scanpy because of the reasons I mentioned above. What do you think @falexwolf and @flying-sheep ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-381980880
https://github.com/scverse/scanpy/pull/125#issuecomment-381980880:300,Usability,user experience,user experience,300,"One of the aims of scanpy is to be self-contained and easy-to-install for users and also to be easy to maintain by the developers. Heavy dependencies like louvain and python-igraph are already troublesome, expecting users to have rpy2 + proper R installation + Bioconductor + scran would risk smooth user experience and easy maintainability. I was wondering whether it makes sense to have a community-maintained `scanpy-contrib` or `scanpy-extensions` repository (and python package) similar to https://github.com/keras-team/keras-contrib ? There are also couple of things I have in mind like `sc.pl.netsne(adata, anotheradata)` for embedding unseen samples via parametric tSNE, or `sc.tl.simlr` and `sc.pl.simlr` for [SIMLR](https://github.com/BatzoglouLabSU/SIMLR) via RPy2 bridge... . These are popular requests for Scanpy and people expect the same convenient API and an easy integration with AnnData objects. However, they will probably not be included in the mainstream Scanpy because of the reasons I mentioned above. What do you think @falexwolf and @flying-sheep ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-381980880
https://github.com/scverse/scanpy/pull/125#issuecomment-381984759:955,Availability,mainten,maintenance,955,"Completely agree, Gökcen!. How I just thought about dealing with this in the past couple of minutes: could we not make a submodule *rtools*? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a `scanpy-contrib`: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: `anndata` is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-381984759
https://github.com/scverse/scanpy/pull/125#issuecomment-381984759:380,Deployability,install,installation,380,"Completely agree, Gökcen!. How I just thought about dealing with this in the past couple of minutes: could we not make a submodule *rtools*? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a `scanpy-contrib`: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: `anndata` is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-381984759
https://github.com/scverse/scanpy/pull/125#issuecomment-381984759:169,Integrability,wrap,wrapper,169,"Completely agree, Gökcen!. How I just thought about dealing with this in the past couple of minutes: could we not make a submodule *rtools*? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a `scanpy-contrib`: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: `anndata` is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-381984759
https://github.com/scverse/scanpy/pull/125#issuecomment-381984759:227,Integrability,depend,dependencies,227,"Completely agree, Gökcen!. How I just thought about dealing with this in the past couple of minutes: could we not make a submodule *rtools*? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a `scanpy-contrib`: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: `anndata` is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-381984759
https://github.com/scverse/scanpy/pull/125#issuecomment-381984759:307,Integrability,wrap,wrapper,307,"Completely agree, Gökcen!. How I just thought about dealing with this in the past couple of minutes: could we not make a submodule *rtools*? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a `scanpy-contrib`: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: `anndata` is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-381984759
https://github.com/scverse/scanpy/pull/125#issuecomment-381984759:571,Integrability,wrap,wrappers,571,"Completely agree, Gökcen!. How I just thought about dealing with this in the past couple of minutes: could we not make a submodule *rtools*? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a `scanpy-contrib`: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: `anndata` is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-381984759
https://github.com/scverse/scanpy/pull/125#issuecomment-381984759:632,Integrability,interface,interfaces,632,"Completely agree, Gökcen!. How I just thought about dealing with this in the past couple of minutes: could we not make a submodule *rtools*? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a `scanpy-contrib`: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: `anndata` is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-381984759
https://github.com/scverse/scanpy/pull/125#issuecomment-381984759:555,Testability,test,tests,555,"Completely agree, Gökcen!. How I just thought about dealing with this in the past couple of minutes: could we not make a submodule *rtools*? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a `scanpy-contrib`: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: `anndata` is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-381984759
https://github.com/scverse/scanpy/pull/125#issuecomment-382002082:241,Energy Efficiency,efficient,efficient,241,"I completely agree that including the R/scran requirements will be troublesome and harms user experience. The reason I used a R-py interface is that there's no decent MNN correct on python yet, and scran's implementation is already fast and efficient enough, and I think this is meant to be an optional feature that provides a handy fix for those in need. Personally I would prefer if you guys create a submodule _rtools_, and put wrappers inside. This is going to be awesome to use and easy to maintain.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-382002082
https://github.com/scverse/scanpy/pull/125#issuecomment-382002082:131,Integrability,interface,interface,131,"I completely agree that including the R/scran requirements will be troublesome and harms user experience. The reason I used a R-py interface is that there's no decent MNN correct on python yet, and scran's implementation is already fast and efficient enough, and I think this is meant to be an optional feature that provides a handy fix for those in need. Personally I would prefer if you guys create a submodule _rtools_, and put wrappers inside. This is going to be awesome to use and easy to maintain.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-382002082
https://github.com/scverse/scanpy/pull/125#issuecomment-382002082:431,Integrability,wrap,wrappers,431,"I completely agree that including the R/scran requirements will be troublesome and harms user experience. The reason I used a R-py interface is that there's no decent MNN correct on python yet, and scran's implementation is already fast and efficient enough, and I think this is meant to be an optional feature that provides a handy fix for those in need. Personally I would prefer if you guys create a submodule _rtools_, and put wrappers inside. This is going to be awesome to use and easy to maintain.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-382002082
https://github.com/scverse/scanpy/pull/125#issuecomment-382002082:89,Usability,user experience,user experience,89,"I completely agree that including the R/scran requirements will be troublesome and harms user experience. The reason I used a R-py interface is that there's no decent MNN correct on python yet, and scran's implementation is already fast and efficient enough, and I think this is meant to be an optional feature that provides a handy fix for those in need. Personally I would prefer if you guys create a submodule _rtools_, and put wrappers inside. This is going to be awesome to use and easy to maintain.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-382002082
https://github.com/scverse/scanpy/pull/125#issuecomment-382002901:1211,Availability,mainten,maintenance,1211,"API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful.; > . That'd make things a lot easier for many people (including myself 😃), I agree. However. 1) There are (and will be) so many R packages about single cell, so once we open the door, there might be so many requests about these packages so that it'd be difficult to decide what to include and what not to include. The decision might be a bit arbitrary. This is why I suggested a contrib repo, which will have everything users request (as soon as there is someone who is willing to maintain it), in a `use at your own risk` way... 2) There might be several bug reports about rpy2 itself or thin wrappers or R installation or R packages themselves. I was wondering whether this might introduce more maintenance burden, although supported packages will be limited. > The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a scanpy-contrib: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: anndata is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions.; > ; > What do you think?. Alternatively, we can just prepare jupyter notebooks with some Python 3 and some R cells in it (which is super easy via rpy2 magics anyway) for some R packages/functions like mnn or SIMLR and put those in scanpy_usage as a reference for the community. For example:. ![image](https://user-images.githubusercontent.com/1140359/38873972-4953977a-4257-11e8-8675-a238738eb558.png). Another question is other single cell Python packages like magi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-382002901
https://github.com/scverse/scanpy/pull/125#issuecomment-382002901:1721,Availability,mainten,maintenance,1721,"ple who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful.; > . That'd make things a lot easier for many people (including myself 😃), I agree. However. 1) There are (and will be) so many R packages about single cell, so once we open the door, there might be so many requests about these packages so that it'd be difficult to decide what to include and what not to include. The decision might be a bit arbitrary. This is why I suggested a contrib repo, which will have everything users request (as soon as there is someone who is willing to maintain it), in a `use at your own risk` way... 2) There might be several bug reports about rpy2 itself or thin wrappers or R installation or R packages themselves. I was wondering whether this might introduce more maintenance burden, although supported packages will be limited. > The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a scanpy-contrib: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: anndata is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions.; > ; > What do you think?. Alternatively, we can just prepare jupyter notebooks with some Python 3 and some R cells in it (which is super easy via rpy2 magics anyway) for some R packages/functions like mnn or SIMLR and put those in scanpy_usage as a reference for the community. For example:. ![image](https://user-images.githubusercontent.com/1140359/38873972-4953977a-4257-11e8-8675-a238738eb558.png). Another question is other single cell Python packages like magic, ZIFA or DCA, for example. There will hopefully be more in the future. A contrib repo might include these, as well i.e. `sc.tl.magic`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-382002901
https://github.com/scverse/scanpy/pull/125#issuecomment-382002901:386,Deployability,install,installation,386,"> Completely agree, Gökcen!; > ; > How I just thought about dealing with this in the past couple of minutes: could we not make a submodule rtools? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful.; > . That'd make things a lot easier for many people (including myself 😃), I agree. However. 1) There are (and will be) so many R packages about single cell, so once we open the door, there might be so many requests about these packages so that it'd be difficult to decide what to include and what not to include. The decision might be a bit arbitrary. This is why I suggested a contrib repo, which will have everything users request (as soon as there is someone who is willing to maintain it), in a `use at your own risk` way... 2) There might be several bug reports about rpy2 itself or thin wrappers or R installation or R packages themselves. I was wondering whether this might introduce more maintenance burden, although supported packages will be limited. > The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a scanpy-contrib: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: anndata is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions.; > ; > What do you think?. Alternatively, we can just prepare jupyter notebooks with some Python 3 and some R cells in it (which is super easy via rpy2 magics anyway) for some R packages/functions like mnn or SIMLR and put those in scanpy_usage as a ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-382002901
https://github.com/scverse/scanpy/pull/125#issuecomment-382002901:1122,Deployability,install,installation,1122,"inutes: could we not make a submodule rtools? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful.; > . That'd make things a lot easier for many people (including myself 😃), I agree. However. 1) There are (and will be) so many R packages about single cell, so once we open the door, there might be so many requests about these packages so that it'd be difficult to decide what to include and what not to include. The decision might be a bit arbitrary. This is why I suggested a contrib repo, which will have everything users request (as soon as there is someone who is willing to maintain it), in a `use at your own risk` way... 2) There might be several bug reports about rpy2 itself or thin wrappers or R installation or R packages themselves. I was wondering whether this might introduce more maintenance burden, although supported packages will be limited. > The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a scanpy-contrib: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: anndata is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions.; > ; > What do you think?. Alternatively, we can just prepare jupyter notebooks with some Python 3 and some R cells in it (which is super easy via rpy2 magics anyway) for some R packages/functions like mnn or SIMLR and put those in scanpy_usage as a reference for the community. For example:. ![image](https://user-images.githubusercontent.com/1140359",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-382002901
https://github.com/scverse/scanpy/pull/125#issuecomment-382002901:175,Integrability,wrap,wrapper,175,"> Completely agree, Gökcen!; > ; > How I just thought about dealing with this in the past couple of minutes: could we not make a submodule rtools? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful.; > . That'd make things a lot easier for many people (including myself 😃), I agree. However. 1) There are (and will be) so many R packages about single cell, so once we open the door, there might be so many requests about these packages so that it'd be difficult to decide what to include and what not to include. The decision might be a bit arbitrary. This is why I suggested a contrib repo, which will have everything users request (as soon as there is someone who is willing to maintain it), in a `use at your own risk` way... 2) There might be several bug reports about rpy2 itself or thin wrappers or R installation or R packages themselves. I was wondering whether this might introduce more maintenance burden, although supported packages will be limited. > The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a scanpy-contrib: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: anndata is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions.; > ; > What do you think?. Alternatively, we can just prepare jupyter notebooks with some Python 3 and some R cells in it (which is super easy via rpy2 magics anyway) for some R packages/functions like mnn or SIMLR and put those in scanpy_usage as a ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-382002901
https://github.com/scverse/scanpy/pull/125#issuecomment-382002901:233,Integrability,depend,dependencies,233,"> Completely agree, Gökcen!; > ; > How I just thought about dealing with this in the past couple of minutes: could we not make a submodule rtools? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful.; > . That'd make things a lot easier for many people (including myself 😃), I agree. However. 1) There are (and will be) so many R packages about single cell, so once we open the door, there might be so many requests about these packages so that it'd be difficult to decide what to include and what not to include. The decision might be a bit arbitrary. This is why I suggested a contrib repo, which will have everything users request (as soon as there is someone who is willing to maintain it), in a `use at your own risk` way... 2) There might be several bug reports about rpy2 itself or thin wrappers or R installation or R packages themselves. I was wondering whether this might introduce more maintenance burden, although supported packages will be limited. > The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a scanpy-contrib: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: anndata is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions.; > ; > What do you think?. Alternatively, we can just prepare jupyter notebooks with some Python 3 and some R cells in it (which is super easy via rpy2 magics anyway) for some R packages/functions like mnn or SIMLR and put those in scanpy_usage as a ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-382002901
https://github.com/scverse/scanpy/pull/125#issuecomment-382002901:313,Integrability,wrap,wrapper,313,"> Completely agree, Gökcen!; > ; > How I just thought about dealing with this in the past couple of minutes: could we not make a submodule rtools? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful.; > . That'd make things a lot easier for many people (including myself 😃), I agree. However. 1) There are (and will be) so many R packages about single cell, so once we open the door, there might be so many requests about these packages so that it'd be difficult to decide what to include and what not to include. The decision might be a bit arbitrary. This is why I suggested a contrib repo, which will have everything users request (as soon as there is someone who is willing to maintain it), in a `use at your own risk` way... 2) There might be several bug reports about rpy2 itself or thin wrappers or R installation or R packages themselves. I was wondering whether this might introduce more maintenance burden, although supported packages will be limited. > The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a scanpy-contrib: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: anndata is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions.; > ; > What do you think?. Alternatively, we can just prepare jupyter notebooks with some Python 3 and some R cells in it (which is super easy via rpy2 magics anyway) for some R packages/functions like mnn or SIMLR and put those in scanpy_usage as a ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-382002901
https://github.com/scverse/scanpy/pull/125#issuecomment-382002901:1108,Integrability,wrap,wrappers,1108,"inutes: could we not make a submodule rtools? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful.; > . That'd make things a lot easier for many people (including myself 😃), I agree. However. 1) There are (and will be) so many R packages about single cell, so once we open the door, there might be so many requests about these packages so that it'd be difficult to decide what to include and what not to include. The decision might be a bit arbitrary. This is why I suggested a contrib repo, which will have everything users request (as soon as there is someone who is willing to maintain it), in a `use at your own risk` way... 2) There might be several bug reports about rpy2 itself or thin wrappers or R installation or R packages themselves. I was wondering whether this might introduce more maintenance burden, although supported packages will be limited. > The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a scanpy-contrib: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: anndata is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions.; > ; > What do you think?. Alternatively, we can just prepare jupyter notebooks with some Python 3 and some R cells in it (which is super easy via rpy2 magics anyway) for some R packages/functions like mnn or SIMLR and put those in scanpy_usage as a reference for the community. For example:. ![image](https://user-images.githubusercontent.com/1140359",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-382002901
https://github.com/scverse/scanpy/pull/125#issuecomment-382002901:1341,Integrability,wrap,wrappers,1341,"ple who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful.; > . That'd make things a lot easier for many people (including myself 😃), I agree. However. 1) There are (and will be) so many R packages about single cell, so once we open the door, there might be so many requests about these packages so that it'd be difficult to decide what to include and what not to include. The decision might be a bit arbitrary. This is why I suggested a contrib repo, which will have everything users request (as soon as there is someone who is willing to maintain it), in a `use at your own risk` way... 2) There might be several bug reports about rpy2 itself or thin wrappers or R installation or R packages themselves. I was wondering whether this might introduce more maintenance burden, although supported packages will be limited. > The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a scanpy-contrib: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: anndata is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions.; > ; > What do you think?. Alternatively, we can just prepare jupyter notebooks with some Python 3 and some R cells in it (which is super easy via rpy2 magics anyway) for some R packages/functions like mnn or SIMLR and put those in scanpy_usage as a reference for the community. For example:. ![image](https://user-images.githubusercontent.com/1140359/38873972-4953977a-4257-11e8-8675-a238738eb558.png). Another question is other single cell Python packages like magic, ZIFA or DCA, for example. There will hopefully be more in the future. A contrib repo might include these, as well i.e. `sc.tl.magic`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-382002901
https://github.com/scverse/scanpy/pull/125#issuecomment-382002901:1402,Integrability,interface,interfaces,1402,"ple who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful.; > . That'd make things a lot easier for many people (including myself 😃), I agree. However. 1) There are (and will be) so many R packages about single cell, so once we open the door, there might be so many requests about these packages so that it'd be difficult to decide what to include and what not to include. The decision might be a bit arbitrary. This is why I suggested a contrib repo, which will have everything users request (as soon as there is someone who is willing to maintain it), in a `use at your own risk` way... 2) There might be several bug reports about rpy2 itself or thin wrappers or R installation or R packages themselves. I was wondering whether this might introduce more maintenance burden, although supported packages will be limited. > The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a scanpy-contrib: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: anndata is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions.; > ; > What do you think?. Alternatively, we can just prepare jupyter notebooks with some Python 3 and some R cells in it (which is super easy via rpy2 magics anyway) for some R packages/functions like mnn or SIMLR and put those in scanpy_usage as a reference for the community. For example:. ![image](https://user-images.githubusercontent.com/1140359/38873972-4953977a-4257-11e8-8675-a238738eb558.png). Another question is other single cell Python packages like magic, ZIFA or DCA, for example. There will hopefully be more in the future. A contrib repo might include these, as well i.e. `sc.tl.magic`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-382002901
https://github.com/scverse/scanpy/pull/125#issuecomment-382002901:1031,Safety,risk,risk,1031,"> Completely agree, Gökcen!; > ; > How I just thought about dealing with this in the past couple of minutes: could we not make a submodule rtools? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful.; > . That'd make things a lot easier for many people (including myself 😃), I agree. However. 1) There are (and will be) so many R packages about single cell, so once we open the door, there might be so many requests about these packages so that it'd be difficult to decide what to include and what not to include. The decision might be a bit arbitrary. This is why I suggested a contrib repo, which will have everything users request (as soon as there is someone who is willing to maintain it), in a `use at your own risk` way... 2) There might be several bug reports about rpy2 itself or thin wrappers or R installation or R packages themselves. I was wondering whether this might introduce more maintenance burden, although supported packages will be limited. > The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a scanpy-contrib: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: anndata is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions.; > ; > What do you think?. Alternatively, we can just prepare jupyter notebooks with some Python 3 and some R cells in it (which is super easy via rpy2 magics anyway) for some R packages/functions like mnn or SIMLR and put those in scanpy_usage as a ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-382002901
https://github.com/scverse/scanpy/pull/125#issuecomment-382002901:1325,Testability,test,tests,1325,"ple who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful.; > . That'd make things a lot easier for many people (including myself 😃), I agree. However. 1) There are (and will be) so many R packages about single cell, so once we open the door, there might be so many requests about these packages so that it'd be difficult to decide what to include and what not to include. The decision might be a bit arbitrary. This is why I suggested a contrib repo, which will have everything users request (as soon as there is someone who is willing to maintain it), in a `use at your own risk` way... 2) There might be several bug reports about rpy2 itself or thin wrappers or R installation or R packages themselves. I was wondering whether this might introduce more maintenance burden, although supported packages will be limited. > The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a scanpy-contrib: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: anndata is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions.; > ; > What do you think?. Alternatively, we can just prepare jupyter notebooks with some Python 3 and some R cells in it (which is super easy via rpy2 magics anyway) for some R packages/functions like mnn or SIMLR and put those in scanpy_usage as a reference for the community. For example:. ![image](https://user-images.githubusercontent.com/1140359/38873972-4953977a-4257-11e8-8675-a238738eb558.png). Another question is other single cell Python packages like magic, ZIFA or DCA, for example. There will hopefully be more in the future. A contrib repo might include these, as well i.e. `sc.tl.magic`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-382002901
https://github.com/scverse/scanpy/pull/125#issuecomment-382344299:843,Deployability,install,installing,843,"I still don't see 100% why the submodule `rtools` would be so much worse than `scanpy-contrib`. The submodule would be separate from the rest of the package and we could write something on top of its API overview page like: interfaces for R tools, address the maintainers of these tools for help... We need to keep some structure so that things remain clean, but yeah, additions will always be somewhat arbitrary. If someone suggests a ""meaningful addition"", we will accept it, if someone suggests something that does not seem to be of good quality, we will reject it. But this is not a problem only for `rtools` but for wrappers of python packages as well... see, for instance, https://github.com/theislab/scanpy/pull/126. So, I would set up the `rtools` submodule to save us the work of maintaining a different repo and the user the work of installing a `scanpy-contrib` package and figuring out which namespaces to use so that notebooks don't get completely messed up. So, if you don't mind, I'd set up the ""rtools"" submodule...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-382344299
https://github.com/scverse/scanpy/pull/125#issuecomment-382344299:224,Integrability,interface,interfaces,224,"I still don't see 100% why the submodule `rtools` would be so much worse than `scanpy-contrib`. The submodule would be separate from the rest of the package and we could write something on top of its API overview page like: interfaces for R tools, address the maintainers of these tools for help... We need to keep some structure so that things remain clean, but yeah, additions will always be somewhat arbitrary. If someone suggests a ""meaningful addition"", we will accept it, if someone suggests something that does not seem to be of good quality, we will reject it. But this is not a problem only for `rtools` but for wrappers of python packages as well... see, for instance, https://github.com/theislab/scanpy/pull/126. So, I would set up the `rtools` submodule to save us the work of maintaining a different repo and the user the work of installing a `scanpy-contrib` package and figuring out which namespaces to use so that notebooks don't get completely messed up. So, if you don't mind, I'd set up the ""rtools"" submodule...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-382344299
https://github.com/scverse/scanpy/pull/125#issuecomment-382344299:621,Integrability,wrap,wrappers,621,"I still don't see 100% why the submodule `rtools` would be so much worse than `scanpy-contrib`. The submodule would be separate from the rest of the package and we could write something on top of its API overview page like: interfaces for R tools, address the maintainers of these tools for help... We need to keep some structure so that things remain clean, but yeah, additions will always be somewhat arbitrary. If someone suggests a ""meaningful addition"", we will accept it, if someone suggests something that does not seem to be of good quality, we will reject it. But this is not a problem only for `rtools` but for wrappers of python packages as well... see, for instance, https://github.com/theislab/scanpy/pull/126. So, I would set up the `rtools` submodule to save us the work of maintaining a different repo and the user the work of installing a `scanpy-contrib` package and figuring out which namespaces to use so that notebooks don't get completely messed up. So, if you don't mind, I'd set up the ""rtools"" submodule...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-382344299
https://github.com/scverse/scanpy/pull/125#issuecomment-382344862:229,Availability,error,error,229,"Regarding the other packages: of course, we will also interface those as optional dependencies... But I'd do it from the original Scanpy repo. To me, the whole problem is simply about keeping a clean structure and throwing clear error messages if optional dependencies are not installed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-382344862
https://github.com/scverse/scanpy/pull/125#issuecomment-382344862:277,Deployability,install,installed,277,"Regarding the other packages: of course, we will also interface those as optional dependencies... But I'd do it from the original Scanpy repo. To me, the whole problem is simply about keeping a clean structure and throwing clear error messages if optional dependencies are not installed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-382344862
https://github.com/scverse/scanpy/pull/125#issuecomment-382344862:54,Integrability,interface,interface,54,"Regarding the other packages: of course, we will also interface those as optional dependencies... But I'd do it from the original Scanpy repo. To me, the whole problem is simply about keeping a clean structure and throwing clear error messages if optional dependencies are not installed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-382344862
https://github.com/scverse/scanpy/pull/125#issuecomment-382344862:82,Integrability,depend,dependencies,82,"Regarding the other packages: of course, we will also interface those as optional dependencies... But I'd do it from the original Scanpy repo. To me, the whole problem is simply about keeping a clean structure and throwing clear error messages if optional dependencies are not installed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-382344862
https://github.com/scverse/scanpy/pull/125#issuecomment-382344862:235,Integrability,message,messages,235,"Regarding the other packages: of course, we will also interface those as optional dependencies... But I'd do it from the original Scanpy repo. To me, the whole problem is simply about keeping a clean structure and throwing clear error messages if optional dependencies are not installed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-382344862
https://github.com/scverse/scanpy/pull/125#issuecomment-382344862:256,Integrability,depend,dependencies,256,"Regarding the other packages: of course, we will also interface those as optional dependencies... But I'd do it from the original Scanpy repo. To me, the whole problem is simply about keeping a clean structure and throwing clear error messages if optional dependencies are not installed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-382344862
https://github.com/scverse/scanpy/pull/125#issuecomment-382344862:171,Usability,simpl,simply,171,"Regarding the other packages: of course, we will also interface those as optional dependencies... But I'd do it from the original Scanpy repo. To me, the whole problem is simply about keeping a clean structure and throwing clear error messages if optional dependencies are not installed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-382344862
https://github.com/scverse/scanpy/pull/125#issuecomment-382344862:223,Usability,clear,clear,223,"Regarding the other packages: of course, we will also interface those as optional dependencies... But I'd do it from the original Scanpy repo. To me, the whole problem is simply about keeping a clean structure and throwing clear error messages if optional dependencies are not installed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-382344862
https://github.com/scverse/scanpy/pull/125#issuecomment-382345473:58,Integrability,depend,dependency,58,"Regarding this pull request: I'd merge it, put the `rpy2` dependency in the interface and remove it from the requirements. and put the interface into `rtools`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-382345473
https://github.com/scverse/scanpy/pull/125#issuecomment-382345473:76,Integrability,interface,interface,76,"Regarding this pull request: I'd merge it, put the `rpy2` dependency in the interface and remove it from the requirements. and put the interface into `rtools`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-382345473
https://github.com/scverse/scanpy/pull/125#issuecomment-382345473:135,Integrability,interface,interface,135,"Regarding this pull request: I'd merge it, put the `rpy2` dependency in the interface and remove it from the requirements. and put the interface into `rtools`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-382345473
https://github.com/scverse/scanpy/pull/125#issuecomment-384126068:170,Testability,test,test,170,I merged it and moved it into `rtools`. There is a reexport so that you should be able to call it using `sc.rtools.mnn_concatenate` where `sc` is `scanpy.api`. Could you test whether this works?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-384126068
https://github.com/scverse/scanpy/pull/125#issuecomment-384268335:122,Deployability,integrat,integrating,122,"Thanks @falexwolf !! I will test if the feature works. Of course I've tested seurat cca, but it seems to work better when integrating data from different sequencing platforms. As for my own data, several batches generated by 10x, output if the MNN method looks more pleasing...; ![unknown](https://user-images.githubusercontent.com/8361080/39244909-b8d3cb38-48c4-11e8-9cdc-82c78703ceee.png). Plus, I haven't looked into the maths of CCA, but I have for MNN and feel more comfortable using it. Actually, @gokceneraslan 's comments do make sense to me, and I've spent quite some time working on a native implementation of MNN correct on python. Now it's nearly complete and features more complete multicore support than the scran implementation.; ![screen shot 2018-04-25 at 20 25 17](https://user-images.githubusercontent.com/8361080/39245687-0a17319a-48c7-11e8-934b-904ee6d75978.png); I built it to be fully compatible with anndata and scanpy. Now it already runs much faster than the scran version, and I'm planning to add more speedups, eg Cython and CUDA. I'm thinking of creating a full toolbox for scanpy, like scran for scater/sce, in python. Perhaps we could work together? 😄. I'm currently writing docstrings and will pack and upload the code to a repository shortly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-384268335
https://github.com/scverse/scanpy/pull/125#issuecomment-384268335:122,Integrability,integrat,integrating,122,"Thanks @falexwolf !! I will test if the feature works. Of course I've tested seurat cca, but it seems to work better when integrating data from different sequencing platforms. As for my own data, several batches generated by 10x, output if the MNN method looks more pleasing...; ![unknown](https://user-images.githubusercontent.com/8361080/39244909-b8d3cb38-48c4-11e8-9cdc-82c78703ceee.png). Plus, I haven't looked into the maths of CCA, but I have for MNN and feel more comfortable using it. Actually, @gokceneraslan 's comments do make sense to me, and I've spent quite some time working on a native implementation of MNN correct on python. Now it's nearly complete and features more complete multicore support than the scran implementation.; ![screen shot 2018-04-25 at 20 25 17](https://user-images.githubusercontent.com/8361080/39245687-0a17319a-48c7-11e8-934b-904ee6d75978.png); I built it to be fully compatible with anndata and scanpy. Now it already runs much faster than the scran version, and I'm planning to add more speedups, eg Cython and CUDA. I'm thinking of creating a full toolbox for scanpy, like scran for scater/sce, in python. Perhaps we could work together? 😄. I'm currently writing docstrings and will pack and upload the code to a repository shortly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-384268335
https://github.com/scverse/scanpy/pull/125#issuecomment-384268335:28,Testability,test,test,28,"Thanks @falexwolf !! I will test if the feature works. Of course I've tested seurat cca, but it seems to work better when integrating data from different sequencing platforms. As for my own data, several batches generated by 10x, output if the MNN method looks more pleasing...; ![unknown](https://user-images.githubusercontent.com/8361080/39244909-b8d3cb38-48c4-11e8-9cdc-82c78703ceee.png). Plus, I haven't looked into the maths of CCA, but I have for MNN and feel more comfortable using it. Actually, @gokceneraslan 's comments do make sense to me, and I've spent quite some time working on a native implementation of MNN correct on python. Now it's nearly complete and features more complete multicore support than the scran implementation.; ![screen shot 2018-04-25 at 20 25 17](https://user-images.githubusercontent.com/8361080/39245687-0a17319a-48c7-11e8-934b-904ee6d75978.png); I built it to be fully compatible with anndata and scanpy. Now it already runs much faster than the scran version, and I'm planning to add more speedups, eg Cython and CUDA. I'm thinking of creating a full toolbox for scanpy, like scran for scater/sce, in python. Perhaps we could work together? 😄. I'm currently writing docstrings and will pack and upload the code to a repository shortly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-384268335
https://github.com/scverse/scanpy/pull/125#issuecomment-384268335:70,Testability,test,tested,70,"Thanks @falexwolf !! I will test if the feature works. Of course I've tested seurat cca, but it seems to work better when integrating data from different sequencing platforms. As for my own data, several batches generated by 10x, output if the MNN method looks more pleasing...; ![unknown](https://user-images.githubusercontent.com/8361080/39244909-b8d3cb38-48c4-11e8-9cdc-82c78703ceee.png). Plus, I haven't looked into the maths of CCA, but I have for MNN and feel more comfortable using it. Actually, @gokceneraslan 's comments do make sense to me, and I've spent quite some time working on a native implementation of MNN correct on python. Now it's nearly complete and features more complete multicore support than the scran implementation.; ![screen shot 2018-04-25 at 20 25 17](https://user-images.githubusercontent.com/8361080/39245687-0a17319a-48c7-11e8-934b-904ee6d75978.png); I built it to be fully compatible with anndata and scanpy. Now it already runs much faster than the scran version, and I'm planning to add more speedups, eg Cython and CUDA. I'm thinking of creating a full toolbox for scanpy, like scran for scater/sce, in python. Perhaps we could work together? 😄. I'm currently writing docstrings and will pack and upload the code to a repository shortly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-384268335
https://github.com/scverse/scanpy/pull/125#issuecomment-384310269:164,Integrability,interface,interface,164,"Cool! I'm of course happy to work together! :smile: And, of course, a Python implementation is way cooler than an R one. :wink:. The last addition to Scanpy was an interface for [pypairs](https://github.com/theislab/scanpy/blob/master/scanpy/tools/pypairs.py). It's still not in the docs but will be very soon, I'm right now missing a link to a short example, there. I'm guess you're planning to put your own package on PyPI, right? If yes, than it's not so crucial: for Scanpy, I changed from cython to numba. Many people currently do that, if you try numba, you'll realize why. So, I'd recommend to use numba if you need to speedup something.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-384310269
https://github.com/scverse/scanpy/pull/125#issuecomment-384385090:203,Performance,optimiz,optimize,203,"You are right I'm putting it on PyPI, and guess what, I'm already using numba on it😄. I finished some docs and uploaded it to my repo [mnnpy](https://github.com/chriscainx/mnnpy). There is still much to optimize, according to my tests, the 'adjust variance' step takes most time, and the best solution may still be rewriting it in C, as scran did, although somehow they made it single-threaded. In the current version I used jit and multiprocess. I'm afraid we'll have to remove the 'mnn_concatenate' from /rtools though 😂",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-384385090
https://github.com/scverse/scanpy/pull/125#issuecomment-384385090:229,Testability,test,tests,229,"You are right I'm putting it on PyPI, and guess what, I'm already using numba on it😄. I finished some docs and uploaded it to my repo [mnnpy](https://github.com/chriscainx/mnnpy). There is still much to optimize, according to my tests, the 'adjust variance' step takes most time, and the best solution may still be rewriting it in C, as scran did, although somehow they made it single-threaded. In the current version I used jit and multiprocess. I'm afraid we'll have to remove the 'mnn_concatenate' from /rtools though 😂",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-384385090
https://github.com/scverse/scanpy/pull/125#issuecomment-384463485:52,Integrability,interface,interface,52,"Cool! Very happy to get another pull request for an interface to `mnnpy`. :smile:. Regarding writing it in C: I disagree, numba-boosted Python code is much nicer for these type of ""relatively simple"" algorithms... . Regarding `rtools`. I'll remove it from the `api` but leave it in scanpy as an example for how one could wrap other r packages... No user will notice that...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-384463485
https://github.com/scverse/scanpy/pull/125#issuecomment-384463485:321,Integrability,wrap,wrap,321,"Cool! Very happy to get another pull request for an interface to `mnnpy`. :smile:. Regarding writing it in C: I disagree, numba-boosted Python code is much nicer for these type of ""relatively simple"" algorithms... . Regarding `rtools`. I'll remove it from the `api` but leave it in scanpy as an example for how one could wrap other r packages... No user will notice that...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-384463485
https://github.com/scverse/scanpy/pull/125#issuecomment-384463485:192,Usability,simpl,simple,192,"Cool! Very happy to get another pull request for an interface to `mnnpy`. :smile:. Regarding writing it in C: I disagree, numba-boosted Python code is much nicer for these type of ""relatively simple"" algorithms... . Regarding `rtools`. I'll remove it from the `api` but leave it in scanpy as an example for how one could wrap other r packages... No user will notice that...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-384463485
https://github.com/scverse/scanpy/issues/129#issuecomment-383915649:120,Usability,Simpl,Simply,120,"Oh yes, it's no longer created on purpose since 1.0. But the plotting function obviously still assumes it. I'll fix it. Simply use the 'dpt_pseudotime' annotation instead. `pl.dpt_timeseries` should work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/129#issuecomment-383915649
https://github.com/scverse/scanpy/pull/136#issuecomment-385959811:210,Integrability,wrap,wrapper,210,"Hi Scott!. Sorry that I didn't see this before writing the following email this moring:. > Sorry about forgetting to mention this in the first email below (just thought I'd send you the link to the most recent wrapper). The tSNE tool is a wrapper, too, and also has an example for a visualization function; > https://github.com/theislab/scanpy/blob/master/scanpy/tools/tsne.py; > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/tools/__init__.py#L426-L518; > You'll find the corresponding pair in the docs. If your wrapper parallelizes this, this will make it very easy for Scanpy users to use out PHATE. Scanpy's toplevel API does - on purpose - not feature objects, but convenience functions. If people want to do more complicated stuff, they should directly use the PHATE package, in this case. So, it would be nice if you'd only provide the convenience function - which is already very similar to what I pasted about tSNE. Also, please remove phate from the requirements... and raise a corresponding ImportError in your convenience function. Scanpy has quite some optional dependencies and Phate will be one of them. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/136#issuecomment-385959811
https://github.com/scverse/scanpy/pull/136#issuecomment-385959811:239,Integrability,wrap,wrapper,239,"Hi Scott!. Sorry that I didn't see this before writing the following email this moring:. > Sorry about forgetting to mention this in the first email below (just thought I'd send you the link to the most recent wrapper). The tSNE tool is a wrapper, too, and also has an example for a visualization function; > https://github.com/theislab/scanpy/blob/master/scanpy/tools/tsne.py; > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/tools/__init__.py#L426-L518; > You'll find the corresponding pair in the docs. If your wrapper parallelizes this, this will make it very easy for Scanpy users to use out PHATE. Scanpy's toplevel API does - on purpose - not feature objects, but convenience functions. If people want to do more complicated stuff, they should directly use the PHATE package, in this case. So, it would be nice if you'd only provide the convenience function - which is already very similar to what I pasted about tSNE. Also, please remove phate from the requirements... and raise a corresponding ImportError in your convenience function. Scanpy has quite some optional dependencies and Phate will be one of them. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/136#issuecomment-385959811
https://github.com/scverse/scanpy/pull/136#issuecomment-385959811:530,Integrability,wrap,wrapper,530,"Hi Scott!. Sorry that I didn't see this before writing the following email this moring:. > Sorry about forgetting to mention this in the first email below (just thought I'd send you the link to the most recent wrapper). The tSNE tool is a wrapper, too, and also has an example for a visualization function; > https://github.com/theislab/scanpy/blob/master/scanpy/tools/tsne.py; > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/tools/__init__.py#L426-L518; > You'll find the corresponding pair in the docs. If your wrapper parallelizes this, this will make it very easy for Scanpy users to use out PHATE. Scanpy's toplevel API does - on purpose - not feature objects, but convenience functions. If people want to do more complicated stuff, they should directly use the PHATE package, in this case. So, it would be nice if you'd only provide the convenience function - which is already very similar to what I pasted about tSNE. Also, please remove phate from the requirements... and raise a corresponding ImportError in your convenience function. Scanpy has quite some optional dependencies and Phate will be one of them. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/136#issuecomment-385959811
https://github.com/scverse/scanpy/pull/136#issuecomment-385959811:1092,Integrability,depend,dependencies,1092,"Hi Scott!. Sorry that I didn't see this before writing the following email this moring:. > Sorry about forgetting to mention this in the first email below (just thought I'd send you the link to the most recent wrapper). The tSNE tool is a wrapper, too, and also has an example for a visualization function; > https://github.com/theislab/scanpy/blob/master/scanpy/tools/tsne.py; > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/tools/__init__.py#L426-L518; > You'll find the corresponding pair in the docs. If your wrapper parallelizes this, this will make it very easy for Scanpy users to use out PHATE. Scanpy's toplevel API does - on purpose - not feature objects, but convenience functions. If people want to do more complicated stuff, they should directly use the PHATE package, in this case. So, it would be nice if you'd only provide the convenience function - which is already very similar to what I pasted about tSNE. Also, please remove phate from the requirements... and raise a corresponding ImportError in your convenience function. Scanpy has quite some optional dependencies and Phate will be one of them. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/136#issuecomment-385959811
https://github.com/scverse/scanpy/pull/136#issuecomment-385960220:55,Deployability,install,installing,55,"PS: You don't need a test for this... it would require installing phate on travis and this would take time... Also, the interface is trivial. You should simply link to your package within the docs to redirect people for bugs and more info.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/136#issuecomment-385960220
https://github.com/scverse/scanpy/pull/136#issuecomment-385960220:120,Integrability,interface,interface,120,"PS: You don't need a test for this... it would require installing phate on travis and this would take time... Also, the interface is trivial. You should simply link to your package within the docs to redirect people for bugs and more info.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/136#issuecomment-385960220
https://github.com/scverse/scanpy/pull/136#issuecomment-385960220:21,Testability,test,test,21,"PS: You don't need a test for this... it would require installing phate on travis and this would take time... Also, the interface is trivial. You should simply link to your package within the docs to redirect people for bugs and more info.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/136#issuecomment-385960220
https://github.com/scverse/scanpy/pull/136#issuecomment-385960220:153,Usability,simpl,simply,153,"PS: You don't need a test for this... it would require installing phate on travis and this would take time... Also, the interface is trivial. You should simply link to your package within the docs to redirect people for bugs and more info.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/136#issuecomment-385960220
https://github.com/scverse/scanpy/pull/136#issuecomment-386075622:132,Integrability,interface,interface,132,@falexwolf thanks very much for the extra information! I've copied the t-SNE API so it should fit much more cleanly into the scanpy interface. Let me know if there's anything else you need me to change. Thanks again!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/136#issuecomment-386075622
https://github.com/scverse/scanpy/issues/137#issuecomment-413354154:149,Availability,error,error,149,"@falexwolf we just tried the solution you posted and it reveals a bug: when `ax` is not `None` you don't create the variable `axs` and thus throw an error here: https://github.com/theislab/scanpy/blob/master/scanpy/plotting/anndata.py#L634. Should be a simple fix (I think):. ```python; if ax is None:; axs, _, _, _ = setup_axes(ax=ax, panels=['x'] if groupby is None else keys, show_ticks=True, right_margin=0.3); else:; axs = [ax]; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/137#issuecomment-413354154
https://github.com/scverse/scanpy/issues/137#issuecomment-413354154:116,Modifiability,variab,variable,116,"@falexwolf we just tried the solution you posted and it reveals a bug: when `ax` is not `None` you don't create the variable `axs` and thus throw an error here: https://github.com/theislab/scanpy/blob/master/scanpy/plotting/anndata.py#L634. Should be a simple fix (I think):. ```python; if ax is None:; axs, _, _, _ = setup_axes(ax=ax, panels=['x'] if groupby is None else keys, show_ticks=True, right_margin=0.3); else:; axs = [ax]; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/137#issuecomment-413354154
https://github.com/scverse/scanpy/issues/137#issuecomment-413354154:253,Usability,simpl,simple,253,"@falexwolf we just tried the solution you posted and it reveals a bug: when `ax` is not `None` you don't create the variable `axs` and thus throw an error here: https://github.com/theislab/scanpy/blob/master/scanpy/plotting/anndata.py#L634. Should be a simple fix (I think):. ```python; if ax is None:; axs, _, _, _ = setup_axes(ax=ax, panels=['x'] if groupby is None else keys, show_ticks=True, right_margin=0.3); else:; axs = [ax]; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/137#issuecomment-413354154
https://github.com/scverse/scanpy/issues/138#issuecomment-385764640:123,Deployability,install,installation,123,I noticed now that the library is `python-igraph` and not `igraph` as explained in https://scanpy.readthedocs.io/en/latest/installation.html .; I am closing this issue since I've solved through the steps in the documentation.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138#issuecomment-385764640
https://github.com/scverse/scanpy/issues/138#issuecomment-495769728:29,Deployability,install,installing,29,The above issue was fixed by installing python-igraph and not igraph. Just run; `pip install python-igraph`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138#issuecomment-495769728
https://github.com/scverse/scanpy/issues/138#issuecomment-495769728:85,Deployability,install,install,85,The above issue was fixed by installing python-igraph and not igraph. Just run; `pip install python-igraph`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138#issuecomment-495769728
https://github.com/scverse/scanpy/issues/138#issuecomment-495920986:74,Deployability,install,install,74,"Thanks for the response. I tried that several times, but couldn’t get the install to work:. “Cannot find the C core of igraph on this system using pkg-config” etc. Any other advice would of course be appreciated greatly. From: MalteDLuecken [mailto:notifications@github.com]; Sent: Friday, May 24, 2019 3:54 PM; To: theislab/scanpy <scanpy@noreply.github.com>; Cc: Moos, Malcolm <Malcolm.Moos@fda.hhs.gov>; Comment <comment@noreply.github.com>; Subject: Re: [theislab/scanpy] igraph problems (#138). The above issue was fixed by installing python-igraph and not igraph. Just run; pip install python-igraph. —; You are receiving this because you commented.; Reply to this email directly, view it on GitHub<https://github.com/theislab/scanpy/issues/138?email_source=notifications&email_token=AMEIEFZ5Y3DOAD5JWFWTCXTPXBBWZA5CNFSM4E5ZJQRKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWGNRAA#issuecomment-495769728>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AMEIEF3USEC33LKKFK4X4R3PXBBWZANCNFSM4E5ZJQRA>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138#issuecomment-495920986
https://github.com/scverse/scanpy/issues/138#issuecomment-495920986:529,Deployability,install,installing,529,"Thanks for the response. I tried that several times, but couldn’t get the install to work:. “Cannot find the C core of igraph on this system using pkg-config” etc. Any other advice would of course be appreciated greatly. From: MalteDLuecken [mailto:notifications@github.com]; Sent: Friday, May 24, 2019 3:54 PM; To: theislab/scanpy <scanpy@noreply.github.com>; Cc: Moos, Malcolm <Malcolm.Moos@fda.hhs.gov>; Comment <comment@noreply.github.com>; Subject: Re: [theislab/scanpy] igraph problems (#138). The above issue was fixed by installing python-igraph and not igraph. Just run; pip install python-igraph. —; You are receiving this because you commented.; Reply to this email directly, view it on GitHub<https://github.com/theislab/scanpy/issues/138?email_source=notifications&email_token=AMEIEFZ5Y3DOAD5JWFWTCXTPXBBWZA5CNFSM4E5ZJQRKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWGNRAA#issuecomment-495769728>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AMEIEF3USEC33LKKFK4X4R3PXBBWZANCNFSM4E5ZJQRA>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138#issuecomment-495920986
https://github.com/scverse/scanpy/issues/138#issuecomment-495920986:584,Deployability,install,install,584,"Thanks for the response. I tried that several times, but couldn’t get the install to work:. “Cannot find the C core of igraph on this system using pkg-config” etc. Any other advice would of course be appreciated greatly. From: MalteDLuecken [mailto:notifications@github.com]; Sent: Friday, May 24, 2019 3:54 PM; To: theislab/scanpy <scanpy@noreply.github.com>; Cc: Moos, Malcolm <Malcolm.Moos@fda.hhs.gov>; Comment <comment@noreply.github.com>; Subject: Re: [theislab/scanpy] igraph problems (#138). The above issue was fixed by installing python-igraph and not igraph. Just run; pip install python-igraph. —; You are receiving this because you commented.; Reply to this email directly, view it on GitHub<https://github.com/theislab/scanpy/issues/138?email_source=notifications&email_token=AMEIEFZ5Y3DOAD5JWFWTCXTPXBBWZA5CNFSM4E5ZJQRKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWGNRAA#issuecomment-495769728>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AMEIEF3USEC33LKKFK4X4R3PXBBWZANCNFSM4E5ZJQRA>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138#issuecomment-495920986
https://github.com/scverse/scanpy/issues/138#issuecomment-495920986:151,Modifiability,config,config,151,"Thanks for the response. I tried that several times, but couldn’t get the install to work:. “Cannot find the C core of igraph on this system using pkg-config” etc. Any other advice would of course be appreciated greatly. From: MalteDLuecken [mailto:notifications@github.com]; Sent: Friday, May 24, 2019 3:54 PM; To: theislab/scanpy <scanpy@noreply.github.com>; Cc: Moos, Malcolm <Malcolm.Moos@fda.hhs.gov>; Comment <comment@noreply.github.com>; Subject: Re: [theislab/scanpy] igraph problems (#138). The above issue was fixed by installing python-igraph and not igraph. Just run; pip install python-igraph. —; You are receiving this because you commented.; Reply to this email directly, view it on GitHub<https://github.com/theislab/scanpy/issues/138?email_source=notifications&email_token=AMEIEFZ5Y3DOAD5JWFWTCXTPXBBWZA5CNFSM4E5ZJQRKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWGNRAA#issuecomment-495769728>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AMEIEF3USEC33LKKFK4X4R3PXBBWZANCNFSM4E5ZJQRA>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138#issuecomment-495920986
https://github.com/scverse/scanpy/issues/138#issuecomment-495938081:61,Deployability,install,install,61,"@RicedeKrispy ; Hi, if you are using Windows, you can try to install python-igraph from the wheel here; https://www.lfd.uci.edu/~gohlke/pythonlibs/#python-igraph",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138#issuecomment-495938081
https://github.com/scverse/scanpy/issues/138#issuecomment-502908611:70,Availability,error,error,70,"I just noticed that the reply I sent Saturday bounced due to ‘unknown error’. I thought I should provide an update in case other Windows users encounter something similar. After identifying the correct version of the several on the link below, I noticed that it was also necessary to install Pycairo. I then found I needed to install the Louvain algorithm to resolve the issue. The vtraag website indicates this algorithm has been superseded by the Leiden algorithm, which I installed with no problem. Thanks much for your reply within hours, on a weekend no less. From: Koncopd [mailto:notifications@github.com]; Sent: Saturday, May 25, 2019 2:15 PM; To: theislab/scanpy <scanpy@noreply.github.com>; Cc: Moos, Malcolm <Malcolm.Moos@fda.hhs.gov>; Mention <mention@noreply.github.com>; Subject: Re: [theislab/scanpy] igraph problems (#138). @RicedeKrispy<https://github.com/RicedeKrispy>; Hi, if you are using Windows, you can try to install python-igraph from the wheel here; https://www.lfd.uci.edu/~gohlke/pythonlibs/#python-igraph. —; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub<https://github.com/theislab/scanpy/issues/138?email_source=notifications&email_token=AMEIEFZ2OGJSDDXGY4TRK4TPXF62HA5CNFSM4E5ZJQRKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWHWUII#issuecomment-495938081>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AMEIEFZFWYPQB7BP5HHCM23PXF62HANCNFSM4E5ZJQRA>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138#issuecomment-502908611
https://github.com/scverse/scanpy/issues/138#issuecomment-502908611:108,Deployability,update,update,108,"I just noticed that the reply I sent Saturday bounced due to ‘unknown error’. I thought I should provide an update in case other Windows users encounter something similar. After identifying the correct version of the several on the link below, I noticed that it was also necessary to install Pycairo. I then found I needed to install the Louvain algorithm to resolve the issue. The vtraag website indicates this algorithm has been superseded by the Leiden algorithm, which I installed with no problem. Thanks much for your reply within hours, on a weekend no less. From: Koncopd [mailto:notifications@github.com]; Sent: Saturday, May 25, 2019 2:15 PM; To: theislab/scanpy <scanpy@noreply.github.com>; Cc: Moos, Malcolm <Malcolm.Moos@fda.hhs.gov>; Mention <mention@noreply.github.com>; Subject: Re: [theislab/scanpy] igraph problems (#138). @RicedeKrispy<https://github.com/RicedeKrispy>; Hi, if you are using Windows, you can try to install python-igraph from the wheel here; https://www.lfd.uci.edu/~gohlke/pythonlibs/#python-igraph. —; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub<https://github.com/theislab/scanpy/issues/138?email_source=notifications&email_token=AMEIEFZ2OGJSDDXGY4TRK4TPXF62HA5CNFSM4E5ZJQRKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWHWUII#issuecomment-495938081>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AMEIEFZFWYPQB7BP5HHCM23PXF62HANCNFSM4E5ZJQRA>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138#issuecomment-502908611
https://github.com/scverse/scanpy/issues/138#issuecomment-502908611:284,Deployability,install,install,284,"I just noticed that the reply I sent Saturday bounced due to ‘unknown error’. I thought I should provide an update in case other Windows users encounter something similar. After identifying the correct version of the several on the link below, I noticed that it was also necessary to install Pycairo. I then found I needed to install the Louvain algorithm to resolve the issue. The vtraag website indicates this algorithm has been superseded by the Leiden algorithm, which I installed with no problem. Thanks much for your reply within hours, on a weekend no less. From: Koncopd [mailto:notifications@github.com]; Sent: Saturday, May 25, 2019 2:15 PM; To: theislab/scanpy <scanpy@noreply.github.com>; Cc: Moos, Malcolm <Malcolm.Moos@fda.hhs.gov>; Mention <mention@noreply.github.com>; Subject: Re: [theislab/scanpy] igraph problems (#138). @RicedeKrispy<https://github.com/RicedeKrispy>; Hi, if you are using Windows, you can try to install python-igraph from the wheel here; https://www.lfd.uci.edu/~gohlke/pythonlibs/#python-igraph. —; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub<https://github.com/theislab/scanpy/issues/138?email_source=notifications&email_token=AMEIEFZ2OGJSDDXGY4TRK4TPXF62HA5CNFSM4E5ZJQRKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWHWUII#issuecomment-495938081>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AMEIEFZFWYPQB7BP5HHCM23PXF62HANCNFSM4E5ZJQRA>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138#issuecomment-502908611
https://github.com/scverse/scanpy/issues/138#issuecomment-502908611:326,Deployability,install,install,326,"I just noticed that the reply I sent Saturday bounced due to ‘unknown error’. I thought I should provide an update in case other Windows users encounter something similar. After identifying the correct version of the several on the link below, I noticed that it was also necessary to install Pycairo. I then found I needed to install the Louvain algorithm to resolve the issue. The vtraag website indicates this algorithm has been superseded by the Leiden algorithm, which I installed with no problem. Thanks much for your reply within hours, on a weekend no less. From: Koncopd [mailto:notifications@github.com]; Sent: Saturday, May 25, 2019 2:15 PM; To: theislab/scanpy <scanpy@noreply.github.com>; Cc: Moos, Malcolm <Malcolm.Moos@fda.hhs.gov>; Mention <mention@noreply.github.com>; Subject: Re: [theislab/scanpy] igraph problems (#138). @RicedeKrispy<https://github.com/RicedeKrispy>; Hi, if you are using Windows, you can try to install python-igraph from the wheel here; https://www.lfd.uci.edu/~gohlke/pythonlibs/#python-igraph. —; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub<https://github.com/theislab/scanpy/issues/138?email_source=notifications&email_token=AMEIEFZ2OGJSDDXGY4TRK4TPXF62HA5CNFSM4E5ZJQRKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWHWUII#issuecomment-495938081>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AMEIEFZFWYPQB7BP5HHCM23PXF62HANCNFSM4E5ZJQRA>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138#issuecomment-502908611
https://github.com/scverse/scanpy/issues/138#issuecomment-502908611:475,Deployability,install,installed,475,"I just noticed that the reply I sent Saturday bounced due to ‘unknown error’. I thought I should provide an update in case other Windows users encounter something similar. After identifying the correct version of the several on the link below, I noticed that it was also necessary to install Pycairo. I then found I needed to install the Louvain algorithm to resolve the issue. The vtraag website indicates this algorithm has been superseded by the Leiden algorithm, which I installed with no problem. Thanks much for your reply within hours, on a weekend no less. From: Koncopd [mailto:notifications@github.com]; Sent: Saturday, May 25, 2019 2:15 PM; To: theislab/scanpy <scanpy@noreply.github.com>; Cc: Moos, Malcolm <Malcolm.Moos@fda.hhs.gov>; Mention <mention@noreply.github.com>; Subject: Re: [theislab/scanpy] igraph problems (#138). @RicedeKrispy<https://github.com/RicedeKrispy>; Hi, if you are using Windows, you can try to install python-igraph from the wheel here; https://www.lfd.uci.edu/~gohlke/pythonlibs/#python-igraph. —; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub<https://github.com/theislab/scanpy/issues/138?email_source=notifications&email_token=AMEIEFZ2OGJSDDXGY4TRK4TPXF62HA5CNFSM4E5ZJQRKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWHWUII#issuecomment-495938081>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AMEIEFZFWYPQB7BP5HHCM23PXF62HANCNFSM4E5ZJQRA>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138#issuecomment-502908611
https://github.com/scverse/scanpy/issues/138#issuecomment-502908611:933,Deployability,install,install,933,"I just noticed that the reply I sent Saturday bounced due to ‘unknown error’. I thought I should provide an update in case other Windows users encounter something similar. After identifying the correct version of the several on the link below, I noticed that it was also necessary to install Pycairo. I then found I needed to install the Louvain algorithm to resolve the issue. The vtraag website indicates this algorithm has been superseded by the Leiden algorithm, which I installed with no problem. Thanks much for your reply within hours, on a weekend no less. From: Koncopd [mailto:notifications@github.com]; Sent: Saturday, May 25, 2019 2:15 PM; To: theislab/scanpy <scanpy@noreply.github.com>; Cc: Moos, Malcolm <Malcolm.Moos@fda.hhs.gov>; Mention <mention@noreply.github.com>; Subject: Re: [theislab/scanpy] igraph problems (#138). @RicedeKrispy<https://github.com/RicedeKrispy>; Hi, if you are using Windows, you can try to install python-igraph from the wheel here; https://www.lfd.uci.edu/~gohlke/pythonlibs/#python-igraph. —; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub<https://github.com/theislab/scanpy/issues/138?email_source=notifications&email_token=AMEIEFZ2OGJSDDXGY4TRK4TPXF62HA5CNFSM4E5ZJQRKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWHWUII#issuecomment-495938081>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AMEIEFZFWYPQB7BP5HHCM23PXF62HANCNFSM4E5ZJQRA>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138#issuecomment-502908611
https://github.com/scverse/scanpy/issues/138#issuecomment-518220318:145,Availability,error,errors,145,"Hello,; I've gotten scanpy working on my local computer, but for memory reasons I need to move to our server (linux). I am running into the same errors as above - any advice is appreciated!. Skipping optional fixer: buffer; Skipping optional fixer: idioms; Skipping optional fixer: set_literal; Skipping optional fixer: ws_comma; running build_ext; Cannot find the C core of igraph on this system using pkg-config.; We will now try to download and compile the C core from scratch.; Version number of the C core: 0.7.1.post6; We will also try: 0.7.1; ; Version 0.7.1.post6 of the C core of igraph is not found among the nightly builds.; Use the --c-core-version switch to try a different version.; ; Could not download and compile the C core of igraph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138#issuecomment-518220318
https://github.com/scverse/scanpy/issues/138#issuecomment-518220318:435,Availability,down,download,435,"Hello,; I've gotten scanpy working on my local computer, but for memory reasons I need to move to our server (linux). I am running into the same errors as above - any advice is appreciated!. Skipping optional fixer: buffer; Skipping optional fixer: idioms; Skipping optional fixer: set_literal; Skipping optional fixer: ws_comma; running build_ext; Cannot find the C core of igraph on this system using pkg-config.; We will now try to download and compile the C core from scratch.; Version number of the C core: 0.7.1.post6; We will also try: 0.7.1; ; Version 0.7.1.post6 of the C core of igraph is not found among the nightly builds.; Use the --c-core-version switch to try a different version.; ; Could not download and compile the C core of igraph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138#issuecomment-518220318
https://github.com/scverse/scanpy/issues/138#issuecomment-518220318:709,Availability,down,download,709,"Hello,; I've gotten scanpy working on my local computer, but for memory reasons I need to move to our server (linux). I am running into the same errors as above - any advice is appreciated!. Skipping optional fixer: buffer; Skipping optional fixer: idioms; Skipping optional fixer: set_literal; Skipping optional fixer: ws_comma; running build_ext; Cannot find the C core of igraph on this system using pkg-config.; We will now try to download and compile the C core from scratch.; Version number of the C core: 0.7.1.post6; We will also try: 0.7.1; ; Version 0.7.1.post6 of the C core of igraph is not found among the nightly builds.; Use the --c-core-version switch to try a different version.; ; Could not download and compile the C core of igraph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138#issuecomment-518220318
https://github.com/scverse/scanpy/issues/138#issuecomment-518220318:407,Modifiability,config,config,407,"Hello,; I've gotten scanpy working on my local computer, but for memory reasons I need to move to our server (linux). I am running into the same errors as above - any advice is appreciated!. Skipping optional fixer: buffer; Skipping optional fixer: idioms; Skipping optional fixer: set_literal; Skipping optional fixer: ws_comma; running build_ext; Cannot find the C core of igraph on this system using pkg-config.; We will now try to download and compile the C core from scratch.; Version number of the C core: 0.7.1.post6; We will also try: 0.7.1; ; Version 0.7.1.post6 of the C core of igraph is not found among the nightly builds.; Use the --c-core-version switch to try a different version.; ; Could not download and compile the C core of igraph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138#issuecomment-518220318
https://github.com/scverse/scanpy/pull/139#issuecomment-386320788:48,Deployability,release,release,48,"No worries, I would have done this for the next release... . Btw: cool that you made your actual `fit_transform` accept `AnnData` objects... Would not have been necessary, but nice! :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/139#issuecomment-386320788
https://github.com/scverse/scanpy/pull/139#issuecomment-386322714:94,Integrability,interface,interface,94,"Thanks, and you're welcome! I figure if a scanpy user wants to take the plunge and use the OO interface, they shouldn't have to change the way they interact with their AnnData. Thanks again!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/139#issuecomment-386322714
https://github.com/scverse/scanpy/pull/141#issuecomment-386769886:107,Usability,simpl,simple,107,"Nice! Thank you! That's certainly very meaningful. It should just go somewhere else, not in `preprocessing.simple`... Let me think about where to put these queries... We'll have more of this sort of thing in the future! I'll certainly merge this and move it around... However, we'll not make bioservices a hard requirement.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/141#issuecomment-386769886
https://github.com/scverse/scanpy/pull/141#issuecomment-386815219:2,Deployability,update,updated,2,"I updated the commits, making bioservices optional.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/141#issuecomment-386815219
https://github.com/scverse/scanpy/pull/141#issuecomment-387106636:511,Availability,error,error,511,"Hopefully last update on this PR. What I did:; - I noticed a regression on the method `rank_genes_groups_violin`, therefore I reverted back the code to the original one and I added an additional method `genes_groups_violin` which should be used if we want to pass the list of genes directly to the violin plot. The code is just a POC, but maybe it can be integrated; - Within the same method `rank_genes_groups_violin`, I found a bug: the ax variable was overwritten for each group (I don't know if it gave you error before). In my case, all the plots were merged into a single figure, every one on top of the previous ones; - Additionally, the parameters `gene_symbols` and `computed_distribution` were not defined within the method `rank_genes_groups_violin`. I added a default parameter (`None`) for `gene_symbols`, since it was defined in the docstring. With `computed_distribution` I didn't know what you wanted to do so I temporarily commented the line that used it",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/141#issuecomment-387106636
https://github.com/scverse/scanpy/pull/141#issuecomment-387106636:15,Deployability,update,update,15,"Hopefully last update on this PR. What I did:; - I noticed a regression on the method `rank_genes_groups_violin`, therefore I reverted back the code to the original one and I added an additional method `genes_groups_violin` which should be used if we want to pass the list of genes directly to the violin plot. The code is just a POC, but maybe it can be integrated; - Within the same method `rank_genes_groups_violin`, I found a bug: the ax variable was overwritten for each group (I don't know if it gave you error before). In my case, all the plots were merged into a single figure, every one on top of the previous ones; - Additionally, the parameters `gene_symbols` and `computed_distribution` were not defined within the method `rank_genes_groups_violin`. I added a default parameter (`None`) for `gene_symbols`, since it was defined in the docstring. With `computed_distribution` I didn't know what you wanted to do so I temporarily commented the line that used it",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/141#issuecomment-387106636
https://github.com/scverse/scanpy/pull/141#issuecomment-387106636:355,Deployability,integrat,integrated,355,"Hopefully last update on this PR. What I did:; - I noticed a regression on the method `rank_genes_groups_violin`, therefore I reverted back the code to the original one and I added an additional method `genes_groups_violin` which should be used if we want to pass the list of genes directly to the violin plot. The code is just a POC, but maybe it can be integrated; - Within the same method `rank_genes_groups_violin`, I found a bug: the ax variable was overwritten for each group (I don't know if it gave you error before). In my case, all the plots were merged into a single figure, every one on top of the previous ones; - Additionally, the parameters `gene_symbols` and `computed_distribution` were not defined within the method `rank_genes_groups_violin`. I added a default parameter (`None`) for `gene_symbols`, since it was defined in the docstring. With `computed_distribution` I didn't know what you wanted to do so I temporarily commented the line that used it",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/141#issuecomment-387106636
https://github.com/scverse/scanpy/pull/141#issuecomment-387106636:355,Integrability,integrat,integrated,355,"Hopefully last update on this PR. What I did:; - I noticed a regression on the method `rank_genes_groups_violin`, therefore I reverted back the code to the original one and I added an additional method `genes_groups_violin` which should be used if we want to pass the list of genes directly to the violin plot. The code is just a POC, but maybe it can be integrated; - Within the same method `rank_genes_groups_violin`, I found a bug: the ax variable was overwritten for each group (I don't know if it gave you error before). In my case, all the plots were merged into a single figure, every one on top of the previous ones; - Additionally, the parameters `gene_symbols` and `computed_distribution` were not defined within the method `rank_genes_groups_violin`. I added a default parameter (`None`) for `gene_symbols`, since it was defined in the docstring. With `computed_distribution` I didn't know what you wanted to do so I temporarily commented the line that used it",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/141#issuecomment-387106636
https://github.com/scverse/scanpy/pull/141#issuecomment-387106636:442,Modifiability,variab,variable,442,"Hopefully last update on this PR. What I did:; - I noticed a regression on the method `rank_genes_groups_violin`, therefore I reverted back the code to the original one and I added an additional method `genes_groups_violin` which should be used if we want to pass the list of genes directly to the violin plot. The code is just a POC, but maybe it can be integrated; - Within the same method `rank_genes_groups_violin`, I found a bug: the ax variable was overwritten for each group (I don't know if it gave you error before). In my case, all the plots were merged into a single figure, every one on top of the previous ones; - Additionally, the parameters `gene_symbols` and `computed_distribution` were not defined within the method `rank_genes_groups_violin`. I added a default parameter (`None`) for `gene_symbols`, since it was defined in the docstring. With `computed_distribution` I didn't know what you wanted to do so I temporarily commented the line that used it",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/141#issuecomment-387106636
https://github.com/scverse/scanpy/issues/143#issuecomment-386718816:42,Deployability,update,update,42,Thank you very much for this remark! I'll update the documentation!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/143#issuecomment-386718816
https://github.com/scverse/scanpy/issues/145#issuecomment-386717637:77,Modifiability,variab,variable,77,I'm improving on this whole organization... Right now you can set the global variable: `sc.settings.figdir = path_to_some_dir/prefix_` or you can use `save='_mysuffix.pdf'` in any plotting function. Does this help?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/145#issuecomment-386717637
https://github.com/scverse/scanpy/issues/146#issuecomment-1944329648:32,Performance,load,loading,32,Is this still fixed? I see that loading a 36GB h5ad file requires me to use a machine with 128GB RAM.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/146#issuecomment-1944329648
https://github.com/scverse/scanpy/issues/148#issuecomment-386885473:158,Deployability,install,installation,158,"Difficult to say something from the trace, it might be related to old setuptools version. You may try updating it. Alternatively, you can set up a [miniconda installation](https://scanpy.readthedocs.io/en/latest/installation.html#installing-miniconda) in your home folder to have more up-to-date packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/148#issuecomment-386885473
https://github.com/scverse/scanpy/issues/148#issuecomment-386885473:212,Deployability,install,installation,212,"Difficult to say something from the trace, it might be related to old setuptools version. You may try updating it. Alternatively, you can set up a [miniconda installation](https://scanpy.readthedocs.io/en/latest/installation.html#installing-miniconda) in your home folder to have more up-to-date packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/148#issuecomment-386885473
https://github.com/scverse/scanpy/issues/148#issuecomment-386885473:230,Deployability,install,installing-miniconda,230,"Difficult to say something from the trace, it might be related to old setuptools version. You may try updating it. Alternatively, you can set up a [miniconda installation](https://scanpy.readthedocs.io/en/latest/installation.html#installing-miniconda) in your home folder to have more up-to-date packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/148#issuecomment-386885473
https://github.com/scverse/scanpy/issues/148#issuecomment-386887420:32,Deployability,install,install,32,"Thanks Gökcen... Yes, try `pip3 install --upgrade setuptools` or `pip` if this defaults to Python 3 already...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/148#issuecomment-386887420
https://github.com/scverse/scanpy/issues/148#issuecomment-386887420:42,Deployability,upgrade,upgrade,42,"Thanks Gökcen... Yes, try `pip3 install --upgrade setuptools` or `pip` if this defaults to Python 3 already...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/148#issuecomment-386887420
https://github.com/scverse/scanpy/issues/148#issuecomment-391375544:118,Deployability,install,installation,118,My coworker and I had the same issue as @ttgump. Upgrading setuptools fixed it. Maybe it's worth adding a note to the installation troubleshooting documentation?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/148#issuecomment-391375544
https://github.com/scverse/scanpy/issues/150#issuecomment-387215323:4,Deployability,update,updated,4,"Ok, updated the docs... Sorry about that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/150#issuecomment-387215323
https://github.com/scverse/scanpy/issues/154#issuecomment-389316074:47,Usability,Simpl,Simply,47,You're calling this function in the wrong way. Simply type; ```; adata.write_loom('myfilename.loom'); ```; if `adata` is an `AnnData` object. See the documentation of the [function](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.AnnData.write_loom.html#scanpy.api.AnnData.write_loom).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/154#issuecomment-389316074
https://github.com/scverse/scanpy/issues/154#issuecomment-389486091:67,Availability,error,error,67,"Indeed it was enlightening and useful...However, still gives me an error... ```pytb; type(adata); anndata.base.AnnData. adata.write_loom('./filtered_gene_bc_matrices_h5.loom'); ... writing to '.loom' file densifies sparse matrix; Converting to csc format; Creating; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-8-333dccc1e180> in <module>(); ----> 1 adata.write_loom('./filtered_gene_bc_matrices_h5.loom'). /anaconda3/lib/python3.6/site-packages/anndata/base.py in write_loom(self, filename); 1736 """"""; 1737 from .readwrite.write import write_loom; -> 1738 write_loom(filename, self); 1739 ; 1740 @staticmethod. /anaconda3/lib/python3.6/site-packages/anndata/readwrite/write.py in write_loom(filename, adata); 71 if os.path.exists(filename):; 72 os.remove(filename); ---> 73 create(filename, X, row_attrs=row_attrs, col_attrs=col_attrs); 74 ; 75 . /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in create(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts); 1019 ; 1020 if scipy.sparse.issparse(matrix):; -> 1021 return _create_sparse(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts); 1022 ; 1023 # Create the file (empty). /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in _create_sparse(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts); 982 if ds is None:; 983 logging.info(""Creating""); --> 984 ds = create(filename, matrix[:, ix:ix + window].toarray(), row_attrs, ca, file_attrs, chunks, chunk_cache, dtype, compression_opts); 985 else:; 986 logging.info(""Adding columns""). /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in create(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts); 1033 ; 1034 for key, vals in row_attrs.items():; -> 1035 ds.set_attr(key, vals, axis=0); 1036 ; 1037",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/154#issuecomment-389486091
https://github.com/scverse/scanpy/issues/154#issuecomment-389486091:1522,Testability,log,logging,1522,"rite_loom('./filtered_gene_bc_matrices_h5.loom'). /anaconda3/lib/python3.6/site-packages/anndata/base.py in write_loom(self, filename); 1736 """"""; 1737 from .readwrite.write import write_loom; -> 1738 write_loom(filename, self); 1739 ; 1740 @staticmethod. /anaconda3/lib/python3.6/site-packages/anndata/readwrite/write.py in write_loom(filename, adata); 71 if os.path.exists(filename):; 72 os.remove(filename); ---> 73 create(filename, X, row_attrs=row_attrs, col_attrs=col_attrs); 74 ; 75 . /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in create(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts); 1019 ; 1020 if scipy.sparse.issparse(matrix):; -> 1021 return _create_sparse(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts); 1022 ; 1023 # Create the file (empty). /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in _create_sparse(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts); 982 if ds is None:; 983 logging.info(""Creating""); --> 984 ds = create(filename, matrix[:, ix:ix + window].toarray(), row_attrs, ca, file_attrs, chunks, chunk_cache, dtype, compression_opts); 985 else:; 986 logging.info(""Adding columns""). /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in create(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts); 1033 ; 1034 for key, vals in row_attrs.items():; -> 1035 ds.set_attr(key, vals, axis=0); 1036 ; 1037 for key, vals in col_attrs.items():. /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in set_attr(self, name, values, axis, dtype); 561 ; 562 self.delete_attr(name, axis, raise_on_missing=False); --> 563 self._save_attr(name, values, axis); 564 self._load_attr(name, axis); 565 . /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in _save_attr(self, name, values, axis); 166 if self.mode != ""r+"":; 167 raise IOError(""Cannot save attributes",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/154#issuecomment-389486091
https://github.com/scverse/scanpy/issues/154#issuecomment-389486091:1704,Testability,log,logging,1704,"e_loom(filename, self); 1739 ; 1740 @staticmethod. /anaconda3/lib/python3.6/site-packages/anndata/readwrite/write.py in write_loom(filename, adata); 71 if os.path.exists(filename):; 72 os.remove(filename); ---> 73 create(filename, X, row_attrs=row_attrs, col_attrs=col_attrs); 74 ; 75 . /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in create(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts); 1019 ; 1020 if scipy.sparse.issparse(matrix):; -> 1021 return _create_sparse(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts); 1022 ; 1023 # Create the file (empty). /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in _create_sparse(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts); 982 if ds is None:; 983 logging.info(""Creating""); --> 984 ds = create(filename, matrix[:, ix:ix + window].toarray(), row_attrs, ca, file_attrs, chunks, chunk_cache, dtype, compression_opts); 985 else:; 986 logging.info(""Adding columns""). /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in create(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts); 1033 ; 1034 for key, vals in row_attrs.items():; -> 1035 ds.set_attr(key, vals, axis=0); 1036 ; 1037 for key, vals in col_attrs.items():. /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in set_attr(self, name, values, axis, dtype); 561 ; 562 self.delete_attr(name, axis, raise_on_missing=False); --> 563 self._save_attr(name, values, axis); 564 self._load_attr(name, axis); 565 . /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in _save_attr(self, name, values, axis); 166 if self.mode != ""r+"":; 167 raise IOError(""Cannot save attributes when connected in read-only mode""); --> 168 if values.dtype.type is np.str_:; 169 values = np.array([x.encode('ascii', 'ignore') for x in values]); 170 . AttributeError: 'list' object has no attribute 'd",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/154#issuecomment-389486091
https://github.com/scverse/scanpy/issues/154#issuecomment-389513091:24,Availability,error,error,24,"As you can see from the error output, this is an error within loompy. I assume you don't have the most recent version of loompy - there used to be a few bugs in it. Try with an update installation of loompy. I'm running 0.2.8 and never had any problem with it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/154#issuecomment-389513091
https://github.com/scverse/scanpy/issues/154#issuecomment-389513091:49,Availability,error,error,49,"As you can see from the error output, this is an error within loompy. I assume you don't have the most recent version of loompy - there used to be a few bugs in it. Try with an update installation of loompy. I'm running 0.2.8 and never had any problem with it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/154#issuecomment-389513091
https://github.com/scverse/scanpy/issues/154#issuecomment-389513091:177,Deployability,update,update,177,"As you can see from the error output, this is an error within loompy. I assume you don't have the most recent version of loompy - there used to be a few bugs in it. Try with an update installation of loompy. I'm running 0.2.8 and never had any problem with it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/154#issuecomment-389513091
https://github.com/scverse/scanpy/issues/154#issuecomment-389513091:184,Deployability,install,installation,184,"As you can see from the error output, this is an error within loompy. I assume you don't have the most recent version of loompy - there used to be a few bugs in it. Try with an update installation of loompy. I'm running 0.2.8 and never had any problem with it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/154#issuecomment-389513091
https://github.com/scverse/scanpy/issues/156#issuecomment-389749721:33,Modifiability,variab,variable,33,"Hi!. The colors of a categorical variable `'louvain'` are stored in `.uns['louvain_colors']`. You can directly modify that. The `palette` keyword can be used to initialize the field in `.uns`. For instance using `sc.pl.tsne(adata, color='louvain', palette=sc.pl.palletes.vega_20)`. But you're right, it should not only be used for initialization but also overwrite an existing color annotation. I'll fix this in version 1.1. Several other plotting flaws will be fixed in there, too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/156#issuecomment-389749721
https://github.com/scverse/scanpy/issues/156#issuecomment-390299178:90,Availability,avail,available,90,"Awesome, thanks for the suggestion to look in the .uns variable! Just to confirm, are the available palettes options that can be used with the palette keyword in the call to the sc.pl.tsne() function listed in the palettes.py file? For example things like vega_20_scanpy, zeileis_26, and godsnot_64?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/156#issuecomment-390299178
https://github.com/scverse/scanpy/issues/156#issuecomment-390299178:55,Modifiability,variab,variable,55,"Awesome, thanks for the suggestion to look in the .uns variable! Just to confirm, are the available palettes options that can be used with the palette keyword in the call to the sc.pl.tsne() function listed in the palettes.py file? For example things like vega_20_scanpy, zeileis_26, and godsnot_64?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/156#issuecomment-390299178
https://github.com/scverse/scanpy/issues/156#issuecomment-390478463:184,Modifiability,extend,extend,184,"Yes, they are in that file: https://github.com/theislab/scanpy/blob/master/scanpy/plotting/palettes.py. Of course, user palettes are also accepted. Let me know if you need more. We'll extend this in the future.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/156#issuecomment-390478463
https://github.com/scverse/scanpy/issues/156#issuecomment-441815667:70,Modifiability,variab,variables,70,@falexwolf @fidelram how to we change the color palette for numerical variables? currently setting `palette = 'Oranges'` only works for the categorical ones,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/156#issuecomment-441815667
https://github.com/scverse/scanpy/issues/156#issuecomment-441950074:46,Availability,avail,available,46,"use `cmap`. In general you can use any option available for; `matplotlib.pyplot.scatter` including vmin, vmax, etc. On Mon, Nov 26, 2018 at 11:01 PM aopisco <notifications@github.com> wrote:. > @falexwolf <https://github.com/falexwolf> @fidelram; > <https://github.com/fidelram> how to we change the color palette for; > numerical variables? currently setting palette = 'Oranges' only works for; > the categorical ones; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/156#issuecomment-441815667>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1RFS_19jC__9pOo04OZkjN_hVZvvks5uzGTBgaJpZM4UCLlA>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/156#issuecomment-441950074
https://github.com/scverse/scanpy/issues/156#issuecomment-441950074:331,Modifiability,variab,variables,331,"use `cmap`. In general you can use any option available for; `matplotlib.pyplot.scatter` including vmin, vmax, etc. On Mon, Nov 26, 2018 at 11:01 PM aopisco <notifications@github.com> wrote:. > @falexwolf <https://github.com/falexwolf> @fidelram; > <https://github.com/fidelram> how to we change the color palette for; > numerical variables? currently setting palette = 'Oranges' only works for; > the categorical ones; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/156#issuecomment-441815667>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1RFS_19jC__9pOo04OZkjN_hVZvvks5uzGTBgaJpZM4UCLlA>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/156#issuecomment-441950074
https://github.com/scverse/scanpy/issues/156#issuecomment-496532324:192,Modifiability,extend,extend,192,"> Yes, they are in that file: https://github.com/theislab/scanpy/blob/master/scanpy/plotting/palettes.py; > ; > Of course, user palettes are also accepted. Let me know if you need more. We'll extend this in the future. I have the data for 120 clusters, is it possible that I can have 120 colors?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/156#issuecomment-496532324
https://github.com/scverse/scanpy/issues/156#issuecomment-496543294:101,Usability,clear,clear,101,"You can always choose a palTete like 'Blues', 'Reds', 'binary' that will; give you a gradient from a clear to a darker color. Maybe that helps but I; agree with Philipp, 120 clusters is a lot to visualize with different; colors. On Tue, May 28, 2019 at 4:21 PM Philipp A. <notifications@github.com> wrote:. > Closed #156 <https://github.com/theislab/scanpy/issues/156>.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/156?email_source=notifications&email_token=ABF37VNHLRL6TEP3I7BBLEDPXU5X7A5CNFSM4FAIXFAKYY3PNVWWK3TUL52HS4DFWZEXG43VMVCXMZLOORHG65DJMZUWGYLUNFXW5KTDN5WW2ZLOORPWSZGORVQ5OJI#event-2371999525>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABF37VNDXU3HBFBIZ6LII73PXU5X7ANCNFSM4FAIXFAA>; > .; >. -- . Fidel Ramirez",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/156#issuecomment-496543294
https://github.com/scverse/scanpy/issues/156#issuecomment-523779587:102,Usability,clear,clear,102,"> You can always choose a palTete like 'Blues', 'Reds', 'binary' that will give you a gradient from a clear to a darker color. Maybe that helps but I agree with Philipp, 120 clusters is a lot to visualize with different colors.; > […](#); > On Tue, May 28, 2019 at 4:21 PM Philipp A. ***@***.***> wrote: Closed #156 <#156>. — You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub <#156?email_source=notifications&email_token=ABF37VNHLRL6TEP3I7BBLEDPXU5X7A5CNFSM4FAIXFAKYY3PNVWWK3TUL52HS4DFWZEXG43VMVCXMZLOORHG65DJMZUWGYLUNFXW5KTDN5WW2ZLOORPWSZGORVQ5OJI#event-2371999525>, or mute the thread <https://github.com/notifications/unsubscribe-auth/ABF37VNDXU3HBFBIZ6LII73PXU5X7ANCNFSM4FAIXFAA> .; > -- Fidel Ramirez. Thanks for your great idea, I will give it a try.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/156#issuecomment-523779587
https://github.com/scverse/scanpy/issues/158#issuecomment-390604622:15,Availability,error,error,15,The same exact error also happens using the docker image suggested on the web site; fastgenomics/scanpy,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/158#issuecomment-390604622
https://github.com/scverse/scanpy/issues/158#issuecomment-390636495:446,Availability,error,error,446,"Hm, this is really strange. Sorry about this. The docker image might be out-of-date now - you compiled, @flying-sheep, what do you think?. But you shouldn't need the docker image to exactly reproduce the results. Which versions do you run?. The latest version of the tutorial says; ```; scanpy==1.0.2 anndata==0.5.8 numpy==1.14.1 scipy==1.0.0 pandas==0.22.0 scikit-learn==0.19.1 statsmodels==0.8.0 python-igraph==0.7.1 louvain==0.6.1 ; ```. Your error is a Pandas error - do you run an older or more recent version of Pandas that might cause the problem?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/158#issuecomment-390636495
https://github.com/scverse/scanpy/issues/158#issuecomment-390636495:464,Availability,error,error,464,"Hm, this is really strange. Sorry about this. The docker image might be out-of-date now - you compiled, @flying-sheep, what do you think?. But you shouldn't need the docker image to exactly reproduce the results. Which versions do you run?. The latest version of the tutorial says; ```; scanpy==1.0.2 anndata==0.5.8 numpy==1.14.1 scipy==1.0.0 pandas==0.22.0 scikit-learn==0.19.1 statsmodels==0.8.0 python-igraph==0.7.1 louvain==0.6.1 ; ```. Your error is a Pandas error - do you run an older or more recent version of Pandas that might cause the problem?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/158#issuecomment-390636495
https://github.com/scverse/scanpy/issues/158#issuecomment-390636495:365,Usability,learn,learn,365,"Hm, this is really strange. Sorry about this. The docker image might be out-of-date now - you compiled, @flying-sheep, what do you think?. But you shouldn't need the docker image to exactly reproduce the results. Which versions do you run?. The latest version of the tutorial says; ```; scanpy==1.0.2 anndata==0.5.8 numpy==1.14.1 scipy==1.0.0 pandas==0.22.0 scikit-learn==0.19.1 statsmodels==0.8.0 python-igraph==0.7.1 louvain==0.6.1 ; ```. Your error is a Pandas error - do you run an older or more recent version of Pandas that might cause the problem?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/158#issuecomment-390636495
https://github.com/scverse/scanpy/issues/158#issuecomment-390636706:10,Deployability,install,install,10,"Type `pip install pandas==0.22.0` to get the same version and run the notebook again... Of course, if we are not compatible with more recent versions of pandas, we'll immediately fix the problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/158#issuecomment-390636706
https://github.com/scverse/scanpy/issues/158#issuecomment-390656723:103,Usability,learn,learn,103,"Hi, here my settings:. scanpy==1.0.4 anndata==0.6.1 numpy==1.14.2 scipy==1.1.0 pandas==0.23.0 ; scikit-learn==0.19.1 statsmodels==0.9.0 python-igraph==0.7.1. Ivan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/158#issuecomment-390656723
https://github.com/scverse/scanpy/issues/158#issuecomment-390823067:140,Deployability,install,install,140,"Ok, it's related to pandas 0.23 - runs on pandas 0.22. Don't know what happened but I'll fix this tomorrow. In the meanwhile you could `pip install pandas==0.22.0`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/158#issuecomment-390823067
https://github.com/scverse/scanpy/issues/158#issuecomment-391253208:64,Deployability,install,installing,64,"> Unfortunately, we had a prerelease with a bug. the dangers of installing prereleases :wink:. the docker image is on 1.1a1 though :innocent:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/158#issuecomment-391253208
https://github.com/scverse/scanpy/issues/158#issuecomment-391277887:16,Availability,down,downgrade,16,The solution do downgrade to pandas=0.22 worked for me. Thanks,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/158#issuecomment-391277887
https://github.com/scverse/scanpy/issues/159#issuecomment-390656402:129,Modifiability,extend,extend,129,"No, it currently doesn't. Instead it uses the `scores`... usually some ""differential z-score"" that goes into the t-test. We will extend differential testing in the future.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/159#issuecomment-390656402
https://github.com/scverse/scanpy/issues/159#issuecomment-390656402:115,Testability,test,test,115,"No, it currently doesn't. Instead it uses the `scores`... usually some ""differential z-score"" that goes into the t-test. We will extend differential testing in the future.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/159#issuecomment-390656402
https://github.com/scverse/scanpy/issues/159#issuecomment-390656402:149,Testability,test,testing,149,"No, it currently doesn't. Instead it uses the `scores`... usually some ""differential z-score"" that goes into the t-test. We will extend differential testing in the future.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/159#issuecomment-390656402
https://github.com/scverse/scanpy/issues/159#issuecomment-420329092:157,Testability,test,testing,157,Just wanted to bring this question back up - it would be great if we could get fold changes and p-values returned from the relevant methods for differential testing in scanpy. Thanks!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/159#issuecomment-420329092
https://github.com/scverse/scanpy/issues/159#issuecomment-420332760:105,Deployability,integrat,integrates,105,"There now is a much more powerful differential testing package `diffxpy`, @davidsebfischer, which easily integrates into Scanpy. @a-munoz-rojas Would you consider making a pull request that adds log-fold changes for t-test etc. in `rank_genes_groups`? My bandwidth is limited these days, I will certainly do it at some point, but it's faster if you do it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/159#issuecomment-420332760
https://github.com/scverse/scanpy/issues/159#issuecomment-420332760:25,Energy Efficiency,power,powerful,25,"There now is a much more powerful differential testing package `diffxpy`, @davidsebfischer, which easily integrates into Scanpy. @a-munoz-rojas Would you consider making a pull request that adds log-fold changes for t-test etc. in `rank_genes_groups`? My bandwidth is limited these days, I will certainly do it at some point, but it's faster if you do it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/159#issuecomment-420332760
https://github.com/scverse/scanpy/issues/159#issuecomment-420332760:105,Integrability,integrat,integrates,105,"There now is a much more powerful differential testing package `diffxpy`, @davidsebfischer, which easily integrates into Scanpy. @a-munoz-rojas Would you consider making a pull request that adds log-fold changes for t-test etc. in `rank_genes_groups`? My bandwidth is limited these days, I will certainly do it at some point, but it's faster if you do it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/159#issuecomment-420332760
https://github.com/scverse/scanpy/issues/159#issuecomment-420332760:47,Testability,test,testing,47,"There now is a much more powerful differential testing package `diffxpy`, @davidsebfischer, which easily integrates into Scanpy. @a-munoz-rojas Would you consider making a pull request that adds log-fold changes for t-test etc. in `rank_genes_groups`? My bandwidth is limited these days, I will certainly do it at some point, but it's faster if you do it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/159#issuecomment-420332760
https://github.com/scverse/scanpy/issues/159#issuecomment-420332760:195,Testability,log,log-fold,195,"There now is a much more powerful differential testing package `diffxpy`, @davidsebfischer, which easily integrates into Scanpy. @a-munoz-rojas Would you consider making a pull request that adds log-fold changes for t-test etc. in `rank_genes_groups`? My bandwidth is limited these days, I will certainly do it at some point, but it's faster if you do it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/159#issuecomment-420332760
https://github.com/scverse/scanpy/issues/159#issuecomment-420332760:218,Testability,test,test,218,"There now is a much more powerful differential testing package `diffxpy`, @davidsebfischer, which easily integrates into Scanpy. @a-munoz-rojas Would you consider making a pull request that adds log-fold changes for t-test etc. in `rank_genes_groups`? My bandwidth is limited these days, I will certainly do it at some point, but it's faster if you do it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/159#issuecomment-420332760
https://github.com/scverse/scanpy/issues/159#issuecomment-420358808:120,Usability,learn,learning,120,"@falexwolf Thanks for pointing out this package, I'll give it a try. I'll also give the pull request a shot - I'm still learning my way around this package but if I can do it I'll submit it for sure.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/159#issuecomment-420358808
https://github.com/scverse/scanpy/issues/159#issuecomment-420362783:86,Availability,avail,available,86,@falexwolf Is there anyplace where we can read into `diffxpy`? I've been benchmarking available marker gene detection algorithms and am interested to see what is included in this new package.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/159#issuecomment-420362783
https://github.com/scverse/scanpy/issues/159#issuecomment-420362783:108,Safety,detect,detection,108,@falexwolf Is there anyplace where we can read into `diffxpy`? I've been benchmarking available marker gene detection algorithms and am interested to see what is included in this new package.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/159#issuecomment-420362783
https://github.com/scverse/scanpy/issues/159#issuecomment-420362783:73,Testability,benchmark,benchmarking,73,@falexwolf Is there anyplace where we can read into `diffxpy`? I've been benchmarking available marker gene detection algorithms and am interested to see what is included in this new package.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/159#issuecomment-420362783
https://github.com/scverse/scanpy/issues/159#issuecomment-420413750:85,Deployability,install,install,85,"@davidsebfischer @falexwolf Also, where can we get this package? I tried doing a pip install, and while the package is listed the installation failed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/159#issuecomment-420413750
https://github.com/scverse/scanpy/issues/159#issuecomment-420413750:130,Deployability,install,installation,130,"@davidsebfischer @falexwolf Also, where can we get this package? I tried doing a pip install, and while the package is listed the installation failed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/159#issuecomment-420413750
https://github.com/scverse/scanpy/issues/159#issuecomment-420550464:130,Testability,test,testing,130,@falexwolf you were a little quick: @davidsebfischer didn’t turn it public yet. maybe david would like to send it to you for beta testing?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/159#issuecomment-420550464
https://github.com/scverse/scanpy/issues/159#issuecomment-421157493:110,Security,validat,validation,110,"Hi @a-munoz-rojas, diffxpy will be public very soon, once we finished running all benchmarks that we need for validation. I would be happy to help you set it up if you still want to give it a go then!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/159#issuecomment-421157493
https://github.com/scverse/scanpy/issues/159#issuecomment-421157493:82,Testability,benchmark,benchmarks,82,"Hi @a-munoz-rojas, diffxpy will be public very soon, once we finished running all benchmarks that we need for validation. I would be happy to help you set it up if you still want to give it a go then!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/159#issuecomment-421157493
https://github.com/scverse/scanpy/issues/160#issuecomment-391972041:34,Availability,error,error,34,"Hi! Thanks for using scanpy!. The error is in [`anndata`](https://github.com/theislab/anndata) so filing a bug there would be more fitting. I did it for you: https://github.com/theislab/anndata/issues/23. ---. Lastly, please do ```` ```pytb ```` for your python tracebacks blocks so we get syntax highlighting:. > ````markdown; > I also get the error when I try to use it with jupyter notebook:; > ; > ```pytb; > import scanpy.api as sc; > ; > Traceback (most recent call last):; > ; > File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-> packages/IPython/core/interactiveshell.py"", line 2910, in run_code; > exec(code_obj, self.user_global_ns, self.user_ns); > ; > […]; > ; > SyntaxError: invalid syntax; > ```; > ````. This would look like this:. > I also get the error when I try to use it with jupyter notebook:; > ; > ```pytb; > import scanpy.api as sc; > ; > Traceback (most recent call last):; > ; > File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-packages/IPython/core/interactiveshell.py"", line 2910, in run_code; > exec(code_obj, self.user_global_ns, self.user_ns); > ; > […]; > ; > SyntaxError: invalid syntax; > ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/160#issuecomment-391972041
https://github.com/scverse/scanpy/issues/160#issuecomment-391972041:345,Availability,error,error,345,"Hi! Thanks for using scanpy!. The error is in [`anndata`](https://github.com/theislab/anndata) so filing a bug there would be more fitting. I did it for you: https://github.com/theislab/anndata/issues/23. ---. Lastly, please do ```` ```pytb ```` for your python tracebacks blocks so we get syntax highlighting:. > ````markdown; > I also get the error when I try to use it with jupyter notebook:; > ; > ```pytb; > import scanpy.api as sc; > ; > Traceback (most recent call last):; > ; > File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-> packages/IPython/core/interactiveshell.py"", line 2910, in run_code; > exec(code_obj, self.user_global_ns, self.user_ns); > ; > […]; > ; > SyntaxError: invalid syntax; > ```; > ````. This would look like this:. > I also get the error when I try to use it with jupyter notebook:; > ; > ```pytb; > import scanpy.api as sc; > ; > Traceback (most recent call last):; > ; > File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-packages/IPython/core/interactiveshell.py"", line 2910, in run_code; > exec(code_obj, self.user_global_ns, self.user_ns); > ; > […]; > ; > SyntaxError: invalid syntax; > ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/160#issuecomment-391972041
https://github.com/scverse/scanpy/issues/160#issuecomment-391972041:786,Availability,error,error,786,"Hi! Thanks for using scanpy!. The error is in [`anndata`](https://github.com/theislab/anndata) so filing a bug there would be more fitting. I did it for you: https://github.com/theislab/anndata/issues/23. ---. Lastly, please do ```` ```pytb ```` for your python tracebacks blocks so we get syntax highlighting:. > ````markdown; > I also get the error when I try to use it with jupyter notebook:; > ; > ```pytb; > import scanpy.api as sc; > ; > Traceback (most recent call last):; > ; > File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-> packages/IPython/core/interactiveshell.py"", line 2910, in run_code; > exec(code_obj, self.user_global_ns, self.user_ns); > ; > […]; > ; > SyntaxError: invalid syntax; > ```; > ````. This would look like this:. > I also get the error when I try to use it with jupyter notebook:; > ; > ```pytb; > import scanpy.api as sc; > ; > Traceback (most recent call last):; > ; > File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-packages/IPython/core/interactiveshell.py"", line 2910, in run_code; > exec(code_obj, self.user_global_ns, self.user_ns); > ; > […]; > ; > SyntaxError: invalid syntax; > ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/160#issuecomment-391972041
https://github.com/scverse/scanpy/issues/160#issuecomment-391984289:132,Deployability,release,release,132,fixed in https://github.com/theislab/anndata/commit/555c15c8a170944b762ba7ce1d8c0b41f4e4dfbe. the fix should be in the next anndata release (0.6.2 or 0.7),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/160#issuecomment-391984289
https://github.com/scverse/scanpy/issues/160#issuecomment-392070292:45,Deployability,install,install,45,"Thank you for the prompt response! I did not install anndata separately, I just followed instructions from https://scanpy.readthedocs.io/en/latest/installation.html to install scanpy using `pip install scanpy` in miniconda environment. Do I need to reinstall another version of anndata? Or of scanpy? Sorry, still not sure how to fix this. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/160#issuecomment-392070292
https://github.com/scverse/scanpy/issues/160#issuecomment-392070292:147,Deployability,install,installation,147,"Thank you for the prompt response! I did not install anndata separately, I just followed instructions from https://scanpy.readthedocs.io/en/latest/installation.html to install scanpy using `pip install scanpy` in miniconda environment. Do I need to reinstall another version of anndata? Or of scanpy? Sorry, still not sure how to fix this. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/160#issuecomment-392070292
https://github.com/scverse/scanpy/issues/160#issuecomment-392070292:168,Deployability,install,install,168,"Thank you for the prompt response! I did not install anndata separately, I just followed instructions from https://scanpy.readthedocs.io/en/latest/installation.html to install scanpy using `pip install scanpy` in miniconda environment. Do I need to reinstall another version of anndata? Or of scanpy? Sorry, still not sure how to fix this. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/160#issuecomment-392070292
https://github.com/scverse/scanpy/issues/160#issuecomment-392070292:194,Deployability,install,install,194,"Thank you for the prompt response! I did not install anndata separately, I just followed instructions from https://scanpy.readthedocs.io/en/latest/installation.html to install scanpy using `pip install scanpy` in miniconda environment. Do I need to reinstall another version of anndata? Or of scanpy? Sorry, still not sure how to fix this. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/160#issuecomment-392070292
https://github.com/scverse/scanpy/issues/160#issuecomment-392088433:55,Deployability,update,update,55,"yeah, this was fixed inside of anndata, so you need to update anndata, not scanpy. `pip install anndata~=0.6.4` should get you a version where this is fixed! 😄",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/160#issuecomment-392088433
https://github.com/scverse/scanpy/issues/160#issuecomment-392088433:88,Deployability,install,install,88,"yeah, this was fixed inside of anndata, so you need to update anndata, not scanpy. `pip install anndata~=0.6.4` should get you a version where this is fixed! 😄",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/160#issuecomment-392088433
https://github.com/scverse/scanpy/pull/161#issuecomment-391988495:9,Testability,test,tests,9,"Only the tests that fail on master also fail here, so this is fine and can be merged",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/161#issuecomment-391988495
https://github.com/scverse/scanpy/issues/162#issuecomment-391991529:51,Availability,error,error,51,I'm pretty sure it's the pandas 0.23 issue... same error message as the one encountered here: https://github.com/theislab/scanpy/issues/158,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/162#issuecomment-391991529
https://github.com/scverse/scanpy/issues/162#issuecomment-391991529:57,Integrability,message,message,57,I'm pretty sure it's the pandas 0.23 issue... same error message as the one encountered here: https://github.com/theislab/scanpy/issues/158,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/162#issuecomment-391991529
https://github.com/scverse/scanpy/issues/162#issuecomment-392000010:6,Testability,Test,Tests,6,Done! Tests fixed in 478e3dcb4706328bb3726fb674473e490f353a33,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/162#issuecomment-392000010
https://github.com/scverse/scanpy/issues/163#issuecomment-392030082:468,Performance,bottleneck,bottleneck,468,"Very nice! Thank you for the benchmark! 🙂 It's a bit astonishing as I investigated this some a bit more than a year ago but maybe there was some improvement... I am in principle happy to change this soon - I don't think that the change within floating point precision will change results [it does, for instance, in PCA... of course not a qualitative result, but nonetheless a tSNE could be rotated etc.]!. How did you come to investigating this? Was the computation a bottleneck for you? The matrix you provide is almost - for current standards - ""unrealistically large"".",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/163#issuecomment-392030082
https://github.com/scverse/scanpy/issues/163#issuecomment-392030082:29,Testability,benchmark,benchmark,29,"Very nice! Thank you for the benchmark! 🙂 It's a bit astonishing as I investigated this some a bit more than a year ago but maybe there was some improvement... I am in principle happy to change this soon - I don't think that the change within floating point precision will change results [it does, for instance, in PCA... of course not a qualitative result, but nonetheless a tSNE could be rotated etc.]!. How did you come to investigating this? Was the computation a bottleneck for you? The matrix you provide is almost - for current standards - ""unrealistically large"".",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/163#issuecomment-392030082
https://github.com/scverse/scanpy/issues/163#issuecomment-392049026:127,Performance,perform,performance,127,"I was looking at the scanpy function to compute the mean and variance an noticed that it had some comments inside, pointing to performance issues. Thus, I looked for an alternative method, found the sklearn sparse function and then tested it in an artificially large matrix. Otherwise, I did not have any trouble with the current implementation. The floating point precision is higher in the sklearn method, thus I suppose this is not an issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/163#issuecomment-392049026
https://github.com/scverse/scanpy/issues/163#issuecomment-392049026:232,Testability,test,tested,232,"I was looking at the scanpy function to compute the mean and variance an noticed that it had some comments inside, pointing to performance issues. Thus, I looked for an alternative method, found the sklearn sparse function and then tested it in an artificially large matrix. Otherwise, I did not have any trouble with the current implementation. The floating point precision is higher in the sklearn method, thus I suppose this is not an issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/163#issuecomment-392049026
https://github.com/scverse/scanpy/issues/163#issuecomment-392421567:651,Deployability,release,releases,651,"Kinda related, I was about to open an issue on the memory usage of this function. The current implementation can double the memory usage of a program, I believe due to intermediate arrays in the current code. I'd come up with a slower but fewer allocation method for dense arrays (which could probably be sped up with a little `numba`):. ```python; def lessalloc_dense(X):; mean = X.mean(axis=0); mean_sq = np.apply_along_axis(lambda x: np.square(x).mean(), 0, X); var = (mean_sq - mean**2) * ((X.shape[0]/(X.shape[0]-1))); return mean, var; ```. And looked at memory usage using [`memory_profiler`](https://github.com/pythonprofilers/memory_profiler/releases), including @fidelram 's method:. ![mean_var_memory](https://user-images.githubusercontent.com/8238804/40597918-eacd8764-6287-11e8-98ff-017e697b350d.png). [Full script for benchmark here.](https://gist.github.com/ivirshup/a6facfa1ace5b356ea2d18ff3ffe0cb9)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/163#issuecomment-392421567
https://github.com/scverse/scanpy/issues/163#issuecomment-392421567:832,Testability,benchmark,benchmark,832,"Kinda related, I was about to open an issue on the memory usage of this function. The current implementation can double the memory usage of a program, I believe due to intermediate arrays in the current code. I'd come up with a slower but fewer allocation method for dense arrays (which could probably be sped up with a little `numba`):. ```python; def lessalloc_dense(X):; mean = X.mean(axis=0); mean_sq = np.apply_along_axis(lambda x: np.square(x).mean(), 0, X); var = (mean_sq - mean**2) * ((X.shape[0]/(X.shape[0]-1))); return mean, var; ```. And looked at memory usage using [`memory_profiler`](https://github.com/pythonprofilers/memory_profiler/releases), including @fidelram 's method:. ![mean_var_memory](https://user-images.githubusercontent.com/8238804/40597918-eacd8764-6287-11e8-98ff-017e697b350d.png). [Full script for benchmark here.](https://gist.github.com/ivirshup/a6facfa1ace5b356ea2d18ff3ffe0cb9)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/163#issuecomment-392421567
https://github.com/scverse/scanpy/pull/164#issuecomment-394292079:134,Energy Efficiency,efficient,efficient,134,"Thanks for merging. I did some test and the memory usage was not very high with 1 or 20 regressors. Nevertheless, I tried more memory efficient methods but the gain was minimal.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/164#issuecomment-394292079
https://github.com/scverse/scanpy/pull/164#issuecomment-394292079:31,Testability,test,test,31,"Thanks for merging. I did some test and the memory usage was not very high with 1 or 20 regressors. Nevertheless, I tried more memory efficient methods but the gain was minimal.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/164#issuecomment-394292079
https://github.com/scverse/scanpy/issues/165#issuecomment-393739002:95,Usability,Simpl,Simply,95,"Hi Huidong,. thank you for the kind words!. `adata_subset` is a `view` on an `AnnData` object. Simply check the output when printing it. You can do; ```; adata_subset = adata[ list_of_barcodes, :].copy(); ```; to get an actual `AnnData`. Of course, you as a user shouldn't have to care about making this copy. As soon as you want to modify the view, it should automatically make this copy itself. This is why all the attributes *listen* to whether you're modifying them or not. I just forgot to add the listener to the *full* `.obs` attribute. I'll do that very soon. In the meanwhile, call `.copy` yourself. ;). Best,; Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/165#issuecomment-393739002
https://github.com/scverse/scanpy/issues/166#issuecomment-393834418:229,Availability,error,error,229,"Yes, this is related to the fact that `sanitize_anndata` cannot be meaningfully applied to a view of `AnnData`. You're right that one should also account for this case... I'll give it a thought. At least there should be a proper error hinting people to call `sc.utils.sanitize_anndata` when trying the call you mention. Thank you very much for pointing this out. :smile: It should have happened also before version 1.1, though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/166#issuecomment-393834418
https://github.com/scverse/scanpy/issues/166#issuecomment-491099687:98,Testability,assert,assert,98,"I have something that might be related:. ```python; ad = ad[ad.obs['cell type'] != 'nan'].copy(); assert np.all(ad.obs['cell type'] != 'nan'); sc.utils.sanitize_anndata(ad); assert np.all(ad.obs['cell type'] != 'nan'); ```. This fails in the second assert:. ```python; AssertionError Traceback (most recent call last); <ipython-input-103-2f44e51fdcae> in <module>; 8 assert np.all(ad.obs['cell type'] != 'nan'); 9 sc.utils.sanitize_anndata(ad); ---> 10 assert np.all(ad.obs['cell type'] != 'nan'); 11 ; 12 . AssertionError: ; ```. It's really black magic, any ideas?. PS: `nan`s are really string, not proper NaNs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/166#issuecomment-491099687
https://github.com/scverse/scanpy/issues/166#issuecomment-491099687:174,Testability,assert,assert,174,"I have something that might be related:. ```python; ad = ad[ad.obs['cell type'] != 'nan'].copy(); assert np.all(ad.obs['cell type'] != 'nan'); sc.utils.sanitize_anndata(ad); assert np.all(ad.obs['cell type'] != 'nan'); ```. This fails in the second assert:. ```python; AssertionError Traceback (most recent call last); <ipython-input-103-2f44e51fdcae> in <module>; 8 assert np.all(ad.obs['cell type'] != 'nan'); 9 sc.utils.sanitize_anndata(ad); ---> 10 assert np.all(ad.obs['cell type'] != 'nan'); 11 ; 12 . AssertionError: ; ```. It's really black magic, any ideas?. PS: `nan`s are really string, not proper NaNs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/166#issuecomment-491099687
https://github.com/scverse/scanpy/issues/166#issuecomment-491099687:249,Testability,assert,assert,249,"I have something that might be related:. ```python; ad = ad[ad.obs['cell type'] != 'nan'].copy(); assert np.all(ad.obs['cell type'] != 'nan'); sc.utils.sanitize_anndata(ad); assert np.all(ad.obs['cell type'] != 'nan'); ```. This fails in the second assert:. ```python; AssertionError Traceback (most recent call last); <ipython-input-103-2f44e51fdcae> in <module>; 8 assert np.all(ad.obs['cell type'] != 'nan'); 9 sc.utils.sanitize_anndata(ad); ---> 10 assert np.all(ad.obs['cell type'] != 'nan'); 11 ; 12 . AssertionError: ; ```. It's really black magic, any ideas?. PS: `nan`s are really string, not proper NaNs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/166#issuecomment-491099687
https://github.com/scverse/scanpy/issues/166#issuecomment-491099687:269,Testability,Assert,AssertionError,269,"I have something that might be related:. ```python; ad = ad[ad.obs['cell type'] != 'nan'].copy(); assert np.all(ad.obs['cell type'] != 'nan'); sc.utils.sanitize_anndata(ad); assert np.all(ad.obs['cell type'] != 'nan'); ```. This fails in the second assert:. ```python; AssertionError Traceback (most recent call last); <ipython-input-103-2f44e51fdcae> in <module>; 8 assert np.all(ad.obs['cell type'] != 'nan'); 9 sc.utils.sanitize_anndata(ad); ---> 10 assert np.all(ad.obs['cell type'] != 'nan'); 11 ; 12 . AssertionError: ; ```. It's really black magic, any ideas?. PS: `nan`s are really string, not proper NaNs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/166#issuecomment-491099687
https://github.com/scverse/scanpy/issues/166#issuecomment-491099687:367,Testability,assert,assert,367,"I have something that might be related:. ```python; ad = ad[ad.obs['cell type'] != 'nan'].copy(); assert np.all(ad.obs['cell type'] != 'nan'); sc.utils.sanitize_anndata(ad); assert np.all(ad.obs['cell type'] != 'nan'); ```. This fails in the second assert:. ```python; AssertionError Traceback (most recent call last); <ipython-input-103-2f44e51fdcae> in <module>; 8 assert np.all(ad.obs['cell type'] != 'nan'); 9 sc.utils.sanitize_anndata(ad); ---> 10 assert np.all(ad.obs['cell type'] != 'nan'); 11 ; 12 . AssertionError: ; ```. It's really black magic, any ideas?. PS: `nan`s are really string, not proper NaNs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/166#issuecomment-491099687
https://github.com/scverse/scanpy/issues/166#issuecomment-491099687:453,Testability,assert,assert,453,"I have something that might be related:. ```python; ad = ad[ad.obs['cell type'] != 'nan'].copy(); assert np.all(ad.obs['cell type'] != 'nan'); sc.utils.sanitize_anndata(ad); assert np.all(ad.obs['cell type'] != 'nan'); ```. This fails in the second assert:. ```python; AssertionError Traceback (most recent call last); <ipython-input-103-2f44e51fdcae> in <module>; 8 assert np.all(ad.obs['cell type'] != 'nan'); 9 sc.utils.sanitize_anndata(ad); ---> 10 assert np.all(ad.obs['cell type'] != 'nan'); 11 ; 12 . AssertionError: ; ```. It's really black magic, any ideas?. PS: `nan`s are really string, not proper NaNs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/166#issuecomment-491099687
https://github.com/scverse/scanpy/issues/166#issuecomment-491099687:508,Testability,Assert,AssertionError,508,"I have something that might be related:. ```python; ad = ad[ad.obs['cell type'] != 'nan'].copy(); assert np.all(ad.obs['cell type'] != 'nan'); sc.utils.sanitize_anndata(ad); assert np.all(ad.obs['cell type'] != 'nan'); ```. This fails in the second assert:. ```python; AssertionError Traceback (most recent call last); <ipython-input-103-2f44e51fdcae> in <module>; 8 assert np.all(ad.obs['cell type'] != 'nan'); 9 sc.utils.sanitize_anndata(ad); ---> 10 assert np.all(ad.obs['cell type'] != 'nan'); 11 ; 12 . AssertionError: ; ```. It's really black magic, any ideas?. PS: `nan`s are really string, not proper NaNs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/166#issuecomment-491099687
https://github.com/scverse/scanpy/issues/166#issuecomment-1696555861:141,Availability,error,error,141,I'm having this issue where I read in and merge multiple anndata's with concat. I can't run any of the plotting functions because I get this error. I tried to convert all object/string obs to categorical (except obs names) but I can't really get around it at all.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/166#issuecomment-1696555861
https://github.com/scverse/scanpy/issues/168#issuecomment-395589629:242,Testability,test,testing,242,"Ah, sorry, maybe this wasn't clear. You need to set the `.raw` attribute of `AnnData` for doing that at some point.; ```; adata.raw = adata # at the point during preprocessing at which you wish store a copy for visualization and differential testing; ```. You can then set `use_raw=False` in several functions, if you want to acess `.X` instead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168#issuecomment-395589629
https://github.com/scverse/scanpy/issues/168#issuecomment-395589629:29,Usability,clear,clear,29,"Ah, sorry, maybe this wasn't clear. You need to set the `.raw` attribute of `AnnData` for doing that at some point.; ```; adata.raw = adata # at the point during preprocessing at which you wish store a copy for visualization and differential testing; ```. You can then set `use_raw=False` in several functions, if you want to acess `.X` instead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168#issuecomment-395589629
https://github.com/scverse/scanpy/issues/168#issuecomment-395615173:519,Availability,down,downstream,519,"It is said that ""Be reminded that it is not advised to use the corrected data matrices for differential expression testing."" in scanpy document (http://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html) when execute MNN correction. However, Haghverdi Laleh (the one who presents MNN correction strategy, https://www.nature.com/articles/nbt.4091) says ""MNN correction improves differential expression analyses, After batch correction is performed, the corrected expression values can be used in routine downstream analyses such as clustering prior to differential gene expression identification"" in his Nature Biotech paper. So, I am a little confused. We have compared some corrections methods, such as regress_out, combat, MNN and MultiCCA (used by seurat), the results show that MNN and CCA have a better effect than regress_out and combat.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168#issuecomment-395615173
https://github.com/scverse/scanpy/issues/168#issuecomment-395615173:511,Integrability,rout,routine,511,"It is said that ""Be reminded that it is not advised to use the corrected data matrices for differential expression testing."" in scanpy document (http://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html) when execute MNN correction. However, Haghverdi Laleh (the one who presents MNN correction strategy, https://www.nature.com/articles/nbt.4091) says ""MNN correction improves differential expression analyses, After batch correction is performed, the corrected expression values can be used in routine downstream analyses such as clustering prior to differential gene expression identification"" in his Nature Biotech paper. So, I am a little confused. We have compared some corrections methods, such as regress_out, combat, MNN and MultiCCA (used by seurat), the results show that MNN and CCA have a better effect than regress_out and combat.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168#issuecomment-395615173
https://github.com/scverse/scanpy/issues/168#issuecomment-395615173:453,Performance,perform,performed,453,"It is said that ""Be reminded that it is not advised to use the corrected data matrices for differential expression testing."" in scanpy document (http://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html) when execute MNN correction. However, Haghverdi Laleh (the one who presents MNN correction strategy, https://www.nature.com/articles/nbt.4091) says ""MNN correction improves differential expression analyses, After batch correction is performed, the corrected expression values can be used in routine downstream analyses such as clustering prior to differential gene expression identification"" in his Nature Biotech paper. So, I am a little confused. We have compared some corrections methods, such as regress_out, combat, MNN and MultiCCA (used by seurat), the results show that MNN and CCA have a better effect than regress_out and combat.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168#issuecomment-395615173
https://github.com/scverse/scanpy/issues/168#issuecomment-395615173:115,Testability,test,testing,115,"It is said that ""Be reminded that it is not advised to use the corrected data matrices for differential expression testing."" in scanpy document (http://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html) when execute MNN correction. However, Haghverdi Laleh (the one who presents MNN correction strategy, https://www.nature.com/articles/nbt.4091) says ""MNN correction improves differential expression analyses, After batch correction is performed, the corrected expression values can be used in routine downstream analyses such as clustering prior to differential gene expression identification"" in his Nature Biotech paper. So, I am a little confused. We have compared some corrections methods, such as regress_out, combat, MNN and MultiCCA (used by seurat), the results show that MNN and CCA have a better effect than regress_out and combat.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168#issuecomment-395615173
https://github.com/scverse/scanpy/issues/168#issuecomment-395726806:74,Deployability,integrat,integration,74,"Hi. Maybe I can help a little as well. Typically batch correction or data integration methods would be used to obtain good clustering of the data, however once differential testing is performed it is still unclear whether the corrected data can or should be used (no batch correction method is perfect and may overcorrect). The standard strategy would be to correct for batch, and any other covariates that you are not interested in for the clustering process. Once you have the clusters, it is standard practice to go back to the raw data and use a differential testing algorithm that allows you to account for batch and other technical covariates in the model (e.g. MAST).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168#issuecomment-395726806
https://github.com/scverse/scanpy/issues/168#issuecomment-395726806:74,Integrability,integrat,integration,74,"Hi. Maybe I can help a little as well. Typically batch correction or data integration methods would be used to obtain good clustering of the data, however once differential testing is performed it is still unclear whether the corrected data can or should be used (no batch correction method is perfect and may overcorrect). The standard strategy would be to correct for batch, and any other covariates that you are not interested in for the clustering process. Once you have the clusters, it is standard practice to go back to the raw data and use a differential testing algorithm that allows you to account for batch and other technical covariates in the model (e.g. MAST).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168#issuecomment-395726806
https://github.com/scverse/scanpy/issues/168#issuecomment-395726806:184,Performance,perform,performed,184,"Hi. Maybe I can help a little as well. Typically batch correction or data integration methods would be used to obtain good clustering of the data, however once differential testing is performed it is still unclear whether the corrected data can or should be used (no batch correction method is perfect and may overcorrect). The standard strategy would be to correct for batch, and any other covariates that you are not interested in for the clustering process. Once you have the clusters, it is standard practice to go back to the raw data and use a differential testing algorithm that allows you to account for batch and other technical covariates in the model (e.g. MAST).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168#issuecomment-395726806
https://github.com/scverse/scanpy/issues/168#issuecomment-395726806:173,Testability,test,testing,173,"Hi. Maybe I can help a little as well. Typically batch correction or data integration methods would be used to obtain good clustering of the data, however once differential testing is performed it is still unclear whether the corrected data can or should be used (no batch correction method is perfect and may overcorrect). The standard strategy would be to correct for batch, and any other covariates that you are not interested in for the clustering process. Once you have the clusters, it is standard practice to go back to the raw data and use a differential testing algorithm that allows you to account for batch and other technical covariates in the model (e.g. MAST).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168#issuecomment-395726806
https://github.com/scverse/scanpy/issues/168#issuecomment-395726806:563,Testability,test,testing,563,"Hi. Maybe I can help a little as well. Typically batch correction or data integration methods would be used to obtain good clustering of the data, however once differential testing is performed it is still unclear whether the corrected data can or should be used (no batch correction method is perfect and may overcorrect). The standard strategy would be to correct for batch, and any other covariates that you are not interested in for the clustering process. Once you have the clusters, it is standard practice to go back to the raw data and use a differential testing algorithm that allows you to account for batch and other technical covariates in the model (e.g. MAST).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168#issuecomment-395726806
https://github.com/scverse/scanpy/issues/168#issuecomment-396115524:263,Performance,perform,perform,263,"@falexwolf ; In your Bioinformatics paper ""destiny: diffusion maps for large-scale single-cell data in R"", you show how to determine the optimal Gaussian kernel width and the plot of The Eigenvalues of the first 100 diffusion components. Could you tell us how to perform it with scanpy?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168#issuecomment-396115524
https://github.com/scverse/scanpy/issues/170#issuecomment-398737615:165,Usability,learn,learn,165,"well, we should warn the user when they do something like this then. lanzcos is an implementation detail that our API hides, so it’s pure coincidence for an user to learn about it beforehand.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/170#issuecomment-398737615
https://github.com/scverse/scanpy/issues/170#issuecomment-398776008:10,Safety,sanity check,sanity check,10,"yes, this sanity check for `n_components` in diffmap definitely makes sense. makes sense for any parameter in any function. but is so much work! :wink: I'll add it for this case, soon...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/170#issuecomment-398776008
https://github.com/scverse/scanpy/issues/172#issuecomment-398710779:97,Testability,log,log,97,"This makes sense. Negative means lead to negative dispersion values and hence NaNs upon taking a log. https://github.com/theislab/scanpy/blob/457d3aa8b7dd7344914edc7991618067cab34dde/scanpy/preprocessing/simple.py#L299-L312. One simply shouldn't take a logarithm in this case, so, I suggest you pass `log=False` to the function. Should we clarify the documentation: https://github.com/theislab/scanpy/blob/457d3aa8b7dd7344914edc7991618067cab34dde/scanpy/preprocessing/simple.py#L241-L243? Maybe put this right at the top so that it's not left unnoticed anymore?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172#issuecomment-398710779
https://github.com/scverse/scanpy/issues/172#issuecomment-398710779:253,Testability,log,logarithm,253,"This makes sense. Negative means lead to negative dispersion values and hence NaNs upon taking a log. https://github.com/theislab/scanpy/blob/457d3aa8b7dd7344914edc7991618067cab34dde/scanpy/preprocessing/simple.py#L299-L312. One simply shouldn't take a logarithm in this case, so, I suggest you pass `log=False` to the function. Should we clarify the documentation: https://github.com/theislab/scanpy/blob/457d3aa8b7dd7344914edc7991618067cab34dde/scanpy/preprocessing/simple.py#L241-L243? Maybe put this right at the top so that it's not left unnoticed anymore?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172#issuecomment-398710779
https://github.com/scverse/scanpy/issues/172#issuecomment-398710779:301,Testability,log,log,301,"This makes sense. Negative means lead to negative dispersion values and hence NaNs upon taking a log. https://github.com/theislab/scanpy/blob/457d3aa8b7dd7344914edc7991618067cab34dde/scanpy/preprocessing/simple.py#L299-L312. One simply shouldn't take a logarithm in this case, so, I suggest you pass `log=False` to the function. Should we clarify the documentation: https://github.com/theislab/scanpy/blob/457d3aa8b7dd7344914edc7991618067cab34dde/scanpy/preprocessing/simple.py#L241-L243? Maybe put this right at the top so that it's not left unnoticed anymore?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172#issuecomment-398710779
https://github.com/scverse/scanpy/issues/172#issuecomment-398710779:204,Usability,simpl,simple,204,"This makes sense. Negative means lead to negative dispersion values and hence NaNs upon taking a log. https://github.com/theislab/scanpy/blob/457d3aa8b7dd7344914edc7991618067cab34dde/scanpy/preprocessing/simple.py#L299-L312. One simply shouldn't take a logarithm in this case, so, I suggest you pass `log=False` to the function. Should we clarify the documentation: https://github.com/theislab/scanpy/blob/457d3aa8b7dd7344914edc7991618067cab34dde/scanpy/preprocessing/simple.py#L241-L243? Maybe put this right at the top so that it's not left unnoticed anymore?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172#issuecomment-398710779
https://github.com/scverse/scanpy/issues/172#issuecomment-398710779:229,Usability,simpl,simply,229,"This makes sense. Negative means lead to negative dispersion values and hence NaNs upon taking a log. https://github.com/theislab/scanpy/blob/457d3aa8b7dd7344914edc7991618067cab34dde/scanpy/preprocessing/simple.py#L299-L312. One simply shouldn't take a logarithm in this case, so, I suggest you pass `log=False` to the function. Should we clarify the documentation: https://github.com/theislab/scanpy/blob/457d3aa8b7dd7344914edc7991618067cab34dde/scanpy/preprocessing/simple.py#L241-L243? Maybe put this right at the top so that it's not left unnoticed anymore?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172#issuecomment-398710779
https://github.com/scverse/scanpy/issues/172#issuecomment-398710779:468,Usability,simpl,simple,468,"This makes sense. Negative means lead to negative dispersion values and hence NaNs upon taking a log. https://github.com/theislab/scanpy/blob/457d3aa8b7dd7344914edc7991618067cab34dde/scanpy/preprocessing/simple.py#L299-L312. One simply shouldn't take a logarithm in this case, so, I suggest you pass `log=False` to the function. Should we clarify the documentation: https://github.com/theislab/scanpy/blob/457d3aa8b7dd7344914edc7991618067cab34dde/scanpy/preprocessing/simple.py#L241-L243? Maybe put this right at the top so that it's not left unnoticed anymore?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172#issuecomment-398710779
https://github.com/scverse/scanpy/issues/172#issuecomment-398721208:210,Performance,perform,performing,210,"Ahh... this makes sense. And this makes it a bit dangerous as well. I would generally compute HVGs after batch correction.. and batch correction generally takes log-normalized data, so the data you have before performing this function will be log-normalized most of the time. I assume this is true not just for me. Maybe log=False should be the default? Or at least a warning should be output I feel.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172#issuecomment-398721208
https://github.com/scverse/scanpy/issues/172#issuecomment-398721208:161,Testability,log,log-normalized,161,"Ahh... this makes sense. And this makes it a bit dangerous as well. I would generally compute HVGs after batch correction.. and batch correction generally takes log-normalized data, so the data you have before performing this function will be log-normalized most of the time. I assume this is true not just for me. Maybe log=False should be the default? Or at least a warning should be output I feel.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172#issuecomment-398721208
https://github.com/scverse/scanpy/issues/172#issuecomment-398721208:243,Testability,log,log-normalized,243,"Ahh... this makes sense. And this makes it a bit dangerous as well. I would generally compute HVGs after batch correction.. and batch correction generally takes log-normalized data, so the data you have before performing this function will be log-normalized most of the time. I assume this is true not just for me. Maybe log=False should be the default? Or at least a warning should be output I feel.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172#issuecomment-398721208
https://github.com/scverse/scanpy/issues/172#issuecomment-398721208:321,Testability,log,log,321,"Ahh... this makes sense. And this makes it a bit dangerous as well. I would generally compute HVGs after batch correction.. and batch correction generally takes log-normalized data, so the data you have before performing this function will be log-normalized most of the time. I assume this is true not just for me. Maybe log=False should be the default? Or at least a warning should be output I feel.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172#issuecomment-398721208
https://github.com/scverse/scanpy/issues/172#issuecomment-398726169:140,Testability,benchmark,benchmarking,140,"Yes, you're of course right. It's not the best default and has been written like that for historic reasons (figuring out the difference and benchmarking vs seurat and vs cell ranger). But simply changing something here messes up everything people have done. I have to think of a quick way of checking whether data is logarithmized or not... Maybe it's enough to simply check whether the data matrix still contains integers - then `log` should be `True`. Otherwise, a warning should be raised if `log` is `True`. Makes sense?. At some point, one might rethink the structure of `filter_genes_dispersion`... one could deprecate it at some point in favor of a new function `extract_highly_variable_genes` or something like this. Or one indeed changes the default behavior and prints a lot of warnings... Not very desirable though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172#issuecomment-398726169
https://github.com/scverse/scanpy/issues/172#issuecomment-398726169:317,Testability,log,logarithmized,317,"Yes, you're of course right. It's not the best default and has been written like that for historic reasons (figuring out the difference and benchmarking vs seurat and vs cell ranger). But simply changing something here messes up everything people have done. I have to think of a quick way of checking whether data is logarithmized or not... Maybe it's enough to simply check whether the data matrix still contains integers - then `log` should be `True`. Otherwise, a warning should be raised if `log` is `True`. Makes sense?. At some point, one might rethink the structure of `filter_genes_dispersion`... one could deprecate it at some point in favor of a new function `extract_highly_variable_genes` or something like this. Or one indeed changes the default behavior and prints a lot of warnings... Not very desirable though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172#issuecomment-398726169
https://github.com/scverse/scanpy/issues/172#issuecomment-398726169:431,Testability,log,log,431,"Yes, you're of course right. It's not the best default and has been written like that for historic reasons (figuring out the difference and benchmarking vs seurat and vs cell ranger). But simply changing something here messes up everything people have done. I have to think of a quick way of checking whether data is logarithmized or not... Maybe it's enough to simply check whether the data matrix still contains integers - then `log` should be `True`. Otherwise, a warning should be raised if `log` is `True`. Makes sense?. At some point, one might rethink the structure of `filter_genes_dispersion`... one could deprecate it at some point in favor of a new function `extract_highly_variable_genes` or something like this. Or one indeed changes the default behavior and prints a lot of warnings... Not very desirable though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172#issuecomment-398726169
https://github.com/scverse/scanpy/issues/172#issuecomment-398726169:496,Testability,log,log,496,"Yes, you're of course right. It's not the best default and has been written like that for historic reasons (figuring out the difference and benchmarking vs seurat and vs cell ranger). But simply changing something here messes up everything people have done. I have to think of a quick way of checking whether data is logarithmized or not... Maybe it's enough to simply check whether the data matrix still contains integers - then `log` should be `True`. Otherwise, a warning should be raised if `log` is `True`. Makes sense?. At some point, one might rethink the structure of `filter_genes_dispersion`... one could deprecate it at some point in favor of a new function `extract_highly_variable_genes` or something like this. Or one indeed changes the default behavior and prints a lot of warnings... Not very desirable though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172#issuecomment-398726169
https://github.com/scverse/scanpy/issues/172#issuecomment-398726169:188,Usability,simpl,simply,188,"Yes, you're of course right. It's not the best default and has been written like that for historic reasons (figuring out the difference and benchmarking vs seurat and vs cell ranger). But simply changing something here messes up everything people have done. I have to think of a quick way of checking whether data is logarithmized or not... Maybe it's enough to simply check whether the data matrix still contains integers - then `log` should be `True`. Otherwise, a warning should be raised if `log` is `True`. Makes sense?. At some point, one might rethink the structure of `filter_genes_dispersion`... one could deprecate it at some point in favor of a new function `extract_highly_variable_genes` or something like this. Or one indeed changes the default behavior and prints a lot of warnings... Not very desirable though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172#issuecomment-398726169
https://github.com/scverse/scanpy/issues/172#issuecomment-398726169:362,Usability,simpl,simply,362,"Yes, you're of course right. It's not the best default and has been written like that for historic reasons (figuring out the difference and benchmarking vs seurat and vs cell ranger). But simply changing something here messes up everything people have done. I have to think of a quick way of checking whether data is logarithmized or not... Maybe it's enough to simply check whether the data matrix still contains integers - then `log` should be `True`. Otherwise, a warning should be raised if `log` is `True`. Makes sense?. At some point, one might rethink the structure of `filter_genes_dispersion`... one could deprecate it at some point in favor of a new function `extract_highly_variable_genes` or something like this. Or one indeed changes the default behavior and prints a lot of warnings... Not very desirable though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172#issuecomment-398726169
https://github.com/scverse/scanpy/issues/172#issuecomment-398761924:269,Testability,log,log-transformation,269,Makes sense that backwards compatibility has to take priority. Integer check will work for most cases (at least it checks if it's still count data). The only exception I can think of is for CPM/size factor normalized data where it would fail (although usually you'd do log-transformation after you normalize otherwise). I can't think of a better method atm either way.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172#issuecomment-398761924
https://github.com/scverse/scanpy/issues/173#issuecomment-398678802:264,Integrability,interface,interface,264,"Sorry for the late response, I was on holidays. I'm happy to merge a pull request for this, if the package appears solid. Would you want to add a file `scanpy/preprocessing/doubletdetection`? We should probably just ask @JonathanShor whether he's interested in an interface for easily accessing his package. If he is, he should also make the pull request, I'd say. :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-398678802
https://github.com/scverse/scanpy/issues/173#issuecomment-398678802:285,Security,access,accessing,285,"Sorry for the late response, I was on holidays. I'm happy to merge a pull request for this, if the package appears solid. Would you want to add a file `scanpy/preprocessing/doubletdetection`? We should probably just ask @JonathanShor whether he's interested in an interface for easily accessing his package. If he is, he should also make the pull request, I'd say. :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-398678802
https://github.com/scverse/scanpy/issues/173#issuecomment-398857838:360,Performance,Perform,Performance,360,"Thanks to @yueqiw for the confidence. :). @falexwolf We have no issue with our package being included here, but we wouldn't be able to create a custom API for your package right now, if that's what you were suggesting?. Given my quick overview of your package, two things you should note:; 1. Our method expects the input count matrix to be from a single run. Performance takes a non-trivial hit on aggregate datasets.; 2. Currently, our runtime will not satisfy those impressive metrics cited in your 1.0 announcement. This may possibly change in the future, as we haven't focused on hard optimization yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-398857838
https://github.com/scverse/scanpy/issues/173#issuecomment-398857838:590,Performance,optimiz,optimization,590,"Thanks to @yueqiw for the confidence. :). @falexwolf We have no issue with our package being included here, but we wouldn't be able to create a custom API for your package right now, if that's what you were suggesting?. Given my quick overview of your package, two things you should note:; 1. Our method expects the input count matrix to be from a single run. Performance takes a non-trivial hit on aggregate datasets.; 2. Currently, our runtime will not satisfy those impressive metrics cited in your 1.0 announcement. This may possibly change in the future, as we haven't focused on hard optimization yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-398857838
https://github.com/scverse/scanpy/issues/173#issuecomment-399367409:532,Availability,reliab,reliably,532,"Hi @JonathanShor,. you don't need to create a custom API. One point of Scanpy is to provide convenient access via `anndata` to many single-cell packages around. The only thing needed for that is to provide a very simple interface like [this](https://github.com/theislab/scanpy/blob/master/scanpy/tools/phate.py#L8-L145) or [this](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/mnn_correct.py#L4-L104) or several of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-399367409
https://github.com/scverse/scanpy/issues/173#issuecomment-399367409:822,Availability,avail,available,822,"Hi @JonathanShor,. you don't need to create a custom API. One point of Scanpy is to provide convenient access via `anndata` to many single-cell packages around. The only thing needed for that is to provide a very simple interface like [this](https://github.com/theislab/scanpy/blob/master/scanpy/tools/phate.py#L8-L145) or [this](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/mnn_correct.py#L4-L104) or several of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-399367409
https://github.com/scverse/scanpy/issues/173#issuecomment-399367409:220,Integrability,interface,interface,220,"Hi @JonathanShor,. you don't need to create a custom API. One point of Scanpy is to provide convenient access via `anndata` to many single-cell packages around. The only thing needed for that is to provide a very simple interface like [this](https://github.com/theislab/scanpy/blob/master/scanpy/tools/phate.py#L8-L145) or [this](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/mnn_correct.py#L4-L104) or several of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-399367409
https://github.com/scverse/scanpy/issues/173#issuecomment-399367409:891,Safety,avoid,avoid,891,"Hi @JonathanShor,. you don't need to create a custom API. One point of Scanpy is to provide convenient access via `anndata` to many single-cell packages around. The only thing needed for that is to provide a very simple interface like [this](https://github.com/theislab/scanpy/blob/master/scanpy/tools/phate.py#L8-L145) or [this](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/mnn_correct.py#L4-L104) or several of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-399367409
https://github.com/scverse/scanpy/issues/173#issuecomment-399367409:1026,Safety,detect,detection,1026,"reate a custom API. One point of Scanpy is to provide convenient access via `anndata` to many single-cell packages around. The only thing needed for that is to provide a very simple interface like [this](https://github.com/theislab/scanpy/blob/master/scanpy/tools/phate.py#L8-L145) or [this](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/mnn_correct.py#L4-L104) or several of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there. We could make a separate page for tha",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-399367409
https://github.com/scverse/scanpy/issues/173#issuecomment-399367409:1527,Safety,detect,detection,1527," of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there. We could make a separate page for that entitled *Cutting Edge Beta Tools* which advertises these tools for people to try out and play around with it. When you have a solid preprint and/or publication or if you think that your tool should go in the main API anyways 😄, we should add your package as `tl.detect_doublets_ONEWORDDESCRIBGINGYOURALGORITHM`... . @flying-sheep @gokceneraslan @fidelram @dawe anyone opinions on such cases?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-399367409
https://github.com/scverse/scanpy/issues/173#issuecomment-399367409:1614,Safety,detect,detecting,1614," of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there. We could make a separate page for that entitled *Cutting Edge Beta Tools* which advertises these tools for people to try out and play around with it. When you have a solid preprint and/or publication or if you think that your tool should go in the main API anyways 😄, we should add your package as `tl.detect_doublets_ONEWORDDESCRIBGINGYOURALGORITHM`... . @flying-sheep @gokceneraslan @fidelram @dawe anyone opinions on such cases?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-399367409
https://github.com/scverse/scanpy/issues/173#issuecomment-399367409:103,Security,access,access,103,"Hi @JonathanShor,. you don't need to create a custom API. One point of Scanpy is to provide convenient access via `anndata` to many single-cell packages around. The only thing needed for that is to provide a very simple interface like [this](https://github.com/theislab/scanpy/blob/master/scanpy/tools/phate.py#L8-L145) or [this](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/mnn_correct.py#L4-L104) or several of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-399367409
https://github.com/scverse/scanpy/issues/173#issuecomment-399367409:213,Usability,simpl,simple,213,"Hi @JonathanShor,. you don't need to create a custom API. One point of Scanpy is to provide convenient access via `anndata` to many single-cell packages around. The only thing needed for that is to provide a very simple interface like [this](https://github.com/theislab/scanpy/blob/master/scanpy/tools/phate.py#L8-L145) or [this](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/mnn_correct.py#L4-L104) or several of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-399367409
https://github.com/scverse/scanpy/issues/173#issuecomment-399367409:455,Usability,Simpl,Simply,455,"Hi @JonathanShor,. you don't need to create a custom API. One point of Scanpy is to provide convenient access via `anndata` to many single-cell packages around. The only thing needed for that is to provide a very simple interface like [this](https://github.com/theislab/scanpy/blob/master/scanpy/tools/phate.py#L8-L145) or [this](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/mnn_correct.py#L4-L104) or several of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-399367409
https://github.com/scverse/scanpy/issues/173#issuecomment-400090424:46,Availability,avail,available,46,"Hi @falexwolf, yes I will be making my method available. A [rough version](https://github.com/swolock/woublet) is already on github, and I also played around with adding it to my [scanpy fork](https://github.com/swolock/scanpy) (though not the right way -- I added it to `tl` rather than `pp`). I'll hopefully clean it up and release something more official when I have the chance.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-400090424
https://github.com/scverse/scanpy/issues/173#issuecomment-400090424:326,Deployability,release,release,326,"Hi @falexwolf, yes I will be making my method available. A [rough version](https://github.com/swolock/woublet) is already on github, and I also played around with adding it to my [scanpy fork](https://github.com/swolock/scanpy) (though not the right way -- I added it to `tl` rather than `pp`). I'll hopefully clean it up and release something more official when I have the chance.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-400090424
https://github.com/scverse/scanpy/issues/173#issuecomment-400277845:270,Availability,down,downstream,270,"Good to hear! Looking forward to learning more about it.; PS: Having a doublet detection tool in `tl` would be fine, I'd say... `pp` and `tl` are just meant to give a rough orientation for users... in some cases, it's not completely clear what *preprocessing* and what *downstream* analysis is...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-400277845
https://github.com/scverse/scanpy/issues/173#issuecomment-400277845:79,Safety,detect,detection,79,"Good to hear! Looking forward to learning more about it.; PS: Having a doublet detection tool in `tl` would be fine, I'd say... `pp` and `tl` are just meant to give a rough orientation for users... in some cases, it's not completely clear what *preprocessing* and what *downstream* analysis is...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-400277845
https://github.com/scverse/scanpy/issues/173#issuecomment-400277845:33,Usability,learn,learning,33,"Good to hear! Looking forward to learning more about it.; PS: Having a doublet detection tool in `tl` would be fine, I'd say... `pp` and `tl` are just meant to give a rough orientation for users... in some cases, it's not completely clear what *preprocessing* and what *downstream* analysis is...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-400277845
https://github.com/scverse/scanpy/issues/173#issuecomment-400277845:233,Usability,clear,clear,233,"Good to hear! Looking forward to learning more about it.; PS: Having a doublet detection tool in `tl` would be fine, I'd say... `pp` and `tl` are just meant to give a rough orientation for users... in some cases, it's not completely clear what *preprocessing* and what *downstream* analysis is...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-400277845
https://github.com/scverse/scanpy/issues/173#issuecomment-481057117:112,Availability,avail,available,112,It looks like this may have stalled a bit. Is anyone currently working on making some form of doublet detection available from scanpy?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-481057117
https://github.com/scverse/scanpy/issues/173#issuecomment-481057117:102,Safety,detect,detection,102,It looks like this may have stalled a bit. Is anyone currently working on making some form of doublet detection available from scanpy?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-481057117
https://github.com/scverse/scanpy/issues/173#issuecomment-492312537:187,Deployability,update,updates,187,"> Hi @ivirshup, I've been meaning to get back to this. I've just started on an AnnData-compatible version of Scrublet which should be easy to hook up to Scanpy. Will keep you posted. Any updates on this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-492312537
https://github.com/scverse/scanpy/issues/173#issuecomment-492392283:83,Integrability,wrap,wrapper,83,"I can apply pretty easily scrublet from the original python package, so I; guess a wrapper should be something fast to implement :) I was thinking; about doing it last week, but I am no expert in this kind of stuff :/. Den tir. 14. maj 2019 kl. 20.18 skrev Alex Wolf <notifications@github.com>:. > I guess, we should ask @swolock <https://github.com/swolock>. 🙂; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/173?email_source=notifications&email_token=ACC66UJZJVVP2VIQXNRHEITPVL7A3A5CNFSM4FE4LIF2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVMK5PQ#issuecomment-492351166>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACC66UN3NQVYPI4KPDUAOK3PVL7A3ANCNFSM4FE4LIFQ>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-492392283
https://github.com/scverse/scanpy/issues/173#issuecomment-492900457:55,Deployability,integrat,integrate,55,"@cartal @SamueleSoraggi ; For some reason I decided to integrate Scrublet using Scanpy's functions where possible, rather than making a simple wrapper. The core functionality is up and running in [this fork](https://github.com/swolock/scanpy), and now I just need to add documentation, make some of the code more Scanpythonic(?), and add an example.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-492900457
https://github.com/scverse/scanpy/issues/173#issuecomment-492900457:55,Integrability,integrat,integrate,55,"@cartal @SamueleSoraggi ; For some reason I decided to integrate Scrublet using Scanpy's functions where possible, rather than making a simple wrapper. The core functionality is up and running in [this fork](https://github.com/swolock/scanpy), and now I just need to add documentation, make some of the code more Scanpythonic(?), and add an example.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-492900457
https://github.com/scverse/scanpy/issues/173#issuecomment-492900457:143,Integrability,wrap,wrapper,143,"@cartal @SamueleSoraggi ; For some reason I decided to integrate Scrublet using Scanpy's functions where possible, rather than making a simple wrapper. The core functionality is up and running in [this fork](https://github.com/swolock/scanpy), and now I just need to add documentation, make some of the code more Scanpythonic(?), and add an example.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-492900457
https://github.com/scverse/scanpy/issues/173#issuecomment-492900457:136,Usability,simpl,simple,136,"@cartal @SamueleSoraggi ; For some reason I decided to integrate Scrublet using Scanpy's functions where possible, rather than making a simple wrapper. The core functionality is up and running in [this fork](https://github.com/swolock/scanpy), and now I just need to add documentation, make some of the code more Scanpythonic(?), and add an example.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-492900457
https://github.com/scverse/scanpy/issues/173#issuecomment-492936700:886,Deployability,integrat,integrate,886,"Your way sounds sure better, many things into the scrublet algorithm are in; redundancy with components of scanpy. It will sure look great :); Just one thing: in the scrublet paper they suggest always to just run the; simulation of doublets and look at the expected vs estimated fraction of; doublets before removing doublets. If those two values do not match, they; say one should rerun scrublet and tune the expected fraction.; Does your script only run simulation of doublets and output the doublets; score, or does it also remove doublets at once? If you do the latter, then; one is not able to simulate doublets more than once to adjust the expected; doublet fraction.; Cheers. Den tor. 16. maj 2019 kl. 05.15 skrev Sam Wolock <notifications@github.com>:. > @cartal <https://github.com/cartal> @SamueleSoraggi; > <https://github.com/SamueleSoraggi>; > For some reason I decided to integrate Scrublet using Scanpy's functions; > where possible, rather than making a simple wrapper. The core functionality; > is up and running in this fork <https://github.com/swolock/scanpy>, and; > now I just need to add documentation, make some of the code more; > Scanpythonic(?), and add an example.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/173?email_source=notifications&email_token=ACC66UNQC744WOUTLRZ2CN3PVTGWTA5CNFSM4FE4LIF2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVQRA2I#issuecomment-492900457>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACC66UI4FF4LES7GRVKHZZDPVTGWTANCNFSM4FE4LIFQ>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-492936700
https://github.com/scverse/scanpy/issues/173#issuecomment-492936700:886,Integrability,integrat,integrate,886,"Your way sounds sure better, many things into the scrublet algorithm are in; redundancy with components of scanpy. It will sure look great :); Just one thing: in the scrublet paper they suggest always to just run the; simulation of doublets and look at the expected vs estimated fraction of; doublets before removing doublets. If those two values do not match, they; say one should rerun scrublet and tune the expected fraction.; Does your script only run simulation of doublets and output the doublets; score, or does it also remove doublets at once? If you do the latter, then; one is not able to simulate doublets more than once to adjust the expected; doublet fraction.; Cheers. Den tor. 16. maj 2019 kl. 05.15 skrev Sam Wolock <notifications@github.com>:. > @cartal <https://github.com/cartal> @SamueleSoraggi; > <https://github.com/SamueleSoraggi>; > For some reason I decided to integrate Scrublet using Scanpy's functions; > where possible, rather than making a simple wrapper. The core functionality; > is up and running in this fork <https://github.com/swolock/scanpy>, and; > now I just need to add documentation, make some of the code more; > Scanpythonic(?), and add an example.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/173?email_source=notifications&email_token=ACC66UNQC744WOUTLRZ2CN3PVTGWTA5CNFSM4FE4LIF2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVQRA2I#issuecomment-492900457>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACC66UI4FF4LES7GRVKHZZDPVTGWTANCNFSM4FE4LIFQ>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-492936700
https://github.com/scverse/scanpy/issues/173#issuecomment-492936700:977,Integrability,wrap,wrapper,977,"Your way sounds sure better, many things into the scrublet algorithm are in; redundancy with components of scanpy. It will sure look great :); Just one thing: in the scrublet paper they suggest always to just run the; simulation of doublets and look at the expected vs estimated fraction of; doublets before removing doublets. If those two values do not match, they; say one should rerun scrublet and tune the expected fraction.; Does your script only run simulation of doublets and output the doublets; score, or does it also remove doublets at once? If you do the latter, then; one is not able to simulate doublets more than once to adjust the expected; doublet fraction.; Cheers. Den tor. 16. maj 2019 kl. 05.15 skrev Sam Wolock <notifications@github.com>:. > @cartal <https://github.com/cartal> @SamueleSoraggi; > <https://github.com/SamueleSoraggi>; > For some reason I decided to integrate Scrublet using Scanpy's functions; > where possible, rather than making a simple wrapper. The core functionality; > is up and running in this fork <https://github.com/swolock/scanpy>, and; > now I just need to add documentation, make some of the code more; > Scanpythonic(?), and add an example.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/173?email_source=notifications&email_token=ACC66UNQC744WOUTLRZ2CN3PVTGWTA5CNFSM4FE4LIF2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVQRA2I#issuecomment-492900457>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACC66UI4FF4LES7GRVKHZZDPVTGWTANCNFSM4FE4LIFQ>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-492936700
https://github.com/scverse/scanpy/issues/173#issuecomment-492936700:401,Performance,tune,tune,401,"Your way sounds sure better, many things into the scrublet algorithm are in; redundancy with components of scanpy. It will sure look great :); Just one thing: in the scrublet paper they suggest always to just run the; simulation of doublets and look at the expected vs estimated fraction of; doublets before removing doublets. If those two values do not match, they; say one should rerun scrublet and tune the expected fraction.; Does your script only run simulation of doublets and output the doublets; score, or does it also remove doublets at once? If you do the latter, then; one is not able to simulate doublets more than once to adjust the expected; doublet fraction.; Cheers. Den tor. 16. maj 2019 kl. 05.15 skrev Sam Wolock <notifications@github.com>:. > @cartal <https://github.com/cartal> @SamueleSoraggi; > <https://github.com/SamueleSoraggi>; > For some reason I decided to integrate Scrublet using Scanpy's functions; > where possible, rather than making a simple wrapper. The core functionality; > is up and running in this fork <https://github.com/swolock/scanpy>, and; > now I just need to add documentation, make some of the code more; > Scanpythonic(?), and add an example.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/173?email_source=notifications&email_token=ACC66UNQC744WOUTLRZ2CN3PVTGWTA5CNFSM4FE4LIF2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVQRA2I#issuecomment-492900457>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACC66UI4FF4LES7GRVKHZZDPVTGWTANCNFSM4FE4LIFQ>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-492936700
https://github.com/scverse/scanpy/issues/173#issuecomment-492936700:77,Safety,redund,redundancy,77,"Your way sounds sure better, many things into the scrublet algorithm are in; redundancy with components of scanpy. It will sure look great :); Just one thing: in the scrublet paper they suggest always to just run the; simulation of doublets and look at the expected vs estimated fraction of; doublets before removing doublets. If those two values do not match, they; say one should rerun scrublet and tune the expected fraction.; Does your script only run simulation of doublets and output the doublets; score, or does it also remove doublets at once? If you do the latter, then; one is not able to simulate doublets more than once to adjust the expected; doublet fraction.; Cheers. Den tor. 16. maj 2019 kl. 05.15 skrev Sam Wolock <notifications@github.com>:. > @cartal <https://github.com/cartal> @SamueleSoraggi; > <https://github.com/SamueleSoraggi>; > For some reason I decided to integrate Scrublet using Scanpy's functions; > where possible, rather than making a simple wrapper. The core functionality; > is up and running in this fork <https://github.com/swolock/scanpy>, and; > now I just need to add documentation, make some of the code more; > Scanpythonic(?), and add an example.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/173?email_source=notifications&email_token=ACC66UNQC744WOUTLRZ2CN3PVTGWTA5CNFSM4FE4LIF2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVQRA2I#issuecomment-492900457>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACC66UI4FF4LES7GRVKHZZDPVTGWTANCNFSM4FE4LIFQ>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-492936700
https://github.com/scverse/scanpy/issues/173#issuecomment-492936700:970,Usability,simpl,simple,970,"Your way sounds sure better, many things into the scrublet algorithm are in; redundancy with components of scanpy. It will sure look great :); Just one thing: in the scrublet paper they suggest always to just run the; simulation of doublets and look at the expected vs estimated fraction of; doublets before removing doublets. If those two values do not match, they; say one should rerun scrublet and tune the expected fraction.; Does your script only run simulation of doublets and output the doublets; score, or does it also remove doublets at once? If you do the latter, then; one is not able to simulate doublets more than once to adjust the expected; doublet fraction.; Cheers. Den tor. 16. maj 2019 kl. 05.15 skrev Sam Wolock <notifications@github.com>:. > @cartal <https://github.com/cartal> @SamueleSoraggi; > <https://github.com/SamueleSoraggi>; > For some reason I decided to integrate Scrublet using Scanpy's functions; > where possible, rather than making a simple wrapper. The core functionality; > is up and running in this fork <https://github.com/swolock/scanpy>, and; > now I just need to add documentation, make some of the code more; > Scanpythonic(?), and add an example.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/173?email_source=notifications&email_token=ACC66UNQC744WOUTLRZ2CN3PVTGWTA5CNFSM4FE4LIF2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVQRA2I#issuecomment-492900457>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACC66UI4FF4LES7GRVKHZZDPVTGWTANCNFSM4FE4LIFQ>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-492936700
https://github.com/scverse/scanpy/issues/173#issuecomment-508722585:43,Testability,test,tested,43,@swolock why don't you submit a PR? I just tested your code and seems to work.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-508722585
https://github.com/scverse/scanpy/issues/173#issuecomment-545010991:37,Deployability,integrat,integrate,37,"How is this work going? We'd love to integrate Scrublet into our workflows, which are currently quite Scanpy-centric.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-545010991
https://github.com/scverse/scanpy/issues/173#issuecomment-545010991:37,Integrability,integrat,integrate,37,"How is this work going? We'd love to integrate Scrublet into our workflows, which are currently quite Scanpy-centric.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-545010991
https://github.com/scverse/scanpy/issues/173#issuecomment-545324439:121,Deployability,integrat,integrated,121,"Thanks @fidelram, that will run the whole Scrublet workflow so will certainly do the trick. But I'd prefer a more Scanpy-integrated approach, which I think I can see how to do from @swolock's fork.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-545324439
https://github.com/scverse/scanpy/issues/173#issuecomment-545324439:121,Integrability,integrat,integrated,121,"Thanks @fidelram, that will run the whole Scrublet workflow so will certainly do the trick. But I'd prefer a more Scanpy-integrated approach, which I think I can see how to do from @swolock's fork.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-545324439
https://github.com/scverse/scanpy/issues/173#issuecomment-1755962194:177,Availability,avail,available,177,"I have problem installing and importing scrublet on windows please can you help me; Here is my code !pip install scrublet; PackagesNotFoundError: The following packages are not available from current channels:. - annoy. Current channels:. - https://conda.anaconda.org/conda-forge/win-64; - https://conda.anaconda.org/conda-forge/noarch; - https://repo.anaconda.com/pkgs/main/win-64; - https://repo.anaconda.com/pkgs/main/noarch; - https://repo.anaconda.com/pkgs/r/win-64; - https://repo.anaconda.com/pkgs/r/noarch; - https://repo.anaconda.com/pkgs/msys2/win-64; - https://repo.anaconda.com/pkgs/msys2/noarch; - https://conda.anaconda.org/pytorch/win-64; - https://conda.anaconda.org/pytorch/noarch. To search for alternate channels that may provide the conda package you're; looking for, navigate to. https://anaconda.org/. and use the search bar at the top of the page.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-1755962194
https://github.com/scverse/scanpy/issues/173#issuecomment-1755962194:15,Deployability,install,installing,15,"I have problem installing and importing scrublet on windows please can you help me; Here is my code !pip install scrublet; PackagesNotFoundError: The following packages are not available from current channels:. - annoy. Current channels:. - https://conda.anaconda.org/conda-forge/win-64; - https://conda.anaconda.org/conda-forge/noarch; - https://repo.anaconda.com/pkgs/main/win-64; - https://repo.anaconda.com/pkgs/main/noarch; - https://repo.anaconda.com/pkgs/r/win-64; - https://repo.anaconda.com/pkgs/r/noarch; - https://repo.anaconda.com/pkgs/msys2/win-64; - https://repo.anaconda.com/pkgs/msys2/noarch; - https://conda.anaconda.org/pytorch/win-64; - https://conda.anaconda.org/pytorch/noarch. To search for alternate channels that may provide the conda package you're; looking for, navigate to. https://anaconda.org/. and use the search bar at the top of the page.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-1755962194
https://github.com/scverse/scanpy/issues/173#issuecomment-1755962194:105,Deployability,install,install,105,"I have problem installing and importing scrublet on windows please can you help me; Here is my code !pip install scrublet; PackagesNotFoundError: The following packages are not available from current channels:. - annoy. Current channels:. - https://conda.anaconda.org/conda-forge/win-64; - https://conda.anaconda.org/conda-forge/noarch; - https://repo.anaconda.com/pkgs/main/win-64; - https://repo.anaconda.com/pkgs/main/noarch; - https://repo.anaconda.com/pkgs/r/win-64; - https://repo.anaconda.com/pkgs/r/noarch; - https://repo.anaconda.com/pkgs/msys2/win-64; - https://repo.anaconda.com/pkgs/msys2/noarch; - https://conda.anaconda.org/pytorch/win-64; - https://conda.anaconda.org/pytorch/noarch. To search for alternate channels that may provide the conda package you're; looking for, navigate to. https://anaconda.org/. and use the search bar at the top of the page.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-1755962194
https://github.com/scverse/scanpy/issues/173#issuecomment-1780971567:94,Safety,safe,safe,94,I started work to move `scrublet` into scanpy (since the last commit was 3 years ago and it’s safe to assume it’s not maintained anymore). https://github.com/scverse/scanpy/pull/2703,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-1780971567
https://github.com/scverse/scanpy/issues/174#issuecomment-398681291:463,Deployability,update,updated,463,"Yes, I also noticed this behavior. I'd say that your `min_dist=0.6` result is essentially the same as the tSNE result. I'd take this result. I'm not sure whether this is fundamentally solvable - optimizing an embedding is a very hard task and UMAP has the best approach to this so far - this is one of the reasons why we came up with PAGA, which is not affected by these problems. PAGA gives you the correct picture of what is connected and what isn't. Note the [updated preprint](https://rawgit.com/falexwolf/paga_paper/master/paga.pdf), which provides an indepth explanation of these issues. Will replace the current bioRxiv preprint or appear in a journal soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/174#issuecomment-398681291
https://github.com/scverse/scanpy/issues/174#issuecomment-398681291:195,Performance,optimiz,optimizing,195,"Yes, I also noticed this behavior. I'd say that your `min_dist=0.6` result is essentially the same as the tSNE result. I'd take this result. I'm not sure whether this is fundamentally solvable - optimizing an embedding is a very hard task and UMAP has the best approach to this so far - this is one of the reasons why we came up with PAGA, which is not affected by these problems. PAGA gives you the correct picture of what is connected and what isn't. Note the [updated preprint](https://rawgit.com/falexwolf/paga_paper/master/paga.pdf), which provides an indepth explanation of these issues. Will replace the current bioRxiv preprint or appear in a journal soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/174#issuecomment-398681291
https://github.com/scverse/scanpy/issues/174#issuecomment-398682339:195,Modifiability,evolve,evolved,195,"Regarding your previous question: Scanpy currently has its own UMAP version as we needed a few more things and wanted to freeze a version. In Scanpy 2, we might change this and use the then more evolved UMAP package instead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/174#issuecomment-398682339
https://github.com/scverse/scanpy/issues/174#issuecomment-399275814:15,Deployability,update,updated,15,Thanks for the updated preprint! It really helps better understand the updated PAGA algorithm.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/174#issuecomment-399275814
https://github.com/scverse/scanpy/issues/174#issuecomment-399275814:71,Deployability,update,updated,71,Thanks for the updated preprint! It really helps better understand the updated PAGA algorithm.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/174#issuecomment-399275814
https://github.com/scverse/scanpy/pull/175#issuecomment-398683241:11,Usability,simpl,simply,11,"Wow! Again simply awesome! :smile:. PS: Sorry for the late response, I was on holidays.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/175#issuecomment-398683241
https://github.com/scverse/scanpy/pull/176#issuecomment-397543101:152,Availability,avail,available,152,"Hi Sidney,. Thanks for the pull request. igraph and louvain are kind of heavy dependencies (e.g. takes long time to compile them and they're not easily available via PyPI for all platforms etc.), this is why they are excluded from requirements file. It's written in the [installation document](https://scanpy.readthedocs.io/en/latest/installation.html) that these need to be installed manually. Also, there should be proper error messages stating that these must be installed separately when their functionality is needed for a function in scanpy and cannot be found. Did you get any other error regarding these packages?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/176#issuecomment-397543101
https://github.com/scverse/scanpy/pull/176#issuecomment-397543101:424,Availability,error,error,424,"Hi Sidney,. Thanks for the pull request. igraph and louvain are kind of heavy dependencies (e.g. takes long time to compile them and they're not easily available via PyPI for all platforms etc.), this is why they are excluded from requirements file. It's written in the [installation document](https://scanpy.readthedocs.io/en/latest/installation.html) that these need to be installed manually. Also, there should be proper error messages stating that these must be installed separately when their functionality is needed for a function in scanpy and cannot be found. Did you get any other error regarding these packages?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/176#issuecomment-397543101
https://github.com/scverse/scanpy/pull/176#issuecomment-397543101:590,Availability,error,error,590,"Hi Sidney,. Thanks for the pull request. igraph and louvain are kind of heavy dependencies (e.g. takes long time to compile them and they're not easily available via PyPI for all platforms etc.), this is why they are excluded from requirements file. It's written in the [installation document](https://scanpy.readthedocs.io/en/latest/installation.html) that these need to be installed manually. Also, there should be proper error messages stating that these must be installed separately when their functionality is needed for a function in scanpy and cannot be found. Did you get any other error regarding these packages?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/176#issuecomment-397543101
https://github.com/scverse/scanpy/pull/176#issuecomment-397543101:271,Deployability,install,installation,271,"Hi Sidney,. Thanks for the pull request. igraph and louvain are kind of heavy dependencies (e.g. takes long time to compile them and they're not easily available via PyPI for all platforms etc.), this is why they are excluded from requirements file. It's written in the [installation document](https://scanpy.readthedocs.io/en/latest/installation.html) that these need to be installed manually. Also, there should be proper error messages stating that these must be installed separately when their functionality is needed for a function in scanpy and cannot be found. Did you get any other error regarding these packages?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/176#issuecomment-397543101
https://github.com/scverse/scanpy/pull/176#issuecomment-397543101:334,Deployability,install,installation,334,"Hi Sidney,. Thanks for the pull request. igraph and louvain are kind of heavy dependencies (e.g. takes long time to compile them and they're not easily available via PyPI for all platforms etc.), this is why they are excluded from requirements file. It's written in the [installation document](https://scanpy.readthedocs.io/en/latest/installation.html) that these need to be installed manually. Also, there should be proper error messages stating that these must be installed separately when their functionality is needed for a function in scanpy and cannot be found. Did you get any other error regarding these packages?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/176#issuecomment-397543101
https://github.com/scverse/scanpy/pull/176#issuecomment-397543101:375,Deployability,install,installed,375,"Hi Sidney,. Thanks for the pull request. igraph and louvain are kind of heavy dependencies (e.g. takes long time to compile them and they're not easily available via PyPI for all platforms etc.), this is why they are excluded from requirements file. It's written in the [installation document](https://scanpy.readthedocs.io/en/latest/installation.html) that these need to be installed manually. Also, there should be proper error messages stating that these must be installed separately when their functionality is needed for a function in scanpy and cannot be found. Did you get any other error regarding these packages?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/176#issuecomment-397543101
https://github.com/scverse/scanpy/pull/176#issuecomment-397543101:466,Deployability,install,installed,466,"Hi Sidney,. Thanks for the pull request. igraph and louvain are kind of heavy dependencies (e.g. takes long time to compile them and they're not easily available via PyPI for all platforms etc.), this is why they are excluded from requirements file. It's written in the [installation document](https://scanpy.readthedocs.io/en/latest/installation.html) that these need to be installed manually. Also, there should be proper error messages stating that these must be installed separately when their functionality is needed for a function in scanpy and cannot be found. Did you get any other error regarding these packages?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/176#issuecomment-397543101
https://github.com/scverse/scanpy/pull/176#issuecomment-397543101:78,Integrability,depend,dependencies,78,"Hi Sidney,. Thanks for the pull request. igraph and louvain are kind of heavy dependencies (e.g. takes long time to compile them and they're not easily available via PyPI for all platforms etc.), this is why they are excluded from requirements file. It's written in the [installation document](https://scanpy.readthedocs.io/en/latest/installation.html) that these need to be installed manually. Also, there should be proper error messages stating that these must be installed separately when their functionality is needed for a function in scanpy and cannot be found. Did you get any other error regarding these packages?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/176#issuecomment-397543101
https://github.com/scverse/scanpy/pull/176#issuecomment-397543101:430,Integrability,message,messages,430,"Hi Sidney,. Thanks for the pull request. igraph and louvain are kind of heavy dependencies (e.g. takes long time to compile them and they're not easily available via PyPI for all platforms etc.), this is why they are excluded from requirements file. It's written in the [installation document](https://scanpy.readthedocs.io/en/latest/installation.html) that these need to be installed manually. Also, there should be proper error messages stating that these must be installed separately when their functionality is needed for a function in scanpy and cannot be found. Did you get any other error regarding these packages?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/176#issuecomment-397543101
https://github.com/scverse/scanpy/pull/176#issuecomment-398656375:246,Deployability,install,install,246,What we could do instead is to add a [extras_require section](http://setuptools.readthedocs.io/en/latest/setuptools.html#declaring-extras-optional-features-with-their-own-dependencies) to the `setup()` call in setup.py. then people could do `pip install scanpy[louvain]` or `pip install scanpy[all]` to get the whole thing.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/176#issuecomment-398656375
https://github.com/scverse/scanpy/pull/176#issuecomment-398656375:279,Deployability,install,install,279,What we could do instead is to add a [extras_require section](http://setuptools.readthedocs.io/en/latest/setuptools.html#declaring-extras-optional-features-with-their-own-dependencies) to the `setup()` call in setup.py. then people could do `pip install scanpy[louvain]` or `pip install scanpy[all]` to get the whole thing.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/176#issuecomment-398656375
https://github.com/scverse/scanpy/pull/176#issuecomment-398656375:171,Integrability,depend,dependencies,171,What we could do instead is to add a [extras_require section](http://setuptools.readthedocs.io/en/latest/setuptools.html#declaring-extras-optional-features-with-their-own-dependencies) to the `setup()` call in setup.py. then people could do `pip install scanpy[louvain]` or `pip install scanpy[all]` to get the whole thing.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/176#issuecomment-398656375
https://github.com/scverse/scanpy/pull/176#issuecomment-398686980:58,Deployability,install,installation,58,"I agree with Phil, but it's not a priority right now. The installation of both igraph and louvain has to be done only once... these packages don't evolve much. So I think it's OK for people to have this little inconvenience as it's only once in the beginning.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/176#issuecomment-398686980
https://github.com/scverse/scanpy/pull/176#issuecomment-398686980:147,Modifiability,evolve,evolve,147,"I agree with Phil, but it's not a priority right now. The installation of both igraph and louvain has to be done only once... these packages don't evolve much. So I think it's OK for people to have this little inconvenience as it's only once in the beginning.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/176#issuecomment-398686980
https://github.com/scverse/scanpy/issues/177#issuecomment-398506015:519,Usability,simpl,simply,519,"Jaccard metric may lead to very different graphs where Louvain communities are not really distinct compared to those in graphs built with euclidean metric. For example:. Graph with euclidean metric and clusters:. ![image](https://user-images.githubusercontent.com/1140359/41617435-c02baf88-7400-11e8-8629-31db4e8e16f1.png). Graph with jaccard metric and clusters (or lack thereof). ![image](https://user-images.githubusercontent.com/1140359/41617406-aff29fb4-7400-11e8-8d44-4219de42a9e9.png). So in this case, there is simply no communities in the Jaccard graph (at least with the default resolution). However, tSNE always uses the euclidean distance to compute a kNN graph from scratch (completely discarding the metric user specifies in sc.pp.neighbors) and the clusters computed on the Jaccard graph are not representative any more due to very different topology of the graph. In your data, something similar is going on, therefore using `sc.tl.draw_graph` would show how Louvain clusters really look like because it uses the kNN graph built with the metric specified by the user. . @falexwolf does it make sense to print a warning in `sc.tl.tsne` if `sc.pp.neighbors` is called with a a metric other than `euclidean` like ""a non-euclidean metric is used to construct the graph, Louvain clusters may not be representative in tSNE"" or would it be too hacky? tSNE is apparently misleading in these cases...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177#issuecomment-398506015
https://github.com/scverse/scanpy/issues/177#issuecomment-398688207:129,Deployability,continuous,continuous,129,Hi Gökcen: makes sense!. Hi Sidney: if I'm not completely mistaken: I don't think that the Jaccard metric makes sense at all for continuous ordinal variables. It would make sense if one had boolean gene expression or something like this... I guess this is the reason why you get a meaningless graph with it. I always only use euclidean distance. All other desired aspects of the metric are engineered in the preprocessing already.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177#issuecomment-398688207
https://github.com/scverse/scanpy/issues/177#issuecomment-398688207:148,Modifiability,variab,variables,148,Hi Gökcen: makes sense!. Hi Sidney: if I'm not completely mistaken: I don't think that the Jaccard metric makes sense at all for continuous ordinal variables. It would make sense if one had boolean gene expression or something like this... I guess this is the reason why you get a meaningless graph with it. I always only use euclidean distance. All other desired aspects of the metric are engineered in the preprocessing already.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177#issuecomment-398688207
https://github.com/scverse/scanpy/issues/177#issuecomment-399263207:568,Availability,down,downsampled,568,"The Jaccard metric, taken from umap, I believe, returns for a pair of vectors x and y the Jaccard distance between their sets of nonzeros. ([Code](https://github.com/theislab/scanpy/blob/7975f0774c0737bccfc312be0f2a81a3922c2185/scanpy/neighbors/umap/distances.py#L156).). In the case that you feed the raw or logged gene expression matrix, this is reasonable: it's the fraction of genes with nonzero expression shared by the two cells. If you compute this on PCA vectors, however, which are essentially all nonzero, you are getting some version of the complete graph, downsampled to k in some random way. This would explain why clustering and embedding based on that graph is garbage. While investigating this, we (me and @sidneymbell) came across some behavior that may or may not be the desired default. If you call `tools._utils.choose_representation` with `use_rep=None, n_pcs=None`, then it will return `X_pca` (all columns) because `X_pca[:,:None] = X_pca`, which will be the top 50 PCs if one is running with defaults. I might have expected this to instead return `X`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177#issuecomment-399263207
https://github.com/scverse/scanpy/issues/177#issuecomment-399263207:309,Testability,log,logged,309,"The Jaccard metric, taken from umap, I believe, returns for a pair of vectors x and y the Jaccard distance between their sets of nonzeros. ([Code](https://github.com/theislab/scanpy/blob/7975f0774c0737bccfc312be0f2a81a3922c2185/scanpy/neighbors/umap/distances.py#L156).). In the case that you feed the raw or logged gene expression matrix, this is reasonable: it's the fraction of genes with nonzero expression shared by the two cells. If you compute this on PCA vectors, however, which are essentially all nonzero, you are getting some version of the complete graph, downsampled to k in some random way. This would explain why clustering and embedding based on that graph is garbage. While investigating this, we (me and @sidneymbell) came across some behavior that may or may not be the desired default. If you call `tools._utils.choose_representation` with `use_rep=None, n_pcs=None`, then it will return `X_pca` (all columns) because `X_pca[:,:None] = X_pca`, which will be the top 50 PCs if one is running with defaults. I might have expected this to instead return `X`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177#issuecomment-399263207
https://github.com/scverse/scanpy/issues/177#issuecomment-399892886:328,Modifiability,variab,variables,328,"Yes, Joshua, thank you... it makes sense that it takes the nonzero overlap - this is what I meant with ""boolean gene expression"". 🙂 And yes, on PCA this does not make sense at all. OK, you dug out the private function `tools._utils.choose_representation`. This returns the PCA representation if the data matrix has more than 50 variables: See the documentation of the **use_rep** argument [here](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.neighbors.html#scanpy.api.pp.neighbors) and the code https://github.com/theislab/scanpy/blob/8e06ff6ecfab892240b58d2206e461685216a926/scanpy/tools/_utils.py#L22-L43. This behavior is intended as it is rarely advisable to compute distances on an uncompressed data matrix with more than 50 dimensions. Don't you think so? If `.X` is already a 100-dimensional compressed latent representation of another model, then, of course, a PCA on top of that could be nonsense - here I'd agree.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177#issuecomment-399892886
https://github.com/scverse/scanpy/issues/177#issuecomment-400070267:63,Usability,clear,clear,63,"Thanks for the clarification, @falexwolf. The documentation is clear, not sure how I missed it. I agree that euclidean distance is well-approximated by PCA (as long as populations are sufficiently large). For other metrics, that may not be the case (and for Jaccard it bails hard), and so maybe a warning would be appropriate in those cases rather than changing the behavior of `choose_representation`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177#issuecomment-400070267
https://github.com/scverse/scanpy/issues/178#issuecomment-399032569:132,Deployability,release,released,132,"Great, it is. what if I want to add multiple obs_keys (such as lovain, cell_type et al.,), and, when will the new version scanpy be released?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/178#issuecomment-399032569
https://github.com/scverse/scanpy/issues/178#issuecomment-399261667:624,Deployability,release,released,624,"I thought about adding that functionality. Probably is useful when the; second obs_key has few categories because the second category subdivides; the first category. A quick hack is to add a new observation that is the; combination of the first and second keys. Eg. For 4 clusters, and 2 cell; types, the new observation would be cluster_1_cell_type_1,; cluster_1_cell_type_2, cluster_2_cell_type_1 etc. On Thu, Jun 21, 2018 at 5:11 AM wangjiawen2013 <notifications@github.com>; wrote:. > Great, it is. what if I want to add multiple obs_keys (such as lovain,; > cell_type et al.,), and, when will the new version scanpy be released?; >; > —; > You are receiving this because you commented.; >; >; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/178#issuecomment-399032569>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1a4oPw4zOxr4mLTDh7__Q5BsB5dUks5t-2NAgaJpZM4Uq06H>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/178#issuecomment-399261667
https://github.com/scverse/scanpy/issues/178#issuecomment-399889641:158,Deployability,release,release,158,"Yes, we should definitely have the possibility of visualizing several observation annotations in the future. But Fidel's function is a very good start. I can release a new subversion of Scanpy anytime if you need it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/178#issuecomment-399889641
https://github.com/scverse/scanpy/issues/179#issuecomment-398095606:160,Modifiability,variab,variables,160,"```restructuredtext; Returns ; ------- ; adata : :class:`~scanpy.api.AnnData` ; Annotated data matrix, where obsevations/cells are named by their ; barcode and variables/genes by gene name. The data matrix is stored in ; `adata.X`, cell names in `adata.obs_names` and gene names in ; `adata.var_names`. The gene IDs are stored in `adata.obs['gene_ids']`. ; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/179#issuecomment-398095606
https://github.com/scverse/scanpy/pull/180#issuecomment-398733707:15,Deployability,update,update,15,Would you also update the installation docs to show people how to call this?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/180#issuecomment-398733707
https://github.com/scverse/scanpy/pull/180#issuecomment-398733707:26,Deployability,install,installation,26,Would you also update the installation docs to show people how to call this?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/180#issuecomment-398733707
https://github.com/scverse/scanpy/issues/181#issuecomment-400238103:775,Testability,Test,Test,775,"I have written a function to do this... I could add it to scanpy. But for the moment, this is it:. ```; def marker_gene_expression(anndata, marker_dict, gene_symbol_key=None, partition_key='louvain_r1'):; """"""A function go get mean z-score expressions of marker genes; # ; # Inputs:; # anndata - An AnnData object containing the data set and a partition; # marker_dict - A dictionary with cell-type markers. The markers should be stores as anndata.var_names or ; # an anndata.var field with the key given by the gene_symbol_key input; # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker ; # genes; # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is; # 'louvain_r1' """""". #Test inputs; if partition_key not in anndata.obs.columns.values:; print('KeyError: The partition key was not found in the passed AnnData object.'); print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'); raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):; print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'); print(' Check that your cell type markers are given in a format that your anndata object knows!'); raise; . if gene_symbol_key:; gene_ids = anndata.var[gene_symbol_key]; else:; gene_ids = anndata.var_names. clusters = anndata.obs[partition_key].cat.categories; n_clust = len(clusters); marker_exp = pd.DataFrame(columns=clusters); marker_exp['cell_type'] = pd.Series({}, dtype='str'); marker_names = []; ; z_scores = sc.pp.scale(anndata, copy=True). i = 0; for group in marker_dict:; # Find the corresponding columns and get their mean expression in the cluster; for gene in marker_dict[group]:; ens_idx = np.in1d(gene_ids, gene) #Note there may be multiple mappings; if np.sum(ens_idx) == 0:; continue; else:; z_scores.obs[ens_idx[0]] = z_scores.X[:,ens_idx].mean(1) #works for both s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/181#issuecomment-400238103
https://github.com/scverse/scanpy/issues/181#issuecomment-534867254:698,Integrability,depend,depending,698,"As a general approach to this kind of problem, I write functions like this:. ```python. def grouped_obs_mean(adata, group_key, layer=None, gene_symbols=None):; if layer is not None:; getX = lambda x: x.layers[layer]; else:; getX = lambda x: x.X; if gene_symbols is not None:; new_idx = adata.var[idx]; else:; new_idx = adata.var_names. grouped = adata.obs.groupby(group_key); out = pd.DataFrame(; np.zeros((adata.shape[1], len(grouped)), dtype=np.float64),; columns=list(grouped.groups.keys()),; index=adata.var_names; ). for group, idx in grouped.indices.items():; X = getX(adata[idx]); out[group] = np.ravel(X.mean(axis=0, dtype=np.float64)); return out; ```. Swapping out the last 8 lines or so depending on what I'm calculating. To use a set of marker genes I'd call it as `grouped_obs_mean(adata[:, marker_genes], ...)`. At some point we might have `groupby` for `AnnData`s, but that'll require figuring out how to be consistent about the returned type.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/181#issuecomment-534867254
https://github.com/scverse/scanpy/issues/181#issuecomment-534867254:202,Modifiability,layers,layers,202,"As a general approach to this kind of problem, I write functions like this:. ```python. def grouped_obs_mean(adata, group_key, layer=None, gene_symbols=None):; if layer is not None:; getX = lambda x: x.layers[layer]; else:; getX = lambda x: x.X; if gene_symbols is not None:; new_idx = adata.var[idx]; else:; new_idx = adata.var_names. grouped = adata.obs.groupby(group_key); out = pd.DataFrame(; np.zeros((adata.shape[1], len(grouped)), dtype=np.float64),; columns=list(grouped.groups.keys()),; index=adata.var_names; ). for group, idx in grouped.indices.items():; X = getX(adata[idx]); out[group] = np.ravel(X.mean(axis=0, dtype=np.float64)); return out; ```. Swapping out the last 8 lines or so depending on what I'm calculating. To use a set of marker genes I'd call it as `grouped_obs_mean(adata[:, marker_genes], ...)`. At some point we might have `groupby` for `AnnData`s, but that'll require figuring out how to be consistent about the returned type.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/181#issuecomment-534867254
https://github.com/scverse/scanpy/issues/181#issuecomment-1871723078:760,Integrability,depend,depending,760,"> As a general approach to this kind of problem, I write functions like this:; > ; > ```python; > def grouped_obs_mean(adata, group_key, layer=None, gene_symbols=None):; > if layer is not None:; > getX = lambda x: x.layers[layer]; > else:; > getX = lambda x: x.X; > if gene_symbols is not None:; > new_idx = adata.var[idx]; > else:; > new_idx = adata.var_names; > ; > grouped = adata.obs.groupby(group_key); > out = pd.DataFrame(; > np.zeros((adata.shape[1], len(grouped)), dtype=np.float64),; > columns=list(grouped.groups.keys()),; > index=adata.var_names; > ); > ; > for group, idx in grouped.indices.items():; > X = getX(adata[idx]); > out[group] = np.ravel(X.mean(axis=0, dtype=np.float64)); > return out; > ```; > ; > Swapping out the last 8 lines or so depending on what I'm calculating. To use a set of marker genes I'd call it as `grouped_obs_mean(adata[:, marker_genes], ...)`.; > ; > At some point we might have `groupby` for `AnnData`s, but that'll require figuring out how to be consistent about the returned type. Thanks. But need to make a tiny amendment to make it work now:; ```python; out[group] = np.ravel(X.mean(axis=0, dtype=np.float64)).tolist(); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/181#issuecomment-1871723078
https://github.com/scverse/scanpy/issues/181#issuecomment-1871723078:216,Modifiability,layers,layers,216,"> As a general approach to this kind of problem, I write functions like this:; > ; > ```python; > def grouped_obs_mean(adata, group_key, layer=None, gene_symbols=None):; > if layer is not None:; > getX = lambda x: x.layers[layer]; > else:; > getX = lambda x: x.X; > if gene_symbols is not None:; > new_idx = adata.var[idx]; > else:; > new_idx = adata.var_names; > ; > grouped = adata.obs.groupby(group_key); > out = pd.DataFrame(; > np.zeros((adata.shape[1], len(grouped)), dtype=np.float64),; > columns=list(grouped.groups.keys()),; > index=adata.var_names; > ); > ; > for group, idx in grouped.indices.items():; > X = getX(adata[idx]); > out[group] = np.ravel(X.mean(axis=0, dtype=np.float64)); > return out; > ```; > ; > Swapping out the last 8 lines or so depending on what I'm calculating. To use a set of marker genes I'd call it as `grouped_obs_mean(adata[:, marker_genes], ...)`.; > ; > At some point we might have `groupby` for `AnnData`s, but that'll require figuring out how to be consistent about the returned type. Thanks. But need to make a tiny amendment to make it work now:; ```python; out[group] = np.ravel(X.mean(axis=0, dtype=np.float64)).tolist(); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/181#issuecomment-1871723078
https://github.com/scverse/scanpy/issues/182#issuecomment-408848214:142,Availability,error,error,142,"I just stumbled upon a similar bug using SciKit Learn. It's not ScanPy, but this issue is the only result Google returned when I looked up my error. Here's my crash log:. ```; Crashed Thread: 0 Dispatch queue: com.apple.main-thread. Exception Type: EXC_BAD_ACCESS (SIGSEGV); Exception Codes: KERN_INVALID_ADDRESS at 0x0000000000000110; Exception Note: EXC_CORPSE_NOTIFY. Termination Signal: Segmentation fault: 11; Termination Reason: Namespace SIGNAL, Code 0xb; Terminating Process: exc handler [0]. VM Regions Near 0x110:; --> ; __TEXT 000000010ddfb000-000000010ddfd000 [ 8K] r-x/rwx SM=COW /usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python. Application Specific Information:; crashed on child side of fork pre-exec. Thread 0 Crashed:: Dispatch queue: com.apple.main-thread; 0 libdispatch.dylib 	0x00007fff4fb578e1 _dispatch_root_queue_push + 108; 1 libBLAS.dylib 	0x00007fff24844c9a rowMajorTranspose + 546; 2 libBLAS.dylib 	0x00007fff24844a65 cblas_dgemv + 757; 3 multiarray.cpython-36m-darwin.so	0x00000001104e3f86 gemv + 182; 4 multiarray.cpython-36m-darwin.so	0x00000001104e3527 cblas_matrixproduct + 2807; 5 multiarray.cpython-36m-darwin.so	0x00000001104a9b27 PyArray_MatrixProduct2 + 215; 6 multiarray.cpython-36m-darwin.so	0x00000001104aeabf array_matrixproduct + 191; 7 org.python.python 	0x000000010de4712e _PyCFunction_FastCallDict + 463; 8 org.python.python 	0x000000010dead0e6 call_function + 491; 9 org.python.python 	0x000000010dea5621 _PyEval_EvalFrameDefault + 1659; 10 org.python.python 	0x000000010dead866 _PyEval_EvalCodeWithName + 1747; ```. It's not very useful as it's the same as the OP's, but it might help shifting the blame to a common dependency of SciKit Learn and ScanPy (like BLAS having an issue with macOS' Grand Central Dispatch).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182#issuecomment-408848214
https://github.com/scverse/scanpy/issues/182#issuecomment-408848214:404,Availability,fault,fault,404,"I just stumbled upon a similar bug using SciKit Learn. It's not ScanPy, but this issue is the only result Google returned when I looked up my error. Here's my crash log:. ```; Crashed Thread: 0 Dispatch queue: com.apple.main-thread. Exception Type: EXC_BAD_ACCESS (SIGSEGV); Exception Codes: KERN_INVALID_ADDRESS at 0x0000000000000110; Exception Note: EXC_CORPSE_NOTIFY. Termination Signal: Segmentation fault: 11; Termination Reason: Namespace SIGNAL, Code 0xb; Terminating Process: exc handler [0]. VM Regions Near 0x110:; --> ; __TEXT 000000010ddfb000-000000010ddfd000 [ 8K] r-x/rwx SM=COW /usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python. Application Specific Information:; crashed on child side of fork pre-exec. Thread 0 Crashed:: Dispatch queue: com.apple.main-thread; 0 libdispatch.dylib 	0x00007fff4fb578e1 _dispatch_root_queue_push + 108; 1 libBLAS.dylib 	0x00007fff24844c9a rowMajorTranspose + 546; 2 libBLAS.dylib 	0x00007fff24844a65 cblas_dgemv + 757; 3 multiarray.cpython-36m-darwin.so	0x00000001104e3f86 gemv + 182; 4 multiarray.cpython-36m-darwin.so	0x00000001104e3527 cblas_matrixproduct + 2807; 5 multiarray.cpython-36m-darwin.so	0x00000001104a9b27 PyArray_MatrixProduct2 + 215; 6 multiarray.cpython-36m-darwin.so	0x00000001104aeabf array_matrixproduct + 191; 7 org.python.python 	0x000000010de4712e _PyCFunction_FastCallDict + 463; 8 org.python.python 	0x000000010dead0e6 call_function + 491; 9 org.python.python 	0x000000010dea5621 _PyEval_EvalFrameDefault + 1659; 10 org.python.python 	0x000000010dead866 _PyEval_EvalCodeWithName + 1747; ```. It's not very useful as it's the same as the OP's, but it might help shifting the blame to a common dependency of SciKit Learn and ScanPy (like BLAS having an issue with macOS' Grand Central Dispatch).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182#issuecomment-408848214
https://github.com/scverse/scanpy/issues/182#issuecomment-408848214:1732,Integrability,depend,dependency,1732,"I just stumbled upon a similar bug using SciKit Learn. It's not ScanPy, but this issue is the only result Google returned when I looked up my error. Here's my crash log:. ```; Crashed Thread: 0 Dispatch queue: com.apple.main-thread. Exception Type: EXC_BAD_ACCESS (SIGSEGV); Exception Codes: KERN_INVALID_ADDRESS at 0x0000000000000110; Exception Note: EXC_CORPSE_NOTIFY. Termination Signal: Segmentation fault: 11; Termination Reason: Namespace SIGNAL, Code 0xb; Terminating Process: exc handler [0]. VM Regions Near 0x110:; --> ; __TEXT 000000010ddfb000-000000010ddfd000 [ 8K] r-x/rwx SM=COW /usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python. Application Specific Information:; crashed on child side of fork pre-exec. Thread 0 Crashed:: Dispatch queue: com.apple.main-thread; 0 libdispatch.dylib 	0x00007fff4fb578e1 _dispatch_root_queue_push + 108; 1 libBLAS.dylib 	0x00007fff24844c9a rowMajorTranspose + 546; 2 libBLAS.dylib 	0x00007fff24844a65 cblas_dgemv + 757; 3 multiarray.cpython-36m-darwin.so	0x00000001104e3f86 gemv + 182; 4 multiarray.cpython-36m-darwin.so	0x00000001104e3527 cblas_matrixproduct + 2807; 5 multiarray.cpython-36m-darwin.so	0x00000001104a9b27 PyArray_MatrixProduct2 + 215; 6 multiarray.cpython-36m-darwin.so	0x00000001104aeabf array_matrixproduct + 191; 7 org.python.python 	0x000000010de4712e _PyCFunction_FastCallDict + 463; 8 org.python.python 	0x000000010dead0e6 call_function + 491; 9 org.python.python 	0x000000010dea5621 _PyEval_EvalFrameDefault + 1659; 10 org.python.python 	0x000000010dead866 _PyEval_EvalCodeWithName + 1747; ```. It's not very useful as it's the same as the OP's, but it might help shifting the blame to a common dependency of SciKit Learn and ScanPy (like BLAS having an issue with macOS' Grand Central Dispatch).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182#issuecomment-408848214
https://github.com/scverse/scanpy/issues/182#issuecomment-408848214:203,Performance,queue,queue,203,"I just stumbled upon a similar bug using SciKit Learn. It's not ScanPy, but this issue is the only result Google returned when I looked up my error. Here's my crash log:. ```; Crashed Thread: 0 Dispatch queue: com.apple.main-thread. Exception Type: EXC_BAD_ACCESS (SIGSEGV); Exception Codes: KERN_INVALID_ADDRESS at 0x0000000000000110; Exception Note: EXC_CORPSE_NOTIFY. Termination Signal: Segmentation fault: 11; Termination Reason: Namespace SIGNAL, Code 0xb; Terminating Process: exc handler [0]. VM Regions Near 0x110:; --> ; __TEXT 000000010ddfb000-000000010ddfd000 [ 8K] r-x/rwx SM=COW /usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python. Application Specific Information:; crashed on child side of fork pre-exec. Thread 0 Crashed:: Dispatch queue: com.apple.main-thread; 0 libdispatch.dylib 	0x00007fff4fb578e1 _dispatch_root_queue_push + 108; 1 libBLAS.dylib 	0x00007fff24844c9a rowMajorTranspose + 546; 2 libBLAS.dylib 	0x00007fff24844a65 cblas_dgemv + 757; 3 multiarray.cpython-36m-darwin.so	0x00000001104e3f86 gemv + 182; 4 multiarray.cpython-36m-darwin.so	0x00000001104e3527 cblas_matrixproduct + 2807; 5 multiarray.cpython-36m-darwin.so	0x00000001104a9b27 PyArray_MatrixProduct2 + 215; 6 multiarray.cpython-36m-darwin.so	0x00000001104aeabf array_matrixproduct + 191; 7 org.python.python 	0x000000010de4712e _PyCFunction_FastCallDict + 463; 8 org.python.python 	0x000000010dead0e6 call_function + 491; 9 org.python.python 	0x000000010dea5621 _PyEval_EvalFrameDefault + 1659; 10 org.python.python 	0x000000010dead866 _PyEval_EvalCodeWithName + 1747; ```. It's not very useful as it's the same as the OP's, but it might help shifting the blame to a common dependency of SciKit Learn and ScanPy (like BLAS having an issue with macOS' Grand Central Dispatch).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182#issuecomment-408848214
https://github.com/scverse/scanpy/issues/182#issuecomment-408848214:814,Performance,queue,queue,814,"I just stumbled upon a similar bug using SciKit Learn. It's not ScanPy, but this issue is the only result Google returned when I looked up my error. Here's my crash log:. ```; Crashed Thread: 0 Dispatch queue: com.apple.main-thread. Exception Type: EXC_BAD_ACCESS (SIGSEGV); Exception Codes: KERN_INVALID_ADDRESS at 0x0000000000000110; Exception Note: EXC_CORPSE_NOTIFY. Termination Signal: Segmentation fault: 11; Termination Reason: Namespace SIGNAL, Code 0xb; Terminating Process: exc handler [0]. VM Regions Near 0x110:; --> ; __TEXT 000000010ddfb000-000000010ddfd000 [ 8K] r-x/rwx SM=COW /usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python. Application Specific Information:; crashed on child side of fork pre-exec. Thread 0 Crashed:: Dispatch queue: com.apple.main-thread; 0 libdispatch.dylib 	0x00007fff4fb578e1 _dispatch_root_queue_push + 108; 1 libBLAS.dylib 	0x00007fff24844c9a rowMajorTranspose + 546; 2 libBLAS.dylib 	0x00007fff24844a65 cblas_dgemv + 757; 3 multiarray.cpython-36m-darwin.so	0x00000001104e3f86 gemv + 182; 4 multiarray.cpython-36m-darwin.so	0x00000001104e3527 cblas_matrixproduct + 2807; 5 multiarray.cpython-36m-darwin.so	0x00000001104a9b27 PyArray_MatrixProduct2 + 215; 6 multiarray.cpython-36m-darwin.so	0x00000001104aeabf array_matrixproduct + 191; 7 org.python.python 	0x000000010de4712e _PyCFunction_FastCallDict + 463; 8 org.python.python 	0x000000010dead0e6 call_function + 491; 9 org.python.python 	0x000000010dea5621 _PyEval_EvalFrameDefault + 1659; 10 org.python.python 	0x000000010dead866 _PyEval_EvalCodeWithName + 1747; ```. It's not very useful as it's the same as the OP's, but it might help shifting the blame to a common dependency of SciKit Learn and ScanPy (like BLAS having an issue with macOS' Grand Central Dispatch).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182#issuecomment-408848214
https://github.com/scverse/scanpy/issues/182#issuecomment-408848214:165,Testability,log,log,165,"I just stumbled upon a similar bug using SciKit Learn. It's not ScanPy, but this issue is the only result Google returned when I looked up my error. Here's my crash log:. ```; Crashed Thread: 0 Dispatch queue: com.apple.main-thread. Exception Type: EXC_BAD_ACCESS (SIGSEGV); Exception Codes: KERN_INVALID_ADDRESS at 0x0000000000000110; Exception Note: EXC_CORPSE_NOTIFY. Termination Signal: Segmentation fault: 11; Termination Reason: Namespace SIGNAL, Code 0xb; Terminating Process: exc handler [0]. VM Regions Near 0x110:; --> ; __TEXT 000000010ddfb000-000000010ddfd000 [ 8K] r-x/rwx SM=COW /usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python. Application Specific Information:; crashed on child side of fork pre-exec. Thread 0 Crashed:: Dispatch queue: com.apple.main-thread; 0 libdispatch.dylib 	0x00007fff4fb578e1 _dispatch_root_queue_push + 108; 1 libBLAS.dylib 	0x00007fff24844c9a rowMajorTranspose + 546; 2 libBLAS.dylib 	0x00007fff24844a65 cblas_dgemv + 757; 3 multiarray.cpython-36m-darwin.so	0x00000001104e3f86 gemv + 182; 4 multiarray.cpython-36m-darwin.so	0x00000001104e3527 cblas_matrixproduct + 2807; 5 multiarray.cpython-36m-darwin.so	0x00000001104a9b27 PyArray_MatrixProduct2 + 215; 6 multiarray.cpython-36m-darwin.so	0x00000001104aeabf array_matrixproduct + 191; 7 org.python.python 	0x000000010de4712e _PyCFunction_FastCallDict + 463; 8 org.python.python 	0x000000010dead0e6 call_function + 491; 9 org.python.python 	0x000000010dea5621 _PyEval_EvalFrameDefault + 1659; 10 org.python.python 	0x000000010dead866 _PyEval_EvalCodeWithName + 1747; ```. It's not very useful as it's the same as the OP's, but it might help shifting the blame to a common dependency of SciKit Learn and ScanPy (like BLAS having an issue with macOS' Grand Central Dispatch).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182#issuecomment-408848214
https://github.com/scverse/scanpy/issues/182#issuecomment-408848214:48,Usability,Learn,Learn,48,"I just stumbled upon a similar bug using SciKit Learn. It's not ScanPy, but this issue is the only result Google returned when I looked up my error. Here's my crash log:. ```; Crashed Thread: 0 Dispatch queue: com.apple.main-thread. Exception Type: EXC_BAD_ACCESS (SIGSEGV); Exception Codes: KERN_INVALID_ADDRESS at 0x0000000000000110; Exception Note: EXC_CORPSE_NOTIFY. Termination Signal: Segmentation fault: 11; Termination Reason: Namespace SIGNAL, Code 0xb; Terminating Process: exc handler [0]. VM Regions Near 0x110:; --> ; __TEXT 000000010ddfb000-000000010ddfd000 [ 8K] r-x/rwx SM=COW /usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python. Application Specific Information:; crashed on child side of fork pre-exec. Thread 0 Crashed:: Dispatch queue: com.apple.main-thread; 0 libdispatch.dylib 	0x00007fff4fb578e1 _dispatch_root_queue_push + 108; 1 libBLAS.dylib 	0x00007fff24844c9a rowMajorTranspose + 546; 2 libBLAS.dylib 	0x00007fff24844a65 cblas_dgemv + 757; 3 multiarray.cpython-36m-darwin.so	0x00000001104e3f86 gemv + 182; 4 multiarray.cpython-36m-darwin.so	0x00000001104e3527 cblas_matrixproduct + 2807; 5 multiarray.cpython-36m-darwin.so	0x00000001104a9b27 PyArray_MatrixProduct2 + 215; 6 multiarray.cpython-36m-darwin.so	0x00000001104aeabf array_matrixproduct + 191; 7 org.python.python 	0x000000010de4712e _PyCFunction_FastCallDict + 463; 8 org.python.python 	0x000000010dead0e6 call_function + 491; 9 org.python.python 	0x000000010dea5621 _PyEval_EvalFrameDefault + 1659; 10 org.python.python 	0x000000010dead866 _PyEval_EvalCodeWithName + 1747; ```. It's not very useful as it's the same as the OP's, but it might help shifting the blame to a common dependency of SciKit Learn and ScanPy (like BLAS having an issue with macOS' Grand Central Dispatch).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182#issuecomment-408848214
https://github.com/scverse/scanpy/issues/182#issuecomment-408848214:1753,Usability,Learn,Learn,1753,"I just stumbled upon a similar bug using SciKit Learn. It's not ScanPy, but this issue is the only result Google returned when I looked up my error. Here's my crash log:. ```; Crashed Thread: 0 Dispatch queue: com.apple.main-thread. Exception Type: EXC_BAD_ACCESS (SIGSEGV); Exception Codes: KERN_INVALID_ADDRESS at 0x0000000000000110; Exception Note: EXC_CORPSE_NOTIFY. Termination Signal: Segmentation fault: 11; Termination Reason: Namespace SIGNAL, Code 0xb; Terminating Process: exc handler [0]. VM Regions Near 0x110:; --> ; __TEXT 000000010ddfb000-000000010ddfd000 [ 8K] r-x/rwx SM=COW /usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python. Application Specific Information:; crashed on child side of fork pre-exec. Thread 0 Crashed:: Dispatch queue: com.apple.main-thread; 0 libdispatch.dylib 	0x00007fff4fb578e1 _dispatch_root_queue_push + 108; 1 libBLAS.dylib 	0x00007fff24844c9a rowMajorTranspose + 546; 2 libBLAS.dylib 	0x00007fff24844a65 cblas_dgemv + 757; 3 multiarray.cpython-36m-darwin.so	0x00000001104e3f86 gemv + 182; 4 multiarray.cpython-36m-darwin.so	0x00000001104e3527 cblas_matrixproduct + 2807; 5 multiarray.cpython-36m-darwin.so	0x00000001104a9b27 PyArray_MatrixProduct2 + 215; 6 multiarray.cpython-36m-darwin.so	0x00000001104aeabf array_matrixproduct + 191; 7 org.python.python 	0x000000010de4712e _PyCFunction_FastCallDict + 463; 8 org.python.python 	0x000000010dead0e6 call_function + 491; 9 org.python.python 	0x000000010dea5621 _PyEval_EvalFrameDefault + 1659; 10 org.python.python 	0x000000010dead866 _PyEval_EvalCodeWithName + 1747; ```. It's not very useful as it's the same as the OP's, but it might help shifting the blame to a common dependency of SciKit Learn and ScanPy (like BLAS having an issue with macOS' Grand Central Dispatch).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182#issuecomment-408848214
https://github.com/scverse/scanpy/issues/182#issuecomment-410620495:128,Deployability,update,updated,128,"Thanks for the information. However, I don't think the problem is the python version but libBLAS. Probably updating python also updated lot of libraries and that solved the problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182#issuecomment-410620495
https://github.com/scverse/scanpy/issues/187#issuecomment-402263798:97,Deployability,release,release,97,"Hi Scott,. sure, I remember! :smile: For some reason, I forgot to mention you personally in the [release notes](http://scanpy.readthedocs.io/en/latest/#version-1-1-may-31-2018), is now fixed. Sorry about that! . You could add MAGIC as a preprocessing similar to DCA in the imputation section: http://scanpy.readthedocs.io/en/latest/api/index.html#preprocessing-pp. In terms of code, I would also adapt the conventions of DCA: https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/dca.py. We had some discussions on how to do this best: https://github.com/theislab/scanpy/issues/142 and https://github.com/theislab/scanpy/pull/186. If you think you have better conventions, happy to adopt these. DCA is also not yet released... Best,; Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/187#issuecomment-402263798
https://github.com/scverse/scanpy/issues/187#issuecomment-402263798:729,Deployability,release,released,729,"Hi Scott,. sure, I remember! :smile: For some reason, I forgot to mention you personally in the [release notes](http://scanpy.readthedocs.io/en/latest/#version-1-1-may-31-2018), is now fixed. Sorry about that! . You could add MAGIC as a preprocessing similar to DCA in the imputation section: http://scanpy.readthedocs.io/en/latest/api/index.html#preprocessing-pp. In terms of code, I would also adapt the conventions of DCA: https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/dca.py. We had some discussions on how to do this best: https://github.com/theislab/scanpy/issues/142 and https://github.com/theislab/scanpy/pull/186. If you think you have better conventions, happy to adopt these. DCA is also not yet released... Best,; Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/187#issuecomment-402263798
https://github.com/scverse/scanpy/issues/187#issuecomment-402263798:396,Energy Efficiency,adapt,adapt,396,"Hi Scott,. sure, I remember! :smile: For some reason, I forgot to mention you personally in the [release notes](http://scanpy.readthedocs.io/en/latest/#version-1-1-may-31-2018), is now fixed. Sorry about that! . You could add MAGIC as a preprocessing similar to DCA in the imputation section: http://scanpy.readthedocs.io/en/latest/api/index.html#preprocessing-pp. In terms of code, I would also adapt the conventions of DCA: https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/dca.py. We had some discussions on how to do this best: https://github.com/theislab/scanpy/issues/142 and https://github.com/theislab/scanpy/pull/186. If you think you have better conventions, happy to adopt these. DCA is also not yet released... Best,; Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/187#issuecomment-402263798
https://github.com/scverse/scanpy/issues/187#issuecomment-402263798:396,Modifiability,adapt,adapt,396,"Hi Scott,. sure, I remember! :smile: For some reason, I forgot to mention you personally in the [release notes](http://scanpy.readthedocs.io/en/latest/#version-1-1-may-31-2018), is now fixed. Sorry about that! . You could add MAGIC as a preprocessing similar to DCA in the imputation section: http://scanpy.readthedocs.io/en/latest/api/index.html#preprocessing-pp. In terms of code, I would also adapt the conventions of DCA: https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/dca.py. We had some discussions on how to do this best: https://github.com/theislab/scanpy/issues/142 and https://github.com/theislab/scanpy/pull/186. If you think you have better conventions, happy to adopt these. DCA is also not yet released... Best,; Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/187#issuecomment-402263798
https://github.com/scverse/scanpy/issues/187#issuecomment-403407217:76,Testability,log,log,76,"@falexwolf ; MAGIC uses root square transformation, not the frequently used log transformation, which causes the incompatibility with batch correction methods, such as CCA and MNN. Is DCA compatible with MNN and CCA ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/187#issuecomment-403407217
https://github.com/scverse/scanpy/issues/187#issuecomment-403501006:189,Testability,log,log,189,"@wangjiawen2013 we recommend using square root transform with MAGIC but it's certainly not incompatible. So long as the inputs have been library size normalized and transformed with any of log, sqrt, arcsinh or some other sublinear transformation, MAGIC will work just fine.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/187#issuecomment-403501006
https://github.com/scverse/scanpy/issues/187#issuecomment-403501395:54,Integrability,message,message,54,"Also I'm surprised to see I never left a note on your message @falexwolf : thanks! I'm working on the API now, will send in a PR when it's done or leave a note here if I think the DCA API could do with some modification.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/187#issuecomment-403501395
https://github.com/scverse/scanpy/issues/188#issuecomment-402090857:100,Testability,log,logarithmized,100,Scanpy precisely reproduces Seurat's output as outlined in the first tutorial. You can also feed in logarithmized data by passing the parameter `log=True`.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/188#issuecomment-402090857
https://github.com/scverse/scanpy/issues/188#issuecomment-402090857:145,Testability,log,log,145,Scanpy precisely reproduces Seurat's output as outlined in the first tutorial. You can also feed in logarithmized data by passing the parameter `log=True`.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/188#issuecomment-402090857
https://github.com/scverse/scanpy/issues/189#issuecomment-403629451:119,Usability,simpl,simply,119,"Dear @wflynny, sorry for the late response. And sorry that I don't feel able to comment on imputation techniques, it's simply something that I don't have a lot of experience with.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189#issuecomment-403629451
https://github.com/scverse/scanpy/issues/189#issuecomment-404781382:18,Testability,benchmark,benchmark,18,"One comprehensive benchmark is [this one](https://ieeexplore.ieee.org/document/8388285/) by Zhang et al (not so up-to-date anymore, though). It'd be nice to establish a ""live"" benchmark repository and compare all methods in a transparent, comprehensive and up-to-date way.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189#issuecomment-404781382
https://github.com/scverse/scanpy/issues/189#issuecomment-404781382:176,Testability,benchmark,benchmark,176,"One comprehensive benchmark is [this one](https://ieeexplore.ieee.org/document/8388285/) by Zhang et al (not so up-to-date anymore, though). It'd be nice to establish a ""live"" benchmark repository and compare all methods in a transparent, comprehensive and up-to-date way.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189#issuecomment-404781382
https://github.com/scverse/scanpy/issues/189#issuecomment-404866769:240,Availability,down,downstream,240,"@gokceneraslan Yes, I agree a transparent benchmark repo would be very valuable. . I'd also like to see a detailed breakdown of the limitations of each method or imputation in general. It seems problematic to me to use imputed data for all downstream analyses, for example sub-clustering or DGE analysis, but I can't find a discussion of those limitations anywhere. I'm a little wary of imputation methods being part of a standard toolkit without sufficient discussion of limitations in the documentation somewhere.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189#issuecomment-404866769
https://github.com/scverse/scanpy/issues/189#issuecomment-404866769:42,Testability,benchmark,benchmark,42,"@gokceneraslan Yes, I agree a transparent benchmark repo would be very valuable. . I'd also like to see a detailed breakdown of the limitations of each method or imputation in general. It seems problematic to me to use imputed data for all downstream analyses, for example sub-clustering or DGE analysis, but I can't find a discussion of those limitations anywhere. I'm a little wary of imputation methods being part of a standard toolkit without sufficient discussion of limitations in the documentation somewhere.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189#issuecomment-404866769
https://github.com/scverse/scanpy/issues/189#issuecomment-405181668:294,Integrability,interface,interfaces,294,"Dear @wflynny. You're completely right, I added to the documentation a note that the whole topic is under debate ([here](http://scanpy.readthedocs.io/en/latest/api/index.html#preprocessing-pp)). Generally, Scanpy aims to enable access to different tools via the same data object and consistent interfaces so that users can conveniently try out different tools. The threshold for including an interface in Scanpy is low and only requires that a preprint/paper together with a solid GitHub repository exist.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189#issuecomment-405181668
https://github.com/scverse/scanpy/issues/189#issuecomment-405181668:392,Integrability,interface,interface,392,"Dear @wflynny. You're completely right, I added to the documentation a note that the whole topic is under debate ([here](http://scanpy.readthedocs.io/en/latest/api/index.html#preprocessing-pp)). Generally, Scanpy aims to enable access to different tools via the same data object and consistent interfaces so that users can conveniently try out different tools. The threshold for including an interface in Scanpy is low and only requires that a preprint/paper together with a solid GitHub repository exist.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189#issuecomment-405181668
https://github.com/scverse/scanpy/issues/189#issuecomment-405181668:228,Security,access,access,228,"Dear @wflynny. You're completely right, I added to the documentation a note that the whole topic is under debate ([here](http://scanpy.readthedocs.io/en/latest/api/index.html#preprocessing-pp)). Generally, Scanpy aims to enable access to different tools via the same data object and consistent interfaces so that users can conveniently try out different tools. The threshold for including an interface in Scanpy is low and only requires that a preprint/paper together with a solid GitHub repository exist.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189#issuecomment-405181668
https://github.com/scverse/scanpy/issues/189#issuecomment-413591251:689,Deployability,continuous,continuous,689,"I think this is an important conversation to have not just for imputation, but also for other analysis methods like visualization and batch effect correction. Every algorithm makes some assumptions and biases, and it is possible to misinterpret for misuse almost any machine learning algorithm. . For example, t-SNE, often used for visualization, is also used as dimensionality reduction for clustering. However, most clustering algorithms assume that global distances in a dataset are relevant. This assumption is broken with t-SNE, as evidenced by the inconsistency of t-SNE embeddings on the same data and inability for t-SNE to capture some global trends in a dataset (especially with continuous data, leading to the popularity of graph-based visualizations). . On top of this, each clustering algorithm makes assumptions that data is in fact distributed in clusters, but this is often not the case in single cell data. I agree that it's important to warn users about the limitations of imputation methods, and make them aware that their decision on which algorithm to run can affect their output. However, it seems to me that this conversation could be much broader in scope. We don't currently have a system for unified benchmarking and standardization of single cell analysis methods, so all approaches should be used with some caution.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189#issuecomment-413591251
https://github.com/scverse/scanpy/issues/189#issuecomment-413591251:1226,Testability,benchmark,benchmarking,1226,"I think this is an important conversation to have not just for imputation, but also for other analysis methods like visualization and batch effect correction. Every algorithm makes some assumptions and biases, and it is possible to misinterpret for misuse almost any machine learning algorithm. . For example, t-SNE, often used for visualization, is also used as dimensionality reduction for clustering. However, most clustering algorithms assume that global distances in a dataset are relevant. This assumption is broken with t-SNE, as evidenced by the inconsistency of t-SNE embeddings on the same data and inability for t-SNE to capture some global trends in a dataset (especially with continuous data, leading to the popularity of graph-based visualizations). . On top of this, each clustering algorithm makes assumptions that data is in fact distributed in clusters, but this is often not the case in single cell data. I agree that it's important to warn users about the limitations of imputation methods, and make them aware that their decision on which algorithm to run can affect their output. However, it seems to me that this conversation could be much broader in scope. We don't currently have a system for unified benchmarking and standardization of single cell analysis methods, so all approaches should be used with some caution.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189#issuecomment-413591251
https://github.com/scverse/scanpy/issues/189#issuecomment-413591251:275,Usability,learn,learning,275,"I think this is an important conversation to have not just for imputation, but also for other analysis methods like visualization and batch effect correction. Every algorithm makes some assumptions and biases, and it is possible to misinterpret for misuse almost any machine learning algorithm. . For example, t-SNE, often used for visualization, is also used as dimensionality reduction for clustering. However, most clustering algorithms assume that global distances in a dataset are relevant. This assumption is broken with t-SNE, as evidenced by the inconsistency of t-SNE embeddings on the same data and inability for t-SNE to capture some global trends in a dataset (especially with continuous data, leading to the popularity of graph-based visualizations). . On top of this, each clustering algorithm makes assumptions that data is in fact distributed in clusters, but this is often not the case in single cell data. I agree that it's important to warn users about the limitations of imputation methods, and make them aware that their decision on which algorithm to run can affect their output. However, it seems to me that this conversation could be much broader in scope. We don't currently have a system for unified benchmarking and standardization of single cell analysis methods, so all approaches should be used with some caution.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189#issuecomment-413591251
https://github.com/scverse/scanpy/issues/189#issuecomment-417692893:1516,Availability,avail,available,1516,"tically. However, I'm wary of abandoning a critical discussion of imputation methods in this space because other portions of the typical workflow have issues as well. Further, I think there are important distinctions to be made between different classes of methodology that are (mis)used in this problem space. I. Methods that are fundamentally flawed by their assumptions or algorithm. These should obviously be avoided.; II. Methods that are fundamentally sound but are not sufficiently validated, e.g. the validation doesn't exist in this problem space, isn't sufficiently comprehensive/relevant, performs poorly against other fundamentally sound methodologies, or has such restrictive assumptions it isn't broadly useful/applicable.; III. Methods that are fundamentally sound in assumption/algorithm and can be used by a competent practitioner but still have the potential to be abused through applying it to data that violate those assumptions. I'd consider t-SNE and a great deal of the clustering algorithms to be in class III for the reasons you said; they're valid, functional tools but can be applied in assumption-violating or quasi-valid ways. I'm pretty sure that scImpute, for example, belongs in class I because its description of dropout and simulated test cases are inappropriate. I'd put MAGIC and several other currently available imputation methods in class II as they've got strong foundations but currently insufficient validation IMO. I'm not trying to pick on MAGIC or any specific imputation method. Instead I'd like to have an open discussion about the benefits, limitations, and relative performance of the various imputation methods available with the goal leading to something like @gokceneraslan suggested. Well, and since you brought it up, batch correction and multimodal integration methods are in definite need of the same open discussion, which I'd be happy to have, and I think they should have the same disclaimer regarding their limitations in the documentation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189#issuecomment-417692893
https://github.com/scverse/scanpy/issues/189#issuecomment-417692893:1837,Availability,avail,available,1837,"tically. However, I'm wary of abandoning a critical discussion of imputation methods in this space because other portions of the typical workflow have issues as well. Further, I think there are important distinctions to be made between different classes of methodology that are (mis)used in this problem space. I. Methods that are fundamentally flawed by their assumptions or algorithm. These should obviously be avoided.; II. Methods that are fundamentally sound but are not sufficiently validated, e.g. the validation doesn't exist in this problem space, isn't sufficiently comprehensive/relevant, performs poorly against other fundamentally sound methodologies, or has such restrictive assumptions it isn't broadly useful/applicable.; III. Methods that are fundamentally sound in assumption/algorithm and can be used by a competent practitioner but still have the potential to be abused through applying it to data that violate those assumptions. I'd consider t-SNE and a great deal of the clustering algorithms to be in class III for the reasons you said; they're valid, functional tools but can be applied in assumption-violating or quasi-valid ways. I'm pretty sure that scImpute, for example, belongs in class I because its description of dropout and simulated test cases are inappropriate. I'd put MAGIC and several other currently available imputation methods in class II as they've got strong foundations but currently insufficient validation IMO. I'm not trying to pick on MAGIC or any specific imputation method. Instead I'd like to have an open discussion about the benefits, limitations, and relative performance of the various imputation methods available with the goal leading to something like @gokceneraslan suggested. Well, and since you brought it up, batch correction and multimodal integration methods are in definite need of the same open discussion, which I'd be happy to have, and I think they should have the same disclaimer regarding their limitations in the documentation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189#issuecomment-417692893
https://github.com/scverse/scanpy/issues/189#issuecomment-417692893:1980,Deployability,integrat,integration,1980,"tically. However, I'm wary of abandoning a critical discussion of imputation methods in this space because other portions of the typical workflow have issues as well. Further, I think there are important distinctions to be made between different classes of methodology that are (mis)used in this problem space. I. Methods that are fundamentally flawed by their assumptions or algorithm. These should obviously be avoided.; II. Methods that are fundamentally sound but are not sufficiently validated, e.g. the validation doesn't exist in this problem space, isn't sufficiently comprehensive/relevant, performs poorly against other fundamentally sound methodologies, or has such restrictive assumptions it isn't broadly useful/applicable.; III. Methods that are fundamentally sound in assumption/algorithm and can be used by a competent practitioner but still have the potential to be abused through applying it to data that violate those assumptions. I'd consider t-SNE and a great deal of the clustering algorithms to be in class III for the reasons you said; they're valid, functional tools but can be applied in assumption-violating or quasi-valid ways. I'm pretty sure that scImpute, for example, belongs in class I because its description of dropout and simulated test cases are inappropriate. I'd put MAGIC and several other currently available imputation methods in class II as they've got strong foundations but currently insufficient validation IMO. I'm not trying to pick on MAGIC or any specific imputation method. Instead I'd like to have an open discussion about the benefits, limitations, and relative performance of the various imputation methods available with the goal leading to something like @gokceneraslan suggested. Well, and since you brought it up, batch correction and multimodal integration methods are in definite need of the same open discussion, which I'd be happy to have, and I think they should have the same disclaimer regarding their limitations in the documentation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189#issuecomment-417692893
https://github.com/scverse/scanpy/issues/189#issuecomment-417692893:1980,Integrability,integrat,integration,1980,"tically. However, I'm wary of abandoning a critical discussion of imputation methods in this space because other portions of the typical workflow have issues as well. Further, I think there are important distinctions to be made between different classes of methodology that are (mis)used in this problem space. I. Methods that are fundamentally flawed by their assumptions or algorithm. These should obviously be avoided.; II. Methods that are fundamentally sound but are not sufficiently validated, e.g. the validation doesn't exist in this problem space, isn't sufficiently comprehensive/relevant, performs poorly against other fundamentally sound methodologies, or has such restrictive assumptions it isn't broadly useful/applicable.; III. Methods that are fundamentally sound in assumption/algorithm and can be used by a competent practitioner but still have the potential to be abused through applying it to data that violate those assumptions. I'd consider t-SNE and a great deal of the clustering algorithms to be in class III for the reasons you said; they're valid, functional tools but can be applied in assumption-violating or quasi-valid ways. I'm pretty sure that scImpute, for example, belongs in class I because its description of dropout and simulated test cases are inappropriate. I'd put MAGIC and several other currently available imputation methods in class II as they've got strong foundations but currently insufficient validation IMO. I'm not trying to pick on MAGIC or any specific imputation method. Instead I'd like to have an open discussion about the benefits, limitations, and relative performance of the various imputation methods available with the goal leading to something like @gokceneraslan suggested. Well, and since you brought it up, batch correction and multimodal integration methods are in definite need of the same open discussion, which I'd be happy to have, and I think they should have the same disclaimer regarding their limitations in the documentation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189#issuecomment-417692893
https://github.com/scverse/scanpy/issues/189#issuecomment-417692893:776,Performance,perform,performs,776,"@dburkhardt Sorry for the late response to this. I agree that the space of single cell 'omics analysis tools is essentially the wild west, where every tool should be viewed critically. However, I'm wary of abandoning a critical discussion of imputation methods in this space because other portions of the typical workflow have issues as well. Further, I think there are important distinctions to be made between different classes of methodology that are (mis)used in this problem space. I. Methods that are fundamentally flawed by their assumptions or algorithm. These should obviously be avoided.; II. Methods that are fundamentally sound but are not sufficiently validated, e.g. the validation doesn't exist in this problem space, isn't sufficiently comprehensive/relevant, performs poorly against other fundamentally sound methodologies, or has such restrictive assumptions it isn't broadly useful/applicable.; III. Methods that are fundamentally sound in assumption/algorithm and can be used by a competent practitioner but still have the potential to be abused through applying it to data that violate those assumptions. I'd consider t-SNE and a great deal of the clustering algorithms to be in class III for the reasons you said; they're valid, functional tools but can be applied in assumption-violating or quasi-valid ways. I'm pretty sure that scImpute, for example, belongs in class I because its description of dropout and simulated test cases are inappropriate. I'd put MAGIC and several other currently available imputation methods in class II as they've got strong foundations but currently insufficient validation IMO. I'm not trying to pick on MAGIC or any specific imputation method. Instead I'd like to have an open discussion about the benefits, limitations, and relative performance of the various imputation methods available with the goal leading to something like @gokceneraslan suggested. Well, and since you brought it up, batch correction and multimodal integration methods a",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189#issuecomment-417692893
https://github.com/scverse/scanpy/issues/189#issuecomment-417692893:1791,Performance,perform,performance,1791,"tically. However, I'm wary of abandoning a critical discussion of imputation methods in this space because other portions of the typical workflow have issues as well. Further, I think there are important distinctions to be made between different classes of methodology that are (mis)used in this problem space. I. Methods that are fundamentally flawed by their assumptions or algorithm. These should obviously be avoided.; II. Methods that are fundamentally sound but are not sufficiently validated, e.g. the validation doesn't exist in this problem space, isn't sufficiently comprehensive/relevant, performs poorly against other fundamentally sound methodologies, or has such restrictive assumptions it isn't broadly useful/applicable.; III. Methods that are fundamentally sound in assumption/algorithm and can be used by a competent practitioner but still have the potential to be abused through applying it to data that violate those assumptions. I'd consider t-SNE and a great deal of the clustering algorithms to be in class III for the reasons you said; they're valid, functional tools but can be applied in assumption-violating or quasi-valid ways. I'm pretty sure that scImpute, for example, belongs in class I because its description of dropout and simulated test cases are inappropriate. I'd put MAGIC and several other currently available imputation methods in class II as they've got strong foundations but currently insufficient validation IMO. I'm not trying to pick on MAGIC or any specific imputation method. Instead I'd like to have an open discussion about the benefits, limitations, and relative performance of the various imputation methods available with the goal leading to something like @gokceneraslan suggested. Well, and since you brought it up, batch correction and multimodal integration methods are in definite need of the same open discussion, which I'd be happy to have, and I think they should have the same disclaimer regarding their limitations in the documentation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189#issuecomment-417692893
https://github.com/scverse/scanpy/issues/189#issuecomment-417692893:589,Safety,avoid,avoided,589,"@dburkhardt Sorry for the late response to this. I agree that the space of single cell 'omics analysis tools is essentially the wild west, where every tool should be viewed critically. However, I'm wary of abandoning a critical discussion of imputation methods in this space because other portions of the typical workflow have issues as well. Further, I think there are important distinctions to be made between different classes of methodology that are (mis)used in this problem space. I. Methods that are fundamentally flawed by their assumptions or algorithm. These should obviously be avoided.; II. Methods that are fundamentally sound but are not sufficiently validated, e.g. the validation doesn't exist in this problem space, isn't sufficiently comprehensive/relevant, performs poorly against other fundamentally sound methodologies, or has such restrictive assumptions it isn't broadly useful/applicable.; III. Methods that are fundamentally sound in assumption/algorithm and can be used by a competent practitioner but still have the potential to be abused through applying it to data that violate those assumptions. I'd consider t-SNE and a great deal of the clustering algorithms to be in class III for the reasons you said; they're valid, functional tools but can be applied in assumption-violating or quasi-valid ways. I'm pretty sure that scImpute, for example, belongs in class I because its description of dropout and simulated test cases are inappropriate. I'd put MAGIC and several other currently available imputation methods in class II as they've got strong foundations but currently insufficient validation IMO. I'm not trying to pick on MAGIC or any specific imputation method. Instead I'd like to have an open discussion about the benefits, limitations, and relative performance of the various imputation methods available with the goal leading to something like @gokceneraslan suggested. Well, and since you brought it up, batch correction and multimodal integration methods a",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189#issuecomment-417692893
https://github.com/scverse/scanpy/issues/189#issuecomment-417692893:665,Security,validat,validated,665,"@dburkhardt Sorry for the late response to this. I agree that the space of single cell 'omics analysis tools is essentially the wild west, where every tool should be viewed critically. However, I'm wary of abandoning a critical discussion of imputation methods in this space because other portions of the typical workflow have issues as well. Further, I think there are important distinctions to be made between different classes of methodology that are (mis)used in this problem space. I. Methods that are fundamentally flawed by their assumptions or algorithm. These should obviously be avoided.; II. Methods that are fundamentally sound but are not sufficiently validated, e.g. the validation doesn't exist in this problem space, isn't sufficiently comprehensive/relevant, performs poorly against other fundamentally sound methodologies, or has such restrictive assumptions it isn't broadly useful/applicable.; III. Methods that are fundamentally sound in assumption/algorithm and can be used by a competent practitioner but still have the potential to be abused through applying it to data that violate those assumptions. I'd consider t-SNE and a great deal of the clustering algorithms to be in class III for the reasons you said; they're valid, functional tools but can be applied in assumption-violating or quasi-valid ways. I'm pretty sure that scImpute, for example, belongs in class I because its description of dropout and simulated test cases are inappropriate. I'd put MAGIC and several other currently available imputation methods in class II as they've got strong foundations but currently insufficient validation IMO. I'm not trying to pick on MAGIC or any specific imputation method. Instead I'd like to have an open discussion about the benefits, limitations, and relative performance of the various imputation methods available with the goal leading to something like @gokceneraslan suggested. Well, and since you brought it up, batch correction and multimodal integration methods a",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189#issuecomment-417692893
https://github.com/scverse/scanpy/issues/189#issuecomment-417692893:685,Security,validat,validation,685,"@dburkhardt Sorry for the late response to this. I agree that the space of single cell 'omics analysis tools is essentially the wild west, where every tool should be viewed critically. However, I'm wary of abandoning a critical discussion of imputation methods in this space because other portions of the typical workflow have issues as well. Further, I think there are important distinctions to be made between different classes of methodology that are (mis)used in this problem space. I. Methods that are fundamentally flawed by their assumptions or algorithm. These should obviously be avoided.; II. Methods that are fundamentally sound but are not sufficiently validated, e.g. the validation doesn't exist in this problem space, isn't sufficiently comprehensive/relevant, performs poorly against other fundamentally sound methodologies, or has such restrictive assumptions it isn't broadly useful/applicable.; III. Methods that are fundamentally sound in assumption/algorithm and can be used by a competent practitioner but still have the potential to be abused through applying it to data that violate those assumptions. I'd consider t-SNE and a great deal of the clustering algorithms to be in class III for the reasons you said; they're valid, functional tools but can be applied in assumption-violating or quasi-valid ways. I'm pretty sure that scImpute, for example, belongs in class I because its description of dropout and simulated test cases are inappropriate. I'd put MAGIC and several other currently available imputation methods in class II as they've got strong foundations but currently insufficient validation IMO. I'm not trying to pick on MAGIC or any specific imputation method. Instead I'd like to have an open discussion about the benefits, limitations, and relative performance of the various imputation methods available with the goal leading to something like @gokceneraslan suggested. Well, and since you brought it up, batch correction and multimodal integration methods a",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189#issuecomment-417692893
https://github.com/scverse/scanpy/issues/189#issuecomment-417692893:1618,Security,validat,validation,1618,"tically. However, I'm wary of abandoning a critical discussion of imputation methods in this space because other portions of the typical workflow have issues as well. Further, I think there are important distinctions to be made between different classes of methodology that are (mis)used in this problem space. I. Methods that are fundamentally flawed by their assumptions or algorithm. These should obviously be avoided.; II. Methods that are fundamentally sound but are not sufficiently validated, e.g. the validation doesn't exist in this problem space, isn't sufficiently comprehensive/relevant, performs poorly against other fundamentally sound methodologies, or has such restrictive assumptions it isn't broadly useful/applicable.; III. Methods that are fundamentally sound in assumption/algorithm and can be used by a competent practitioner but still have the potential to be abused through applying it to data that violate those assumptions. I'd consider t-SNE and a great deal of the clustering algorithms to be in class III for the reasons you said; they're valid, functional tools but can be applied in assumption-violating or quasi-valid ways. I'm pretty sure that scImpute, for example, belongs in class I because its description of dropout and simulated test cases are inappropriate. I'd put MAGIC and several other currently available imputation methods in class II as they've got strong foundations but currently insufficient validation IMO. I'm not trying to pick on MAGIC or any specific imputation method. Instead I'd like to have an open discussion about the benefits, limitations, and relative performance of the various imputation methods available with the goal leading to something like @gokceneraslan suggested. Well, and since you brought it up, batch correction and multimodal integration methods are in definite need of the same open discussion, which I'd be happy to have, and I think they should have the same disclaimer regarding their limitations in the documentation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189#issuecomment-417692893
https://github.com/scverse/scanpy/issues/189#issuecomment-417692893:1444,Testability,test,test,1444,"tically. However, I'm wary of abandoning a critical discussion of imputation methods in this space because other portions of the typical workflow have issues as well. Further, I think there are important distinctions to be made between different classes of methodology that are (mis)used in this problem space. I. Methods that are fundamentally flawed by their assumptions or algorithm. These should obviously be avoided.; II. Methods that are fundamentally sound but are not sufficiently validated, e.g. the validation doesn't exist in this problem space, isn't sufficiently comprehensive/relevant, performs poorly against other fundamentally sound methodologies, or has such restrictive assumptions it isn't broadly useful/applicable.; III. Methods that are fundamentally sound in assumption/algorithm and can be used by a competent practitioner but still have the potential to be abused through applying it to data that violate those assumptions. I'd consider t-SNE and a great deal of the clustering algorithms to be in class III for the reasons you said; they're valid, functional tools but can be applied in assumption-violating or quasi-valid ways. I'm pretty sure that scImpute, for example, belongs in class I because its description of dropout and simulated test cases are inappropriate. I'd put MAGIC and several other currently available imputation methods in class II as they've got strong foundations but currently insufficient validation IMO. I'm not trying to pick on MAGIC or any specific imputation method. Instead I'd like to have an open discussion about the benefits, limitations, and relative performance of the various imputation methods available with the goal leading to something like @gokceneraslan suggested. Well, and since you brought it up, batch correction and multimodal integration methods are in definite need of the same open discussion, which I'd be happy to have, and I think they should have the same disclaimer regarding their limitations in the documentation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189#issuecomment-417692893
https://github.com/scverse/scanpy/pull/191#issuecomment-403163212:232,Deployability,pipeline,pipeline,232,As a side-note: there isn't much harm in always returning the resulting object (rather than `None` when `copy=False`). It doesn't use memory (just another reference) and it allows for a more functional style of writing a processing pipeline.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191#issuecomment-403163212
https://github.com/scverse/scanpy/pull/191#issuecomment-403240196:2327,Availability,redundant,redundant,2327,"for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions.; - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. This is a dangerous source for bugs - this was one of the few instances where I produced more bugs than in C++, where one would always write inplace functions (taking pointers or references) that return `void`. In addition, returning `None` directly tells the user that the typical code for writing pipelines does not have to be redundant: `function(adata)` instead of `adata = function(adata)`. Finally: all of Scanpy is consistently written using these principles and it would cause a lot of trouble both changing it in a simple function and changing it everywhere. Why do you think that _it allows for a more functional style of writing a processing pipeline_?. Hence, I'm sorry that I tend to not merge your pull request as is. Either you restore everything else that was there and solely add the inplace `np.log1p` or I'd do that. :smile:. Have a good Sunday!; Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191#issuecomment-403240196
https://github.com/scverse/scanpy/pull/191#issuecomment-403240196:1247,Deployability,pipeline,pipeline,1247,"nd we've discussed it even for log1p... I don't know why we missed using it... Also, I wouldn't have thought that the speedup would be so dramatic, but of course, already for better memory efficiency we should have done it. I'll go through the rest of the toolkit and see whether there is another such striking omission... Regarding the two other things you changed:; - `chunked` and `chunk_size` are in particular important when running an `AnnData` object in `backed` mode, when it's so large that it doesn't fit into memory. To date, this only works for the two functions that were the bottleneck for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions.; - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. This is a dangerous source for bugs - this was one of the few instances where I produced more bugs than in C++, where one would always write inplace functions (taking pointers or references) that return `void`. In addition, returning `None` directly tells the user that t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191#issuecomment-403240196
https://github.com/scverse/scanpy/pull/191#issuecomment-403240196:1573,Deployability,pipeline,pipeline,1573,"arding the two other things you changed:; - `chunked` and `chunk_size` are in particular important when running an `AnnData` object in `backed` mode, when it's so large that it doesn't fit into memory. To date, this only works for the two functions that were the bottleneck for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions.; - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. This is a dangerous source for bugs - this was one of the few instances where I produced more bugs than in C++, where one would always write inplace functions (taking pointers or references) that return `void`. In addition, returning `None` directly tells the user that the typical code for writing pipelines does not have to be redundant: `function(adata)` instead of `adata = function(adata)`. Finally: all of Scanpy is consistently written using these principles and it would cause a lot of trouble both changing it in a simple function and changing it everywhere. Why do you think that _it al",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191#issuecomment-403240196
https://github.com/scverse/scanpy/pull/191#issuecomment-403240196:2297,Deployability,pipeline,pipelines,2297,"for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions.; - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. This is a dangerous source for bugs - this was one of the few instances where I produced more bugs than in C++, where one would always write inplace functions (taking pointers or references) that return `void`. In addition, returning `None` directly tells the user that the typical code for writing pipelines does not have to be redundant: `function(adata)` instead of `adata = function(adata)`. Finally: all of Scanpy is consistently written using these principles and it would cause a lot of trouble both changing it in a simple function and changing it everywhere. Why do you think that _it allows for a more functional style of writing a processing pipeline_?. Hence, I'm sorry that I tend to not merge your pull request as is. Either you restore everything else that was there and solely add the inplace `np.log1p` or I'd do that. :smile:. Have a good Sunday!; Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191#issuecomment-403240196
https://github.com/scverse/scanpy/pull/191#issuecomment-403240196:1959,Modifiability,variab,variable,1959,"for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions.; - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. This is a dangerous source for bugs - this was one of the few instances where I produced more bugs than in C++, where one would always write inplace functions (taking pointers or references) that return `void`. In addition, returning `None` directly tells the user that the typical code for writing pipelines does not have to be redundant: `function(adata)` instead of `adata = function(adata)`. Finally: all of Scanpy is consistently written using these principles and it would cause a lot of trouble both changing it in a simple function and changing it everywhere. Why do you think that _it allows for a more functional style of writing a processing pipeline_?. Hence, I'm sorry that I tend to not merge your pull request as is. Either you restore everything else that was there and solely add the inplace `np.log1p` or I'd do that. :smile:. Have a good Sunday!; Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191#issuecomment-403240196
https://github.com/scverse/scanpy/pull/191#issuecomment-403240196:857,Performance,bottleneck,bottleneck,857,"Hi James!. Thank you for the remark! And you're right... several repetitions of the following are consistent:; ![image](https://user-images.githubusercontent.com/16916678/42413998-8527ef8e-822c-11e8-9e45-8aa30f5bb9a7.png). I'm aware of numpy's inplace functionality, and we've discussed it even for log1p... I don't know why we missed using it... Also, I wouldn't have thought that the speedup would be so dramatic, but of course, already for better memory efficiency we should have done it. I'll go through the rest of the toolkit and see whether there is another such striking omission... Regarding the two other things you changed:; - `chunked` and `chunk_size` are in particular important when running an `AnnData` object in `backed` mode, when it's so large that it doesn't fit into memory. To date, this only works for the two functions that were the bottleneck for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions.; - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. Thi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191#issuecomment-403240196
https://github.com/scverse/scanpy/pull/191#issuecomment-403240196:1613,Performance,load,load,1613,"arding the two other things you changed:; - `chunked` and `chunk_size` are in particular important when running an `AnnData` object in `backed` mode, when it's so large that it doesn't fit into memory. To date, this only works for the two functions that were the bottleneck for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions.; - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. This is a dangerous source for bugs - this was one of the few instances where I produced more bugs than in C++, where one would always write inplace functions (taking pointers or references) that return `void`. In addition, returning `None` directly tells the user that the typical code for writing pipelines does not have to be redundant: `function(adata)` instead of `adata = function(adata)`. Finally: all of Scanpy is consistently written using these principles and it would cause a lot of trouble both changing it in a simple function and changing it everywhere. Why do you think that _it al",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191#issuecomment-403240196
https://github.com/scverse/scanpy/pull/191#issuecomment-403240196:2327,Safety,redund,redundant,2327,"for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions.; - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. This is a dangerous source for bugs - this was one of the few instances where I produced more bugs than in C++, where one would always write inplace functions (taking pointers or references) that return `void`. In addition, returning `None` directly tells the user that the typical code for writing pipelines does not have to be redundant: `function(adata)` instead of `adata = function(adata)`. Finally: all of Scanpy is consistently written using these principles and it would cause a lot of trouble both changing it in a simple function and changing it everywhere. Why do you think that _it allows for a more functional style of writing a processing pipeline_?. Hence, I'm sorry that I tend to not merge your pull request as is. Either you restore everything else that was there and solely add the inplace `np.log1p` or I'd do that. :smile:. Have a good Sunday!; Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191#issuecomment-403240196
https://github.com/scverse/scanpy/pull/191#issuecomment-403240196:2522,Usability,simpl,simple,2522,"for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions.; - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. This is a dangerous source for bugs - this was one of the few instances where I produced more bugs than in C++, where one would always write inplace functions (taking pointers or references) that return `void`. In addition, returning `None` directly tells the user that the typical code for writing pipelines does not have to be redundant: `function(adata)` instead of `adata = function(adata)`. Finally: all of Scanpy is consistently written using these principles and it would cause a lot of trouble both changing it in a simple function and changing it everywhere. Why do you think that _it allows for a more functional style of writing a processing pipeline_?. Hence, I'm sorry that I tend to not merge your pull request as is. Either you restore everything else that was there and solely add the inplace `np.log1p` or I'd do that. :smile:. Have a good Sunday!; Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191#issuecomment-403240196
https://github.com/scverse/scanpy/pull/191#issuecomment-403242179:898,Integrability,depend,depending,898,"No problem, I'll change it to your preferred style. I don't think it's a problem to add the chunking but I'll need to test it for sparse matrices. Just to clarify, what I meant by ""more functional style"" is something like this:. ```; processed_data = raw_data.log1p().normalize(options...).some_other_method(options...); ```. That is, it allows a [functional programming](https://en.wikipedia.org/wiki/Functional_programming) style. Similar to libraries like `scikit-learn` (e.g. `fit()` returns `self` so you can immediately call another method) or `keras` (see the [functional API guide](https://keras.io/getting-started/functional-api-guide/). But as you say, that might be a dramatic change in coding style for your library. I find it can lead to simpler code but that's a personal preference. The above examples are notable because they allow both functional and declarative styles of coding, depending on the user.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191#issuecomment-403242179
https://github.com/scverse/scanpy/pull/191#issuecomment-403242179:118,Testability,test,test,118,"No problem, I'll change it to your preferred style. I don't think it's a problem to add the chunking but I'll need to test it for sparse matrices. Just to clarify, what I meant by ""more functional style"" is something like this:. ```; processed_data = raw_data.log1p().normalize(options...).some_other_method(options...); ```. That is, it allows a [functional programming](https://en.wikipedia.org/wiki/Functional_programming) style. Similar to libraries like `scikit-learn` (e.g. `fit()` returns `self` so you can immediately call another method) or `keras` (see the [functional API guide](https://keras.io/getting-started/functional-api-guide/). But as you say, that might be a dramatic change in coding style for your library. I find it can lead to simpler code but that's a personal preference. The above examples are notable because they allow both functional and declarative styles of coding, depending on the user.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191#issuecomment-403242179
https://github.com/scverse/scanpy/pull/191#issuecomment-403242179:467,Usability,learn,learn,467,"No problem, I'll change it to your preferred style. I don't think it's a problem to add the chunking but I'll need to test it for sparse matrices. Just to clarify, what I meant by ""more functional style"" is something like this:. ```; processed_data = raw_data.log1p().normalize(options...).some_other_method(options...); ```. That is, it allows a [functional programming](https://en.wikipedia.org/wiki/Functional_programming) style. Similar to libraries like `scikit-learn` (e.g. `fit()` returns `self` so you can immediately call another method) or `keras` (see the [functional API guide](https://keras.io/getting-started/functional-api-guide/). But as you say, that might be a dramatic change in coding style for your library. I find it can lead to simpler code but that's a personal preference. The above examples are notable because they allow both functional and declarative styles of coding, depending on the user.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191#issuecomment-403242179
https://github.com/scverse/scanpy/pull/191#issuecomment-403242179:583,Usability,guid,guide,583,"No problem, I'll change it to your preferred style. I don't think it's a problem to add the chunking but I'll need to test it for sparse matrices. Just to clarify, what I meant by ""more functional style"" is something like this:. ```; processed_data = raw_data.log1p().normalize(options...).some_other_method(options...); ```. That is, it allows a [functional programming](https://en.wikipedia.org/wiki/Functional_programming) style. Similar to libraries like `scikit-learn` (e.g. `fit()` returns `self` so you can immediately call another method) or `keras` (see the [functional API guide](https://keras.io/getting-started/functional-api-guide/). But as you say, that might be a dramatic change in coding style for your library. I find it can lead to simpler code but that's a personal preference. The above examples are notable because they allow both functional and declarative styles of coding, depending on the user.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191#issuecomment-403242179
https://github.com/scverse/scanpy/pull/191#issuecomment-403242179:638,Usability,guid,guide,638,"No problem, I'll change it to your preferred style. I don't think it's a problem to add the chunking but I'll need to test it for sparse matrices. Just to clarify, what I meant by ""more functional style"" is something like this:. ```; processed_data = raw_data.log1p().normalize(options...).some_other_method(options...); ```. That is, it allows a [functional programming](https://en.wikipedia.org/wiki/Functional_programming) style. Similar to libraries like `scikit-learn` (e.g. `fit()` returns `self` so you can immediately call another method) or `keras` (see the [functional API guide](https://keras.io/getting-started/functional-api-guide/). But as you say, that might be a dramatic change in coding style for your library. I find it can lead to simpler code but that's a personal preference. The above examples are notable because they allow both functional and declarative styles of coding, depending on the user.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191#issuecomment-403242179
https://github.com/scverse/scanpy/pull/191#issuecomment-403242179:751,Usability,simpl,simpler,751,"No problem, I'll change it to your preferred style. I don't think it's a problem to add the chunking but I'll need to test it for sparse matrices. Just to clarify, what I meant by ""more functional style"" is something like this:. ```; processed_data = raw_data.log1p().normalize(options...).some_other_method(options...); ```. That is, it allows a [functional programming](https://en.wikipedia.org/wiki/Functional_programming) style. Similar to libraries like `scikit-learn` (e.g. `fit()` returns `self` so you can immediately call another method) or `keras` (see the [functional API guide](https://keras.io/getting-started/functional-api-guide/). But as you say, that might be a dramatic change in coding style for your library. I find it can lead to simpler code but that's a personal preference. The above examples are notable because they allow both functional and declarative styles of coding, depending on the user.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191#issuecomment-403242179
https://github.com/scverse/scanpy/pull/191#issuecomment-403244826:69,Testability,test,tested,69,I modified the code to reintroduce `chunked` but I confess I haven't tested it because I've never been able to get it to work. I think there might be other issues with that functionality...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191#issuecomment-403244826
https://github.com/scverse/scanpy/pull/191#issuecomment-403310077:207,Usability,learn,learn,207,"@jamestwebber Thank you for reintroducing the functionality! :smile:. @Koncopd, could you check whether it still works as you're the one who introduced it in the first place?. Functional programming (scikit-learn): Yes, I assumed you were referring to functional programming. Also, I completely agree that it makes sense that a class method like `.fit(X)` should return `self` in order to be able to further operate on the modified object. But I wouldn't call that _functional programming_ and for a non-class-method, I'd say this is a completely different case. I think we're interpreting the Wikipedia article and all the tutorials on functional vs. object-oriented a bit differently, which is not problematic, of course. 🙂. Functional programming (Scanpy): When thinking about Scanpy's structure in the first place, I thought I'd write it in a largely functional way - where I mean that the central operations are based on functions on some fixed data container, not on objects that contain both data and functionality for the central operations - for two reasons: (1) the data structure was clear from the beginning, it was not terribly complicated but sufficiently complicated to write some non-trival infrastruture (`anndata.AnnData`) (2) the needed functionality of the toolkit was not clear and is not entirely clear to date. Also, a lot of different functionality is needed and putting all of it into one object would require to define a generic god class, which wouldn't be very transparent. Functional programming (Keras): I'd be very astonished if `processed_sequences = TimeDistributed(model)(input_sequences)` modified `input_sequences` - this is taken from the tutorial you linked and summarizes the central operation. Hence, I think that the Keras functional API doesn't use `inplace` functions, as most APIs. Then, of course, there should be a return value. But maybe I'm mistaken. :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191#issuecomment-403310077
https://github.com/scverse/scanpy/pull/191#issuecomment-403310077:1095,Usability,clear,clear,1095,"@jamestwebber Thank you for reintroducing the functionality! :smile:. @Koncopd, could you check whether it still works as you're the one who introduced it in the first place?. Functional programming (scikit-learn): Yes, I assumed you were referring to functional programming. Also, I completely agree that it makes sense that a class method like `.fit(X)` should return `self` in order to be able to further operate on the modified object. But I wouldn't call that _functional programming_ and for a non-class-method, I'd say this is a completely different case. I think we're interpreting the Wikipedia article and all the tutorials on functional vs. object-oriented a bit differently, which is not problematic, of course. 🙂. Functional programming (Scanpy): When thinking about Scanpy's structure in the first place, I thought I'd write it in a largely functional way - where I mean that the central operations are based on functions on some fixed data container, not on objects that contain both data and functionality for the central operations - for two reasons: (1) the data structure was clear from the beginning, it was not terribly complicated but sufficiently complicated to write some non-trival infrastruture (`anndata.AnnData`) (2) the needed functionality of the toolkit was not clear and is not entirely clear to date. Also, a lot of different functionality is needed and putting all of it into one object would require to define a generic god class, which wouldn't be very transparent. Functional programming (Keras): I'd be very astonished if `processed_sequences = TimeDistributed(model)(input_sequences)` modified `input_sequences` - this is taken from the tutorial you linked and summarizes the central operation. Hence, I think that the Keras functional API doesn't use `inplace` functions, as most APIs. Then, of course, there should be a return value. But maybe I'm mistaken. :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191#issuecomment-403310077
https://github.com/scverse/scanpy/pull/191#issuecomment-403310077:1293,Usability,clear,clear,1293,"@jamestwebber Thank you for reintroducing the functionality! :smile:. @Koncopd, could you check whether it still works as you're the one who introduced it in the first place?. Functional programming (scikit-learn): Yes, I assumed you were referring to functional programming. Also, I completely agree that it makes sense that a class method like `.fit(X)` should return `self` in order to be able to further operate on the modified object. But I wouldn't call that _functional programming_ and for a non-class-method, I'd say this is a completely different case. I think we're interpreting the Wikipedia article and all the tutorials on functional vs. object-oriented a bit differently, which is not problematic, of course. 🙂. Functional programming (Scanpy): When thinking about Scanpy's structure in the first place, I thought I'd write it in a largely functional way - where I mean that the central operations are based on functions on some fixed data container, not on objects that contain both data and functionality for the central operations - for two reasons: (1) the data structure was clear from the beginning, it was not terribly complicated but sufficiently complicated to write some non-trival infrastruture (`anndata.AnnData`) (2) the needed functionality of the toolkit was not clear and is not entirely clear to date. Also, a lot of different functionality is needed and putting all of it into one object would require to define a generic god class, which wouldn't be very transparent. Functional programming (Keras): I'd be very astonished if `processed_sequences = TimeDistributed(model)(input_sequences)` modified `input_sequences` - this is taken from the tutorial you linked and summarizes the central operation. Hence, I think that the Keras functional API doesn't use `inplace` functions, as most APIs. Then, of course, there should be a return value. But maybe I'm mistaken. :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191#issuecomment-403310077
https://github.com/scverse/scanpy/pull/191#issuecomment-403310077:1319,Usability,clear,clear,1319,"@jamestwebber Thank you for reintroducing the functionality! :smile:. @Koncopd, could you check whether it still works as you're the one who introduced it in the first place?. Functional programming (scikit-learn): Yes, I assumed you were referring to functional programming. Also, I completely agree that it makes sense that a class method like `.fit(X)` should return `self` in order to be able to further operate on the modified object. But I wouldn't call that _functional programming_ and for a non-class-method, I'd say this is a completely different case. I think we're interpreting the Wikipedia article and all the tutorials on functional vs. object-oriented a bit differently, which is not problematic, of course. 🙂. Functional programming (Scanpy): When thinking about Scanpy's structure in the first place, I thought I'd write it in a largely functional way - where I mean that the central operations are based on functions on some fixed data container, not on objects that contain both data and functionality for the central operations - for two reasons: (1) the data structure was clear from the beginning, it was not terribly complicated but sufficiently complicated to write some non-trival infrastruture (`anndata.AnnData`) (2) the needed functionality of the toolkit was not clear and is not entirely clear to date. Also, a lot of different functionality is needed and putting all of it into one object would require to define a generic god class, which wouldn't be very transparent. Functional programming (Keras): I'd be very astonished if `processed_sequences = TimeDistributed(model)(input_sequences)` modified `input_sequences` - this is taken from the tutorial you linked and summarizes the central operation. Hence, I think that the Keras functional API doesn't use `inplace` functions, as most APIs. Then, of course, there should be a return value. But maybe I'm mistaken. :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191#issuecomment-403310077
https://github.com/scverse/scanpy/pull/191#issuecomment-403311910:159,Usability,learn,learn,159,"I think you're right that Keras doesn't do in-place modifications (it's just building a computation graph there so there is nothing to modify). And the scikit-learn example is a bit different because it's a class method (although it would also make sense for `log1p` to be a class method, given that it only needs to exist for `AnnData` objects). Your rationale for the Scanpy structure makes a lot of sense for the reasons you mentioned, but to me functional programming means immutable objects and functions that return their result with no side effects. Of course, memory constraints come into play here and immutability isn't viable, but the ""return your result"" part is still possible. Personally I don't mind the possibility of multiple references to one object and so I prefer the option of either assigning the result or chaining functions together. In such a library it's easy to switch between an in-place or copying workflow, to inspect intermediate output if desired. This behavior is what `numpy.log1p` itself is doing here, for that matter–with an `out` argument it still returns the array. Anyway, obviously it's your call on how to design Scanpy, but I thought I'd give my perspective. Looking forward to see where you go with it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191#issuecomment-403311910
https://github.com/scverse/scanpy/pull/191#issuecomment-403313076:670,Availability,avail,available,670,"Thank you for all your thoughts! That's very interesting and helpful!. > although it would also make sense for log1p to be a class method, given that it only needs to exist for AnnData objects. Yes! I also think so. But then the question is which function makes into AnnData and which doesn't. Right now we only put functionality that is related to bookkeeping of the data into AnnData. Everything else remains out of it, even it's something as simple as `log1p`... but that's just a safeguard towards cluttering the object... I agree that it would be more convenient to have some of this in `AnnData`. I guess numpy went a similar way: not all of numpy's functions are available as `np.ndarray`'s class methods. > In such a library it's easy to switch between an in-place or copying workflow, to inspect intermediate output if desired. Interesting! I never thought of this. > This behavior is what numpy.log1p itself is doing here, for that matter–with an out argument it still returns the array. Yes! I think that's a good solution. The `out` argument is very verbose and allows setting a second name for the reference to the modified object, which is returned in addition. I thought about making `inplace` the default for Scanpy's function or not for a long time and finally decided for the unorthodox choice of making it the default - having in mind that AnnData's will become pretty large and at some point backed on disk (which hugely limits the possibilities of how you can write pipelines). Then the `out` rationale doesn't work anymore, as, by default, there simply is no second reference around... Again, thank you for your perspective. And, I'll merge this as soon as having figured out the `chunked` issue. Should be tomorrow or so...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191#issuecomment-403313076
https://github.com/scverse/scanpy/pull/191#issuecomment-403313076:1487,Deployability,pipeline,pipelines,1487,"Thank you for all your thoughts! That's very interesting and helpful!. > although it would also make sense for log1p to be a class method, given that it only needs to exist for AnnData objects. Yes! I also think so. But then the question is which function makes into AnnData and which doesn't. Right now we only put functionality that is related to bookkeeping of the data into AnnData. Everything else remains out of it, even it's something as simple as `log1p`... but that's just a safeguard towards cluttering the object... I agree that it would be more convenient to have some of this in `AnnData`. I guess numpy went a similar way: not all of numpy's functions are available as `np.ndarray`'s class methods. > In such a library it's easy to switch between an in-place or copying workflow, to inspect intermediate output if desired. Interesting! I never thought of this. > This behavior is what numpy.log1p itself is doing here, for that matter–with an out argument it still returns the array. Yes! I think that's a good solution. The `out` argument is very verbose and allows setting a second name for the reference to the modified object, which is returned in addition. I thought about making `inplace` the default for Scanpy's function or not for a long time and finally decided for the unorthodox choice of making it the default - having in mind that AnnData's will become pretty large and at some point backed on disk (which hugely limits the possibilities of how you can write pipelines). Then the `out` rationale doesn't work anymore, as, by default, there simply is no second reference around... Again, thank you for your perspective. And, I'll merge this as soon as having figured out the `chunked` issue. Should be tomorrow or so...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191#issuecomment-403313076
https://github.com/scverse/scanpy/pull/191#issuecomment-403313076:484,Safety,safe,safeguard,484,"Thank you for all your thoughts! That's very interesting and helpful!. > although it would also make sense for log1p to be a class method, given that it only needs to exist for AnnData objects. Yes! I also think so. But then the question is which function makes into AnnData and which doesn't. Right now we only put functionality that is related to bookkeeping of the data into AnnData. Everything else remains out of it, even it's something as simple as `log1p`... but that's just a safeguard towards cluttering the object... I agree that it would be more convenient to have some of this in `AnnData`. I guess numpy went a similar way: not all of numpy's functions are available as `np.ndarray`'s class methods. > In such a library it's easy to switch between an in-place or copying workflow, to inspect intermediate output if desired. Interesting! I never thought of this. > This behavior is what numpy.log1p itself is doing here, for that matter–with an out argument it still returns the array. Yes! I think that's a good solution. The `out` argument is very verbose and allows setting a second name for the reference to the modified object, which is returned in addition. I thought about making `inplace` the default for Scanpy's function or not for a long time and finally decided for the unorthodox choice of making it the default - having in mind that AnnData's will become pretty large and at some point backed on disk (which hugely limits the possibilities of how you can write pipelines). Then the `out` rationale doesn't work anymore, as, by default, there simply is no second reference around... Again, thank you for your perspective. And, I'll merge this as soon as having figured out the `chunked` issue. Should be tomorrow or so...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191#issuecomment-403313076
https://github.com/scverse/scanpy/pull/191#issuecomment-403313076:445,Usability,simpl,simple,445,"Thank you for all your thoughts! That's very interesting and helpful!. > although it would also make sense for log1p to be a class method, given that it only needs to exist for AnnData objects. Yes! I also think so. But then the question is which function makes into AnnData and which doesn't. Right now we only put functionality that is related to bookkeeping of the data into AnnData. Everything else remains out of it, even it's something as simple as `log1p`... but that's just a safeguard towards cluttering the object... I agree that it would be more convenient to have some of this in `AnnData`. I guess numpy went a similar way: not all of numpy's functions are available as `np.ndarray`'s class methods. > In such a library it's easy to switch between an in-place or copying workflow, to inspect intermediate output if desired. Interesting! I never thought of this. > This behavior is what numpy.log1p itself is doing here, for that matter–with an out argument it still returns the array. Yes! I think that's a good solution. The `out` argument is very verbose and allows setting a second name for the reference to the modified object, which is returned in addition. I thought about making `inplace` the default for Scanpy's function or not for a long time and finally decided for the unorthodox choice of making it the default - having in mind that AnnData's will become pretty large and at some point backed on disk (which hugely limits the possibilities of how you can write pipelines). Then the `out` rationale doesn't work anymore, as, by default, there simply is no second reference around... Again, thank you for your perspective. And, I'll merge this as soon as having figured out the `chunked` issue. Should be tomorrow or so...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191#issuecomment-403313076
https://github.com/scverse/scanpy/pull/191#issuecomment-403313076:1568,Usability,simpl,simply,1568,"Thank you for all your thoughts! That's very interesting and helpful!. > although it would also make sense for log1p to be a class method, given that it only needs to exist for AnnData objects. Yes! I also think so. But then the question is which function makes into AnnData and which doesn't. Right now we only put functionality that is related to bookkeeping of the data into AnnData. Everything else remains out of it, even it's something as simple as `log1p`... but that's just a safeguard towards cluttering the object... I agree that it would be more convenient to have some of this in `AnnData`. I guess numpy went a similar way: not all of numpy's functions are available as `np.ndarray`'s class methods. > In such a library it's easy to switch between an in-place or copying workflow, to inspect intermediate output if desired. Interesting! I never thought of this. > This behavior is what numpy.log1p itself is doing here, for that matter–with an out argument it still returns the array. Yes! I think that's a good solution. The `out` argument is very verbose and allows setting a second name for the reference to the modified object, which is returned in addition. I thought about making `inplace` the default for Scanpy's function or not for a long time and finally decided for the unorthodox choice of making it the default - having in mind that AnnData's will become pretty large and at some point backed on disk (which hugely limits the possibilities of how you can write pipelines). Then the `out` rationale doesn't work anymore, as, by default, there simply is no second reference around... Again, thank you for your perspective. And, I'll merge this as soon as having figured out the `chunked` issue. Should be tomorrow or so...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191#issuecomment-403313076
https://github.com/scverse/scanpy/pull/191#issuecomment-403512265:67,Testability,benchmark,benchmarks,67,Seems it works properly; https://github.com/Koncopd/anndata-scanpy-benchmarks/blob/master/log1p_test.ipynb,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191#issuecomment-403512265
https://github.com/scverse/scanpy/pull/191#issuecomment-404142288:26,Testability,test,test,26,@Koncopd I made an actual test from your notebook :smile:: https://github.com/theislab/scanpy/commit/8d4ec6376c5b338456ced0bc051683052f15aa37,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191#issuecomment-404142288
https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:3924,Availability,error,error,3924,"ggeration factor or the learning rate; might be too high. If the cost function gets stuck in a bad local; minimum increasing the learning rate helps sometimes. **random_state** : `int` or `None`, optional (default: 0). Change this to use different intial states for the optimization. If `None`,; the initial state is not reproducible. **use_fast_tsne** : `bool`, optional (default: `True`). Use the MulticoreTSNE package by D. Ulyanov if it is installed. **n_jobs** : `int` or `None` (default: `sc.settings.n_jobs`). Number of jobs. **copy** : `bool` (default: `False`). Return a copy instead of writing to adata. :Returns:. Depending on `copy`, returns or updates `adata` with the following fields. . **X_tsne** : `np.ndarray` (`adata.obs`, dtype `float`); ```. Now let's look at `pp.neighbors` where you're reading the type annotations from the signature.; - Obviously, the signature itself now is a mess for humans to read. But ok, that's fine if the docstring is easy to read.; - There is an error ` <class 'inspect._empty'>`; - The rest looks good to me, except for the superficial stylistic remarks above.; ```; Signature: sc.pp.neighbors(adata:anndata.base.AnnData, n_neighbors:int=15, n_pcs:Union[int, NoneType]=None, use_rep:Union[str, NoneType]=None, knn:bool=True, random_state:Union[int, mtrand.RandomState, NoneType]=0, method:str='umap', metric:Union[str, Callable[[numpy.ndarray, numpy.ndarray], float]]='euclidean', metric_kwds:Mapping[str, Any]={}, copy:bool=False) -> Union[anndata.base.AnnData, NoneType]; Docstring:; Compute a neighborhood graph of observations [McInnes18]_. The neighbor search efficiency of this heavily relies on UMAP [McInnes18]_,; which also provides a method for estimating connectivities of data points -; the connectivity of the manifold (`method=='umap'`). If `method=='diffmap'`,; connectivities are computed according to [Coifman05]_, in the adaption of; [Haghverdi16]_. :Parameters:. **adata** : AnnData, optional (default: <class 'inspect._empty'>). ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999
https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:7690,Availability,error,errors,7690,". **random_state** : typing.Union[int, mtrand.RandomState, NoneType]. A numpy random seed. **method** : {'umap', 'gauss', `None`} (default: `'umap'`). Use 'umap' [McInnes18]_ or 'gauss' (Gauss kernel following [Coifman05]_; with adaptive width [Haghverdi16]_) for computing connectivities. **metric** : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], float]], optional (default: 'euclidean'). A known metric’s name or a callable that returns a distance. **metric_kwds** : Mapping. Options for the metric. **copy** : bool. Return a copy instead of writing to adata. :Returns:. Depending on `copy`, updates or returns `adata` with the following:. . **connectivities** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Weighted adjacency matrix of the neighborhood graph of data; points. Weights should be interpreted as connectivities. **distances** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Instead of decaying weights, this stores distances for each pair of; neighbors.; File: ~/_hholtz/01_projects/1512_scanpy/scanpy/scanpy/neighbors/__init__.py; Type: function; ```. PS: ; - Already the [docs](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.Neighbors.compute_neighbors.html) show that `Neighbors.compute_neighbors` has invalid numpydoc... this was the case in several instances and I'm slowly fixing all of them... It's just a matter of adding `\` at the line breaks.; - I completely agree that the redundency between signature and docstring information lead to a a very small number of errors in the docstrings. However, in several instances, I'm setting the default value in the signature to `None`. But in the docstring, I'm giving the value to which this `None` evaluates in the default case (depending on what is passed)... There is quite a number of such cases. Clearly, one could replace all of them with `'auto'` parameters, which is probably the better way of doing this. As the whole thing is backwards compat, this is not an immediate problem",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999
https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:1195,Deployability,install,install,1195,"ghts the variable names enough already. Also, Jupyter notebooks don't even interpret them.; - Why don't we stick with the underlined sections? `:Parameters:` is a lot less pretty than the underlined counterpart.; - Why do we indent? Jupyter's typical help box is very narrow and the output really gets more squashed. Also, there seem to be a lot of unnecessary newlines. Pasting `tl.tsne` here looks somewhat acceptable (though not nice). But invoking it in a Jupyter notebook doesn't look nice...; ```; Signature: sc.tl.tsne(adata, n_pcs=None, use_rep=None, perplexity=30, early_exaggeration=12, learning_rate=1000, random_state=0, use_fast_tsne=True, n_jobs=None, copy=False); Docstring:; t-SNE [Maaten08]_ [Amir13]_ [Pedregosa11]_. t-distributed stochastic neighborhood embedding (tSNE) [Maaten08]_ has been; proposed for visualizating single-cell data by [Amir13]_. Here, by default,; we use the implementation of *scikit-learn* [Pedregosa11]_. You can achieve; a huge speedup and better convergence if you install `Multicore-tSNE; <https://github.com/DmitryUlyanov/Multicore-TSNE>`__ by [Ulyanov16]_, which; will be automatically detected by Scanpy. :Parameters:. **adata** : :class:`~anndata.AnnData`. Annotated data matrix. **n_pcs** : `int` or `None`, optional (default: `None`). Use this many PCs. If `n_pcs==0` use `.X` if `use_rep is None`. **use_rep** : \{`None`, 'X'\} or any key for `.obsm`, optional (default: `None`). Use the indicated representation. If `None`, the representation is chosen; automatically: for `.n_vars` < 50, `.X` is used, otherwise 'X_pca' is used.; If 'X_pca' is not present, it's computed with default parameters. **perplexity** : `float`, optional (default: 30). The perplexity is related to the number of nearest neighbors that; is used in other manifold learning algorithms. Larger datasets; usually require a larger perplexity. Consider selecting a value; between 5 and 50. The choice is not extremely critical since t-SNE; is quite insensitive to this param",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999
https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:3372,Deployability,install,installed,3372,"n them. For larger; values, the space between natural clusters will be larger in the; embedded space. Again, the choice of this parameter is not very; critical. If the cost function increases during initial optimization,; the early exaggeration factor or the learning rate might be too high. **learning_rate** : `float`, optional (default: 1000). Note that the R-package ""Rtsne"" uses a default of 200.; The learning rate can be a critical parameter. It should be; between 100 and 1000. If the cost function increases during initial; optimization, the early exaggeration factor or the learning rate; might be too high. If the cost function gets stuck in a bad local; minimum increasing the learning rate helps sometimes. **random_state** : `int` or `None`, optional (default: 0). Change this to use different intial states for the optimization. If `None`,; the initial state is not reproducible. **use_fast_tsne** : `bool`, optional (default: `True`). Use the MulticoreTSNE package by D. Ulyanov if it is installed. **n_jobs** : `int` or `None` (default: `sc.settings.n_jobs`). Number of jobs. **copy** : `bool` (default: `False`). Return a copy instead of writing to adata. :Returns:. Depending on `copy`, returns or updates `adata` with the following fields. . **X_tsne** : `np.ndarray` (`adata.obs`, dtype `float`); ```. Now let's look at `pp.neighbors` where you're reading the type annotations from the signature.; - Obviously, the signature itself now is a mess for humans to read. But ok, that's fine if the docstring is easy to read.; - There is an error ` <class 'inspect._empty'>`; - The rest looks good to me, except for the superficial stylistic remarks above.; ```; Signature: sc.pp.neighbors(adata:anndata.base.AnnData, n_neighbors:int=15, n_pcs:Union[int, NoneType]=None, use_rep:Union[str, NoneType]=None, knn:bool=True, random_state:Union[int, mtrand.RandomState, NoneType]=0, method:str='umap', metric:Union[str, Callable[[numpy.ndarray, numpy.ndarray], float]]='euclidean', metric_k",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999
https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:3585,Deployability,update,updates,3585," the early exaggeration factor or the learning rate might be too high. **learning_rate** : `float`, optional (default: 1000). Note that the R-package ""Rtsne"" uses a default of 200.; The learning rate can be a critical parameter. It should be; between 100 and 1000. If the cost function increases during initial; optimization, the early exaggeration factor or the learning rate; might be too high. If the cost function gets stuck in a bad local; minimum increasing the learning rate helps sometimes. **random_state** : `int` or `None`, optional (default: 0). Change this to use different intial states for the optimization. If `None`,; the initial state is not reproducible. **use_fast_tsne** : `bool`, optional (default: `True`). Use the MulticoreTSNE package by D. Ulyanov if it is installed. **n_jobs** : `int` or `None` (default: `sc.settings.n_jobs`). Number of jobs. **copy** : `bool` (default: `False`). Return a copy instead of writing to adata. :Returns:. Depending on `copy`, returns or updates `adata` with the following fields. . **X_tsne** : `np.ndarray` (`adata.obs`, dtype `float`); ```. Now let's look at `pp.neighbors` where you're reading the type annotations from the signature.; - Obviously, the signature itself now is a mess for humans to read. But ok, that's fine if the docstring is easy to read.; - There is an error ` <class 'inspect._empty'>`; - The rest looks good to me, except for the superficial stylistic remarks above.; ```; Signature: sc.pp.neighbors(adata:anndata.base.AnnData, n_neighbors:int=15, n_pcs:Union[int, NoneType]=None, use_rep:Union[str, NoneType]=None, knn:bool=True, random_state:Union[int, mtrand.RandomState, NoneType]=0, method:str='umap', metric:Union[str, Callable[[numpy.ndarray, numpy.ndarray], float]]='euclidean', metric_kwds:Mapping[str, Any]={}, copy:bool=False) -> Union[anndata.base.AnnData, NoneType]; Docstring:; Compute a neighborhood graph of observations [McInnes18]_. The neighbor search efficiency of this heavily relies on UMAP [McI",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999
https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:6771,Deployability,update,updates,6771,"erwise 'X_pca' is used.; If 'X_pca' is not present, it's computed with default parameters. **knn** : bool, optional (default: True). If `True`, use a hard threshold to restrict the number of neighbors to; `n_neighbors`, that is, consider a knn graph. Otherwise, use a Gaussian; Kernel to assign low weights to neighbors more distant than the; `n_neighbors` nearest neighbor. **random_state** : typing.Union[int, mtrand.RandomState, NoneType]. A numpy random seed. **method** : {'umap', 'gauss', `None`} (default: `'umap'`). Use 'umap' [McInnes18]_ or 'gauss' (Gauss kernel following [Coifman05]_; with adaptive width [Haghverdi16]_) for computing connectivities. **metric** : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], float]], optional (default: 'euclidean'). A known metric’s name or a callable that returns a distance. **metric_kwds** : Mapping. Options for the metric. **copy** : bool. Return a copy instead of writing to adata. :Returns:. Depending on `copy`, updates or returns `adata` with the following:. . **connectivities** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Weighted adjacency matrix of the neighborhood graph of data; points. Weights should be interpreted as connectivities. **distances** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Instead of decaying weights, this stores distances for each pair of; neighbors.; File: ~/_hholtz/01_projects/1512_scanpy/scanpy/scanpy/neighbors/__init__.py; Type: function; ```. PS: ; - Already the [docs](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.Neighbors.compute_neighbors.html) show that `Neighbors.compute_neighbors` has invalid numpydoc... this was the case in several instances and I'm slowly fixing all of them... It's just a matter of adding `\` at the line breaks.; - I completely agree that the redundency between signature and docstring information lead to a a very small number of errors in the docstrings. However, in several instances, I'm setting the default value in the ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999
https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:4818,Energy Efficiency,adapt,adaption,4818," Obviously, the signature itself now is a mess for humans to read. But ok, that's fine if the docstring is easy to read.; - There is an error ` <class 'inspect._empty'>`; - The rest looks good to me, except for the superficial stylistic remarks above.; ```; Signature: sc.pp.neighbors(adata:anndata.base.AnnData, n_neighbors:int=15, n_pcs:Union[int, NoneType]=None, use_rep:Union[str, NoneType]=None, knn:bool=True, random_state:Union[int, mtrand.RandomState, NoneType]=0, method:str='umap', metric:Union[str, Callable[[numpy.ndarray, numpy.ndarray], float]]='euclidean', metric_kwds:Mapping[str, Any]={}, copy:bool=False) -> Union[anndata.base.AnnData, NoneType]; Docstring:; Compute a neighborhood graph of observations [McInnes18]_. The neighbor search efficiency of this heavily relies on UMAP [McInnes18]_,; which also provides a method for estimating connectivities of data points -; the connectivity of the manifold (`method=='umap'`). If `method=='diffmap'`,; connectivities are computed according to [Coifman05]_, in the adaption of; [Haghverdi16]_. :Parameters:. **adata** : AnnData, optional (default: <class 'inspect._empty'>). Annotated data matrix. **n_neighbors** : int, optional (default: 15). The size of local neighborhood (in terms of number of neighboring data; points) used for manifold approximation. Larger values result in more; global views of the manifold, while smaller values result in more local; data being preserved. In general values should be in the range 2 to 100.; If `knn` is `True`, number of nearest neighbors to be searched. If `knn`; is `False`, a Gaussian kernel width is set to the distance of the; `n_neighbors` neighbor. **n_pcs** : `int` or `None`, optional (default: `None`). Use this many PCs. If `n_pcs==0` use `.X` if `use_rep is None`. **use_rep** : \{`None`, 'X'\} or any key for `.obsm`, optional (default: `None`). Use the indicated representation. If `None`, the representation is chosen; automatically: for `.n_vars` < 50, `.X` is used, otherwis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999
https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:6385,Energy Efficiency,adapt,adaptive,6385," Gaussian kernel width is set to the distance of the; `n_neighbors` neighbor. **n_pcs** : `int` or `None`, optional (default: `None`). Use this many PCs. If `n_pcs==0` use `.X` if `use_rep is None`. **use_rep** : \{`None`, 'X'\} or any key for `.obsm`, optional (default: `None`). Use the indicated representation. If `None`, the representation is chosen; automatically: for `.n_vars` < 50, `.X` is used, otherwise 'X_pca' is used.; If 'X_pca' is not present, it's computed with default parameters. **knn** : bool, optional (default: True). If `True`, use a hard threshold to restrict the number of neighbors to; `n_neighbors`, that is, consider a knn graph. Otherwise, use a Gaussian; Kernel to assign low weights to neighbors more distant than the; `n_neighbors` nearest neighbor. **random_state** : typing.Union[int, mtrand.RandomState, NoneType]. A numpy random seed. **method** : {'umap', 'gauss', `None`} (default: `'umap'`). Use 'umap' [McInnes18]_ or 'gauss' (Gauss kernel following [Coifman05]_; with adaptive width [Haghverdi16]_) for computing connectivities. **metric** : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], float]], optional (default: 'euclidean'). A known metric’s name or a callable that returns a distance. **metric_kwds** : Mapping. Options for the metric. **copy** : bool. Return a copy instead of writing to adata. :Returns:. Depending on `copy`, updates or returns `adata` with the following:. . **connectivities** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Weighted adjacency matrix of the neighborhood graph of data; points. Weights should be interpreted as connectivities. **distances** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Instead of decaying weights, this stores distances for each pair of; neighbors.; File: ~/_hholtz/01_projects/1512_scanpy/scanpy/scanpy/neighbors/__init__.py; Type: function; ```. PS: ; - Already the [docs](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.Neighbors.compute_neighbors.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999
https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:3553,Integrability,Depend,Depending,3553," the early exaggeration factor or the learning rate might be too high. **learning_rate** : `float`, optional (default: 1000). Note that the R-package ""Rtsne"" uses a default of 200.; The learning rate can be a critical parameter. It should be; between 100 and 1000. If the cost function increases during initial; optimization, the early exaggeration factor or the learning rate; might be too high. If the cost function gets stuck in a bad local; minimum increasing the learning rate helps sometimes. **random_state** : `int` or `None`, optional (default: 0). Change this to use different intial states for the optimization. If `None`,; the initial state is not reproducible. **use_fast_tsne** : `bool`, optional (default: `True`). Use the MulticoreTSNE package by D. Ulyanov if it is installed. **n_jobs** : `int` or `None` (default: `sc.settings.n_jobs`). Number of jobs. **copy** : `bool` (default: `False`). Return a copy instead of writing to adata. :Returns:. Depending on `copy`, returns or updates `adata` with the following fields. . **X_tsne** : `np.ndarray` (`adata.obs`, dtype `float`); ```. Now let's look at `pp.neighbors` where you're reading the type annotations from the signature.; - Obviously, the signature itself now is a mess for humans to read. But ok, that's fine if the docstring is easy to read.; - There is an error ` <class 'inspect._empty'>`; - The rest looks good to me, except for the superficial stylistic remarks above.; ```; Signature: sc.pp.neighbors(adata:anndata.base.AnnData, n_neighbors:int=15, n_pcs:Union[int, NoneType]=None, use_rep:Union[str, NoneType]=None, knn:bool=True, random_state:Union[int, mtrand.RandomState, NoneType]=0, method:str='umap', metric:Union[str, Callable[[numpy.ndarray, numpy.ndarray], float]]='euclidean', metric_kwds:Mapping[str, Any]={}, copy:bool=False) -> Union[anndata.base.AnnData, NoneType]; Docstring:; Compute a neighborhood graph of observations [McInnes18]_. The neighbor search efficiency of this heavily relies on UMAP [McI",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999
https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:6750,Integrability,Depend,Depending,6750,"erwise 'X_pca' is used.; If 'X_pca' is not present, it's computed with default parameters. **knn** : bool, optional (default: True). If `True`, use a hard threshold to restrict the number of neighbors to; `n_neighbors`, that is, consider a knn graph. Otherwise, use a Gaussian; Kernel to assign low weights to neighbors more distant than the; `n_neighbors` nearest neighbor. **random_state** : typing.Union[int, mtrand.RandomState, NoneType]. A numpy random seed. **method** : {'umap', 'gauss', `None`} (default: `'umap'`). Use 'umap' [McInnes18]_ or 'gauss' (Gauss kernel following [Coifman05]_; with adaptive width [Haghverdi16]_) for computing connectivities. **metric** : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], float]], optional (default: 'euclidean'). A known metric’s name or a callable that returns a distance. **metric_kwds** : Mapping. Options for the metric. **copy** : bool. Return a copy instead of writing to adata. :Returns:. Depending on `copy`, updates or returns `adata` with the following:. . **connectivities** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Weighted adjacency matrix of the neighborhood graph of data; points. Weights should be interpreted as connectivities. **distances** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Instead of decaying weights, this stores distances for each pair of; neighbors.; File: ~/_hholtz/01_projects/1512_scanpy/scanpy/scanpy/neighbors/__init__.py; Type: function; ```. PS: ; - Already the [docs](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.Neighbors.compute_neighbors.html) show that `Neighbors.compute_neighbors` has invalid numpydoc... this was the case in several instances and I'm slowly fixing all of them... It's just a matter of adding `\` at the line breaks.; - I completely agree that the redundency between signature and docstring information lead to a a very small number of errors in the docstrings. However, in several instances, I'm setting the default value in the ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999
https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:7900,Integrability,depend,depending,7900,". **random_state** : typing.Union[int, mtrand.RandomState, NoneType]. A numpy random seed. **method** : {'umap', 'gauss', `None`} (default: `'umap'`). Use 'umap' [McInnes18]_ or 'gauss' (Gauss kernel following [Coifman05]_; with adaptive width [Haghverdi16]_) for computing connectivities. **metric** : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], float]], optional (default: 'euclidean'). A known metric’s name or a callable that returns a distance. **metric_kwds** : Mapping. Options for the metric. **copy** : bool. Return a copy instead of writing to adata. :Returns:. Depending on `copy`, updates or returns `adata` with the following:. . **connectivities** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Weighted adjacency matrix of the neighborhood graph of data; points. Weights should be interpreted as connectivities. **distances** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Instead of decaying weights, this stores distances for each pair of; neighbors.; File: ~/_hholtz/01_projects/1512_scanpy/scanpy/scanpy/neighbors/__init__.py; Type: function; ```. PS: ; - Already the [docs](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.Neighbors.compute_neighbors.html) show that `Neighbors.compute_neighbors` has invalid numpydoc... this was the case in several instances and I'm slowly fixing all of them... It's just a matter of adding `\` at the line breaks.; - I completely agree that the redundency between signature and docstring information lead to a a very small number of errors in the docstrings. However, in several instances, I'm setting the default value in the signature to `None`. But in the docstring, I'm giving the value to which this `None` evaluates in the default case (depending on what is passed)... There is quite a number of such cases. Clearly, one could replace all of them with `'auto'` parameters, which is probably the better way of doing this. As the whole thing is backwards compat, this is not an immediate problem",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999
https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:193,Modifiability,variab,variable,193,"Hey Phil!. Nice, I played around with it! :smile:. I think it's really cool and I only have some minor stylistic remarks. - Why do we want the `**`? The layout and the indenting highlights the variable names enough already. Also, Jupyter notebooks don't even interpret them.; - Why don't we stick with the underlined sections? `:Parameters:` is a lot less pretty than the underlined counterpart.; - Why do we indent? Jupyter's typical help box is very narrow and the output really gets more squashed. Also, there seem to be a lot of unnecessary newlines. Pasting `tl.tsne` here looks somewhat acceptable (though not nice). But invoking it in a Jupyter notebook doesn't look nice...; ```; Signature: sc.tl.tsne(adata, n_pcs=None, use_rep=None, perplexity=30, early_exaggeration=12, learning_rate=1000, random_state=0, use_fast_tsne=True, n_jobs=None, copy=False); Docstring:; t-SNE [Maaten08]_ [Amir13]_ [Pedregosa11]_. t-distributed stochastic neighborhood embedding (tSNE) [Maaten08]_ has been; proposed for visualizating single-cell data by [Amir13]_. Here, by default,; we use the implementation of *scikit-learn* [Pedregosa11]_. You can achieve; a huge speedup and better convergence if you install `Multicore-tSNE; <https://github.com/DmitryUlyanov/Multicore-TSNE>`__ by [Ulyanov16]_, which; will be automatically detected by Scanpy. :Parameters:. **adata** : :class:`~anndata.AnnData`. Annotated data matrix. **n_pcs** : `int` or `None`, optional (default: `None`). Use this many PCs. If `n_pcs==0` use `.X` if `use_rep is None`. **use_rep** : \{`None`, 'X'\} or any key for `.obsm`, optional (default: `None`). Use the indicated representation. If `None`, the representation is chosen; automatically: for `.n_vars` < 50, `.X` is used, otherwise 'X_pca' is used.; If 'X_pca' is not present, it's computed with default parameters. **perplexity** : `float`, optional (default: 30). The perplexity is related to the number of nearest neighbors that; is used in other manifold learning algorithms. L",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999
https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:4818,Modifiability,adapt,adaption,4818," Obviously, the signature itself now is a mess for humans to read. But ok, that's fine if the docstring is easy to read.; - There is an error ` <class 'inspect._empty'>`; - The rest looks good to me, except for the superficial stylistic remarks above.; ```; Signature: sc.pp.neighbors(adata:anndata.base.AnnData, n_neighbors:int=15, n_pcs:Union[int, NoneType]=None, use_rep:Union[str, NoneType]=None, knn:bool=True, random_state:Union[int, mtrand.RandomState, NoneType]=0, method:str='umap', metric:Union[str, Callable[[numpy.ndarray, numpy.ndarray], float]]='euclidean', metric_kwds:Mapping[str, Any]={}, copy:bool=False) -> Union[anndata.base.AnnData, NoneType]; Docstring:; Compute a neighborhood graph of observations [McInnes18]_. The neighbor search efficiency of this heavily relies on UMAP [McInnes18]_,; which also provides a method for estimating connectivities of data points -; the connectivity of the manifold (`method=='umap'`). If `method=='diffmap'`,; connectivities are computed according to [Coifman05]_, in the adaption of; [Haghverdi16]_. :Parameters:. **adata** : AnnData, optional (default: <class 'inspect._empty'>). Annotated data matrix. **n_neighbors** : int, optional (default: 15). The size of local neighborhood (in terms of number of neighboring data; points) used for manifold approximation. Larger values result in more; global views of the manifold, while smaller values result in more local; data being preserved. In general values should be in the range 2 to 100.; If `knn` is `True`, number of nearest neighbors to be searched. If `knn`; is `False`, a Gaussian kernel width is set to the distance of the; `n_neighbors` neighbor. **n_pcs** : `int` or `None`, optional (default: `None`). Use this many PCs. If `n_pcs==0` use `.X` if `use_rep is None`. **use_rep** : \{`None`, 'X'\} or any key for `.obsm`, optional (default: `None`). Use the indicated representation. If `None`, the representation is chosen; automatically: for `.n_vars` < 50, `.X` is used, otherwis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999
https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:6385,Modifiability,adapt,adaptive,6385," Gaussian kernel width is set to the distance of the; `n_neighbors` neighbor. **n_pcs** : `int` or `None`, optional (default: `None`). Use this many PCs. If `n_pcs==0` use `.X` if `use_rep is None`. **use_rep** : \{`None`, 'X'\} or any key for `.obsm`, optional (default: `None`). Use the indicated representation. If `None`, the representation is chosen; automatically: for `.n_vars` < 50, `.X` is used, otherwise 'X_pca' is used.; If 'X_pca' is not present, it's computed with default parameters. **knn** : bool, optional (default: True). If `True`, use a hard threshold to restrict the number of neighbors to; `n_neighbors`, that is, consider a knn graph. Otherwise, use a Gaussian; Kernel to assign low weights to neighbors more distant than the; `n_neighbors` nearest neighbor. **random_state** : typing.Union[int, mtrand.RandomState, NoneType]. A numpy random seed. **method** : {'umap', 'gauss', `None`} (default: `'umap'`). Use 'umap' [McInnes18]_ or 'gauss' (Gauss kernel following [Coifman05]_; with adaptive width [Haghverdi16]_) for computing connectivities. **metric** : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], float]], optional (default: 'euclidean'). A known metric’s name or a callable that returns a distance. **metric_kwds** : Mapping. Options for the metric. **copy** : bool. Return a copy instead of writing to adata. :Returns:. Depending on `copy`, updates or returns `adata` with the following:. . **connectivities** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Weighted adjacency matrix of the neighborhood graph of data; points. Weights should be interpreted as connectivities. **distances** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Instead of decaying weights, this stores distances for each pair of; neighbors.; File: ~/_hholtz/01_projects/1512_scanpy/scanpy/scanpy/neighbors/__init__.py; Type: function; ```. PS: ; - Already the [docs](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.Neighbors.compute_neighbors.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999
https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:2575,Performance,optimiz,optimization,2575,"ional (default: `None`). Use the indicated representation. If `None`, the representation is chosen; automatically: for `.n_vars` < 50, `.X` is used, otherwise 'X_pca' is used.; If 'X_pca' is not present, it's computed with default parameters. **perplexity** : `float`, optional (default: 30). The perplexity is related to the number of nearest neighbors that; is used in other manifold learning algorithms. Larger datasets; usually require a larger perplexity. Consider selecting a value; between 5 and 50. The choice is not extremely critical since t-SNE; is quite insensitive to this parameter. **early_exaggeration** : `float`, optional (default: 12.0). Controls how tight natural clusters in the original space are in the; embedded space and how much space will be between them. For larger; values, the space between natural clusters will be larger in the; embedded space. Again, the choice of this parameter is not very; critical. If the cost function increases during initial optimization,; the early exaggeration factor or the learning rate might be too high. **learning_rate** : `float`, optional (default: 1000). Note that the R-package ""Rtsne"" uses a default of 200.; The learning rate can be a critical parameter. It should be; between 100 and 1000. If the cost function increases during initial; optimization, the early exaggeration factor or the learning rate; might be too high. If the cost function gets stuck in a bad local; minimum increasing the learning rate helps sometimes. **random_state** : `int` or `None`, optional (default: 0). Change this to use different intial states for the optimization. If `None`,; the initial state is not reproducible. **use_fast_tsne** : `bool`, optional (default: `True`). Use the MulticoreTSNE package by D. Ulyanov if it is installed. **n_jobs** : `int` or `None` (default: `sc.settings.n_jobs`). Number of jobs. **copy** : `bool` (default: `False`). Return a copy instead of writing to adata. :Returns:. Depending on `copy`, returns or updates `",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999
https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:2901,Performance,optimiz,optimization,2901,"number of nearest neighbors that; is used in other manifold learning algorithms. Larger datasets; usually require a larger perplexity. Consider selecting a value; between 5 and 50. The choice is not extremely critical since t-SNE; is quite insensitive to this parameter. **early_exaggeration** : `float`, optional (default: 12.0). Controls how tight natural clusters in the original space are in the; embedded space and how much space will be between them. For larger; values, the space between natural clusters will be larger in the; embedded space. Again, the choice of this parameter is not very; critical. If the cost function increases during initial optimization,; the early exaggeration factor or the learning rate might be too high. **learning_rate** : `float`, optional (default: 1000). Note that the R-package ""Rtsne"" uses a default of 200.; The learning rate can be a critical parameter. It should be; between 100 and 1000. If the cost function increases during initial; optimization, the early exaggeration factor or the learning rate; might be too high. If the cost function gets stuck in a bad local; minimum increasing the learning rate helps sometimes. **random_state** : `int` or `None`, optional (default: 0). Change this to use different intial states for the optimization. If `None`,; the initial state is not reproducible. **use_fast_tsne** : `bool`, optional (default: `True`). Use the MulticoreTSNE package by D. Ulyanov if it is installed. **n_jobs** : `int` or `None` (default: `sc.settings.n_jobs`). Number of jobs. **copy** : `bool` (default: `False`). Return a copy instead of writing to adata. :Returns:. Depending on `copy`, returns or updates `adata` with the following fields. . **X_tsne** : `np.ndarray` (`adata.obs`, dtype `float`); ```. Now let's look at `pp.neighbors` where you're reading the type annotations from the signature.; - Obviously, the signature itself now is a mess for humans to read. But ok, that's fine if the docstring is easy to read.; - There i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999
https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:3198,Performance,optimiz,optimization,3198," parameter. **early_exaggeration** : `float`, optional (default: 12.0). Controls how tight natural clusters in the original space are in the; embedded space and how much space will be between them. For larger; values, the space between natural clusters will be larger in the; embedded space. Again, the choice of this parameter is not very; critical. If the cost function increases during initial optimization,; the early exaggeration factor or the learning rate might be too high. **learning_rate** : `float`, optional (default: 1000). Note that the R-package ""Rtsne"" uses a default of 200.; The learning rate can be a critical parameter. It should be; between 100 and 1000. If the cost function increases during initial; optimization, the early exaggeration factor or the learning rate; might be too high. If the cost function gets stuck in a bad local; minimum increasing the learning rate helps sometimes. **random_state** : `int` or `None`, optional (default: 0). Change this to use different intial states for the optimization. If `None`,; the initial state is not reproducible. **use_fast_tsne** : `bool`, optional (default: `True`). Use the MulticoreTSNE package by D. Ulyanov if it is installed. **n_jobs** : `int` or `None` (default: `sc.settings.n_jobs`). Number of jobs. **copy** : `bool` (default: `False`). Return a copy instead of writing to adata. :Returns:. Depending on `copy`, returns or updates `adata` with the following fields. . **X_tsne** : `np.ndarray` (`adata.obs`, dtype `float`); ```. Now let's look at `pp.neighbors` where you're reading the type annotations from the signature.; - Obviously, the signature itself now is a mess for humans to read. But ok, that's fine if the docstring is easy to read.; - There is an error ` <class 'inspect._empty'>`; - The rest looks good to me, except for the superficial stylistic remarks above.; ```; Signature: sc.pp.neighbors(adata:anndata.base.AnnData, n_neighbors:int=15, n_pcs:Union[int, NoneType]=None, use_rep:Union[str, NoneTy",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999
https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:1319,Safety,detect,detected,1319," we stick with the underlined sections? `:Parameters:` is a lot less pretty than the underlined counterpart.; - Why do we indent? Jupyter's typical help box is very narrow and the output really gets more squashed. Also, there seem to be a lot of unnecessary newlines. Pasting `tl.tsne` here looks somewhat acceptable (though not nice). But invoking it in a Jupyter notebook doesn't look nice...; ```; Signature: sc.tl.tsne(adata, n_pcs=None, use_rep=None, perplexity=30, early_exaggeration=12, learning_rate=1000, random_state=0, use_fast_tsne=True, n_jobs=None, copy=False); Docstring:; t-SNE [Maaten08]_ [Amir13]_ [Pedregosa11]_. t-distributed stochastic neighborhood embedding (tSNE) [Maaten08]_ has been; proposed for visualizating single-cell data by [Amir13]_. Here, by default,; we use the implementation of *scikit-learn* [Pedregosa11]_. You can achieve; a huge speedup and better convergence if you install `Multicore-tSNE; <https://github.com/DmitryUlyanov/Multicore-TSNE>`__ by [Ulyanov16]_, which; will be automatically detected by Scanpy. :Parameters:. **adata** : :class:`~anndata.AnnData`. Annotated data matrix. **n_pcs** : `int` or `None`, optional (default: `None`). Use this many PCs. If `n_pcs==0` use `.X` if `use_rep is None`. **use_rep** : \{`None`, 'X'\} or any key for `.obsm`, optional (default: `None`). Use the indicated representation. If `None`, the representation is chosen; automatically: for `.n_vars` < 50, `.X` is used, otherwise 'X_pca' is used.; If 'X_pca' is not present, it's computed with default parameters. **perplexity** : `float`, optional (default: 30). The perplexity is related to the number of nearest neighbors that; is used in other manifold learning algorithms. Larger datasets; usually require a larger perplexity. Consider selecting a value; between 5 and 50. The choice is not extremely critical since t-SNE; is quite insensitive to this parameter. **early_exaggeration** : `float`, optional (default: 12.0). Controls how tight natural clusters i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999
https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:7602,Safety,redund,redundency,7602,". **random_state** : typing.Union[int, mtrand.RandomState, NoneType]. A numpy random seed. **method** : {'umap', 'gauss', `None`} (default: `'umap'`). Use 'umap' [McInnes18]_ or 'gauss' (Gauss kernel following [Coifman05]_; with adaptive width [Haghverdi16]_) for computing connectivities. **metric** : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], float]], optional (default: 'euclidean'). A known metric’s name or a callable that returns a distance. **metric_kwds** : Mapping. Options for the metric. **copy** : bool. Return a copy instead of writing to adata. :Returns:. Depending on `copy`, updates or returns `adata` with the following:. . **connectivities** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Weighted adjacency matrix of the neighborhood graph of data; points. Weights should be interpreted as connectivities. **distances** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Instead of decaying weights, this stores distances for each pair of; neighbors.; File: ~/_hholtz/01_projects/1512_scanpy/scanpy/scanpy/neighbors/__init__.py; Type: function; ```. PS: ; - Already the [docs](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.Neighbors.compute_neighbors.html) show that `Neighbors.compute_neighbors` has invalid numpydoc... this was the case in several instances and I'm slowly fixing all of them... It's just a matter of adding `\` at the line breaks.; - I completely agree that the redundency between signature and docstring information lead to a a very small number of errors in the docstrings. However, in several instances, I'm setting the default value in the signature to `None`. But in the docstring, I'm giving the value to which this `None` evaluates in the default case (depending on what is passed)... There is quite a number of such cases. Clearly, one could replace all of them with `'auto'` parameters, which is probably the better way of doing this. As the whole thing is backwards compat, this is not an immediate problem",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999
https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:1110,Usability,learn,learn,1110,"some minor stylistic remarks. - Why do we want the `**`? The layout and the indenting highlights the variable names enough already. Also, Jupyter notebooks don't even interpret them.; - Why don't we stick with the underlined sections? `:Parameters:` is a lot less pretty than the underlined counterpart.; - Why do we indent? Jupyter's typical help box is very narrow and the output really gets more squashed. Also, there seem to be a lot of unnecessary newlines. Pasting `tl.tsne` here looks somewhat acceptable (though not nice). But invoking it in a Jupyter notebook doesn't look nice...; ```; Signature: sc.tl.tsne(adata, n_pcs=None, use_rep=None, perplexity=30, early_exaggeration=12, learning_rate=1000, random_state=0, use_fast_tsne=True, n_jobs=None, copy=False); Docstring:; t-SNE [Maaten08]_ [Amir13]_ [Pedregosa11]_. t-distributed stochastic neighborhood embedding (tSNE) [Maaten08]_ has been; proposed for visualizating single-cell data by [Amir13]_. Here, by default,; we use the implementation of *scikit-learn* [Pedregosa11]_. You can achieve; a huge speedup and better convergence if you install `Multicore-tSNE; <https://github.com/DmitryUlyanov/Multicore-TSNE>`__ by [Ulyanov16]_, which; will be automatically detected by Scanpy. :Parameters:. **adata** : :class:`~anndata.AnnData`. Annotated data matrix. **n_pcs** : `int` or `None`, optional (default: `None`). Use this many PCs. If `n_pcs==0` use `.X` if `use_rep is None`. **use_rep** : \{`None`, 'X'\} or any key for `.obsm`, optional (default: `None`). Use the indicated representation. If `None`, the representation is chosen; automatically: for `.n_vars` < 50, `.X` is used, otherwise 'X_pca' is used.; If 'X_pca' is not present, it's computed with default parameters. **perplexity** : `float`, optional (default: 30). The perplexity is related to the number of nearest neighbors that; is used in other manifold learning algorithms. Larger datasets; usually require a larger perplexity. Consider selecting a value; between 5 a",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999
https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:1979,Usability,learn,learning,1979,"c neighborhood embedding (tSNE) [Maaten08]_ has been; proposed for visualizating single-cell data by [Amir13]_. Here, by default,; we use the implementation of *scikit-learn* [Pedregosa11]_. You can achieve; a huge speedup and better convergence if you install `Multicore-tSNE; <https://github.com/DmitryUlyanov/Multicore-TSNE>`__ by [Ulyanov16]_, which; will be automatically detected by Scanpy. :Parameters:. **adata** : :class:`~anndata.AnnData`. Annotated data matrix. **n_pcs** : `int` or `None`, optional (default: `None`). Use this many PCs. If `n_pcs==0` use `.X` if `use_rep is None`. **use_rep** : \{`None`, 'X'\} or any key for `.obsm`, optional (default: `None`). Use the indicated representation. If `None`, the representation is chosen; automatically: for `.n_vars` < 50, `.X` is used, otherwise 'X_pca' is used.; If 'X_pca' is not present, it's computed with default parameters. **perplexity** : `float`, optional (default: 30). The perplexity is related to the number of nearest neighbors that; is used in other manifold learning algorithms. Larger datasets; usually require a larger perplexity. Consider selecting a value; between 5 and 50. The choice is not extremely critical since t-SNE; is quite insensitive to this parameter. **early_exaggeration** : `float`, optional (default: 12.0). Controls how tight natural clusters in the original space are in the; embedded space and how much space will be between them. For larger; values, the space between natural clusters will be larger in the; embedded space. Again, the choice of this parameter is not very; critical. If the cost function increases during initial optimization,; the early exaggeration factor or the learning rate might be too high. **learning_rate** : `float`, optional (default: 1000). Note that the R-package ""Rtsne"" uses a default of 200.; The learning rate can be a critical parameter. It should be; between 100 and 1000. If the cost function increases during initial; optimization, the early exaggeration fact",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999
https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:2627,Usability,learn,learning,2627,"ional (default: `None`). Use the indicated representation. If `None`, the representation is chosen; automatically: for `.n_vars` < 50, `.X` is used, otherwise 'X_pca' is used.; If 'X_pca' is not present, it's computed with default parameters. **perplexity** : `float`, optional (default: 30). The perplexity is related to the number of nearest neighbors that; is used in other manifold learning algorithms. Larger datasets; usually require a larger perplexity. Consider selecting a value; between 5 and 50. The choice is not extremely critical since t-SNE; is quite insensitive to this parameter. **early_exaggeration** : `float`, optional (default: 12.0). Controls how tight natural clusters in the original space are in the; embedded space and how much space will be between them. For larger; values, the space between natural clusters will be larger in the; embedded space. Again, the choice of this parameter is not very; critical. If the cost function increases during initial optimization,; the early exaggeration factor or the learning rate might be too high. **learning_rate** : `float`, optional (default: 1000). Note that the R-package ""Rtsne"" uses a default of 200.; The learning rate can be a critical parameter. It should be; between 100 and 1000. If the cost function increases during initial; optimization, the early exaggeration factor or the learning rate; might be too high. If the cost function gets stuck in a bad local; minimum increasing the learning rate helps sometimes. **random_state** : `int` or `None`, optional (default: 0). Change this to use different intial states for the optimization. If `None`,; the initial state is not reproducible. **use_fast_tsne** : `bool`, optional (default: `True`). Use the MulticoreTSNE package by D. Ulyanov if it is installed. **n_jobs** : `int` or `None` (default: `sc.settings.n_jobs`). Number of jobs. **copy** : `bool` (default: `False`). Return a copy instead of writing to adata. :Returns:. Depending on `copy`, returns or updates `",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999
https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:2775,Usability,learn,learning,2775,"nt, it's computed with default parameters. **perplexity** : `float`, optional (default: 30). The perplexity is related to the number of nearest neighbors that; is used in other manifold learning algorithms. Larger datasets; usually require a larger perplexity. Consider selecting a value; between 5 and 50. The choice is not extremely critical since t-SNE; is quite insensitive to this parameter. **early_exaggeration** : `float`, optional (default: 12.0). Controls how tight natural clusters in the original space are in the; embedded space and how much space will be between them. For larger; values, the space between natural clusters will be larger in the; embedded space. Again, the choice of this parameter is not very; critical. If the cost function increases during initial optimization,; the early exaggeration factor or the learning rate might be too high. **learning_rate** : `float`, optional (default: 1000). Note that the R-package ""Rtsne"" uses a default of 200.; The learning rate can be a critical parameter. It should be; between 100 and 1000. If the cost function increases during initial; optimization, the early exaggeration factor or the learning rate; might be too high. If the cost function gets stuck in a bad local; minimum increasing the learning rate helps sometimes. **random_state** : `int` or `None`, optional (default: 0). Change this to use different intial states for the optimization. If `None`,; the initial state is not reproducible. **use_fast_tsne** : `bool`, optional (default: `True`). Use the MulticoreTSNE package by D. Ulyanov if it is installed. **n_jobs** : `int` or `None` (default: `sc.settings.n_jobs`). Number of jobs. **copy** : `bool` (default: `False`). Return a copy instead of writing to adata. :Returns:. Depending on `copy`, returns or updates `adata` with the following fields. . **X_tsne** : `np.ndarray` (`adata.obs`, dtype `float`); ```. Now let's look at `pp.neighbors` where you're reading the type annotations from the signature.; - Obvi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999
https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:2952,Usability,learn,learning,2952,"number of nearest neighbors that; is used in other manifold learning algorithms. Larger datasets; usually require a larger perplexity. Consider selecting a value; between 5 and 50. The choice is not extremely critical since t-SNE; is quite insensitive to this parameter. **early_exaggeration** : `float`, optional (default: 12.0). Controls how tight natural clusters in the original space are in the; embedded space and how much space will be between them. For larger; values, the space between natural clusters will be larger in the; embedded space. Again, the choice of this parameter is not very; critical. If the cost function increases during initial optimization,; the early exaggeration factor or the learning rate might be too high. **learning_rate** : `float`, optional (default: 1000). Note that the R-package ""Rtsne"" uses a default of 200.; The learning rate can be a critical parameter. It should be; between 100 and 1000. If the cost function increases during initial; optimization, the early exaggeration factor or the learning rate; might be too high. If the cost function gets stuck in a bad local; minimum increasing the learning rate helps sometimes. **random_state** : `int` or `None`, optional (default: 0). Change this to use different intial states for the optimization. If `None`,; the initial state is not reproducible. **use_fast_tsne** : `bool`, optional (default: `True`). Use the MulticoreTSNE package by D. Ulyanov if it is installed. **n_jobs** : `int` or `None` (default: `sc.settings.n_jobs`). Number of jobs. **copy** : `bool` (default: `False`). Return a copy instead of writing to adata. :Returns:. Depending on `copy`, returns or updates `adata` with the following fields. . **X_tsne** : `np.ndarray` (`adata.obs`, dtype `float`); ```. Now let's look at `pp.neighbors` where you're reading the type annotations from the signature.; - Obviously, the signature itself now is a mess for humans to read. But ok, that's fine if the docstring is easy to read.; - There i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999
https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:3057,Usability,learn,learning,3057,"arger perplexity. Consider selecting a value; between 5 and 50. The choice is not extremely critical since t-SNE; is quite insensitive to this parameter. **early_exaggeration** : `float`, optional (default: 12.0). Controls how tight natural clusters in the original space are in the; embedded space and how much space will be between them. For larger; values, the space between natural clusters will be larger in the; embedded space. Again, the choice of this parameter is not very; critical. If the cost function increases during initial optimization,; the early exaggeration factor or the learning rate might be too high. **learning_rate** : `float`, optional (default: 1000). Note that the R-package ""Rtsne"" uses a default of 200.; The learning rate can be a critical parameter. It should be; between 100 and 1000. If the cost function increases during initial; optimization, the early exaggeration factor or the learning rate; might be too high. If the cost function gets stuck in a bad local; minimum increasing the learning rate helps sometimes. **random_state** : `int` or `None`, optional (default: 0). Change this to use different intial states for the optimization. If `None`,; the initial state is not reproducible. **use_fast_tsne** : `bool`, optional (default: `True`). Use the MulticoreTSNE package by D. Ulyanov if it is installed. **n_jobs** : `int` or `None` (default: `sc.settings.n_jobs`). Number of jobs. **copy** : `bool` (default: `False`). Return a copy instead of writing to adata. :Returns:. Depending on `copy`, returns or updates `adata` with the following fields. . **X_tsne** : `np.ndarray` (`adata.obs`, dtype `float`); ```. Now let's look at `pp.neighbors` where you're reading the type annotations from the signature.; - Obviously, the signature itself now is a mess for humans to read. But ok, that's fine if the docstring is easy to read.; - There is an error ` <class 'inspect._empty'>`; - The rest looks good to me, except for the superficial stylistic remarks abo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999
https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:7971,Usability,Clear,Clearly,7971,". **random_state** : typing.Union[int, mtrand.RandomState, NoneType]. A numpy random seed. **method** : {'umap', 'gauss', `None`} (default: `'umap'`). Use 'umap' [McInnes18]_ or 'gauss' (Gauss kernel following [Coifman05]_; with adaptive width [Haghverdi16]_) for computing connectivities. **metric** : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], float]], optional (default: 'euclidean'). A known metric’s name or a callable that returns a distance. **metric_kwds** : Mapping. Options for the metric. **copy** : bool. Return a copy instead of writing to adata. :Returns:. Depending on `copy`, updates or returns `adata` with the following:. . **connectivities** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Weighted adjacency matrix of the neighborhood graph of data; points. Weights should be interpreted as connectivities. **distances** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Instead of decaying weights, this stores distances for each pair of; neighbors.; File: ~/_hholtz/01_projects/1512_scanpy/scanpy/scanpy/neighbors/__init__.py; Type: function; ```. PS: ; - Already the [docs](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.Neighbors.compute_neighbors.html) show that `Neighbors.compute_neighbors` has invalid numpydoc... this was the case in several instances and I'm slowly fixing all of them... It's just a matter of adding `\` at the line breaks.; - I completely agree that the redundency between signature and docstring information lead to a a very small number of errors in the docstrings. However, in several instances, I'm setting the default value in the signature to `None`. But in the docstring, I'm giving the value to which this `None` evaluates in the default case (depending on what is passed)... There is quite a number of such cases. Clearly, one could replace all of them with `'auto'` parameters, which is probably the better way of doing this. As the whole thing is backwards compat, this is not an immediate problem",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999
https://github.com/scverse/scanpy/pull/192#issuecomment-404199758:418,Usability,simpl,simple,418,"about your first questions: i’m using the numpydoc extension to parse the strings into a structure that I can modify. the normal way to render this structure back to a string is doing `str(numpydoc_object)`. This however doesn’t give us the original format, but instead the reStructuredText outpt of numpydoc. What do you mean with `auto` parameters? using `None` is the pythonic way to do things like this. i.e. if a simple string or int or so isn’t sufficient to explain the default case, use `None` and do something complex, which you then describe in the docs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404199758
https://github.com/scverse/scanpy/pull/192#issuecomment-404268015:107,Modifiability,rewrite,rewrite,107,"OK, got the formatting issues. Let's discuss at the office. Let's stick with `None`, this just requires to rewrite a very small number of strings... will not be a problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404268015
https://github.com/scverse/scanpy/pull/192#issuecomment-404504765:52,Usability,simpl,simpler,52,"So I switched everything to Napoleon after all. Far simpler and prettier. I no longer parse the docstrings, and opted for a different approach: If a line contains only the name of a parameter, it will be amended with type info. Since no fancy parsing is involved, that could theoretically go wrong. On the other hand, the docstrings now stay in the same format and the probability for a line to consist of just the parameter name (and e.g. no `.` behind it) is relatively low and can be fixed when discovered.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404504765
https://github.com/scverse/scanpy/issues/194#issuecomment-404259032:91,Deployability,install,installation,91,"Yes, `n_jobs>2` will be faster as computations are done in parallel. But something in your installation doesn't seem to go well with this. I now set the default number of jobs to 1, so in the next Scanpy release, you won't have to set it manually anymore.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/194#issuecomment-404259032
https://github.com/scverse/scanpy/issues/194#issuecomment-404259032:204,Deployability,release,release,204,"Yes, `n_jobs>2` will be faster as computations are done in parallel. But something in your installation doesn't seem to go well with this. I now set the default number of jobs to 1, so in the next Scanpy release, you won't have to set it manually anymore.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/194#issuecomment-404259032
https://github.com/scverse/scanpy/pull/199#issuecomment-404874304:32,Availability,error,error,32,Anyone familiar with the travis error?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/199#issuecomment-404874304
https://github.com/scverse/scanpy/pull/199#issuecomment-405085782:124,Availability,error,error,124,"Due to a misconfiguration in Travis setup, all tests are now running only with Python 3.7 now and there is a mysterious HDF error somewhat related to Python 3.7 and pytables.; Python version is fixed in https://github.com/theislab/scanpy/pull/201, so until we have Python 3.7 tests, we are good.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/199#issuecomment-405085782
https://github.com/scverse/scanpy/pull/199#issuecomment-405085782:47,Testability,test,tests,47,"Due to a misconfiguration in Travis setup, all tests are now running only with Python 3.7 now and there is a mysterious HDF error somewhat related to Python 3.7 and pytables.; Python version is fixed in https://github.com/theislab/scanpy/pull/201, so until we have Python 3.7 tests, we are good.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/199#issuecomment-405085782
https://github.com/scverse/scanpy/pull/199#issuecomment-405085782:276,Testability,test,tests,276,"Due to a misconfiguration in Travis setup, all tests are now running only with Python 3.7 now and there is a mysterious HDF error somewhat related to Python 3.7 and pytables.; Python version is fixed in https://github.com/theislab/scanpy/pull/201, so until we have Python 3.7 tests, we are good.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/199#issuecomment-405085782
https://github.com/scverse/scanpy/issues/203#issuecomment-405780122:188,Integrability,wrap,wrapper,188,"@falexwolf ; Is it caused by the parametter svd_solver ? Acorrding to the tl.pca() document ( svd_solver : str, optional (default: ‘auto’)SVD solver to use. Either ‘arpack’ for the ARPACK wrapper in SciPy (scipy.sparse.linalg.svds), or ‘randomized’ for the randomized algorithm due to Halko (2009). “auto” chooses automatically depending on the size of the problem.; The 'randomized' algorithm will give different result each time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/203#issuecomment-405780122
https://github.com/scverse/scanpy/issues/203#issuecomment-405780122:328,Integrability,depend,depending,328,"@falexwolf ; Is it caused by the parametter svd_solver ? Acorrding to the tl.pca() document ( svd_solver : str, optional (default: ‘auto’)SVD solver to use. Either ‘arpack’ for the ARPACK wrapper in SciPy (scipy.sparse.linalg.svds), or ‘randomized’ for the randomized algorithm due to Halko (2009). “auto” chooses automatically depending on the size of the problem.; The 'randomized' algorithm will give different result each time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/203#issuecomment-405780122
https://github.com/scverse/scanpy/issues/204#issuecomment-405303780:46,Availability,error,errors,46,"I will check. Meanwhile, I realized that some errors were introduced in the latest plotting functions, thus I started working in a list of tests to avoid those problems in the future.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/204#issuecomment-405303780
https://github.com/scverse/scanpy/issues/204#issuecomment-405303780:148,Safety,avoid,avoid,148,"I will check. Meanwhile, I realized that some errors were introduced in the latest plotting functions, thus I started working in a list of tests to avoid those problems in the future.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/204#issuecomment-405303780
https://github.com/scverse/scanpy/issues/204#issuecomment-405303780:139,Testability,test,tests,139,"I will check. Meanwhile, I realized that some errors were introduced in the latest plotting functions, thus I started working in a list of tests to avoid those problems in the future.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/204#issuecomment-405303780
https://github.com/scverse/scanpy/issues/206#issuecomment-405262289:199,Deployability,update,updated,199,"@doncarlos999 , can you please confirm that you have the latest version of MAGIC? You can do this by running; ```; import magic; magic.__version__; ```; MAGIC was only very recently (four days ago!) updated to support scanpy, so if you don't have version 1.1.0, I recommend reinstalling:; ```; pip install --upgrade --user git+git://github.com/KrishnaswamyLab/MAGIC.git#subdirectory=python; ```; Thanks for using MAGIC!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/206#issuecomment-405262289
https://github.com/scverse/scanpy/issues/206#issuecomment-405262289:298,Deployability,install,install,298,"@doncarlos999 , can you please confirm that you have the latest version of MAGIC? You can do this by running; ```; import magic; magic.__version__; ```; MAGIC was only very recently (four days ago!) updated to support scanpy, so if you don't have version 1.1.0, I recommend reinstalling:; ```; pip install --upgrade --user git+git://github.com/KrishnaswamyLab/MAGIC.git#subdirectory=python; ```; Thanks for using MAGIC!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/206#issuecomment-405262289
https://github.com/scverse/scanpy/issues/206#issuecomment-405262289:308,Deployability,upgrade,upgrade,308,"@doncarlos999 , can you please confirm that you have the latest version of MAGIC? You can do this by running; ```; import magic; magic.__version__; ```; MAGIC was only very recently (four days ago!) updated to support scanpy, so if you don't have version 1.1.0, I recommend reinstalling:; ```; pip install --upgrade --user git+git://github.com/KrishnaswamyLab/MAGIC.git#subdirectory=python; ```; Thanks for using MAGIC!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/206#issuecomment-405262289
https://github.com/scverse/scanpy/pull/207#issuecomment-405339417:111,Usability,clear,clear,111,@falexwolf regarding #204 the image that didn't work for you is this? I will address the conflict once this is clear because they are related. ![image](https://user-images.githubusercontent.com/4964309/42776678-05350dc6-8938-11e8-8109-901e94abbfee.png),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207#issuecomment-405339417
https://github.com/scverse/scanpy/pull/207#issuecomment-405496882:153,Testability,test,tests,153,"Yes, this is the correct plot. I re-enabled the feature with https://github.com/theislab/scanpy/commit/2a869d6060ba0de12feaf08a809bdf745d39ef10. 🙂. Your tests look very good! Seems like a great solution. :smile:. How about, instead of committing extra image files to the test repo, using the image files that we use for the docs? See here https://github.com/theislab/scanpy/tree/master/docs/api. It would be great to test whether what one sees in the docs is actually still the same as the current version produces. The conflicts are solely due to the fact that I renamed `kwargs` to `kwds` as this fits better with the ""parameters"" (instead of ""args"") convention, and to the fix of the plot above.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207#issuecomment-405496882
https://github.com/scverse/scanpy/pull/207#issuecomment-405496882:271,Testability,test,test,271,"Yes, this is the correct plot. I re-enabled the feature with https://github.com/theislab/scanpy/commit/2a869d6060ba0de12feaf08a809bdf745d39ef10. 🙂. Your tests look very good! Seems like a great solution. :smile:. How about, instead of committing extra image files to the test repo, using the image files that we use for the docs? See here https://github.com/theislab/scanpy/tree/master/docs/api. It would be great to test whether what one sees in the docs is actually still the same as the current version produces. The conflicts are solely due to the fact that I renamed `kwargs` to `kwds` as this fits better with the ""parameters"" (instead of ""args"") convention, and to the fix of the plot above.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207#issuecomment-405496882
https://github.com/scverse/scanpy/pull/207#issuecomment-405496882:417,Testability,test,test,417,"Yes, this is the correct plot. I re-enabled the feature with https://github.com/theislab/scanpy/commit/2a869d6060ba0de12feaf08a809bdf745d39ef10. 🙂. Your tests look very good! Seems like a great solution. :smile:. How about, instead of committing extra image files to the test repo, using the image files that we use for the docs? See here https://github.com/theislab/scanpy/tree/master/docs/api. It would be great to test whether what one sees in the docs is actually still the same as the current version produces. The conflicts are solely due to the fact that I renamed `kwargs` to `kwds` as this fits better with the ""parameters"" (instead of ""args"") convention, and to the fix of the plot above.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207#issuecomment-405496882
https://github.com/scverse/scanpy/pull/207#issuecomment-405505301:43,Deployability,integrat,integrate,43,"I will take a look later to see how we can integrate better the visualizations, the tests and the documentation. I will put back `kwds` also. . Have you consider adding another dataset to the repository? This will be good for showing examples.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207#issuecomment-405505301
https://github.com/scverse/scanpy/pull/207#issuecomment-405505301:43,Integrability,integrat,integrate,43,"I will take a look later to see how we can integrate better the visualizations, the tests and the documentation. I will put back `kwds` also. . Have you consider adding another dataset to the repository? This will be good for showing examples.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207#issuecomment-405505301
https://github.com/scverse/scanpy/pull/207#issuecomment-405505301:84,Testability,test,tests,84,"I will take a look later to see how we can integrate better the visualizations, the tests and the documentation. I will put back `kwds` also. . Have you consider adding another dataset to the repository? This will be good for showing examples.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207#issuecomment-405505301
https://github.com/scverse/scanpy/pull/207#issuecomment-405508287:201,Availability,down,downloaded,201,"I don't feel well adding a large dataset to the repository. Adding something subsampled and rather low-dimensional would be fine. Alternatively, you could also just add a dataset that is automatically downloaded: as [here](https://github.com/theislab/scanpy/blob/7646c947f632ea7b09fea783e32a017136cfed24/scanpy/datasets/__init__.py#L104-L106) or [here](https://github.com/theislab/scanpy/blob/7646c947f632ea7b09fea783e32a017136cfed24/scanpy/datasets/__init__.py#L142-L144). As both travis and readthedocs will cache this, it should be a viable solution that avoids bloating the repository with several MB of data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207#issuecomment-405508287
https://github.com/scverse/scanpy/pull/207#issuecomment-405508287:510,Performance,cache,cache,510,"I don't feel well adding a large dataset to the repository. Adding something subsampled and rather low-dimensional would be fine. Alternatively, you could also just add a dataset that is automatically downloaded: as [here](https://github.com/theislab/scanpy/blob/7646c947f632ea7b09fea783e32a017136cfed24/scanpy/datasets/__init__.py#L104-L106) or [here](https://github.com/theislab/scanpy/blob/7646c947f632ea7b09fea783e32a017136cfed24/scanpy/datasets/__init__.py#L142-L144). As both travis and readthedocs will cache this, it should be a viable solution that avoids bloating the repository with several MB of data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207#issuecomment-405508287
https://github.com/scverse/scanpy/pull/207#issuecomment-405508287:558,Safety,avoid,avoids,558,"I don't feel well adding a large dataset to the repository. Adding something subsampled and rather low-dimensional would be fine. Alternatively, you could also just add a dataset that is automatically downloaded: as [here](https://github.com/theislab/scanpy/blob/7646c947f632ea7b09fea783e32a017136cfed24/scanpy/datasets/__init__.py#L104-L106) or [here](https://github.com/theislab/scanpy/blob/7646c947f632ea7b09fea783e32a017136cfed24/scanpy/datasets/__init__.py#L142-L144). As both travis and readthedocs will cache this, it should be a viable solution that avoids bloating the repository with several MB of data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207#issuecomment-405508287
https://github.com/scverse/scanpy/pull/207#issuecomment-405579971:30,Availability,down,downloading,30,"@falexwolf I like the idea of downloading the data on the fly!. For an idea of a small dataset that is already filtered and normalized, do you know if the pbmc data from 10x Genomics has some restrictions? ... nevermind, they have a Creative Commons licence.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207#issuecomment-405579971
https://github.com/scverse/scanpy/pull/207#issuecomment-405917660:52,Testability,test,tests,52,"I think we can merge this for now. I will add other tests I think next week. There is something I don't understand: For the docs, I see that an image is presented together with the API documentation. E.g. [api.pl.dotplot](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pl.dotplot.html) But I don't see how is this image is referred in the function description. How does this work?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207#issuecomment-405917660
https://github.com/scverse/scanpy/pull/207#issuecomment-405943292:61,Testability,test,tests,61,"Sorry, but I think there still is a range-related bug in the tests:; ```; E matplotlib.testing.exceptions.ImageComparisonFailure: Image sizes do not match expected size: (376, 439, 3) actual size (376, 440, 3); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207#issuecomment-405943292
https://github.com/scverse/scanpy/pull/207#issuecomment-405943292:87,Testability,test,testing,87,"Sorry, but I think there still is a range-related bug in the tests:; ```; E matplotlib.testing.exceptions.ImageComparisonFailure: Image sizes do not match expected size: (376, 439, 3) actual size (376, 440, 3); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207#issuecomment-405943292
https://github.com/scverse/scanpy/pull/207#issuecomment-405945676:9,Testability,test,tests,9,my local tests pass. Which means that I will need to check the version of matplotlib. As soon as I can I will take a closer look.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207#issuecomment-405945676
https://github.com/scverse/scanpy/pull/207#issuecomment-406266348:0,Testability,Test,Test,0,Test are passing now! . @flying-sheep can you tell me more about the images that appear in the automatic API documentation?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207#issuecomment-406266348
https://github.com/scverse/scanpy/pull/207#issuecomment-406512161:377,Usability,simpl,simply,377,"Sure! I made it as easy as possible, by adding it directly to the [autosummary template](https://github.com/theislab/scanpy/blob/181240602f2face3c49532f40b2fad4f300e3685/docs/_templates/autosummary/base.rst#L3). `api_image` is defined [in `scanpy/docs/conf.py` here](https://github.com/theislab/scanpy/blob/181240602f2face3c49532f40b2fad4f300e3685/docs/conf.py#L213-L218), and simply inserts a `..image` directive if a file exists at the right place. Just add your (.png) image to [`scanpy/docs/api`](https://github.com/theislab/scanpy/tree/master/docs/api) using the qualified name of the function, e.g. `scanpy/docs/api/scanpy.api.pl.dotplot.png`. Maybe we should set up [Contribution guidelines](https://help.github.com/articles/setting-guidelines-for-repository-contributors/) where people can read this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207#issuecomment-406512161
https://github.com/scverse/scanpy/pull/207#issuecomment-406512161:687,Usability,guid,guidelines,687,"Sure! I made it as easy as possible, by adding it directly to the [autosummary template](https://github.com/theislab/scanpy/blob/181240602f2face3c49532f40b2fad4f300e3685/docs/_templates/autosummary/base.rst#L3). `api_image` is defined [in `scanpy/docs/conf.py` here](https://github.com/theislab/scanpy/blob/181240602f2face3c49532f40b2fad4f300e3685/docs/conf.py#L213-L218), and simply inserts a `..image` directive if a file exists at the right place. Just add your (.png) image to [`scanpy/docs/api`](https://github.com/theislab/scanpy/tree/master/docs/api) using the qualified name of the function, e.g. `scanpy/docs/api/scanpy.api.pl.dotplot.png`. Maybe we should set up [Contribution guidelines](https://help.github.com/articles/setting-guidelines-for-repository-contributors/) where people can read this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207#issuecomment-406512161
https://github.com/scverse/scanpy/pull/207#issuecomment-406512161:740,Usability,guid,guidelines-for-repository-contributors,740,"Sure! I made it as easy as possible, by adding it directly to the [autosummary template](https://github.com/theislab/scanpy/blob/181240602f2face3c49532f40b2fad4f300e3685/docs/_templates/autosummary/base.rst#L3). `api_image` is defined [in `scanpy/docs/conf.py` here](https://github.com/theislab/scanpy/blob/181240602f2face3c49532f40b2fad4f300e3685/docs/conf.py#L213-L218), and simply inserts a `..image` directive if a file exists at the right place. Just add your (.png) image to [`scanpy/docs/api`](https://github.com/theislab/scanpy/tree/master/docs/api) using the qualified name of the function, e.g. `scanpy/docs/api/scanpy.api.pl.dotplot.png`. Maybe we should set up [Contribution guidelines](https://help.github.com/articles/setting-guidelines-for-repository-contributors/) where people can read this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207#issuecomment-406512161
https://github.com/scverse/scanpy/pull/207#issuecomment-407332243:79,Testability,test,test,79,"I hope that soon I can add some figures into the documentation and add further test. ; . > Am 23.07.2018 um 23:12 schrieb Alex Wolf <notifications@github.com>:; > ; > OK, merged this into master on the command line after fixing plotting/anndata.py, where some changes would have otherwise been reversed...; > ; > Thank you very much, Fidel! Really cool!; > ; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207#issuecomment-407332243
https://github.com/scverse/scanpy/issues/208#issuecomment-405897974:18,Deployability,update,updated,18,"Yes, It helped. I updated the magic version and it seems to be working fine now. ; Thanks a lot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/208#issuecomment-405897974
https://github.com/scverse/scanpy/issues/210#issuecomment-407038976:697,Availability,error,error,697,"Thank you for your thoughts!. 1. `normalize_per_cell` needs to remove zero-expression cells as these can't be normalized, the alternative would be to require it as a preprocessing step; but you're right, wflynny, I'll frame it as a fall-back for `normalize_per_cell` in the next version and output a warning... which will make things backwards compatible...; 2. Any filtering operation on the cells/observations should also affect `.raw`. I'll look into this today. ; 3. Any filtering operation on the variables should **not** affect `.raw`. I didn't know that this gives problems in `rank_genes_groups`? Of course, you don't find everything in `.X` that you find in `.raw.X` and you'll get a key error if you try to; but is there a fundamental problem, @LuckyMD?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/210#issuecomment-407038976
https://github.com/scverse/scanpy/issues/210#issuecomment-407038976:502,Modifiability,variab,variables,502,"Thank you for your thoughts!. 1. `normalize_per_cell` needs to remove zero-expression cells as these can't be normalized, the alternative would be to require it as a preprocessing step; but you're right, wflynny, I'll frame it as a fall-back for `normalize_per_cell` in the next version and output a warning... which will make things backwards compatible...; 2. Any filtering operation on the cells/observations should also affect `.raw`. I'll look into this today. ; 3. Any filtering operation on the variables should **not** affect `.raw`. I didn't know that this gives problems in `rank_genes_groups`? Of course, you don't find everything in `.X` that you find in `.raw.X` and you'll get a key error if you try to; but is there a fundamental problem, @LuckyMD?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/210#issuecomment-407038976
https://github.com/scverse/scanpy/issues/210#issuecomment-407082502:611,Testability,test,tested,611,"I would say there is a more general problem. You should be able to expect that `sc.tl.rank_genes_groups(adata)` and then `sc.pl.rank_genes_groups(adata)` always works. However, if adata is subsetted to HVGs and then `sc.tl.rank_genes_groups()` is run with the default of use_raw=True, then you can get genes as top ranked markers which are not in the adata.X subset. Thus, `sc.pl.rank_genes_groups()` or `sc.pl.rank_genes_groups_violin()` will fail as the gene in `adata.uns['rank_genes_groups']['name']` is not found in adata.var_names. This occurs as the plotting is done on adata.X and not adata.raw.X. I've tested this when using the gene_symbols parameter, but it feels like it should also happen when just using regular var_names.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/210#issuecomment-407082502
https://github.com/scverse/scanpy/issues/210#issuecomment-407213235:54,Testability,log,logging,54,"1. for now, only partially fixed through more verbose logging output: https://github.com/theislab/scanpy/commit/8dcacda93c91a5466c2ff23bceb6120ce1d5e0cf; 2. was a bug that only affected ""inplace subsetting"", fixed via https://github.com/theislab/anndata/commit/0becb7b068dda31a60fd0ecb24360d0b5e3d3d7f and in anndata 0.6.6; 3. hm, of course you should be able to call `tl.rank_genes_groups` and `pl.rank_genes_groups`, but https://github.com/theislab/scanpy/blob/8dcacda93c91a5466c2ff23bceb6120ce1d5e0cf/scanpy/plotting/tools/__init__.py#L271-L282 should do the job for `pl.rank_genes_groups` and https://github.com/theislab/scanpy/blob/8dcacda93c91a5466c2ff23bceb6120ce1d5e0cf/scanpy/plotting/tools/__init__.py#L355-L358 for `pl.rank_genes_groups_violin`, right? I don't see how you can observe what you describe. let's look an example together.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/210#issuecomment-407213235
https://github.com/scverse/scanpy/issues/210#issuecomment-428983959:391,Safety,predict,predicted,391,"The fact that `normalize_per_cell` filters cells caused me some trouble recently, especially because there is nothing in documentation that says this will happen and there the subset indices are not returned. My use case is a little different: I am running a K-fold cross validation and `normalize_per_cell` is a step in the process. Since this function filters out some cells, the array of predicted labels has fewer entries than I would expect for the fold. Ideally, this behavior could be switched off. For example, you could pass a `min_counts` argument to `normalize_per_cell`. I've made this change if this is an approach you would like to take: https://github.com/umangv/scanpy/tree/umangv-normalize",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/210#issuecomment-428983959
https://github.com/scverse/scanpy/issues/210#issuecomment-428983959:272,Security,validat,validation,272,"The fact that `normalize_per_cell` filters cells caused me some trouble recently, especially because there is nothing in documentation that says this will happen and there the subset indices are not returned. My use case is a little different: I am running a K-fold cross validation and `normalize_per_cell` is a step in the process. Since this function filters out some cells, the array of predicted labels has fewer entries than I would expect for the fold. Ideally, this behavior could be switched off. For example, you could pass a `min_counts` argument to `normalize_per_cell`. I've made this change if this is an approach you would like to take: https://github.com/umangv/scanpy/tree/umangv-normalize",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/210#issuecomment-428983959
https://github.com/scverse/scanpy/issues/212#issuecomment-407333853:39,Availability,error,error,39,"I am not so sure, but you may get that error when the matrix contains columns of only ceros. Did you do any filtering of the data before?. Fidel Ramírez . > Am 24.07.2018 um 07:51 schrieb Alex Wolf <notifications@github.com>:; > ; > Can you try running with n_jobs=1?; > ; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/212#issuecomment-407333853
https://github.com/scverse/scanpy/issues/212#issuecomment-407454918:38,Deployability,pipeline,pipeline,38,"I didn't use any filtering before the pipeline, but I read the barcode and gene names from index and columns of the data frame. and i have a cuff-off of the cell for my own. update——————————————; The calculation is depends on module ""patsy"" and it is not in the dependency list of scanpy or I have some problems with installing that package. ; After I reinstalled and updated ""pasty"", problem fixed",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/212#issuecomment-407454918
https://github.com/scverse/scanpy/issues/212#issuecomment-407454918:174,Deployability,update,update,174,"I didn't use any filtering before the pipeline, but I read the barcode and gene names from index and columns of the data frame. and i have a cuff-off of the cell for my own. update——————————————; The calculation is depends on module ""patsy"" and it is not in the dependency list of scanpy or I have some problems with installing that package. ; After I reinstalled and updated ""pasty"", problem fixed",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/212#issuecomment-407454918
https://github.com/scverse/scanpy/issues/212#issuecomment-407454918:317,Deployability,install,installing,317,"I didn't use any filtering before the pipeline, but I read the barcode and gene names from index and columns of the data frame. and i have a cuff-off of the cell for my own. update——————————————; The calculation is depends on module ""patsy"" and it is not in the dependency list of scanpy or I have some problems with installing that package. ; After I reinstalled and updated ""pasty"", problem fixed",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/212#issuecomment-407454918
https://github.com/scverse/scanpy/issues/212#issuecomment-407454918:368,Deployability,update,updated,368,"I didn't use any filtering before the pipeline, but I read the barcode and gene names from index and columns of the data frame. and i have a cuff-off of the cell for my own. update——————————————; The calculation is depends on module ""patsy"" and it is not in the dependency list of scanpy or I have some problems with installing that package. ; After I reinstalled and updated ""pasty"", problem fixed",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/212#issuecomment-407454918
https://github.com/scverse/scanpy/issues/212#issuecomment-407454918:215,Integrability,depend,depends,215,"I didn't use any filtering before the pipeline, but I read the barcode and gene names from index and columns of the data frame. and i have a cuff-off of the cell for my own. update——————————————; The calculation is depends on module ""patsy"" and it is not in the dependency list of scanpy or I have some problems with installing that package. ; After I reinstalled and updated ""pasty"", problem fixed",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/212#issuecomment-407454918
https://github.com/scverse/scanpy/issues/212#issuecomment-407454918:262,Integrability,depend,dependency,262,"I didn't use any filtering before the pipeline, but I read the barcode and gene names from index and columns of the data frame. and i have a cuff-off of the cell for my own. update——————————————; The calculation is depends on module ""patsy"" and it is not in the dependency list of scanpy or I have some problems with installing that package. ; After I reinstalled and updated ""pasty"", problem fixed",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/212#issuecomment-407454918
https://github.com/scverse/scanpy/issues/218#issuecomment-408749326:605,Usability,clear,clears,605,"Dear @fbrundu,. very sorry for the late response. I took a couple of days off with the family. Everything is dictated by the order in `.obs['mycategorical'].cat.categories`. If you're starting off from a string `.obs['mystring']` annotation, then this will default to `natsorted` categories. If you pandas `.reorder_categories` then this will be reflected, too. Now, in `AnnData`, we have the additional possibility to store colors for each category. The corresponding array matches `.obs['mycategorical'].cat.categories` just that instead of the category names, it stores the colors. Let me know if this clears things up for you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/218#issuecomment-408749326
https://github.com/scverse/scanpy/pull/219#issuecomment-408230756:31,Testability,test,tested,31,"Sorry for the PR, was not prop tested.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/219#issuecomment-408230756
https://github.com/scverse/scanpy/issues/220#issuecomment-408263645:258,Integrability,interface,interface,258,"It's numpy.ndarray:; ```type(adata.X)```; ``` numpy.ndarray```; I guess it should be matrix? It's loaded once like this ; ```; path = '../count-genes/datafiles/all_counts.csv'; adata = sc.read(path, cache=True); ```; and then always manipulated with anndata interface. Maybe it should be transformed right after loading?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/220#issuecomment-408263645
https://github.com/scverse/scanpy/issues/220#issuecomment-408263645:98,Performance,load,loaded,98,"It's numpy.ndarray:; ```type(adata.X)```; ``` numpy.ndarray```; I guess it should be matrix? It's loaded once like this ; ```; path = '../count-genes/datafiles/all_counts.csv'; adata = sc.read(path, cache=True); ```; and then always manipulated with anndata interface. Maybe it should be transformed right after loading?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/220#issuecomment-408263645
https://github.com/scverse/scanpy/issues/220#issuecomment-408263645:199,Performance,cache,cache,199,"It's numpy.ndarray:; ```type(adata.X)```; ``` numpy.ndarray```; I guess it should be matrix? It's loaded once like this ; ```; path = '../count-genes/datafiles/all_counts.csv'; adata = sc.read(path, cache=True); ```; and then always manipulated with anndata interface. Maybe it should be transformed right after loading?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/220#issuecomment-408263645
https://github.com/scverse/scanpy/issues/220#issuecomment-408263645:312,Performance,load,loading,312,"It's numpy.ndarray:; ```type(adata.X)```; ``` numpy.ndarray```; I guess it should be matrix? It's loaded once like this ; ```; path = '../count-genes/datafiles/all_counts.csv'; adata = sc.read(path, cache=True); ```; and then always manipulated with anndata interface. Maybe it should be transformed right after loading?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/220#issuecomment-408263645
https://github.com/scverse/scanpy/issues/222#issuecomment-408842788:1020,Deployability,update,update,1020,"`adata.obsm['X_pca']` seems to have predicted better clustering. Thank you. I'm going to try that with the rest of my data and see what clustering it suggests. . As for `adata.uns['neighbors']['distances']`, shortly after my post I read that the `silhouette_score` function wasn't designed to handle spares matrices, so you're right with that. ; (source: https://books.google.com/books?id=skvZDQAAQBAJ&pg=PA786&lpg=PA786&dq=silhouette_score+precomputed+python3&source=bl&ots=YRC9VPTPPW&sig=7KPSQDWtZG6537-f_vZGwMpMvCc&hl=en&sa=X&ved=2ahUKEwjO3ruPqsXcAhWEg-AKHXD5BUAQ6AEwCXoECAMQAQ#v=onepage&q=silhouette_score%20precomputed%20python3&f=false). ; It suggested using todense() if the matrix is small, but to avoid this function all together if the matrix is large. When I trade toarray() for todense() it seems to produce similar results. Since single-cell datasets are likely always to be too big, it suggested using V-Measure or Adjusted Mutual Information as a way to evaluate sparse matrices instead. Just thought I'd update with my findings. Again, the `adata.obsm['X_pca']` suggestion seems to predict better clustering arrangements. Big ups for that! . Best",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/222#issuecomment-408842788
https://github.com/scverse/scanpy/issues/222#issuecomment-408842788:36,Safety,predict,predicted,36,"`adata.obsm['X_pca']` seems to have predicted better clustering. Thank you. I'm going to try that with the rest of my data and see what clustering it suggests. . As for `adata.uns['neighbors']['distances']`, shortly after my post I read that the `silhouette_score` function wasn't designed to handle spares matrices, so you're right with that. ; (source: https://books.google.com/books?id=skvZDQAAQBAJ&pg=PA786&lpg=PA786&dq=silhouette_score+precomputed+python3&source=bl&ots=YRC9VPTPPW&sig=7KPSQDWtZG6537-f_vZGwMpMvCc&hl=en&sa=X&ved=2ahUKEwjO3ruPqsXcAhWEg-AKHXD5BUAQ6AEwCXoECAMQAQ#v=onepage&q=silhouette_score%20precomputed%20python3&f=false). ; It suggested using todense() if the matrix is small, but to avoid this function all together if the matrix is large. When I trade toarray() for todense() it seems to produce similar results. Since single-cell datasets are likely always to be too big, it suggested using V-Measure or Adjusted Mutual Information as a way to evaluate sparse matrices instead. Just thought I'd update with my findings. Again, the `adata.obsm['X_pca']` suggestion seems to predict better clustering arrangements. Big ups for that! . Best",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/222#issuecomment-408842788
https://github.com/scverse/scanpy/issues/222#issuecomment-408842788:706,Safety,avoid,avoid,706,"`adata.obsm['X_pca']` seems to have predicted better clustering. Thank you. I'm going to try that with the rest of my data and see what clustering it suggests. . As for `adata.uns['neighbors']['distances']`, shortly after my post I read that the `silhouette_score` function wasn't designed to handle spares matrices, so you're right with that. ; (source: https://books.google.com/books?id=skvZDQAAQBAJ&pg=PA786&lpg=PA786&dq=silhouette_score+precomputed+python3&source=bl&ots=YRC9VPTPPW&sig=7KPSQDWtZG6537-f_vZGwMpMvCc&hl=en&sa=X&ved=2ahUKEwjO3ruPqsXcAhWEg-AKHXD5BUAQ6AEwCXoECAMQAQ#v=onepage&q=silhouette_score%20precomputed%20python3&f=false). ; It suggested using todense() if the matrix is small, but to avoid this function all together if the matrix is large. When I trade toarray() for todense() it seems to produce similar results. Since single-cell datasets are likely always to be too big, it suggested using V-Measure or Adjusted Mutual Information as a way to evaluate sparse matrices instead. Just thought I'd update with my findings. Again, the `adata.obsm['X_pca']` suggestion seems to predict better clustering arrangements. Big ups for that! . Best",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/222#issuecomment-408842788
https://github.com/scverse/scanpy/issues/222#issuecomment-408842788:1098,Safety,predict,predict,1098,"`adata.obsm['X_pca']` seems to have predicted better clustering. Thank you. I'm going to try that with the rest of my data and see what clustering it suggests. . As for `adata.uns['neighbors']['distances']`, shortly after my post I read that the `silhouette_score` function wasn't designed to handle spares matrices, so you're right with that. ; (source: https://books.google.com/books?id=skvZDQAAQBAJ&pg=PA786&lpg=PA786&dq=silhouette_score+precomputed+python3&source=bl&ots=YRC9VPTPPW&sig=7KPSQDWtZG6537-f_vZGwMpMvCc&hl=en&sa=X&ved=2ahUKEwjO3ruPqsXcAhWEg-AKHXD5BUAQ6AEwCXoECAMQAQ#v=onepage&q=silhouette_score%20precomputed%20python3&f=false). ; It suggested using todense() if the matrix is small, but to avoid this function all together if the matrix is large. When I trade toarray() for todense() it seems to produce similar results. Since single-cell datasets are likely always to be too big, it suggested using V-Measure or Adjusted Mutual Information as a way to evaluate sparse matrices instead. Just thought I'd update with my findings. Again, the `adata.obsm['X_pca']` suggestion seems to predict better clustering arrangements. Big ups for that! . Best",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/222#issuecomment-408842788
https://github.com/scverse/scanpy/issues/223#issuecomment-409829464:463,Performance,optimiz,optimizes,463,"I never used something different than `n_neighbors=5` for very small datasets (~1000 cells) up to `n_neighbors=30` for large datasets. However, I rarely change the default `n_neighbors=15` anyways. For very large datasets and in very rare cases I can imagine that it pays off to go up to `n_neighbors=50` or even more, but I never did this... I'd say it doesn't actually make a lot of sense to use the silhouette coefficient for evaluation: the Louvain algorithm optimizes modularity, which you can view as the graph-based version of a silhouette coefficient (""ratio"" of intra-cluster edges versus inter-cluster edges as compared to intra-cluster distances vs. inter-cluster distances in the silhouette coefficient). Once the graph is computed, there is no point in going back to the feature space for the computation of topological properties. In the end, you describe the common workflow. You start with some coarse clustering and recluster the parts of the graph in which you want higher resolution. It's not at all surprising that the clustering by default doesn't agree with marker genes: modularity clustering clusters densely connected partitions of the graph, an information that comes from averaging over all genes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223#issuecomment-409829464
https://github.com/scverse/scanpy/issues/223#issuecomment-409960942:1502,Integrability,Depend,Depending,1502,"As the clustering is an optimization of the modularity I would argue it makes little sense to use modularity to evaluate the clustering again. Especially at different resolutions, the modularity values you obtain are not really comparable (as the resolution parameter is introduced to not optimize pure modularity and get the same result you would otherwise get at resolution 1). A comparison between knn-graph modularity at the same resolution would tell you how inherently modular the graph is. Is that what you want to know? Or what does an 'improved' graph look like to you?. I agree with Alex that using the silhouette coefficient wouldn't be much more informative. It would then just be an assessment of the approach of using a KNN graph and modularity optimization. And as that approach has been shown to work quite well, evaluating it based on something that works less well (clustering in the feature space directly) feels a bit uninformative. I would go with what you suggested: evaluating based on marker gene expression. In the end the graph is a tool to describe the biology, so any graph structure means little without it. . If you want to evaluate how well the graph represents the biology, maybe the best way forward would be to infer cell-type labels (or use a dataset with labels) and look at the normalized mutual information between clusters and the labels. The clusters would have to be obtained in the same way for each graph (e.g. modularity optimization at a fixed resolution). Depending on how specific the labels are you will get a different result though ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223#issuecomment-409960942
https://github.com/scverse/scanpy/issues/223#issuecomment-409960942:24,Performance,optimiz,optimization,24,"As the clustering is an optimization of the modularity I would argue it makes little sense to use modularity to evaluate the clustering again. Especially at different resolutions, the modularity values you obtain are not really comparable (as the resolution parameter is introduced to not optimize pure modularity and get the same result you would otherwise get at resolution 1). A comparison between knn-graph modularity at the same resolution would tell you how inherently modular the graph is. Is that what you want to know? Or what does an 'improved' graph look like to you?. I agree with Alex that using the silhouette coefficient wouldn't be much more informative. It would then just be an assessment of the approach of using a KNN graph and modularity optimization. And as that approach has been shown to work quite well, evaluating it based on something that works less well (clustering in the feature space directly) feels a bit uninformative. I would go with what you suggested: evaluating based on marker gene expression. In the end the graph is a tool to describe the biology, so any graph structure means little without it. . If you want to evaluate how well the graph represents the biology, maybe the best way forward would be to infer cell-type labels (or use a dataset with labels) and look at the normalized mutual information between clusters and the labels. The clusters would have to be obtained in the same way for each graph (e.g. modularity optimization at a fixed resolution). Depending on how specific the labels are you will get a different result though ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223#issuecomment-409960942
https://github.com/scverse/scanpy/issues/223#issuecomment-409960942:289,Performance,optimiz,optimize,289,"As the clustering is an optimization of the modularity I would argue it makes little sense to use modularity to evaluate the clustering again. Especially at different resolutions, the modularity values you obtain are not really comparable (as the resolution parameter is introduced to not optimize pure modularity and get the same result you would otherwise get at resolution 1). A comparison between knn-graph modularity at the same resolution would tell you how inherently modular the graph is. Is that what you want to know? Or what does an 'improved' graph look like to you?. I agree with Alex that using the silhouette coefficient wouldn't be much more informative. It would then just be an assessment of the approach of using a KNN graph and modularity optimization. And as that approach has been shown to work quite well, evaluating it based on something that works less well (clustering in the feature space directly) feels a bit uninformative. I would go with what you suggested: evaluating based on marker gene expression. In the end the graph is a tool to describe the biology, so any graph structure means little without it. . If you want to evaluate how well the graph represents the biology, maybe the best way forward would be to infer cell-type labels (or use a dataset with labels) and look at the normalized mutual information between clusters and the labels. The clusters would have to be obtained in the same way for each graph (e.g. modularity optimization at a fixed resolution). Depending on how specific the labels are you will get a different result though ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223#issuecomment-409960942
https://github.com/scverse/scanpy/issues/223#issuecomment-409960942:759,Performance,optimiz,optimization,759,"As the clustering is an optimization of the modularity I would argue it makes little sense to use modularity to evaluate the clustering again. Especially at different resolutions, the modularity values you obtain are not really comparable (as the resolution parameter is introduced to not optimize pure modularity and get the same result you would otherwise get at resolution 1). A comparison between knn-graph modularity at the same resolution would tell you how inherently modular the graph is. Is that what you want to know? Or what does an 'improved' graph look like to you?. I agree with Alex that using the silhouette coefficient wouldn't be much more informative. It would then just be an assessment of the approach of using a KNN graph and modularity optimization. And as that approach has been shown to work quite well, evaluating it based on something that works less well (clustering in the feature space directly) feels a bit uninformative. I would go with what you suggested: evaluating based on marker gene expression. In the end the graph is a tool to describe the biology, so any graph structure means little without it. . If you want to evaluate how well the graph represents the biology, maybe the best way forward would be to infer cell-type labels (or use a dataset with labels) and look at the normalized mutual information between clusters and the labels. The clusters would have to be obtained in the same way for each graph (e.g. modularity optimization at a fixed resolution). Depending on how specific the labels are you will get a different result though ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223#issuecomment-409960942
https://github.com/scverse/scanpy/issues/223#issuecomment-409960942:1465,Performance,optimiz,optimization,1465,"As the clustering is an optimization of the modularity I would argue it makes little sense to use modularity to evaluate the clustering again. Especially at different resolutions, the modularity values you obtain are not really comparable (as the resolution parameter is introduced to not optimize pure modularity and get the same result you would otherwise get at resolution 1). A comparison between knn-graph modularity at the same resolution would tell you how inherently modular the graph is. Is that what you want to know? Or what does an 'improved' graph look like to you?. I agree with Alex that using the silhouette coefficient wouldn't be much more informative. It would then just be an assessment of the approach of using a KNN graph and modularity optimization. And as that approach has been shown to work quite well, evaluating it based on something that works less well (clustering in the feature space directly) feels a bit uninformative. I would go with what you suggested: evaluating based on marker gene expression. In the end the graph is a tool to describe the biology, so any graph structure means little without it. . If you want to evaluate how well the graph represents the biology, maybe the best way forward would be to infer cell-type labels (or use a dataset with labels) and look at the normalized mutual information between clusters and the labels. The clusters would have to be obtained in the same way for each graph (e.g. modularity optimization at a fixed resolution). Depending on how specific the labels are you will get a different result though ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223#issuecomment-409960942
https://github.com/scverse/scanpy/issues/223#issuecomment-409996535:453,Usability,learn,learn,453,"I guess what I mean is a metric to describe how well the data points are clustered in their own cluster relative to every other cluster and/or data point. But what you both said makes sense. Looking at marker expression and cell type classification seems to be the most obvious, practical way to assign clusters. At the end of the day, it's the biology we care about. . Thanks for your detail explanations everyone, I'm new to this but am continuing to learn a lot. . Best",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223#issuecomment-409996535
https://github.com/scverse/scanpy/issues/226#issuecomment-438879520:1024,Modifiability,layers,layers,1024," object with n_obs × n_vars = 29322 × 19860. ```python; >>> tiss[tiss.obs['cell_ontology_class']=='B cell']; ```. ```pytb; IndexError Traceback (most recent call last); <ipython-input-269-28b4524131cb> in <module>(); ----> 1 tiss[tiss.obs['cell_ontology_class']=='B cell']. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index); 1299 def __getitem__(self, index):; 1300 """"""Returns a sliced view of the object.""""""; -> 1301 return self._getitem_view(index); 1302 ; 1303 def _getitem_view(self, index):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index); 1303 def _getitem_view(self, index):; 1304 oidx, vidx = self._normalize_indices(index); -> 1305 return AnnData(self, oidx=oidx, vidx=vidx, asview=True); 1306 ; 1307 def _remove_unused_categories(self, df_full, df_sub, uns):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx); 662 if not isinstance(X, AnnData):; 663 raise ValueError('`X` has to be an AnnData object.'); --> 664 self._init_as_view(X, oidx, vidx); 665 else:; 666 self._init_as_actual(. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _init_as_view(self, adata_ref, oidx, vidx); 713 raise KeyError('Unknown Index type'); 714 # fix categories; --> 715 self._remove_unused_categories(adata_ref.obs, obs_sub, uns_new); 716 self._remove_unused_categories(adata_ref.var, var_sub, uns_new); 717 # set attributes. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _remove_unused_categories(self, df_full, df_sub, uns); 1318 uns[k + '_colors'] = np.array(uns[k + '_colors'])[; 1319 np.where(np.in1d(; -> 1320 all_categories, df_sub[k].cat.categories))[0]]; 1321 ; 1322 def rename_categories(self, key, categories):. IndexError: index 7 is out of bounds for axis 1 with size 7; ```. even though it's part of the set:; ```py; >>> set(tiss.obs['cell_ontology_class']); {'B cell',; 'NA',; '",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/226#issuecomment-438879520
https://github.com/scverse/scanpy/issues/226#issuecomment-438963856:235,Safety,avoid,avoid,235,"It should work. Since what fails is some clean up of annData categories,; you may want to check that `tiss.obs['cell_ontology_class']` is set as; categorical. Also check `'B cell' in; tiss.obs['cell_ontology_class'].cat.categories` to avoid a typo. Finally, I think that I had a similar issue which I resolved by removing a adata.uns elements that end in `_colors`. In your case I think you would need to do `del tiss.uns['cell_ontology_class_colors]`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/226#issuecomment-438963856
https://github.com/scverse/scanpy/issues/227#issuecomment-411656281:64,Deployability,update,update,64,"Thank you, this obviously makes sense. I'll first merge another update on plotting functions and then get back to this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/227#issuecomment-411656281
https://github.com/scverse/scanpy/pull/228#issuecomment-411039951:174,Availability,reliab,reliably,174,"@gokceneraslan I like the idea, that would save quite some space. However, as a partial color blind person, I tend to avoid legends based on color because I can not map them reliably back to the figure. But otherwise, I think it is easy to adapt the current code to add such feature.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/228#issuecomment-411039951
https://github.com/scverse/scanpy/pull/228#issuecomment-411039951:240,Energy Efficiency,adapt,adapt,240,"@gokceneraslan I like the idea, that would save quite some space. However, as a partial color blind person, I tend to avoid legends based on color because I can not map them reliably back to the figure. But otherwise, I think it is easy to adapt the current code to add such feature.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/228#issuecomment-411039951
https://github.com/scverse/scanpy/pull/228#issuecomment-411039951:240,Modifiability,adapt,adapt,240,"@gokceneraslan I like the idea, that would save quite some space. However, as a partial color blind person, I tend to avoid legends based on color because I can not map them reliably back to the figure. But otherwise, I think it is easy to adapt the current code to add such feature.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/228#issuecomment-411039951
https://github.com/scverse/scanpy/pull/228#issuecomment-411039951:118,Safety,avoid,avoid,118,"@gokceneraslan I like the idea, that would save quite some space. However, as a partial color blind person, I tend to avoid legends based on color because I can not map them reliably back to the figure. But otherwise, I think it is easy to adapt the current code to add such feature.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/228#issuecomment-411039951
https://github.com/scverse/scanpy/pull/228#issuecomment-411070568:161,Security,access,access,161,I don't know why the tests related to violin plots fail. I assume that it has to do with different libraries that produce slightly different shapes. But without access to the images generated during the testing would be difficult to say.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/228#issuecomment-411070568
https://github.com/scverse/scanpy/pull/228#issuecomment-411070568:21,Testability,test,tests,21,I don't know why the tests related to violin plots fail. I assume that it has to do with different libraries that produce slightly different shapes. But without access to the images generated during the testing would be difficult to say.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/228#issuecomment-411070568
https://github.com/scverse/scanpy/pull/228#issuecomment-411070568:203,Testability,test,testing,203,I don't know why the tests related to violin plots fail. I assume that it has to do with different libraries that produce slightly different shapes. But without access to the images generated during the testing would be difficult to say.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/228#issuecomment-411070568
https://github.com/scverse/scanpy/issues/229#issuecomment-411656016:28,Availability,error,error,28,"Thanks for pointing out the error in the documentation. This only affects the scatter plots. I observed that most people I interact with prefer the scatter plots without the frames. But we can change this back if it worries you. No problem. You have become the plotting expert for Scanpy, I'd say; so your opinion obviously counts a lot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/229#issuecomment-411656016
https://github.com/scverse/scanpy/issues/230#issuecomment-411603192:34,Integrability,protocol,protocol,34,"Note also, when I follow the same protocol for a similar dataset (different timepoint for sequencing), regressing out does not cause this problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230#issuecomment-411603192
https://github.com/scverse/scanpy/issues/230#issuecomment-412098297:1744,Performance,load,loading,1744,"kages/anndata/base.py"", line 1205, in __getitem__; return self._getitem_view(index); File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 1209, in _getitem_view; return AnnData(self, oidx=oidx, vidx=vidx, asview=True); File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 635, in __init__; self._init_as_view(X, oidx, vidx); File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 661, in _init_as_view; var_sub = adata_ref.var.iloc[vidx_normalized]; File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1478, in __getitem__; return self._getitem_axis(maybe_callable, axis=axis); File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 2087, in _getitem_axis; return self._getbool_axis(key, axis=axis); File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1494, in _getbool_axis; inds, = key.nonzero(); ValueError: too many values to unpack (expected 1); ```. I've tried several variations of this yet I don't see why your command wouldn't work, it seems like it should do what you intend .. . Note however, I ran:. ```py; print(np.any(adata.X.sum(axis=0) == 0)) # True; print(np.any(adata.X.sum(axis=1) == 0)) # False; ```. right after loading the dataset and it still shows True and False, yet if I were to regress out WITHOUT removing cell types via:. ```py; keep_cells = [i for i in adata.obs.index if i not in adata_blood.obs.index]; adata = adata[keep_cells, :]; ```. or . ```py; adata = adata[adata.obs['blood'] < 0.25, :] # classification score threshold for blood cells ; ```. ... the regression will work. However, once I remove, it won't. I could try to remove the 0 columns with R, unless you have another suggestion?. Thank you for any feedback.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230#issuecomment-412098297
https://github.com/scverse/scanpy/issues/230#issuecomment-412098297:2256,Usability,feedback,feedback,2256,"kages/anndata/base.py"", line 1205, in __getitem__; return self._getitem_view(index); File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 1209, in _getitem_view; return AnnData(self, oidx=oidx, vidx=vidx, asview=True); File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 635, in __init__; self._init_as_view(X, oidx, vidx); File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 661, in _init_as_view; var_sub = adata_ref.var.iloc[vidx_normalized]; File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1478, in __getitem__; return self._getitem_axis(maybe_callable, axis=axis); File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 2087, in _getitem_axis; return self._getbool_axis(key, axis=axis); File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1494, in _getbool_axis; inds, = key.nonzero(); ValueError: too many values to unpack (expected 1); ```. I've tried several variations of this yet I don't see why your command wouldn't work, it seems like it should do what you intend .. . Note however, I ran:. ```py; print(np.any(adata.X.sum(axis=0) == 0)) # True; print(np.any(adata.X.sum(axis=1) == 0)) # False; ```. right after loading the dataset and it still shows True and False, yet if I were to regress out WITHOUT removing cell types via:. ```py; keep_cells = [i for i in adata.obs.index if i not in adata_blood.obs.index]; adata = adata[keep_cells, :]; ```. or . ```py; adata = adata[adata.obs['blood'] < 0.25, :] # classification score threshold for blood cells ; ```. ... the regression will work. However, once I remove, it won't. I could try to remove the 0 columns with R, unless you have another suggestion?. Thank you for any feedback.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230#issuecomment-412098297
https://github.com/scverse/scanpy/issues/230#issuecomment-412105475:2046,Performance,load,loading,2046,"ase.py"",; > line 635, in *init*; > self._init_as_view(X, oidx, vidx); > File; > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",; > line 661, in _init_as_view; > var_sub = adata_ref.var.iloc[vidx_normalized]; > File; > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",; > line 1478, in *getitem*; > return self._getitem_axis(maybe_callable, axis=axis); > File; > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",; > line 2087, in _getitem_axis; > return self._getbool_axis(key, axis=axis); > File; > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",; > line 1494, in _getbool_axis; > inds, = key.nonzero(); > ValueError: too many values to unpack (expected 1); >; > I've tried several variations of this yet I don't see why your command; > wouldn't work, it seems like it should do what you intend ..; >; > Note however, I ran :; >; > print(np.any(adata.X.sum(axis=0) == 0)) # True; > print(np.any(adata.X.sum(axis=1) == 0)) # False; >; > right after loading the dataset and it still shows True and False, yet if; > I were to regress out WITHOUT removing cell types via:; >; > Temp = [i for i in adata.obs.index if i not in adata_blood.obs.index]; > adata = adata[Temp,:]; >; > or; >; > adata = adata[adata.obs['blood'] < 0.25, :] # classification score threshold for blood cells; >; > ... the regression will work. However, once I remove, it won't. I could; > try to remove the 0 columns with R, unless you have another suggestion?; >; > Thank you for any feedback.; >; > —; > You are receiving this because you commented.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/230#issuecomment-412098297>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1bD5gUmq-bUVfAJT2AGlXKXEkOmxks5uPZfTgaJpZM4V0Faw>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230#issuecomment-412105475
https://github.com/scverse/scanpy/issues/230#issuecomment-412105475:2552,Usability,feedback,feedback,2552,"ase.py"",; > line 635, in *init*; > self._init_as_view(X, oidx, vidx); > File; > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",; > line 661, in _init_as_view; > var_sub = adata_ref.var.iloc[vidx_normalized]; > File; > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",; > line 1478, in *getitem*; > return self._getitem_axis(maybe_callable, axis=axis); > File; > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",; > line 2087, in _getitem_axis; > return self._getbool_axis(key, axis=axis); > File; > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",; > line 1494, in _getbool_axis; > inds, = key.nonzero(); > ValueError: too many values to unpack (expected 1); >; > I've tried several variations of this yet I don't see why your command; > wouldn't work, it seems like it should do what you intend ..; >; > Note however, I ran :; >; > print(np.any(adata.X.sum(axis=0) == 0)) # True; > print(np.any(adata.X.sum(axis=1) == 0)) # False; >; > right after loading the dataset and it still shows True and False, yet if; > I were to regress out WITHOUT removing cell types via:; >; > Temp = [i for i in adata.obs.index if i not in adata_blood.obs.index]; > adata = adata[Temp,:]; >; > or; >; > adata = adata[adata.obs['blood'] < 0.25, :] # classification score threshold for blood cells; >; > ... the regression will work. However, once I remove, it won't. I could; > try to remove the 0 columns with R, unless you have another suggestion?; >; > Thank you for any feedback.; >; > —; > You are receiving this because you commented.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/230#issuecomment-412098297>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1bD5gUmq-bUVfAJT2AGlXKXEkOmxks5uPZfTgaJpZM4V0Faw>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230#issuecomment-412105475
https://github.com/scverse/scanpy/issues/230#issuecomment-412237509:119,Performance,load,loaded,119,"Thanks fellas, it worked. . ```py; pp.filter_genes(adata, min_counts=1); ``` ; Weird, because the other datasets, when loaded fresh have the same pattern:; ```py; print(np.any(adata.X.sum(axis=0) == 0)) # True ; print(np.any(adata.X.sum(axis=1) == 0)) # False; ```. Before and after removal of cell types. Yet they still regress out fine. Anyways, big help. . Cheers",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230#issuecomment-412237509
https://github.com/scverse/scanpy/issues/230#issuecomment-596790394:93,Availability,error,error,93,"teresting that if I ran your PBMC tutorial without filtering out the non-HVG then I get this error. But I thought these filtering steps in the beginning already eliminated the empty rows and columns?; ```py; sc.pp.filter_cells(adata, min_genes=200); sc.pp.filter_genes(adata, min_cells=3); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230#issuecomment-596790394
https://github.com/scverse/scanpy/issues/231#issuecomment-412140748:128,Usability,simpl,simply,128,"Hm, very strange. Generally: `groups` is somehow reminiscent of times when AnnData didn't have views. Today, I'd say one should simply pass a subsetted AnnData (by default a view). What about deprecating the `groups` parameter?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/231#issuecomment-412140748
https://github.com/scverse/scanpy/issues/231#issuecomment-414236960:226,Availability,error,error,226,"umap expects a list as group, so it will work if you do:. ```python; sc.pl.umap(adata, color='blobs', groups=['Zero']); ````. the improvement that I would consider is to automatically convert a string into a list to avoid the error message.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/231#issuecomment-414236960
https://github.com/scverse/scanpy/issues/231#issuecomment-414236960:232,Integrability,message,message,232,"umap expects a list as group, so it will work if you do:. ```python; sc.pl.umap(adata, color='blobs', groups=['Zero']); ````. the improvement that I would consider is to automatically convert a string into a list to avoid the error message.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/231#issuecomment-414236960
https://github.com/scverse/scanpy/issues/231#issuecomment-414236960:216,Safety,avoid,avoid,216,"umap expects a list as group, so it will work if you do:. ```python; sc.pl.umap(adata, color='blobs', groups=['Zero']); ````. the improvement that I would consider is to automatically convert a string into a list to avoid the error message.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/231#issuecomment-414236960
https://github.com/scverse/scanpy/issues/232#issuecomment-414240268:6,Security,validat,validate,6,"I can validate this problem. Actually, any value from C0 to C9 causes the problem but each case gets a different color. ![image](https://user-images.githubusercontent.com/4964309/44329411-a82ce700-a464-11e8-940c-c85a67bf579a.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/232#issuecomment-414240268
https://github.com/scverse/scanpy/pull/236#issuecomment-414600952:10,Modifiability,layers,layers,10,Are these layers for the velocyto implementation? I.e. spliced and unspliced count data layers. And is this then also usable for different layers of data processing?. Or am I missing the point entirely here?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/236#issuecomment-414600952
https://github.com/scverse/scanpy/pull/236#issuecomment-414600952:88,Modifiability,layers,layers,88,Are these layers for the velocyto implementation? I.e. spliced and unspliced count data layers. And is this then also usable for different layers of data processing?. Or am I missing the point entirely here?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/236#issuecomment-414600952
https://github.com/scverse/scanpy/pull/236#issuecomment-414600952:139,Modifiability,layers,layers,139,Are these layers for the velocyto implementation? I.e. spliced and unspliced count data layers. And is this then also usable for different layers of data processing?. Or am I missing the point entirely here?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/236#issuecomment-414600952
https://github.com/scverse/scanpy/pull/236#issuecomment-414600952:118,Usability,usab,usable,118,Are these layers for the velocyto implementation? I.e. spliced and unspliced count data layers. And is this then also usable for different layers of data processing?. Or am I missing the point entirely here?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/236#issuecomment-414600952
https://github.com/scverse/scanpy/pull/236#issuecomment-414602407:42,Modifiability,layers,layers,42,"This is just being built up, but yes, the layers have the same constraints as in loom files and are meant to enable the transformation of loom files to AnnData objects without loss of information. Here's the stub of the docs: https://anndata.readthedocs.io/en/latest/anndata.AnnData.layers.html. Here's the very first draft https://github.com/theislab/scvelo of our stochastic RNA velocity package... There will be more information on this very soon. :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/236#issuecomment-414602407
https://github.com/scverse/scanpy/pull/236#issuecomment-414602407:283,Modifiability,layers,layers,283,"This is just being built up, but yes, the layers have the same constraints as in loom files and are meant to enable the transformation of loom files to AnnData objects without loss of information. Here's the stub of the docs: https://anndata.readthedocs.io/en/latest/anndata.AnnData.layers.html. Here's the very first draft https://github.com/theislab/scvelo of our stochastic RNA velocity package... There will be more information on this very soon. :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/236#issuecomment-414602407
https://github.com/scverse/scanpy/pull/236#issuecomment-414602407:208,Testability,stub,stub,208,"This is just being built up, but yes, the layers have the same constraints as in loom files and are meant to enable the transformation of loom files to AnnData objects without loss of information. Here's the stub of the docs: https://anndata.readthedocs.io/en/latest/anndata.AnnData.layers.html. Here's the very first draft https://github.com/theislab/scvelo of our stochastic RNA velocity package... There will be more information on this very soon. :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/236#issuecomment-414602407
https://github.com/scverse/scanpy/pull/236#issuecomment-414606358:349,Energy Efficiency,adapt,adapt,349,"I'm not that familiar with loom files, but it definitely sounds good ;). So I guess every tool and plotting function would need a layers argument to determine which layer it should work on? At least if layers is implemented in a sufficiently general way to be used for data processing layers as well. Any idea on the timeline for this? Then I could adapt the tutorial to work with this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/236#issuecomment-414606358
https://github.com/scverse/scanpy/pull/236#issuecomment-414606358:130,Modifiability,layers,layers,130,"I'm not that familiar with loom files, but it definitely sounds good ;). So I guess every tool and plotting function would need a layers argument to determine which layer it should work on? At least if layers is implemented in a sufficiently general way to be used for data processing layers as well. Any idea on the timeline for this? Then I could adapt the tutorial to work with this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/236#issuecomment-414606358
https://github.com/scverse/scanpy/pull/236#issuecomment-414606358:202,Modifiability,layers,layers,202,"I'm not that familiar with loom files, but it definitely sounds good ;). So I guess every tool and plotting function would need a layers argument to determine which layer it should work on? At least if layers is implemented in a sufficiently general way to be used for data processing layers as well. Any idea on the timeline for this? Then I could adapt the tutorial to work with this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/236#issuecomment-414606358
https://github.com/scverse/scanpy/pull/236#issuecomment-414606358:285,Modifiability,layers,layers,285,"I'm not that familiar with loom files, but it definitely sounds good ;). So I guess every tool and plotting function would need a layers argument to determine which layer it should work on? At least if layers is implemented in a sufficiently general way to be used for data processing layers as well. Any idea on the timeline for this? Then I could adapt the tutorial to work with this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/236#issuecomment-414606358
https://github.com/scverse/scanpy/pull/236#issuecomment-414606358:349,Modifiability,adapt,adapt,349,"I'm not that familiar with loom files, but it definitely sounds good ;). So I guess every tool and plotting function would need a layers argument to determine which layer it should work on? At least if layers is implemented in a sufficiently general way to be used for data processing layers as well. Any idea on the timeline for this? Then I could adapt the tutorial to work with this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/236#issuecomment-414606358
https://github.com/scverse/scanpy/issues/240#issuecomment-415728359:11,Testability,test,test,11,"As a quick test, I tried the weighted version of the louvain method and it was able to identify small clusters that are no identified with the non weighted louvain. However, I did not use `knn=False` as this does not work well with the UMAP representation. Still, I could see differences (eg. cluster 6 and 9 in the top figure):. ![image](https://user-images.githubusercontent.com/4964309/44581659-e85ed300-a79e-11e8-8236-cc149e9c17d4.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240#issuecomment-415728359
https://github.com/scverse/scanpy/issues/240#issuecomment-415853701:355,Performance,perform,perform,355,"@fidelram Can you change the resolution of the non-weighted version to reproduce a clustering similar to the weighted case? Weighting can change the scale of the resolution parameter. I would assume clustering on a weighted, fully connected network would be a lot more computationally expensive. I'm curious if it's worth the additional cost. It may also perform worse as you are putting more emphasis on the euclidean distances between transcriptomes than you may want (are the values more important or only the order of the distances?).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240#issuecomment-415853701
https://github.com/scverse/scanpy/issues/240#issuecomment-415956113:1080,Modifiability,flexible,flexible,1080,"Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary *k* neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. From some preliminary attempts of my own, it seems clustering solutions become more stable with respect to parameter choice when I use the `method=gauss, knn=False` weighted network. I'm still in the process of verifying this, however. I would also note that the documentation for `sc.tl.louvain` references [this](; https://doi.org/10.1016/j.cell.2015.05.047) paper (the Phenograph method), which uses the louvain method on a a weighted graph. If the method is cited, why not allow using it?. Just from a package design/ usability perspective, I think it's nice to include. It would make the package more flexible and allows the user to take more advantage of the `louvain-igraph` library. If the user could also specify the kind of partition used, even better. @LuckyMD, it's definitely more memory intensive, but I'm not sure it's prohibitively computationally expensive. Also weights don't have to be based on the euclidean distance (`Phenograph` uses Jaccard distances between nodes' neighborhoods) and there's [some evidence](; https://doi.org/10.1093/bib/bby076) to suggest we should using correlation based distance metrics anyways.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240#issuecomment-415956113
https://github.com/scverse/scanpy/issues/240#issuecomment-415956113:330,Performance,perform,perform,330,"Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary *k* neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. From some preliminary attempts of my own, it seems clustering solutions become more stable with respect to parameter choice when I use the `method=gauss, knn=False` weighted network. I'm still in the process of verifying this, however. I would also note that the documentation for `sc.tl.louvain` references [this](; https://doi.org/10.1016/j.cell.2015.05.047) paper (the Phenograph method), which uses the louvain method on a a weighted graph. If the method is cited, why not allow using it?. Just from a package design/ usability perspective, I think it's nice to include. It would make the package more flexible and allows the user to take more advantage of the `louvain-igraph` library. If the user could also specify the kind of partition used, even better. @LuckyMD, it's definitely more memory intensive, but I'm not sure it's prohibitively computationally expensive. Also weights don't have to be based on the euclidean distance (`Phenograph` uses Jaccard distances between nodes' neighborhoods) and there's [some evidence](; https://doi.org/10.1093/bib/bby076) to suggest we should using correlation based distance metrics anyways.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240#issuecomment-415956113
https://github.com/scverse/scanpy/issues/240#issuecomment-415956113:0,Usability,Intuit,Intuitively,0,"Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary *k* neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. From some preliminary attempts of my own, it seems clustering solutions become more stable with respect to parameter choice when I use the `method=gauss, knn=False` weighted network. I'm still in the process of verifying this, however. I would also note that the documentation for `sc.tl.louvain` references [this](; https://doi.org/10.1016/j.cell.2015.05.047) paper (the Phenograph method), which uses the louvain method on a a weighted graph. If the method is cited, why not allow using it?. Just from a package design/ usability perspective, I think it's nice to include. It would make the package more flexible and allows the user to take more advantage of the `louvain-igraph` library. If the user could also specify the kind of partition used, even better. @LuckyMD, it's definitely more memory intensive, but I'm not sure it's prohibitively computationally expensive. Also weights don't have to be based on the euclidean distance (`Phenograph` uses Jaccard distances between nodes' neighborhoods) and there's [some evidence](; https://doi.org/10.1093/bib/bby076) to suggest we should using correlation based distance metrics anyways.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240#issuecomment-415956113
https://github.com/scverse/scanpy/issues/240#issuecomment-415956113:996,Usability,usab,usability,996,"Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary *k* neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. From some preliminary attempts of my own, it seems clustering solutions become more stable with respect to parameter choice when I use the `method=gauss, knn=False` weighted network. I'm still in the process of verifying this, however. I would also note that the documentation for `sc.tl.louvain` references [this](; https://doi.org/10.1016/j.cell.2015.05.047) paper (the Phenograph method), which uses the louvain method on a a weighted graph. If the method is cited, why not allow using it?. Just from a package design/ usability perspective, I think it's nice to include. It would make the package more flexible and allows the user to take more advantage of the `louvain-igraph` library. If the user could also specify the kind of partition used, even better. @LuckyMD, it's definitely more memory intensive, but I'm not sure it's prohibitively computationally expensive. Also weights don't have to be based on the euclidean distance (`Phenograph` uses Jaccard distances between nodes' neighborhoods) and there's [some evidence](; https://doi.org/10.1093/bib/bby076) to suggest we should using correlation based distance metrics anyways.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240#issuecomment-415956113
https://github.com/scverse/scanpy/issues/240#issuecomment-416161676:614,Availability,robust,robust,614,"I see no reason why the possibility shouldn't exist to run the weighted version on the full graph. I'm still curious about the quality of the outcome though. Using protein-protein interaction data, I've noticed that similarity scores perform worse than using network neighbourhoods based on cutoffs to cluster data (this does not have to be the case for scRNA-seq of course). In the latter case you require cells to be each others nearest neighbours to create dense network regions, rather than highly similar transcriptomes based on one calculation of similarity. I would have thought the cutoff approach is more robust to changing similarity metrics as well. It's definitely worth testing this though. Maybe I'm just too skeptical of similarity metrics over all. @fidelram do you have labels on your data where you could verify the quality of those two partitions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240#issuecomment-416161676
https://github.com/scverse/scanpy/issues/240#issuecomment-416161676:234,Performance,perform,perform,234,"I see no reason why the possibility shouldn't exist to run the weighted version on the full graph. I'm still curious about the quality of the outcome though. Using protein-protein interaction data, I've noticed that similarity scores perform worse than using network neighbourhoods based on cutoffs to cluster data (this does not have to be the case for scRNA-seq of course). In the latter case you require cells to be each others nearest neighbours to create dense network regions, rather than highly similar transcriptomes based on one calculation of similarity. I would have thought the cutoff approach is more robust to changing similarity metrics as well. It's definitely worth testing this though. Maybe I'm just too skeptical of similarity metrics over all. @fidelram do you have labels on your data where you could verify the quality of those two partitions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240#issuecomment-416161676
https://github.com/scverse/scanpy/issues/240#issuecomment-416161676:683,Testability,test,testing,683,"I see no reason why the possibility shouldn't exist to run the weighted version on the full graph. I'm still curious about the quality of the outcome though. Using protein-protein interaction data, I've noticed that similarity scores perform worse than using network neighbourhoods based on cutoffs to cluster data (this does not have to be the case for scRNA-seq of course). In the latter case you require cells to be each others nearest neighbours to create dense network regions, rather than highly similar transcriptomes based on one calculation of similarity. I would have thought the cutoff approach is more robust to changing similarity metrics as well. It's definitely worth testing this though. Maybe I'm just too skeptical of similarity metrics over all. @fidelram do you have labels on your data where you could verify the quality of those two partitions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240#issuecomment-416161676
https://github.com/scverse/scanpy/issues/240#issuecomment-416207545:70,Performance,perform,performs,70,"@fidelram So based on that could you say that the non-weighted method performs better for cluster 10 (PNEC/Brush cluster) as it is identified in this partition, but merged with other cells in cluster 3 in the weighted partition?. I guess it might take a more thorough analysis to make those types of conclusions though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240#issuecomment-416207545
https://github.com/scverse/scanpy/issues/240#issuecomment-416437517:167,Security,access,access,167,"So, I'll go ahead and start a pull request?. Something I think could be useful to include in implementing this is allowing multiple network representations with keyed access (similar to `use_rep`). This would be useful for the cases where you want to cluster on a network that would be inappropriate to use for UMAP.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240#issuecomment-416437517
https://github.com/scverse/scanpy/issues/240#issuecomment-416725777:1915,Availability,avail,available,1915,"ed graph is prohibitive computationally (memory and CPU time wise).; > Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary k neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. That's just speculation to me. I never saw convincing benchmarks. No one claims that ""discounting all distance information gives a more accurate result"". It's just that it's computationally cheaper. I acknowledge that a ""non-fixed-degree knn graph"" varying say, between 5 and 100, would be computationally tractable and would carry information about the sampling density of the data in the given representation. This information is only indirectly available in the fixed-degree knn graph (more loops etc. in high-density regions). I never investigated this as I never saw fundamental results on such a non-fixed-degree knn graph. As it's also hard to benchmark this, I'd be afraid of getting into this if one doesn't have the time to get the fundamentals right. I want to note that even in the context of diffusion processes, we managed to obtain meaningful results with kNN graphs in practice. And this clearly contradicts the fundamental results found in all the Coifman papers. Having said that: if the code is simple, I don't mind at all to have the possibility that you suggest, @ivirshup. Please go ahead with a pull request and I'll see whether the changes are simple enough. The user will still use the default plain knn version, which is also what is done in Seurat. But my philosophy rests the same: rather than engineering the clustering or any other aspect of the manifold analysis, one should engineer the representation. Sorry that this got a b",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240#issuecomment-416725777
https://github.com/scverse/scanpy/issues/240#issuecomment-416725777:1323,Performance,perform,perform,1323," the data (points belonging to one clusters should be connected). Any graph clustering algorithm respects that, even spectral clustering. So, I'm not a big fan of trying 5 clustering algorithms to produce sensible results. Either a given representation of the data clusters clearly or it doesn't. If it doesn't, Louvain clustering just gives you one possible, representative partitioning of the data. But there are many others that are equally meaningful. Similar for other graph clustering algorithms. Now, running Louvain clustering on a fully connected graph is prohibitive computationally (memory and CPU time wise).; > Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary k neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. That's just speculation to me. I never saw convincing benchmarks. No one claims that ""discounting all distance information gives a more accurate result"". It's just that it's computationally cheaper. I acknowledge that a ""non-fixed-degree knn graph"" varying say, between 5 and 100, would be computationally tractable and would carry information about the sampling density of the data in the given representation. This information is only indirectly available in the fixed-degree knn graph (more loops etc. in high-density regions). I never investigated this as I never saw fundamental results on such a non-fixed-degree knn graph. As it's also hard to benchmark this, I'd be afraid of getting into this if one doesn't have the time to get the fundamentals right. I want to note that even in the context of diffusion processes, we managed to obtain meaningful results with kNN graphs in practice. And this c",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240#issuecomment-416725777
https://github.com/scverse/scanpy/issues/240#issuecomment-416725777:134,Safety,detect,detection,134,"Hi all,. thanks for the nice discussion!. Phenograph is cited not for the specific implementation but for suggesting to use community detection for clustering in single-cell data. This is what the docs state ""The Louvain algorithm has been proposed for single-cell analysis by [Levine15]."". My opinion on clustering algorithms: use something that respects the topology of the data (points belonging to one clusters should be connected). Any graph clustering algorithm respects that, even spectral clustering. So, I'm not a big fan of trying 5 clustering algorithms to produce sensible results. Either a given representation of the data clusters clearly or it doesn't. If it doesn't, Louvain clustering just gives you one possible, representative partitioning of the data. But there are many others that are equally meaningful. Similar for other graph clustering algorithms. Now, running Louvain clustering on a fully connected graph is prohibitive computationally (memory and CPU time wise).; > Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary k neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. That's just speculation to me. I never saw convincing benchmarks. No one claims that ""discounting all distance information gives a more accurate result"". It's just that it's computationally cheaper. I acknowledge that a ""non-fixed-degree knn graph"" varying say, between 5 and 100, would be computationally tractable and would carry information about the sampling density of the data in the given representation. This information is only indirectly available in the fixed-degree knn graph (more loops etc. in high-density regions). I n",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240#issuecomment-416725777
https://github.com/scverse/scanpy/issues/240#issuecomment-416725777:1521,Testability,benchmark,benchmarks,1521,"'m not a big fan of trying 5 clustering algorithms to produce sensible results. Either a given representation of the data clusters clearly or it doesn't. If it doesn't, Louvain clustering just gives you one possible, representative partitioning of the data. But there are many others that are equally meaningful. Similar for other graph clustering algorithms. Now, running Louvain clustering on a fully connected graph is prohibitive computationally (memory and CPU time wise).; > Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary k neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. That's just speculation to me. I never saw convincing benchmarks. No one claims that ""discounting all distance information gives a more accurate result"". It's just that it's computationally cheaper. I acknowledge that a ""non-fixed-degree knn graph"" varying say, between 5 and 100, would be computationally tractable and would carry information about the sampling density of the data in the given representation. This information is only indirectly available in the fixed-degree knn graph (more loops etc. in high-density regions). I never investigated this as I never saw fundamental results on such a non-fixed-degree knn graph. As it's also hard to benchmark this, I'd be afraid of getting into this if one doesn't have the time to get the fundamentals right. I want to note that even in the context of diffusion processes, we managed to obtain meaningful results with kNN graphs in practice. And this clearly contradicts the fundamental results found in all the Coifman papers. Having said that: if the code is simple, I don't mind at all to hav",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240#issuecomment-416725777
https://github.com/scverse/scanpy/issues/240#issuecomment-416725777:2118,Testability,benchmark,benchmark,2118," is more representative of the data than an arbitrary k neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. That's just speculation to me. I never saw convincing benchmarks. No one claims that ""discounting all distance information gives a more accurate result"". It's just that it's computationally cheaper. I acknowledge that a ""non-fixed-degree knn graph"" varying say, between 5 and 100, would be computationally tractable and would carry information about the sampling density of the data in the given representation. This information is only indirectly available in the fixed-degree knn graph (more loops etc. in high-density regions). I never investigated this as I never saw fundamental results on such a non-fixed-degree knn graph. As it's also hard to benchmark this, I'd be afraid of getting into this if one doesn't have the time to get the fundamentals right. I want to note that even in the context of diffusion processes, we managed to obtain meaningful results with kNN graphs in practice. And this clearly contradicts the fundamental results found in all the Coifman papers. Having said that: if the code is simple, I don't mind at all to have the possibility that you suggest, @ivirshup. Please go ahead with a pull request and I'll see whether the changes are simple enough. The user will still use the default plain knn version, which is also what is done in Seurat. But my philosophy rests the same: rather than engineering the clustering or any other aspect of the manifold analysis, one should engineer the representation. Sorry that this got a bit length and confused. I hope I addressed everything but I feel I'm still missing something - I'm currently working through a lot of issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240#issuecomment-416725777
https://github.com/scverse/scanpy/issues/240#issuecomment-416725777:645,Usability,clear,clearly,645,"Hi all,. thanks for the nice discussion!. Phenograph is cited not for the specific implementation but for suggesting to use community detection for clustering in single-cell data. This is what the docs state ""The Louvain algorithm has been proposed for single-cell analysis by [Levine15]."". My opinion on clustering algorithms: use something that respects the topology of the data (points belonging to one clusters should be connected). Any graph clustering algorithm respects that, even spectral clustering. So, I'm not a big fan of trying 5 clustering algorithms to produce sensible results. Either a given representation of the data clusters clearly or it doesn't. If it doesn't, Louvain clustering just gives you one possible, representative partitioning of the data. But there are many others that are equally meaningful. Similar for other graph clustering algorithms. Now, running Louvain clustering on a fully connected graph is prohibitive computationally (memory and CPU time wise).; > Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary k neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. That's just speculation to me. I never saw convincing benchmarks. No one claims that ""discounting all distance information gives a more accurate result"". It's just that it's computationally cheaper. I acknowledge that a ""non-fixed-degree knn graph"" varying say, between 5 and 100, would be computationally tractable and would carry information about the sampling density of the data in the given representation. This information is only indirectly available in the fixed-degree knn graph (more loops etc. in high-density regions). I n",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240#issuecomment-416725777
https://github.com/scverse/scanpy/issues/240#issuecomment-416725777:995,Usability,Intuit,Intuitively,995," not for the specific implementation but for suggesting to use community detection for clustering in single-cell data. This is what the docs state ""The Louvain algorithm has been proposed for single-cell analysis by [Levine15]."". My opinion on clustering algorithms: use something that respects the topology of the data (points belonging to one clusters should be connected). Any graph clustering algorithm respects that, even spectral clustering. So, I'm not a big fan of trying 5 clustering algorithms to produce sensible results. Either a given representation of the data clusters clearly or it doesn't. If it doesn't, Louvain clustering just gives you one possible, representative partitioning of the data. But there are many others that are equally meaningful. Similar for other graph clustering algorithms. Now, running Louvain clustering on a fully connected graph is prohibitive computationally (memory and CPU time wise).; > Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary k neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. That's just speculation to me. I never saw convincing benchmarks. No one claims that ""discounting all distance information gives a more accurate result"". It's just that it's computationally cheaper. I acknowledge that a ""non-fixed-degree knn graph"" varying say, between 5 and 100, would be computationally tractable and would carry information about the sampling density of the data in the given representation. This information is only indirectly available in the fixed-degree knn graph (more loops etc. in high-density regions). I never investigated this as I never saw fundamental results on ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240#issuecomment-416725777
https://github.com/scverse/scanpy/issues/240#issuecomment-416725777:2371,Usability,clear,clearly,2371," is more representative of the data than an arbitrary k neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. That's just speculation to me. I never saw convincing benchmarks. No one claims that ""discounting all distance information gives a more accurate result"". It's just that it's computationally cheaper. I acknowledge that a ""non-fixed-degree knn graph"" varying say, between 5 and 100, would be computationally tractable and would carry information about the sampling density of the data in the given representation. This information is only indirectly available in the fixed-degree knn graph (more loops etc. in high-density regions). I never investigated this as I never saw fundamental results on such a non-fixed-degree knn graph. As it's also hard to benchmark this, I'd be afraid of getting into this if one doesn't have the time to get the fundamentals right. I want to note that even in the context of diffusion processes, we managed to obtain meaningful results with kNN graphs in practice. And this clearly contradicts the fundamental results found in all the Coifman papers. Having said that: if the code is simple, I don't mind at all to have the possibility that you suggest, @ivirshup. Please go ahead with a pull request and I'll see whether the changes are simple enough. The user will still use the default plain knn version, which is also what is done in Seurat. But my philosophy rests the same: rather than engineering the clustering or any other aspect of the manifold analysis, one should engineer the representation. Sorry that this got a bit length and confused. I hope I addressed everything but I feel I'm still missing something - I'm currently working through a lot of issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240#issuecomment-416725777
https://github.com/scverse/scanpy/issues/240#issuecomment-416725777:2481,Usability,simpl,simple,2481," is more representative of the data than an arbitrary k neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. That's just speculation to me. I never saw convincing benchmarks. No one claims that ""discounting all distance information gives a more accurate result"". It's just that it's computationally cheaper. I acknowledge that a ""non-fixed-degree knn graph"" varying say, between 5 and 100, would be computationally tractable and would carry information about the sampling density of the data in the given representation. This information is only indirectly available in the fixed-degree knn graph (more loops etc. in high-density regions). I never investigated this as I never saw fundamental results on such a non-fixed-degree knn graph. As it's also hard to benchmark this, I'd be afraid of getting into this if one doesn't have the time to get the fundamentals right. I want to note that even in the context of diffusion processes, we managed to obtain meaningful results with kNN graphs in practice. And this clearly contradicts the fundamental results found in all the Coifman papers. Having said that: if the code is simple, I don't mind at all to have the possibility that you suggest, @ivirshup. Please go ahead with a pull request and I'll see whether the changes are simple enough. The user will still use the default plain knn version, which is also what is done in Seurat. But my philosophy rests the same: rather than engineering the clustering or any other aspect of the manifold analysis, one should engineer the representation. Sorry that this got a bit length and confused. I hope I addressed everything but I feel I'm still missing something - I'm currently working through a lot of issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240#issuecomment-416725777
https://github.com/scverse/scanpy/issues/240#issuecomment-416725777:2635,Usability,simpl,simple,2635," is more representative of the data than an arbitrary k neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. That's just speculation to me. I never saw convincing benchmarks. No one claims that ""discounting all distance information gives a more accurate result"". It's just that it's computationally cheaper. I acknowledge that a ""non-fixed-degree knn graph"" varying say, between 5 and 100, would be computationally tractable and would carry information about the sampling density of the data in the given representation. This information is only indirectly available in the fixed-degree knn graph (more loops etc. in high-density regions). I never investigated this as I never saw fundamental results on such a non-fixed-degree knn graph. As it's also hard to benchmark this, I'd be afraid of getting into this if one doesn't have the time to get the fundamentals right. I want to note that even in the context of diffusion processes, we managed to obtain meaningful results with kNN graphs in practice. And this clearly contradicts the fundamental results found in all the Coifman papers. Having said that: if the code is simple, I don't mind at all to have the possibility that you suggest, @ivirshup. Please go ahead with a pull request and I'll see whether the changes are simple enough. The user will still use the default plain knn version, which is also what is done in Seurat. But my philosophy rests the same: rather than engineering the clustering or any other aspect of the manifold analysis, one should engineer the representation. Sorry that this got a bit length and confused. I hope I addressed everything but I feel I'm still missing something - I'm currently working through a lot of issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240#issuecomment-416725777
https://github.com/scverse/scanpy/issues/240#issuecomment-416730034:182,Usability,learn,learning,182,"Ah, what I missed is the statement about metrics: I know that many people play around with different metrics. But then you mix ""representation engineering"" (preprocessing or machine learning) with manifold analysis. I'd say the cleanest is to always just use Euclidean distance and all the other work should be done already before.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240#issuecomment-416730034
https://github.com/scverse/scanpy/issues/240#issuecomment-416821973:286,Integrability,depend,depend,286,I'd agree with your statement that engineering the representation is more important than the analysis. I view my goal here as allowing more representations as input. Would you mind saying more about why you thing using different metics is less clean (simple?)? I would think that would depend on what representation you're calculating the distances on.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240#issuecomment-416821973
https://github.com/scverse/scanpy/issues/240#issuecomment-416821973:251,Usability,simpl,simple,251,I'd agree with your statement that engineering the representation is more important than the analysis. I view my goal here as allowing more representations as input. Would you mind saying more about why you thing using different metics is less clean (simple?)? I would think that would depend on what representation you're calculating the distances on.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240#issuecomment-416821973
https://github.com/scverse/scanpy/issues/240#issuecomment-424802342:268,Performance,optimiz,optimization,268,"I guess we can close with this. And sorry, I forgot to answer the above:; > I view my goal here as allowing more representations as input. When I say ""representation"", I mean a feature space representation, which is directly amenable to differentiable mappings, hence optimization and learning. When you say ""graph representation"" that might be a legit notion, too; and one can definitely think about learning different graph representations (by transforming them back to a vector space). But to me, it appears much more reasonable and straight forward to do all the inference on the feature space representation. And one should do it an a way so that the applied metric used to analyze the arising manifold in the learned representation does make sense. Of course, if you work directly with raw data, using different metrics can capture a lot of what you'd otherwise need to learn or preprocess (invariance to scales, ...). Hope this helps a little bit.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240#issuecomment-424802342
https://github.com/scverse/scanpy/issues/240#issuecomment-424802342:285,Usability,learn,learning,285,"I guess we can close with this. And sorry, I forgot to answer the above:; > I view my goal here as allowing more representations as input. When I say ""representation"", I mean a feature space representation, which is directly amenable to differentiable mappings, hence optimization and learning. When you say ""graph representation"" that might be a legit notion, too; and one can definitely think about learning different graph representations (by transforming them back to a vector space). But to me, it appears much more reasonable and straight forward to do all the inference on the feature space representation. And one should do it an a way so that the applied metric used to analyze the arising manifold in the learned representation does make sense. Of course, if you work directly with raw data, using different metrics can capture a lot of what you'd otherwise need to learn or preprocess (invariance to scales, ...). Hope this helps a little bit.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240#issuecomment-424802342
https://github.com/scverse/scanpy/issues/240#issuecomment-424802342:401,Usability,learn,learning,401,"I guess we can close with this. And sorry, I forgot to answer the above:; > I view my goal here as allowing more representations as input. When I say ""representation"", I mean a feature space representation, which is directly amenable to differentiable mappings, hence optimization and learning. When you say ""graph representation"" that might be a legit notion, too; and one can definitely think about learning different graph representations (by transforming them back to a vector space). But to me, it appears much more reasonable and straight forward to do all the inference on the feature space representation. And one should do it an a way so that the applied metric used to analyze the arising manifold in the learned representation does make sense. Of course, if you work directly with raw data, using different metrics can capture a lot of what you'd otherwise need to learn or preprocess (invariance to scales, ...). Hope this helps a little bit.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240#issuecomment-424802342
https://github.com/scverse/scanpy/issues/240#issuecomment-424802342:715,Usability,learn,learned,715,"I guess we can close with this. And sorry, I forgot to answer the above:; > I view my goal here as allowing more representations as input. When I say ""representation"", I mean a feature space representation, which is directly amenable to differentiable mappings, hence optimization and learning. When you say ""graph representation"" that might be a legit notion, too; and one can definitely think about learning different graph representations (by transforming them back to a vector space). But to me, it appears much more reasonable and straight forward to do all the inference on the feature space representation. And one should do it an a way so that the applied metric used to analyze the arising manifold in the learned representation does make sense. Of course, if you work directly with raw data, using different metrics can capture a lot of what you'd otherwise need to learn or preprocess (invariance to scales, ...). Hope this helps a little bit.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240#issuecomment-424802342
https://github.com/scverse/scanpy/issues/240#issuecomment-424802342:876,Usability,learn,learn,876,"I guess we can close with this. And sorry, I forgot to answer the above:; > I view my goal here as allowing more representations as input. When I say ""representation"", I mean a feature space representation, which is directly amenable to differentiable mappings, hence optimization and learning. When you say ""graph representation"" that might be a legit notion, too; and one can definitely think about learning different graph representations (by transforming them back to a vector space). But to me, it appears much more reasonable and straight forward to do all the inference on the feature space representation. And one should do it an a way so that the applied metric used to analyze the arising manifold in the learned representation does make sense. Of course, if you work directly with raw data, using different metrics can capture a lot of what you'd otherwise need to learn or preprocess (invariance to scales, ...). Hope this helps a little bit.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240#issuecomment-424802342
https://github.com/scverse/scanpy/issues/242#issuecomment-416231775:11,Availability,error,error,11,"I had this error in the past. Try with a different mirror, you can look them from [here](http://www.ensembl.org/info/about/mirrors.html). For example, I usually use `useast.ensembl.org`. Hope it helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242#issuecomment-416231775
https://github.com/scverse/scanpy/issues/242#issuecomment-416814067:21,Availability,error,error,21,"I still get the same error with `useast`. ```python; sc.queries.mitochondrial_genes(""useast.ensembl.org"", ""hsapiens""); ```; <details>; <summary>The output and traceback</summary>. ```python; You must set the host (e.g. f.host='www.ensembl.org' ; You must set the host (e.g. f.host='www.ensembl.org' ; You must set the host (e.g. f.host='www.ensembl.org' ; You must set the host (e.g. f.host='www.ensembl.org' ; You must set the host (e.g. f.host='www.ensembl.org' ; You must set the host (e.g. f.host='www.ensembl.org' ; ---------------------------------------------------------------------------; EmptyDataError Traceback (most recent call last); <ipython-input-4-66c3fcd14dab> in <module>(); ----> 1 sc.queries.mitochondrial_genes(""useast.ensembl.org"", ""hsapiens""). ~/github/scanpy/scanpy/queries/__init__.py in mitochondrial_genes(host, org); 40 ; 41 # parsing mitochondrial gene symbols; ---> 42 res = pd.read_csv(StringIO(s.query(xml)), sep='\t', header=None); 43 res.columns = ['symbol', 'chromosome_name']; 44 res = res.dropna(). /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision); 676 skip_blank_lines=skip_blank_lines); 677 ; --> 678 return _read(filepath_or_buffer, kwds); 679 ; 680 parser_f.__name__ = name. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds); 438 ; 439 # Create the parser.; --> 440 parser = TextFileReader(file",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242#issuecomment-416814067
https://github.com/scverse/scanpy/issues/242#issuecomment-416814067:3778,Modifiability,config,config,3778,"/local/lib/python3.6/site-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds); 438 ; 439 # Create the parser.; --> 440 parser = TextFileReader(filepath_or_buffer, **kwds); 441 ; 442 if chunksize or iterator:. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, f, engine, **kwds); 785 self.options['has_index_names'] = kwds['has_index_names']; 786 ; --> 787 self._make_engine(self.engine); 788 ; 789 def close(self):. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in _make_engine(self, engine); 1012 def _make_engine(self, engine='c'):; 1013 if engine == 'c':; -> 1014 self._engine = CParserWrapper(self.f, **self.options); 1015 else:; 1016 if engine == 'python':. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, src, **kwds); 1706 kwds['usecols'] = self.usecols; 1707 ; -> 1708 self._reader = parsers.TextReader(src, **kwds); 1709 ; 1710 passed_names = self.names is None. pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader.__cinit__(). EmptyDataError: No columns to parse from file; ```. </details>. But it seems to work no matter what server I choose in R, here's the test code. ```R; library(biomaRt). wwwmart = useMart(biomart=""ensembl"", dataset=""hsapiens_gene_ensembl"", host=""www.ensembl.org""); asiamart = useMart(biomart=""ensembl"", dataset=""hsapiens_gene_ensembl"", host=""asia.ensembl.org""); useastmart = useMart(biomart=""ensembl"", dataset=""hsapiens_gene_ensembl"", host=""useast.ensembl.org""). wwwresult = getBM(attributes=c(""hgnc_symbol"", ""chromosome_name""), mart=wwwmart); asiaresult = getBM(attributes=c(""hgnc_symbol"", ""chromosome_name""), mart=asiamart); useastresult = getBM(attributes=c(""hgnc_symbol"", ""chromosome_name""), mart=useastmart). assertthat::assert_that(all(wwwresult == asiaresult)); assertthat::assert_that(all(useastresult == asiaresult)); ```. I don't think the issue is with ensembl's servers. Is it possible you have a config for this `bioservices` module you've done anything with?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242#issuecomment-416814067
https://github.com/scverse/scanpy/issues/242#issuecomment-416814067:3004,Testability,test,test,3004,"/local/lib/python3.6/site-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds); 438 ; 439 # Create the parser.; --> 440 parser = TextFileReader(filepath_or_buffer, **kwds); 441 ; 442 if chunksize or iterator:. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, f, engine, **kwds); 785 self.options['has_index_names'] = kwds['has_index_names']; 786 ; --> 787 self._make_engine(self.engine); 788 ; 789 def close(self):. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in _make_engine(self, engine); 1012 def _make_engine(self, engine='c'):; 1013 if engine == 'c':; -> 1014 self._engine = CParserWrapper(self.f, **self.options); 1015 else:; 1016 if engine == 'python':. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, src, **kwds); 1706 kwds['usecols'] = self.usecols; 1707 ; -> 1708 self._reader = parsers.TextReader(src, **kwds); 1709 ; 1710 passed_names = self.names is None. pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader.__cinit__(). EmptyDataError: No columns to parse from file; ```. </details>. But it seems to work no matter what server I choose in R, here's the test code. ```R; library(biomaRt). wwwmart = useMart(biomart=""ensembl"", dataset=""hsapiens_gene_ensembl"", host=""www.ensembl.org""); asiamart = useMart(biomart=""ensembl"", dataset=""hsapiens_gene_ensembl"", host=""asia.ensembl.org""); useastmart = useMart(biomart=""ensembl"", dataset=""hsapiens_gene_ensembl"", host=""useast.ensembl.org""). wwwresult = getBM(attributes=c(""hgnc_symbol"", ""chromosome_name""), mart=wwwmart); asiaresult = getBM(attributes=c(""hgnc_symbol"", ""chromosome_name""), mart=asiamart); useastresult = getBM(attributes=c(""hgnc_symbol"", ""chromosome_name""), mart=useastmart). assertthat::assert_that(all(wwwresult == asiaresult)); assertthat::assert_that(all(useastresult == asiaresult)); ```. I don't think the issue is with ensembl's servers. Is it possible you have a config for this `bioservices` module you've done anything with?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242#issuecomment-416814067
https://github.com/scverse/scanpy/issues/242#issuecomment-416814067:3583,Testability,assert,assertthat,3583,"/local/lib/python3.6/site-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds); 438 ; 439 # Create the parser.; --> 440 parser = TextFileReader(filepath_or_buffer, **kwds); 441 ; 442 if chunksize or iterator:. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, f, engine, **kwds); 785 self.options['has_index_names'] = kwds['has_index_names']; 786 ; --> 787 self._make_engine(self.engine); 788 ; 789 def close(self):. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in _make_engine(self, engine); 1012 def _make_engine(self, engine='c'):; 1013 if engine == 'c':; -> 1014 self._engine = CParserWrapper(self.f, **self.options); 1015 else:; 1016 if engine == 'python':. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, src, **kwds); 1706 kwds['usecols'] = self.usecols; 1707 ; -> 1708 self._reader = parsers.TextReader(src, **kwds); 1709 ; 1710 passed_names = self.names is None. pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader.__cinit__(). EmptyDataError: No columns to parse from file; ```. </details>. But it seems to work no matter what server I choose in R, here's the test code. ```R; library(biomaRt). wwwmart = useMart(biomart=""ensembl"", dataset=""hsapiens_gene_ensembl"", host=""www.ensembl.org""); asiamart = useMart(biomart=""ensembl"", dataset=""hsapiens_gene_ensembl"", host=""asia.ensembl.org""); useastmart = useMart(biomart=""ensembl"", dataset=""hsapiens_gene_ensembl"", host=""useast.ensembl.org""). wwwresult = getBM(attributes=c(""hgnc_symbol"", ""chromosome_name""), mart=wwwmart); asiaresult = getBM(attributes=c(""hgnc_symbol"", ""chromosome_name""), mart=asiamart); useastresult = getBM(attributes=c(""hgnc_symbol"", ""chromosome_name""), mart=useastmart). assertthat::assert_that(all(wwwresult == asiaresult)); assertthat::assert_that(all(useastresult == asiaresult)); ```. I don't think the issue is with ensembl's servers. Is it possible you have a config for this `bioservices` module you've done anything with?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242#issuecomment-416814067
https://github.com/scverse/scanpy/issues/242#issuecomment-416814067:3638,Testability,assert,assertthat,3638,"/local/lib/python3.6/site-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds); 438 ; 439 # Create the parser.; --> 440 parser = TextFileReader(filepath_or_buffer, **kwds); 441 ; 442 if chunksize or iterator:. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, f, engine, **kwds); 785 self.options['has_index_names'] = kwds['has_index_names']; 786 ; --> 787 self._make_engine(self.engine); 788 ; 789 def close(self):. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in _make_engine(self, engine); 1012 def _make_engine(self, engine='c'):; 1013 if engine == 'c':; -> 1014 self._engine = CParserWrapper(self.f, **self.options); 1015 else:; 1016 if engine == 'python':. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, src, **kwds); 1706 kwds['usecols'] = self.usecols; 1707 ; -> 1708 self._reader = parsers.TextReader(src, **kwds); 1709 ; 1710 passed_names = self.names is None. pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader.__cinit__(). EmptyDataError: No columns to parse from file; ```. </details>. But it seems to work no matter what server I choose in R, here's the test code. ```R; library(biomaRt). wwwmart = useMart(biomart=""ensembl"", dataset=""hsapiens_gene_ensembl"", host=""www.ensembl.org""); asiamart = useMart(biomart=""ensembl"", dataset=""hsapiens_gene_ensembl"", host=""asia.ensembl.org""); useastmart = useMart(biomart=""ensembl"", dataset=""hsapiens_gene_ensembl"", host=""useast.ensembl.org""). wwwresult = getBM(attributes=c(""hgnc_symbol"", ""chromosome_name""), mart=wwwmart); asiaresult = getBM(attributes=c(""hgnc_symbol"", ""chromosome_name""), mart=asiamart); useastresult = getBM(attributes=c(""hgnc_symbol"", ""chromosome_name""), mart=useastmart). assertthat::assert_that(all(wwwresult == asiaresult)); assertthat::assert_that(all(useastresult == asiaresult)); ```. I don't think the issue is with ensembl's servers. Is it possible you have a config for this `bioservices` module you've done anything with?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242#issuecomment-416814067
https://github.com/scverse/scanpy/issues/242#issuecomment-419692328:405,Availability,down,down,405,"I did a little more looking into this, and think I see what's going on. If you set the host, it sends a query to the host to make sure it's real. If it doesn't get a 200 response, it silently fails and does not set the host. Every time I've checked, `www.ensembl.org` returns some `30*` code, which is a redirect, and would successfully fill any query anyways. `asia.ensembl.org` might have actually been down when I was checking, but it might be worth updating the docs that this probably won't work with `www.ensembl.org`. Also I'm pretty sure setting the host at instantiation would never work, cause I don't see it happening in their `__init__` function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242#issuecomment-419692328
https://github.com/scverse/scanpy/issues/242#issuecomment-456889768:160,Availability,avail,available,160,"You can check if the mirror is working by using the url:. http://<ensembl_biomart_mirror>/biomart/martview. This is the url used internally by bioservices. The available mirrors are listed [here](http://www.ensembl.org/info/about/mirrors.html). For example, at the moment the mirror useast is not available. > Service Temporarily Unavailable; > The server is temporarily unable to service your request due to maintenance downtime or capacity problems. Please try again later. I think we can close this issue since it's not related to the scanpy's code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242#issuecomment-456889768
https://github.com/scverse/scanpy/issues/242#issuecomment-456889768:297,Availability,avail,available,297,"You can check if the mirror is working by using the url:. http://<ensembl_biomart_mirror>/biomart/martview. This is the url used internally by bioservices. The available mirrors are listed [here](http://www.ensembl.org/info/about/mirrors.html). For example, at the moment the mirror useast is not available. > Service Temporarily Unavailable; > The server is temporarily unable to service your request due to maintenance downtime or capacity problems. Please try again later. I think we can close this issue since it's not related to the scanpy's code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242#issuecomment-456889768
https://github.com/scverse/scanpy/issues/242#issuecomment-456889768:409,Availability,mainten,maintenance,409,"You can check if the mirror is working by using the url:. http://<ensembl_biomart_mirror>/biomart/martview. This is the url used internally by bioservices. The available mirrors are listed [here](http://www.ensembl.org/info/about/mirrors.html). For example, at the moment the mirror useast is not available. > Service Temporarily Unavailable; > The server is temporarily unable to service your request due to maintenance downtime or capacity problems. Please try again later. I think we can close this issue since it's not related to the scanpy's code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242#issuecomment-456889768
https://github.com/scverse/scanpy/issues/242#issuecomment-456889768:421,Availability,downtime,downtime,421,"You can check if the mirror is working by using the url:. http://<ensembl_biomart_mirror>/biomart/martview. This is the url used internally by bioservices. The available mirrors are listed [here](http://www.ensembl.org/info/about/mirrors.html). For example, at the moment the mirror useast is not available. > Service Temporarily Unavailable; > The server is temporarily unable to service your request due to maintenance downtime or capacity problems. Please try again later. I think we can close this issue since it's not related to the scanpy's code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242#issuecomment-456889768
https://github.com/scverse/scanpy/issues/242#issuecomment-457039514:898,Deployability,install,install,898,"I'd argue a janky dependency is an issue with scanpy's code, as there are interfaces to BioMart which are better behaved. Here's a proof of concept:. ```python; def mitochondrial_genes(org, attrname=""external_gene_name"", host=""www.ensembl.org""):; """"""Mitochondrial gene symbols for specific organism through BioMart. Parameters; ----------; org : {{""hsapiens"", ""mmusculus"", ""drerio""}}; Organism to query. Must be an organism in ensembl biomart.; fieldname : `str`, optional (default: ""external_gene_name""); Biomart attribute field to return. Possible values include ; ""external_gene_name"", ""ensembl_gene_id"", ""hgnc_symbol"", ""mgi_symbol"",; and ""zfin_id_symbol"".; host : {{""www.ensembl.org"", ...}}; A valid BioMart host URL. Returns; -------; An `np.array` containing identifiers for mitochondrial genes.; """"""; try:; from pybiomart import Server; except ImportError:; raise ImportError(; ""You need to install the `pybiomart` module.""); server = Server(host); dataset = (server.marts[""ENSEMBL_MART_ENSEMBL""]; .datasets[""{}_gene_ensembl"".format(org)]); res = dataset.query(; attributes=[attrname], ; filters={""chromosome_name"": [""MT""]},; use_attr_names=True; ); return res[attrname].values; ```. Running it:. ```python; >>> mitochondrial_genes(""hsapiens""); array(['MT-TF', 'MT-RNR1', 'MT-TV', 'MT-RNR2', 'MT-TL1', 'MT-ND1',; 'MT-TI', 'MT-TQ', 'MT-TM', 'MT-ND2', 'MT-TW', 'MT-TA', 'MT-TN',; 'MT-TC', 'MT-TY', 'MT-CO1', 'MT-TS1', 'MT-TD', 'MT-CO2', 'MT-TK',; 'MT-ATP8', 'MT-ATP6', 'MT-CO3', 'MT-TG', 'MT-ND3', 'MT-TR',; 'MT-ND4L', 'MT-ND4', 'MT-TH', 'MT-TS2', 'MT-TL2', 'MT-ND5',; 'MT-ND6', 'MT-TE', 'MT-CYB', 'MT-TT', 'MT-TP'], dtype=object); >>> mitochondrial_genes(""hsapiens"", host=""asia.ensembl.org""); array(['MT-TF', 'MT-RNR1', 'MT-TV', 'MT-RNR2', 'MT-TL1', 'MT-ND1',; 'MT-TI', 'MT-TQ', 'MT-TM', 'MT-ND2', 'MT-TW', 'MT-TA', 'MT-TN',; 'MT-TC', 'MT-TY', 'MT-CO1', 'MT-TS1', 'MT-TD', 'MT-CO2', 'MT-TK',; 'MT-ATP8', 'MT-ATP6', 'MT-CO3', 'MT-TG', 'MT-ND3', 'MT-TR',; 'MT-ND4L', 'MT-ND4', 'MT-TH', 'MT-TS2', '",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242#issuecomment-457039514
https://github.com/scverse/scanpy/issues/242#issuecomment-457039514:18,Integrability,depend,dependency,18,"I'd argue a janky dependency is an issue with scanpy's code, as there are interfaces to BioMart which are better behaved. Here's a proof of concept:. ```python; def mitochondrial_genes(org, attrname=""external_gene_name"", host=""www.ensembl.org""):; """"""Mitochondrial gene symbols for specific organism through BioMart. Parameters; ----------; org : {{""hsapiens"", ""mmusculus"", ""drerio""}}; Organism to query. Must be an organism in ensembl biomart.; fieldname : `str`, optional (default: ""external_gene_name""); Biomart attribute field to return. Possible values include ; ""external_gene_name"", ""ensembl_gene_id"", ""hgnc_symbol"", ""mgi_symbol"",; and ""zfin_id_symbol"".; host : {{""www.ensembl.org"", ...}}; A valid BioMart host URL. Returns; -------; An `np.array` containing identifiers for mitochondrial genes.; """"""; try:; from pybiomart import Server; except ImportError:; raise ImportError(; ""You need to install the `pybiomart` module.""); server = Server(host); dataset = (server.marts[""ENSEMBL_MART_ENSEMBL""]; .datasets[""{}_gene_ensembl"".format(org)]); res = dataset.query(; attributes=[attrname], ; filters={""chromosome_name"": [""MT""]},; use_attr_names=True; ); return res[attrname].values; ```. Running it:. ```python; >>> mitochondrial_genes(""hsapiens""); array(['MT-TF', 'MT-RNR1', 'MT-TV', 'MT-RNR2', 'MT-TL1', 'MT-ND1',; 'MT-TI', 'MT-TQ', 'MT-TM', 'MT-ND2', 'MT-TW', 'MT-TA', 'MT-TN',; 'MT-TC', 'MT-TY', 'MT-CO1', 'MT-TS1', 'MT-TD', 'MT-CO2', 'MT-TK',; 'MT-ATP8', 'MT-ATP6', 'MT-CO3', 'MT-TG', 'MT-ND3', 'MT-TR',; 'MT-ND4L', 'MT-ND4', 'MT-TH', 'MT-TS2', 'MT-TL2', 'MT-ND5',; 'MT-ND6', 'MT-TE', 'MT-CYB', 'MT-TT', 'MT-TP'], dtype=object); >>> mitochondrial_genes(""hsapiens"", host=""asia.ensembl.org""); array(['MT-TF', 'MT-RNR1', 'MT-TV', 'MT-RNR2', 'MT-TL1', 'MT-ND1',; 'MT-TI', 'MT-TQ', 'MT-TM', 'MT-ND2', 'MT-TW', 'MT-TA', 'MT-TN',; 'MT-TC', 'MT-TY', 'MT-CO1', 'MT-TS1', 'MT-TD', 'MT-CO2', 'MT-TK',; 'MT-ATP8', 'MT-ATP6', 'MT-CO3', 'MT-TG', 'MT-ND3', 'MT-TR',; 'MT-ND4L', 'MT-ND4', 'MT-TH', 'MT-TS2', '",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242#issuecomment-457039514
https://github.com/scverse/scanpy/issues/242#issuecomment-457039514:74,Integrability,interface,interfaces,74,"I'd argue a janky dependency is an issue with scanpy's code, as there are interfaces to BioMart which are better behaved. Here's a proof of concept:. ```python; def mitochondrial_genes(org, attrname=""external_gene_name"", host=""www.ensembl.org""):; """"""Mitochondrial gene symbols for specific organism through BioMart. Parameters; ----------; org : {{""hsapiens"", ""mmusculus"", ""drerio""}}; Organism to query. Must be an organism in ensembl biomart.; fieldname : `str`, optional (default: ""external_gene_name""); Biomart attribute field to return. Possible values include ; ""external_gene_name"", ""ensembl_gene_id"", ""hgnc_symbol"", ""mgi_symbol"",; and ""zfin_id_symbol"".; host : {{""www.ensembl.org"", ...}}; A valid BioMart host URL. Returns; -------; An `np.array` containing identifiers for mitochondrial genes.; """"""; try:; from pybiomart import Server; except ImportError:; raise ImportError(; ""You need to install the `pybiomart` module.""); server = Server(host); dataset = (server.marts[""ENSEMBL_MART_ENSEMBL""]; .datasets[""{}_gene_ensembl"".format(org)]); res = dataset.query(; attributes=[attrname], ; filters={""chromosome_name"": [""MT""]},; use_attr_names=True; ); return res[attrname].values; ```. Running it:. ```python; >>> mitochondrial_genes(""hsapiens""); array(['MT-TF', 'MT-RNR1', 'MT-TV', 'MT-RNR2', 'MT-TL1', 'MT-ND1',; 'MT-TI', 'MT-TQ', 'MT-TM', 'MT-ND2', 'MT-TW', 'MT-TA', 'MT-TN',; 'MT-TC', 'MT-TY', 'MT-CO1', 'MT-TS1', 'MT-TD', 'MT-CO2', 'MT-TK',; 'MT-ATP8', 'MT-ATP6', 'MT-CO3', 'MT-TG', 'MT-ND3', 'MT-TR',; 'MT-ND4L', 'MT-ND4', 'MT-TH', 'MT-TS2', 'MT-TL2', 'MT-ND5',; 'MT-ND6', 'MT-TE', 'MT-CYB', 'MT-TT', 'MT-TP'], dtype=object); >>> mitochondrial_genes(""hsapiens"", host=""asia.ensembl.org""); array(['MT-TF', 'MT-RNR1', 'MT-TV', 'MT-RNR2', 'MT-TL1', 'MT-ND1',; 'MT-TI', 'MT-TQ', 'MT-TM', 'MT-ND2', 'MT-TW', 'MT-TA', 'MT-TN',; 'MT-TC', 'MT-TY', 'MT-CO1', 'MT-TS1', 'MT-TD', 'MT-CO2', 'MT-TK',; 'MT-ATP8', 'MT-ATP6', 'MT-CO3', 'MT-TG', 'MT-ND3', 'MT-TR',; 'MT-ND4L', 'MT-ND4', 'MT-TH', 'MT-TS2', '",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242#issuecomment-457039514
https://github.com/scverse/scanpy/issues/242#issuecomment-457056054:23,Energy Efficiency,charge,charge,23,I agree but I'm not in charge of the PRs. Probably a better solution would be to provide a backend parameter in the PR to choose which module to use. In my opinion bioservices looks more mature and maintained.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242#issuecomment-457056054
https://github.com/scverse/scanpy/issues/242#issuecomment-457868949:441,Usability,simpl,simple,441,"Hm, my impression is also that `bioservices` is relatively mature but tries to achieve a lot of things. `pybiomart` looks minimalistic and evidently only focuses on interfacing biomart and has a high coverage, which is nice. For that reason, I tend to prefer @ivirshup implementation over the @fbrundu's original `bioservices`-based one. But I don't have a strong opinion. I would not have implementations for two ""backends"" here for such a simple thing (querying biomart). @fbrundu, would you be alright with changing this to @ivirshup implementation?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242#issuecomment-457868949
https://github.com/scverse/scanpy/issues/242#issuecomment-458065068:247,Deployability,update,updated,247,Maybe I could throw in another ID mapping tool. [BED](https://f1000research.com/articles/7-195/v1) is pretty good. More comprehensive than Biomart and quicker too. It is however a local implementation that runs in a docker container. The image is updated every month or so. At the moment I run a container internally here... but maybe we could make a webserver out of this which can be directly integrated with scanpy?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242#issuecomment-458065068
https://github.com/scverse/scanpy/issues/242#issuecomment-458065068:395,Deployability,integrat,integrated,395,Maybe I could throw in another ID mapping tool. [BED](https://f1000research.com/articles/7-195/v1) is pretty good. More comprehensive than Biomart and quicker too. It is however a local implementation that runs in a docker container. The image is updated every month or so. At the moment I run a container internally here... but maybe we could make a webserver out of this which can be directly integrated with scanpy?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242#issuecomment-458065068
https://github.com/scverse/scanpy/issues/242#issuecomment-458065068:395,Integrability,integrat,integrated,395,Maybe I could throw in another ID mapping tool. [BED](https://f1000research.com/articles/7-195/v1) is pretty good. More comprehensive than Biomart and quicker too. It is however a local implementation that runs in a docker container. The image is updated every month or so. At the moment I run a container internally here... but maybe we could make a webserver out of this which can be directly integrated with scanpy?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242#issuecomment-458065068
https://github.com/scverse/scanpy/issues/242#issuecomment-460865434:8,Deployability,update,update,8,"Just an update, I've got the PR mostly done, but I'm having trouble keeping the arguments to `sc.queries.gene_coordinates` simple. Could someone who uses that function show me their use case? Is it that common to want the coordinates for only one gene, but also want to limit the coordinates to particular coordinates? . Would it be reasonable to replace this function with something more simple and open ended? I'm thinking just letting the user specify an organism and the fields they'd like. <details>; <summary>Here's a doc-string for what I'm thinking:</summary>. ```python; def biomart_annotations(org, attrs, host=""www.ensembl.org""):; """"""; Retrieve gene annotations from ensembl biomart. Parameters; ----------; org : `str`; Organism to query. Must be an organism in ensembl biomart. ""hsapiens"",; ""mmusculus"", ""drerio"", etc.; attrs : `List[str]`; Attributes to query biomart for.; host : `str`, optional (default: ""www.ensembl.org""); A valid BioMart host URL. Alternative values include archive urls (like; ""grch37.ensembl.org"") or regional mirrors (like ""useast.ensembl.org""). Returns; -------; A `pd.DataFrame` containing annotations. Examples; --------; Retrieve genes coordinates and chromosomes. >>> annot = sc.query.biomart_annotations(; ""hsapiens"",; [""ensembl_gene_id"", ""start_position"", ""end_position"", ""chromosome_name""],; ).set_index(""ensembl_gene_id""); >>> adata.var[annot.columns] = annot; ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242#issuecomment-460865434
https://github.com/scverse/scanpy/issues/242#issuecomment-460865434:123,Usability,simpl,simple,123,"Just an update, I've got the PR mostly done, but I'm having trouble keeping the arguments to `sc.queries.gene_coordinates` simple. Could someone who uses that function show me their use case? Is it that common to want the coordinates for only one gene, but also want to limit the coordinates to particular coordinates? . Would it be reasonable to replace this function with something more simple and open ended? I'm thinking just letting the user specify an organism and the fields they'd like. <details>; <summary>Here's a doc-string for what I'm thinking:</summary>. ```python; def biomart_annotations(org, attrs, host=""www.ensembl.org""):; """"""; Retrieve gene annotations from ensembl biomart. Parameters; ----------; org : `str`; Organism to query. Must be an organism in ensembl biomart. ""hsapiens"",; ""mmusculus"", ""drerio"", etc.; attrs : `List[str]`; Attributes to query biomart for.; host : `str`, optional (default: ""www.ensembl.org""); A valid BioMart host URL. Alternative values include archive urls (like; ""grch37.ensembl.org"") or regional mirrors (like ""useast.ensembl.org""). Returns; -------; A `pd.DataFrame` containing annotations. Examples; --------; Retrieve genes coordinates and chromosomes. >>> annot = sc.query.biomart_annotations(; ""hsapiens"",; [""ensembl_gene_id"", ""start_position"", ""end_position"", ""chromosome_name""],; ).set_index(""ensembl_gene_id""); >>> adata.var[annot.columns] = annot; ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242#issuecomment-460865434
https://github.com/scverse/scanpy/issues/242#issuecomment-460865434:389,Usability,simpl,simple,389,"Just an update, I've got the PR mostly done, but I'm having trouble keeping the arguments to `sc.queries.gene_coordinates` simple. Could someone who uses that function show me their use case? Is it that common to want the coordinates for only one gene, but also want to limit the coordinates to particular coordinates? . Would it be reasonable to replace this function with something more simple and open ended? I'm thinking just letting the user specify an organism and the fields they'd like. <details>; <summary>Here's a doc-string for what I'm thinking:</summary>. ```python; def biomart_annotations(org, attrs, host=""www.ensembl.org""):; """"""; Retrieve gene annotations from ensembl biomart. Parameters; ----------; org : `str`; Organism to query. Must be an organism in ensembl biomart. ""hsapiens"",; ""mmusculus"", ""drerio"", etc.; attrs : `List[str]`; Attributes to query biomart for.; host : `str`, optional (default: ""www.ensembl.org""); A valid BioMart host URL. Alternative values include archive urls (like; ""grch37.ensembl.org"") or regional mirrors (like ""useast.ensembl.org""). Returns; -------; A `pd.DataFrame` containing annotations. Examples; --------; Retrieve genes coordinates and chromosomes. >>> annot = sc.query.biomart_annotations(; ""hsapiens"",; [""ensembl_gene_id"", ""start_position"", ""end_position"", ""chromosome_name""],; ).set_index(""ensembl_gene_id""); >>> adata.var[annot.columns] = annot; ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242#issuecomment-460865434
https://github.com/scverse/scanpy/issues/242#issuecomment-460942661:433,Performance,cache,cache,433,"The latest BED docker image is 1.62 GB, which is not that big for an ID mapping DB I reckon (with the docker environment of course). You can also build your own, more limited versions of the database. There are instructions for this in the [github](https://github.com/patzaw/bed). BED memory usage I haven't really checked. Server side it's using 1 GB idly at the moment, no idea how that changes at peak usage. It has a client-side cache system to speed up multiple look-ups, but that can be turned off to limit client-side memory usage. And yes, as @flying-sheep said it's really easy to use for mapping between biological entities and even organisms as well. For transcript locations, you may need to use a separate file that contains the locations which will probably be annotated with RefSeq IDs. You can get to the RefSeq IDs easily though. It was actually designed for company use... but I think Thompson-Reuters (now Clarivate analytics) or UCB aren't interested in hosting public servers ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242#issuecomment-460942661
https://github.com/scverse/scanpy/pull/244#issuecomment-416733931:302,Deployability,release,release,302,"This looks good!. I'd also say that we should move towards a more transparent code for the scatter plots. It's a result of 1.5 years of subsequently adding features. No good initial plan about that. The only problems I see:; - there might be a few bugs in this, so I'd like to wait until after the 1.3 release. ; - some people might want exactly the same appearance of the plots as before. I'd say that this should be possible as you just used the existing code snippets. Currently, however, there are slight differences regarding defaults of spaces etc. This is also why the tests fail. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-416733931
https://github.com/scverse/scanpy/pull/244#issuecomment-416733931:576,Testability,test,tests,576,"This looks good!. I'd also say that we should move towards a more transparent code for the scatter plots. It's a result of 1.5 years of subsequently adding features. No good initial plan about that. The only problems I see:; - there might be a few bugs in this, so I'd like to wait until after the 1.3 release. ; - some people might want exactly the same appearance of the plots as before. I'd say that this should be possible as you just used the existing code snippets. Currently, however, there are slight differences regarding defaults of spaces etc. This is also why the tests fail. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-416733931
https://github.com/scverse/scanpy/pull/244#issuecomment-416855357:710,Availability,redundant,redundant,710,"@falexwolf Thanks for the support. . I agree that this is not an urgent changes that can quietly be tested. Have you consider having a 'develop' branch were we can put all code like this? . As you point out some differences are seen with respect to the shape of the plot when multiple panels are plot. This is mostly due to some code to add space for the colorbar and legends that can overlap nearby figures. Nevertheless, I can further adjust this to get plots that are more similar to the actual ones. I think that the tests are failing because there is a clash between the module and a method called `scatter`. Once the code is cleaned this should go away. . If you don't mind I would slowly start removing redundant code and adding further tests. So, lets keep this PR open.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-416855357
https://github.com/scverse/scanpy/pull/244#issuecomment-416855357:710,Safety,redund,redundant,710,"@falexwolf Thanks for the support. . I agree that this is not an urgent changes that can quietly be tested. Have you consider having a 'develop' branch were we can put all code like this? . As you point out some differences are seen with respect to the shape of the plot when multiple panels are plot. This is mostly due to some code to add space for the colorbar and legends that can overlap nearby figures. Nevertheless, I can further adjust this to get plots that are more similar to the actual ones. I think that the tests are failing because there is a clash between the module and a method called `scatter`. Once the code is cleaned this should go away. . If you don't mind I would slowly start removing redundant code and adding further tests. So, lets keep this PR open.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-416855357
https://github.com/scverse/scanpy/pull/244#issuecomment-416855357:100,Testability,test,tested,100,"@falexwolf Thanks for the support. . I agree that this is not an urgent changes that can quietly be tested. Have you consider having a 'develop' branch were we can put all code like this? . As you point out some differences are seen with respect to the shape of the plot when multiple panels are plot. This is mostly due to some code to add space for the colorbar and legends that can overlap nearby figures. Nevertheless, I can further adjust this to get plots that are more similar to the actual ones. I think that the tests are failing because there is a clash between the module and a method called `scatter`. Once the code is cleaned this should go away. . If you don't mind I would slowly start removing redundant code and adding further tests. So, lets keep this PR open.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-416855357
https://github.com/scverse/scanpy/pull/244#issuecomment-416855357:521,Testability,test,tests,521,"@falexwolf Thanks for the support. . I agree that this is not an urgent changes that can quietly be tested. Have you consider having a 'develop' branch were we can put all code like this? . As you point out some differences are seen with respect to the shape of the plot when multiple panels are plot. This is mostly due to some code to add space for the colorbar and legends that can overlap nearby figures. Nevertheless, I can further adjust this to get plots that are more similar to the actual ones. I think that the tests are failing because there is a clash between the module and a method called `scatter`. Once the code is cleaned this should go away. . If you don't mind I would slowly start removing redundant code and adding further tests. So, lets keep this PR open.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-416855357
https://github.com/scverse/scanpy/pull/244#issuecomment-416855357:744,Testability,test,tests,744,"@falexwolf Thanks for the support. . I agree that this is not an urgent changes that can quietly be tested. Have you consider having a 'develop' branch were we can put all code like this? . As you point out some differences are seen with respect to the shape of the plot when multiple panels are plot. This is mostly due to some code to add space for the colorbar and legends that can overlap nearby figures. Nevertheless, I can further adjust this to get plots that are more similar to the actual ones. I think that the tests are failing because there is a clash between the module and a method called `scatter`. Once the code is cleaned this should go away. . If you don't mind I would slowly start removing redundant code and adding further tests. So, lets keep this PR open.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-416855357
https://github.com/scverse/scanpy/pull/244#issuecomment-418062501:191,Availability,redundant,redundant,191,"Apologies for again, the late response @fidel! I married and moved to the US with twin babies last week. And in between, I spilled something over my laptop... Yes, please go ahead and remove redundant code and add further tests. We'll merge this PR eventually. And yes, we can think about a `develop` branch starting from 1.3. What do you say, @flying-sheep?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-418062501
https://github.com/scverse/scanpy/pull/244#issuecomment-418062501:191,Safety,redund,redundant,191,"Apologies for again, the late response @fidel! I married and moved to the US with twin babies last week. And in between, I spilled something over my laptop... Yes, please go ahead and remove redundant code and add further tests. We'll merge this PR eventually. And yes, we can think about a `develop` branch starting from 1.3. What do you say, @flying-sheep?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-418062501
https://github.com/scverse/scanpy/pull/244#issuecomment-418062501:222,Testability,test,tests,222,"Apologies for again, the late response @fidel! I married and moved to the US with twin babies last week. And in between, I spilled something over my laptop... Yes, please go ahead and remove redundant code and add further tests. We'll merge this PR eventually. And yes, we can think about a `develop` branch starting from 1.3. What do you say, @flying-sheep?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-418062501
https://github.com/scverse/scanpy/pull/244#issuecomment-420689737:6,Deployability,update,updated,6,PS: I updated the [seurat-based tutorial](https://github.com/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb) and added a few of your plotting functions and a link to your gist. Feel free to update it further!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-420689737
https://github.com/scverse/scanpy/pull/244#issuecomment-420689737:204,Deployability,update,update,204,PS: I updated the [seurat-based tutorial](https://github.com/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb) and added a few of your plotting functions and a link to your gist. Feel free to update it further!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-420689737
https://github.com/scverse/scanpy/pull/244#issuecomment-420917087:38,Deployability,update,updated,38,"@falexwolf Can you take a look at the updated gist: https://gist.github.com/fidelram/8b43f786e7519bcfb7ffc0d5ccdbb0fe. Most of the previous and new plots are quite similar. For diffmap I see different results but I suspect that there is a bug in the previous code. Also, I don't have any example with arrows. Do you have any?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-420917087
https://github.com/scverse/scanpy/pull/244#issuecomment-421258458:30,Energy Efficiency,efficient,efficient,30,"@falexwolf Do you know a more efficient way to get the value of a single column given the gene name. Currently, I am using:. ```; adata[:, 'gene_name'].X; ```. This is easy but not optimal. Any idea?. I will start updating the test once we are happy we the new results.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-421258458
https://github.com/scverse/scanpy/pull/244#issuecomment-421258458:227,Testability,test,test,227,"@falexwolf Do you know a more efficient way to get the value of a single column given the gene name. Currently, I am using:. ```; adata[:, 'gene_name'].X; ```. This is easy but not optimal. Any idea?. I will start updating the test once we are happy we the new results.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-421258458
https://github.com/scverse/scanpy/pull/244#issuecomment-422361197:118,Availability,redundant,redundant,118,"@flying-sheep I thought that by doing `adata[:, 'gene_name'].X` the AnnData object does all sorts of checks which are redundant if I only want to access a single column on the data matrix. But if you say this is fine I will not change it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-422361197
https://github.com/scverse/scanpy/pull/244#issuecomment-422361197:118,Safety,redund,redundant,118,"@flying-sheep I thought that by doing `adata[:, 'gene_name'].X` the AnnData object does all sorts of checks which are redundant if I only want to access a single column on the data matrix. But if you say this is fine I will not change it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-422361197
https://github.com/scverse/scanpy/pull/244#issuecomment-422361197:146,Security,access,access,146,"@flying-sheep I thought that by doing `adata[:, 'gene_name'].X` the AnnData object does all sorts of checks which are redundant if I only want to access a single column on the data matrix. But if you say this is fine I will not change it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-422361197
https://github.com/scverse/scanpy/pull/244#issuecomment-422400480:185,Availability,down,down,185,"The checks shouldn't take any time. But you're right; you're also slicing the `.var` and the `.varm` annotations if you make this call and we could check whether this perceivably slows down things (only for very large data, I guess). If it does, it would be very simple to add an accessor `.slice_X` that enables convenient slicing of the data matrix. That's a bit ugly but would vanish in the plotting function. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-422400480
https://github.com/scverse/scanpy/pull/244#issuecomment-422400480:280,Security,access,accessor,280,"The checks shouldn't take any time. But you're right; you're also slicing the `.var` and the `.varm` annotations if you make this call and we could check whether this perceivably slows down things (only for very large data, I guess). If it does, it would be very simple to add an accessor `.slice_X` that enables convenient slicing of the data matrix. That's a bit ugly but would vanish in the plotting function. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-422400480
https://github.com/scverse/scanpy/pull/244#issuecomment-422400480:263,Usability,simpl,simple,263,"The checks shouldn't take any time. But you're right; you're also slicing the `.var` and the `.varm` annotations if you make this call and we could check whether this perceivably slows down things (only for very large data, I guess). If it does, it would be very simple to add an accessor `.slice_X` that enables convenient slicing of the data matrix. That's a bit ugly but would vanish in the plotting function. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-422400480
https://github.com/scverse/scanpy/pull/244#issuecomment-423269634:9,Testability,test,test,9,The tsne test is giving me a headache. There are some small difference even setting a `random_state`. I will remove the test.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-423269634
https://github.com/scverse/scanpy/pull/244#issuecomment-423269634:120,Testability,test,test,120,The tsne test is giving me a headache. There are some small difference even setting a `random_state`. I will remove the test.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-423269634
https://github.com/scverse/scanpy/pull/244#issuecomment-423783837:11,Testability,test,test,11,"> The tsne test is giving me a headache. There are some small difference even setting a `random_state`. I will remove the test. I completely get this... the exact UMAP and tSNE plots are simply not _exactly_ reproducible, just very similar... fortunately, clustering (even though that's also a greedy algorithm) and everything else are exactly reproducible. I also removed the tests for UMAP: https://github.com/theislab/scanpy/blob/1df151f678c50b9f85f5d65e7a47d061e4e6784b/scanpy/tests/notebooks/pbmc3k.py#L88-L91 :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-423783837
https://github.com/scverse/scanpy/pull/244#issuecomment-423783837:122,Testability,test,test,122,"> The tsne test is giving me a headache. There are some small difference even setting a `random_state`. I will remove the test. I completely get this... the exact UMAP and tSNE plots are simply not _exactly_ reproducible, just very similar... fortunately, clustering (even though that's also a greedy algorithm) and everything else are exactly reproducible. I also removed the tests for UMAP: https://github.com/theislab/scanpy/blob/1df151f678c50b9f85f5d65e7a47d061e4e6784b/scanpy/tests/notebooks/pbmc3k.py#L88-L91 :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-423783837
https://github.com/scverse/scanpy/pull/244#issuecomment-423783837:377,Testability,test,tests,377,"> The tsne test is giving me a headache. There are some small difference even setting a `random_state`. I will remove the test. I completely get this... the exact UMAP and tSNE plots are simply not _exactly_ reproducible, just very similar... fortunately, clustering (even though that's also a greedy algorithm) and everything else are exactly reproducible. I also removed the tests for UMAP: https://github.com/theislab/scanpy/blob/1df151f678c50b9f85f5d65e7a47d061e4e6784b/scanpy/tests/notebooks/pbmc3k.py#L88-L91 :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-423783837
https://github.com/scverse/scanpy/pull/244#issuecomment-423783837:481,Testability,test,tests,481,"> The tsne test is giving me a headache. There are some small difference even setting a `random_state`. I will remove the test. I completely get this... the exact UMAP and tSNE plots are simply not _exactly_ reproducible, just very similar... fortunately, clustering (even though that's also a greedy algorithm) and everything else are exactly reproducible. I also removed the tests for UMAP: https://github.com/theislab/scanpy/blob/1df151f678c50b9f85f5d65e7a47d061e4e6784b/scanpy/tests/notebooks/pbmc3k.py#L88-L91 :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-423783837
https://github.com/scverse/scanpy/pull/244#issuecomment-423783837:187,Usability,simpl,simply,187,"> The tsne test is giving me a headache. There are some small difference even setting a `random_state`. I will remove the test. I completely get this... the exact UMAP and tSNE plots are simply not _exactly_ reproducible, just very similar... fortunately, clustering (even though that's also a greedy algorithm) and everything else are exactly reproducible. I also removed the tests for UMAP: https://github.com/theislab/scanpy/blob/1df151f678c50b9f85f5d65e7a47d061e4e6784b/scanpy/tests/notebooks/pbmc3k.py#L88-L91 :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-423783837
https://github.com/scverse/scanpy/pull/244#issuecomment-424247659:20,Testability,test,test,20,"@falexwolf The UMAP test works because the umap coordinates are saved along the pbmc datataset that I am using. In contrast, the tsne coordinates need to be computed. Thus I suggest to leave the UMAP test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-424247659
https://github.com/scverse/scanpy/pull/244#issuecomment-424247659:200,Testability,test,test,200,"@falexwolf The UMAP test works because the umap coordinates are saved along the pbmc datataset that I am using. In contrast, the tsne coordinates need to be computed. Thus I suggest to leave the UMAP test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-424247659
https://github.com/scverse/scanpy/pull/244#issuecomment-424803869:64,Testability,test,test,64,"I'll work a little bit with this branch for a couple of days to test it out myself, I might also push little changes to it. I'm super happy to merge after these tests. :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-424803869
https://github.com/scverse/scanpy/pull/244#issuecomment-424803869:161,Testability,test,tests,161,"I'll work a little bit with this branch for a couple of days to test it out myself, I might also push little changes to it. I'm super happy to merge after these tests. :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-424803869
https://github.com/scverse/scanpy/pull/244#issuecomment-424844986:637,Testability,test,test,637,"No problem. Maybe something we should consider comes from my attempt to use; plotly with the scatter functions output (probably for bokeh is similar).; Plotly has a function to convert a matplotlib fig object to plotly.; However, for this to work the figure object (the one returned by; pyplot.figure()) has to be returned. Currently, only the axes object are; returned. Thus, we should consider returning the fig object instead of the; axis or add this separately not to break any other code. On Wed, Sep 26, 2018 at 7:33 PM Alex Wolf <notifications@github.com> wrote:. > I'll work a little bit with this branch for a couple of days to test it; > out myself, I might also push little changes to it. I'm super happy to; > merge after these tests. 😄; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/pull/244#issuecomment-424803869>, or mute; > the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1afq5UYTS8faVtwGlqyLCpKCIgQkks5ue7pWgaJpZM4WNj5_>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-424844986
https://github.com/scverse/scanpy/pull/244#issuecomment-424844986:740,Testability,test,tests,740,"No problem. Maybe something we should consider comes from my attempt to use; plotly with the scatter functions output (probably for bokeh is similar).; Plotly has a function to convert a matplotlib fig object to plotly.; However, for this to work the figure object (the one returned by; pyplot.figure()) has to be returned. Currently, only the axes object are; returned. Thus, we should consider returning the fig object instead of the; axis or add this separately not to break any other code. On Wed, Sep 26, 2018 at 7:33 PM Alex Wolf <notifications@github.com> wrote:. > I'll work a little bit with this branch for a couple of days to test it; > out myself, I might also push little changes to it. I'm super happy to; > merge after these tests. 😄; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/pull/244#issuecomment-424803869>, or mute; > the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1afq5UYTS8faVtwGlqyLCpKCIgQkks5ue7pWgaJpZM4WNj5_>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-424844986
https://github.com/scverse/scanpy/pull/244#issuecomment-424863360:569,Safety,predict,prediction,569,"All of this is really nice! :smile:. Regarding your comment on plotly: returning a fig instead of an ax would follow a different convention as in seaborn; which I'd try to mimic as closely as possible. But for the scatter plots, which are the only ones that would profit a lot from interactive exploration, one could think about breaking this convention. Regarding bugs for now: One bug I could quickly fix myself, two further bugs that I've stumbled across in the past few hours and couldn't fix right away: If I'm calling; ```; sc.pl.umap(adata, color=['celltypes', 'prediction'], palette=[sc.pl.palettes.vega_20, sc.pl.palettes.default_64]); ```; the color maps are ignored. And if I'm calling; ```; sc.pl.umap(adata, color=['louvain', 'Gata1'], legend_loc='on data'; ); ```; the first plot has a different shape than the second.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-424863360
https://github.com/scverse/scanpy/pull/244#issuecomment-424986420:798,Safety,predict,prediction,798,"I will get that fixed soon. I don't know why the palette assignment is not; working as I am using the previous code for that. On Wed, Sep 26, 2018 at 10:44 PM Alex Wolf <notifications@github.com> wrote:. > All of this is really nice! 😄; >; > Regarding your comment on plotly: returning a fig instead of an ax would; > follow a different convention as in seaborn; which I'd try to mimic as; > closely as possible. But for the scatter plots, which are the only ones; > that would profit a lot from interactive exploration, one could think about; > breaking this convention.; >; > Regarding bugs for now: One bug I could quickly fix myself, two further; > bugs that I've stumbled across in the past few hours and couldn't fix right; > away: If I'm calling; >; > sc.pl.umap(adata, color=['celltypes', 'prediction'], palette=[sc.pl.palettes.vega_20, sc.pl.palettes.default_64]); >; > the color maps are ignored. And if I'm calling; >; > sc.pl.umap(adata, color=['louvain', 'Gata1'], legend_loc='on data'; > ); >; > the first plot has a different shape than the second.; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/pull/244#issuecomment-424863360>, or mute; > the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1Z-BMqIuY9jlSyFcI4TEHi4ULBoZks5ue-cdgaJpZM4WNj5_>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-424986420
https://github.com/scverse/scanpy/pull/244#issuecomment-425036021:643,Deployability,update,update,643,"@falexwolf I took the opportunity to add a change that I wanted with respect to the palette which is the ability to set a palette based on a matplotlib colormap. For example using `palette='tab20'`:. ![image](https://user-images.githubusercontent.com/4964309/46139067-dcf34180-c24d-11e8-892a-a6f3bbda2c4b.png). or using `palette='Set3'`. ![image](https://user-images.githubusercontent.com/4964309/46139126-feecc400-c24d-11e8-9e34-f8395c70aeb9.png). I didn't want to modify the previous code that handles setting the palette to avoid breaking other code, but if we have some tests for other functions that use that functionality I could try to update the original methods.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-425036021
https://github.com/scverse/scanpy/pull/244#issuecomment-425036021:527,Safety,avoid,avoid,527,"@falexwolf I took the opportunity to add a change that I wanted with respect to the palette which is the ability to set a palette based on a matplotlib colormap. For example using `palette='tab20'`:. ![image](https://user-images.githubusercontent.com/4964309/46139067-dcf34180-c24d-11e8-892a-a6f3bbda2c4b.png). or using `palette='Set3'`. ![image](https://user-images.githubusercontent.com/4964309/46139126-feecc400-c24d-11e8-9e34-f8395c70aeb9.png). I didn't want to modify the previous code that handles setting the palette to avoid breaking other code, but if we have some tests for other functions that use that functionality I could try to update the original methods.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-425036021
https://github.com/scverse/scanpy/pull/244#issuecomment-425036021:574,Testability,test,tests,574,"@falexwolf I took the opportunity to add a change that I wanted with respect to the palette which is the ability to set a palette based on a matplotlib colormap. For example using `palette='tab20'`:. ![image](https://user-images.githubusercontent.com/4964309/46139067-dcf34180-c24d-11e8-892a-a6f3bbda2c4b.png). or using `palette='Set3'`. ![image](https://user-images.githubusercontent.com/4964309/46139126-feecc400-c24d-11e8-9e34-f8395c70aeb9.png). I didn't want to modify the previous code that handles setting the palette to avoid breaking other code, but if we have some tests for other functions that use that functionality I could try to update the original methods.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-425036021
https://github.com/scverse/scanpy/pull/244#issuecomment-425450932:110,Deployability,continuous,continuous,110,"Really awesome changes! Now things are nicely tiled along the same grid independent of whether categorical or continuous annotation is plotted. :smile:. ![image](https://user-images.githubusercontent.com/16916678/46213585-e8567380-c306-11e8-9bdf-eb38d3410c3b.png). I added a docstring for `panels_per_row`; maybe one should simply call it `ncols` as in `matplotlib.GridSpec`? Essentially, in scanpy, sklearn and many other packages all things that are integer numebers are called `nsomething` or `n_something`. I'd merge immediately, things seem to work perfectly now. Just one tiny cosmetic thing; for these scatter plots, don't you think it would be nice to have them be a perfect square? As there is no meaningful scales on x and y axis? It gave me a bit of a headache when I first wrote it (couldn't make it work with GridSpec, hence all the mess that you encountered)... You're new clean code is definitely more important than this cosmetic thing, but if you have a quick solution, that's the last thing I can think of... ; ![image](https://user-images.githubusercontent.com/16916678/46212751-b9d79900-c304-11e8-9511-3e19559e8e83.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-425450932
https://github.com/scverse/scanpy/pull/244#issuecomment-425450932:324,Usability,simpl,simply,324,"Really awesome changes! Now things are nicely tiled along the same grid independent of whether categorical or continuous annotation is plotted. :smile:. ![image](https://user-images.githubusercontent.com/16916678/46213585-e8567380-c306-11e8-9bdf-eb38d3410c3b.png). I added a docstring for `panels_per_row`; maybe one should simply call it `ncols` as in `matplotlib.GridSpec`? Essentially, in scanpy, sklearn and many other packages all things that are integer numebers are called `nsomething` or `n_something`. I'd merge immediately, things seem to work perfectly now. Just one tiny cosmetic thing; for these scatter plots, don't you think it would be nice to have them be a perfect square? As there is no meaningful scales on x and y axis? It gave me a bit of a headache when I first wrote it (couldn't make it work with GridSpec, hence all the mess that you encountered)... You're new clean code is definitely more important than this cosmetic thing, but if you have a quick solution, that's the last thing I can think of... ; ![image](https://user-images.githubusercontent.com/16916678/46212751-b9d79900-c304-11e8-9511-3e19559e8e83.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-425450932
https://github.com/scverse/scanpy/pull/244#issuecomment-425866204:453,Energy Efficiency,reduce,reduce,453,"It is not much trouble to get something closer to a more square aspect, but the issue is whether the rcParams['figure.figsize'] is respected or not. To make room for the colormap on the right, the plot area is shrinked a bit but the figsize continues to be a square. . In my view, the solutions are:; * respect the rcParams['figure.figsize'] but make the colormap thinner, thus aiming towards a more squared image.; * enlarge the width of the figure or reduce the height to make it more square. I will experiment a bit to see what is better. As for the panels_per_row, I think is better to use some standard naming. I will change that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-425866204
https://github.com/scverse/scanpy/pull/244#issuecomment-425947033:40,Deployability,install,installed,40,I noticed that matplotlib v. 3 is being installed in travis. This may be the reason why some tests not related to the changes are now failing. I am updating my matplotlib version to update de tests.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-425947033
https://github.com/scverse/scanpy/pull/244#issuecomment-425947033:182,Deployability,update,update,182,I noticed that matplotlib v. 3 is being installed in travis. This may be the reason why some tests not related to the changes are now failing. I am updating my matplotlib version to update de tests.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-425947033
https://github.com/scverse/scanpy/pull/244#issuecomment-425947033:93,Testability,test,tests,93,I noticed that matplotlib v. 3 is being installed in travis. This may be the reason why some tests not related to the changes are now failing. I am updating my matplotlib version to update de tests.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-425947033
https://github.com/scverse/scanpy/pull/244#issuecomment-425947033:192,Testability,test,tests,192,I noticed that matplotlib v. 3 is being installed in travis. This may be the reason why some tests not related to the changes are now failing. I am updating my matplotlib version to update de tests.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-425947033
https://github.com/scverse/scanpy/pull/244#issuecomment-426281432:81,Availability,toler,tolerance,81,I don't know what is happening with the new matplotlib but I had to increase the tolerance for the image comparison in order for the tests to pass. Locally I noticed small differences in the margins and axis labels. Also I noticed that running a single test is different than running several tests at once. This probably has to do with some internal matplotlib parameters that are modified. . At least the tests are passing now.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-426281432
https://github.com/scverse/scanpy/pull/244#issuecomment-426281432:133,Testability,test,tests,133,I don't know what is happening with the new matplotlib but I had to increase the tolerance for the image comparison in order for the tests to pass. Locally I noticed small differences in the margins and axis labels. Also I noticed that running a single test is different than running several tests at once. This probably has to do with some internal matplotlib parameters that are modified. . At least the tests are passing now.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-426281432
https://github.com/scverse/scanpy/pull/244#issuecomment-426281432:253,Testability,test,test,253,I don't know what is happening with the new matplotlib but I had to increase the tolerance for the image comparison in order for the tests to pass. Locally I noticed small differences in the margins and axis labels. Also I noticed that running a single test is different than running several tests at once. This probably has to do with some internal matplotlib parameters that are modified. . At least the tests are passing now.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-426281432
https://github.com/scverse/scanpy/pull/244#issuecomment-426281432:292,Testability,test,tests,292,I don't know what is happening with the new matplotlib but I had to increase the tolerance for the image comparison in order for the tests to pass. Locally I noticed small differences in the margins and axis labels. Also I noticed that running a single test is different than running several tests at once. This probably has to do with some internal matplotlib parameters that are modified. . At least the tests are passing now.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-426281432
https://github.com/scverse/scanpy/pull/244#issuecomment-426281432:406,Testability,test,tests,406,I don't know what is happening with the new matplotlib but I had to increase the tolerance for the image comparison in order for the tests to pass. Locally I noticed small differences in the margins and axis labels. Also I noticed that running a single test is different than running several tests at once. This probably has to do with some internal matplotlib parameters that are modified. . At least the tests are passing now.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-426281432
https://github.com/scverse/scanpy/pull/244#issuecomment-426852062:495,Energy Efficiency,reduce,reduce,495,"@fidelram, really excited to try this out! I've got a couple questions about this PR:. * On that grid of plots @falexwolf posted, it looks like there a shared set of colorbars for two genes. Is this a coincidence (i.e. both genes happen to be expressed on a similar scale), is the colorscale being generated on the range of both genes, or is something else going on?; * When making a set of plots on the same coordinates (different genes on the same UMAP coordinates), have you found any way to reduce computational load? I'd like to think there are repeated computations (like layout) some memoization could speed up, but haven't figured out how.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-426852062
https://github.com/scverse/scanpy/pull/244#issuecomment-426852062:516,Performance,load,load,516,"@fidelram, really excited to try this out! I've got a couple questions about this PR:. * On that grid of plots @falexwolf posted, it looks like there a shared set of colorbars for two genes. Is this a coincidence (i.e. both genes happen to be expressed on a similar scale), is the colorscale being generated on the range of both genes, or is something else going on?; * When making a set of plots on the same coordinates (different genes on the same UMAP coordinates), have you found any way to reduce computational load? I'd like to think there are repeated computations (like layout) some memoization could speed up, but haven't figured out how.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-426852062
https://github.com/scverse/scanpy/pull/244#issuecomment-426894394:568,Performance,optimiz,optimized,568,"@ivirshup:; * If I am looking at the right example, the two genes have a different scale. There are 6 plots, but only two genes being plotted repeatedly. This is the same behaviour as before. However, now you can pass `vmin` and `vmax` to have the same scale in all plots that are quantitative.; * The point coordinates (e.g. tSNE or UMAP) are already precomputed, what changes in each plot is the order in which to plot the points because by default `sort_order=True`. The matplotlib scatter function takes care of the layout, but I don't see how this can be further optimized. Do you have any idea? Plotting large numbers of cells does not seem particularly slow for me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-426894394
https://github.com/scverse/scanpy/pull/244#issuecomment-427228991:410,Deployability,update,update,410,"@fidelram . * Cool. I think I had gotten confused about some of the labelling on there which I'm going to blame jet lag for 😊; * Using the `inline` backend with ~300k cells, it takes about 5 seconds per plot for me. Since I'm trying to improve the experience of sitting with a biologist figuring out labels for clusters, this is a little slow once you get to 5 or more plots – especially when you just want to update one. From `%prun`, it looks like about half of `matplotlib`'s plotting time is spent figuring out where to put points, and the extents of the plot, so I figured that could be a good target for optimization. I've looked into copying the plot after layout, but before coloring, but I'm not sure how feasible that is.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-427228991
https://github.com/scverse/scanpy/pull/244#issuecomment-427228991:610,Performance,optimiz,optimization,610,"@fidelram . * Cool. I think I had gotten confused about some of the labelling on there which I'm going to blame jet lag for 😊; * Using the `inline` backend with ~300k cells, it takes about 5 seconds per plot for me. Since I'm trying to improve the experience of sitting with a biologist figuring out labels for clusters, this is a little slow once you get to 5 or more plots – especially when you just want to update one. From `%prun`, it looks like about half of `matplotlib`'s plotting time is spent figuring out where to put points, and the extents of the plot, so I figured that could be a good target for optimization. I've looked into copying the plot after layout, but before coloring, but I'm not sure how feasible that is.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-427228991
https://github.com/scverse/scanpy/pull/244#issuecomment-427291301:77,Performance,optimiz,optimized,77,"@ivirshup For what you want we need to look into bokeh or plotly as they are optimized to render thousands of points quickly. I think that matplotlib is not going to be a solution here. Last week I played a bit with Dash and I found that is quite easy to set up an app to quickly explore gene expression. Maybe this is something that we can further develop. Currently, it uses matplotlib but I also tried it with plotly with decent results. Here is a very crude but functional demo using human lung airway data from *Plasschaert et a. Nature. 2018. “A Single-Cell Atlas of the Airway Epithelium Reveals the CFTR-Rich Pulmonary Ionocyte.”* https://doi.org/10.1038/s41586-018-0394-6.: https://demo-scexplorer.herokuapp.com/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-427291301
https://github.com/scverse/scanpy/pull/245#issuecomment-416728795:144,Testability,test,tests,144,"Ah, thank you! This might break some small things, as you replaced an exact search with an approximate neighbor search. It might not affect the tests as this is hard to test. Nonetheless, you're completely right, there shouldn't be two neighbor functions... I'll briefly check the harder cases and see whether I can reproduce some old notebooks. Please remind me if I don't get back to this very soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/245#issuecomment-416728795
https://github.com/scverse/scanpy/pull/245#issuecomment-416728795:169,Testability,test,test,169,"Ah, thank you! This might break some small things, as you replaced an exact search with an approximate neighbor search. It might not affect the tests as this is hard to test. Nonetheless, you're completely right, there shouldn't be two neighbor functions... I'll briefly check the harder cases and see whether I can reproduce some old notebooks. Please remind me if I don't get back to this very soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/245#issuecomment-416728795
https://github.com/scverse/scanpy/pull/245#issuecomment-416827079:190,Testability,test,tests,190,"I'd be interested in hearing what this might break. While I was checking to see if the euclidean distance implementation here got different results than the sklearn one, my (fairly cursory) tests at the repl passed the `np.allclose` test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/245#issuecomment-416827079
https://github.com/scverse/scanpy/pull/245#issuecomment-416827079:233,Testability,test,test,233,"I'd be interested in hearing what this might break. While I was checking to see if the euclidean distance implementation here got different results than the sklearn one, my (fairly cursory) tests at the repl passed the `np.allclose` test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/245#issuecomment-416827079
https://github.com/scverse/scanpy/pull/245#issuecomment-418072872:382,Testability,test,test,382,"Apologies for the late response @ivirshup! I married and moved to the US with twin babies last week. And in between, I spilled something over my laptop... I reproduced all the old notebooks. :smile:. PS: As mentioned, things might ""break"" as you replaced something exact with something approximate. The difference becomes pronounced on ""hard datasets"", which are not present in the test of the neighborhood search but only for the ""standard clustering tutorial"", which doesn't use `knn=False`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/245#issuecomment-418072872
https://github.com/scverse/scanpy/issues/246#issuecomment-416565579:7,Deployability,install,installed,7,I also installed DCA and tensorflow in the meantime... Maybe it has to do with different backend functions being used?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/246#issuecomment-416565579
https://github.com/scverse/scanpy/issues/246#issuecomment-416572288:118,Testability,log,log,118,"Nevermind... it turns out I had changed the parameter before, but not rerun it apparently... I reproduced it setting `log=True`. My bad...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/246#issuecomment-416572288
https://github.com/scverse/scanpy/issues/247#issuecomment-416903663:67,Availability,error,error,67,I upgraded loompy and scanpy as well but now I am getting an other error. ![screen shot 2018-08-29 at 19 21 34](https://user-images.githubusercontent.com/42487820/44782019-db881800-abc0-11e8-8948-90aa0b0c20a1.png). ![screen shot 2018-08-29 at 19 14 14](https://user-images.githubusercontent.com/42487820/44782040-eb076100-abc0-11e8-961e-75b4ba0c8ec7.png),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/247#issuecomment-416903663
https://github.com/scverse/scanpy/issues/247#issuecomment-416903663:2,Deployability,upgrade,upgraded,2,I upgraded loompy and scanpy as well but now I am getting an other error. ![screen shot 2018-08-29 at 19 21 34](https://user-images.githubusercontent.com/42487820/44782019-db881800-abc0-11e8-8948-90aa0b0c20a1.png). ![screen shot 2018-08-29 at 19 14 14](https://user-images.githubusercontent.com/42487820/44782040-eb076100-abc0-11e8-961e-75b4ba0c8ec7.png),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/247#issuecomment-416903663
https://github.com/scverse/scanpy/issues/247#issuecomment-418059347:210,Deployability,release,releases,210,"Apologies for the late response @hawaiiki! I married and moved to the US with twin babies last week. And in between, I spilled something over my laptop... Yes, unfortunately, there were two half-cooked anndata releases out there. 😒 All these issues are fixed on GitHub and in anndata 0.6.10. anndata is now able to fully handle loom's layers, which it wasn't before and hence gained quite some additional functionality, thanks to @Koncopd and @VolkerBergen.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/247#issuecomment-418059347
https://github.com/scverse/scanpy/issues/247#issuecomment-418059347:335,Modifiability,layers,layers,335,"Apologies for the late response @hawaiiki! I married and moved to the US with twin babies last week. And in between, I spilled something over my laptop... Yes, unfortunately, there were two half-cooked anndata releases out there. 😒 All these issues are fixed on GitHub and in anndata 0.6.10. anndata is now able to fully handle loom's layers, which it wasn't before and hence gained quite some additional functionality, thanks to @Koncopd and @VolkerBergen.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/247#issuecomment-418059347
https://github.com/scverse/scanpy/pull/248#issuecomment-418079395:672,Modifiability,extend,extending,672,"**Tests**. For the present changes, these tests are sufficient. Only since recently, Scanpy is becoming properly tested via the example notebooks (previously, I always ran everything manually). So, if you want to test this properly, come up with an interesting use case that uses clustering on weighted graphs, make a pull request for a notebook on `theislab/scanpy_usage`, link to it in the docs (https://scanpy.readthedocs.io/en/latest/examples.html) and commit a corresponding test in `tests/notebooks`. That's a lot of work and I think too much for this present case. Eventually, most of Scanpy's functionality should be tested this way. A lot will also be covered by extending existing notebooks. But I don't think it's possible to meaningfully add the weighted graphs to the present standard Louvain clustering example. **Multiple network representations**. I don't mind as long as the standard user still uses a single one - otherwise people will get confused. But let's not call these grahs ""networks"". The notion ""network"" suggests that nodes ""interact"". The current graphs in Scanpy, however, are all proxies for manifolds. Edges only represent similarity and, through that, have a topological interpretation. I'd really like to reserve the notion ""network"" for cases where nodes possibly or actually interact. Hence, a gene regulatory network or a network of cells with cell-cell interactions. Both are completely different things compared to the ""neighbors"" stuff in Scanpy. **Allow more choice of partition method for louvain-igraph package**. Fine for me as long as the default stays unchanged. I'm not sure whether it adds a lot of value but it will also not be harmful. ---. Could you also fix the docs for the `flavor` parameter and specify the default? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248#issuecomment-418079395
https://github.com/scverse/scanpy/pull/248#issuecomment-418079395:2,Testability,Test,Tests,2,"**Tests**. For the present changes, these tests are sufficient. Only since recently, Scanpy is becoming properly tested via the example notebooks (previously, I always ran everything manually). So, if you want to test this properly, come up with an interesting use case that uses clustering on weighted graphs, make a pull request for a notebook on `theislab/scanpy_usage`, link to it in the docs (https://scanpy.readthedocs.io/en/latest/examples.html) and commit a corresponding test in `tests/notebooks`. That's a lot of work and I think too much for this present case. Eventually, most of Scanpy's functionality should be tested this way. A lot will also be covered by extending existing notebooks. But I don't think it's possible to meaningfully add the weighted graphs to the present standard Louvain clustering example. **Multiple network representations**. I don't mind as long as the standard user still uses a single one - otherwise people will get confused. But let's not call these grahs ""networks"". The notion ""network"" suggests that nodes ""interact"". The current graphs in Scanpy, however, are all proxies for manifolds. Edges only represent similarity and, through that, have a topological interpretation. I'd really like to reserve the notion ""network"" for cases where nodes possibly or actually interact. Hence, a gene regulatory network or a network of cells with cell-cell interactions. Both are completely different things compared to the ""neighbors"" stuff in Scanpy. **Allow more choice of partition method for louvain-igraph package**. Fine for me as long as the default stays unchanged. I'm not sure whether it adds a lot of value but it will also not be harmful. ---. Could you also fix the docs for the `flavor` parameter and specify the default? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248#issuecomment-418079395
https://github.com/scverse/scanpy/pull/248#issuecomment-418079395:42,Testability,test,tests,42,"**Tests**. For the present changes, these tests are sufficient. Only since recently, Scanpy is becoming properly tested via the example notebooks (previously, I always ran everything manually). So, if you want to test this properly, come up with an interesting use case that uses clustering on weighted graphs, make a pull request for a notebook on `theislab/scanpy_usage`, link to it in the docs (https://scanpy.readthedocs.io/en/latest/examples.html) and commit a corresponding test in `tests/notebooks`. That's a lot of work and I think too much for this present case. Eventually, most of Scanpy's functionality should be tested this way. A lot will also be covered by extending existing notebooks. But I don't think it's possible to meaningfully add the weighted graphs to the present standard Louvain clustering example. **Multiple network representations**. I don't mind as long as the standard user still uses a single one - otherwise people will get confused. But let's not call these grahs ""networks"". The notion ""network"" suggests that nodes ""interact"". The current graphs in Scanpy, however, are all proxies for manifolds. Edges only represent similarity and, through that, have a topological interpretation. I'd really like to reserve the notion ""network"" for cases where nodes possibly or actually interact. Hence, a gene regulatory network or a network of cells with cell-cell interactions. Both are completely different things compared to the ""neighbors"" stuff in Scanpy. **Allow more choice of partition method for louvain-igraph package**. Fine for me as long as the default stays unchanged. I'm not sure whether it adds a lot of value but it will also not be harmful. ---. Could you also fix the docs for the `flavor` parameter and specify the default? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248#issuecomment-418079395
https://github.com/scverse/scanpy/pull/248#issuecomment-418079395:113,Testability,test,tested,113,"**Tests**. For the present changes, these tests are sufficient. Only since recently, Scanpy is becoming properly tested via the example notebooks (previously, I always ran everything manually). So, if you want to test this properly, come up with an interesting use case that uses clustering on weighted graphs, make a pull request for a notebook on `theislab/scanpy_usage`, link to it in the docs (https://scanpy.readthedocs.io/en/latest/examples.html) and commit a corresponding test in `tests/notebooks`. That's a lot of work and I think too much for this present case. Eventually, most of Scanpy's functionality should be tested this way. A lot will also be covered by extending existing notebooks. But I don't think it's possible to meaningfully add the weighted graphs to the present standard Louvain clustering example. **Multiple network representations**. I don't mind as long as the standard user still uses a single one - otherwise people will get confused. But let's not call these grahs ""networks"". The notion ""network"" suggests that nodes ""interact"". The current graphs in Scanpy, however, are all proxies for manifolds. Edges only represent similarity and, through that, have a topological interpretation. I'd really like to reserve the notion ""network"" for cases where nodes possibly or actually interact. Hence, a gene regulatory network or a network of cells with cell-cell interactions. Both are completely different things compared to the ""neighbors"" stuff in Scanpy. **Allow more choice of partition method for louvain-igraph package**. Fine for me as long as the default stays unchanged. I'm not sure whether it adds a lot of value but it will also not be harmful. ---. Could you also fix the docs for the `flavor` parameter and specify the default? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248#issuecomment-418079395
https://github.com/scverse/scanpy/pull/248#issuecomment-418079395:213,Testability,test,test,213,"**Tests**. For the present changes, these tests are sufficient. Only since recently, Scanpy is becoming properly tested via the example notebooks (previously, I always ran everything manually). So, if you want to test this properly, come up with an interesting use case that uses clustering on weighted graphs, make a pull request for a notebook on `theislab/scanpy_usage`, link to it in the docs (https://scanpy.readthedocs.io/en/latest/examples.html) and commit a corresponding test in `tests/notebooks`. That's a lot of work and I think too much for this present case. Eventually, most of Scanpy's functionality should be tested this way. A lot will also be covered by extending existing notebooks. But I don't think it's possible to meaningfully add the weighted graphs to the present standard Louvain clustering example. **Multiple network representations**. I don't mind as long as the standard user still uses a single one - otherwise people will get confused. But let's not call these grahs ""networks"". The notion ""network"" suggests that nodes ""interact"". The current graphs in Scanpy, however, are all proxies for manifolds. Edges only represent similarity and, through that, have a topological interpretation. I'd really like to reserve the notion ""network"" for cases where nodes possibly or actually interact. Hence, a gene regulatory network or a network of cells with cell-cell interactions. Both are completely different things compared to the ""neighbors"" stuff in Scanpy. **Allow more choice of partition method for louvain-igraph package**. Fine for me as long as the default stays unchanged. I'm not sure whether it adds a lot of value but it will also not be harmful. ---. Could you also fix the docs for the `flavor` parameter and specify the default? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248#issuecomment-418079395
https://github.com/scverse/scanpy/pull/248#issuecomment-418079395:480,Testability,test,test,480,"**Tests**. For the present changes, these tests are sufficient. Only since recently, Scanpy is becoming properly tested via the example notebooks (previously, I always ran everything manually). So, if you want to test this properly, come up with an interesting use case that uses clustering on weighted graphs, make a pull request for a notebook on `theislab/scanpy_usage`, link to it in the docs (https://scanpy.readthedocs.io/en/latest/examples.html) and commit a corresponding test in `tests/notebooks`. That's a lot of work and I think too much for this present case. Eventually, most of Scanpy's functionality should be tested this way. A lot will also be covered by extending existing notebooks. But I don't think it's possible to meaningfully add the weighted graphs to the present standard Louvain clustering example. **Multiple network representations**. I don't mind as long as the standard user still uses a single one - otherwise people will get confused. But let's not call these grahs ""networks"". The notion ""network"" suggests that nodes ""interact"". The current graphs in Scanpy, however, are all proxies for manifolds. Edges only represent similarity and, through that, have a topological interpretation. I'd really like to reserve the notion ""network"" for cases where nodes possibly or actually interact. Hence, a gene regulatory network or a network of cells with cell-cell interactions. Both are completely different things compared to the ""neighbors"" stuff in Scanpy. **Allow more choice of partition method for louvain-igraph package**. Fine for me as long as the default stays unchanged. I'm not sure whether it adds a lot of value but it will also not be harmful. ---. Could you also fix the docs for the `flavor` parameter and specify the default? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248#issuecomment-418079395
https://github.com/scverse/scanpy/pull/248#issuecomment-418079395:489,Testability,test,tests,489,"**Tests**. For the present changes, these tests are sufficient. Only since recently, Scanpy is becoming properly tested via the example notebooks (previously, I always ran everything manually). So, if you want to test this properly, come up with an interesting use case that uses clustering on weighted graphs, make a pull request for a notebook on `theislab/scanpy_usage`, link to it in the docs (https://scanpy.readthedocs.io/en/latest/examples.html) and commit a corresponding test in `tests/notebooks`. That's a lot of work and I think too much for this present case. Eventually, most of Scanpy's functionality should be tested this way. A lot will also be covered by extending existing notebooks. But I don't think it's possible to meaningfully add the weighted graphs to the present standard Louvain clustering example. **Multiple network representations**. I don't mind as long as the standard user still uses a single one - otherwise people will get confused. But let's not call these grahs ""networks"". The notion ""network"" suggests that nodes ""interact"". The current graphs in Scanpy, however, are all proxies for manifolds. Edges only represent similarity and, through that, have a topological interpretation. I'd really like to reserve the notion ""network"" for cases where nodes possibly or actually interact. Hence, a gene regulatory network or a network of cells with cell-cell interactions. Both are completely different things compared to the ""neighbors"" stuff in Scanpy. **Allow more choice of partition method for louvain-igraph package**. Fine for me as long as the default stays unchanged. I'm not sure whether it adds a lot of value but it will also not be harmful. ---. Could you also fix the docs for the `flavor` parameter and specify the default? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248#issuecomment-418079395
https://github.com/scverse/scanpy/pull/248#issuecomment-418079395:625,Testability,test,tested,625,"**Tests**. For the present changes, these tests are sufficient. Only since recently, Scanpy is becoming properly tested via the example notebooks (previously, I always ran everything manually). So, if you want to test this properly, come up with an interesting use case that uses clustering on weighted graphs, make a pull request for a notebook on `theislab/scanpy_usage`, link to it in the docs (https://scanpy.readthedocs.io/en/latest/examples.html) and commit a corresponding test in `tests/notebooks`. That's a lot of work and I think too much for this present case. Eventually, most of Scanpy's functionality should be tested this way. A lot will also be covered by extending existing notebooks. But I don't think it's possible to meaningfully add the weighted graphs to the present standard Louvain clustering example. **Multiple network representations**. I don't mind as long as the standard user still uses a single one - otherwise people will get confused. But let's not call these grahs ""networks"". The notion ""network"" suggests that nodes ""interact"". The current graphs in Scanpy, however, are all proxies for manifolds. Edges only represent similarity and, through that, have a topological interpretation. I'd really like to reserve the notion ""network"" for cases where nodes possibly or actually interact. Hence, a gene regulatory network or a network of cells with cell-cell interactions. Both are completely different things compared to the ""neighbors"" stuff in Scanpy. **Allow more choice of partition method for louvain-igraph package**. Fine for me as long as the default stays unchanged. I'm not sure whether it adds a lot of value but it will also not be harmful. ---. Could you also fix the docs for the `flavor` parameter and specify the default? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248#issuecomment-418079395
https://github.com/scverse/scanpy/pull/248#issuecomment-419007563:3,Testability,Test,Tests,3,"## Tests. Fair enough. ## More graph/ neighbor representations. For sure, I'm thinking of just. I've taken another look at some of the Neighbors code, and am realizing the amount of code this touches. I still think this is important, but might be out of scope for this pull request. I'm probably going to hack together something for personal use first, and see where I go from there. Just to make sure anything I play around with isn't breaking other parts of `scanpy`, would you mind sharing how you run those notebook tests?. ## Partition. Would you mind if I change the requirement in `setup.py` from `louvain` to `louvain>=0.6`? It'd make adding this a bit cleaner. ## Doc changes. Sure! All that needs to change is specifying the default, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248#issuecomment-419007563
https://github.com/scverse/scanpy/pull/248#issuecomment-419007563:520,Testability,test,tests,520,"## Tests. Fair enough. ## More graph/ neighbor representations. For sure, I'm thinking of just. I've taken another look at some of the Neighbors code, and am realizing the amount of code this touches. I still think this is important, but might be out of scope for this pull request. I'm probably going to hack together something for personal use first, and see where I go from there. Just to make sure anything I play around with isn't breaking other parts of `scanpy`, would you mind sharing how you run those notebook tests?. ## Partition. Would you mind if I change the requirement in `setup.py` from `louvain` to `louvain>=0.6`? It'd make adding this a bit cleaner. ## Doc changes. Sure! All that needs to change is specifying the default, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248#issuecomment-419007563
https://github.com/scverse/scanpy/pull/248#issuecomment-419283151:9,Testability,test,tests,9,"Notebook tests: there is only one there in the tests, see https://github.com/theislab/scanpy/tree/master/scanpy/tests/notebooks. I run all other linked examples notebooks run manually... So this is not really an option for you, I'd say. I think I can add two important further notebooks very soon so that almost all of the functionality is covered. `setup.py`: yes, definitely, `louvain-igraph>=0.6` is fine! no one should use an earlier version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248#issuecomment-419283151
https://github.com/scverse/scanpy/pull/248#issuecomment-419283151:47,Testability,test,tests,47,"Notebook tests: there is only one there in the tests, see https://github.com/theislab/scanpy/tree/master/scanpy/tests/notebooks. I run all other linked examples notebooks run manually... So this is not really an option for you, I'd say. I think I can add two important further notebooks very soon so that almost all of the functionality is covered. `setup.py`: yes, definitely, `louvain-igraph>=0.6` is fine! no one should use an earlier version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248#issuecomment-419283151
https://github.com/scverse/scanpy/pull/248#issuecomment-419283151:112,Testability,test,tests,112,"Notebook tests: there is only one there in the tests, see https://github.com/theislab/scanpy/tree/master/scanpy/tests/notebooks. I run all other linked examples notebooks run manually... So this is not really an option for you, I'd say. I think I can add two important further notebooks very soon so that almost all of the functionality is covered. `setup.py`: yes, definitely, `louvain-igraph>=0.6` is fine! no one should use an earlier version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248#issuecomment-419283151
https://github.com/scverse/scanpy/pull/248#issuecomment-419695136:160,Availability,failure,failure,160,"I'm having some trouble debugging whatever is going wrong with the notebook tests here. I get the same results if I run `pytest` on my machine, but don't get a failure if I run the code manually. Additionally, I don't get an error (the `abort`) if I *only* run the notebook tests (`pytest -k ""test_pbmc3k""`). Pretty sure the error is happening on the call to louvain in the notebook tests – an `assert False` fails the tests, one after gives current result – but I can't reproduce the abort interactively. Any idea what's going on/ how I can get a more helpful error message here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248#issuecomment-419695136
https://github.com/scverse/scanpy/pull/248#issuecomment-419695136:225,Availability,error,error,225,"I'm having some trouble debugging whatever is going wrong with the notebook tests here. I get the same results if I run `pytest` on my machine, but don't get a failure if I run the code manually. Additionally, I don't get an error (the `abort`) if I *only* run the notebook tests (`pytest -k ""test_pbmc3k""`). Pretty sure the error is happening on the call to louvain in the notebook tests – an `assert False` fails the tests, one after gives current result – but I can't reproduce the abort interactively. Any idea what's going on/ how I can get a more helpful error message here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248#issuecomment-419695136
https://github.com/scverse/scanpy/pull/248#issuecomment-419695136:325,Availability,error,error,325,"I'm having some trouble debugging whatever is going wrong with the notebook tests here. I get the same results if I run `pytest` on my machine, but don't get a failure if I run the code manually. Additionally, I don't get an error (the `abort`) if I *only* run the notebook tests (`pytest -k ""test_pbmc3k""`). Pretty sure the error is happening on the call to louvain in the notebook tests – an `assert False` fails the tests, one after gives current result – but I can't reproduce the abort interactively. Any idea what's going on/ how I can get a more helpful error message here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248#issuecomment-419695136
https://github.com/scverse/scanpy/pull/248#issuecomment-419695136:561,Availability,error,error,561,"I'm having some trouble debugging whatever is going wrong with the notebook tests here. I get the same results if I run `pytest` on my machine, but don't get a failure if I run the code manually. Additionally, I don't get an error (the `abort`) if I *only* run the notebook tests (`pytest -k ""test_pbmc3k""`). Pretty sure the error is happening on the call to louvain in the notebook tests – an `assert False` fails the tests, one after gives current result – but I can't reproduce the abort interactively. Any idea what's going on/ how I can get a more helpful error message here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248#issuecomment-419695136
https://github.com/scverse/scanpy/pull/248#issuecomment-419695136:567,Integrability,message,message,567,"I'm having some trouble debugging whatever is going wrong with the notebook tests here. I get the same results if I run `pytest` on my machine, but don't get a failure if I run the code manually. Additionally, I don't get an error (the `abort`) if I *only* run the notebook tests (`pytest -k ""test_pbmc3k""`). Pretty sure the error is happening on the call to louvain in the notebook tests – an `assert False` fails the tests, one after gives current result – but I can't reproduce the abort interactively. Any idea what's going on/ how I can get a more helpful error message here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248#issuecomment-419695136
https://github.com/scverse/scanpy/pull/248#issuecomment-419695136:237,Safety,abort,abort,237,"I'm having some trouble debugging whatever is going wrong with the notebook tests here. I get the same results if I run `pytest` on my machine, but don't get a failure if I run the code manually. Additionally, I don't get an error (the `abort`) if I *only* run the notebook tests (`pytest -k ""test_pbmc3k""`). Pretty sure the error is happening on the call to louvain in the notebook tests – an `assert False` fails the tests, one after gives current result – but I can't reproduce the abort interactively. Any idea what's going on/ how I can get a more helpful error message here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248#issuecomment-419695136
https://github.com/scverse/scanpy/pull/248#issuecomment-419695136:485,Safety,abort,abort,485,"I'm having some trouble debugging whatever is going wrong with the notebook tests here. I get the same results if I run `pytest` on my machine, but don't get a failure if I run the code manually. Additionally, I don't get an error (the `abort`) if I *only* run the notebook tests (`pytest -k ""test_pbmc3k""`). Pretty sure the error is happening on the call to louvain in the notebook tests – an `assert False` fails the tests, one after gives current result – but I can't reproduce the abort interactively. Any idea what's going on/ how I can get a more helpful error message here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248#issuecomment-419695136
https://github.com/scverse/scanpy/pull/248#issuecomment-419695136:76,Testability,test,tests,76,"I'm having some trouble debugging whatever is going wrong with the notebook tests here. I get the same results if I run `pytest` on my machine, but don't get a failure if I run the code manually. Additionally, I don't get an error (the `abort`) if I *only* run the notebook tests (`pytest -k ""test_pbmc3k""`). Pretty sure the error is happening on the call to louvain in the notebook tests – an `assert False` fails the tests, one after gives current result – but I can't reproduce the abort interactively. Any idea what's going on/ how I can get a more helpful error message here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248#issuecomment-419695136
https://github.com/scverse/scanpy/pull/248#issuecomment-419695136:274,Testability,test,tests,274,"I'm having some trouble debugging whatever is going wrong with the notebook tests here. I get the same results if I run `pytest` on my machine, but don't get a failure if I run the code manually. Additionally, I don't get an error (the `abort`) if I *only* run the notebook tests (`pytest -k ""test_pbmc3k""`). Pretty sure the error is happening on the call to louvain in the notebook tests – an `assert False` fails the tests, one after gives current result – but I can't reproduce the abort interactively. Any idea what's going on/ how I can get a more helpful error message here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248#issuecomment-419695136
https://github.com/scverse/scanpy/pull/248#issuecomment-419695136:383,Testability,test,tests,383,"I'm having some trouble debugging whatever is going wrong with the notebook tests here. I get the same results if I run `pytest` on my machine, but don't get a failure if I run the code manually. Additionally, I don't get an error (the `abort`) if I *only* run the notebook tests (`pytest -k ""test_pbmc3k""`). Pretty sure the error is happening on the call to louvain in the notebook tests – an `assert False` fails the tests, one after gives current result – but I can't reproduce the abort interactively. Any idea what's going on/ how I can get a more helpful error message here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248#issuecomment-419695136
https://github.com/scverse/scanpy/pull/248#issuecomment-419695136:395,Testability,assert,assert,395,"I'm having some trouble debugging whatever is going wrong with the notebook tests here. I get the same results if I run `pytest` on my machine, but don't get a failure if I run the code manually. Additionally, I don't get an error (the `abort`) if I *only* run the notebook tests (`pytest -k ""test_pbmc3k""`). Pretty sure the error is happening on the call to louvain in the notebook tests – an `assert False` fails the tests, one after gives current result – but I can't reproduce the abort interactively. Any idea what's going on/ how I can get a more helpful error message here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248#issuecomment-419695136
https://github.com/scverse/scanpy/pull/248#issuecomment-419695136:419,Testability,test,tests,419,"I'm having some trouble debugging whatever is going wrong with the notebook tests here. I get the same results if I run `pytest` on my machine, but don't get a failure if I run the code manually. Additionally, I don't get an error (the `abort`) if I *only* run the notebook tests (`pytest -k ""test_pbmc3k""`). Pretty sure the error is happening on the call to louvain in the notebook tests – an `assert False` fails the tests, one after gives current result – but I can't reproduce the abort interactively. Any idea what's going on/ how I can get a more helpful error message here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248#issuecomment-419695136
https://github.com/scverse/scanpy/pull/248#issuecomment-419698370:274,Availability,error,error,274,"Caught it. I had forgotten that arguments only get evaluated once, so if you mutate them, there is state which is maintained to other calls. I think the unhelpful `abort` message is from `louvain-igraph` expecting a weight vector of the right shape, which ended up with the error:. ```; libc++abi.dylib: terminating with uncaught exception of type char const*; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248#issuecomment-419698370
https://github.com/scverse/scanpy/pull/248#issuecomment-419698370:171,Integrability,message,message,171,"Caught it. I had forgotten that arguments only get evaluated once, so if you mutate them, there is state which is maintained to other calls. I think the unhelpful `abort` message is from `louvain-igraph` expecting a weight vector of the right shape, which ended up with the error:. ```; libc++abi.dylib: terminating with uncaught exception of type char const*; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248#issuecomment-419698370
https://github.com/scverse/scanpy/pull/248#issuecomment-419698370:164,Safety,abort,abort,164,"Caught it. I had forgotten that arguments only get evaluated once, so if you mutate them, there is state which is maintained to other calls. I think the unhelpful `abort` message is from `louvain-igraph` expecting a weight vector of the right shape, which ended up with the error:. ```; libc++abi.dylib: terminating with uncaught exception of type char const*; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248#issuecomment-419698370
https://github.com/scverse/scanpy/pull/248#issuecomment-420660906:463,Availability,redundant,redundant,463,"Great that you fixed it! :). I'm in principle happy to merge the pull request! However, it contains a lot of the previous commits to master by other people; maybe you haven't properly rebased your branch to master at some point? Looking at the diff across the whole request, I see mostly old things from the 25 commits before. So, (1) could you point me to the diff across all your commits that you actually did? (2) Are we going to have a messed up history with redundant commits on the master branch if I merge this, I don't think so, but I'm not 100% sure. Sorry for the additional work, but blindly merging something into master is of course too dangerous, and going through each single commit is too tedious. ;). Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248#issuecomment-420660906
https://github.com/scverse/scanpy/pull/248#issuecomment-420660906:463,Safety,redund,redundant,463,"Great that you fixed it! :). I'm in principle happy to merge the pull request! However, it contains a lot of the previous commits to master by other people; maybe you haven't properly rebased your branch to master at some point? Looking at the diff across the whole request, I see mostly old things from the 25 commits before. So, (1) could you point me to the diff across all your commits that you actually did? (2) Are we going to have a messed up history with redundant commits on the master branch if I merge this, I don't think so, but I'm not 100% sure. Sorry for the additional work, but blindly merging something into master is of course too dangerous, and going through each single commit is too tedious. ;). Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248#issuecomment-420660906
https://github.com/scverse/scanpy/issues/252#issuecomment-417836666:908,Availability,error,error,908,"does the problem also happen in matrixplot or heatmap? seems to me like an; issue with the underlying seaborn violin plot. On Fri, Aug 31, 2018 at 6:25 PM a-munoz-rojas <notifications@github.com>; wrote:. > Thanks for all the work in developing this package, it's truly fantastic.; >; > I ran into what seems like a bug in the new plotting function; > sc.pl.rank_genes_groups_stacked_violin. It seems that when the ranked genes; > between 2 groups are similar (e.g. 'Tnf' is a highly ranked gene between; > two groups), then 'Tnf' is only plotted once on the first group, and any; > following groups with the same gene are truncated. You can see this in the; > toy example image I attached - when comparing groups M1 and M1+M2, 'Tnf'; > should be plotted for each group, but it is only plotted on group M1,; > therefore truncating group M2. When I plot the same data using; > rank_genes_groups_dotplot, this error doesn't happen and 'Tnf' is correctly; > plotted twice.; >; > I know this is a small bug that most people will probably not run across,; > but just in case you're comparing expression across similar groups this; > might be a useful fix. Thanks!; >; > [image: stacked_violin_global]; > <https://user-images.githubusercontent.com/37122760/44924265-bd353000-ad18-11e8-84d0-a0136083dbdd.png>; >; > [image: dotplot_global]; > <https://user-images.githubusercontent.com/37122760/44924244-aa226000-ad18-11e8-9351-4b28d11a7ee5.png>; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/252>, or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1Z1BA7WgQycvMb5E4fHkMuW1p1idks5uWWNxgaJpZM4WVgcM>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/252#issuecomment-417836666
https://github.com/scverse/scanpy/issues/252#issuecomment-421671797:32,Availability,error,error,32,I am also running into the same error. Thank you!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/252#issuecomment-421671797
https://github.com/scverse/scanpy/issues/253#issuecomment-418597651:174,Energy Efficiency,power,powerful,174,"@falexwolf, I think it would be worth going over what kind of interactivity would be most useful. I find linked selection and summary statistics on selected groups is pretty powerful. For QC plots, it's nice to know other properties of cells which look like outliers. It can also be useful for figuring out what's up with the classification that's not agreeing with your reduced dimension plot. Being able to interactively search and select genes to view would also be nice. It would also be good if it were easy to share this kind of visualization with non-technical collaborators easily. I think there's also a question of scale, and whether it would be nice to use libraries like [datashader](http://datashader.org) to avoid the over plotting problems that are so common in this field. I'm working on a few prototypes at the moment, but I'm not sure how well they fit into the api of adding an `interact` flag. Once I have things a little more formalized I'll set up a repo, but am open to suggestions for other plot types. I'd be interested in getting opinions on the usefulness of `datashader`'s edge bundling for graph plots ([examples here](http://holoviews.org/user_guide/Network_Graphs.html#) under the header ""Bundling graphs""). There's also the issue of libraries. Currently, I'm frustrated with every python plotting library, but am leaning towards the `holoviews`, `bokeh`, `datashader` stack for this. @gokceneraslan If you're asking about what usage of bokeh looks like, [they have a bunch of notebooks](https://github.com/bokeh/bokeh-notebooks) in a repo that'll run on [Binder](https://mybinder.org).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/253#issuecomment-418597651
https://github.com/scverse/scanpy/issues/253#issuecomment-418597651:371,Energy Efficiency,reduce,reduced,371,"@falexwolf, I think it would be worth going over what kind of interactivity would be most useful. I find linked selection and summary statistics on selected groups is pretty powerful. For QC plots, it's nice to know other properties of cells which look like outliers. It can also be useful for figuring out what's up with the classification that's not agreeing with your reduced dimension plot. Being able to interactively search and select genes to view would also be nice. It would also be good if it were easy to share this kind of visualization with non-technical collaborators easily. I think there's also a question of scale, and whether it would be nice to use libraries like [datashader](http://datashader.org) to avoid the over plotting problems that are so common in this field. I'm working on a few prototypes at the moment, but I'm not sure how well they fit into the api of adding an `interact` flag. Once I have things a little more formalized I'll set up a repo, but am open to suggestions for other plot types. I'd be interested in getting opinions on the usefulness of `datashader`'s edge bundling for graph plots ([examples here](http://holoviews.org/user_guide/Network_Graphs.html#) under the header ""Bundling graphs""). There's also the issue of libraries. Currently, I'm frustrated with every python plotting library, but am leaning towards the `holoviews`, `bokeh`, `datashader` stack for this. @gokceneraslan If you're asking about what usage of bokeh looks like, [they have a bunch of notebooks](https://github.com/bokeh/bokeh-notebooks) in a repo that'll run on [Binder](https://mybinder.org).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/253#issuecomment-418597651
https://github.com/scverse/scanpy/issues/253#issuecomment-418597651:722,Safety,avoid,avoid,722,"@falexwolf, I think it would be worth going over what kind of interactivity would be most useful. I find linked selection and summary statistics on selected groups is pretty powerful. For QC plots, it's nice to know other properties of cells which look like outliers. It can also be useful for figuring out what's up with the classification that's not agreeing with your reduced dimension plot. Being able to interactively search and select genes to view would also be nice. It would also be good if it were easy to share this kind of visualization with non-technical collaborators easily. I think there's also a question of scale, and whether it would be nice to use libraries like [datashader](http://datashader.org) to avoid the over plotting problems that are so common in this field. I'm working on a few prototypes at the moment, but I'm not sure how well they fit into the api of adding an `interact` flag. Once I have things a little more formalized I'll set up a repo, but am open to suggestions for other plot types. I'd be interested in getting opinions on the usefulness of `datashader`'s edge bundling for graph plots ([examples here](http://holoviews.org/user_guide/Network_Graphs.html#) under the header ""Bundling graphs""). There's also the issue of libraries. Currently, I'm frustrated with every python plotting library, but am leaning towards the `holoviews`, `bokeh`, `datashader` stack for this. @gokceneraslan If you're asking about what usage of bokeh looks like, [they have a bunch of notebooks](https://github.com/bokeh/bokeh-notebooks) in a repo that'll run on [Binder](https://mybinder.org).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/253#issuecomment-418597651
https://github.com/scverse/scanpy/issues/253#issuecomment-420664106:195,Integrability,wrap,wrapper,195,"@ivirshup Thank you for your super elaborate response and treatment of the topic. I completely understand that you're going for a more comprehensive solution than something like the simple bokeh wrapper that I pasted above. I'd really be interested in something that combines datashader and bokeh, for instance. If you're creating your own package for that, it would be awesome if it was somehow possible to use it also for Scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/253#issuecomment-420664106
https://github.com/scverse/scanpy/issues/253#issuecomment-420664106:182,Usability,simpl,simple,182,"@ivirshup Thank you for your super elaborate response and treatment of the topic. I completely understand that you're going for a more comprehensive solution than something like the simple bokeh wrapper that I pasted above. I'd really be interested in something that combines datashader and bokeh, for instance. If you're creating your own package for that, it would be awesome if it was somehow possible to use it also for Scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/253#issuecomment-420664106
https://github.com/scverse/scanpy/issues/256#issuecomment-418711035:317,Deployability,pipeline,pipeline,317,"Yes, fine with me. I didn't do it for Scanpy in the beginning as the formatting I wanted requires quite a few lines. I then closely mimicked Scanpy's logging using Python's logging for another package here: https://github.com/NDKoehler/DataScienceBowl2017_7th_place/blob/638542c3cde5af45bf34d0391695ab0e54ce78b8/dsb3/pipeline.py#L373-L430. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/256#issuecomment-418711035
https://github.com/scverse/scanpy/issues/256#issuecomment-418711035:150,Testability,log,logging,150,"Yes, fine with me. I didn't do it for Scanpy in the beginning as the formatting I wanted requires quite a few lines. I then closely mimicked Scanpy's logging using Python's logging for another package here: https://github.com/NDKoehler/DataScienceBowl2017_7th_place/blob/638542c3cde5af45bf34d0391695ab0e54ce78b8/dsb3/pipeline.py#L373-L430. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/256#issuecomment-418711035
https://github.com/scverse/scanpy/issues/256#issuecomment-418711035:173,Testability,log,logging,173,"Yes, fine with me. I didn't do it for Scanpy in the beginning as the formatting I wanted requires quite a few lines. I then closely mimicked Scanpy's logging using Python's logging for another package here: https://github.com/NDKoehler/DataScienceBowl2017_7th_place/blob/638542c3cde5af45bf34d0391695ab0e54ce78b8/dsb3/pipeline.py#L373-L430. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/256#issuecomment-418711035
https://github.com/scverse/scanpy/issues/257#issuecomment-418822885:52,Availability,down,down,52,"Sorry, this seems to be a UMAP issue. To really dig down to this, you'd probably run this using the UMAP package and submit a bug report there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/257#issuecomment-418822885
https://github.com/scverse/scanpy/issues/262#issuecomment-421082246:269,Deployability,update,update,269,"If you want to extract it in python, you can load the h5ad file using `adata = sc.read(filename)` and then use `adata.X`, which is the expression matrix. To extract the matrix into R, you can use the `rhdf5` library. That's a bit more complicated as there was a recent update to this library I believe. Note that you need to transpose the expression matrix from python into R due to different conventions (R expects a genes x cells matrix, python a cells x genes matrix). An alternative to the `rhdf5` library is to just save the expression matrix via `numpy.savetxt()` to save it, for example, as a space-delimited file. I hope this helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262#issuecomment-421082246
https://github.com/scverse/scanpy/issues/262#issuecomment-421082246:45,Performance,load,load,45,"If you want to extract it in python, you can load the h5ad file using `adata = sc.read(filename)` and then use `adata.X`, which is the expression matrix. To extract the matrix into R, you can use the `rhdf5` library. That's a bit more complicated as there was a recent update to this library I believe. Note that you need to transpose the expression matrix from python into R due to different conventions (R expects a genes x cells matrix, python a cells x genes matrix). An alternative to the `rhdf5` library is to just save the expression matrix via `numpy.savetxt()` to save it, for example, as a space-delimited file. I hope this helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262#issuecomment-421082246
https://github.com/scverse/scanpy/issues/262#issuecomment-422093244:150,Availability,down,downstream,150,"Hi @cartal, it wouldn't be very hard to export to a 10x h5 file, but I'd need to write a custom function for it. Why is it needed? Does 10x offer any downstream analysis that you'd want to use on the data? I thought there are none, hence there is only `sc.read_10x_h5` and no `sc.write_10x_h5`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262#issuecomment-422093244
https://github.com/scverse/scanpy/issues/262#issuecomment-460475684:94,Usability,simpl,simply,94,"No, there is no way to produce a single file with data and metadata. Having genes as rows can simply be achieved by transposing the matrix (`adata.T.write_csvs(...)`).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262#issuecomment-460475684
https://github.com/scverse/scanpy/issues/262#issuecomment-461540169:26,Usability,feedback,feedback,26,"@falexwolf thanks for the feedback. As @maximilianh suggested, I was able to export the expression matrix from the cellbrowser export function. Thank you for your help.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262#issuecomment-461540169
https://github.com/scverse/scanpy/issues/262#issuecomment-461557503:262,Usability,feedback,feedback,262,"Nice to hear that it worked. As a side product, you can now create a cell; browser html directory from the generated directory. On Thu, Feb 7, 2019 at 10:21 AM aditisk <notifications@github.com> wrote:. > @falexwolf <https://github.com/falexwolf> thanks for the feedback. As; > @maximilianh <https://github.com/maximilianh> suggested, I was able to; > export the expression matrix from the cellbrowser export function. Thank; > you for your help.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/262#issuecomment-461540169>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AAS-TXGCkdaQWO8ks_x7uOm-P2_ISArRks5vLG6RgaJpZM4Wne7Z>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262#issuecomment-461557503
https://github.com/scverse/scanpy/issues/262#issuecomment-478685403:106,Availability,error,error,106,"@maximilianh I was able to use the cell browser export function in the past but this time I am getting an error message:. INFO:root:Writing scanpy matrix to adata_cellbrowser_04_01_19_CD8_subclustered/exprMatrix.tsv.gz; INFO:root:Transposing matrix; INFO:root:Writing gene-by-gene, without using pandas; INFO:root:Writing 8068 genes in total; INFO:root:Wrote 0 genes; INFO:root:Wrote 2000 genes; INFO:root:Wrote 4000 genes; INFO:root:Wrote 6000 genes; INFO:root:Wrote 8000 genes; INFO:root:Writing UMAP coords to adata_cellbrowser_04_01_19_CD8_subclustered/umap_coords.tsv; ERROR:root:Couldnt find cluster markers list. I am using an h5ad file to import my ann data object. Is that why there is some issue with finding cluster markers ? I am able to plot the clusters in a UMAP plot so I know that the 'louvain' observation exists. Any thoughts on why this is happening ?. Thanks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262#issuecomment-478685403
https://github.com/scverse/scanpy/issues/262#issuecomment-478685403:574,Availability,ERROR,ERROR,574,"@maximilianh I was able to use the cell browser export function in the past but this time I am getting an error message:. INFO:root:Writing scanpy matrix to adata_cellbrowser_04_01_19_CD8_subclustered/exprMatrix.tsv.gz; INFO:root:Transposing matrix; INFO:root:Writing gene-by-gene, without using pandas; INFO:root:Writing 8068 genes in total; INFO:root:Wrote 0 genes; INFO:root:Wrote 2000 genes; INFO:root:Wrote 4000 genes; INFO:root:Wrote 6000 genes; INFO:root:Wrote 8000 genes; INFO:root:Writing UMAP coords to adata_cellbrowser_04_01_19_CD8_subclustered/umap_coords.tsv; ERROR:root:Couldnt find cluster markers list. I am using an h5ad file to import my ann data object. Is that why there is some issue with finding cluster markers ? I am able to plot the clusters in a UMAP plot so I know that the 'louvain' observation exists. Any thoughts on why this is happening ?. Thanks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262#issuecomment-478685403
https://github.com/scverse/scanpy/issues/262#issuecomment-478685403:112,Integrability,message,message,112,"@maximilianh I was able to use the cell browser export function in the past but this time I am getting an error message:. INFO:root:Writing scanpy matrix to adata_cellbrowser_04_01_19_CD8_subclustered/exprMatrix.tsv.gz; INFO:root:Transposing matrix; INFO:root:Writing gene-by-gene, without using pandas; INFO:root:Writing 8068 genes in total; INFO:root:Wrote 0 genes; INFO:root:Wrote 2000 genes; INFO:root:Wrote 4000 genes; INFO:root:Wrote 6000 genes; INFO:root:Wrote 8000 genes; INFO:root:Writing UMAP coords to adata_cellbrowser_04_01_19_CD8_subclustered/umap_coords.tsv; ERROR:root:Couldnt find cluster markers list. I am using an h5ad file to import my ann data object. Is that why there is some issue with finding cluster markers ? I am able to plot the clusters in a UMAP plot so I know that the 'louvain' observation exists. Any thoughts on why this is happening ?. Thanks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262#issuecomment-478685403
https://github.com/scverse/scanpy/issues/262#issuecomment-478907896:85,Availability,error,error,85,@maximilianh I think those messages are from your code? maybe you should improve the error message to include something like. > Try running sc.tl.rank_genes_groups(adata) to create the cluster annotation,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262#issuecomment-478907896
https://github.com/scverse/scanpy/issues/262#issuecomment-478907896:27,Integrability,message,messages,27,@maximilianh I think those messages are from your code? maybe you should improve the error message to include something like. > Try running sc.tl.rank_genes_groups(adata) to create the cluster annotation,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262#issuecomment-478907896
https://github.com/scverse/scanpy/issues/262#issuecomment-478907896:91,Integrability,message,message,91,@maximilianh I think those messages are from your code? maybe you should improve the error message to include something like. > Try running sc.tl.rank_genes_groups(adata) to create the cluster annotation,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262#issuecomment-478907896
https://github.com/scverse/scanpy/issues/262#issuecomment-497746073:72,Modifiability,variab,variable,72,"Hi, the expression matrix I exported from adata.write only have the top variable genes. Is there a way to output the raw matrix including all genes?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262#issuecomment-497746073
https://github.com/scverse/scanpy/issues/262#issuecomment-498194560:189,Modifiability,variab,variable,189,"The scanpyToCellbrowser function has an option useRaw that will use the; .raw matrix, if present, for the .tsv export. Otherwise, the raw matrix of all genes is stored as ad.raw.X and the; variable names are in ad.raw.var. You can use scanpyToCellbrowser to write; the matrix and all annotations, or anndataToTsv to write just the matrix.; Or use code from there to write your own. On Fri, May 31, 2019 at 5:14 PM Jing He <notifications@github.com> wrote:. > Hi, the expression matrix I exported from adata.write only have the top; > variable genes. Is there a way to output the raw matrix including all genes?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/262?email_source=notifications&email_token=AACL4TNOFS6MLIH44P6J5HDPYE6ENA5CNFSM4FU553M2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWVQBGI#issuecomment-497746073>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AACL4TORHPOQ2GTWTUGTAI3PYE6ENANCNFSM4FU553MQ>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262#issuecomment-498194560
https://github.com/scverse/scanpy/issues/262#issuecomment-498194560:534,Modifiability,variab,variable,534,"The scanpyToCellbrowser function has an option useRaw that will use the; .raw matrix, if present, for the .tsv export. Otherwise, the raw matrix of all genes is stored as ad.raw.X and the; variable names are in ad.raw.var. You can use scanpyToCellbrowser to write; the matrix and all annotations, or anndataToTsv to write just the matrix.; Or use code from there to write your own. On Fri, May 31, 2019 at 5:14 PM Jing He <notifications@github.com> wrote:. > Hi, the expression matrix I exported from adata.write only have the top; > variable genes. Is there a way to output the raw matrix including all genes?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/262?email_source=notifications&email_token=AACL4TNOFS6MLIH44P6J5HDPYE6ENA5CNFSM4FU553M2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWVQBGI#issuecomment-497746073>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AACL4TORHPOQ2GTWTUGTAI3PYE6ENANCNFSM4FU553MQ>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262#issuecomment-498194560
https://github.com/scverse/scanpy/issues/262#issuecomment-499091368:310,Availability,error,error,310,"Thanks for the suggestion. Actually, I am using cellxgene which takes the; h5ad file as an input. when using anndata.write() function, it only output; the anndata.X as the expression matrix. And also there is no option of; useRaw here.; Also, I tried to re-assign anndata.X = anndata.raw.X, but it returns an; error saying its wrong shape.; Do you have any suggestions?. Thanks a lot!. On Mon, Jun 3, 2019 at 6:03 AM Maximilian Haeussler <; notifications@github.com> wrote:. > The scanpyToCellbrowser function has an option useRaw that will use the; > .raw matrix, if present, for the .tsv export.; >; > Otherwise, the raw matrix of all genes is stored as ad.raw.X and the; > variable names are in ad.raw.var. You can use scanpyToCellbrowser to write; > the matrix and all annotations, or anndataToTsv to write just the matrix.; > Or use code from there to write your own.; >; > On Fri, May 31, 2019 at 5:14 PM Jing He <notifications@github.com> wrote:; >; > > Hi, the expression matrix I exported from adata.write only have the top; > > variable genes. Is there a way to output the raw matrix including all; > genes?; > >; > > —; > > You are receiving this because you were mentioned.; > > Reply to this email directly, view it on GitHub; > > <; > https://github.com/theislab/scanpy/issues/262?email_source=notifications&email_token=AACL4TNOFS6MLIH44P6J5HDPYE6ENA5CNFSM4FU553M2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWVQBGI#issuecomment-497746073; > >,; > > or mute the thread; > > <; > https://github.com/notifications/unsubscribe-auth/AACL4TORHPOQ2GTWTUGTAI3PYE6ENANCNFSM4FU553MQ; > >; > > .; > >; >; > —; > You are receiving this because you commented.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/262?email_source=notifications&email_token=AAUAIIIOXG5HSDCKTFYS7KLPYTT6BA5CNFSM4FU553M2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWY5RAA#issuecomment-498194560>,; > or mute the thread; > <https://github.com/n",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262#issuecomment-499091368
https://github.com/scverse/scanpy/issues/262#issuecomment-499091368:676,Modifiability,variab,variable,676,"Thanks for the suggestion. Actually, I am using cellxgene which takes the; h5ad file as an input. when using anndata.write() function, it only output; the anndata.X as the expression matrix. And also there is no option of; useRaw here.; Also, I tried to re-assign anndata.X = anndata.raw.X, but it returns an; error saying its wrong shape.; Do you have any suggestions?. Thanks a lot!. On Mon, Jun 3, 2019 at 6:03 AM Maximilian Haeussler <; notifications@github.com> wrote:. > The scanpyToCellbrowser function has an option useRaw that will use the; > .raw matrix, if present, for the .tsv export.; >; > Otherwise, the raw matrix of all genes is stored as ad.raw.X and the; > variable names are in ad.raw.var. You can use scanpyToCellbrowser to write; > the matrix and all annotations, or anndataToTsv to write just the matrix.; > Or use code from there to write your own.; >; > On Fri, May 31, 2019 at 5:14 PM Jing He <notifications@github.com> wrote:; >; > > Hi, the expression matrix I exported from adata.write only have the top; > > variable genes. Is there a way to output the raw matrix including all; > genes?; > >; > > —; > > You are receiving this because you were mentioned.; > > Reply to this email directly, view it on GitHub; > > <; > https://github.com/theislab/scanpy/issues/262?email_source=notifications&email_token=AACL4TNOFS6MLIH44P6J5HDPYE6ENA5CNFSM4FU553M2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWVQBGI#issuecomment-497746073; > >,; > > or mute the thread; > > <; > https://github.com/notifications/unsubscribe-auth/AACL4TORHPOQ2GTWTUGTAI3PYE6ENANCNFSM4FU553MQ; > >; > > .; > >; >; > —; > You are receiving this because you commented.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/262?email_source=notifications&email_token=AAUAIIIOXG5HSDCKTFYS7KLPYTT6BA5CNFSM4FU553M2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWY5RAA#issuecomment-498194560>,; > or mute the thread; > <https://github.com/n",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262#issuecomment-499091368
https://github.com/scverse/scanpy/issues/262#issuecomment-499091368:1038,Modifiability,variab,variable,1038,"ally, I am using cellxgene which takes the; h5ad file as an input. when using anndata.write() function, it only output; the anndata.X as the expression matrix. And also there is no option of; useRaw here.; Also, I tried to re-assign anndata.X = anndata.raw.X, but it returns an; error saying its wrong shape.; Do you have any suggestions?. Thanks a lot!. On Mon, Jun 3, 2019 at 6:03 AM Maximilian Haeussler <; notifications@github.com> wrote:. > The scanpyToCellbrowser function has an option useRaw that will use the; > .raw matrix, if present, for the .tsv export.; >; > Otherwise, the raw matrix of all genes is stored as ad.raw.X and the; > variable names are in ad.raw.var. You can use scanpyToCellbrowser to write; > the matrix and all annotations, or anndataToTsv to write just the matrix.; > Or use code from there to write your own.; >; > On Fri, May 31, 2019 at 5:14 PM Jing He <notifications@github.com> wrote:; >; > > Hi, the expression matrix I exported from adata.write only have the top; > > variable genes. Is there a way to output the raw matrix including all; > genes?; > >; > > —; > > You are receiving this because you were mentioned.; > > Reply to this email directly, view it on GitHub; > > <; > https://github.com/theislab/scanpy/issues/262?email_source=notifications&email_token=AACL4TNOFS6MLIH44P6J5HDPYE6ENA5CNFSM4FU553M2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWVQBGI#issuecomment-497746073; > >,; > > or mute the thread; > > <; > https://github.com/notifications/unsubscribe-auth/AACL4TORHPOQ2GTWTUGTAI3PYE6ENANCNFSM4FU553MQ; > >; > > .; > >; >; > —; > You are receiving this because you commented.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/262?email_source=notifications&email_token=AAUAIIIOXG5HSDCKTFYS7KLPYTT6BA5CNFSM4FU553M2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWY5RAA#issuecomment-498194560>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262#issuecomment-499091368
https://github.com/scverse/scanpy/issues/262#issuecomment-499111938:92,Modifiability,variab,variable,92,"Hi @hejing3283,. The wrong shape is probably because you have subsetted `adata.X` to highly variable genes, or did some additional filtering after storing data in `adata.raw`. For a while now scanpy avoids filtering highly variable genes, but instead annotates them in `adata.var['highly_variable']` which is then used in `sc.pp.pca()`. I would suggest you use `subset=False` next time you use `sc.pp.highly_variable()` to avoid different dimensions in `adata.X` and `adata.raw.X`. You can easily proceed by just making a new anndata object from `adata.raw.X`, `adata.raw.var` and `adata.raw.obs` and storing this to be loaded into cellxgene. Just do the following:; ```; adata_raw = sc.AnnData(X=adata.raw.X, obs=adata.raw.obs, var=adata.raw.var); adata_raw.write(my_file); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262#issuecomment-499111938
https://github.com/scverse/scanpy/issues/262#issuecomment-499111938:223,Modifiability,variab,variable,223,"Hi @hejing3283,. The wrong shape is probably because you have subsetted `adata.X` to highly variable genes, or did some additional filtering after storing data in `adata.raw`. For a while now scanpy avoids filtering highly variable genes, but instead annotates them in `adata.var['highly_variable']` which is then used in `sc.pp.pca()`. I would suggest you use `subset=False` next time you use `sc.pp.highly_variable()` to avoid different dimensions in `adata.X` and `adata.raw.X`. You can easily proceed by just making a new anndata object from `adata.raw.X`, `adata.raw.var` and `adata.raw.obs` and storing this to be loaded into cellxgene. Just do the following:; ```; adata_raw = sc.AnnData(X=adata.raw.X, obs=adata.raw.obs, var=adata.raw.var); adata_raw.write(my_file); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262#issuecomment-499111938
https://github.com/scverse/scanpy/issues/262#issuecomment-499111938:620,Performance,load,loaded,620,"Hi @hejing3283,. The wrong shape is probably because you have subsetted `adata.X` to highly variable genes, or did some additional filtering after storing data in `adata.raw`. For a while now scanpy avoids filtering highly variable genes, but instead annotates them in `adata.var['highly_variable']` which is then used in `sc.pp.pca()`. I would suggest you use `subset=False` next time you use `sc.pp.highly_variable()` to avoid different dimensions in `adata.X` and `adata.raw.X`. You can easily proceed by just making a new anndata object from `adata.raw.X`, `adata.raw.var` and `adata.raw.obs` and storing this to be loaded into cellxgene. Just do the following:; ```; adata_raw = sc.AnnData(X=adata.raw.X, obs=adata.raw.obs, var=adata.raw.var); adata_raw.write(my_file); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262#issuecomment-499111938
https://github.com/scverse/scanpy/issues/262#issuecomment-499111938:199,Safety,avoid,avoids,199,"Hi @hejing3283,. The wrong shape is probably because you have subsetted `adata.X` to highly variable genes, or did some additional filtering after storing data in `adata.raw`. For a while now scanpy avoids filtering highly variable genes, but instead annotates them in `adata.var['highly_variable']` which is then used in `sc.pp.pca()`. I would suggest you use `subset=False` next time you use `sc.pp.highly_variable()` to avoid different dimensions in `adata.X` and `adata.raw.X`. You can easily proceed by just making a new anndata object from `adata.raw.X`, `adata.raw.var` and `adata.raw.obs` and storing this to be loaded into cellxgene. Just do the following:; ```; adata_raw = sc.AnnData(X=adata.raw.X, obs=adata.raw.obs, var=adata.raw.var); adata_raw.write(my_file); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262#issuecomment-499111938
https://github.com/scverse/scanpy/issues/262#issuecomment-499111938:423,Safety,avoid,avoid,423,"Hi @hejing3283,. The wrong shape is probably because you have subsetted `adata.X` to highly variable genes, or did some additional filtering after storing data in `adata.raw`. For a while now scanpy avoids filtering highly variable genes, but instead annotates them in `adata.var['highly_variable']` which is then used in `sc.pp.pca()`. I would suggest you use `subset=False` next time you use `sc.pp.highly_variable()` to avoid different dimensions in `adata.X` and `adata.raw.X`. You can easily proceed by just making a new anndata object from `adata.raw.X`, `adata.raw.var` and `adata.raw.obs` and storing this to be loaded into cellxgene. Just do the following:; ```; adata_raw = sc.AnnData(X=adata.raw.X, obs=adata.raw.obs, var=adata.raw.var); adata_raw.write(my_file); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262#issuecomment-499111938
https://github.com/scverse/scanpy/issues/262#issuecomment-499126695:264,Modifiability,variab,variable,264,"Hi, . Thanks so much for the explanations! Doing it now and it works. . Best,; Jing. > On Jun 5, 2019, at 10:39, MalteDLuecken <notifications@github.com> wrote:; > ; > Hi @hejing3283,; > ; > The wrong shape is probably because you have subsetted adata.X to highly variable genes, or did some additional filtering after storing data in adata.raw. For a while now scanpy avoids filtering highly variable genes, but instead annotates them in adata.var['highly_variable'] which is then used in sc.pp.pca(). I would suggest you use subset=False next time you use sc.pp.highly_variable() to avoid different dimensions in adata.X and adata.raw.X.; > ; > You can easily proceed by just making a new anndata object from adata.raw.X, adata.raw.var and adata.raw.obs and storing this to be loaded into cellxgene. Just do the following:; > ; > adata_raw.write(my_file); > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262#issuecomment-499126695
https://github.com/scverse/scanpy/issues/262#issuecomment-499126695:393,Modifiability,variab,variable,393,"Hi, . Thanks so much for the explanations! Doing it now and it works. . Best,; Jing. > On Jun 5, 2019, at 10:39, MalteDLuecken <notifications@github.com> wrote:; > ; > Hi @hejing3283,; > ; > The wrong shape is probably because you have subsetted adata.X to highly variable genes, or did some additional filtering after storing data in adata.raw. For a while now scanpy avoids filtering highly variable genes, but instead annotates them in adata.var['highly_variable'] which is then used in sc.pp.pca(). I would suggest you use subset=False next time you use sc.pp.highly_variable() to avoid different dimensions in adata.X and adata.raw.X.; > ; > You can easily proceed by just making a new anndata object from adata.raw.X, adata.raw.var and adata.raw.obs and storing this to be loaded into cellxgene. Just do the following:; > ; > adata_raw.write(my_file); > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262#issuecomment-499126695
https://github.com/scverse/scanpy/issues/262#issuecomment-499126695:779,Performance,load,loaded,779,"Hi, . Thanks so much for the explanations! Doing it now and it works. . Best,; Jing. > On Jun 5, 2019, at 10:39, MalteDLuecken <notifications@github.com> wrote:; > ; > Hi @hejing3283,; > ; > The wrong shape is probably because you have subsetted adata.X to highly variable genes, or did some additional filtering after storing data in adata.raw. For a while now scanpy avoids filtering highly variable genes, but instead annotates them in adata.var['highly_variable'] which is then used in sc.pp.pca(). I would suggest you use subset=False next time you use sc.pp.highly_variable() to avoid different dimensions in adata.X and adata.raw.X.; > ; > You can easily proceed by just making a new anndata object from adata.raw.X, adata.raw.var and adata.raw.obs and storing this to be loaded into cellxgene. Just do the following:; > ; > adata_raw.write(my_file); > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262#issuecomment-499126695
https://github.com/scverse/scanpy/issues/262#issuecomment-499126695:369,Safety,avoid,avoids,369,"Hi, . Thanks so much for the explanations! Doing it now and it works. . Best,; Jing. > On Jun 5, 2019, at 10:39, MalteDLuecken <notifications@github.com> wrote:; > ; > Hi @hejing3283,; > ; > The wrong shape is probably because you have subsetted adata.X to highly variable genes, or did some additional filtering after storing data in adata.raw. For a while now scanpy avoids filtering highly variable genes, but instead annotates them in adata.var['highly_variable'] which is then used in sc.pp.pca(). I would suggest you use subset=False next time you use sc.pp.highly_variable() to avoid different dimensions in adata.X and adata.raw.X.; > ; > You can easily proceed by just making a new anndata object from adata.raw.X, adata.raw.var and adata.raw.obs and storing this to be loaded into cellxgene. Just do the following:; > ; > adata_raw.write(my_file); > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262#issuecomment-499126695
https://github.com/scverse/scanpy/issues/262#issuecomment-499126695:585,Safety,avoid,avoid,585,"Hi, . Thanks so much for the explanations! Doing it now and it works. . Best,; Jing. > On Jun 5, 2019, at 10:39, MalteDLuecken <notifications@github.com> wrote:; > ; > Hi @hejing3283,; > ; > The wrong shape is probably because you have subsetted adata.X to highly variable genes, or did some additional filtering after storing data in adata.raw. For a while now scanpy avoids filtering highly variable genes, but instead annotates them in adata.var['highly_variable'] which is then used in sc.pp.pca(). I would suggest you use subset=False next time you use sc.pp.highly_variable() to avoid different dimensions in adata.X and adata.raw.X.; > ; > You can easily proceed by just making a new anndata object from adata.raw.X, adata.raw.var and adata.raw.obs and storing this to be loaded into cellxgene. Just do the following:; > ; > adata_raw.write(my_file); > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262#issuecomment-499126695
https://github.com/scverse/scanpy/issues/262#issuecomment-1476035869:138,Availability,error,error,138,"> ```python; > scipy.io.mmwrite; > ```. This code doesn't actually work - rows and columns are switched in the matrix, and it produces an error when you try to read in the output using either `Scanpy` or `Seurat` wrapper functions. Perhaps it's a package version thing though..",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262#issuecomment-1476035869
https://github.com/scverse/scanpy/issues/262#issuecomment-1476035869:213,Integrability,wrap,wrapper,213,"> ```python; > scipy.io.mmwrite; > ```. This code doesn't actually work - rows and columns are switched in the matrix, and it produces an error when you try to read in the output using either `Scanpy` or `Seurat` wrapper functions. Perhaps it's a package version thing though..",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262#issuecomment-1476035869
https://github.com/scverse/scanpy/issues/262#issuecomment-1520696083:150,Availability,error,error,150,"> > ```python; > > scipy.io.mmwrite; > > ```; > ; > This code doesn't actually work - rows and columns are switched in the matrix, and it produces an error when you try to read in the output using either `Scanpy` or `Seurat` wrapper functions. Perhaps it's a package version thing though.. I was having the same issue as well. I ended up doing what was suggested above:. `adata.T.to_df().to_csv('matrix.csv')`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262#issuecomment-1520696083
https://github.com/scverse/scanpy/issues/262#issuecomment-1520696083:225,Integrability,wrap,wrapper,225,"> > ```python; > > scipy.io.mmwrite; > > ```; > ; > This code doesn't actually work - rows and columns are switched in the matrix, and it produces an error when you try to read in the output using either `Scanpy` or `Seurat` wrapper functions. Perhaps it's a package version thing though.. I was having the same issue as well. I ended up doing what was suggested above:. `adata.T.to_df().to_csv('matrix.csv')`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262#issuecomment-1520696083
https://github.com/scverse/scanpy/issues/263#issuecomment-421872224:289,Availability,error,errors,289,"This works with a non-backed adata, this works:. ```python; sc.pl.pca(adata[:, :5], color=""0""); ```. I'm not totally sure what exactly the other backed modes are supposed to do (especially when called instantiated by `read`, but none of them work either. `r`, `r+`, and `a` return similar errors, while `x`, `w`, and `w+` don't work, but that's because the file exists.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263#issuecomment-421872224
https://github.com/scverse/scanpy/issues/263#issuecomment-421873798:168,Availability,error,error,168,"Oh, and I just realized it's meant to be `backed=False` and `backed=True`. I'd assumed the documentation was wrong, as `backed=True` and `backed=""True""` both throw the error:. ```python; ValueError: Invalid mode; must be one of r, r+, w, w-, x, a; ```. And I'd never checked `backed=False`. That's probably more for another issue though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263#issuecomment-421873798
https://github.com/scverse/scanpy/issues/263#issuecomment-422099872:223,Performance,load,loading,223,"Slicing backed AnnData objects is not fully stable, yet. It's a bit tricky as `h5py.datasets` don't support the same general indexing operations as `AnnData`. The sliced AnnData should not be in backed mode [think of it as loading a small portion of the data into memory]. As for https://github.com/theislab/anndata/issues/61, @Sergei, do you have bandwidth? You're still the person who would be supposed to make the backed mode fully functional.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263#issuecomment-422099872
https://github.com/scverse/scanpy/issues/263#issuecomment-422243349:115,Security,access,access,115,"Just a little about my use case for backed mode:. I run all of my computations in memory. However, the HPCs I have access to limit the ways I can use them interactively, so I like to do my visualization locally on a backed object. Personally, I really like this feature of Scanpy. Since I've been playing with more complicated visualization code, I'm starting to run into these issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263#issuecomment-422243349
https://github.com/scverse/scanpy/issues/263#issuecomment-422394446:163,Security,access,accessing,163,"OK, got it! So you're actually moving the data between your HPC and your laptop? Why does the typical forwarding via `ssh -L ...` that I think most people use for accessing the server running jupyter lab, notebooks, tensorboard etc. locally doesn't work for you (e.g. [here](http://benjlindsay.com/blog/running-jupyter-lab-remotely/))? I can do all the visualizations on the remote infrastructure and never have to move around data, which is nice. ;). Of course, though, the backed functionality of Scanpy should become fully stable.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263#issuecomment-422394446
https://github.com/scverse/scanpy/issues/263#issuecomment-422402199:334,Security,access,accessing,334,"I don't know about Isaac, but our HPC nodes don't like long-running; jypyters on them. On Tue, Sep 18, 2018 at 9:32 AM, Alex Wolf <notifications@github.com> wrote:. > OK, got it! So you're actually moving the data between your HPC and your; > laptop? Why does the typical forwarding via ssh -L ... that I think most; > people use for accessing the server running jupyter lab, notebooks,; > tensorboard etc. locally doesn't work for you (e.g. here; > <http://benjlindsay.com/blog/running-jupyter-lab-remotely/>)? I can do; > all the visualizations on the remote infrastructure and never have to move; > around data, which is nice. ;); >; > Of course, though, the backed functionality of Scanpy should become fully; > stable.; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/263#issuecomment-422394446>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AAS-TQtLnSFiZOCsL0oqDgJcrzo1VJXJks5ucPXugaJpZM4WokmY>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263#issuecomment-422402199
https://github.com/scverse/scanpy/issues/263#issuecomment-422629703:186,Availability,down,down,186,"Yeah, two out of three of the HPCs I have access to don't make using a jupyter server particularly easy (one corporate firewall, one government). I use that with the other one, but it's down this week 😢",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263#issuecomment-422629703
https://github.com/scverse/scanpy/issues/263#issuecomment-422629703:42,Security,access,access,42,"Yeah, two out of three of the HPCs I have access to don't make using a jupyter server particularly easy (one corporate firewall, one government). I use that with the other one, but it's down this week 😢",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263#issuecomment-422629703
https://github.com/scverse/scanpy/issues/263#issuecomment-422629703:119,Security,firewall,firewall,119,"Yeah, two out of three of the HPCs I have access to don't make using a jupyter server particularly easy (one corporate firewall, one government). I use that with the other one, but it's down this week 😢",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263#issuecomment-422629703
https://github.com/scverse/scanpy/issues/264#issuecomment-423300225:1224,Usability,learn,learn,1224,"ng with the `svd_solver='arpack'` parameter. I attach some examples:. ```python; >>> sc.tl.pca(adata_h, svd_solver='arpack'). >>> adata_h.uns['pca']. {'variance': array([ 0. , 0. , 0. , 0. , 0. ,; 0. , 0. , 0. , 0. , 0. ,; 0. , 0. , 0. , 0. , 0. ,; 0. , 0. , 0. , 0. , 0. ,; 0. , 0. , 0. , 0. , 0. ,; 0. , 0. , 0. , 0. , 0. ,; 0. , 0. , 0. , 0. , 0. ,; 0. , 0. , 2633.797 , 457.86526 , 316.44687 ,; 237.71556 , 143.87927 , 119.6577 , 105.01371 , 91.51559 ,; 66.951355, 61.23979 , 59.957714, 58.998177, 57.82413 ],; dtype=float32),; 'variance_ratio': array([0. , 0. , 0. , 0. , 0. ,; 0. , 0. , 0. , 0. , 0. ,; 0. , 0. , 0. , 0. , 0. ,; 0. , 0. , 0. , 0. , 0. ,; 0. , 0. , 0. , 0. , 0. ,; 0. , 0. , 0. , 0. , 0. ,; 0. , 0. , 0. , 0. , 0. ,; 0. , 0. , 0.5971161 , 0.10380401, 0.07174262,; 0.05389321, 0.0326193 , 0.02712796, 0.02380797, 0.02074778,; 0.01517874, 0.01388386, 0.01359319, 0.01337565, 0.01310948],; dtype=float32)}. >>> sc.tl.pca(adata_h); Note that scikit-learn's randomized PCA might not be exactly reproducible across different computational platforms. For exact reproducibility, choose `svd_solver='arpack'.` This will likely become the Scanpy default in the future. >>> adata_h.uns['pca']. {'variance': array([2.63379761e+03, 4.57865112e+02, 3.16446930e+02, 2.37715851e+02,; 1.43879318e+02, 1.19657700e+02, 1.05013855e+02, 9.15156784e+01,; 6.69513855e+01, 6.12398453e+01, 5.99577942e+01, 5.89982376e+01,; 5.78241539e+01, 6.29622976e-09, 9.35712879e-12, 9.35712879e-12,; 9.35712879e-12, 9.35712879e-12, 9.35712879e-12, 9.35712879e-12,; 9.35712879e-12, 9.35712879e-12, 9.35712879e-12, 9.35712879e-12,; 9.35712879e-12, 9.35712879e-12, 9.35712879e-12, 9.35712879e-12,; 9.35712879e-12, 9.35712879e-12, 9.35712879e-12, 9.35712879e-12,; 9.35712879e-12, 9.35712879e-12, 9.35712879e-12, 9.35712879e-12,; 9.35712879e-12, 9.35712879e-12, 9.35712879e-12, 9.35712879e-12,; 9.35712879e-12, 9.35712879e-12, 9.35712879e-12, 9.35712879e-12,; 9.35712879e-12, 9.35712879e-12, 9.35712879e-12, 9.35712879e-1",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/264#issuecomment-423300225
https://github.com/scverse/scanpy/issues/264#issuecomment-423784002:53,Usability,learn,learn,53,"Arrrgh, this prepending of the 0s is a bug in scikit-learn. Thank you very much for pointing it out. Let me briefly think about solving this elegantly...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/264#issuecomment-423784002
https://github.com/scverse/scanpy/issues/264#issuecomment-766319589:281,Availability,error,error,281,"Still occurring, though it's a really weird case. Here's some code to reproduce:. ```python; a = sc.AnnData(np.ones((100, 100))); sc.pp.pca(a); # RuntimeWarning: invalid value encountered in true_divide; # self.explained_variance_ / total_var.sum(); sc.pl.pca(a) # Throws the same error as above; a.uns[""pca""]; ```. ```; {'params': {'zero_center': True, 'use_highly_variable': False},; 'variance': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,; 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,; 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],; dtype=float32),; 'variance_ratio': array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,; nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,; nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,; nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],; dtype=float32)}; ```. Though I do get the same behaviour with `svd_solver=""randomized""`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/264#issuecomment-766319589
https://github.com/scverse/scanpy/issues/265#issuecomment-423514211:66,Deployability,integrat,integrate,66,"CCA does not have code in python, which will make it difficult to integrate, pySCENIC is probably easier but I would rather ask the developers. @falexwolf We should consider a way to facilitate scanpy 'plugins'. A quick search shows me that this could be possible: https://packaging.python.org/guides/creating-and-discovering-plugins/ but honestly I don't know how it works. Nevertheless, given the number of tools that continue to appear we should consider a scheme that facilitate how developers can take advantage of scanpy preprocessing, storing, analysis and visualization tools.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-423514211
https://github.com/scverse/scanpy/issues/265#issuecomment-423514211:66,Integrability,integrat,integrate,66,"CCA does not have code in python, which will make it difficult to integrate, pySCENIC is probably easier but I would rather ask the developers. @falexwolf We should consider a way to facilitate scanpy 'plugins'. A quick search shows me that this could be possible: https://packaging.python.org/guides/creating-and-discovering-plugins/ but honestly I don't know how it works. Nevertheless, given the number of tools that continue to appear we should consider a scheme that facilitate how developers can take advantage of scanpy preprocessing, storing, analysis and visualization tools.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-423514211
https://github.com/scverse/scanpy/issues/265#issuecomment-423514211:202,Modifiability,plugin,plugins,202,"CCA does not have code in python, which will make it difficult to integrate, pySCENIC is probably easier but I would rather ask the developers. @falexwolf We should consider a way to facilitate scanpy 'plugins'. A quick search shows me that this could be possible: https://packaging.python.org/guides/creating-and-discovering-plugins/ but honestly I don't know how it works. Nevertheless, given the number of tools that continue to appear we should consider a scheme that facilitate how developers can take advantage of scanpy preprocessing, storing, analysis and visualization tools.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-423514211
https://github.com/scverse/scanpy/issues/265#issuecomment-423514211:326,Modifiability,plugin,plugins,326,"CCA does not have code in python, which will make it difficult to integrate, pySCENIC is probably easier but I would rather ask the developers. @falexwolf We should consider a way to facilitate scanpy 'plugins'. A quick search shows me that this could be possible: https://packaging.python.org/guides/creating-and-discovering-plugins/ but honestly I don't know how it works. Nevertheless, given the number of tools that continue to appear we should consider a scheme that facilitate how developers can take advantage of scanpy preprocessing, storing, analysis and visualization tools.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-423514211
https://github.com/scverse/scanpy/issues/265#issuecomment-423514211:294,Usability,guid,guides,294,"CCA does not have code in python, which will make it difficult to integrate, pySCENIC is probably easier but I would rather ask the developers. @falexwolf We should consider a way to facilitate scanpy 'plugins'. A quick search shows me that this could be possible: https://packaging.python.org/guides/creating-and-discovering-plugins/ but honestly I don't know how it works. Nevertheless, given the number of tools that continue to appear we should consider a scheme that facilitate how developers can take advantage of scanpy preprocessing, storing, analysis and visualization tools.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-423514211
https://github.com/scverse/scanpy/issues/265#issuecomment-423784343:66,Integrability,wrap,wrapper,66,"I like Seurat's CCA. A pull request using `rpy2` similar to the R wrapper of Haghverdi et al.'s version of [MNN](https://github.com/theislab/scanpy/blob/master/scanpy/rtools/mnn_correct.py) would be welcome. Regarding ""plugins"": I guess a lot of Scanpy's functionality already consists in ""plugins"":; - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html; - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.dca.html; - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.magic.html; - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.phate.html; - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html. and a lot more are on the way, as far as I know. I guess the strategy of having an optional dependency of the respective and a small wrapper in Scanpy is a scalable strategy. Do you think we need to do more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-423784343
https://github.com/scverse/scanpy/issues/265#issuecomment-423784343:759,Integrability,depend,dependency,759,"I like Seurat's CCA. A pull request using `rpy2` similar to the R wrapper of Haghverdi et al.'s version of [MNN](https://github.com/theislab/scanpy/blob/master/scanpy/rtools/mnn_correct.py) would be welcome. Regarding ""plugins"": I guess a lot of Scanpy's functionality already consists in ""plugins"":; - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html; - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.dca.html; - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.magic.html; - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.phate.html; - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html. and a lot more are on the way, as far as I know. I guess the strategy of having an optional dependency of the respective and a small wrapper in Scanpy is a scalable strategy. Do you think we need to do more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-423784343
https://github.com/scverse/scanpy/issues/265#issuecomment-423784343:800,Integrability,wrap,wrapper,800,"I like Seurat's CCA. A pull request using `rpy2` similar to the R wrapper of Haghverdi et al.'s version of [MNN](https://github.com/theislab/scanpy/blob/master/scanpy/rtools/mnn_correct.py) would be welcome. Regarding ""plugins"": I guess a lot of Scanpy's functionality already consists in ""plugins"":; - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html; - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.dca.html; - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.magic.html; - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.phate.html; - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html. and a lot more are on the way, as far as I know. I guess the strategy of having an optional dependency of the respective and a small wrapper in Scanpy is a scalable strategy. Do you think we need to do more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-423784343
https://github.com/scverse/scanpy/issues/265#issuecomment-423784343:219,Modifiability,plugin,plugins,219,"I like Seurat's CCA. A pull request using `rpy2` similar to the R wrapper of Haghverdi et al.'s version of [MNN](https://github.com/theislab/scanpy/blob/master/scanpy/rtools/mnn_correct.py) would be welcome. Regarding ""plugins"": I guess a lot of Scanpy's functionality already consists in ""plugins"":; - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html; - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.dca.html; - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.magic.html; - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.phate.html; - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html. and a lot more are on the way, as far as I know. I guess the strategy of having an optional dependency of the respective and a small wrapper in Scanpy is a scalable strategy. Do you think we need to do more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-423784343
https://github.com/scverse/scanpy/issues/265#issuecomment-423784343:290,Modifiability,plugin,plugins,290,"I like Seurat's CCA. A pull request using `rpy2` similar to the R wrapper of Haghverdi et al.'s version of [MNN](https://github.com/theislab/scanpy/blob/master/scanpy/rtools/mnn_correct.py) would be welcome. Regarding ""plugins"": I guess a lot of Scanpy's functionality already consists in ""plugins"":; - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html; - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.dca.html; - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.magic.html; - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.phate.html; - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html. and a lot more are on the way, as far as I know. I guess the strategy of having an optional dependency of the respective and a small wrapper in Scanpy is a scalable strategy. Do you think we need to do more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-423784343
https://github.com/scverse/scanpy/issues/265#issuecomment-423784343:823,Performance,scalab,scalable,823,"I like Seurat's CCA. A pull request using `rpy2` similar to the R wrapper of Haghverdi et al.'s version of [MNN](https://github.com/theislab/scanpy/blob/master/scanpy/rtools/mnn_correct.py) would be welcome. Regarding ""plugins"": I guess a lot of Scanpy's functionality already consists in ""plugins"":; - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html; - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.dca.html; - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.magic.html; - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.phate.html; - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html. and a lot more are on the way, as far as I know. I guess the strategy of having an optional dependency of the respective and a small wrapper in Scanpy is a scalable strategy. Do you think we need to do more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-423784343
https://github.com/scverse/scanpy/issues/265#issuecomment-423799757:56,Usability,learn,learn,56,Sklearn has its implementation of [CCA](; http://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.CCA.html) but that would allow the alignment of two samples only. Recently a multi sample approach was implemented in [pyrcca](; https://github.com/gallantlab/pyrcca) library for which there is a biorXiv paper.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-423799757
https://github.com/scverse/scanpy/issues/265#issuecomment-423915827:298,Deployability,install,installed,298,"@falexwolf I think it is worth to have a `scanpy.plugin` or `scanpy.extension` or something shorter like `scanpy.pg` or `scanpy.ext` that aggregates all plugins. First, this clarifies for the user that the tool he/she is using is not directly developed by scanpy. Second, this allows plugins to be installed separately without having to update scanpy's code. The idea is that scanpy will be able to discover any plugins installed. On the side of developers, this could facilitate integration with scanpy. We can get inspired by flask extensions: http://flask.pocoo.org/docs/1.0/extensiondev/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-423915827
https://github.com/scverse/scanpy/issues/265#issuecomment-423915827:337,Deployability,update,update,337,"@falexwolf I think it is worth to have a `scanpy.plugin` or `scanpy.extension` or something shorter like `scanpy.pg` or `scanpy.ext` that aggregates all plugins. First, this clarifies for the user that the tool he/she is using is not directly developed by scanpy. Second, this allows plugins to be installed separately without having to update scanpy's code. The idea is that scanpy will be able to discover any plugins installed. On the side of developers, this could facilitate integration with scanpy. We can get inspired by flask extensions: http://flask.pocoo.org/docs/1.0/extensiondev/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-423915827
https://github.com/scverse/scanpy/issues/265#issuecomment-423915827:420,Deployability,install,installed,420,"@falexwolf I think it is worth to have a `scanpy.plugin` or `scanpy.extension` or something shorter like `scanpy.pg` or `scanpy.ext` that aggregates all plugins. First, this clarifies for the user that the tool he/she is using is not directly developed by scanpy. Second, this allows plugins to be installed separately without having to update scanpy's code. The idea is that scanpy will be able to discover any plugins installed. On the side of developers, this could facilitate integration with scanpy. We can get inspired by flask extensions: http://flask.pocoo.org/docs/1.0/extensiondev/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-423915827
https://github.com/scverse/scanpy/issues/265#issuecomment-423915827:480,Deployability,integrat,integration,480,"@falexwolf I think it is worth to have a `scanpy.plugin` or `scanpy.extension` or something shorter like `scanpy.pg` or `scanpy.ext` that aggregates all plugins. First, this clarifies for the user that the tool he/she is using is not directly developed by scanpy. Second, this allows plugins to be installed separately without having to update scanpy's code. The idea is that scanpy will be able to discover any plugins installed. On the side of developers, this could facilitate integration with scanpy. We can get inspired by flask extensions: http://flask.pocoo.org/docs/1.0/extensiondev/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-423915827
https://github.com/scverse/scanpy/issues/265#issuecomment-423915827:480,Integrability,integrat,integration,480,"@falexwolf I think it is worth to have a `scanpy.plugin` or `scanpy.extension` or something shorter like `scanpy.pg` or `scanpy.ext` that aggregates all plugins. First, this clarifies for the user that the tool he/she is using is not directly developed by scanpy. Second, this allows plugins to be installed separately without having to update scanpy's code. The idea is that scanpy will be able to discover any plugins installed. On the side of developers, this could facilitate integration with scanpy. We can get inspired by flask extensions: http://flask.pocoo.org/docs/1.0/extensiondev/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-423915827
https://github.com/scverse/scanpy/issues/265#issuecomment-423915827:49,Modifiability,plugin,plugin,49,"@falexwolf I think it is worth to have a `scanpy.plugin` or `scanpy.extension` or something shorter like `scanpy.pg` or `scanpy.ext` that aggregates all plugins. First, this clarifies for the user that the tool he/she is using is not directly developed by scanpy. Second, this allows plugins to be installed separately without having to update scanpy's code. The idea is that scanpy will be able to discover any plugins installed. On the side of developers, this could facilitate integration with scanpy. We can get inspired by flask extensions: http://flask.pocoo.org/docs/1.0/extensiondev/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-423915827
https://github.com/scverse/scanpy/issues/265#issuecomment-423915827:153,Modifiability,plugin,plugins,153,"@falexwolf I think it is worth to have a `scanpy.plugin` or `scanpy.extension` or something shorter like `scanpy.pg` or `scanpy.ext` that aggregates all plugins. First, this clarifies for the user that the tool he/she is using is not directly developed by scanpy. Second, this allows plugins to be installed separately without having to update scanpy's code. The idea is that scanpy will be able to discover any plugins installed. On the side of developers, this could facilitate integration with scanpy. We can get inspired by flask extensions: http://flask.pocoo.org/docs/1.0/extensiondev/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-423915827
https://github.com/scverse/scanpy/issues/265#issuecomment-423915827:284,Modifiability,plugin,plugins,284,"@falexwolf I think it is worth to have a `scanpy.plugin` or `scanpy.extension` or something shorter like `scanpy.pg` or `scanpy.ext` that aggregates all plugins. First, this clarifies for the user that the tool he/she is using is not directly developed by scanpy. Second, this allows plugins to be installed separately without having to update scanpy's code. The idea is that scanpy will be able to discover any plugins installed. On the side of developers, this could facilitate integration with scanpy. We can get inspired by flask extensions: http://flask.pocoo.org/docs/1.0/extensiondev/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-423915827
https://github.com/scverse/scanpy/issues/265#issuecomment-423915827:412,Modifiability,plugin,plugins,412,"@falexwolf I think it is worth to have a `scanpy.plugin` or `scanpy.extension` or something shorter like `scanpy.pg` or `scanpy.ext` that aggregates all plugins. First, this clarifies for the user that the tool he/she is using is not directly developed by scanpy. Second, this allows plugins to be installed separately without having to update scanpy's code. The idea is that scanpy will be able to discover any plugins installed. On the side of developers, this could facilitate integration with scanpy. We can get inspired by flask extensions: http://flask.pocoo.org/docs/1.0/extensiondev/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-423915827
https://github.com/scverse/scanpy/issues/265#issuecomment-424271229:56,Usability,learn,learn,56,> Sklearn has its implementation of [CCA](http://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.CCA.html) but that would allow the alignment of two samples only. Recently a multi sample approach was implemented in [pyrcca](https://github.com/gallantlab/pyrcca) library for which there is a biorXiv paper. Is it suitable for single cell data ?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-424271229
https://github.com/scverse/scanpy/issues/265#issuecomment-424548158:298,Integrability,wrap,wrap,298,"@fidelram Yes, makes sense. Let's see whether we manage to organize it this way. There will be a few plugins coming soon and I'll talk with the one doing it about this. @wangjiawen2013 The Seurat developers did a bit more than simply fitting a standard CCA. So I'd assume that it'd be some work to wrap sklearn's CCA or pyrcca so that it performs similar to Seurat's CCA on single cell data...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-424548158
https://github.com/scverse/scanpy/issues/265#issuecomment-424548158:101,Modifiability,plugin,plugins,101,"@fidelram Yes, makes sense. Let's see whether we manage to organize it this way. There will be a few plugins coming soon and I'll talk with the one doing it about this. @wangjiawen2013 The Seurat developers did a bit more than simply fitting a standard CCA. So I'd assume that it'd be some work to wrap sklearn's CCA or pyrcca so that it performs similar to Seurat's CCA on single cell data...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-424548158
https://github.com/scverse/scanpy/issues/265#issuecomment-424548158:338,Performance,perform,performs,338,"@fidelram Yes, makes sense. Let's see whether we manage to organize it this way. There will be a few plugins coming soon and I'll talk with the one doing it about this. @wangjiawen2013 The Seurat developers did a bit more than simply fitting a standard CCA. So I'd assume that it'd be some work to wrap sklearn's CCA or pyrcca so that it performs similar to Seurat's CCA on single cell data...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-424548158
https://github.com/scverse/scanpy/issues/265#issuecomment-424548158:227,Usability,simpl,simply,227,"@fidelram Yes, makes sense. Let's see whether we manage to organize it this way. There will be a few plugins coming soon and I'll talk with the one doing it about this. @wangjiawen2013 The Seurat developers did a bit more than simply fitting a standard CCA. So I'd assume that it'd be some work to wrap sklearn's CCA or pyrcca so that it performs similar to Seurat's CCA on single cell data...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-424548158
https://github.com/scverse/scanpy/issues/265#issuecomment-424605733:41,Usability,learn,learning,41,"Given that UMAP can be used for manifold learning, shouldn’t be possibile to align experiments using UMAP? Who wants to join me in this evaluation?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-424605733
https://github.com/scverse/scanpy/issues/265#issuecomment-424704691:697,Deployability,integrat,integrates,697,"UMAP won't do any correction of batch effects for you, like CCA (it looks at the basis that leads to the greatest overlap between the batches, assuming that this captures the common biological variation and projects out everything else, assuming it's nuisance/technical batch effects). Similar for all other ""alignment tools"": you throw away some information in order to align. When you map a new dataset into an existing dataset using UMAP, this will do an _exact_ mapping. If you have pronounced batch effects, the second dataset will cluster as a whole far away from the first. So, I don't think that there will be much to gain. Why not give BBKNN (https://github.com/Teichlab/bbknn) a try? It integrates nicely with Scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-424704691
https://github.com/scverse/scanpy/issues/265#issuecomment-424704691:697,Integrability,integrat,integrates,697,"UMAP won't do any correction of batch effects for you, like CCA (it looks at the basis that leads to the greatest overlap between the batches, assuming that this captures the common biological variation and projects out everything else, assuming it's nuisance/technical batch effects). Similar for all other ""alignment tools"": you throw away some information in order to align. When you map a new dataset into an existing dataset using UMAP, this will do an _exact_ mapping. If you have pronounced batch effects, the second dataset will cluster as a whole far away from the first. So, I don't think that there will be much to gain. Why not give BBKNN (https://github.com/Teichlab/bbknn) a try? It integrates nicely with Scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-424704691
https://github.com/scverse/scanpy/issues/265#issuecomment-424799480:132,Usability,simpl,simple,132,"Cool! . You're right, if you don't have strong batch effects across your samples, you don't need any batch correction like CCA. A a simple UMAP of all the samples gives you a reasonable picture of what happens.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-424799480
https://github.com/scverse/scanpy/issues/265#issuecomment-424995243:46,Modifiability,plugin,plugin,46,"Let’s continue the discussion about a general plugin mechanism in #271, and this thread for CCA specifically.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-424995243
https://github.com/scverse/scanpy/issues/265#issuecomment-471439971:88,Integrability,protocol,protocol,88,"Just curious about the case of 'batch effect', it looks like to me library construction protocol/chemicals is main source of batch effects. However, if I use same protocols, sequencing platform and slight difference of sequencing depth for some sample, but in different time course, would you call it batch effect?. A more specific case is, if I have time-course data1 which has not geneX, however, I time-course data2 will have geneX till days later. In mnn correction, a prerequisite is same genes, will it filter out some genes meaningful?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-471439971
https://github.com/scverse/scanpy/issues/265#issuecomment-471439971:163,Integrability,protocol,protocols,163,"Just curious about the case of 'batch effect', it looks like to me library construction protocol/chemicals is main source of batch effects. However, if I use same protocols, sequencing platform and slight difference of sequencing depth for some sample, but in different time course, would you call it batch effect?. A more specific case is, if I have time-course data1 which has not geneX, however, I time-course data2 will have geneX till days later. In mnn correction, a prerequisite is same genes, will it filter out some genes meaningful?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-471439971
https://github.com/scverse/scanpy/issues/265#issuecomment-471808354:407,Deployability,integrat,integrated,407,"In different time course, the batch effect and true biological variation will be entangled. . Batch effects, which occur because measurements are affected by laboratory conditions,reagent lots and personnel differences. This becomes a major problem when batch effects are correlated with an outcome of interest and lead to incorrect conclusions. However, in single cell RNAseq, different datasets should be integrated with suitable algorithm (such as mnn, CCA, bbknn, harmony, scvi et al.), even no batch effect exists.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-471808354
https://github.com/scverse/scanpy/issues/265#issuecomment-471808354:407,Integrability,integrat,integrated,407,"In different time course, the batch effect and true biological variation will be entangled. . Batch effects, which occur because measurements are affected by laboratory conditions,reagent lots and personnel differences. This becomes a major problem when batch effects are correlated with an outcome of interest and lead to incorrect conclusions. However, in single cell RNAseq, different datasets should be integrated with suitable algorithm (such as mnn, CCA, bbknn, harmony, scvi et al.), even no batch effect exists.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-471808354
https://github.com/scverse/scanpy/issues/265#issuecomment-509063881:68,Deployability,integrat,integrate,68,"> CCA does not have code in python, which will make it difficult to integrate, pySCENIC is probably easier but I would rather ask the developers.; > ; > @falexwolf We should consider a way to facilitate scanpy 'plugins'. A quick search shows me that this could be possible: https://packaging.python.org/guides/creating-and-discovering-plugins/ but honestly I don't know how it works. Nevertheless, given the number of tools that continue to appear we should consider a scheme that facilitate how developers can take advantage of scanpy preprocessing, storing, analysis and visualization tools. Pyscenic has been integrated into scanpy now! Here is the hyper link:; https://github.com/aertslab/pySCENIC/blob/master/notebooks/pySCENIC%20-%20Integration%20with%20scanpy.ipynb",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-509063881
https://github.com/scverse/scanpy/issues/265#issuecomment-509063881:612,Deployability,integrat,integrated,612,"> CCA does not have code in python, which will make it difficult to integrate, pySCENIC is probably easier but I would rather ask the developers.; > ; > @falexwolf We should consider a way to facilitate scanpy 'plugins'. A quick search shows me that this could be possible: https://packaging.python.org/guides/creating-and-discovering-plugins/ but honestly I don't know how it works. Nevertheless, given the number of tools that continue to appear we should consider a scheme that facilitate how developers can take advantage of scanpy preprocessing, storing, analysis and visualization tools. Pyscenic has been integrated into scanpy now! Here is the hyper link:; https://github.com/aertslab/pySCENIC/blob/master/notebooks/pySCENIC%20-%20Integration%20with%20scanpy.ipynb",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-509063881
https://github.com/scverse/scanpy/issues/265#issuecomment-509063881:68,Integrability,integrat,integrate,68,"> CCA does not have code in python, which will make it difficult to integrate, pySCENIC is probably easier but I would rather ask the developers.; > ; > @falexwolf We should consider a way to facilitate scanpy 'plugins'. A quick search shows me that this could be possible: https://packaging.python.org/guides/creating-and-discovering-plugins/ but honestly I don't know how it works. Nevertheless, given the number of tools that continue to appear we should consider a scheme that facilitate how developers can take advantage of scanpy preprocessing, storing, analysis and visualization tools. Pyscenic has been integrated into scanpy now! Here is the hyper link:; https://github.com/aertslab/pySCENIC/blob/master/notebooks/pySCENIC%20-%20Integration%20with%20scanpy.ipynb",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-509063881
https://github.com/scverse/scanpy/issues/265#issuecomment-509063881:612,Integrability,integrat,integrated,612,"> CCA does not have code in python, which will make it difficult to integrate, pySCENIC is probably easier but I would rather ask the developers.; > ; > @falexwolf We should consider a way to facilitate scanpy 'plugins'. A quick search shows me that this could be possible: https://packaging.python.org/guides/creating-and-discovering-plugins/ but honestly I don't know how it works. Nevertheless, given the number of tools that continue to appear we should consider a scheme that facilitate how developers can take advantage of scanpy preprocessing, storing, analysis and visualization tools. Pyscenic has been integrated into scanpy now! Here is the hyper link:; https://github.com/aertslab/pySCENIC/blob/master/notebooks/pySCENIC%20-%20Integration%20with%20scanpy.ipynb",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-509063881
https://github.com/scverse/scanpy/issues/265#issuecomment-509063881:211,Modifiability,plugin,plugins,211,"> CCA does not have code in python, which will make it difficult to integrate, pySCENIC is probably easier but I would rather ask the developers.; > ; > @falexwolf We should consider a way to facilitate scanpy 'plugins'. A quick search shows me that this could be possible: https://packaging.python.org/guides/creating-and-discovering-plugins/ but honestly I don't know how it works. Nevertheless, given the number of tools that continue to appear we should consider a scheme that facilitate how developers can take advantage of scanpy preprocessing, storing, analysis and visualization tools. Pyscenic has been integrated into scanpy now! Here is the hyper link:; https://github.com/aertslab/pySCENIC/blob/master/notebooks/pySCENIC%20-%20Integration%20with%20scanpy.ipynb",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-509063881
https://github.com/scverse/scanpy/issues/265#issuecomment-509063881:335,Modifiability,plugin,plugins,335,"> CCA does not have code in python, which will make it difficult to integrate, pySCENIC is probably easier but I would rather ask the developers.; > ; > @falexwolf We should consider a way to facilitate scanpy 'plugins'. A quick search shows me that this could be possible: https://packaging.python.org/guides/creating-and-discovering-plugins/ but honestly I don't know how it works. Nevertheless, given the number of tools that continue to appear we should consider a scheme that facilitate how developers can take advantage of scanpy preprocessing, storing, analysis and visualization tools. Pyscenic has been integrated into scanpy now! Here is the hyper link:; https://github.com/aertslab/pySCENIC/blob/master/notebooks/pySCENIC%20-%20Integration%20with%20scanpy.ipynb",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-509063881
https://github.com/scverse/scanpy/issues/265#issuecomment-509063881:303,Usability,guid,guides,303,"> CCA does not have code in python, which will make it difficult to integrate, pySCENIC is probably easier but I would rather ask the developers.; > ; > @falexwolf We should consider a way to facilitate scanpy 'plugins'. A quick search shows me that this could be possible: https://packaging.python.org/guides/creating-and-discovering-plugins/ but honestly I don't know how it works. Nevertheless, given the number of tools that continue to appear we should consider a scheme that facilitate how developers can take advantage of scanpy preprocessing, storing, analysis and visualization tools. Pyscenic has been integrated into scanpy now! Here is the hyper link:; https://github.com/aertslab/pySCENIC/blob/master/notebooks/pySCENIC%20-%20Integration%20with%20scanpy.ipynb",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-509063881
https://github.com/scverse/scanpy/issues/267#issuecomment-424547172:44,Usability,simpl,simply,44,"I don't know how they do it Seurat, but I'd simply do; ```; filenames = ['name0.h5', 'name1.h5', 'name2.h5']; adatas = [sc.read_10x_h5(filename) for filename in filenames]; adata = adatas[0].concatenate(adatas[1:]); ```; Does this help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267#issuecomment-424547172
https://github.com/scverse/scanpy/issues/267#issuecomment-424700191:72,Deployability,release,release,72,"You can do the same as above using `sc.read_10x_mtx`, which is not in a release yet but on GitHub's Master branch. In `.concatenate()` you have the option to pass how you want to name your batches/samples by passing `batch_categories`. PS: Note that I edited the example above to show `sc.read_10x_h5`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267#issuecomment-424700191
https://github.com/scverse/scanpy/issues/267#issuecomment-486916420:170,Performance,cache,cache,170,"Hi falexwolf,. I try to use concatenate to read multiple 10X mtx and put them together.; But it seems like if I concatenate more than 15 mtx(already stored and read from cache), it becomes very slow. Do you have any advice?; Thanks for any information you may provide.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267#issuecomment-486916420
https://github.com/scverse/scanpy/issues/267#issuecomment-602964900:196,Availability,error,error,196,"Hi @falexwolf, thanks for the solution you provided above for reading multiple files. I tried it and it worked when I had just 2 files. I am trying the same code with 23 files and I am getting an error message in the concatenation step. Any idea on how to fix this ? Thanks. ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-20-662b857d9182> in <module>; 12 adatas.obs['cell_names'] = pd.read_csv(path + sample + 'barcodes.tsv.gz', header=None)[0].values; 13 ; ---> 14 adata = adatas[0].concatenate(adatas[1:]). /Applications/anaconda3/lib/python3.7/site-packages/anndata/core/anndata.py in concatenate(self, join, batch_key, batch_categories, index_unique, *adatas); 1908 ; 1909 if any_sparse:; -> 1910 sparse_format = all_adatas[0].X.getformat(); 1911 X = X.asformat(sparse_format); 1912 . AttributeError: 'numpy.ndarray' object has no attribute 'getformat'",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267#issuecomment-602964900
https://github.com/scverse/scanpy/issues/267#issuecomment-602964900:202,Integrability,message,message,202,"Hi @falexwolf, thanks for the solution you provided above for reading multiple files. I tried it and it worked when I had just 2 files. I am trying the same code with 23 files and I am getting an error message in the concatenation step. Any idea on how to fix this ? Thanks. ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-20-662b857d9182> in <module>; 12 adatas.obs['cell_names'] = pd.read_csv(path + sample + 'barcodes.tsv.gz', header=None)[0].values; 13 ; ---> 14 adata = adatas[0].concatenate(adatas[1:]). /Applications/anaconda3/lib/python3.7/site-packages/anndata/core/anndata.py in concatenate(self, join, batch_key, batch_categories, index_unique, *adatas); 1908 ; 1909 if any_sparse:; -> 1910 sparse_format = all_adatas[0].X.getformat(); 1911 X = X.asformat(sparse_format); 1912 . AttributeError: 'numpy.ndarray' object has no attribute 'getformat'",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267#issuecomment-602964900
https://github.com/scverse/scanpy/issues/267#issuecomment-1018908683:442,Availability,error,error,442,"Hello,. I am having problems with reading in multiple h5 files using the code snipped that was posted by falexwolf. I am doing:; ```; filenames = ['./a.h5', './b.h5', './c.h5', './d.h5']; adatas = [sc.read_10x_h5(filename, gex_only = True) for filename in filenames]; adata = adatas[0].concatenate(adatas[1:], batch_key='gene_ids', batch_categories=filenames); ```. With or without the batch_key and batch_categories arguments I get the same error:; ```; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-20-e23ba2ca6e37> in <module>; 1 filenames = ['./a.h5', './b.h5', './c.h5', './d.h5']; 2 adatas = [sc.read_10x_h5(filename, gex_only = True) for filename in filenames]; ----> 3 adata = adatas[0].concatenate(adatas[1:], batch_key='gene_ids'). ~/anaconda3/lib/python3.7/site-packages/anndata/_core/anndata.py in concatenate(self, join, batch_key, batch_categories, uns_merge, index_unique, fill_value, *adatas); 1764 fill_value=fill_value,; 1765 index_unique=index_unique,; -> 1766 pairwise=False,; 1767 ); 1768 . ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in concat(adatas, axis, join, merge, uns_merge, label, keys, index_unique, fill_value, pairwise); 817 # Annotation for other axis; 818 alt_annot = merge_dataframes(; --> 819 [getattr(a, alt_dim) for a in adatas], alt_indices, merge; 820 ); 821 . ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in merge_dataframes(dfs, new_index, merge_strategy); 529 dfs: Iterable[pd.DataFrame], new_index, merge_strategy=merge_unique; 530 ) -> pd.DataFrame:; --> 531 dfs = [df.reindex(index=new_index) for df in dfs]; 532 # New dataframe with all shared data; 533 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in <listcomp>(.0); 529 dfs: Iterable[pd.DataFrame], new_index, merge_strategy=merge_unique; 530 ) -> pd.DataFrame:; --> 531 dfs = [df.reindex(inde",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267#issuecomment-1018908683
https://github.com/scverse/scanpy/issues/267#issuecomment-1018908683:2828,Availability,toler,tolerance,2828,"ges/anndata/_core/merge.py in <listcomp>(.0); 529 dfs: Iterable[pd.DataFrame], new_index, merge_strategy=merge_unique; 530 ) -> pd.DataFrame:; --> 531 dfs = [df.reindex(index=new_index) for df in dfs]; 532 # New dataframe with all shared data; 533 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs); 310 @wraps(func); 311 def wrapper(*args, **kwargs) -> Callable[..., Any]:; --> 312 return func(*args, **kwargs); 313 ; 314 kind = inspect.Parameter.POSITIONAL_OR_KEYWORD. ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in reindex(self, *args, **kwargs); 4174 kwargs.pop(""axis"", None); 4175 kwargs.pop(""labels"", None); -> 4176 return super().reindex(**kwargs); 4177 ; 4178 def drop(. ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs); 4810 # perform the reindex on the axes; 4811 return self._reindex_axes(; -> 4812 axes, level, limit, tolerance, method, fill_value, copy; 4813 ).__finalize__(self, method=""reindex""); 4814 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy); 4021 if index is not None:; 4022 frame = frame._reindex_index(; -> 4023 index, method, copy, level, fill_value, limit, tolerance; 4024 ); 4025 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_index(self, new_index, method, copy, level, fill_value, limit, tolerance); 4043 copy=copy,; 4044 fill_value=fill_value,; -> 4045 allow_dups=False,; 4046 ); 4047 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in _reindex_with_indexers(self, reindexers, fill_value, copy, allow_dups); 4881 fill_value=fill_value,; 4882 allow_dups=allow_dups,; -> 4883 copy=copy,; 4884 ); 4885 # If we've made a copy once, no need to make another one. ~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py in reindex_indexer(self, ne",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267#issuecomment-1018908683
https://github.com/scverse/scanpy/issues/267#issuecomment-1018908683:3021,Availability,toler,tolerance,3021,"dataframe with all shared data; 533 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs); 310 @wraps(func); 311 def wrapper(*args, **kwargs) -> Callable[..., Any]:; --> 312 return func(*args, **kwargs); 313 ; 314 kind = inspect.Parameter.POSITIONAL_OR_KEYWORD. ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in reindex(self, *args, **kwargs); 4174 kwargs.pop(""axis"", None); 4175 kwargs.pop(""labels"", None); -> 4176 return super().reindex(**kwargs); 4177 ; 4178 def drop(. ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs); 4810 # perform the reindex on the axes; 4811 return self._reindex_axes(; -> 4812 axes, level, limit, tolerance, method, fill_value, copy; 4813 ).__finalize__(self, method=""reindex""); 4814 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy); 4021 if index is not None:; 4022 frame = frame._reindex_index(; -> 4023 index, method, copy, level, fill_value, limit, tolerance; 4024 ); 4025 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_index(self, new_index, method, copy, level, fill_value, limit, tolerance); 4043 copy=copy,; 4044 fill_value=fill_value,; -> 4045 allow_dups=False,; 4046 ); 4047 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in _reindex_with_indexers(self, reindexers, fill_value, copy, allow_dups); 4881 fill_value=fill_value,; 4882 allow_dups=allow_dups,; -> 4883 copy=copy,; 4884 ); 4885 # If we've made a copy once, no need to make another one. ~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py in reindex_indexer(self, new_axis, indexer, axis, fill_value, allow_dups, copy, consolidate, only_slice); 1299 # some axes don't allow reindexing with dups; 1300 if not allow_dups:; -> 1301 self.axes[axis]._can_reindex(indexer); 1302 ; 1303",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267#issuecomment-1018908683
https://github.com/scverse/scanpy/issues/267#issuecomment-1018908683:3178,Availability,toler,tolerance,3178,"ython3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs); 310 @wraps(func); 311 def wrapper(*args, **kwargs) -> Callable[..., Any]:; --> 312 return func(*args, **kwargs); 313 ; 314 kind = inspect.Parameter.POSITIONAL_OR_KEYWORD. ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in reindex(self, *args, **kwargs); 4174 kwargs.pop(""axis"", None); 4175 kwargs.pop(""labels"", None); -> 4176 return super().reindex(**kwargs); 4177 ; 4178 def drop(. ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs); 4810 # perform the reindex on the axes; 4811 return self._reindex_axes(; -> 4812 axes, level, limit, tolerance, method, fill_value, copy; 4813 ).__finalize__(self, method=""reindex""); 4814 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy); 4021 if index is not None:; 4022 frame = frame._reindex_index(; -> 4023 index, method, copy, level, fill_value, limit, tolerance; 4024 ); 4025 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_index(self, new_index, method, copy, level, fill_value, limit, tolerance); 4043 copy=copy,; 4044 fill_value=fill_value,; -> 4045 allow_dups=False,; 4046 ); 4047 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in _reindex_with_indexers(self, reindexers, fill_value, copy, allow_dups); 4881 fill_value=fill_value,; 4882 allow_dups=allow_dups,; -> 4883 copy=copy,; 4884 ); 4885 # If we've made a copy once, no need to make another one. ~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py in reindex_indexer(self, new_axis, indexer, axis, fill_value, allow_dups, copy, consolidate, only_slice); 1299 # some axes don't allow reindexing with dups; 1300 if not allow_dups:; -> 1301 self.axes[axis]._can_reindex(indexer); 1302 ; 1303 if axis >= self.ndim:. ~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py in _can_reindex(self, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267#issuecomment-1018908683
https://github.com/scverse/scanpy/issues/267#issuecomment-1018908683:3340,Availability,toler,tolerance,3340,"; 314 kind = inspect.Parameter.POSITIONAL_OR_KEYWORD. ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in reindex(self, *args, **kwargs); 4174 kwargs.pop(""axis"", None); 4175 kwargs.pop(""labels"", None); -> 4176 return super().reindex(**kwargs); 4177 ; 4178 def drop(. ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs); 4810 # perform the reindex on the axes; 4811 return self._reindex_axes(; -> 4812 axes, level, limit, tolerance, method, fill_value, copy; 4813 ).__finalize__(self, method=""reindex""); 4814 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy); 4021 if index is not None:; 4022 frame = frame._reindex_index(; -> 4023 index, method, copy, level, fill_value, limit, tolerance; 4024 ); 4025 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_index(self, new_index, method, copy, level, fill_value, limit, tolerance); 4043 copy=copy,; 4044 fill_value=fill_value,; -> 4045 allow_dups=False,; 4046 ); 4047 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in _reindex_with_indexers(self, reindexers, fill_value, copy, allow_dups); 4881 fill_value=fill_value,; 4882 allow_dups=allow_dups,; -> 4883 copy=copy,; 4884 ); 4885 # If we've made a copy once, no need to make another one. ~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py in reindex_indexer(self, new_axis, indexer, axis, fill_value, allow_dups, copy, consolidate, only_slice); 1299 # some axes don't allow reindexing with dups; 1300 if not allow_dups:; -> 1301 self.axes[axis]._can_reindex(indexer); 1302 ; 1303 if axis >= self.ndim:. ~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py in _can_reindex(self, indexer); 3475 # trying to reindex on an axis with duplicates; 3476 if not self._index_as_unique and len(indexer):; -> 3477 raise ValueError(""cannot reindex from a duplicate axis""); 3478 ; 3479 de",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267#issuecomment-1018908683
https://github.com/scverse/scanpy/issues/267#issuecomment-1018908683:4412,Availability,toler,tolerance,4412,", tolerance, method, fill_value, copy; 4813 ).__finalize__(self, method=""reindex""); 4814 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy); 4021 if index is not None:; 4022 frame = frame._reindex_index(; -> 4023 index, method, copy, level, fill_value, limit, tolerance; 4024 ); 4025 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_index(self, new_index, method, copy, level, fill_value, limit, tolerance); 4043 copy=copy,; 4044 fill_value=fill_value,; -> 4045 allow_dups=False,; 4046 ); 4047 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in _reindex_with_indexers(self, reindexers, fill_value, copy, allow_dups); 4881 fill_value=fill_value,; 4882 allow_dups=allow_dups,; -> 4883 copy=copy,; 4884 ); 4885 # If we've made a copy once, no need to make another one. ~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py in reindex_indexer(self, new_axis, indexer, axis, fill_value, allow_dups, copy, consolidate, only_slice); 1299 # some axes don't allow reindexing with dups; 1300 if not allow_dups:; -> 1301 self.axes[axis]._can_reindex(indexer); 1302 ; 1303 if axis >= self.ndim:. ~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py in _can_reindex(self, indexer); 3475 # trying to reindex on an axis with duplicates; 3476 if not self._index_as_unique and len(indexer):; -> 3477 raise ValueError(""cannot reindex from a duplicate axis""); 3478 ; 3479 def reindex(self, target, method=None, level=None, limit=None, tolerance=None):. ValueError: cannot reindex from a duplicate axis; ```; Loading a single h5 file works and produces expected output:; ```; a = sc.read_10x_h5('./a.h5', gex_only = True); a; AnnData object with n_obs × n_vars = 7474 × 31053; var: 'gene_ids', 'feature_types', 'genome'; ```. So the input files appear to be valid I just can't get them to concatenate to a single object. . Any ideas would be welcome.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267#issuecomment-1018908683
https://github.com/scverse/scanpy/issues/267#issuecomment-1018908683:2207,Integrability,wrap,wrapper,2207,"lue, pairwise); 817 # Annotation for other axis; 818 alt_annot = merge_dataframes(; --> 819 [getattr(a, alt_dim) for a in adatas], alt_indices, merge; 820 ); 821 . ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in merge_dataframes(dfs, new_index, merge_strategy); 529 dfs: Iterable[pd.DataFrame], new_index, merge_strategy=merge_unique; 530 ) -> pd.DataFrame:; --> 531 dfs = [df.reindex(index=new_index) for df in dfs]; 532 # New dataframe with all shared data; 533 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in <listcomp>(.0); 529 dfs: Iterable[pd.DataFrame], new_index, merge_strategy=merge_unique; 530 ) -> pd.DataFrame:; --> 531 dfs = [df.reindex(index=new_index) for df in dfs]; 532 # New dataframe with all shared data; 533 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs); 310 @wraps(func); 311 def wrapper(*args, **kwargs) -> Callable[..., Any]:; --> 312 return func(*args, **kwargs); 313 ; 314 kind = inspect.Parameter.POSITIONAL_OR_KEYWORD. ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in reindex(self, *args, **kwargs); 4174 kwargs.pop(""axis"", None); 4175 kwargs.pop(""labels"", None); -> 4176 return super().reindex(**kwargs); 4177 ; 4178 def drop(. ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs); 4810 # perform the reindex on the axes; 4811 return self._reindex_axes(; -> 4812 axes, level, limit, tolerance, method, fill_value, copy; 4813 ).__finalize__(self, method=""reindex""); 4814 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy); 4021 if index is not None:; 4022 frame = frame._reindex_index(; -> 4023 index, method, copy, level, fill_value, limit, tolerance; 4024 ); 4025 . ~/anaconda3/lib/python3.7/site-packages/panda",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267#issuecomment-1018908683
https://github.com/scverse/scanpy/issues/267#issuecomment-1018908683:2238,Integrability,wrap,wraps,2238,"lue, pairwise); 817 # Annotation for other axis; 818 alt_annot = merge_dataframes(; --> 819 [getattr(a, alt_dim) for a in adatas], alt_indices, merge; 820 ); 821 . ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in merge_dataframes(dfs, new_index, merge_strategy); 529 dfs: Iterable[pd.DataFrame], new_index, merge_strategy=merge_unique; 530 ) -> pd.DataFrame:; --> 531 dfs = [df.reindex(index=new_index) for df in dfs]; 532 # New dataframe with all shared data; 533 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in <listcomp>(.0); 529 dfs: Iterable[pd.DataFrame], new_index, merge_strategy=merge_unique; 530 ) -> pd.DataFrame:; --> 531 dfs = [df.reindex(index=new_index) for df in dfs]; 532 # New dataframe with all shared data; 533 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs); 310 @wraps(func); 311 def wrapper(*args, **kwargs) -> Callable[..., Any]:; --> 312 return func(*args, **kwargs); 313 ; 314 kind = inspect.Parameter.POSITIONAL_OR_KEYWORD. ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in reindex(self, *args, **kwargs); 4174 kwargs.pop(""axis"", None); 4175 kwargs.pop(""labels"", None); -> 4176 return super().reindex(**kwargs); 4177 ; 4178 def drop(. ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs); 4810 # perform the reindex on the axes; 4811 return self._reindex_axes(; -> 4812 axes, level, limit, tolerance, method, fill_value, copy; 4813 ).__finalize__(self, method=""reindex""); 4814 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy); 4021 if index is not None:; 4022 frame = frame._reindex_index(; -> 4023 index, method, copy, level, fill_value, limit, tolerance; 4024 ); 4025 . ~/anaconda3/lib/python3.7/site-packages/panda",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267#issuecomment-1018908683
https://github.com/scverse/scanpy/issues/267#issuecomment-1018908683:2259,Integrability,wrap,wrapper,2259,"lue, pairwise); 817 # Annotation for other axis; 818 alt_annot = merge_dataframes(; --> 819 [getattr(a, alt_dim) for a in adatas], alt_indices, merge; 820 ); 821 . ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in merge_dataframes(dfs, new_index, merge_strategy); 529 dfs: Iterable[pd.DataFrame], new_index, merge_strategy=merge_unique; 530 ) -> pd.DataFrame:; --> 531 dfs = [df.reindex(index=new_index) for df in dfs]; 532 # New dataframe with all shared data; 533 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in <listcomp>(.0); 529 dfs: Iterable[pd.DataFrame], new_index, merge_strategy=merge_unique; 530 ) -> pd.DataFrame:; --> 531 dfs = [df.reindex(index=new_index) for df in dfs]; 532 # New dataframe with all shared data; 533 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs); 310 @wraps(func); 311 def wrapper(*args, **kwargs) -> Callable[..., Any]:; --> 312 return func(*args, **kwargs); 313 ; 314 kind = inspect.Parameter.POSITIONAL_OR_KEYWORD. ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in reindex(self, *args, **kwargs); 4174 kwargs.pop(""axis"", None); 4175 kwargs.pop(""labels"", None); -> 4176 return super().reindex(**kwargs); 4177 ; 4178 def drop(. ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs); 4810 # perform the reindex on the axes; 4811 return self._reindex_axes(; -> 4812 axes, level, limit, tolerance, method, fill_value, copy; 4813 ).__finalize__(self, method=""reindex""); 4814 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy); 4021 if index is not None:; 4022 frame = frame._reindex_index(; -> 4023 index, method, copy, level, fill_value, limit, tolerance; 4024 ); 4025 . ~/anaconda3/lib/python3.7/site-packages/panda",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267#issuecomment-1018908683
https://github.com/scverse/scanpy/issues/267#issuecomment-1018908683:2734,Performance,perform,perform,2734,"f = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in <listcomp>(.0); 529 dfs: Iterable[pd.DataFrame], new_index, merge_strategy=merge_unique; 530 ) -> pd.DataFrame:; --> 531 dfs = [df.reindex(index=new_index) for df in dfs]; 532 # New dataframe with all shared data; 533 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs); 310 @wraps(func); 311 def wrapper(*args, **kwargs) -> Callable[..., Any]:; --> 312 return func(*args, **kwargs); 313 ; 314 kind = inspect.Parameter.POSITIONAL_OR_KEYWORD. ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in reindex(self, *args, **kwargs); 4174 kwargs.pop(""axis"", None); 4175 kwargs.pop(""labels"", None); -> 4176 return super().reindex(**kwargs); 4177 ; 4178 def drop(. ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs); 4810 # perform the reindex on the axes; 4811 return self._reindex_axes(; -> 4812 axes, level, limit, tolerance, method, fill_value, copy; 4813 ).__finalize__(self, method=""reindex""); 4814 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy); 4021 if index is not None:; 4022 frame = frame._reindex_index(; -> 4023 index, method, copy, level, fill_value, limit, tolerance; 4024 ); 4025 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_index(self, new_index, method, copy, level, fill_value, limit, tolerance); 4043 copy=copy,; 4044 fill_value=fill_value,; -> 4045 allow_dups=False,; 4046 ); 4047 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in _reindex_with_indexers(self, reindexers, fill_value, copy, allow_dups); 4881 fill_value=fill_value,; 4882 allow_dups=allow_dups,; -> 4883 copy=copy,; 4884 ); 4885 # If we've made a copy once, no need to make another one. ~/anaconda",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267#issuecomment-1018908683
https://github.com/scverse/scanpy/issues/267#issuecomment-1018908683:4485,Performance,Load,Loading,4485,", tolerance, method, fill_value, copy; 4813 ).__finalize__(self, method=""reindex""); 4814 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy); 4021 if index is not None:; 4022 frame = frame._reindex_index(; -> 4023 index, method, copy, level, fill_value, limit, tolerance; 4024 ); 4025 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_index(self, new_index, method, copy, level, fill_value, limit, tolerance); 4043 copy=copy,; 4044 fill_value=fill_value,; -> 4045 allow_dups=False,; 4046 ); 4047 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in _reindex_with_indexers(self, reindexers, fill_value, copy, allow_dups); 4881 fill_value=fill_value,; 4882 allow_dups=allow_dups,; -> 4883 copy=copy,; 4884 ); 4885 # If we've made a copy once, no need to make another one. ~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py in reindex_indexer(self, new_axis, indexer, axis, fill_value, allow_dups, copy, consolidate, only_slice); 1299 # some axes don't allow reindexing with dups; 1300 if not allow_dups:; -> 1301 self.axes[axis]._can_reindex(indexer); 1302 ; 1303 if axis >= self.ndim:. ~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py in _can_reindex(self, indexer); 3475 # trying to reindex on an axis with duplicates; 3476 if not self._index_as_unique and len(indexer):; -> 3477 raise ValueError(""cannot reindex from a duplicate axis""); 3478 ; 3479 def reindex(self, target, method=None, level=None, limit=None, tolerance=None):. ValueError: cannot reindex from a duplicate axis; ```; Loading a single h5 file works and produces expected output:; ```; a = sc.read_10x_h5('./a.h5', gex_only = True); a; AnnData object with n_obs × n_vars = 7474 × 31053; var: 'gene_ids', 'feature_types', 'genome'; ```. So the input files appear to be valid I just can't get them to concatenate to a single object. . Any ideas would be welcome.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267#issuecomment-1018908683
https://github.com/scverse/scanpy/issues/267#issuecomment-1431472348:57,Availability,error,error,57,"HI, I tried to do what you suggested but I am getting an error saying `ValueError: only one regex group is supported with Index`.; I have multiple h5ad files with varying n_obs × n_vars. Here is my code:; ```adatas = [an.read_h5ad(filename) for filename in filenames]; batch_names = []; for i in range(len(adatas)):; adatas[i].var_names_make_unique(); batch_names.append(filenames[i].split('.')[0]); print(i,adatas[i]). adata = adatas[0].concatenate(adatas[1:],; batch_key = 'ID',; uns_merge=""unique"",; index_unique=None,; batch_categories=batch_names); ```. and this produces the above error. Can anyone help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267#issuecomment-1431472348
https://github.com/scverse/scanpy/issues/267#issuecomment-1431472348:587,Availability,error,error,587,"HI, I tried to do what you suggested but I am getting an error saying `ValueError: only one regex group is supported with Index`.; I have multiple h5ad files with varying n_obs × n_vars. Here is my code:; ```adatas = [an.read_h5ad(filename) for filename in filenames]; batch_names = []; for i in range(len(adatas)):; adatas[i].var_names_make_unique(); batch_names.append(filenames[i].split('.')[0]); print(i,adatas[i]). adata = adatas[0].concatenate(adatas[1:],; batch_key = 'ID',; uns_merge=""unique"",; index_unique=None,; batch_categories=batch_names); ```. and this produces the above error. Can anyone help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267#issuecomment-1431472348
https://github.com/scverse/scanpy/issues/269#issuecomment-426990596:57,Testability,test,tests,57,"Both of these files would need some major clean up, some tests and some documentation, also in notebooks. I'm also happy to merge a pull request adding the new functionality, but I can't do this myself right now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/269#issuecomment-426990596
https://github.com/scverse/scanpy/pull/270#issuecomment-425353186:249,Testability,log,log,249,"Is it generally a good idea to output P-values for marker gene identification? As the null model is not valid for this setup, P-values are not really a measure of significance, but should only serve to order genes. I can see the point of outputting log fold changes, but outputting P-values may be misleading.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270#issuecomment-425353186
https://github.com/scverse/scanpy/pull/270#issuecomment-425357700:36,Testability,Test,Testing,36,"Why is the null hypothesis invalid? Testing whether two cell types have the same mean for the expression of a given gene seems valid, no? One-vs-rest might be problematic, though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270#issuecomment-425357700
https://github.com/scverse/scanpy/pull/270#issuecomment-425375105:482,Testability,test,test,482,"Hmm, I think it's more complicated than that. First, cell type labels may not come from clustering. Even if it's the case, the design of Louvain does not rely on univariate mean difference between groups due to the distance metrics used in kNN graph, which take all genes into account at once. . Besides, I believe that how cell types are defined e.g. by biologist's manually annotation of each cell, Louvain, bulk comparison etc. doesn't really make the null hypothesis invalid. t-test just tests if the means are the same or not. Regarding false positives with random clustering, that's why we have Bonferroni, no? But this is now a different story about p-values and multiple testing correction in general. So it's not our duty to solve it in rank_genes_group function. I'd rather prefer reporting a p-value (or maybe t-stat) and letting the user decide how valid/useful it is for his/her research instead of not outputting it with the consideration of the chance that it's not valid.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270#issuecomment-425375105
https://github.com/scverse/scanpy/pull/270#issuecomment-425375105:492,Testability,test,tests,492,"Hmm, I think it's more complicated than that. First, cell type labels may not come from clustering. Even if it's the case, the design of Louvain does not rely on univariate mean difference between groups due to the distance metrics used in kNN graph, which take all genes into account at once. . Besides, I believe that how cell types are defined e.g. by biologist's manually annotation of each cell, Louvain, bulk comparison etc. doesn't really make the null hypothesis invalid. t-test just tests if the means are the same or not. Regarding false positives with random clustering, that's why we have Bonferroni, no? But this is now a different story about p-values and multiple testing correction in general. So it's not our duty to solve it in rank_genes_group function. I'd rather prefer reporting a p-value (or maybe t-stat) and letting the user decide how valid/useful it is for his/her research instead of not outputting it with the consideration of the chance that it's not valid.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270#issuecomment-425375105
https://github.com/scverse/scanpy/pull/270#issuecomment-425375105:679,Testability,test,testing,679,"Hmm, I think it's more complicated than that. First, cell type labels may not come from clustering. Even if it's the case, the design of Louvain does not rely on univariate mean difference between groups due to the distance metrics used in kNN graph, which take all genes into account at once. . Besides, I believe that how cell types are defined e.g. by biologist's manually annotation of each cell, Louvain, bulk comparison etc. doesn't really make the null hypothesis invalid. t-test just tests if the means are the same or not. Regarding false positives with random clustering, that's why we have Bonferroni, no? But this is now a different story about p-values and multiple testing correction in general. So it's not our duty to solve it in rank_genes_group function. I'd rather prefer reporting a p-value (or maybe t-stat) and letting the user decide how valid/useful it is for his/her research instead of not outputting it with the consideration of the chance that it's not valid.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270#issuecomment-425375105
https://github.com/scverse/scanpy/pull/270#issuecomment-425395345:145,Testability,test,test,145,"So you should easily be able to see that the null hypothesis is not valid by the distribution of p-values for all genes in one rank_genes_groups test. If it is valid, this distribution should be uniform. In that case it's only a multiple testing problem... I would guarantee you that it's not uniform though. Cell type labels from expression-independent sources should not have the same confounding effect. However, I would bet you can trace back all biological annotations of cell types back to expression.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270#issuecomment-425395345
https://github.com/scverse/scanpy/pull/270#issuecomment-425395345:238,Testability,test,testing,238,"So you should easily be able to see that the null hypothesis is not valid by the distribution of p-values for all genes in one rank_genes_groups test. If it is valid, this distribution should be uniform. In that case it's only a multiple testing problem... I would guarantee you that it's not uniform though. Cell type labels from expression-independent sources should not have the same confounding effect. However, I would bet you can trace back all biological annotations of cell types back to expression.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270#issuecomment-425395345
https://github.com/scverse/scanpy/pull/270#issuecomment-425729339:100,Integrability,depend,depends,100,"Thanks for your comments! This is an interesting discussion. I agree that the validity of a p-value depends on the user's data. From the standpoint of an analysis tool, I think it's better to report a p-value (and a t-statistic, which was already reported in the original function) and let the user decide what to do with that information.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270#issuecomment-425729339
https://github.com/scverse/scanpy/pull/270#issuecomment-425737634:63,Testability,test,testing-correction,63,"I just noticed I forgot to answer to your point about multiple-testing-correction. Multiple testing correction deals with the effect that obtaining any false positives become more likely with repeated tests. However, it does not deal with bias. Bias is however present in the test given that the correct null hypothesis should assume some genes are different between groups, and not that all genes are equal. This comes back to the observation that some genes will be always be different between groups in random data, given that clustering defines the groups over which we test. And as for louvain using whole transcriptome similarities rather than individual gene similarities... these are related. If you defined groups by differences of whole transcriptomes, this necesitates individual genes having different means. The correct p-value would be the output of a permutation test. . Could discuss this offline if you like @gokceneraslan. I'm preparing a manuscript which makes a point of this... if I am wrong, please convince me of this soon ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270#issuecomment-425737634
https://github.com/scverse/scanpy/pull/270#issuecomment-425737634:92,Testability,test,testing,92,"I just noticed I forgot to answer to your point about multiple-testing-correction. Multiple testing correction deals with the effect that obtaining any false positives become more likely with repeated tests. However, it does not deal with bias. Bias is however present in the test given that the correct null hypothesis should assume some genes are different between groups, and not that all genes are equal. This comes back to the observation that some genes will be always be different between groups in random data, given that clustering defines the groups over which we test. And as for louvain using whole transcriptome similarities rather than individual gene similarities... these are related. If you defined groups by differences of whole transcriptomes, this necesitates individual genes having different means. The correct p-value would be the output of a permutation test. . Could discuss this offline if you like @gokceneraslan. I'm preparing a manuscript which makes a point of this... if I am wrong, please convince me of this soon ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270#issuecomment-425737634
https://github.com/scverse/scanpy/pull/270#issuecomment-425737634:201,Testability,test,tests,201,"I just noticed I forgot to answer to your point about multiple-testing-correction. Multiple testing correction deals with the effect that obtaining any false positives become more likely with repeated tests. However, it does not deal with bias. Bias is however present in the test given that the correct null hypothesis should assume some genes are different between groups, and not that all genes are equal. This comes back to the observation that some genes will be always be different between groups in random data, given that clustering defines the groups over which we test. And as for louvain using whole transcriptome similarities rather than individual gene similarities... these are related. If you defined groups by differences of whole transcriptomes, this necesitates individual genes having different means. The correct p-value would be the output of a permutation test. . Could discuss this offline if you like @gokceneraslan. I'm preparing a manuscript which makes a point of this... if I am wrong, please convince me of this soon ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270#issuecomment-425737634
https://github.com/scverse/scanpy/pull/270#issuecomment-425737634:276,Testability,test,test,276,"I just noticed I forgot to answer to your point about multiple-testing-correction. Multiple testing correction deals with the effect that obtaining any false positives become more likely with repeated tests. However, it does not deal with bias. Bias is however present in the test given that the correct null hypothesis should assume some genes are different between groups, and not that all genes are equal. This comes back to the observation that some genes will be always be different between groups in random data, given that clustering defines the groups over which we test. And as for louvain using whole transcriptome similarities rather than individual gene similarities... these are related. If you defined groups by differences of whole transcriptomes, this necesitates individual genes having different means. The correct p-value would be the output of a permutation test. . Could discuss this offline if you like @gokceneraslan. I'm preparing a manuscript which makes a point of this... if I am wrong, please convince me of this soon ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270#issuecomment-425737634
https://github.com/scverse/scanpy/pull/270#issuecomment-425737634:574,Testability,test,test,574,"I just noticed I forgot to answer to your point about multiple-testing-correction. Multiple testing correction deals with the effect that obtaining any false positives become more likely with repeated tests. However, it does not deal with bias. Bias is however present in the test given that the correct null hypothesis should assume some genes are different between groups, and not that all genes are equal. This comes back to the observation that some genes will be always be different between groups in random data, given that clustering defines the groups over which we test. And as for louvain using whole transcriptome similarities rather than individual gene similarities... these are related. If you defined groups by differences of whole transcriptomes, this necesitates individual genes having different means. The correct p-value would be the output of a permutation test. . Could discuss this offline if you like @gokceneraslan. I'm preparing a manuscript which makes a point of this... if I am wrong, please convince me of this soon ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270#issuecomment-425737634
https://github.com/scverse/scanpy/pull/270#issuecomment-425737634:878,Testability,test,test,878,"I just noticed I forgot to answer to your point about multiple-testing-correction. Multiple testing correction deals with the effect that obtaining any false positives become more likely with repeated tests. However, it does not deal with bias. Bias is however present in the test given that the correct null hypothesis should assume some genes are different between groups, and not that all genes are equal. This comes back to the observation that some genes will be always be different between groups in random data, given that clustering defines the groups over which we test. And as for louvain using whole transcriptome similarities rather than individual gene similarities... these are related. If you defined groups by differences of whole transcriptomes, this necesitates individual genes having different means. The correct p-value would be the output of a permutation test. . Could discuss this offline if you like @gokceneraslan. I'm preparing a manuscript which makes a point of this... if I am wrong, please convince me of this soon ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270#issuecomment-425737634
https://github.com/scverse/scanpy/pull/270#issuecomment-425737907:162,Testability,log,logFC,162,I can understand your line of reasoning @a-munoz-rojas. But is it not also dangerous to allow a user an interpretation which may be incorrect? I think outputting logFC values is great... I just have issues with calling the other output P-values. It is at the very least contentious whether they are measures of significance.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270#issuecomment-425737907
https://github.com/scverse/scanpy/pull/270#issuecomment-426409855:189,Testability,test,tests,189,"Sorry about the super late response and thank you for the PR, @a-munoz-rojas. I have not read most of the above discussion, yet. Right now, I just wanted to note that (@a-munoz-rojas), the tests are failing as the wilcoxon test seems to yield different results, now. I don't know whether it's just marginal, but it should be clarified; users should still get the same results as before.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270#issuecomment-426409855
https://github.com/scverse/scanpy/pull/270#issuecomment-426409855:223,Testability,test,test,223,"Sorry about the super late response and thank you for the PR, @a-munoz-rojas. I have not read most of the above discussion, yet. Right now, I just wanted to note that (@a-munoz-rojas), the tests are failing as the wilcoxon test seems to yield different results, now. I don't know whether it's just marginal, but it should be clarified; users should still get the same results as before.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270#issuecomment-426409855
https://github.com/scverse/scanpy/pull/270#issuecomment-426424354:66,Availability,error,error,66,"Thanks for your reply, @falexwolf. I was looking through the same error and I can't really understand why the results are different - it might be a difference in accuracy levels between the manual wilcoxon method that was used before, and the built-in scipy.stats function I used. I looked at the differences and they indeed look marginal. **Edit:** I actually just went back through the check results, and the comparison between the results before and after are pretty much identical - it could just be a difference in bit-depth. For example, it's tagging (2.292195 , 5.7448500e-01) as different from (2.292195 , 0.574485). . I also compared a before-after with my dataset and I get very similar marker genes, albeit in slightly different order (see attached images). I don't really know what would be the best way to address these differences - I am simply using the built in spicy.stats function and not changing the output it gives me. Could this marginal difference be caused by the estimation in the ""chunk"" approach used in the previous version? Even with this marginal difference, I would assume that using the scipy function is more ""accurate"". Please let me know what you would prefer and what would be the best way to proceed.; ![figure_1_newwilcox](https://user-images.githubusercontent.com/37122760/46375973-c93b4700-c662-11e8-8581-b85a28e36dbc.png); ![figure_1_originalwilcox](https://user-images.githubusercontent.com/37122760/46375974-c93b4700-c662-11e8-810b-48238394be1e.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270#issuecomment-426424354
https://github.com/scverse/scanpy/pull/270#issuecomment-426424354:852,Usability,simpl,simply,852,"Thanks for your reply, @falexwolf. I was looking through the same error and I can't really understand why the results are different - it might be a difference in accuracy levels between the manual wilcoxon method that was used before, and the built-in scipy.stats function I used. I looked at the differences and they indeed look marginal. **Edit:** I actually just went back through the check results, and the comparison between the results before and after are pretty much identical - it could just be a difference in bit-depth. For example, it's tagging (2.292195 , 5.7448500e-01) as different from (2.292195 , 0.574485). . I also compared a before-after with my dataset and I get very similar marker genes, albeit in slightly different order (see attached images). I don't really know what would be the best way to address these differences - I am simply using the built in spicy.stats function and not changing the output it gives me. Could this marginal difference be caused by the estimation in the ""chunk"" approach used in the previous version? Even with this marginal difference, I would assume that using the scipy function is more ""accurate"". Please let me know what you would prefer and what would be the best way to proceed.; ![figure_1_newwilcox](https://user-images.githubusercontent.com/37122760/46375973-c93b4700-c662-11e8-8581-b85a28e36dbc.png); ![figure_1_originalwilcox](https://user-images.githubusercontent.com/37122760/46375974-c93b4700-c662-11e8-810b-48238394be1e.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270#issuecomment-426424354
https://github.com/scverse/scanpy/pull/270#issuecomment-427040589:401,Deployability,update,update,401,"@a-munoz-rojas Thanks for checking and thank you very much for the PR :smile! I like your version of the implementation a lot better than the manual one, as the code is much more readable. I don't know why @tcallies added the wilcoxon this way at the time, but I assume for speed and memory reasons. So, I'm very happy to merge this PR; I'll just briefly give this another check today or tomorrow and update the tests so that they don't fail anymore. Regarding the general discussion: Yes, let's just add a disclaimer that several assumptions on how meaningful the null hypotheses are both for wilxocon and t-test for single-cell data, should do the job. Then people will interpret the p-values with care.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270#issuecomment-427040589
https://github.com/scverse/scanpy/pull/270#issuecomment-427040589:412,Testability,test,tests,412,"@a-munoz-rojas Thanks for checking and thank you very much for the PR :smile! I like your version of the implementation a lot better than the manual one, as the code is much more readable. I don't know why @tcallies added the wilcoxon this way at the time, but I assume for speed and memory reasons. So, I'm very happy to merge this PR; I'll just briefly give this another check today or tomorrow and update the tests so that they don't fail anymore. Regarding the general discussion: Yes, let's just add a disclaimer that several assumptions on how meaningful the null hypotheses are both for wilxocon and t-test for single-cell data, should do the job. Then people will interpret the p-values with care.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270#issuecomment-427040589
https://github.com/scverse/scanpy/pull/270#issuecomment-427040589:609,Testability,test,test,609,"@a-munoz-rojas Thanks for checking and thank you very much for the PR :smile! I like your version of the implementation a lot better than the manual one, as the code is much more readable. I don't know why @tcallies added the wilcoxon this way at the time, but I assume for speed and memory reasons. So, I'm very happy to merge this PR; I'll just briefly give this another check today or tomorrow and update the tests so that they don't fail anymore. Regarding the general discussion: Yes, let's just add a disclaimer that several assumptions on how meaningful the null hypotheses are both for wilxocon and t-test for single-cell data, should do the job. Then people will interpret the p-values with care.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270#issuecomment-427040589
https://github.com/scverse/scanpy/pull/270#issuecomment-427480716:204,Energy Efficiency,adapt,adapt,204,"I'm merging this but will restore the previous Wilcoxon implementation, for speed reasons. The essential problem is that scipy.stats does not have a multi-dimensional implementation; it should be easy to adapt the previous implementation so that it provides pvalues, too; simply via multi-dimensional adaption of https://github.com/scipy/scipy/blob/v1.1.0/scipy/stats/stats.py#L4931-L4974.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270#issuecomment-427480716
https://github.com/scverse/scanpy/pull/270#issuecomment-427480716:301,Energy Efficiency,adapt,adaption,301,"I'm merging this but will restore the previous Wilcoxon implementation, for speed reasons. The essential problem is that scipy.stats does not have a multi-dimensional implementation; it should be easy to adapt the previous implementation so that it provides pvalues, too; simply via multi-dimensional adaption of https://github.com/scipy/scipy/blob/v1.1.0/scipy/stats/stats.py#L4931-L4974.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270#issuecomment-427480716
https://github.com/scverse/scanpy/pull/270#issuecomment-427480716:204,Modifiability,adapt,adapt,204,"I'm merging this but will restore the previous Wilcoxon implementation, for speed reasons. The essential problem is that scipy.stats does not have a multi-dimensional implementation; it should be easy to adapt the previous implementation so that it provides pvalues, too; simply via multi-dimensional adaption of https://github.com/scipy/scipy/blob/v1.1.0/scipy/stats/stats.py#L4931-L4974.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270#issuecomment-427480716
https://github.com/scverse/scanpy/pull/270#issuecomment-427480716:301,Modifiability,adapt,adaption,301,"I'm merging this but will restore the previous Wilcoxon implementation, for speed reasons. The essential problem is that scipy.stats does not have a multi-dimensional implementation; it should be easy to adapt the previous implementation so that it provides pvalues, too; simply via multi-dimensional adaption of https://github.com/scipy/scipy/blob/v1.1.0/scipy/stats/stats.py#L4931-L4974.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270#issuecomment-427480716
https://github.com/scverse/scanpy/pull/270#issuecomment-427480716:272,Usability,simpl,simply,272,"I'm merging this but will restore the previous Wilcoxon implementation, for speed reasons. The essential problem is that scipy.stats does not have a multi-dimensional implementation; it should be easy to adapt the previous implementation so that it provides pvalues, too; simply via multi-dimensional adaption of https://github.com/scipy/scipy/blob/v1.1.0/scipy/stats/stats.py#L4931-L4974.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270#issuecomment-427480716
https://github.com/scverse/scanpy/pull/270#issuecomment-427484003:48,Availability,down,down,48,"Sounds great, @falexwolf! I did notice the slow-down and agree it's not great. That's a great suggestion, I'll take a look. Thanks, glad it helped!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270#issuecomment-427484003
https://github.com/scverse/scanpy/pull/270#issuecomment-427484452:455,Testability,test,tests,455,"If you @a-munoz-rojas would give it another try, using a multi-dimensional chunked implementation along the current implementation and the comment above, then that would be very cool! If you don't have bandwidth for that, could you pass this on to @Koncopd, who might have bandwidth? It would be nice to clean up the whole module (in particular, split up the long code chunks into three functions `_t_test()`, `_wilcoxon()`, `_logreg()`). I also made the tests work again; I guess they failed as the current implementation and scipy are handling ties a little different; the results for scores were exactly the same up to the position of a single gene. . Please make a new PR for any of this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270#issuecomment-427484452
https://github.com/scverse/scanpy/pull/270#issuecomment-427485305:12,Testability,test,tests,12,"Ok, now the tests are actually passing again, everything is in the three commits prior and including this one: https://github.com/theislab/scanpy/commit/d889faf9a58d8981c0783584b3f333680fc161ce",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270#issuecomment-427485305
https://github.com/scverse/scanpy/pull/270#issuecomment-427489214:141,Energy Efficiency,adapt,adaptation,141,"I was just about to ask about the chunking along genes - you read my mind @falexwolf. I think it might be possible to do a multi-dimensional adaptation of the scipy.stats code you linked to, and still do the math with sparse matrices, similar to how we implemented the t-tests. This way we could possibly avoid the chunking (it might help with readability of the code). Would this be worth pursuing?. I'll give this a quick try, but I am a little limited in bandwidth. I'll let you know soon if it would be best to get some help from @Koncopd (if they have time!)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270#issuecomment-427489214
https://github.com/scverse/scanpy/pull/270#issuecomment-427489214:141,Modifiability,adapt,adaptation,141,"I was just about to ask about the chunking along genes - you read my mind @falexwolf. I think it might be possible to do a multi-dimensional adaptation of the scipy.stats code you linked to, and still do the math with sparse matrices, similar to how we implemented the t-tests. This way we could possibly avoid the chunking (it might help with readability of the code). Would this be worth pursuing?. I'll give this a quick try, but I am a little limited in bandwidth. I'll let you know soon if it would be best to get some help from @Koncopd (if they have time!)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270#issuecomment-427489214
https://github.com/scverse/scanpy/pull/270#issuecomment-427489214:305,Safety,avoid,avoid,305,"I was just about to ask about the chunking along genes - you read my mind @falexwolf. I think it might be possible to do a multi-dimensional adaptation of the scipy.stats code you linked to, and still do the math with sparse matrices, similar to how we implemented the t-tests. This way we could possibly avoid the chunking (it might help with readability of the code). Would this be worth pursuing?. I'll give this a quick try, but I am a little limited in bandwidth. I'll let you know soon if it would be best to get some help from @Koncopd (if they have time!)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270#issuecomment-427489214
https://github.com/scverse/scanpy/pull/270#issuecomment-427489214:271,Testability,test,tests,271,"I was just about to ask about the chunking along genes - you read my mind @falexwolf. I think it might be possible to do a multi-dimensional adaptation of the scipy.stats code you linked to, and still do the math with sparse matrices, similar to how we implemented the t-tests. This way we could possibly avoid the chunking (it might help with readability of the code). Would this be worth pursuing?. I'll give this a quick try, but I am a little limited in bandwidth. I'll let you know soon if it would be best to get some help from @Koncopd (if they have time!)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270#issuecomment-427489214
https://github.com/scverse/scanpy/issues/271#issuecomment-425031141:166,Modifiability,plugin,plugins,166,This page summarizes the approaches mentioned by @flying-sheep together with examples to implement them: https://packaging.python.org/guides/creating-and-discovering-plugins/. My opinion is to implement the option that is easier for the plugin developer to facilitate adoption.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271#issuecomment-425031141
https://github.com/scverse/scanpy/issues/271#issuecomment-425031141:237,Modifiability,plugin,plugin,237,This page summarizes the approaches mentioned by @flying-sheep together with examples to implement them: https://packaging.python.org/guides/creating-and-discovering-plugins/. My opinion is to implement the option that is easier for the plugin developer to facilitate adoption.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271#issuecomment-425031141
https://github.com/scverse/scanpy/issues/271#issuecomment-425031141:134,Usability,guid,guides,134,This page summarizes the approaches mentioned by @flying-sheep together with examples to implement them: https://packaging.python.org/guides/creating-and-discovering-plugins/. My opinion is to implement the option that is easier for the plugin developer to facilitate adoption.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271#issuecomment-425031141
https://github.com/scverse/scanpy/issues/271#issuecomment-425038336:131,Modifiability,plugin,plugins,131,Thank you! It talks about option 2 there: [#using-namespace-packages](https://packaging.python.org/guides/creating-and-discovering-plugins/#using-namespace-packages) and Option 1 here: [#using-package-metadata](https://packaging.python.org/guides/creating-and-discovering-plugins/#using-package-metadata). The amount of work for plugin devs is completely covered in the first comment: one line added to `setup.py` each.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271#issuecomment-425038336
https://github.com/scverse/scanpy/issues/271#issuecomment-425038336:272,Modifiability,plugin,plugins,272,Thank you! It talks about option 2 there: [#using-namespace-packages](https://packaging.python.org/guides/creating-and-discovering-plugins/#using-namespace-packages) and Option 1 here: [#using-package-metadata](https://packaging.python.org/guides/creating-and-discovering-plugins/#using-package-metadata). The amount of work for plugin devs is completely covered in the first comment: one line added to `setup.py` each.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271#issuecomment-425038336
https://github.com/scverse/scanpy/issues/271#issuecomment-425038336:329,Modifiability,plugin,plugin,329,Thank you! It talks about option 2 there: [#using-namespace-packages](https://packaging.python.org/guides/creating-and-discovering-plugins/#using-namespace-packages) and Option 1 here: [#using-package-metadata](https://packaging.python.org/guides/creating-and-discovering-plugins/#using-package-metadata). The amount of work for plugin devs is completely covered in the first comment: one line added to `setup.py` each.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271#issuecomment-425038336
https://github.com/scverse/scanpy/issues/271#issuecomment-425038336:99,Usability,guid,guides,99,Thank you! It talks about option 2 there: [#using-namespace-packages](https://packaging.python.org/guides/creating-and-discovering-plugins/#using-namespace-packages) and Option 1 here: [#using-package-metadata](https://packaging.python.org/guides/creating-and-discovering-plugins/#using-package-metadata). The amount of work for plugin devs is completely covered in the first comment: one line added to `setup.py` each.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271#issuecomment-425038336
https://github.com/scverse/scanpy/issues/271#issuecomment-425038336:240,Usability,guid,guides,240,Thank you! It talks about option 2 there: [#using-namespace-packages](https://packaging.python.org/guides/creating-and-discovering-plugins/#using-namespace-packages) and Option 1 here: [#using-package-metadata](https://packaging.python.org/guides/creating-and-discovering-plugins/#using-package-metadata). The amount of work for plugin devs is completely covered in the first comment: one line added to `setup.py` each.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271#issuecomment-425038336
https://github.com/scverse/scanpy/issues/271#issuecomment-431634492:763,Deployability,install,installing,763,"I think the current approach - a very simple interface as in `scanpy/tools/phate.py` and a bunch of others is the easiest way to go for the developer. So, I'd say we make a submodule `.ext` with the `.tools`, `.plotting`, `.preprocessing` substructure in it. We move things like `phate.py` into `scanpy/ext/tools`. We maintain backwards compat by still reexporting it in `scanpy.api`. The canonical way of calling these extension will be by importing `import scanpy.ext as sce` and people can use that extension namespace and call everything in the same way that they are used to. Users can look up extension tools on docs site like [this](https://scanpy.readthedocs.io/en/latest/api/index.html). It will also be clear to users that these extensions will require installing additional packages, which don't come with the default scanpy. Of course, all of this needs none of the ""extension mechanisms"" mentioned above. But people really don't want to write actual ""scanpy extensions""; they want to write their own packages and have them interface with scanpy so that convenient calls are enabled without the need to adapt to new conventions. For the scanpy users, the cool things is that a large number of tools can be quickly tested out. If you don't mind, @fidelram and @flying-sheep, @Koncopd would go along and make this modest change.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271#issuecomment-431634492
https://github.com/scverse/scanpy/issues/271#issuecomment-431634492:1115,Energy Efficiency,adapt,adapt,1115,"I think the current approach - a very simple interface as in `scanpy/tools/phate.py` and a bunch of others is the easiest way to go for the developer. So, I'd say we make a submodule `.ext` with the `.tools`, `.plotting`, `.preprocessing` substructure in it. We move things like `phate.py` into `scanpy/ext/tools`. We maintain backwards compat by still reexporting it in `scanpy.api`. The canonical way of calling these extension will be by importing `import scanpy.ext as sce` and people can use that extension namespace and call everything in the same way that they are used to. Users can look up extension tools on docs site like [this](https://scanpy.readthedocs.io/en/latest/api/index.html). It will also be clear to users that these extensions will require installing additional packages, which don't come with the default scanpy. Of course, all of this needs none of the ""extension mechanisms"" mentioned above. But people really don't want to write actual ""scanpy extensions""; they want to write their own packages and have them interface with scanpy so that convenient calls are enabled without the need to adapt to new conventions. For the scanpy users, the cool things is that a large number of tools can be quickly tested out. If you don't mind, @fidelram and @flying-sheep, @Koncopd would go along and make this modest change.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271#issuecomment-431634492
https://github.com/scverse/scanpy/issues/271#issuecomment-431634492:45,Integrability,interface,interface,45,"I think the current approach - a very simple interface as in `scanpy/tools/phate.py` and a bunch of others is the easiest way to go for the developer. So, I'd say we make a submodule `.ext` with the `.tools`, `.plotting`, `.preprocessing` substructure in it. We move things like `phate.py` into `scanpy/ext/tools`. We maintain backwards compat by still reexporting it in `scanpy.api`. The canonical way of calling these extension will be by importing `import scanpy.ext as sce` and people can use that extension namespace and call everything in the same way that they are used to. Users can look up extension tools on docs site like [this](https://scanpy.readthedocs.io/en/latest/api/index.html). It will also be clear to users that these extensions will require installing additional packages, which don't come with the default scanpy. Of course, all of this needs none of the ""extension mechanisms"" mentioned above. But people really don't want to write actual ""scanpy extensions""; they want to write their own packages and have them interface with scanpy so that convenient calls are enabled without the need to adapt to new conventions. For the scanpy users, the cool things is that a large number of tools can be quickly tested out. If you don't mind, @fidelram and @flying-sheep, @Koncopd would go along and make this modest change.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271#issuecomment-431634492
https://github.com/scverse/scanpy/issues/271#issuecomment-431634492:1036,Integrability,interface,interface,1036,"I think the current approach - a very simple interface as in `scanpy/tools/phate.py` and a bunch of others is the easiest way to go for the developer. So, I'd say we make a submodule `.ext` with the `.tools`, `.plotting`, `.preprocessing` substructure in it. We move things like `phate.py` into `scanpy/ext/tools`. We maintain backwards compat by still reexporting it in `scanpy.api`. The canonical way of calling these extension will be by importing `import scanpy.ext as sce` and people can use that extension namespace and call everything in the same way that they are used to. Users can look up extension tools on docs site like [this](https://scanpy.readthedocs.io/en/latest/api/index.html). It will also be clear to users that these extensions will require installing additional packages, which don't come with the default scanpy. Of course, all of this needs none of the ""extension mechanisms"" mentioned above. But people really don't want to write actual ""scanpy extensions""; they want to write their own packages and have them interface with scanpy so that convenient calls are enabled without the need to adapt to new conventions. For the scanpy users, the cool things is that a large number of tools can be quickly tested out. If you don't mind, @fidelram and @flying-sheep, @Koncopd would go along and make this modest change.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271#issuecomment-431634492
https://github.com/scverse/scanpy/issues/271#issuecomment-431634492:1115,Modifiability,adapt,adapt,1115,"I think the current approach - a very simple interface as in `scanpy/tools/phate.py` and a bunch of others is the easiest way to go for the developer. So, I'd say we make a submodule `.ext` with the `.tools`, `.plotting`, `.preprocessing` substructure in it. We move things like `phate.py` into `scanpy/ext/tools`. We maintain backwards compat by still reexporting it in `scanpy.api`. The canonical way of calling these extension will be by importing `import scanpy.ext as sce` and people can use that extension namespace and call everything in the same way that they are used to. Users can look up extension tools on docs site like [this](https://scanpy.readthedocs.io/en/latest/api/index.html). It will also be clear to users that these extensions will require installing additional packages, which don't come with the default scanpy. Of course, all of this needs none of the ""extension mechanisms"" mentioned above. But people really don't want to write actual ""scanpy extensions""; they want to write their own packages and have them interface with scanpy so that convenient calls are enabled without the need to adapt to new conventions. For the scanpy users, the cool things is that a large number of tools can be quickly tested out. If you don't mind, @fidelram and @flying-sheep, @Koncopd would go along and make this modest change.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271#issuecomment-431634492
https://github.com/scverse/scanpy/issues/271#issuecomment-431634492:1226,Testability,test,tested,1226,"I think the current approach - a very simple interface as in `scanpy/tools/phate.py` and a bunch of others is the easiest way to go for the developer. So, I'd say we make a submodule `.ext` with the `.tools`, `.plotting`, `.preprocessing` substructure in it. We move things like `phate.py` into `scanpy/ext/tools`. We maintain backwards compat by still reexporting it in `scanpy.api`. The canonical way of calling these extension will be by importing `import scanpy.ext as sce` and people can use that extension namespace and call everything in the same way that they are used to. Users can look up extension tools on docs site like [this](https://scanpy.readthedocs.io/en/latest/api/index.html). It will also be clear to users that these extensions will require installing additional packages, which don't come with the default scanpy. Of course, all of this needs none of the ""extension mechanisms"" mentioned above. But people really don't want to write actual ""scanpy extensions""; they want to write their own packages and have them interface with scanpy so that convenient calls are enabled without the need to adapt to new conventions. For the scanpy users, the cool things is that a large number of tools can be quickly tested out. If you don't mind, @fidelram and @flying-sheep, @Koncopd would go along and make this modest change.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271#issuecomment-431634492
https://github.com/scverse/scanpy/issues/271#issuecomment-431634492:38,Usability,simpl,simple,38,"I think the current approach - a very simple interface as in `scanpy/tools/phate.py` and a bunch of others is the easiest way to go for the developer. So, I'd say we make a submodule `.ext` with the `.tools`, `.plotting`, `.preprocessing` substructure in it. We move things like `phate.py` into `scanpy/ext/tools`. We maintain backwards compat by still reexporting it in `scanpy.api`. The canonical way of calling these extension will be by importing `import scanpy.ext as sce` and people can use that extension namespace and call everything in the same way that they are used to. Users can look up extension tools on docs site like [this](https://scanpy.readthedocs.io/en/latest/api/index.html). It will also be clear to users that these extensions will require installing additional packages, which don't come with the default scanpy. Of course, all of this needs none of the ""extension mechanisms"" mentioned above. But people really don't want to write actual ""scanpy extensions""; they want to write their own packages and have them interface with scanpy so that convenient calls are enabled without the need to adapt to new conventions. For the scanpy users, the cool things is that a large number of tools can be quickly tested out. If you don't mind, @fidelram and @flying-sheep, @Koncopd would go along and make this modest change.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271#issuecomment-431634492
https://github.com/scverse/scanpy/issues/271#issuecomment-431634492:713,Usability,clear,clear,713,"I think the current approach - a very simple interface as in `scanpy/tools/phate.py` and a bunch of others is the easiest way to go for the developer. So, I'd say we make a submodule `.ext` with the `.tools`, `.plotting`, `.preprocessing` substructure in it. We move things like `phate.py` into `scanpy/ext/tools`. We maintain backwards compat by still reexporting it in `scanpy.api`. The canonical way of calling these extension will be by importing `import scanpy.ext as sce` and people can use that extension namespace and call everything in the same way that they are used to. Users can look up extension tools on docs site like [this](https://scanpy.readthedocs.io/en/latest/api/index.html). It will also be clear to users that these extensions will require installing additional packages, which don't come with the default scanpy. Of course, all of this needs none of the ""extension mechanisms"" mentioned above. But people really don't want to write actual ""scanpy extensions""; they want to write their own packages and have them interface with scanpy so that convenient calls are enabled without the need to adapt to new conventions. For the scanpy users, the cool things is that a large number of tools can be quickly tested out. If you don't mind, @fidelram and @flying-sheep, @Koncopd would go along and make this modest change.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271#issuecomment-431634492
https://github.com/scverse/scanpy/issues/271#issuecomment-432728241:64,Modifiability,extend,extend,64,"sounds good!. but of course, having a way to find packages that extend scanpy would be cool. if we could search PyPI for a specific entry point type, and there was a scanpy entry point, that would make finding them a breeze. or we do a `awesome-scanpy` repo that collects tutorials, extensions, and so on!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271#issuecomment-432728241
https://github.com/scverse/scanpy/issues/273#issuecomment-425136119:312,Testability,log,logreg,312,"The same issue happens when renaming categories with the built-in rename categories function. I think, this definitely should not happen. Example:. ```python; import pandas as pd; import scanpy.api as sc. adata = sc.datasets.blobs(640, 3); sc.tl.pca(adata); sc.pp.neighbors(adata); sc.tl.louvain(adata). method='logreg'. sc.tl.rank_genes_groups(adata, 'louvain', method=method); print(pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(1)). adata.rename_categories('louvain', ['Zero', 'One', 'Two']). sc.tl.rank_genes_groups(adata, 'louvain', method=method); print(pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(1)); ```; gives; ```python; 0 1 2; 0 570 63 126; Zero One Two; 0 63 126 570; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/273#issuecomment-425136119
https://github.com/scverse/scanpy/issues/273#issuecomment-427033148:197,Usability,learn,learn,197,"Thank you very much for the super clean examples here! Is fixed via https://github.com/theislab/scanpy/commit/0ed304a64038f7d2c11b36fe5883ab9765ffba57 (@yugeji, you fed a pandas series into scikit learn, which is generally a bad idea :wink: ).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/273#issuecomment-427033148
https://github.com/scverse/scanpy/issues/275#issuecomment-427036065:78,Deployability,update,updated,78,"Thank you for pointing it out, it is fixed in the current version and will be updated online soon. You're right that it didn't have any effect on the tutorial otherwise.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/275#issuecomment-427036065
https://github.com/scverse/scanpy/issues/277#issuecomment-426845759:365,Usability,clear,clear,365,"Part of the UMAP computation is creating an approximate KNN graph. Scanpy uses that part of the UMAP package to create it's neighborhood graph. UMAP uses whatever distance metric and feature space the user specifies for finding neighbors. Scanpy uses euclidean distance in PCA space as the default for finding neighbors, but that can be changed. I hope that's more clear.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/277#issuecomment-426845759
https://github.com/scverse/scanpy/issues/277#issuecomment-427333649:320,Availability,avail,available,320,"> I'm not to sure what the assumptions are behind each method though. @falexwolf, any reason in particular you've chosen UMAP's method for the KNN calculation?. It's highly competitive in terms of speed and accuracy with other libraries (https://github.com/erikbern/ann-benchmarks, pynndescent is what umap uses, wasn't available at the time for Scanpy), it's a lot easier to install than everything else, and the result has been shown to harmonize well with UMAP, which I expected would become the canonical way of visualizing things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/277#issuecomment-427333649
https://github.com/scverse/scanpy/issues/277#issuecomment-427333649:376,Deployability,install,install,376,"> I'm not to sure what the assumptions are behind each method though. @falexwolf, any reason in particular you've chosen UMAP's method for the KNN calculation?. It's highly competitive in terms of speed and accuracy with other libraries (https://github.com/erikbern/ann-benchmarks, pynndescent is what umap uses, wasn't available at the time for Scanpy), it's a lot easier to install than everything else, and the result has been shown to harmonize well with UMAP, which I expected would become the canonical way of visualizing things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/277#issuecomment-427333649
https://github.com/scverse/scanpy/issues/277#issuecomment-427333649:270,Testability,benchmark,benchmarks,270,"> I'm not to sure what the assumptions are behind each method though. @falexwolf, any reason in particular you've chosen UMAP's method for the KNN calculation?. It's highly competitive in terms of speed and accuracy with other libraries (https://github.com/erikbern/ann-benchmarks, pynndescent is what umap uses, wasn't available at the time for Scanpy), it's a lot easier to install than everything else, and the result has been shown to harmonize well with UMAP, which I expected would become the canonical way of visualizing things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/277#issuecomment-427333649
https://github.com/scverse/scanpy/issues/277#issuecomment-427379020:64,Performance,bottleneck,bottleneck,64,"Thanks for the explanation! I had no idea this was an important bottleneck. I always assumed all distances are calculated... that shows how little I think about optimization ^^. On another note, it might be a little confusing to see method='umap' as a parameter. I would have immediately assumed that umap is used as a distance metric. But maybe that's me being too lazy to read as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/277#issuecomment-427379020
https://github.com/scverse/scanpy/issues/277#issuecomment-427379020:161,Performance,optimiz,optimization,161,"Thanks for the explanation! I had no idea this was an important bottleneck. I always assumed all distances are calculated... that shows how little I think about optimization ^^. On another note, it might be a little confusing to see method='umap' as a parameter. I would have immediately assumed that umap is used as a distance metric. But maybe that's me being too lazy to read as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/277#issuecomment-427379020
https://github.com/scverse/scanpy/issues/278#issuecomment-427037743:27,Testability,test,tested,27,"No, each of '1' and '2' is tested against the ""rest"" of the data, that is the union of '2' & '3' in the first, and the union of '1' and '3' in the second case. `groups` merely subsets which groups to look at, the default is to look at all, where 'all' will be equivalent to `['1', '2', '3']`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278#issuecomment-427037743
https://github.com/scverse/scanpy/issues/278#issuecomment-427063030:733,Safety,detect,detected,733,"What worries me is that the behaviour you describe might not so for `method='logreg'`. Let's have a look at the following example:. ```python; import scanpy.api as sc; adata = sc.datasets.blobs(); sc.pp.scale(adata). sc.pp.neighbors(adata); sc.tl.louvain(adata). for method in ['t-test', 'logreg']:; # first call, without groups; sc.tl.rank_genes_groups(adata, 'louvain', method=method); sc.pl.rank_genes_groups_matrixplot(adata, n_genes=1). # first call, with groups ; sc.tl.rank_genes_groups(adata, 'louvain', method=method, groups=['0', '1', '2']); sc.pl.rank_genes_groups_matrixplot(adata, n_genes=1); ```; As setting `groups` to `['0', '1', '2']` should not change the reference dataset, exactly the same marker genes should be detected for the first and the second call of `sc.tl.rank_genes_groups`. This is indeed true if I set the method to `t-test`. However, when setting method to `logreg`, I get other marker genes. Visually it appears to me that only the groups `['0', '1', '2']` are used a the reference set when using the method `logreg`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278#issuecomment-427063030
https://github.com/scverse/scanpy/issues/278#issuecomment-427063030:77,Testability,log,logreg,77,"What worries me is that the behaviour you describe might not so for `method='logreg'`. Let's have a look at the following example:. ```python; import scanpy.api as sc; adata = sc.datasets.blobs(); sc.pp.scale(adata). sc.pp.neighbors(adata); sc.tl.louvain(adata). for method in ['t-test', 'logreg']:; # first call, without groups; sc.tl.rank_genes_groups(adata, 'louvain', method=method); sc.pl.rank_genes_groups_matrixplot(adata, n_genes=1). # first call, with groups ; sc.tl.rank_genes_groups(adata, 'louvain', method=method, groups=['0', '1', '2']); sc.pl.rank_genes_groups_matrixplot(adata, n_genes=1); ```; As setting `groups` to `['0', '1', '2']` should not change the reference dataset, exactly the same marker genes should be detected for the first and the second call of `sc.tl.rank_genes_groups`. This is indeed true if I set the method to `t-test`. However, when setting method to `logreg`, I get other marker genes. Visually it appears to me that only the groups `['0', '1', '2']` are used a the reference set when using the method `logreg`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278#issuecomment-427063030
https://github.com/scverse/scanpy/issues/278#issuecomment-427063030:281,Testability,test,test,281,"What worries me is that the behaviour you describe might not so for `method='logreg'`. Let's have a look at the following example:. ```python; import scanpy.api as sc; adata = sc.datasets.blobs(); sc.pp.scale(adata). sc.pp.neighbors(adata); sc.tl.louvain(adata). for method in ['t-test', 'logreg']:; # first call, without groups; sc.tl.rank_genes_groups(adata, 'louvain', method=method); sc.pl.rank_genes_groups_matrixplot(adata, n_genes=1). # first call, with groups ; sc.tl.rank_genes_groups(adata, 'louvain', method=method, groups=['0', '1', '2']); sc.pl.rank_genes_groups_matrixplot(adata, n_genes=1); ```; As setting `groups` to `['0', '1', '2']` should not change the reference dataset, exactly the same marker genes should be detected for the first and the second call of `sc.tl.rank_genes_groups`. This is indeed true if I set the method to `t-test`. However, when setting method to `logreg`, I get other marker genes. Visually it appears to me that only the groups `['0', '1', '2']` are used a the reference set when using the method `logreg`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278#issuecomment-427063030
https://github.com/scverse/scanpy/issues/278#issuecomment-427063030:289,Testability,log,logreg,289,"What worries me is that the behaviour you describe might not so for `method='logreg'`. Let's have a look at the following example:. ```python; import scanpy.api as sc; adata = sc.datasets.blobs(); sc.pp.scale(adata). sc.pp.neighbors(adata); sc.tl.louvain(adata). for method in ['t-test', 'logreg']:; # first call, without groups; sc.tl.rank_genes_groups(adata, 'louvain', method=method); sc.pl.rank_genes_groups_matrixplot(adata, n_genes=1). # first call, with groups ; sc.tl.rank_genes_groups(adata, 'louvain', method=method, groups=['0', '1', '2']); sc.pl.rank_genes_groups_matrixplot(adata, n_genes=1); ```; As setting `groups` to `['0', '1', '2']` should not change the reference dataset, exactly the same marker genes should be detected for the first and the second call of `sc.tl.rank_genes_groups`. This is indeed true if I set the method to `t-test`. However, when setting method to `logreg`, I get other marker genes. Visually it appears to me that only the groups `['0', '1', '2']` are used a the reference set when using the method `logreg`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278#issuecomment-427063030
https://github.com/scverse/scanpy/issues/278#issuecomment-427063030:852,Testability,test,test,852,"What worries me is that the behaviour you describe might not so for `method='logreg'`. Let's have a look at the following example:. ```python; import scanpy.api as sc; adata = sc.datasets.blobs(); sc.pp.scale(adata). sc.pp.neighbors(adata); sc.tl.louvain(adata). for method in ['t-test', 'logreg']:; # first call, without groups; sc.tl.rank_genes_groups(adata, 'louvain', method=method); sc.pl.rank_genes_groups_matrixplot(adata, n_genes=1). # first call, with groups ; sc.tl.rank_genes_groups(adata, 'louvain', method=method, groups=['0', '1', '2']); sc.pl.rank_genes_groups_matrixplot(adata, n_genes=1); ```; As setting `groups` to `['0', '1', '2']` should not change the reference dataset, exactly the same marker genes should be detected for the first and the second call of `sc.tl.rank_genes_groups`. This is indeed true if I set the method to `t-test`. However, when setting method to `logreg`, I get other marker genes. Visually it appears to me that only the groups `['0', '1', '2']` are used a the reference set when using the method `logreg`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278#issuecomment-427063030
https://github.com/scverse/scanpy/issues/278#issuecomment-427063030:892,Testability,log,logreg,892,"What worries me is that the behaviour you describe might not so for `method='logreg'`. Let's have a look at the following example:. ```python; import scanpy.api as sc; adata = sc.datasets.blobs(); sc.pp.scale(adata). sc.pp.neighbors(adata); sc.tl.louvain(adata). for method in ['t-test', 'logreg']:; # first call, without groups; sc.tl.rank_genes_groups(adata, 'louvain', method=method); sc.pl.rank_genes_groups_matrixplot(adata, n_genes=1). # first call, with groups ; sc.tl.rank_genes_groups(adata, 'louvain', method=method, groups=['0', '1', '2']); sc.pl.rank_genes_groups_matrixplot(adata, n_genes=1); ```; As setting `groups` to `['0', '1', '2']` should not change the reference dataset, exactly the same marker genes should be detected for the first and the second call of `sc.tl.rank_genes_groups`. This is indeed true if I set the method to `t-test`. However, when setting method to `logreg`, I get other marker genes. Visually it appears to me that only the groups `['0', '1', '2']` are used a the reference set when using the method `logreg`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278#issuecomment-427063030
https://github.com/scverse/scanpy/issues/278#issuecomment-427063030:1044,Testability,log,logreg,1044,"What worries me is that the behaviour you describe might not so for `method='logreg'`. Let's have a look at the following example:. ```python; import scanpy.api as sc; adata = sc.datasets.blobs(); sc.pp.scale(adata). sc.pp.neighbors(adata); sc.tl.louvain(adata). for method in ['t-test', 'logreg']:; # first call, without groups; sc.tl.rank_genes_groups(adata, 'louvain', method=method); sc.pl.rank_genes_groups_matrixplot(adata, n_genes=1). # first call, with groups ; sc.tl.rank_genes_groups(adata, 'louvain', method=method, groups=['0', '1', '2']); sc.pl.rank_genes_groups_matrixplot(adata, n_genes=1); ```; As setting `groups` to `['0', '1', '2']` should not change the reference dataset, exactly the same marker genes should be detected for the first and the second call of `sc.tl.rank_genes_groups`. This is indeed true if I set the method to `t-test`. However, when setting method to `logreg`, I get other marker genes. Visually it appears to me that only the groups `['0', '1', '2']` are used a the reference set when using the method `logreg`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278#issuecomment-427063030
https://github.com/scverse/scanpy/issues/278#issuecomment-427093467:441,Modifiability,extend,extended,441,"Yes, right, for `logreg` you always regress on data that includes the respective group as this is not a differential test, but a regression. It's a shame that this is not evident from the documentation. It's sort of a recent alternative way of approaching the definition of marker genes and we should give it a bit more of a thought. Regarding the behavior that is inconsistent up there, there is the following in the code by the person who extended this not long ago:; ```; # if reference is not set, then the groups listed will be compared to the rest; # if reference is set, then the groups listed will be compared only to the other groups listed; from sklearn.linear_model import LogisticRegression; if len(groups) == 1:; raise Exception('Cannot perform logistic regression on a single cluster.'); adata_copy = adata[adata.obs[groupby].isin(groups_order)] ; adata_comp = adata_copy; if adata.raw is not None and use_raw:; adata_comp = adata_copy.raw; X = adata_comp.X. clf = LogisticRegression(**kwds); clf.fit(X, adata_copy.obs[groupby].cat.codes); ```. You're right that logreg only includes the passed groups, if groups are passed. This should not be the case. I wonder why it's a problem in your specific case as I'd expect that 0, 1, 2 make up the whole data; but maybe the resolution in Louvain is somewhat set to a high value. In any case, I'll change the implementation so that irrespective of whether `groups` is passed or not, one gets the same result.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278#issuecomment-427093467
https://github.com/scverse/scanpy/issues/278#issuecomment-427093467:750,Performance,perform,perform,750,"Yes, right, for `logreg` you always regress on data that includes the respective group as this is not a differential test, but a regression. It's a shame that this is not evident from the documentation. It's sort of a recent alternative way of approaching the definition of marker genes and we should give it a bit more of a thought. Regarding the behavior that is inconsistent up there, there is the following in the code by the person who extended this not long ago:; ```; # if reference is not set, then the groups listed will be compared to the rest; # if reference is set, then the groups listed will be compared only to the other groups listed; from sklearn.linear_model import LogisticRegression; if len(groups) == 1:; raise Exception('Cannot perform logistic regression on a single cluster.'); adata_copy = adata[adata.obs[groupby].isin(groups_order)] ; adata_comp = adata_copy; if adata.raw is not None and use_raw:; adata_comp = adata_copy.raw; X = adata_comp.X. clf = LogisticRegression(**kwds); clf.fit(X, adata_copy.obs[groupby].cat.codes); ```. You're right that logreg only includes the passed groups, if groups are passed. This should not be the case. I wonder why it's a problem in your specific case as I'd expect that 0, 1, 2 make up the whole data; but maybe the resolution in Louvain is somewhat set to a high value. In any case, I'll change the implementation so that irrespective of whether `groups` is passed or not, one gets the same result.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278#issuecomment-427093467
https://github.com/scverse/scanpy/issues/278#issuecomment-427093467:17,Testability,log,logreg,17,"Yes, right, for `logreg` you always regress on data that includes the respective group as this is not a differential test, but a regression. It's a shame that this is not evident from the documentation. It's sort of a recent alternative way of approaching the definition of marker genes and we should give it a bit more of a thought. Regarding the behavior that is inconsistent up there, there is the following in the code by the person who extended this not long ago:; ```; # if reference is not set, then the groups listed will be compared to the rest; # if reference is set, then the groups listed will be compared only to the other groups listed; from sklearn.linear_model import LogisticRegression; if len(groups) == 1:; raise Exception('Cannot perform logistic regression on a single cluster.'); adata_copy = adata[adata.obs[groupby].isin(groups_order)] ; adata_comp = adata_copy; if adata.raw is not None and use_raw:; adata_comp = adata_copy.raw; X = adata_comp.X. clf = LogisticRegression(**kwds); clf.fit(X, adata_copy.obs[groupby].cat.codes); ```. You're right that logreg only includes the passed groups, if groups are passed. This should not be the case. I wonder why it's a problem in your specific case as I'd expect that 0, 1, 2 make up the whole data; but maybe the resolution in Louvain is somewhat set to a high value. In any case, I'll change the implementation so that irrespective of whether `groups` is passed or not, one gets the same result.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278#issuecomment-427093467
https://github.com/scverse/scanpy/issues/278#issuecomment-427093467:117,Testability,test,test,117,"Yes, right, for `logreg` you always regress on data that includes the respective group as this is not a differential test, but a regression. It's a shame that this is not evident from the documentation. It's sort of a recent alternative way of approaching the definition of marker genes and we should give it a bit more of a thought. Regarding the behavior that is inconsistent up there, there is the following in the code by the person who extended this not long ago:; ```; # if reference is not set, then the groups listed will be compared to the rest; # if reference is set, then the groups listed will be compared only to the other groups listed; from sklearn.linear_model import LogisticRegression; if len(groups) == 1:; raise Exception('Cannot perform logistic regression on a single cluster.'); adata_copy = adata[adata.obs[groupby].isin(groups_order)] ; adata_comp = adata_copy; if adata.raw is not None and use_raw:; adata_comp = adata_copy.raw; X = adata_comp.X. clf = LogisticRegression(**kwds); clf.fit(X, adata_copy.obs[groupby].cat.codes); ```. You're right that logreg only includes the passed groups, if groups are passed. This should not be the case. I wonder why it's a problem in your specific case as I'd expect that 0, 1, 2 make up the whole data; but maybe the resolution in Louvain is somewhat set to a high value. In any case, I'll change the implementation so that irrespective of whether `groups` is passed or not, one gets the same result.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278#issuecomment-427093467
https://github.com/scverse/scanpy/issues/278#issuecomment-427093467:684,Testability,Log,LogisticRegression,684,"Yes, right, for `logreg` you always regress on data that includes the respective group as this is not a differential test, but a regression. It's a shame that this is not evident from the documentation. It's sort of a recent alternative way of approaching the definition of marker genes and we should give it a bit more of a thought. Regarding the behavior that is inconsistent up there, there is the following in the code by the person who extended this not long ago:; ```; # if reference is not set, then the groups listed will be compared to the rest; # if reference is set, then the groups listed will be compared only to the other groups listed; from sklearn.linear_model import LogisticRegression; if len(groups) == 1:; raise Exception('Cannot perform logistic regression on a single cluster.'); adata_copy = adata[adata.obs[groupby].isin(groups_order)] ; adata_comp = adata_copy; if adata.raw is not None and use_raw:; adata_comp = adata_copy.raw; X = adata_comp.X. clf = LogisticRegression(**kwds); clf.fit(X, adata_copy.obs[groupby].cat.codes); ```. You're right that logreg only includes the passed groups, if groups are passed. This should not be the case. I wonder why it's a problem in your specific case as I'd expect that 0, 1, 2 make up the whole data; but maybe the resolution in Louvain is somewhat set to a high value. In any case, I'll change the implementation so that irrespective of whether `groups` is passed or not, one gets the same result.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278#issuecomment-427093467
https://github.com/scverse/scanpy/issues/278#issuecomment-427093467:758,Testability,log,logistic,758,"Yes, right, for `logreg` you always regress on data that includes the respective group as this is not a differential test, but a regression. It's a shame that this is not evident from the documentation. It's sort of a recent alternative way of approaching the definition of marker genes and we should give it a bit more of a thought. Regarding the behavior that is inconsistent up there, there is the following in the code by the person who extended this not long ago:; ```; # if reference is not set, then the groups listed will be compared to the rest; # if reference is set, then the groups listed will be compared only to the other groups listed; from sklearn.linear_model import LogisticRegression; if len(groups) == 1:; raise Exception('Cannot perform logistic regression on a single cluster.'); adata_copy = adata[adata.obs[groupby].isin(groups_order)] ; adata_comp = adata_copy; if adata.raw is not None and use_raw:; adata_comp = adata_copy.raw; X = adata_comp.X. clf = LogisticRegression(**kwds); clf.fit(X, adata_copy.obs[groupby].cat.codes); ```. You're right that logreg only includes the passed groups, if groups are passed. This should not be the case. I wonder why it's a problem in your specific case as I'd expect that 0, 1, 2 make up the whole data; but maybe the resolution in Louvain is somewhat set to a high value. In any case, I'll change the implementation so that irrespective of whether `groups` is passed or not, one gets the same result.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278#issuecomment-427093467
https://github.com/scverse/scanpy/issues/278#issuecomment-427093467:979,Testability,Log,LogisticRegression,979,"Yes, right, for `logreg` you always regress on data that includes the respective group as this is not a differential test, but a regression. It's a shame that this is not evident from the documentation. It's sort of a recent alternative way of approaching the definition of marker genes and we should give it a bit more of a thought. Regarding the behavior that is inconsistent up there, there is the following in the code by the person who extended this not long ago:; ```; # if reference is not set, then the groups listed will be compared to the rest; # if reference is set, then the groups listed will be compared only to the other groups listed; from sklearn.linear_model import LogisticRegression; if len(groups) == 1:; raise Exception('Cannot perform logistic regression on a single cluster.'); adata_copy = adata[adata.obs[groupby].isin(groups_order)] ; adata_comp = adata_copy; if adata.raw is not None and use_raw:; adata_comp = adata_copy.raw; X = adata_comp.X. clf = LogisticRegression(**kwds); clf.fit(X, adata_copy.obs[groupby].cat.codes); ```. You're right that logreg only includes the passed groups, if groups are passed. This should not be the case. I wonder why it's a problem in your specific case as I'd expect that 0, 1, 2 make up the whole data; but maybe the resolution in Louvain is somewhat set to a high value. In any case, I'll change the implementation so that irrespective of whether `groups` is passed or not, one gets the same result.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278#issuecomment-427093467
https://github.com/scverse/scanpy/issues/278#issuecomment-427093467:1077,Testability,log,logreg,1077,"Yes, right, for `logreg` you always regress on data that includes the respective group as this is not a differential test, but a regression. It's a shame that this is not evident from the documentation. It's sort of a recent alternative way of approaching the definition of marker genes and we should give it a bit more of a thought. Regarding the behavior that is inconsistent up there, there is the following in the code by the person who extended this not long ago:; ```; # if reference is not set, then the groups listed will be compared to the rest; # if reference is set, then the groups listed will be compared only to the other groups listed; from sklearn.linear_model import LogisticRegression; if len(groups) == 1:; raise Exception('Cannot perform logistic regression on a single cluster.'); adata_copy = adata[adata.obs[groupby].isin(groups_order)] ; adata_comp = adata_copy; if adata.raw is not None and use_raw:; adata_comp = adata_copy.raw; X = adata_comp.X. clf = LogisticRegression(**kwds); clf.fit(X, adata_copy.obs[groupby].cat.codes); ```. You're right that logreg only includes the passed groups, if groups are passed. This should not be the case. I wonder why it's a problem in your specific case as I'd expect that 0, 1, 2 make up the whole data; but maybe the resolution in Louvain is somewhat set to a high value. In any case, I'll change the implementation so that irrespective of whether `groups` is passed or not, one gets the same result.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278#issuecomment-427093467
https://github.com/scverse/scanpy/issues/278#issuecomment-427339773:328,Deployability,update,update,328,"Hi @falexwolf, thanks a lot for the clarifications. This helps me a lot. In the example I provided yesterday, `louvain` found 5 clusters, so 0, 1, 2 made up only part of the data. I should have provided the output as well to make this clear right away. Concerning a PR for the documentation, I think I would wait until you will update the behaviour of `logreg`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278#issuecomment-427339773
https://github.com/scverse/scanpy/issues/278#issuecomment-427339773:353,Testability,log,logreg,353,"Hi @falexwolf, thanks a lot for the clarifications. This helps me a lot. In the example I provided yesterday, `louvain` found 5 clusters, so 0, 1, 2 made up only part of the data. I should have provided the output as well to make this clear right away. Concerning a PR for the documentation, I think I would wait until you will update the behaviour of `logreg`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278#issuecomment-427339773
https://github.com/scverse/scanpy/issues/278#issuecomment-427339773:235,Usability,clear,clear,235,"Hi @falexwolf, thanks a lot for the clarifications. This helps me a lot. In the example I provided yesterday, `louvain` found 5 clusters, so 0, 1, 2 made up only part of the data. I should have provided the output as well to make this clear right away. Concerning a PR for the documentation, I think I would wait until you will update the behaviour of `logreg`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278#issuecomment-427339773
https://github.com/scverse/scanpy/issues/279#issuecomment-426834933:399,Safety,detect,detected,399,"My hunch would be that both runs just find different local minima: one being further away from the global minimum than the other. As Louvain is just a heuristic solution to an NP-hard problem, this is not surprising. Just use a random number generator for the random seed and run it 100 times for each resolution, it would probably show the expected picture again for the average number of clusters detected. . I'm not sure if the tie breaking rule is deterministic or not. But you'd get different ties in both cases anyway I assume.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/279#issuecomment-426834933
https://github.com/scverse/scanpy/issues/279#issuecomment-426898375:43,Performance,optimiz,optimizes,43,"AFAIK this is not an issue. Louvain method optimizes global modularity but, as other methods, may miss some “true” communities. Communities in Louvain method are not intended in hierarchical way.; I suspect that what you observed applies to many scRNA data at, at least, one resolution value.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/279#issuecomment-426898375
https://github.com/scverse/scanpy/issues/279#issuecomment-426950213:279,Performance,optimiz,optimization,279,"I guess one could calculate the multi-resolution moduliarity score of both partitions with both resolutions and see if the lower number of communities is actually a more optimal modularity score than the higher number that is found. If that's the case, it's just about imperfect optimization, which is expected.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/279#issuecomment-426950213
https://github.com/scverse/scanpy/issues/280#issuecomment-426896350:421,Availability,error,error,421,"How did you installed scanpy?. Try:. conda install --file requirements.txt. this may install all the right versions of the packages that you need. On Thu, Oct 4, 2018 at 2:26 AM ar-baya <notifications@github.com> wrote:. > Hi, I am reproducing this tutorial; > https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170503_zheng17/zheng17.ipynb; >; > the line sc.pp.neighbors(adata) produces the following error:; >; > Inconsistency detected by ld.so: dl-version.c: 205:; > _dl_check_map_versions: Assertion `needed != NULL' failed!; >; > Ubuntu 18.04; > Python 3.6.6; >; > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4; > scikit-learn==0.19.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1; >; > Can you help me? Thank You; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/280>, or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1Sgm2UxCRL2y2-EGlah7YmtIrmmeks5uhVXGgaJpZM4XHKo6>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280#issuecomment-426896350
https://github.com/scverse/scanpy/issues/280#issuecomment-426896350:12,Deployability,install,installed,12,"How did you installed scanpy?. Try:. conda install --file requirements.txt. this may install all the right versions of the packages that you need. On Thu, Oct 4, 2018 at 2:26 AM ar-baya <notifications@github.com> wrote:. > Hi, I am reproducing this tutorial; > https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170503_zheng17/zheng17.ipynb; >; > the line sc.pp.neighbors(adata) produces the following error:; >; > Inconsistency detected by ld.so: dl-version.c: 205:; > _dl_check_map_versions: Assertion `needed != NULL' failed!; >; > Ubuntu 18.04; > Python 3.6.6; >; > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4; > scikit-learn==0.19.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1; >; > Can you help me? Thank You; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/280>, or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1Sgm2UxCRL2y2-EGlah7YmtIrmmeks5uhVXGgaJpZM4XHKo6>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280#issuecomment-426896350
https://github.com/scverse/scanpy/issues/280#issuecomment-426896350:43,Deployability,install,install,43,"How did you installed scanpy?. Try:. conda install --file requirements.txt. this may install all the right versions of the packages that you need. On Thu, Oct 4, 2018 at 2:26 AM ar-baya <notifications@github.com> wrote:. > Hi, I am reproducing this tutorial; > https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170503_zheng17/zheng17.ipynb; >; > the line sc.pp.neighbors(adata) produces the following error:; >; > Inconsistency detected by ld.so: dl-version.c: 205:; > _dl_check_map_versions: Assertion `needed != NULL' failed!; >; > Ubuntu 18.04; > Python 3.6.6; >; > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4; > scikit-learn==0.19.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1; >; > Can you help me? Thank You; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/280>, or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1Sgm2UxCRL2y2-EGlah7YmtIrmmeks5uhVXGgaJpZM4XHKo6>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280#issuecomment-426896350
https://github.com/scverse/scanpy/issues/280#issuecomment-426896350:85,Deployability,install,install,85,"How did you installed scanpy?. Try:. conda install --file requirements.txt. this may install all the right versions of the packages that you need. On Thu, Oct 4, 2018 at 2:26 AM ar-baya <notifications@github.com> wrote:. > Hi, I am reproducing this tutorial; > https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170503_zheng17/zheng17.ipynb; >; > the line sc.pp.neighbors(adata) produces the following error:; >; > Inconsistency detected by ld.so: dl-version.c: 205:; > _dl_check_map_versions: Assertion `needed != NULL' failed!; >; > Ubuntu 18.04; > Python 3.6.6; >; > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4; > scikit-learn==0.19.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1; >; > Can you help me? Thank You; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/280>, or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1Sgm2UxCRL2y2-EGlah7YmtIrmmeks5uhVXGgaJpZM4XHKo6>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280#issuecomment-426896350
https://github.com/scverse/scanpy/issues/280#issuecomment-426896350:448,Safety,detect,detected,448,"How did you installed scanpy?. Try:. conda install --file requirements.txt. this may install all the right versions of the packages that you need. On Thu, Oct 4, 2018 at 2:26 AM ar-baya <notifications@github.com> wrote:. > Hi, I am reproducing this tutorial; > https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170503_zheng17/zheng17.ipynb; >; > the line sc.pp.neighbors(adata) produces the following error:; >; > Inconsistency detected by ld.so: dl-version.c: 205:; > _dl_check_map_versions: Assertion `needed != NULL' failed!; >; > Ubuntu 18.04; > Python 3.6.6; >; > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4; > scikit-learn==0.19.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1; >; > Can you help me? Thank You; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/280>, or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1Sgm2UxCRL2y2-EGlah7YmtIrmmeks5uhVXGgaJpZM4XHKo6>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280#issuecomment-426896350
https://github.com/scverse/scanpy/issues/280#issuecomment-426896350:513,Testability,Assert,Assertion,513,"How did you installed scanpy?. Try:. conda install --file requirements.txt. this may install all the right versions of the packages that you need. On Thu, Oct 4, 2018 at 2:26 AM ar-baya <notifications@github.com> wrote:. > Hi, I am reproducing this tutorial; > https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170503_zheng17/zheng17.ipynb; >; > the line sc.pp.neighbors(adata) produces the following error:; >; > Inconsistency detected by ld.so: dl-version.c: 205:; > _dl_check_map_versions: Assertion `needed != NULL' failed!; >; > Ubuntu 18.04; > Python 3.6.6; >; > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4; > scikit-learn==0.19.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1; >; > Can you help me? Thank You; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/280>, or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1Sgm2UxCRL2y2-EGlah7YmtIrmmeks5uhVXGgaJpZM4XHKo6>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280#issuecomment-426896350
https://github.com/scverse/scanpy/issues/280#issuecomment-426896350:671,Usability,learn,learn,671,"How did you installed scanpy?. Try:. conda install --file requirements.txt. this may install all the right versions of the packages that you need. On Thu, Oct 4, 2018 at 2:26 AM ar-baya <notifications@github.com> wrote:. > Hi, I am reproducing this tutorial; > https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170503_zheng17/zheng17.ipynb; >; > the line sc.pp.neighbors(adata) produces the following error:; >; > Inconsistency detected by ld.so: dl-version.c: 205:; > _dl_check_map_versions: Assertion `needed != NULL' failed!; >; > Ubuntu 18.04; > Python 3.6.6; >; > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4; > scikit-learn==0.19.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1; >; > Can you help me? Thank You; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/280>, or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1Sgm2UxCRL2y2-EGlah7YmtIrmmeks5uhVXGgaJpZM4XHKo6>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280#issuecomment-426896350
https://github.com/scverse/scanpy/issues/280#issuecomment-427037414:8,Deployability,install,installed,8,"First I installed scanpy using sudo and pip as any python package. But, now I followed your advice and installed it using Bioconda, and it solved the issue. Thank you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280#issuecomment-427037414
https://github.com/scverse/scanpy/issues/280#issuecomment-427037414:103,Deployability,install,installed,103,"First I installed scanpy using sudo and pip as any python package. But, now I followed your advice and installed it using Bioconda, and it solved the issue. Thank you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280#issuecomment-427037414
https://github.com/scverse/scanpy/issues/280#issuecomment-427357518:57,Deployability,install,install,57,Same issue here. Using `pip` +pyhton3.7 and not conda to install from pypi. Is there a way to resolve it without installing using conda?. Logs:. ```; [dilawars@chamcham scanpy_exp]$ python planaria.py ; /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses; import imp; scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 ; ... storing 'clusters' as categorical; computing tSNE; using data matrix X directly; using the 'MulticoreTSNE' package by Ulyanov (2017); finished (0:02:53.98); saving figure to file ./figures/tsne_full.pdf; computing neighbors; using data matrix X directly; Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed!; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280#issuecomment-427357518
https://github.com/scverse/scanpy/issues/280#issuecomment-427357518:113,Deployability,install,installing,113,Same issue here. Using `pip` +pyhton3.7 and not conda to install from pypi. Is there a way to resolve it without installing using conda?. Logs:. ```; [dilawars@chamcham scanpy_exp]$ python planaria.py ; /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses; import imp; scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 ; ... storing 'clusters' as categorical; computing tSNE; using data matrix X directly; using the 'MulticoreTSNE' package by Ulyanov (2017); finished (0:02:53.98); saving figure to file ./figures/tsne_full.pdf; computing neighbors; using data matrix X directly; Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed!; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280#issuecomment-427357518
https://github.com/scverse/scanpy/issues/280#issuecomment-427357518:866,Safety,detect,detected,866,Same issue here. Using `pip` +pyhton3.7 and not conda to install from pypi. Is there a way to resolve it without installing using conda?. Logs:. ```; [dilawars@chamcham scanpy_exp]$ python planaria.py ; /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses; import imp; scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 ; ... storing 'clusters' as categorical; computing tSNE; using data matrix X directly; using the 'MulticoreTSNE' package by Ulyanov (2017); finished (0:02:53.98); saving figure to file ./figures/tsne_full.pdf; computing neighbors; using data matrix X directly; Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed!; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280#issuecomment-427357518
https://github.com/scverse/scanpy/issues/280#issuecomment-427357518:138,Testability,Log,Logs,138,Same issue here. Using `pip` +pyhton3.7 and not conda to install from pypi. Is there a way to resolve it without installing using conda?. Logs:. ```; [dilawars@chamcham scanpy_exp]$ python planaria.py ; /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses; import imp; scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 ; ... storing 'clusters' as categorical; computing tSNE; using data matrix X directly; using the 'MulticoreTSNE' package by Ulyanov (2017); finished (0:02:53.98); saving figure to file ./figures/tsne_full.pdf; computing neighbors; using data matrix X directly; Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed!; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280#issuecomment-427357518
https://github.com/scverse/scanpy/issues/280#issuecomment-427357518:928,Testability,Assert,Assertion,928,Same issue here. Using `pip` +pyhton3.7 and not conda to install from pypi. Is there a way to resolve it without installing using conda?. Logs:. ```; [dilawars@chamcham scanpy_exp]$ python planaria.py ; /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses; import imp; scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 ; ... storing 'clusters' as categorical; computing tSNE; using data matrix X directly; using the 'MulticoreTSNE' package by Ulyanov (2017); finished (0:02:53.98); saving figure to file ./figures/tsne_full.pdf; computing neighbors; using data matrix X directly; Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed!; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280#issuecomment-427357518
https://github.com/scverse/scanpy/issues/280#issuecomment-427357518:537,Usability,learn,learn,537,Same issue here. Using `pip` +pyhton3.7 and not conda to install from pypi. Is there a way to resolve it without installing using conda?. Logs:. ```; [dilawars@chamcham scanpy_exp]$ python planaria.py ; /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses; import imp; scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 ; ... storing 'clusters' as categorical; computing tSNE; using data matrix X directly; using the 'MulticoreTSNE' package by Ulyanov (2017); finished (0:02:53.98); saving figure to file ./figures/tsne_full.pdf; computing neighbors; using data matrix X directly; Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed!; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280#issuecomment-427357518
https://github.com/scverse/scanpy/issues/280#issuecomment-427359171:5,Availability,error,error,5,"That error is not specific to scanpy. It would be good to know which; library is causing the problem such that it can be updated but most likely; is either numpy, scipy, matplotlib or sklearn. Maybe try to update those; packages and see if the error goes away or try to google the error to find; some solution. On Fri, Oct 5, 2018 at 2:59 PM Dilawar Singh <notifications@github.com>; wrote:. > Same issue here. Using pip +pyhton3.7 and not conda to install from pypi.; > Is there a way to resolve it without installing using conda?; >; > Logs:; >; > [dilawars@chamcham scanpy_exp]$ python planaria.py; > /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses; > import imp; > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1; > ... storing 'clusters' as categorical; > computing tSNE; > using data matrix X directly; > using the 'MulticoreTSNE' package by Ulyanov (2017); > finished (0:02:53.98); > saving figure to file ./figures/tsne_full.pdf; > computing neighbors; > using data matrix X directly; > Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed!; >; > —; > You are receiving this because you commented.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/280#issuecomment-427357518>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1WzuXR5Mhpb3jNte9UkVDqzQjb1pks5uh1eZgaJpZM4XHKo6>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280#issuecomment-427359171
https://github.com/scverse/scanpy/issues/280#issuecomment-427359171:244,Availability,error,error,244,"That error is not specific to scanpy. It would be good to know which; library is causing the problem such that it can be updated but most likely; is either numpy, scipy, matplotlib or sklearn. Maybe try to update those; packages and see if the error goes away or try to google the error to find; some solution. On Fri, Oct 5, 2018 at 2:59 PM Dilawar Singh <notifications@github.com>; wrote:. > Same issue here. Using pip +pyhton3.7 and not conda to install from pypi.; > Is there a way to resolve it without installing using conda?; >; > Logs:; >; > [dilawars@chamcham scanpy_exp]$ python planaria.py; > /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses; > import imp; > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1; > ... storing 'clusters' as categorical; > computing tSNE; > using data matrix X directly; > using the 'MulticoreTSNE' package by Ulyanov (2017); > finished (0:02:53.98); > saving figure to file ./figures/tsne_full.pdf; > computing neighbors; > using data matrix X directly; > Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed!; >; > —; > You are receiving this because you commented.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/280#issuecomment-427357518>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1WzuXR5Mhpb3jNte9UkVDqzQjb1pks5uh1eZgaJpZM4XHKo6>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280#issuecomment-427359171
https://github.com/scverse/scanpy/issues/280#issuecomment-427359171:281,Availability,error,error,281,"That error is not specific to scanpy. It would be good to know which; library is causing the problem such that it can be updated but most likely; is either numpy, scipy, matplotlib or sklearn. Maybe try to update those; packages and see if the error goes away or try to google the error to find; some solution. On Fri, Oct 5, 2018 at 2:59 PM Dilawar Singh <notifications@github.com>; wrote:. > Same issue here. Using pip +pyhton3.7 and not conda to install from pypi.; > Is there a way to resolve it without installing using conda?; >; > Logs:; >; > [dilawars@chamcham scanpy_exp]$ python planaria.py; > /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses; > import imp; > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1; > ... storing 'clusters' as categorical; > computing tSNE; > using data matrix X directly; > using the 'MulticoreTSNE' package by Ulyanov (2017); > finished (0:02:53.98); > saving figure to file ./figures/tsne_full.pdf; > computing neighbors; > using data matrix X directly; > Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed!; >; > —; > You are receiving this because you commented.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/280#issuecomment-427357518>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1WzuXR5Mhpb3jNte9UkVDqzQjb1pks5uh1eZgaJpZM4XHKo6>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280#issuecomment-427359171
https://github.com/scverse/scanpy/issues/280#issuecomment-427359171:121,Deployability,update,updated,121,"That error is not specific to scanpy. It would be good to know which; library is causing the problem such that it can be updated but most likely; is either numpy, scipy, matplotlib or sklearn. Maybe try to update those; packages and see if the error goes away or try to google the error to find; some solution. On Fri, Oct 5, 2018 at 2:59 PM Dilawar Singh <notifications@github.com>; wrote:. > Same issue here. Using pip +pyhton3.7 and not conda to install from pypi.; > Is there a way to resolve it without installing using conda?; >; > Logs:; >; > [dilawars@chamcham scanpy_exp]$ python planaria.py; > /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses; > import imp; > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1; > ... storing 'clusters' as categorical; > computing tSNE; > using data matrix X directly; > using the 'MulticoreTSNE' package by Ulyanov (2017); > finished (0:02:53.98); > saving figure to file ./figures/tsne_full.pdf; > computing neighbors; > using data matrix X directly; > Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed!; >; > —; > You are receiving this because you commented.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/280#issuecomment-427357518>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1WzuXR5Mhpb3jNte9UkVDqzQjb1pks5uh1eZgaJpZM4XHKo6>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280#issuecomment-427359171
https://github.com/scverse/scanpy/issues/280#issuecomment-427359171:206,Deployability,update,update,206,"That error is not specific to scanpy. It would be good to know which; library is causing the problem such that it can be updated but most likely; is either numpy, scipy, matplotlib or sklearn. Maybe try to update those; packages and see if the error goes away or try to google the error to find; some solution. On Fri, Oct 5, 2018 at 2:59 PM Dilawar Singh <notifications@github.com>; wrote:. > Same issue here. Using pip +pyhton3.7 and not conda to install from pypi.; > Is there a way to resolve it without installing using conda?; >; > Logs:; >; > [dilawars@chamcham scanpy_exp]$ python planaria.py; > /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses; > import imp; > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1; > ... storing 'clusters' as categorical; > computing tSNE; > using data matrix X directly; > using the 'MulticoreTSNE' package by Ulyanov (2017); > finished (0:02:53.98); > saving figure to file ./figures/tsne_full.pdf; > computing neighbors; > using data matrix X directly; > Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed!; >; > —; > You are receiving this because you commented.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/280#issuecomment-427357518>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1WzuXR5Mhpb3jNte9UkVDqzQjb1pks5uh1eZgaJpZM4XHKo6>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280#issuecomment-427359171
https://github.com/scverse/scanpy/issues/280#issuecomment-427359171:449,Deployability,install,install,449,"That error is not specific to scanpy. It would be good to know which; library is causing the problem such that it can be updated but most likely; is either numpy, scipy, matplotlib or sklearn. Maybe try to update those; packages and see if the error goes away or try to google the error to find; some solution. On Fri, Oct 5, 2018 at 2:59 PM Dilawar Singh <notifications@github.com>; wrote:. > Same issue here. Using pip +pyhton3.7 and not conda to install from pypi.; > Is there a way to resolve it without installing using conda?; >; > Logs:; >; > [dilawars@chamcham scanpy_exp]$ python planaria.py; > /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses; > import imp; > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1; > ... storing 'clusters' as categorical; > computing tSNE; > using data matrix X directly; > using the 'MulticoreTSNE' package by Ulyanov (2017); > finished (0:02:53.98); > saving figure to file ./figures/tsne_full.pdf; > computing neighbors; > using data matrix X directly; > Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed!; >; > —; > You are receiving this because you commented.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/280#issuecomment-427357518>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1WzuXR5Mhpb3jNte9UkVDqzQjb1pks5uh1eZgaJpZM4XHKo6>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280#issuecomment-427359171
https://github.com/scverse/scanpy/issues/280#issuecomment-427359171:508,Deployability,install,installing,508,"That error is not specific to scanpy. It would be good to know which; library is causing the problem such that it can be updated but most likely; is either numpy, scipy, matplotlib or sklearn. Maybe try to update those; packages and see if the error goes away or try to google the error to find; some solution. On Fri, Oct 5, 2018 at 2:59 PM Dilawar Singh <notifications@github.com>; wrote:. > Same issue here. Using pip +pyhton3.7 and not conda to install from pypi.; > Is there a way to resolve it without installing using conda?; >; > Logs:; >; > [dilawars@chamcham scanpy_exp]$ python planaria.py; > /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses; > import imp; > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1; > ... storing 'clusters' as categorical; > computing tSNE; > using data matrix X directly; > using the 'MulticoreTSNE' package by Ulyanov (2017); > finished (0:02:53.98); > saving figure to file ./figures/tsne_full.pdf; > computing neighbors; > using data matrix X directly; > Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed!; >; > —; > You are receiving this because you commented.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/280#issuecomment-427357518>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1WzuXR5Mhpb3jNte9UkVDqzQjb1pks5uh1eZgaJpZM4XHKo6>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280#issuecomment-427359171
https://github.com/scverse/scanpy/issues/280#issuecomment-427359171:1288,Safety,detect,detected,1288,"That error is not specific to scanpy. It would be good to know which; library is causing the problem such that it can be updated but most likely; is either numpy, scipy, matplotlib or sklearn. Maybe try to update those; packages and see if the error goes away or try to google the error to find; some solution. On Fri, Oct 5, 2018 at 2:59 PM Dilawar Singh <notifications@github.com>; wrote:. > Same issue here. Using pip +pyhton3.7 and not conda to install from pypi.; > Is there a way to resolve it without installing using conda?; >; > Logs:; >; > [dilawars@chamcham scanpy_exp]$ python planaria.py; > /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses; > import imp; > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1; > ... storing 'clusters' as categorical; > computing tSNE; > using data matrix X directly; > using the 'MulticoreTSNE' package by Ulyanov (2017); > finished (0:02:53.98); > saving figure to file ./figures/tsne_full.pdf; > computing neighbors; > using data matrix X directly; > Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed!; >; > —; > You are receiving this because you commented.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/280#issuecomment-427357518>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1WzuXR5Mhpb3jNte9UkVDqzQjb1pks5uh1eZgaJpZM4XHKo6>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280#issuecomment-427359171
https://github.com/scverse/scanpy/issues/280#issuecomment-427359171:538,Testability,Log,Logs,538,"That error is not specific to scanpy. It would be good to know which; library is causing the problem such that it can be updated but most likely; is either numpy, scipy, matplotlib or sklearn. Maybe try to update those; packages and see if the error goes away or try to google the error to find; some solution. On Fri, Oct 5, 2018 at 2:59 PM Dilawar Singh <notifications@github.com>; wrote:. > Same issue here. Using pip +pyhton3.7 and not conda to install from pypi.; > Is there a way to resolve it without installing using conda?; >; > Logs:; >; > [dilawars@chamcham scanpy_exp]$ python planaria.py; > /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses; > import imp; > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1; > ... storing 'clusters' as categorical; > computing tSNE; > using data matrix X directly; > using the 'MulticoreTSNE' package by Ulyanov (2017); > finished (0:02:53.98); > saving figure to file ./figures/tsne_full.pdf; > computing neighbors; > using data matrix X directly; > Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed!; >; > —; > You are receiving this because you commented.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/280#issuecomment-427357518>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1WzuXR5Mhpb3jNte9UkVDqzQjb1pks5uh1eZgaJpZM4XHKo6>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280#issuecomment-427359171
https://github.com/scverse/scanpy/issues/280#issuecomment-427359171:1350,Testability,Assert,Assertion,1350,"That error is not specific to scanpy. It would be good to know which; library is causing the problem such that it can be updated but most likely; is either numpy, scipy, matplotlib or sklearn. Maybe try to update those; packages and see if the error goes away or try to google the error to find; some solution. On Fri, Oct 5, 2018 at 2:59 PM Dilawar Singh <notifications@github.com>; wrote:. > Same issue here. Using pip +pyhton3.7 and not conda to install from pypi.; > Is there a way to resolve it without installing using conda?; >; > Logs:; >; > [dilawars@chamcham scanpy_exp]$ python planaria.py; > /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses; > import imp; > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1; > ... storing 'clusters' as categorical; > computing tSNE; > using data matrix X directly; > using the 'MulticoreTSNE' package by Ulyanov (2017); > finished (0:02:53.98); > saving figure to file ./figures/tsne_full.pdf; > computing neighbors; > using data matrix X directly; > Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed!; >; > —; > You are receiving this because you commented.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/280#issuecomment-427357518>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1WzuXR5Mhpb3jNte9UkVDqzQjb1pks5uh1eZgaJpZM4XHKo6>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280#issuecomment-427359171
https://github.com/scverse/scanpy/issues/280#issuecomment-427359171:942,Usability,learn,learn,942,"That error is not specific to scanpy. It would be good to know which; library is causing the problem such that it can be updated but most likely; is either numpy, scipy, matplotlib or sklearn. Maybe try to update those; packages and see if the error goes away or try to google the error to find; some solution. On Fri, Oct 5, 2018 at 2:59 PM Dilawar Singh <notifications@github.com>; wrote:. > Same issue here. Using pip +pyhton3.7 and not conda to install from pypi.; > Is there a way to resolve it without installing using conda?; >; > Logs:; >; > [dilawars@chamcham scanpy_exp]$ python planaria.py; > /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses; > import imp; > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1; > ... storing 'clusters' as categorical; > computing tSNE; > using data matrix X directly; > using the 'MulticoreTSNE' package by Ulyanov (2017); > finished (0:02:53.98); > saving figure to file ./figures/tsne_full.pdf; > computing neighbors; > using data matrix X directly; > Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed!; >; > —; > You are receiving this because you commented.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/280#issuecomment-427357518>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1WzuXR5Mhpb3jNte9UkVDqzQjb1pks5uh1eZgaJpZM4XHKo6>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280#issuecomment-427359171
https://github.com/scverse/scanpy/issues/280#issuecomment-427364460:182,Availability,error,error,182,"Its an issue with numba. See here https://github.com/jmschrei/apricot/blob/98693788ca315ceceeb2eb0f4ce8526f40e0049b/README.md. *Update* Quoting from the REAME above. > If you get an error that looks like; > ; > Inconsistency detected by ld.so: dl-version.c: 224: _dl_check_map_versions: Assertion `needed != NULL' failed!; > ; > or a segmentation fault when importing apricot for the first time then you should try reinstalling numba through conda using; > ; > conda install numba.; > ; > or; > ; > pip install numba==0.39.0; > ; > The issue appears to be with the most recent verson of numba, v0.40.0. Downgrading to numba v0.39.0 should solve the issue. ; > . That means numba should be frozen to v0.39.0 .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280#issuecomment-427364460
https://github.com/scverse/scanpy/issues/280#issuecomment-427364460:347,Availability,fault,fault,347,"Its an issue with numba. See here https://github.com/jmschrei/apricot/blob/98693788ca315ceceeb2eb0f4ce8526f40e0049b/README.md. *Update* Quoting from the REAME above. > If you get an error that looks like; > ; > Inconsistency detected by ld.so: dl-version.c: 224: _dl_check_map_versions: Assertion `needed != NULL' failed!; > ; > or a segmentation fault when importing apricot for the first time then you should try reinstalling numba through conda using; > ; > conda install numba.; > ; > or; > ; > pip install numba==0.39.0; > ; > The issue appears to be with the most recent verson of numba, v0.40.0. Downgrading to numba v0.39.0 should solve the issue. ; > . That means numba should be frozen to v0.39.0 .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280#issuecomment-427364460
https://github.com/scverse/scanpy/issues/280#issuecomment-427364460:603,Availability,Down,Downgrading,603,"Its an issue with numba. See here https://github.com/jmschrei/apricot/blob/98693788ca315ceceeb2eb0f4ce8526f40e0049b/README.md. *Update* Quoting from the REAME above. > If you get an error that looks like; > ; > Inconsistency detected by ld.so: dl-version.c: 224: _dl_check_map_versions: Assertion `needed != NULL' failed!; > ; > or a segmentation fault when importing apricot for the first time then you should try reinstalling numba through conda using; > ; > conda install numba.; > ; > or; > ; > pip install numba==0.39.0; > ; > The issue appears to be with the most recent verson of numba, v0.40.0. Downgrading to numba v0.39.0 should solve the issue. ; > . That means numba should be frozen to v0.39.0 .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280#issuecomment-427364460
https://github.com/scverse/scanpy/issues/280#issuecomment-427364460:128,Deployability,Update,Update,128,"Its an issue with numba. See here https://github.com/jmschrei/apricot/blob/98693788ca315ceceeb2eb0f4ce8526f40e0049b/README.md. *Update* Quoting from the REAME above. > If you get an error that looks like; > ; > Inconsistency detected by ld.so: dl-version.c: 224: _dl_check_map_versions: Assertion `needed != NULL' failed!; > ; > or a segmentation fault when importing apricot for the first time then you should try reinstalling numba through conda using; > ; > conda install numba.; > ; > or; > ; > pip install numba==0.39.0; > ; > The issue appears to be with the most recent verson of numba, v0.40.0. Downgrading to numba v0.39.0 should solve the issue. ; > . That means numba should be frozen to v0.39.0 .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280#issuecomment-427364460
https://github.com/scverse/scanpy/issues/280#issuecomment-427364460:467,Deployability,install,install,467,"Its an issue with numba. See here https://github.com/jmschrei/apricot/blob/98693788ca315ceceeb2eb0f4ce8526f40e0049b/README.md. *Update* Quoting from the REAME above. > If you get an error that looks like; > ; > Inconsistency detected by ld.so: dl-version.c: 224: _dl_check_map_versions: Assertion `needed != NULL' failed!; > ; > or a segmentation fault when importing apricot for the first time then you should try reinstalling numba through conda using; > ; > conda install numba.; > ; > or; > ; > pip install numba==0.39.0; > ; > The issue appears to be with the most recent verson of numba, v0.40.0. Downgrading to numba v0.39.0 should solve the issue. ; > . That means numba should be frozen to v0.39.0 .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280#issuecomment-427364460
https://github.com/scverse/scanpy/issues/280#issuecomment-427364460:503,Deployability,install,install,503,"Its an issue with numba. See here https://github.com/jmschrei/apricot/blob/98693788ca315ceceeb2eb0f4ce8526f40e0049b/README.md. *Update* Quoting from the REAME above. > If you get an error that looks like; > ; > Inconsistency detected by ld.so: dl-version.c: 224: _dl_check_map_versions: Assertion `needed != NULL' failed!; > ; > or a segmentation fault when importing apricot for the first time then you should try reinstalling numba through conda using; > ; > conda install numba.; > ; > or; > ; > pip install numba==0.39.0; > ; > The issue appears to be with the most recent verson of numba, v0.40.0. Downgrading to numba v0.39.0 should solve the issue. ; > . That means numba should be frozen to v0.39.0 .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280#issuecomment-427364460
https://github.com/scverse/scanpy/issues/280#issuecomment-427364460:225,Safety,detect,detected,225,"Its an issue with numba. See here https://github.com/jmschrei/apricot/blob/98693788ca315ceceeb2eb0f4ce8526f40e0049b/README.md. *Update* Quoting from the REAME above. > If you get an error that looks like; > ; > Inconsistency detected by ld.so: dl-version.c: 224: _dl_check_map_versions: Assertion `needed != NULL' failed!; > ; > or a segmentation fault when importing apricot for the first time then you should try reinstalling numba through conda using; > ; > conda install numba.; > ; > or; > ; > pip install numba==0.39.0; > ; > The issue appears to be with the most recent verson of numba, v0.40.0. Downgrading to numba v0.39.0 should solve the issue. ; > . That means numba should be frozen to v0.39.0 .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280#issuecomment-427364460
https://github.com/scverse/scanpy/issues/280#issuecomment-427364460:287,Testability,Assert,Assertion,287,"Its an issue with numba. See here https://github.com/jmschrei/apricot/blob/98693788ca315ceceeb2eb0f4ce8526f40e0049b/README.md. *Update* Quoting from the REAME above. > If you get an error that looks like; > ; > Inconsistency detected by ld.so: dl-version.c: 224: _dl_check_map_versions: Assertion `needed != NULL' failed!; > ; > or a segmentation fault when importing apricot for the first time then you should try reinstalling numba through conda using; > ; > conda install numba.; > ; > or; > ; > pip install numba==0.39.0; > ; > The issue appears to be with the most recent verson of numba, v0.40.0. Downgrading to numba v0.39.0 should solve the issue. ; > . That means numba should be frozen to v0.39.0 .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280#issuecomment-427364460
https://github.com/scverse/scanpy/issues/281#issuecomment-427044578:59,Integrability,interface,interface,59,"Cool!. Initially, Scanpy came with a built-in command-line interface. But I threw it out after a couple of months as it was hard to maintain both the interface and the library at the same time, I also expected a bit too much from the command-line interface at the time, I guess. I'd really like to support your much better efforts for generating command line interfaces. But I'm a bit hesitant to maintain it here in the main repo - already the library is already so much to take care of and other libraries typically don't involve a ""script-wrapper"", too. What about a PyPI package `scanpy-scripts`, which directly advertise from the Scanpy main docs, we could also have a little tutorial there. Similar to that, there will be interactive visual interfaces to scanpy, again as separate packages. I tend to think that this would be the better option. But if I miss something fundamental, please convince me of the opposite.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281#issuecomment-427044578
https://github.com/scverse/scanpy/issues/281#issuecomment-427044578:150,Integrability,interface,interface,150,"Cool!. Initially, Scanpy came with a built-in command-line interface. But I threw it out after a couple of months as it was hard to maintain both the interface and the library at the same time, I also expected a bit too much from the command-line interface at the time, I guess. I'd really like to support your much better efforts for generating command line interfaces. But I'm a bit hesitant to maintain it here in the main repo - already the library is already so much to take care of and other libraries typically don't involve a ""script-wrapper"", too. What about a PyPI package `scanpy-scripts`, which directly advertise from the Scanpy main docs, we could also have a little tutorial there. Similar to that, there will be interactive visual interfaces to scanpy, again as separate packages. I tend to think that this would be the better option. But if I miss something fundamental, please convince me of the opposite.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281#issuecomment-427044578
https://github.com/scverse/scanpy/issues/281#issuecomment-427044578:247,Integrability,interface,interface,247,"Cool!. Initially, Scanpy came with a built-in command-line interface. But I threw it out after a couple of months as it was hard to maintain both the interface and the library at the same time, I also expected a bit too much from the command-line interface at the time, I guess. I'd really like to support your much better efforts for generating command line interfaces. But I'm a bit hesitant to maintain it here in the main repo - already the library is already so much to take care of and other libraries typically don't involve a ""script-wrapper"", too. What about a PyPI package `scanpy-scripts`, which directly advertise from the Scanpy main docs, we could also have a little tutorial there. Similar to that, there will be interactive visual interfaces to scanpy, again as separate packages. I tend to think that this would be the better option. But if I miss something fundamental, please convince me of the opposite.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281#issuecomment-427044578
https://github.com/scverse/scanpy/issues/281#issuecomment-427044578:359,Integrability,interface,interfaces,359,"Cool!. Initially, Scanpy came with a built-in command-line interface. But I threw it out after a couple of months as it was hard to maintain both the interface and the library at the same time, I also expected a bit too much from the command-line interface at the time, I guess. I'd really like to support your much better efforts for generating command line interfaces. But I'm a bit hesitant to maintain it here in the main repo - already the library is already so much to take care of and other libraries typically don't involve a ""script-wrapper"", too. What about a PyPI package `scanpy-scripts`, which directly advertise from the Scanpy main docs, we could also have a little tutorial there. Similar to that, there will be interactive visual interfaces to scanpy, again as separate packages. I tend to think that this would be the better option. But if I miss something fundamental, please convince me of the opposite.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281#issuecomment-427044578
https://github.com/scverse/scanpy/issues/281#issuecomment-427044578:542,Integrability,wrap,wrapper,542,"Cool!. Initially, Scanpy came with a built-in command-line interface. But I threw it out after a couple of months as it was hard to maintain both the interface and the library at the same time, I also expected a bit too much from the command-line interface at the time, I guess. I'd really like to support your much better efforts for generating command line interfaces. But I'm a bit hesitant to maintain it here in the main repo - already the library is already so much to take care of and other libraries typically don't involve a ""script-wrapper"", too. What about a PyPI package `scanpy-scripts`, which directly advertise from the Scanpy main docs, we could also have a little tutorial there. Similar to that, there will be interactive visual interfaces to scanpy, again as separate packages. I tend to think that this would be the better option. But if I miss something fundamental, please convince me of the opposite.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281#issuecomment-427044578
https://github.com/scverse/scanpy/issues/281#issuecomment-427044578:747,Integrability,interface,interfaces,747,"Cool!. Initially, Scanpy came with a built-in command-line interface. But I threw it out after a couple of months as it was hard to maintain both the interface and the library at the same time, I also expected a bit too much from the command-line interface at the time, I guess. I'd really like to support your much better efforts for generating command line interfaces. But I'm a bit hesitant to maintain it here in the main repo - already the library is already so much to take care of and other libraries typically don't involve a ""script-wrapper"", too. What about a PyPI package `scanpy-scripts`, which directly advertise from the Scanpy main docs, we could also have a little tutorial there. Similar to that, there will be interactive visual interfaces to scanpy, again as separate packages. I tend to think that this would be the better option. But if I miss something fundamental, please convince me of the opposite.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281#issuecomment-427044578
https://github.com/scverse/scanpy/issues/281#issuecomment-431712261:199,Deployability,install,installable,199,"Hi @freeman-lab, we have now quite a number of modules:. https://github.com/ebi-gene-expression-group/scanpy-scripts. And, as our other seurat-scripts, sc3-scripts and scater-scripts, it is bioconda installable (or in the way to be). We would be happy to accept your module, although it would be good to see how much it overlaps or not with existing parts already there, to find the best way to integrate it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281#issuecomment-431712261
https://github.com/scverse/scanpy/issues/281#issuecomment-431712261:395,Deployability,integrat,integrate,395,"Hi @freeman-lab, we have now quite a number of modules:. https://github.com/ebi-gene-expression-group/scanpy-scripts. And, as our other seurat-scripts, sc3-scripts and scater-scripts, it is bioconda installable (or in the way to be). We would be happy to accept your module, although it would be good to see how much it overlaps or not with existing parts already there, to find the best way to integrate it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281#issuecomment-431712261
https://github.com/scverse/scanpy/issues/281#issuecomment-431712261:395,Integrability,integrat,integrate,395,"Hi @freeman-lab, we have now quite a number of modules:. https://github.com/ebi-gene-expression-group/scanpy-scripts. And, as our other seurat-scripts, sc3-scripts and scater-scripts, it is bioconda installable (or in the way to be). We would be happy to accept your module, although it would be good to see how much it overlaps or not with existing parts already there, to find the best way to integrate it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281#issuecomment-431712261
https://github.com/scverse/scanpy/issues/281#issuecomment-436977469:180,Deployability,install,installs,180,"I have an idea. We provide a base `scanpy` command here. It works the same as the `jupyter` binary, i.e. it searches for commands named `scanpy-something` in the `$PATH`. Once one installs `scanpy-scripts`, there will be many commands like `scanpy-filter-genes` (without `.py`, I can assist in making that happen via entry-points and publishing it to PyPI). Calling `scanpy` without arguments will then list all those commands, and `scanpy filter-genes` (note the space) will call the respective command.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281#issuecomment-436977469
https://github.com/scverse/scanpy/issues/281#issuecomment-437031478:437,Availability,mainten,maintenance,437,"Sounds good, we are currently hosting the whole scanpy-scripts as a bioconda package, but we could look into having it as pip installable as well. Does this means that scanpy administrators are happy to have the scanpy-scripts code poured here to make them pip installable, or that you want to contribute the pip packaging to the repo where we currently have scanpy-scripts? We have travis testing for our scripts layer, that would make maintenance easier on your side, as it would detect any changes on scanpy that break the scripts layers (taking you to only break API if extremely necessary - in turn making your tool more stable for external users).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281#issuecomment-437031478
https://github.com/scverse/scanpy/issues/281#issuecomment-437031478:126,Deployability,install,installable,126,"Sounds good, we are currently hosting the whole scanpy-scripts as a bioconda package, but we could look into having it as pip installable as well. Does this means that scanpy administrators are happy to have the scanpy-scripts code poured here to make them pip installable, or that you want to contribute the pip packaging to the repo where we currently have scanpy-scripts? We have travis testing for our scripts layer, that would make maintenance easier on your side, as it would detect any changes on scanpy that break the scripts layers (taking you to only break API if extremely necessary - in turn making your tool more stable for external users).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281#issuecomment-437031478
https://github.com/scverse/scanpy/issues/281#issuecomment-437031478:261,Deployability,install,installable,261,"Sounds good, we are currently hosting the whole scanpy-scripts as a bioconda package, but we could look into having it as pip installable as well. Does this means that scanpy administrators are happy to have the scanpy-scripts code poured here to make them pip installable, or that you want to contribute the pip packaging to the repo where we currently have scanpy-scripts? We have travis testing for our scripts layer, that would make maintenance easier on your side, as it would detect any changes on scanpy that break the scripts layers (taking you to only break API if extremely necessary - in turn making your tool more stable for external users).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281#issuecomment-437031478
https://github.com/scverse/scanpy/issues/281#issuecomment-437031478:534,Modifiability,layers,layers,534,"Sounds good, we are currently hosting the whole scanpy-scripts as a bioconda package, but we could look into having it as pip installable as well. Does this means that scanpy administrators are happy to have the scanpy-scripts code poured here to make them pip installable, or that you want to contribute the pip packaging to the repo where we currently have scanpy-scripts? We have travis testing for our scripts layer, that would make maintenance easier on your side, as it would detect any changes on scanpy that break the scripts layers (taking you to only break API if extremely necessary - in turn making your tool more stable for external users).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281#issuecomment-437031478
https://github.com/scverse/scanpy/issues/281#issuecomment-437031478:482,Safety,detect,detect,482,"Sounds good, we are currently hosting the whole scanpy-scripts as a bioconda package, but we could look into having it as pip installable as well. Does this means that scanpy administrators are happy to have the scanpy-scripts code poured here to make them pip installable, or that you want to contribute the pip packaging to the repo where we currently have scanpy-scripts? We have travis testing for our scripts layer, that would make maintenance easier on your side, as it would detect any changes on scanpy that break the scripts layers (taking you to only break API if extremely necessary - in turn making your tool more stable for external users).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281#issuecomment-437031478
https://github.com/scverse/scanpy/issues/281#issuecomment-437031478:390,Testability,test,testing,390,"Sounds good, we are currently hosting the whole scanpy-scripts as a bioconda package, but we could look into having it as pip installable as well. Does this means that scanpy administrators are happy to have the scanpy-scripts code poured here to make them pip installable, or that you want to contribute the pip packaging to the repo where we currently have scanpy-scripts? We have travis testing for our scripts layer, that would make maintenance easier on your side, as it would detect any changes on scanpy that break the scripts layers (taking you to only break API if extremely necessary - in turn making your tool more stable for external users).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281#issuecomment-437031478
https://github.com/scverse/scanpy/issues/281#issuecomment-437046217:159,Deployability,install,installation,159,"Sure, by all means, open a PR at https://github.com/ebi-gene-expression-group/scanpy-scripts with the directory reformatting and needed metadata files for pip installation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281#issuecomment-437046217
https://github.com/scverse/scanpy/issues/281#issuecomment-437729436:261,Availability,mainten,maintenance,261,@flying-sheep Thanks for fielding all this! You never wrote what thought about having the CLI layer in the scanpy repo... my main reason is that I simply think that I cannot maintain a layer that I'm not actively using (at least right now) and that the library maintenance and development is already quite some work...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281#issuecomment-437729436
https://github.com/scverse/scanpy/issues/281#issuecomment-437729436:147,Usability,simpl,simply,147,@flying-sheep Thanks for fielding all this! You never wrote what thought about having the CLI layer in the scanpy repo... my main reason is that I simply think that I cannot maintain a layer that I'm not actively using (at least right now) and that the library maintenance and development is already quite some work...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281#issuecomment-437729436
https://github.com/scverse/scanpy/issues/281#issuecomment-437812095:119,Integrability,wrap,wrapper,119,"Exactly, this is why I think it’s a great solution to advertise that repo in the scanpy docs and make scanpy provide a wrapper binary. I added ebi-gene-expression-group/scanpy-scripts#24, let me hear what you think",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281#issuecomment-437812095
https://github.com/scverse/scanpy/issues/281#issuecomment-484475390:32,Deployability,update,updates,32,"Hi @flying-sheep, just saw your updates here. I was slowly working on a re-structured scanpy-scripts that has a sub-command interface i.e `scanpy-cli read`, `scanpy-cli filter` etc and with some added functionality for convenience (https://github.com/ebi-gene-expression-group/scanpy-scripts/pull/40). I'll try change the interface back to make it compatible with yours.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281#issuecomment-484475390
https://github.com/scverse/scanpy/issues/281#issuecomment-484475390:124,Integrability,interface,interface,124,"Hi @flying-sheep, just saw your updates here. I was slowly working on a re-structured scanpy-scripts that has a sub-command interface i.e `scanpy-cli read`, `scanpy-cli filter` etc and with some added functionality for convenience (https://github.com/ebi-gene-expression-group/scanpy-scripts/pull/40). I'll try change the interface back to make it compatible with yours.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281#issuecomment-484475390
https://github.com/scverse/scanpy/issues/281#issuecomment-484475390:322,Integrability,interface,interface,322,"Hi @flying-sheep, just saw your updates here. I was slowly working on a re-structured scanpy-scripts that has a sub-command interface i.e `scanpy-cli read`, `scanpy-cli filter` etc and with some added functionality for convenience (https://github.com/ebi-gene-expression-group/scanpy-scripts/pull/40). I'll try change the interface back to make it compatible with yours.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281#issuecomment-484475390
https://github.com/scverse/scanpy/issues/281#issuecomment-484503926:5,Testability,test,tested,5,"Just tested, works perfectly with the main command. Brilliant job @flying-sheep!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281#issuecomment-484503926
https://github.com/scverse/scanpy/pull/284#issuecomment-427676209:114,Availability,down,downstream,114,"This is great, thanks! Now just for the neighbours `use_hvgs=` parameter and then we're sorted for using HVGs for downstream analysis without filtering the whole dataset.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/284#issuecomment-427676209
https://github.com/scverse/scanpy/pull/284#issuecomment-428458237:29,Usability,simpl,simple,29,@VolkerBergen can you type a simple example on how to use this new functionality. I think that I want to use this.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/284#issuecomment-428458237
https://github.com/scverse/scanpy/pull/284#issuecomment-428513659:7,Modifiability,variab,variable,7,"Highly variable genes (hvg) can now be used without removing the non-hvg from your data. That's simply `sc.pp.filter_genes_dispersion(adata, subset=False, **params)`, which then does not do the actual filtering but just stores the result in `.var['highly_variable']`. . `sc.pp.pca(adata, **params)` is then performed on the those hvg per default. As all other operations such as neighbors, embeddings etc. are usually performed on PCA space, they implicitly use hvg as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/284#issuecomment-428513659
https://github.com/scverse/scanpy/pull/284#issuecomment-428513659:307,Performance,perform,performed,307,"Highly variable genes (hvg) can now be used without removing the non-hvg from your data. That's simply `sc.pp.filter_genes_dispersion(adata, subset=False, **params)`, which then does not do the actual filtering but just stores the result in `.var['highly_variable']`. . `sc.pp.pca(adata, **params)` is then performed on the those hvg per default. As all other operations such as neighbors, embeddings etc. are usually performed on PCA space, they implicitly use hvg as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/284#issuecomment-428513659
https://github.com/scverse/scanpy/pull/284#issuecomment-428513659:418,Performance,perform,performed,418,"Highly variable genes (hvg) can now be used without removing the non-hvg from your data. That's simply `sc.pp.filter_genes_dispersion(adata, subset=False, **params)`, which then does not do the actual filtering but just stores the result in `.var['highly_variable']`. . `sc.pp.pca(adata, **params)` is then performed on the those hvg per default. As all other operations such as neighbors, embeddings etc. are usually performed on PCA space, they implicitly use hvg as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/284#issuecomment-428513659
https://github.com/scverse/scanpy/pull/284#issuecomment-428513659:96,Usability,simpl,simply,96,"Highly variable genes (hvg) can now be used without removing the non-hvg from your data. That's simply `sc.pp.filter_genes_dispersion(adata, subset=False, **params)`, which then does not do the actual filtering but just stores the result in `.var['highly_variable']`. . `sc.pp.pca(adata, **params)` is then performed on the those hvg per default. As all other operations such as neighbors, embeddings etc. are usually performed on PCA space, they implicitly use hvg as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/284#issuecomment-428513659
https://github.com/scverse/scanpy/pull/284#issuecomment-432836664:211,Modifiability,variab,variable,211,"I just tested this out... The idea was that if `sc.pp.pca()` has the parameter `use_highly_variable` that be extension `sc.tl.umap()`, `sc.tl.tsne()`, and `sc.tl.draw_graph()` would also be based only on highly variable genes. That however doesn't seem to be the case. When I subset my anndata object to only highly variable genes I get a different result than when I just run it with `sc.pp.pca(adata, use_highly_variable=True)`. The `sc.pl.pca()` is the same, but `sc.pl.diffmap` seems somehow inverted, and umap, tsne, and draw_graph are all slightly different.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/284#issuecomment-432836664
https://github.com/scverse/scanpy/pull/284#issuecomment-432836664:316,Modifiability,variab,variable,316,"I just tested this out... The idea was that if `sc.pp.pca()` has the parameter `use_highly_variable` that be extension `sc.tl.umap()`, `sc.tl.tsne()`, and `sc.tl.draw_graph()` would also be based only on highly variable genes. That however doesn't seem to be the case. When I subset my anndata object to only highly variable genes I get a different result than when I just run it with `sc.pp.pca(adata, use_highly_variable=True)`. The `sc.pl.pca()` is the same, but `sc.pl.diffmap` seems somehow inverted, and umap, tsne, and draw_graph are all slightly different.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/284#issuecomment-432836664
https://github.com/scverse/scanpy/pull/284#issuecomment-432836664:7,Testability,test,tested,7,"I just tested this out... The idea was that if `sc.pp.pca()` has the parameter `use_highly_variable` that be extension `sc.tl.umap()`, `sc.tl.tsne()`, and `sc.tl.draw_graph()` would also be based only on highly variable genes. That however doesn't seem to be the case. When I subset my anndata object to only highly variable genes I get a different result than when I just run it with `sc.pp.pca(adata, use_highly_variable=True)`. The `sc.pl.pca()` is the same, but `sc.pl.diffmap` seems somehow inverted, and umap, tsne, and draw_graph are all slightly different.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/284#issuecomment-432836664
https://github.com/scverse/scanpy/issues/286#issuecomment-427822048:52,Availability,error,error,52,I am attaching reduced files. I could reproduce the error with this dataset. Color names are `colors_dataset.txt` file. Note that python script is renamed to `.py.txt` . There was an error in `paga` related plotting function as well. . [R_pca_seurat.txt](https://github.com/theislab/scanpy/files/2455948/R_pca_seurat.txt); [R_annotation.txt](https://github.com/theislab/scanpy/files/2455949/R_annotation.txt); [colors_dataset.txt](https://github.com/theislab/scanpy/files/2455950/colors_dataset.txt); [planaria.py.txt](https://github.com/theislab/scanpy/files/2455953/planaria.py.txt),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286#issuecomment-427822048
https://github.com/scverse/scanpy/issues/286#issuecomment-427822048:183,Availability,error,error,183,I am attaching reduced files. I could reproduce the error with this dataset. Color names are `colors_dataset.txt` file. Note that python script is renamed to `.py.txt` . There was an error in `paga` related plotting function as well. . [R_pca_seurat.txt](https://github.com/theislab/scanpy/files/2455948/R_pca_seurat.txt); [R_annotation.txt](https://github.com/theislab/scanpy/files/2455949/R_annotation.txt); [colors_dataset.txt](https://github.com/theislab/scanpy/files/2455950/colors_dataset.txt); [planaria.py.txt](https://github.com/theislab/scanpy/files/2455953/planaria.py.txt),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286#issuecomment-427822048
https://github.com/scverse/scanpy/issues/286#issuecomment-427822048:15,Energy Efficiency,reduce,reduced,15,I am attaching reduced files. I could reproduce the error with this dataset. Color names are `colors_dataset.txt` file. Note that python script is renamed to `.py.txt` . There was an error in `paga` related plotting function as well. . [R_pca_seurat.txt](https://github.com/theislab/scanpy/files/2455948/R_pca_seurat.txt); [R_annotation.txt](https://github.com/theislab/scanpy/files/2455949/R_annotation.txt); [colors_dataset.txt](https://github.com/theislab/scanpy/files/2455950/colors_dataset.txt); [planaria.py.txt](https://github.com/theislab/scanpy/files/2455953/planaria.py.txt),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286#issuecomment-427822048
https://github.com/scverse/scanpy/issues/286#issuecomment-427909754:319,Deployability,integrat,integrate,319,"@fidelram It might be that the new plotting backend doesn't support the ""additional colors"" ([here](https://github.com/theislab/scanpy/blob/7c9fb1a5f2293956adda0673d47e7dec1b32ddfb/scanpy/plotting/utils.py#L166-L186)) anymore. These are colors that are standard in R and used for the Planaria example. We should try to integrate them for the sake of easily moving between python and R.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286#issuecomment-427909754
https://github.com/scverse/scanpy/issues/286#issuecomment-427909754:319,Integrability,integrat,integrate,319,"@fidelram It might be that the new plotting backend doesn't support the ""additional colors"" ([here](https://github.com/theislab/scanpy/blob/7c9fb1a5f2293956adda0673d47e7dec1b32ddfb/scanpy/plotting/utils.py#L166-L186)) anymore. These are colors that are standard in R and used for the Planaria example. We should try to integrate them for the sake of easily moving between python and R.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286#issuecomment-427909754
https://github.com/scverse/scanpy/issues/286#issuecomment-429198145:121,Deployability,install,install,121,"No. There is still some issue with colors. Note that now I am on python3.7 (which is default on ArchLinux). . ```; $ pip install git+https://github.com/theislab/scanpy --upgrade --user; $ python planaria.py ; /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses; import imp; scanpy==1.3.2+19.g94c3dc5 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 ; ... storing 'clusters' as categorical; computing tSNE; using data matrix X directly; using the 'MulticoreTSNE' package by Ulyanov (2017); finished (0:01:09.28); Traceback (most recent call last):; File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 166, in to_rgba; rgba = _colors_full_map.cache[c, alpha]; KeyError: ('mediumpurple3', None). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/lib/python3.7/site-packages/matplotlib/axes/_axes.py"", line 4288, in scatter; colors = mcolors.to_rgba_array(c); File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 267, in to_rgba_array; result[i] = to_rgba(cc, alpha); File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 168, in to_rgba; rgba = _to_rgba_no_colorcycle(c, alpha); File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 212, in _to_rgba_no_colorcycle; raise ValueError(""Invalid RGBA argument: {!r}"".format(orig_c)); ValueError: Invalid RGBA argument: 'mediumpurple3'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""planaria.py"", line 47, in <module>; sc.pl.tsne(adata, color='clusters', legend_loc='on data', legend_fontsize=5, save='_full'); File ""/home1/dilawars/.local/lib/python3.7/site-packages/scanpy/plotting/tools/scatterplots.py"", line 4",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286#issuecomment-429198145
https://github.com/scverse/scanpy/issues/286#issuecomment-429198145:170,Deployability,upgrade,upgrade,170,"No. There is still some issue with colors. Note that now I am on python3.7 (which is default on ArchLinux). . ```; $ pip install git+https://github.com/theislab/scanpy --upgrade --user; $ python planaria.py ; /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses; import imp; scanpy==1.3.2+19.g94c3dc5 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 ; ... storing 'clusters' as categorical; computing tSNE; using data matrix X directly; using the 'MulticoreTSNE' package by Ulyanov (2017); finished (0:01:09.28); Traceback (most recent call last):; File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 166, in to_rgba; rgba = _colors_full_map.cache[c, alpha]; KeyError: ('mediumpurple3', None). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/lib/python3.7/site-packages/matplotlib/axes/_axes.py"", line 4288, in scatter; colors = mcolors.to_rgba_array(c); File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 267, in to_rgba_array; result[i] = to_rgba(cc, alpha); File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 168, in to_rgba; rgba = _to_rgba_no_colorcycle(c, alpha); File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 212, in _to_rgba_no_colorcycle; raise ValueError(""Invalid RGBA argument: {!r}"".format(orig_c)); ValueError: Invalid RGBA argument: 'mediumpurple3'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""planaria.py"", line 47, in <module>; sc.pl.tsne(adata, color='clusters', legend_loc='on data', legend_fontsize=5, save='_full'); File ""/home1/dilawars/.local/lib/python3.7/site-packages/scanpy/plotting/tools/scatterplots.py"", line 4",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286#issuecomment-429198145
https://github.com/scverse/scanpy/issues/286#issuecomment-429198145:916,Performance,cache,cache,916,"No. There is still some issue with colors. Note that now I am on python3.7 (which is default on ArchLinux). . ```; $ pip install git+https://github.com/theislab/scanpy --upgrade --user; $ python planaria.py ; /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses; import imp; scanpy==1.3.2+19.g94c3dc5 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 ; ... storing 'clusters' as categorical; computing tSNE; using data matrix X directly; using the 'MulticoreTSNE' package by Ulyanov (2017); finished (0:01:09.28); Traceback (most recent call last):; File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 166, in to_rgba; rgba = _colors_full_map.cache[c, alpha]; KeyError: ('mediumpurple3', None). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/lib/python3.7/site-packages/matplotlib/axes/_axes.py"", line 4288, in scatter; colors = mcolors.to_rgba_array(c); File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 267, in to_rgba_array; result[i] = to_rgba(cc, alpha); File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 168, in to_rgba; rgba = _to_rgba_no_colorcycle(c, alpha); File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 212, in _to_rgba_no_colorcycle; raise ValueError(""Invalid RGBA argument: {!r}"".format(orig_c)); ValueError: Invalid RGBA argument: 'mediumpurple3'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""planaria.py"", line 47, in <module>; sc.pl.tsne(adata, color='clusters', legend_loc='on data', legend_fontsize=5, save='_full'); File ""/home1/dilawars/.local/lib/python3.7/site-packages/scanpy/plotting/tools/scatterplots.py"", line 4",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286#issuecomment-429198145
https://github.com/scverse/scanpy/issues/286#issuecomment-429198145:555,Usability,learn,learn,555,"No. There is still some issue with colors. Note that now I am on python3.7 (which is default on ArchLinux). . ```; $ pip install git+https://github.com/theislab/scanpy --upgrade --user; $ python planaria.py ; /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses; import imp; scanpy==1.3.2+19.g94c3dc5 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 ; ... storing 'clusters' as categorical; computing tSNE; using data matrix X directly; using the 'MulticoreTSNE' package by Ulyanov (2017); finished (0:01:09.28); Traceback (most recent call last):; File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 166, in to_rgba; rgba = _colors_full_map.cache[c, alpha]; KeyError: ('mediumpurple3', None). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/lib/python3.7/site-packages/matplotlib/axes/_axes.py"", line 4288, in scatter; colors = mcolors.to_rgba_array(c); File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 267, in to_rgba_array; result[i] = to_rgba(cc, alpha); File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 168, in to_rgba; rgba = _to_rgba_no_colorcycle(c, alpha); File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 212, in _to_rgba_no_colorcycle; raise ValueError(""Invalid RGBA argument: {!r}"".format(orig_c)); ValueError: Invalid RGBA argument: 'mediumpurple3'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""planaria.py"", line 47, in <module>; sc.pl.tsne(adata, color='clusters', legend_loc='on data', legend_fontsize=5, save='_full'); File ""/home1/dilawars/.local/lib/python3.7/site-packages/scanpy/plotting/tools/scatterplots.py"", line 4",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286#issuecomment-429198145
https://github.com/scverse/scanpy/issues/286#issuecomment-429728146:59,Testability,test,test,59,Can you type the command you are using? Or better set up a test case. See the example on #293 maybe you can reproduce your problem with that set up.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286#issuecomment-429728146
https://github.com/scverse/scanpy/issues/286#issuecomment-430385889:272,Modifiability,variab,variables,272,"Hi, sorry for not giving more of a description of the issue I was having. I tried to recreate a minimal example today using the PBMC_68k dataset and the cmap argument seemed to be working fine when using a gene as the color, but I'm still having problems with categorical variables like louvain clusters or user-defined cluster names. ```; fig, ax = plt.subplots(2,2,figsize=(12,8)); sc.pl.umap(adata, color='louvain', ax = ax[0,0], show=False); sc.pl.umap(adata, color='louvain', ax = ax[0,1], cmap=""tab10"", show=False); ax[1,0].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],; c=adata.obs['louvain'], cmap=""tab10"", s=0.1); ax[1,1].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],; c=adata.obs['louvain'], cmap=""tab20b"", s=0.1); ```; ![image](https://user-images.githubusercontent.com/7407663/47044553-8008ee00-d15e-11e8-8791-65ccb0fc7769.png). ```; fig, ax = plt.subplots(2,2,figsize=(12,8)); sc.pl.umap(adata, color=[""CD74""], ax=ax[0,0], show=False); sc.pl.umap(adata, color=[""CD74""], cmap=""viridis"", ax=ax[0,1], show=False); ax[1,0].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],; c=adata.X[:,adata.var_names==""CD74""].flatten(), cmap=""magma"", s=0.1); ax[1,1].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],; c=adata.X[:,adata.var_names==""CD74""].flatten(), cmap=""viridis"",; s=0.1, vmin=-0.6, vmax=3.5); ```; ![image](https://user-images.githubusercontent.com/7407663/47044843-45538580-d15f-11e8-8b05-89a1f75d3cee.png). These are the versions I'm using:; scanpy==1.3.2 anndata==0.6.11 numpy==1.14.6 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 ; My matplotlib version is 3.0.0.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286#issuecomment-430385889
https://github.com/scverse/scanpy/issues/286#issuecomment-430385889:1592,Usability,learn,learn,1592,"Hi, sorry for not giving more of a description of the issue I was having. I tried to recreate a minimal example today using the PBMC_68k dataset and the cmap argument seemed to be working fine when using a gene as the color, but I'm still having problems with categorical variables like louvain clusters or user-defined cluster names. ```; fig, ax = plt.subplots(2,2,figsize=(12,8)); sc.pl.umap(adata, color='louvain', ax = ax[0,0], show=False); sc.pl.umap(adata, color='louvain', ax = ax[0,1], cmap=""tab10"", show=False); ax[1,0].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],; c=adata.obs['louvain'], cmap=""tab10"", s=0.1); ax[1,1].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],; c=adata.obs['louvain'], cmap=""tab20b"", s=0.1); ```; ![image](https://user-images.githubusercontent.com/7407663/47044553-8008ee00-d15e-11e8-8791-65ccb0fc7769.png). ```; fig, ax = plt.subplots(2,2,figsize=(12,8)); sc.pl.umap(adata, color=[""CD74""], ax=ax[0,0], show=False); sc.pl.umap(adata, color=[""CD74""], cmap=""viridis"", ax=ax[0,1], show=False); ax[1,0].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],; c=adata.X[:,adata.var_names==""CD74""].flatten(), cmap=""magma"", s=0.1); ax[1,1].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],; c=adata.X[:,adata.var_names==""CD74""].flatten(), cmap=""viridis"",; s=0.1, vmin=-0.6, vmax=3.5); ```; ![image](https://user-images.githubusercontent.com/7407663/47044843-45538580-d15f-11e8-8b05-89a1f75d3cee.png). These are the versions I'm using:; scanpy==1.3.2 anndata==0.6.11 numpy==1.14.6 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 ; My matplotlib version is 3.0.0.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286#issuecomment-430385889
https://github.com/scverse/scanpy/issues/286#issuecomment-430389498:296,Deployability,continuous,continuous,296,"Can you try using the `palette` argument? After 1.3.1, Scanpy's plotting underwent quite some fundamental changes due to @fidelram. The code base improved a lot, there might be a few small issues, though. I had both `cmap` and `palette` as argument as I wanted users to choose a default for both continuous and categorical annotation. So if someone passes a `cmap` this only affects the continuous annotation, but for categoricals the `rcParams` default is used. Does this make sense? It might stop making sense when you provide lists to `cmap` and/or `palette`; in order to plot two different categoricals with two different palettes (which should be the default behavior at some point). Happy to discuss, whether we should depricate the `palette` argument and have the default access via `cmap`, that can be provided as a list.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286#issuecomment-430389498
https://github.com/scverse/scanpy/issues/286#issuecomment-430389498:387,Deployability,continuous,continuous,387,"Can you try using the `palette` argument? After 1.3.1, Scanpy's plotting underwent quite some fundamental changes due to @fidelram. The code base improved a lot, there might be a few small issues, though. I had both `cmap` and `palette` as argument as I wanted users to choose a default for both continuous and categorical annotation. So if someone passes a `cmap` this only affects the continuous annotation, but for categoricals the `rcParams` default is used. Does this make sense? It might stop making sense when you provide lists to `cmap` and/or `palette`; in order to plot two different categoricals with two different palettes (which should be the default behavior at some point). Happy to discuss, whether we should depricate the `palette` argument and have the default access via `cmap`, that can be provided as a list.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286#issuecomment-430389498
https://github.com/scverse/scanpy/issues/286#issuecomment-430389498:779,Security,access,access,779,"Can you try using the `palette` argument? After 1.3.1, Scanpy's plotting underwent quite some fundamental changes due to @fidelram. The code base improved a lot, there might be a few small issues, though. I had both `cmap` and `palette` as argument as I wanted users to choose a default for both continuous and categorical annotation. So if someone passes a `cmap` this only affects the continuous annotation, but for categoricals the `rcParams` default is used. Does this make sense? It might stop making sense when you provide lists to `cmap` and/or `palette`; in order to plot two different categoricals with two different palettes (which should be the default behavior at some point). Happy to discuss, whether we should depricate the `palette` argument and have the default access via `cmap`, that can be provided as a list.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286#issuecomment-430389498
https://github.com/scverse/scanpy/issues/286#issuecomment-430393368:25,Availability,error,error,25,"I'm also seeing the same error when using `sc.ppl.scatter`:; `sc.pl.scatter(adata, color='louvain', basis=""umap"", palette=""tab20"")`. ![image](https://user-images.githubusercontent.com/7407663/47046149-80a38380-d162-11e8-9865-6548c7e9454d.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286#issuecomment-430393368
https://github.com/scverse/scanpy/issues/286#issuecomment-430659918:11,Deployability,release,release,11,"Maybe it's release 1.3.2 - I wouldn't have made that release if I hadn't been asked to, I expected the new plotting backend to still have several bugs. The current master has several fixes. Do you think we should move forward with another release, @fidelram, @ivirshup; or are there still a few striking bugs in the scatter plots that I'm not aware of? It seems like a lot has been fixed in the past week.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286#issuecomment-430659918
https://github.com/scverse/scanpy/issues/286#issuecomment-430659918:53,Deployability,release,release,53,"Maybe it's release 1.3.2 - I wouldn't have made that release if I hadn't been asked to, I expected the new plotting backend to still have several bugs. The current master has several fixes. Do you think we should move forward with another release, @fidelram, @ivirshup; or are there still a few striking bugs in the scatter plots that I'm not aware of? It seems like a lot has been fixed in the past week.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286#issuecomment-430659918
https://github.com/scverse/scanpy/issues/286#issuecomment-430659918:239,Deployability,release,release,239,"Maybe it's release 1.3.2 - I wouldn't have made that release if I hadn't been asked to, I expected the new plotting backend to still have several bugs. The current master has several fixes. Do you think we should move forward with another release, @fidelram, @ivirshup; or are there still a few striking bugs in the scatter plots that I'm not aware of? It seems like a lot has been fixed in the past week.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286#issuecomment-430659918
https://github.com/scverse/scanpy/pull/289#issuecomment-428010274:91,Testability,test,testing,91,"Just a quick comment... Benjamini-Hochberg correction is usually the standard for multiple-testing correction in differential expression testing. Not sure if you want to take it into account, but I thought I should mention it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289#issuecomment-428010274
https://github.com/scverse/scanpy/pull/289#issuecomment-428010274:137,Testability,test,testing,137,"Just a quick comment... Benjamini-Hochberg correction is usually the standard for multiple-testing correction in differential expression testing. Not sure if you want to take it into account, but I thought I should mention it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289#issuecomment-428010274
https://github.com/scverse/scanpy/pull/289#issuecomment-428210702:109,Energy Efficiency,power,power,109,"That's a good point, @LuckyMD. I chose Bonferroni to have a more stringent correction (albeit with a loss in power), particularly due to the increased power inherent in the large sample sizes of single cell data. I might be wrong, but I think the Benjamini-Hochberg standard was established with bulk RNAseq, where limited sample sizes required an approach with more power. . However, I'm happy to change it to Benjamini-Hochberg if that's the consensus! It's a simple one-liner - we can even provide both and let the user choose by passing a parameter. Whatever is preferred!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289#issuecomment-428210702
https://github.com/scverse/scanpy/pull/289#issuecomment-428210702:151,Energy Efficiency,power,power,151,"That's a good point, @LuckyMD. I chose Bonferroni to have a more stringent correction (albeit with a loss in power), particularly due to the increased power inherent in the large sample sizes of single cell data. I might be wrong, but I think the Benjamini-Hochberg standard was established with bulk RNAseq, where limited sample sizes required an approach with more power. . However, I'm happy to change it to Benjamini-Hochberg if that's the consensus! It's a simple one-liner - we can even provide both and let the user choose by passing a parameter. Whatever is preferred!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289#issuecomment-428210702
https://github.com/scverse/scanpy/pull/289#issuecomment-428210702:367,Energy Efficiency,power,power,367,"That's a good point, @LuckyMD. I chose Bonferroni to have a more stringent correction (albeit with a loss in power), particularly due to the increased power inherent in the large sample sizes of single cell data. I might be wrong, but I think the Benjamini-Hochberg standard was established with bulk RNAseq, where limited sample sizes required an approach with more power. . However, I'm happy to change it to Benjamini-Hochberg if that's the consensus! It's a simple one-liner - we can even provide both and let the user choose by passing a parameter. Whatever is preferred!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289#issuecomment-428210702
https://github.com/scverse/scanpy/pull/289#issuecomment-428210702:462,Usability,simpl,simple,462,"That's a good point, @LuckyMD. I chose Bonferroni to have a more stringent correction (albeit with a loss in power), particularly due to the increased power inherent in the large sample sizes of single cell data. I might be wrong, but I think the Benjamini-Hochberg standard was established with bulk RNAseq, where limited sample sizes required an approach with more power. . However, I'm happy to change it to Benjamini-Hochberg if that's the consensus! It's a simple one-liner - we can even provide both and let the user choose by passing a parameter. Whatever is preferred!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289#issuecomment-428210702
https://github.com/scverse/scanpy/pull/289#issuecomment-428239213:175,Availability,error,error,175,"I definitely don't define the consensus, but I normally prefer FDR correction. It makes a bit more sense to me to correct for a false discovery rate, rather than a test-based error, if you are only interested in the rejected null hypotheses. . They also test for FDR control in a [recent comparison of differential testing methods](http://www.nature.com/doifinder/10.1038/nmeth.4612).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289#issuecomment-428239213
https://github.com/scverse/scanpy/pull/289#issuecomment-428239213:164,Testability,test,test-based,164,"I definitely don't define the consensus, but I normally prefer FDR correction. It makes a bit more sense to me to correct for a false discovery rate, rather than a test-based error, if you are only interested in the rejected null hypotheses. . They also test for FDR control in a [recent comparison of differential testing methods](http://www.nature.com/doifinder/10.1038/nmeth.4612).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289#issuecomment-428239213
https://github.com/scverse/scanpy/pull/289#issuecomment-428239213:254,Testability,test,test,254,"I definitely don't define the consensus, but I normally prefer FDR correction. It makes a bit more sense to me to correct for a false discovery rate, rather than a test-based error, if you are only interested in the rejected null hypotheses. . They also test for FDR control in a [recent comparison of differential testing methods](http://www.nature.com/doifinder/10.1038/nmeth.4612).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289#issuecomment-428239213
https://github.com/scverse/scanpy/pull/289#issuecomment-428239213:315,Testability,test,testing,315,"I definitely don't define the consensus, but I normally prefer FDR correction. It makes a bit more sense to me to correct for a false discovery rate, rather than a test-based error, if you are only interested in the rejected null hypotheses. . They also test for FDR control in a [recent comparison of differential testing methods](http://www.nature.com/doifinder/10.1038/nmeth.4612).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289#issuecomment-428239213
https://github.com/scverse/scanpy/pull/289#issuecomment-428734103:435,Availability,down,downstream,435,"That's a good point! I just added the Benjamini-Hochberg correction to the Wilcoxon code (as well as to the t-tests) and left that as the default. I left the Bonferroni code in there and marked the place with comments. If and when the pull request is complete, they can choose what to keep and remove the extra comments. Either way, since the uncorrected p-values are also outputted, the user can choose whichever correction they want downstream.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289#issuecomment-428734103
https://github.com/scverse/scanpy/pull/289#issuecomment-428734103:110,Testability,test,tests,110,"That's a good point! I just added the Benjamini-Hochberg correction to the Wilcoxon code (as well as to the t-tests) and left that as the default. I left the Bonferroni code in there and marked the place with comments. If and when the pull request is complete, they can choose what to keep and remove the extra comments. Either way, since the uncorrected p-values are also outputted, the user can choose whichever correction they want downstream.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289#issuecomment-428734103
https://github.com/scverse/scanpy/pull/289#issuecomment-429445105:695,Deployability,release,release,695,"Looks very good to me, thank you very much!. Would you mind adding an option to select for the correction type that defaults to 'benjamini-hochberg' and can be set to 'bonferroni'?. In the best of all world's, you'd also extend the tests for rank_genes_groups so that the p values are tested and not messed up by pull requests in the future. We want people to get the same p values again and again. And as the whole module sort of involves a lot of custom code as the scipy alternatives are not there for mult-dimensional and sparse data, it's easy to mess this up in the future. Thank you so much for the awesome addition @a-munoz-rojas , I'll add you both to the Scanpy author list and to the release notes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289#issuecomment-429445105
https://github.com/scverse/scanpy/pull/289#issuecomment-429445105:221,Modifiability,extend,extend,221,"Looks very good to me, thank you very much!. Would you mind adding an option to select for the correction type that defaults to 'benjamini-hochberg' and can be set to 'bonferroni'?. In the best of all world's, you'd also extend the tests for rank_genes_groups so that the p values are tested and not messed up by pull requests in the future. We want people to get the same p values again and again. And as the whole module sort of involves a lot of custom code as the scipy alternatives are not there for mult-dimensional and sparse data, it's easy to mess this up in the future. Thank you so much for the awesome addition @a-munoz-rojas , I'll add you both to the Scanpy author list and to the release notes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289#issuecomment-429445105
https://github.com/scverse/scanpy/pull/289#issuecomment-429445105:232,Testability,test,tests,232,"Looks very good to me, thank you very much!. Would you mind adding an option to select for the correction type that defaults to 'benjamini-hochberg' and can be set to 'bonferroni'?. In the best of all world's, you'd also extend the tests for rank_genes_groups so that the p values are tested and not messed up by pull requests in the future. We want people to get the same p values again and again. And as the whole module sort of involves a lot of custom code as the scipy alternatives are not there for mult-dimensional and sparse data, it's easy to mess this up in the future. Thank you so much for the awesome addition @a-munoz-rojas , I'll add you both to the Scanpy author list and to the release notes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289#issuecomment-429445105
https://github.com/scverse/scanpy/pull/289#issuecomment-429445105:285,Testability,test,tested,285,"Looks very good to me, thank you very much!. Would you mind adding an option to select for the correction type that defaults to 'benjamini-hochberg' and can be set to 'bonferroni'?. In the best of all world's, you'd also extend the tests for rank_genes_groups so that the p values are tested and not messed up by pull requests in the future. We want people to get the same p values again and again. And as the whole module sort of involves a lot of custom code as the scipy alternatives are not there for mult-dimensional and sparse data, it's easy to mess this up in the future. Thank you so much for the awesome addition @a-munoz-rojas , I'll add you both to the Scanpy author list and to the release notes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289#issuecomment-429445105
https://github.com/scverse/scanpy/pull/289#issuecomment-429919051:430,Modifiability,extend,extend,430,"Thank you very much, @falexwolf! I really appreciate the addition to the author list! I'm glad this is useful. I just now added the option to choose which correction method to use, and set benjamini-hochberg as the default, so that should be all set. With regards to the test, I unfortunately don't have experience building those tests, and have limited bandwidth at the moment. So it would probably be best if someone else could extend those tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289#issuecomment-429919051
https://github.com/scverse/scanpy/pull/289#issuecomment-429919051:271,Testability,test,test,271,"Thank you very much, @falexwolf! I really appreciate the addition to the author list! I'm glad this is useful. I just now added the option to choose which correction method to use, and set benjamini-hochberg as the default, so that should be all set. With regards to the test, I unfortunately don't have experience building those tests, and have limited bandwidth at the moment. So it would probably be best if someone else could extend those tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289#issuecomment-429919051
https://github.com/scverse/scanpy/pull/289#issuecomment-429919051:330,Testability,test,tests,330,"Thank you very much, @falexwolf! I really appreciate the addition to the author list! I'm glad this is useful. I just now added the option to choose which correction method to use, and set benjamini-hochberg as the default, so that should be all set. With regards to the test, I unfortunately don't have experience building those tests, and have limited bandwidth at the moment. So it would probably be best if someone else could extend those tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289#issuecomment-429919051
https://github.com/scverse/scanpy/pull/289#issuecomment-429919051:443,Testability,test,tests,443,"Thank you very much, @falexwolf! I really appreciate the addition to the author list! I'm glad this is useful. I just now added the option to choose which correction method to use, and set benjamini-hochberg as the default, so that should be all set. With regards to the test, I unfortunately don't have experience building those tests, and have limited bandwidth at the moment. So it would probably be best if someone else could extend those tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289#issuecomment-429919051
https://github.com/scverse/scanpy/pull/289#issuecomment-430649606:17,Testability,test,tests,17,"OK, I'll add the tests myself. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289#issuecomment-430649606
https://github.com/scverse/scanpy/issues/290#issuecomment-428240965:201,Testability,test,tests,201,"I have written a couple of functions to match clusters and marker genes. The simplest case is just a table of overlap score. Alternatively, I know someone who has used the Jaccard Index and enrichment tests. The other functions I wrote calculate average z-scores of marker genes in clusters (not sure if this is similar to `score_genes` or not. I could paste the functions in here if you like.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290#issuecomment-428240965
https://github.com/scverse/scanpy/issues/290#issuecomment-428240965:77,Usability,simpl,simplest,77,"I have written a couple of functions to match clusters and marker genes. The simplest case is just a table of overlap score. Alternatively, I know someone who has used the Jaccard Index and enrichment tests. The other functions I wrote calculate average z-scores of marker genes in clusters (not sure if this is similar to `score_genes` or not. I could paste the functions in here if you like.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290#issuecomment-428240965
https://github.com/scverse/scanpy/issues/290#issuecomment-428457349:140,Testability,test,tests,140,@LuckyMD I would be interested into looking at your method. It is different than that of `score_genes`. . I was considering to use the same tests from scanpy to identify marker genes but with a given set of markers as I want to know if a cluster could be annotated with a marker (which is different than to annotate a single cell). Any thoughts on this idea?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290#issuecomment-428457349
https://github.com/scverse/scanpy/issues/290#issuecomment-428502782:782,Testability,Test,Test,782,"Not sure what the best way of posting this is, but I'll just paste it for now:. Function to score clusters using multiple cell-type markers; ```; #Define cluster score for all markers; def evaluate_partition(anndata, marker_dict, gene_symbol_key=None, partition_key='louvain_r1'):; # Inputs:; # anndata - An AnnData object containing the data set and a partition; # marker_dict - A dictionary with cell-type markers. The markers should be stores as anndata.var_names or ; # an anndata.var field with the key given by the gene_symbol_key input; # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker ; # genes; # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is; # 'louvain_r1' . #Test inputs; if partition_key not in anndata.obs.columns.values:; print('KeyError: The partition key was not found in the passed AnnData object.'); print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'); raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):; print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'); print(' Check that your cell type markers are given in a format that your anndata object knows!'); raise; . if gene_symbol_key:; gene_ids = anndata.var[gene_symbol_key]; else:; gene_ids = anndata.var_names. clusters = np.unique(anndata.obs[partition_key]); n_clust = len(clusters); n_groups = len(marker_dict); ; marker_res = np.zeros((n_groups, n_clust)); z_scores = sc.pp.scale(anndata, copy=True). i = 0; for group in marker_dict:; # Find the corresponding columns and get their mean expression in the cluster; j = 0; for clust in clusters:; cluster_cells = np.in1d(z_scores.obs[partition_key], clust); marker_genes = np.in1d(gene_ids, marker_dict[group]); marker_res[i,j] = z_scores.X[np.ix_(cluster_cells,marker_genes)].mean(); j += 1; i+=1. variances = np.nanvar(marker_re",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290#issuecomment-428502782
https://github.com/scverse/scanpy/issues/290#issuecomment-428502782:3229,Testability,Test,Test,3229,"_res_df = pd.DataFrame(marker_res, columns=clusters, index=marker_dict.keys()). #Return the median of the variances over the clusters; return([np.median(variances), marker_res_df]); ```. Function to obtain scores for each marker individually per cluster; ```; #Define cluster score for individual genes; def marker_gene_expression(anndata, marker_dict, gene_symbol_key=None, partition_key='louvain_r1'):; """"""; A function to get mean z-score expressions of marker genes; # ; # Inputs:; # anndata - An AnnData object containing the data set and a partition; # marker_dict - A dictionary with cell-type markers. The markers should be stores as anndata.var_names or ; # an anndata.var field with the key given by the gene_symbol_key input; # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker ; # genes; # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is; # 'louvain_r1' ; """""". #Test inputs; if partition_key not in anndata.obs.columns.values:; print('KeyError: The partition key was not found in the passed AnnData object.'); print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'); raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):; print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'); print(' Check that your cell type markers are given in a format that your anndata object knows!'); raise; ; if gene_symbol_key:; gene_ids = anndata.var[gene_symbol_key]; else:; gene_ids = anndata.var_names. clusters = anndata.obs[partition_key].cat.categories; n_clust = len(clusters); marker_exp = pd.DataFrame(columns=clusters); marker_exp['cell_type'] = pd.Series({}, dtype='str'); marker_names = []; ; z_scores = sc.pp.scale(anndata, copy=True). i = 0; for group in marker_dict:; # Find the corresponding columns and get their mean expression in the cluster; for gene in marker",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290#issuecomment-428502782
https://github.com/scverse/scanpy/issues/290#issuecomment-428506572:36,Testability,test,tests,36,"> I was considering to use the same tests from scanpy to identify marker genes but with a given set of markers as I want to know if a cluster could be annotated with a marker (which is different than to annotate a single cell). Any thoughts on this idea?. I think that makes sense too. Without multiple testing correction, I feel like that should be equivalent to just filtering the results for the marker genes you have. And as the scores/p-values of that test are not really measures of significance (#270), it would be difficult to evaluate whether the score/p-value is sufficient to assign the cluster annotation. However, this approach is no worse than mine... I wonder how you can evaluate these approaches? Is there a dataset with very similar cells? Maybe gut with goblet and tuft cells appearing annoyingly similar (@mbuttner)?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290#issuecomment-428506572
https://github.com/scverse/scanpy/issues/290#issuecomment-428506572:303,Testability,test,testing,303,"> I was considering to use the same tests from scanpy to identify marker genes but with a given set of markers as I want to know if a cluster could be annotated with a marker (which is different than to annotate a single cell). Any thoughts on this idea?. I think that makes sense too. Without multiple testing correction, I feel like that should be equivalent to just filtering the results for the marker genes you have. And as the scores/p-values of that test are not really measures of significance (#270), it would be difficult to evaluate whether the score/p-value is sufficient to assign the cluster annotation. However, this approach is no worse than mine... I wonder how you can evaluate these approaches? Is there a dataset with very similar cells? Maybe gut with goblet and tuft cells appearing annoyingly similar (@mbuttner)?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290#issuecomment-428506572
https://github.com/scverse/scanpy/issues/290#issuecomment-428506572:457,Testability,test,test,457,"> I was considering to use the same tests from scanpy to identify marker genes but with a given set of markers as I want to know if a cluster could be annotated with a marker (which is different than to annotate a single cell). Any thoughts on this idea?. I think that makes sense too. Without multiple testing correction, I feel like that should be equivalent to just filtering the results for the marker genes you have. And as the scores/p-values of that test are not really measures of significance (#270), it would be difficult to evaluate whether the score/p-value is sufficient to assign the cluster annotation. However, this approach is no worse than mine... I wonder how you can evaluate these approaches? Is there a dataset with very similar cells? Maybe gut with goblet and tuft cells appearing annoyingly similar (@mbuttner)?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290#issuecomment-428506572
https://github.com/scverse/scanpy/issues/290#issuecomment-428935816:196,Energy Efficiency,reduce,reduced,196,"@LuckyMD Thanks for sharing your code, I will try it. As for a dataset with very similar cells, I think the pbmc68k has T-cells that are very similar to each other. You can take a quick look at a reduced datase by doing:. ```PYTHON; adata = sc.datasets.pbmc68k_reduced(); sc.pl.umap(adata, color='bulk_labels'); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290#issuecomment-428935816
https://github.com/scverse/scanpy/pull/292#issuecomment-429443444:19,Energy Efficiency,adapt,adapt,19,Thank you! Can you adapt the doc string so that it matches the other tools. We need numpydoc style documentation for it to render properly. You can also check whether it looks good by running `make html` in the docs folder.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/292#issuecomment-429443444
https://github.com/scverse/scanpy/pull/292#issuecomment-429443444:19,Modifiability,adapt,adapt,19,Thank you! Can you adapt the doc string so that it matches the other tools. We need numpydoc style documentation for it to render properly. You can also check whether it looks good by running `make html` in the docs folder.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/292#issuecomment-429443444
https://github.com/scverse/scanpy/pull/292#issuecomment-432779289:185,Energy Efficiency,adapt,adapted,185,"Sorry about this taking so long, I'm waiting until we have the new way of handling extensions in place... It will only be another couple of days and then this is going to be merged and adapted to that... There's nothing to do from your end on this... Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/292#issuecomment-432779289
https://github.com/scverse/scanpy/pull/292#issuecomment-432779289:185,Modifiability,adapt,adapted,185,"Sorry about this taking so long, I'm waiting until we have the new way of handling extensions in place... It will only be another couple of days and then this is going to be merged and adapted to that... There's nothing to do from your end on this... Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/292#issuecomment-432779289
https://github.com/scverse/scanpy/pull/292#issuecomment-435734288:2,Usability,simpl,simply,2,I simply meant to do what I described in that issue: https://github.com/theislab/scanpy/issues/271#issuecomment-431634492,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/292#issuecomment-435734288
https://github.com/scverse/scanpy/pull/292#issuecomment-450770914:141,Integrability,wrap,wrappers,141,"I'm finally merging this as we know have the `scanpy.external` submodule, where this can be properly linked - along with PHATE and all other wrappers to external single-cell packages. Sorry for that this took so long, I only could do this during the past calm couple of days. Thank you for the PR!. A very happy new year to you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/292#issuecomment-450770914
https://github.com/scverse/scanpy/issues/293#issuecomment-429227670:132,Availability,down,down,132,"I don't need to manually pass size with `edgecolor='none'`. I'll see if I can replicate in a conda environment, and then try to cut down the example a bit.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/293#issuecomment-429227670
https://github.com/scverse/scanpy/issues/293#issuecomment-429244882:726,Testability,test,test,726,"[Here are conda environment files](https://gist.github.com/ivirshup/505a63e54d0aad6ed6289cd1e080a8a2) for mac and linux which reproduce this behavior for me. I've also included a little script which should generate a plot demonstrating the issue [(here's an example output)](https://github.com/theislab/scanpy/files/2472189/pcatest.pdf). . ```python; import scanpy.api as sc; import numpy as np; import pandas as pd. sc.set_figure_params(dpi=300, dpi_save=300) # Makes it more visible. N = 1000; M = 2000. adata = sc.AnnData(; X=np.random.random_sample((N, M)),; obs=pd.DataFrame({""a"": np.random.randint(0, 2, N)}); ). adata.obs[""a""] = adata.obs[""a""].astype(str). sc.pp.pca(adata); sc.pl.pca(adata, color=""a"", size=0.1, save=""test.pdf""); ```. Could you try running this and let me know if you get a similar result?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/293#issuecomment-429244882
https://github.com/scverse/scanpy/issues/301#issuecomment-438749595:180,Usability,simpl,simple,180,"@falexwolf ; Another question - now [normalize_per_cell_weinreb16_deprecated](https://github.com/theislab/scanpy/blob/b4c2479eed302707a4d098f8f3c85037c82f07ca/scanpy/preprocessing/simple.py#L579) doesn't filter anything, just divides by sums of chosen genes, this normalization looks strange; ```; X = np.array([[1, 0, 1], [3, 0, 1], [5, 6, 1]]); normalize_per_cell_weinreb16_deprecated(X, max_fraction=0.7); array([[1. , 0. , 1. ],; [3. , 0. , 1. ],; [0.71428571, 0.85714286, 0.14285714]]); ```; Should it be this way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/301#issuecomment-438749595
https://github.com/scverse/scanpy/issues/302#issuecomment-441476438:92,Usability,simpl,simply,92,"Why not using https://nbsphinx.readthedocs.io? It works completely fine for me. So, I would simply moved forward with it as soon as there is some bandwidth.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/302#issuecomment-441476438
https://github.com/scverse/scanpy/issues/302#issuecomment-441700746:115,Testability,Test,Tests,115,You mean it uses/relies on [nbstripout](https://github.com/kynan/nbstripout)? In that case:. Pro | Con; --- | ---; Tests the Notebooks by running them | Docs need a long time to build; Only Text under version control |,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/302#issuecomment-441700746
https://github.com/scverse/scanpy/issues/302#issuecomment-441766825:656,Testability,test,tests,656,"There are different ways of stripping output from jupyter notebooks, several of them can be combined with the git commit workflow. I've played around a bit with them. The only thing I need to make sure is if there is a way that people *cannot* commit notebooks with output in them to the scanpy repo. The latter is an absolute no go. @grst, committing things in sphinx-gallery format could be a new way. I'll check whether this offers some convenience. In general I'd rather prefer to have the stripped `.ipynb` files in the repo, as it's done in tensorflow. @flying-sheep: I'm not worrying about the build time of the docs, that's decently fast in all my tests so far. @flying-sheep: After a bit of research I've done some time ago, I also don't consider tests based on notebooks a possibility. I made [versions of two notebooks](https://github.com/theislab/scanpy/tree/master/scanpy/tests/notebooks) running on subsampled data producing ugly looking low-resolution figures, which ensures small test times and prevents the repo from blowing up. For the tutorials, I want extremely high resolution figures and no subsampled data. I think I should be almost there. Hopefully we'll have the first two notebooks in the Scanpy repo this or next week...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/302#issuecomment-441766825
https://github.com/scverse/scanpy/issues/302#issuecomment-441766825:756,Testability,test,tests,756,"There are different ways of stripping output from jupyter notebooks, several of them can be combined with the git commit workflow. I've played around a bit with them. The only thing I need to make sure is if there is a way that people *cannot* commit notebooks with output in them to the scanpy repo. The latter is an absolute no go. @grst, committing things in sphinx-gallery format could be a new way. I'll check whether this offers some convenience. In general I'd rather prefer to have the stripped `.ipynb` files in the repo, as it's done in tensorflow. @flying-sheep: I'm not worrying about the build time of the docs, that's decently fast in all my tests so far. @flying-sheep: After a bit of research I've done some time ago, I also don't consider tests based on notebooks a possibility. I made [versions of two notebooks](https://github.com/theislab/scanpy/tree/master/scanpy/tests/notebooks) running on subsampled data producing ugly looking low-resolution figures, which ensures small test times and prevents the repo from blowing up. For the tutorials, I want extremely high resolution figures and no subsampled data. I think I should be almost there. Hopefully we'll have the first two notebooks in the Scanpy repo this or next week...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/302#issuecomment-441766825
https://github.com/scverse/scanpy/issues/302#issuecomment-441766825:885,Testability,test,tests,885,"There are different ways of stripping output from jupyter notebooks, several of them can be combined with the git commit workflow. I've played around a bit with them. The only thing I need to make sure is if there is a way that people *cannot* commit notebooks with output in them to the scanpy repo. The latter is an absolute no go. @grst, committing things in sphinx-gallery format could be a new way. I'll check whether this offers some convenience. In general I'd rather prefer to have the stripped `.ipynb` files in the repo, as it's done in tensorflow. @flying-sheep: I'm not worrying about the build time of the docs, that's decently fast in all my tests so far. @flying-sheep: After a bit of research I've done some time ago, I also don't consider tests based on notebooks a possibility. I made [versions of two notebooks](https://github.com/theislab/scanpy/tree/master/scanpy/tests/notebooks) running on subsampled data producing ugly looking low-resolution figures, which ensures small test times and prevents the repo from blowing up. For the tutorials, I want extremely high resolution figures and no subsampled data. I think I should be almost there. Hopefully we'll have the first two notebooks in the Scanpy repo this or next week...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/302#issuecomment-441766825
https://github.com/scverse/scanpy/issues/302#issuecomment-441766825:996,Testability,test,test,996,"There are different ways of stripping output from jupyter notebooks, several of them can be combined with the git commit workflow. I've played around a bit with them. The only thing I need to make sure is if there is a way that people *cannot* commit notebooks with output in them to the scanpy repo. The latter is an absolute no go. @grst, committing things in sphinx-gallery format could be a new way. I'll check whether this offers some convenience. In general I'd rather prefer to have the stripped `.ipynb` files in the repo, as it's done in tensorflow. @flying-sheep: I'm not worrying about the build time of the docs, that's decently fast in all my tests so far. @flying-sheep: After a bit of research I've done some time ago, I also don't consider tests based on notebooks a possibility. I made [versions of two notebooks](https://github.com/theislab/scanpy/tree/master/scanpy/tests/notebooks) running on subsampled data producing ugly looking low-resolution figures, which ensures small test times and prevents the repo from blowing up. For the tutorials, I want extremely high resolution figures and no subsampled data. I think I should be almost there. Hopefully we'll have the first two notebooks in the Scanpy repo this or next week...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/302#issuecomment-441766825
https://github.com/scverse/scanpy/issues/302#issuecomment-441965409:468,Usability,responsiv,responsive,468,"> The only thing I need to make sure is if there is a way that people cannot commit notebooks with output in them to the scanpy repo. The latter is an absolute no go. Should be easy to have that checked by travis. > committing things in sphinx-gallery format could be a new way. I'll check whether this offers some convenience. If you have any questions/issues regarding this approach, don't hesitate to open an issue in the jupytext repo. The maintainer is extremely responsive and willing to help.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/302#issuecomment-441965409
https://github.com/scverse/scanpy/issues/302#issuecomment-447622248:448,Deployability,update,updated,448,"@flying-sheep and anyone interested: I started putting the notebooks on `scanpy-tutorials`: https://github.com/theislab/scanpy-tutorials and https://scanpy.readthedocs.io/en/latest/tutorials.html now links to the docs generated from `scanpy-tutorials`. I'm doing it like this for now as it's quite a bit less work than getting everything to run on readthedocs, there might indeed be problems with the runtime for building the docs and I think this updated solution isn't so bad after all... . Opinions are welcome!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/302#issuecomment-447622248
https://github.com/scverse/scanpy/issues/303#issuecomment-441476938:288,Performance,perform,performance,288,"OK, very interesting! Can we have a video call on this? I'd be very interested in seeing a few benchmarks. . At first sight, I'd say it shouldn't be that as the problem also appears when there are no ""deep"" recursions. I'd have thought that it could be this line that brings considerable performance gain (I sent you the reference in an email some time ago):. https://github.com/cmap/cmapPy/blob/7a2e18030f713865e8038bc7351e5ca44d061205/cmapPy/pandasGEXpress/parse_gctx.py#L332-L333. To get away from the recursions and to use `read_direct`, one needs to start exploiting the naming conventions in the `.h5ad` files. As these has have converged since about a year ago, it's save to do it, along with a table that explains the file format and provides an official reference. Right now, the only reference on the file format is [this](https://github.com/theislab/scanpy_usage/blob/master/170505_seurat/info_h5ad.md), which is ridiculous. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/303#issuecomment-441476938
https://github.com/scverse/scanpy/issues/303#issuecomment-441476938:95,Testability,benchmark,benchmarks,95,"OK, very interesting! Can we have a video call on this? I'd be very interested in seeing a few benchmarks. . At first sight, I'd say it shouldn't be that as the problem also appears when there are no ""deep"" recursions. I'd have thought that it could be this line that brings considerable performance gain (I sent you the reference in an email some time ago):. https://github.com/cmap/cmapPy/blob/7a2e18030f713865e8038bc7351e5ca44d061205/cmapPy/pandasGEXpress/parse_gctx.py#L332-L333. To get away from the recursions and to use `read_direct`, one needs to start exploiting the naming conventions in the `.h5ad` files. As these has have converged since about a year ago, it's save to do it, along with a table that explains the file format and provides an official reference. Right now, the only reference on the file format is [this](https://github.com/theislab/scanpy_usage/blob/master/170505_seurat/info_h5ad.md), which is ridiculous. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/303#issuecomment-441476938
https://github.com/scverse/scanpy/issues/303#issuecomment-441478499:484,Testability,benchmark,benchmarks,484,"I haven't tried `read_direct ` yet but, in my opinion, it is not that helpful when we are reading the full array in memory without any type conversions. But i will check it of course. Now it seems like the problem in the recursion as reading simple files with pre-specified paths is faster and takes less memory.; Also, it can be that the problem is somewhere in the step of transforming dictionary to AnnData, but i don't see where for now. I'll check a few things, prepare readable benchmarks next week and we can have a call about it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/303#issuecomment-441478499
https://github.com/scverse/scanpy/issues/303#issuecomment-441478499:242,Usability,simpl,simple,242,"I haven't tried `read_direct ` yet but, in my opinion, it is not that helpful when we are reading the full array in memory without any type conversions. But i will check it of course. Now it seems like the problem in the recursion as reading simple files with pre-specified paths is faster and takes less memory.; Also, it can be that the problem is somewhere in the step of transforming dictionary to AnnData, but i don't see where for now. I'll check a few things, prepare readable benchmarks next week and we can have a call about it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/303#issuecomment-441478499
https://github.com/scverse/scanpy/issues/303#issuecomment-441478797:608,Performance,perform,performance,608,"Awesome, thank you!. Making use of the file conventions, we can move completely away from the dict. The way this was done is a pain and is really only there for historical reasons (I started working with dicts and then @flying-sheep said I shouldn't do that but make a data container...). So, I'm more than happy if the dict disappears completely and instead, one simply walks through the files and checks for the presence of certain predefined things. Of course, there will still be a lot of flexibility and a need to iterate through the `.uns` group, which can store dicts. But I hope that this won't be a performance bottleneck, as it's all small-scale.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/303#issuecomment-441478797
https://github.com/scverse/scanpy/issues/303#issuecomment-441478797:620,Performance,bottleneck,bottleneck,620,"Awesome, thank you!. Making use of the file conventions, we can move completely away from the dict. The way this was done is a pain and is really only there for historical reasons (I started working with dicts and then @flying-sheep said I shouldn't do that but make a data container...). So, I'm more than happy if the dict disappears completely and instead, one simply walks through the files and checks for the presence of certain predefined things. Of course, there will still be a lot of flexibility and a need to iterate through the `.uns` group, which can store dicts. But I hope that this won't be a performance bottleneck, as it's all small-scale.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/303#issuecomment-441478797
https://github.com/scverse/scanpy/issues/303#issuecomment-441478797:364,Usability,simpl,simply,364,"Awesome, thank you!. Making use of the file conventions, we can move completely away from the dict. The way this was done is a pain and is really only there for historical reasons (I started working with dicts and then @flying-sheep said I shouldn't do that but make a data container...). So, I'm more than happy if the dict disappears completely and instead, one simply walks through the files and checks for the presence of certain predefined things. Of course, there will still be a lot of flexibility and a need to iterate through the `.uns` group, which can store dicts. But I hope that this won't be a performance bottleneck, as it's all small-scale.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/303#issuecomment-441478797
https://github.com/scverse/scanpy/issues/303#issuecomment-443483600:202,Energy Efficiency,efficient,efficient,202,"https://github.com/theislab/anndata/pull/85; https://github.com/Koncopd/anndata-scanpy-benchmarks/blob/master/memory_issue_huge.ipynb. I wasn't right about recursion, almost no effect. `read_direct` is efficient.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/303#issuecomment-443483600
https://github.com/scverse/scanpy/issues/303#issuecomment-443483600:87,Testability,benchmark,benchmarks,87,"https://github.com/theislab/anndata/pull/85; https://github.com/Koncopd/anndata-scanpy-benchmarks/blob/master/memory_issue_huge.ipynb. I wasn't right about recursion, almost no effect. `read_direct` is efficient.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/303#issuecomment-443483600
https://github.com/scverse/scanpy/issues/303#issuecomment-443549085:63,Testability,benchmark,benchmarks,63,"OK, this is absolutely great, thank you for actually doing the benchmarks! :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/303#issuecomment-443549085
https://github.com/scverse/scanpy/pull/305#issuecomment-430195572:133,Deployability,install,installed,133,"Sorry, all of these packages aren't necessary for Scanpy's core functionality, supposed to be treated as extensions and shouldn't be installed by default. Hopefully we'll have a way of handling this that makes it more clear in the future.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/305#issuecomment-430195572
https://github.com/scverse/scanpy/pull/305#issuecomment-430195572:218,Usability,clear,clear,218,"Sorry, all of these packages aren't necessary for Scanpy's core functionality, supposed to be treated as extensions and shouldn't be installed by default. Hopefully we'll have a way of handling this that makes it more clear in the future.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/305#issuecomment-430195572
https://github.com/scverse/scanpy/pull/305#issuecomment-433357089:89,Availability,down,down,89,"@falexwolf the problem is that there are functions that do now work without them and our down stream packages do not work ootb. Would it be possible to add a second file, e.g. `requirements-ect.txt` to keep track of this?. This file could also be used to create testing environments easily.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/305#issuecomment-433357089
https://github.com/scverse/scanpy/pull/305#issuecomment-433357089:262,Testability,test,testing,262,"@falexwolf the problem is that there are functions that do now work without them and our down stream packages do not work ootb. Would it be possible to add a second file, e.g. `requirements-ect.txt` to keep track of this?. This file could also be used to create testing environments easily.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/305#issuecomment-433357089
https://github.com/scverse/scanpy/pull/305#issuecomment-435734577:12,Usability,simpl,simply,12,"Yes, we can simply add @bebatut's packages as an `ext` flag in the setup.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/305#issuecomment-435734577
https://github.com/scverse/scanpy/issues/306#issuecomment-430363010:28,Availability,error,error,28,"It's a bug that there is no error output. I haven't thought about drawing the PAGA graph in higher dimensions yet, hence no possibility to initialize from that. One could think about adding this functionality... but I don't know how meaningful that is at this stage.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/306#issuecomment-430363010
https://github.com/scverse/scanpy/pull/308#issuecomment-430667617:494,Modifiability,variab,variables,494,"So quick! thanks. I have been using the dendrograms for a while so; hopefully not so many bugs appear. Something that I wanted to have for; discussion is on some parameters relevant for the dendrogram, like the; genes used, the correlation method and the linkage method. All this can be; modified but currently is hard coded as I didn't want to add 3 more; parameters to the plotting functions. Maybe you have faced a similar problem and have an elegant solution. I; thought about setting some variables like the rcParams for matplotlib but I; think is not justified for just 3 parameters and can be very confusing. Or; we can have a function to compute a dendrogram with all parameters; required, and save this in .uns like rank_genes_groups. Then if other; functions find this information they add the dendrogram. On Wed, Oct 17, 2018 at 4:30 PM Alex Wolf <notifications@github.com> wrote:. > Merged #308 <https://github.com/theislab/scanpy/pull/308> into master.; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/pull/308#event-1909725548>, or mute; > the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1RD6Qm1iNFaKaG6elUL189hS5yFcks5ulz8SgaJpZM4Xjwsu>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/308#issuecomment-430667617
https://github.com/scverse/scanpy/pull/308#issuecomment-430674069:785,Modifiability,variab,variables,785,"The last option is the way forward, I'd say. We should have a tool `tl.dendogram` in the clustering section that has a parameter to select the clustering for which one wants a dendogram, typically defaulting to `louvain` (unfortunately, we still don't have a good consistent naming convention across all tools; `groupby` predominates but is not ideal in this setting. `grouping` or `cluster_key` would also be possible). You can still have the plotting functions call that function with default parameters. But the user wants more control, he or she can run the dendogram tool. Of course, we also want dendograms for genes. I think the most elegant (but maybe confusing solution) is to do it as in `pl.scatter`, where annotation of observations is selected if the key is in `.obs` and variables annotations are selected if the key is in `.var`. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/308#issuecomment-430674069
https://github.com/scverse/scanpy/pull/308#issuecomment-430880800:956,Modifiability,variab,variables,956,"Sounds good. If I find some time soon I will try that approach. On Wed, Oct 17, 2018 at 5:25 PM Alex Wolf <notifications@github.com> wrote:. > The last option is the way forward, I'd say. We should have a tool; > tl.dendogram in the clustering section that has a parameter to select the; > clustering for which one wants a dendogram, typically defaulting to; > louvain (unfortunately, we still don't have a good consistent naming; > convention across all tools; groupby predominates but is not ideal in; > this setting. grouping or cluster_key would also be possible).; >; > You can still have the plotting functions call that function with default; > parameters. But the user wants more control, he or she can run the; > dendogram tool.; >; > Of course, we also want dendograms for genes. I think the most elegant; > (but maybe confusing solution) is to do it as in pl.scatter, where; > annotation of observations is selected if the key is in .obs and; > variables annotations are selected if the key is in .var. What do you; > think?; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/pull/308#issuecomment-430674069>, or mute; > the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1TgBY1iDfvfhL2ravwfKRfL-A6wxks5ul0wAgaJpZM4Xjwsu>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/308#issuecomment-430880800
https://github.com/scverse/scanpy/issues/309#issuecomment-431269990:71,Usability,guid,guide,71,Does this problem also happens with the previous code? this is just to guide me on a solution.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/309#issuecomment-431269990
https://github.com/scverse/scanpy/issues/310#issuecomment-430990692:92,Integrability,interface,interface,92,"Yes, thank you. That would be very welcome! . @rfechtner: could it be that compat with your interface got messed up? It would be nice if you'd maintained the interface when you do so drastic changes to the underlying package. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/310#issuecomment-430990692
https://github.com/scverse/scanpy/issues/310#issuecomment-430990692:158,Integrability,interface,interface,158,"Yes, thank you. That would be very welcome! . @rfechtner: could it be that compat with your interface got messed up? It would be nice if you'd maintained the interface when you do so drastic changes to the underlying package. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/310#issuecomment-430990692
https://github.com/scverse/scanpy/issues/310#issuecomment-431115831:137,Deployability,update,updates,137,"@bebatut Thank you for pointing that out. I will as well take a look at it. . @falexwolf I will make sure to keep it in mind for further updates. The restructuring of the package structure came with the update from 1.x to 2.x and was necessary for some major improvements. I'm sorry for caused inconveniences. @bebatut if you have further questions or issues, please let me know, I'd be happy to help you out. Ron",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/310#issuecomment-431115831
https://github.com/scverse/scanpy/issues/310#issuecomment-431115831:203,Deployability,update,update,203,"@bebatut Thank you for pointing that out. I will as well take a look at it. . @falexwolf I will make sure to keep it in mind for further updates. The restructuring of the package structure came with the update from 1.x to 2.x and was necessary for some major improvements. I'm sorry for caused inconveniences. @bebatut if you have further questions or issues, please let me know, I'd be happy to help you out. Ron",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/310#issuecomment-431115831
https://github.com/scverse/scanpy/issues/311#issuecomment-431284136:287,Availability,error,error,287,"Hi @fidelram, good to see you :smile:. I was working on the Galaxy integration. I tested that with the `1.3.2` version from Bioconda. I tested with adata from krumsiek11. - For colors, I tried with `sc.pl.scatter(adata=adata, x='EKLF', y='Cebpa', color=['EgrNab', 'cJun']) and I got the error:. ```; ...; and (color is None or color in adata.obs.keys() or color in adata.var.index)):; File ""path/to/lib/python3.6/site-packages/pandas/core/indexes/base.py"", line 2035, in __contains__; hash(key); TypeError: unhashable type: 'list'; ```. - For components: the command was . ```; sc.pl.scatter(; adata=adata,; x='EKLF',; y='Cebpa',; color='EgrNab',; layers=('X', 'X', 'X'),; use_raw=False,; sort_order=True,; components='all',; projection='2d',; legend_loc='right margin',; legend_fontsize=1,; legend_fontweight='normal',; palette='viridis',; frameon=True,; right_margin=1.0,; size=1.0,; show=False,; save='.png'); ```; and the error:. ```; components = np.array(components).astype(int) - 1; ValueError: invalid literal for int() with base 10: 'all'; ```. Did I put the parameters in a wrong way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311#issuecomment-431284136
https://github.com/scverse/scanpy/issues/311#issuecomment-431284136:926,Availability,error,error,926,"Hi @fidelram, good to see you :smile:. I was working on the Galaxy integration. I tested that with the `1.3.2` version from Bioconda. I tested with adata from krumsiek11. - For colors, I tried with `sc.pl.scatter(adata=adata, x='EKLF', y='Cebpa', color=['EgrNab', 'cJun']) and I got the error:. ```; ...; and (color is None or color in adata.obs.keys() or color in adata.var.index)):; File ""path/to/lib/python3.6/site-packages/pandas/core/indexes/base.py"", line 2035, in __contains__; hash(key); TypeError: unhashable type: 'list'; ```. - For components: the command was . ```; sc.pl.scatter(; adata=adata,; x='EKLF',; y='Cebpa',; color='EgrNab',; layers=('X', 'X', 'X'),; use_raw=False,; sort_order=True,; components='all',; projection='2d',; legend_loc='right margin',; legend_fontsize=1,; legend_fontweight='normal',; palette='viridis',; frameon=True,; right_margin=1.0,; size=1.0,; show=False,; save='.png'); ```; and the error:. ```; components = np.array(components).astype(int) - 1; ValueError: invalid literal for int() with base 10: 'all'; ```. Did I put the parameters in a wrong way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311#issuecomment-431284136
https://github.com/scverse/scanpy/issues/311#issuecomment-431284136:67,Deployability,integrat,integration,67,"Hi @fidelram, good to see you :smile:. I was working on the Galaxy integration. I tested that with the `1.3.2` version from Bioconda. I tested with adata from krumsiek11. - For colors, I tried with `sc.pl.scatter(adata=adata, x='EKLF', y='Cebpa', color=['EgrNab', 'cJun']) and I got the error:. ```; ...; and (color is None or color in adata.obs.keys() or color in adata.var.index)):; File ""path/to/lib/python3.6/site-packages/pandas/core/indexes/base.py"", line 2035, in __contains__; hash(key); TypeError: unhashable type: 'list'; ```. - For components: the command was . ```; sc.pl.scatter(; adata=adata,; x='EKLF',; y='Cebpa',; color='EgrNab',; layers=('X', 'X', 'X'),; use_raw=False,; sort_order=True,; components='all',; projection='2d',; legend_loc='right margin',; legend_fontsize=1,; legend_fontweight='normal',; palette='viridis',; frameon=True,; right_margin=1.0,; size=1.0,; show=False,; save='.png'); ```; and the error:. ```; components = np.array(components).astype(int) - 1; ValueError: invalid literal for int() with base 10: 'all'; ```. Did I put the parameters in a wrong way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311#issuecomment-431284136
https://github.com/scverse/scanpy/issues/311#issuecomment-431284136:67,Integrability,integrat,integration,67,"Hi @fidelram, good to see you :smile:. I was working on the Galaxy integration. I tested that with the `1.3.2` version from Bioconda. I tested with adata from krumsiek11. - For colors, I tried with `sc.pl.scatter(adata=adata, x='EKLF', y='Cebpa', color=['EgrNab', 'cJun']) and I got the error:. ```; ...; and (color is None or color in adata.obs.keys() or color in adata.var.index)):; File ""path/to/lib/python3.6/site-packages/pandas/core/indexes/base.py"", line 2035, in __contains__; hash(key); TypeError: unhashable type: 'list'; ```. - For components: the command was . ```; sc.pl.scatter(; adata=adata,; x='EKLF',; y='Cebpa',; color='EgrNab',; layers=('X', 'X', 'X'),; use_raw=False,; sort_order=True,; components='all',; projection='2d',; legend_loc='right margin',; legend_fontsize=1,; legend_fontweight='normal',; palette='viridis',; frameon=True,; right_margin=1.0,; size=1.0,; show=False,; save='.png'); ```; and the error:. ```; components = np.array(components).astype(int) - 1; ValueError: invalid literal for int() with base 10: 'all'; ```. Did I put the parameters in a wrong way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311#issuecomment-431284136
https://github.com/scverse/scanpy/issues/311#issuecomment-431284136:648,Modifiability,layers,layers,648,"Hi @fidelram, good to see you :smile:. I was working on the Galaxy integration. I tested that with the `1.3.2` version from Bioconda. I tested with adata from krumsiek11. - For colors, I tried with `sc.pl.scatter(adata=adata, x='EKLF', y='Cebpa', color=['EgrNab', 'cJun']) and I got the error:. ```; ...; and (color is None or color in adata.obs.keys() or color in adata.var.index)):; File ""path/to/lib/python3.6/site-packages/pandas/core/indexes/base.py"", line 2035, in __contains__; hash(key); TypeError: unhashable type: 'list'; ```. - For components: the command was . ```; sc.pl.scatter(; adata=adata,; x='EKLF',; y='Cebpa',; color='EgrNab',; layers=('X', 'X', 'X'),; use_raw=False,; sort_order=True,; components='all',; projection='2d',; legend_loc='right margin',; legend_fontsize=1,; legend_fontweight='normal',; palette='viridis',; frameon=True,; right_margin=1.0,; size=1.0,; show=False,; save='.png'); ```; and the error:. ```; components = np.array(components).astype(int) - 1; ValueError: invalid literal for int() with base 10: 'all'; ```. Did I put the parameters in a wrong way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311#issuecomment-431284136
https://github.com/scverse/scanpy/issues/311#issuecomment-431284136:485,Security,hash,hash,485,"Hi @fidelram, good to see you :smile:. I was working on the Galaxy integration. I tested that with the `1.3.2` version from Bioconda. I tested with adata from krumsiek11. - For colors, I tried with `sc.pl.scatter(adata=adata, x='EKLF', y='Cebpa', color=['EgrNab', 'cJun']) and I got the error:. ```; ...; and (color is None or color in adata.obs.keys() or color in adata.var.index)):; File ""path/to/lib/python3.6/site-packages/pandas/core/indexes/base.py"", line 2035, in __contains__; hash(key); TypeError: unhashable type: 'list'; ```. - For components: the command was . ```; sc.pl.scatter(; adata=adata,; x='EKLF',; y='Cebpa',; color='EgrNab',; layers=('X', 'X', 'X'),; use_raw=False,; sort_order=True,; components='all',; projection='2d',; legend_loc='right margin',; legend_fontsize=1,; legend_fontweight='normal',; palette='viridis',; frameon=True,; right_margin=1.0,; size=1.0,; show=False,; save='.png'); ```; and the error:. ```; components = np.array(components).astype(int) - 1; ValueError: invalid literal for int() with base 10: 'all'; ```. Did I put the parameters in a wrong way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311#issuecomment-431284136
https://github.com/scverse/scanpy/issues/311#issuecomment-431284136:82,Testability,test,tested,82,"Hi @fidelram, good to see you :smile:. I was working on the Galaxy integration. I tested that with the `1.3.2` version from Bioconda. I tested with adata from krumsiek11. - For colors, I tried with `sc.pl.scatter(adata=adata, x='EKLF', y='Cebpa', color=['EgrNab', 'cJun']) and I got the error:. ```; ...; and (color is None or color in adata.obs.keys() or color in adata.var.index)):; File ""path/to/lib/python3.6/site-packages/pandas/core/indexes/base.py"", line 2035, in __contains__; hash(key); TypeError: unhashable type: 'list'; ```. - For components: the command was . ```; sc.pl.scatter(; adata=adata,; x='EKLF',; y='Cebpa',; color='EgrNab',; layers=('X', 'X', 'X'),; use_raw=False,; sort_order=True,; components='all',; projection='2d',; legend_loc='right margin',; legend_fontsize=1,; legend_fontweight='normal',; palette='viridis',; frameon=True,; right_margin=1.0,; size=1.0,; show=False,; save='.png'); ```; and the error:. ```; components = np.array(components).astype(int) - 1; ValueError: invalid literal for int() with base 10: 'all'; ```. Did I put the parameters in a wrong way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311#issuecomment-431284136
https://github.com/scverse/scanpy/issues/311#issuecomment-431284136:136,Testability,test,tested,136,"Hi @fidelram, good to see you :smile:. I was working on the Galaxy integration. I tested that with the `1.3.2` version from Bioconda. I tested with adata from krumsiek11. - For colors, I tried with `sc.pl.scatter(adata=adata, x='EKLF', y='Cebpa', color=['EgrNab', 'cJun']) and I got the error:. ```; ...; and (color is None or color in adata.obs.keys() or color in adata.var.index)):; File ""path/to/lib/python3.6/site-packages/pandas/core/indexes/base.py"", line 2035, in __contains__; hash(key); TypeError: unhashable type: 'list'; ```. - For components: the command was . ```; sc.pl.scatter(; adata=adata,; x='EKLF',; y='Cebpa',; color='EgrNab',; layers=('X', 'X', 'X'),; use_raw=False,; sort_order=True,; components='all',; projection='2d',; legend_loc='right margin',; legend_fontsize=1,; legend_fontweight='normal',; palette='viridis',; frameon=True,; right_margin=1.0,; size=1.0,; show=False,; save='.png'); ```; and the error:. ```; components = np.array(components).astype(int) - 1; ValueError: invalid literal for int() with base 10: 'all'; ```. Did I put the parameters in a wrong way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311#issuecomment-431284136
https://github.com/scverse/scanpy/issues/311#issuecomment-431635169:116,Deployability,release,release,116,"Yes, `1.3.2` might contain still a few bugs on the scatter side (I should have made this a prerelease); I wanted to release `1.3.3` quickly so that the fixes are there but I think there still remain small issues. There will be more bug-free release soon. You can just go back to `1.3.1`, which is working fine.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311#issuecomment-431635169
https://github.com/scverse/scanpy/issues/311#issuecomment-431635169:241,Deployability,release,release,241,"Yes, `1.3.2` might contain still a few bugs on the scatter side (I should have made this a prerelease); I wanted to release `1.3.3` quickly so that the fixes are there but I think there still remain small issues. There will be more bug-free release soon. You can just go back to `1.3.1`, which is working fine.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311#issuecomment-431635169
https://github.com/scverse/scanpy/issues/311#issuecomment-432212556:35,Availability,error,errors,35,@falexwolf Do you think that these errors come from the code that I modified? I think I didn't touch the function that @bebatut is using.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311#issuecomment-432212556
https://github.com/scverse/scanpy/issues/311#issuecomment-432533052:108,Usability,simpl,simpler,108,"I think that the best is to have separate scatter functions. For the; remaining use cases of scatter a much simpler version can be devised. But; meanwhile, @bebatut <https://github.com/bebatut> why did you try that; combination of parameters? Is this in some tutorial?. On Tue, Oct 23, 2018 at 7:15 PM Alex Wolf <notifications@github.com> wrote:. > Ah right, we now have to separate scatter functions, which isn't a good; > situation. @bebatut <https://github.com/bebatut> components does only; > make sense if you provide basis as an argument. a list-like color was; > also only meant for that case. both is now deprecated as @fidelram; > <https://github.com/fidelram> wrote a whole new scatter backend; however,; > which for now, misses the x and y parameters...; >; > In any case, pl.scatter should continue to work with the canonical calls; > and also with non-list-like color.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/311#issuecomment-432336990>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1TxK09r7RbbmfArW1Pt-UBFWhFQzks5un07HgaJpZM4XtV0c>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311#issuecomment-432533052
https://github.com/scverse/scanpy/issues/311#issuecomment-433267386:30,Energy Efficiency,reduce,reduce,30,"Yes, you are right! We should reduce the complexity of the current `sc.pl.scatter` now that it's no longer used for the embeddings...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311#issuecomment-433267386
https://github.com/scverse/scanpy/issues/311#issuecomment-463299529:13,Deployability,update,update,13,"Hi all,; any update on this? I'm on version 1.4 and even if in the documentation the color parameter is defined as string or list of strings, I'm still unable to pass to the scatter method a list of strings as value.; Thanks",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311#issuecomment-463299529
https://github.com/scverse/scanpy/issues/311#issuecomment-463771320:389,Testability,test,test,389,"Hi @fidelram,; One way in which I'd like to do it is like the following:. ```python; sc.pl.scatter(adata, x='<gene1>', y='<gene2>', color=['Mki67', 'Pclaf'],; save=False, use_raw=False); ```. to show the relationship between two genes (i.e. gene1 and gene2), and one third gene (in this case Mki67 in one subplot, Pclaf in the second).; One of the subplots could be like the following:. ![test](https://user-images.githubusercontent.com/697622/52814026-1e538480-3069-11e9-9af5-ef7a4761ff25.png). Hope this is clear. Thanks",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311#issuecomment-463771320
https://github.com/scverse/scanpy/issues/311#issuecomment-463771320:509,Usability,clear,clear,509,"Hi @fidelram,; One way in which I'd like to do it is like the following:. ```python; sc.pl.scatter(adata, x='<gene1>', y='<gene2>', color=['Mki67', 'Pclaf'],; save=False, use_raw=False); ```. to show the relationship between two genes (i.e. gene1 and gene2), and one third gene (in this case Mki67 in one subplot, Pclaf in the second).; One of the subplots could be like the following:. ![test](https://user-images.githubusercontent.com/697622/52814026-1e538480-3069-11e9-9af5-ef7a4761ff25.png). Hope this is clear. Thanks",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311#issuecomment-463771320
https://github.com/scverse/scanpy/issues/313#issuecomment-431367763:349,Availability,down,down,349,"Hm, but it looks like https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.score_genes.html should be reproducible with the default settings. In what you describe, only pca, louvain and umap have stochastic elements all of which set a default seed in the function call. I have never observed issues with reproducibility there. Can we narrow it down to score_genes? Or do you think it's somewhere else?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/313#issuecomment-431367763
https://github.com/scverse/scanpy/issues/313#issuecomment-432462799:214,Availability,down,downstream,214,"Thanks for the help. I have traced the randomness to sc.tl.score_genes_cell_cycle. Even if I explicitly state the random state to be 0, two repeats of score_genes_cell_cycle from the same data would give different downstream clustering. On the other hand, if I use one score_genes_cell_cycle output and repeat the downstream clustering methods, the clusters are the same. ; I am currently using scanpy==1.3.1",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/313#issuecomment-432462799
https://github.com/scverse/scanpy/issues/313#issuecomment-432462799:314,Availability,down,downstream,314,"Thanks for the help. I have traced the randomness to sc.tl.score_genes_cell_cycle. Even if I explicitly state the random state to be 0, two repeats of score_genes_cell_cycle from the same data would give different downstream clustering. On the other hand, if I use one score_genes_cell_cycle output and repeat the downstream clustering methods, the clusters are the same. ; I am currently using scanpy==1.3.1",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/313#issuecomment-432462799
https://github.com/scverse/scanpy/issues/313#issuecomment-551163746:360,Security,hash,hash,360,"Was there any progress on this? I also have an issue of non-reproducible UMAPs when re-starting Jupyter notebook. I have not been able to trace the exact cause of the issue, but it appears unrelated to the PCA step (I am using `svd_solver='arpack'`). In my case setting `random.seed` did not fix the issue. However, surprisingly, combining this with disabling hash randomization by setting `PYTHONHASHSEED` to `0` did generate reproducible results",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/313#issuecomment-551163746
https://github.com/scverse/scanpy/issues/313#issuecomment-849730924:0,Availability,Ping,Pinging,0,"Pinging this, as I've encountered it as well. I ran into non-reproducible UMAPs when rerunning code/notebooks and systematically went through my pipeline to find the source(s) of error, one of which was `sc.tl.score_genes_cell_cycle`. Setting the random seed externally did not help, but @Iwo-K's comment got me on the right track. I am now using the following simple hack, which fixes the issue for me:. ```python; adata.X = adata.X.astype('<f8') # Make float64 to ensure stability; sc.tl.score_genes_cell_cycle(adata, use_raw=False,; s_genes=cc_s_genes, g2m_genes=cc_g2m_genes,; random_state=0); adata.X = adata.X.astype('<f4') # Return to float32 for consistency; ```. Would be great if this would be fixed internally, perhaps using @Iwo-K's solution?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/313#issuecomment-849730924
https://github.com/scverse/scanpy/issues/313#issuecomment-849730924:179,Availability,error,error,179,"Pinging this, as I've encountered it as well. I ran into non-reproducible UMAPs when rerunning code/notebooks and systematically went through my pipeline to find the source(s) of error, one of which was `sc.tl.score_genes_cell_cycle`. Setting the random seed externally did not help, but @Iwo-K's comment got me on the right track. I am now using the following simple hack, which fixes the issue for me:. ```python; adata.X = adata.X.astype('<f8') # Make float64 to ensure stability; sc.tl.score_genes_cell_cycle(adata, use_raw=False,; s_genes=cc_s_genes, g2m_genes=cc_g2m_genes,; random_state=0); adata.X = adata.X.astype('<f4') # Return to float32 for consistency; ```. Would be great if this would be fixed internally, perhaps using @Iwo-K's solution?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/313#issuecomment-849730924
https://github.com/scverse/scanpy/issues/313#issuecomment-849730924:145,Deployability,pipeline,pipeline,145,"Pinging this, as I've encountered it as well. I ran into non-reproducible UMAPs when rerunning code/notebooks and systematically went through my pipeline to find the source(s) of error, one of which was `sc.tl.score_genes_cell_cycle`. Setting the random seed externally did not help, but @Iwo-K's comment got me on the right track. I am now using the following simple hack, which fixes the issue for me:. ```python; adata.X = adata.X.astype('<f8') # Make float64 to ensure stability; sc.tl.score_genes_cell_cycle(adata, use_raw=False,; s_genes=cc_s_genes, g2m_genes=cc_g2m_genes,; random_state=0); adata.X = adata.X.astype('<f4') # Return to float32 for consistency; ```. Would be great if this would be fixed internally, perhaps using @Iwo-K's solution?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/313#issuecomment-849730924
https://github.com/scverse/scanpy/issues/313#issuecomment-849730924:361,Usability,simpl,simple,361,"Pinging this, as I've encountered it as well. I ran into non-reproducible UMAPs when rerunning code/notebooks and systematically went through my pipeline to find the source(s) of error, one of which was `sc.tl.score_genes_cell_cycle`. Setting the random seed externally did not help, but @Iwo-K's comment got me on the right track. I am now using the following simple hack, which fixes the issue for me:. ```python; adata.X = adata.X.astype('<f8') # Make float64 to ensure stability; sc.tl.score_genes_cell_cycle(adata, use_raw=False,; s_genes=cc_s_genes, g2m_genes=cc_g2m_genes,; random_state=0); adata.X = adata.X.astype('<f4') # Return to float32 for consistency; ```. Would be great if this would be fixed internally, perhaps using @Iwo-K's solution?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/313#issuecomment-849730924
https://github.com/scverse/scanpy/issues/314#issuecomment-431635269:176,Availability,avail,available,176,"Using `adata.T.write_csvs(skip_data=False)` gives you this. If you only want the data matrix, you can also do `adata.to_df().to_csv()` using pandas. The last call will soon be available in a release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/314#issuecomment-431635269
https://github.com/scverse/scanpy/issues/314#issuecomment-431635269:191,Deployability,release,release,191,"Using `adata.T.write_csvs(skip_data=False)` gives you this. If you only want the data matrix, you can also do `adata.to_df().to_csv()` using pandas. The last call will soon be available in a release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/314#issuecomment-431635269
https://github.com/scverse/scanpy/pull/316#issuecomment-432531663:838,Deployability,release,release,838,"* I think it fits the standard tutorial, I calculate these things all the time. All the tutorials are in the [`scanpy_usage`](https://github.com/theislab/scanpy_usage) repo, right?; * Do you mean the `top_segment_proportions` and/ or `top_proportions` functions?. ## Sparse matrix support. I took the easy way out for calculations on other sparse matrices types – just converted them to a `CSR` – so there's room for improvement. I'm considering writing a more involved implementation, but I'd have to benchmark it against conversion. . I'd probably try an online sort (insertion?) for each cell, keeping only the top `max(ns)` expression values, while iterating through the `COO` or `CSC` matrix. ## Numba. This currently throws a lot of warnings about `np.partition` not being implemented in `numba`. This should change with their next release, and give some speedup here. ## `cell_controls`. I'm trying to decide on including this. I haven't used data with control wells, so I don't know how common it is. It could be nice to implement it a bit differently, and have able to get something like`.var[""mean_counts_in_sampletype-CD8""]`, but I'm already returning a lot of values. Any thoughts?. ## f-strings. Just noticed this is why my builds are failing. This might get ugly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-432531663
https://github.com/scverse/scanpy/pull/316#issuecomment-432531663:502,Testability,benchmark,benchmark,502,"* I think it fits the standard tutorial, I calculate these things all the time. All the tutorials are in the [`scanpy_usage`](https://github.com/theislab/scanpy_usage) repo, right?; * Do you mean the `top_segment_proportions` and/ or `top_proportions` functions?. ## Sparse matrix support. I took the easy way out for calculations on other sparse matrices types – just converted them to a `CSR` – so there's room for improvement. I'm considering writing a more involved implementation, but I'd have to benchmark it against conversion. . I'd probably try an online sort (insertion?) for each cell, keeping only the top `max(ns)` expression values, while iterating through the `COO` or `CSC` matrix. ## Numba. This currently throws a lot of warnings about `np.partition` not being implemented in `numba`. This should change with their next release, and give some speedup here. ## `cell_controls`. I'm trying to decide on including this. I haven't used data with control wells, so I don't know how common it is. It could be nice to implement it a bit differently, and have able to get something like`.var[""mean_counts_in_sampletype-CD8""]`, but I'm already returning a lot of values. Any thoughts?. ## f-strings. Just noticed this is why my builds are failing. This might get ugly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-432531663
https://github.com/scverse/scanpy/pull/316#issuecomment-433284121:196,Integrability,wrap,wrapper,196,"I've pretty much just gotta write the docs and rebase on my other PR, and this should be good to go. For adding `top_segment_proportions` and `top_proportions` to preprocessing, should they get a wrapper to work on AnnData objects? Also, I'm not super happy with the name `top_segment_proportions`, and am open to suggestions for a better name.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-433284121
https://github.com/scverse/scanpy/pull/316#issuecomment-433397997:301,Usability,simpl,simply,301,"I'd say that if the user is supposed to work with `top_segment_proportions` and `top_proportions` on a regular base, it should also accept `AnnData`s. Many Scanpy functions in the preprocessing module do both. In the beginning, I did this via recursive call, these days, I'd wouldn't recommend it but simply do:; ```; X = data; if isinstance(data, AnnData):; X = data.X; ```; or if you don't like `X` then `passed_data` or something... If you provide examples for your stuff here on this PR (simply paste a few pictures with a few lines of code), then we can discuss on a better name. Maybe @fidelram also has some opinion on this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-433397997
https://github.com/scverse/scanpy/pull/316#issuecomment-433397997:492,Usability,simpl,simply,492,"I'd say that if the user is supposed to work with `top_segment_proportions` and `top_proportions` on a regular base, it should also accept `AnnData`s. Many Scanpy functions in the preprocessing module do both. In the beginning, I did this via recursive call, these days, I'd wouldn't recommend it but simply do:; ```; X = data; if isinstance(data, AnnData):; X = data.X; ```; or if you don't like `X` then `passed_data` or something... If you provide examples for your stuff here on this PR (simply paste a few pictures with a few lines of code), then we can discuss on a better name. Maybe @fidelram also has some opinion on this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-433397997
https://github.com/scverse/scanpy/pull/316#issuecomment-433422519:83,Usability,simpl,simple,83,"As mentioned before, can you move everything you currently have in `/preprocessing/simple.py` to `qc.py`? We shouldn't grow that file even larger...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-433422519
https://github.com/scverse/scanpy/pull/316#issuecomment-433771528:546,Deployability,update,update,546,"Oh, I think I misunderstood earlier when you said:. > I just think that you should probably also add the top-level function to the qc.py file in preprocessing.; ; I wasn't sure if you meant move `calculate_qc_metrics` to `qc.py` or add `top_proportions` and `top_segment_proportions` to the preprocessing module. If you're not asking for that, I'm not sure if they're important enough to go there. I use `top_proportions` to make a `plotScater` kind of plot, but that's about it. Otherwise, I think this might be good for now. I was thinking I'd update the tutorial to use this function after the PR is merged. Once that's done, is there a script to update the tests under `notebooks` or is that done manually?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-433771528
https://github.com/scverse/scanpy/pull/316#issuecomment-433771528:650,Deployability,update,update,650,"Oh, I think I misunderstood earlier when you said:. > I just think that you should probably also add the top-level function to the qc.py file in preprocessing.; ; I wasn't sure if you meant move `calculate_qc_metrics` to `qc.py` or add `top_proportions` and `top_segment_proportions` to the preprocessing module. If you're not asking for that, I'm not sure if they're important enough to go there. I use `top_proportions` to make a `plotScater` kind of plot, but that's about it. Otherwise, I think this might be good for now. I was thinking I'd update the tutorial to use this function after the PR is merged. Once that's done, is there a script to update the tests under `notebooks` or is that done manually?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-433771528
https://github.com/scverse/scanpy/pull/316#issuecomment-433771528:661,Testability,test,tests,661,"Oh, I think I misunderstood earlier when you said:. > I just think that you should probably also add the top-level function to the qc.py file in preprocessing.; ; I wasn't sure if you meant move `calculate_qc_metrics` to `qc.py` or add `top_proportions` and `top_segment_proportions` to the preprocessing module. If you're not asking for that, I'm not sure if they're important enough to go there. I use `top_proportions` to make a `plotScater` kind of plot, but that's about it. Otherwise, I think this might be good for now. I was thinking I'd update the tutorial to use this function after the PR is merged. Once that's done, is there a script to update the tests under `notebooks` or is that done manually?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-433771528
https://github.com/scverse/scanpy/pull/316#issuecomment-434085531:154,Deployability,update,updated,154,"Thank you, great! I meant moving `calculate_qc_metrics` to `qc.py`. Updating the tutorial after this is good!. The tests under `notebooks/` are currently updated manually. . :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-434085531
https://github.com/scverse/scanpy/pull/316#issuecomment-434085531:115,Testability,test,tests,115,"Thank you, great! I meant moving `calculate_qc_metrics` to `qc.py`. Updating the tutorial after this is good!. The tests under `notebooks/` are currently updated manually. . :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-434085531
https://github.com/scverse/scanpy/pull/316#issuecomment-434087328:36,Deployability,release,release,36,Would you mind adding a note to the release notes and adding the function to the docs to complete the whole PR: https://github.com/theislab/scanpy/blob/master/docs/release_notes.rst?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-434087328
https://github.com/scverse/scanpy/pull/316#issuecomment-434172473:714,Deployability,update,updated,714,"I think I got the docs right, let me know if there are any issues. I'm noticing some conflict with convention for metric names. I'm copying `scater`, and using labels like `total_counts` and `total_features_by_counts`. `filter_genes`, `filter_cells`, `spring_project`, and a couple of the recipes use `n_counts` or `n_cells`. My preference is for the `scater ` way, since formatting allows it to be bit more flexible. It'd be nice for there to be a consistent default key for these features. For example, while updating the clustering tutorial, I ended up with both `n_counts` and `total_counts` in the same `adata.obs`. As changing the defaults could break some code, what's the right path forward? When `scater` updated their metric names, I think they used both the old and new keys with a deprecation warning. They talk about it a bit under the documentation for `scatter::calcuateQCMetrics`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-434172473
https://github.com/scverse/scanpy/pull/316#issuecomment-434172473:408,Modifiability,flexible,flexible,408,"I think I got the docs right, let me know if there are any issues. I'm noticing some conflict with convention for metric names. I'm copying `scater`, and using labels like `total_counts` and `total_features_by_counts`. `filter_genes`, `filter_cells`, `spring_project`, and a couple of the recipes use `n_counts` or `n_cells`. My preference is for the `scater ` way, since formatting allows it to be bit more flexible. It'd be nice for there to be a consistent default key for these features. For example, while updating the clustering tutorial, I ended up with both `n_counts` and `total_counts` in the same `adata.obs`. As changing the defaults could break some code, what's the right path forward? When `scater` updated their metric names, I think they used both the old and new keys with a deprecation warning. They talk about it a bit under the documentation for `scatter::calcuateQCMetrics`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-434172473
https://github.com/scverse/scanpy/pull/316#issuecomment-435731327:74,Energy Efficiency,adapt,adapt,74,"Hm, `n_counts` and `total_counts` is of course non-sense. Scanpy tries to adapt the `n_...` convention in scikit-learn and statsmodels for anything that is a number. We'll soon expose the quantile normalization preprocessing function to the users in a proper way. Then we'll have 95%-quantile counts vs. total counts. Then it starts making sense to use the notion `total_`. So, in the light of that, we could think about moving there. Yes, we'd deprecate old names and output a warning, too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-435731327
https://github.com/scverse/scanpy/pull/316#issuecomment-435731327:74,Modifiability,adapt,adapt,74,"Hm, `n_counts` and `total_counts` is of course non-sense. Scanpy tries to adapt the `n_...` convention in scikit-learn and statsmodels for anything that is a number. We'll soon expose the quantile normalization preprocessing function to the users in a proper way. Then we'll have 95%-quantile counts vs. total counts. Then it starts making sense to use the notion `total_`. So, in the light of that, we could think about moving there. Yes, we'd deprecate old names and output a warning, too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-435731327
https://github.com/scverse/scanpy/pull/316#issuecomment-435731327:177,Security,expose,expose,177,"Hm, `n_counts` and `total_counts` is of course non-sense. Scanpy tries to adapt the `n_...` convention in scikit-learn and statsmodels for anything that is a number. We'll soon expose the quantile normalization preprocessing function to the users in a proper way. Then we'll have 95%-quantile counts vs. total counts. Then it starts making sense to use the notion `total_`. So, in the light of that, we could think about moving there. Yes, we'd deprecate old names and output a warning, too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-435731327
https://github.com/scverse/scanpy/pull/316#issuecomment-435731327:113,Usability,learn,learn,113,"Hm, `n_counts` and `total_counts` is of course non-sense. Scanpy tries to adapt the `n_...` convention in scikit-learn and statsmodels for anything that is a number. We'll soon expose the quantile normalization preprocessing function to the users in a proper way. Then we'll have 95%-quantile counts vs. total counts. Then it starts making sense to use the notion `total_`. So, in the light of that, we could think about moving there. Yes, we'd deprecate old names and output a warning, too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-435731327
https://github.com/scverse/scanpy/pull/316#issuecomment-436124398:277,Modifiability,variab,variables,277,"Thanks for the tutorial!. Let's get most of this right now that no one has still used the function. We don't want to be consistent with Scater, we want to be consistent with the rest of Scanpy and the other python ecosystem. - Can we replace all occurances of `features` with `variables`? We had quite some discussions whether an AnnData is samples of features or observations of variables, and throughout, we stick with the latter convention. It's a very simple change. ; - Can we replace `exprs_values` with `expr_type` or something more suggestive of the fact that it's just a string denoting the kind of expression values? ; - Can we replace `total_features_by_counts` with `n_genes`? Why so complicated? And in contrast to `total_counts`, `features` does not suggest that it's a number, so there has to be an `n_...` before it, otherwise it completely breaks the convention.; - Can we call `feature_controls`, `control_variables`, which would be a much more intuitive name? ; - Why is `n_cells_by_{expr_values}` not simply `n_cells`? Am I missing something?. Regarding `n_counts` versus `total_counts`, I mentioned already that I see that `total_counts` has some advantages when starting to compare with quantile counts, etc. Also, it doesn't require an `n_` as it's clear that it's a number. But for all the rest that I mentioned above, I don't see these arguments. What do you think?. I really like that you use an `inplace` parameter instead of the usual `copy`, we might have exaggerated it in some places.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-436124398
https://github.com/scverse/scanpy/pull/316#issuecomment-436124398:380,Modifiability,variab,variables,380,"Thanks for the tutorial!. Let's get most of this right now that no one has still used the function. We don't want to be consistent with Scater, we want to be consistent with the rest of Scanpy and the other python ecosystem. - Can we replace all occurances of `features` with `variables`? We had quite some discussions whether an AnnData is samples of features or observations of variables, and throughout, we stick with the latter convention. It's a very simple change. ; - Can we replace `exprs_values` with `expr_type` or something more suggestive of the fact that it's just a string denoting the kind of expression values? ; - Can we replace `total_features_by_counts` with `n_genes`? Why so complicated? And in contrast to `total_counts`, `features` does not suggest that it's a number, so there has to be an `n_...` before it, otherwise it completely breaks the convention.; - Can we call `feature_controls`, `control_variables`, which would be a much more intuitive name? ; - Why is `n_cells_by_{expr_values}` not simply `n_cells`? Am I missing something?. Regarding `n_counts` versus `total_counts`, I mentioned already that I see that `total_counts` has some advantages when starting to compare with quantile counts, etc. Also, it doesn't require an `n_` as it's clear that it's a number. But for all the rest that I mentioned above, I don't see these arguments. What do you think?. I really like that you use an `inplace` parameter instead of the usual `copy`, we might have exaggerated it in some places.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-436124398
https://github.com/scverse/scanpy/pull/316#issuecomment-436124398:456,Usability,simpl,simple,456,"Thanks for the tutorial!. Let's get most of this right now that no one has still used the function. We don't want to be consistent with Scater, we want to be consistent with the rest of Scanpy and the other python ecosystem. - Can we replace all occurances of `features` with `variables`? We had quite some discussions whether an AnnData is samples of features or observations of variables, and throughout, we stick with the latter convention. It's a very simple change. ; - Can we replace `exprs_values` with `expr_type` or something more suggestive of the fact that it's just a string denoting the kind of expression values? ; - Can we replace `total_features_by_counts` with `n_genes`? Why so complicated? And in contrast to `total_counts`, `features` does not suggest that it's a number, so there has to be an `n_...` before it, otherwise it completely breaks the convention.; - Can we call `feature_controls`, `control_variables`, which would be a much more intuitive name? ; - Why is `n_cells_by_{expr_values}` not simply `n_cells`? Am I missing something?. Regarding `n_counts` versus `total_counts`, I mentioned already that I see that `total_counts` has some advantages when starting to compare with quantile counts, etc. Also, it doesn't require an `n_` as it's clear that it's a number. But for all the rest that I mentioned above, I don't see these arguments. What do you think?. I really like that you use an `inplace` parameter instead of the usual `copy`, we might have exaggerated it in some places.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-436124398
https://github.com/scverse/scanpy/pull/316#issuecomment-436124398:963,Usability,intuit,intuitive,963,"Thanks for the tutorial!. Let's get most of this right now that no one has still used the function. We don't want to be consistent with Scater, we want to be consistent with the rest of Scanpy and the other python ecosystem. - Can we replace all occurances of `features` with `variables`? We had quite some discussions whether an AnnData is samples of features or observations of variables, and throughout, we stick with the latter convention. It's a very simple change. ; - Can we replace `exprs_values` with `expr_type` or something more suggestive of the fact that it's just a string denoting the kind of expression values? ; - Can we replace `total_features_by_counts` with `n_genes`? Why so complicated? And in contrast to `total_counts`, `features` does not suggest that it's a number, so there has to be an `n_...` before it, otherwise it completely breaks the convention.; - Can we call `feature_controls`, `control_variables`, which would be a much more intuitive name? ; - Why is `n_cells_by_{expr_values}` not simply `n_cells`? Am I missing something?. Regarding `n_counts` versus `total_counts`, I mentioned already that I see that `total_counts` has some advantages when starting to compare with quantile counts, etc. Also, it doesn't require an `n_` as it's clear that it's a number. But for all the rest that I mentioned above, I don't see these arguments. What do you think?. I really like that you use an `inplace` parameter instead of the usual `copy`, we might have exaggerated it in some places.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-436124398
https://github.com/scverse/scanpy/pull/316#issuecomment-436124398:1021,Usability,simpl,simply,1021,"Thanks for the tutorial!. Let's get most of this right now that no one has still used the function. We don't want to be consistent with Scater, we want to be consistent with the rest of Scanpy and the other python ecosystem. - Can we replace all occurances of `features` with `variables`? We had quite some discussions whether an AnnData is samples of features or observations of variables, and throughout, we stick with the latter convention. It's a very simple change. ; - Can we replace `exprs_values` with `expr_type` or something more suggestive of the fact that it's just a string denoting the kind of expression values? ; - Can we replace `total_features_by_counts` with `n_genes`? Why so complicated? And in contrast to `total_counts`, `features` does not suggest that it's a number, so there has to be an `n_...` before it, otherwise it completely breaks the convention.; - Can we call `feature_controls`, `control_variables`, which would be a much more intuitive name? ; - Why is `n_cells_by_{expr_values}` not simply `n_cells`? Am I missing something?. Regarding `n_counts` versus `total_counts`, I mentioned already that I see that `total_counts` has some advantages when starting to compare with quantile counts, etc. Also, it doesn't require an `n_` as it's clear that it's a number. But for all the rest that I mentioned above, I don't see these arguments. What do you think?. I really like that you use an `inplace` parameter instead of the usual `copy`, we might have exaggerated it in some places.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-436124398
https://github.com/scverse/scanpy/pull/316#issuecomment-436124398:1272,Usability,clear,clear,1272,"Thanks for the tutorial!. Let's get most of this right now that no one has still used the function. We don't want to be consistent with Scater, we want to be consistent with the rest of Scanpy and the other python ecosystem. - Can we replace all occurances of `features` with `variables`? We had quite some discussions whether an AnnData is samples of features or observations of variables, and throughout, we stick with the latter convention. It's a very simple change. ; - Can we replace `exprs_values` with `expr_type` or something more suggestive of the fact that it's just a string denoting the kind of expression values? ; - Can we replace `total_features_by_counts` with `n_genes`? Why so complicated? And in contrast to `total_counts`, `features` does not suggest that it's a number, so there has to be an `n_...` before it, otherwise it completely breaks the convention.; - Can we call `feature_controls`, `control_variables`, which would be a much more intuitive name? ; - Why is `n_cells_by_{expr_values}` not simply `n_cells`? Am I missing something?. Regarding `n_counts` versus `total_counts`, I mentioned already that I see that `total_counts` has some advantages when starting to compare with quantile counts, etc. Also, it doesn't require an `n_` as it's clear that it's a number. But for all the rest that I mentioned above, I don't see these arguments. What do you think?. I really like that you use an `inplace` parameter instead of the usual `copy`, we might have exaggerated it in some places.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-436124398
https://github.com/scverse/scanpy/pull/316#issuecomment-436161904:1502,Deployability,update,update,1502,"No problem!. * `features` sounds more natural to me, but `variables` is fine. Maybe we could do `vars` instead of `variables` for reduced verbosity?; * `expr_type` would work. Maybe `vars_type`?; * How about `n_genes_by_{exprs_type/vars_type}`? `n` works great for this, since it's integer valued. I might like `vars` over `genes` since the variables could be transcripts or surface markers, but I'm not sure on this. I like the `by_{vars_type}` convention for a couple reasons, which also apply to your last point:; * It allows recording at multiple steps in the process. You could imagine: `n_{vars/genes}_by_counts` and `n_{vars/genes}_by_imputed_counts` or `n_{vars/genes}_by_normed_expression`; * The convention allows for multi-omic measurements on a gene, `n_{vars/genes}_by_fluorescence` for example. This is a case where `genes` makes more sense than `vars`.; * `control_variables` does sound more natural. I'd possibly like to replace `control` as well, since these aren't necessarily controlled variables.; * Largely similar thoughts as the third point, e.g.; * Recording at multiple steps: `n_cells_by_counts` and `n_cells_by_imputed_counts`; * Multi-omic measurements: `n_cells_by_fluorescence`. I think `total` can be more widely used than `n`, allowing more consistency. To me, `total_cells` or `total_vars` make sense while `n_fluorescence` or `n_log_counts` don't. It's also totally fine to have a mix. Yeah, I figured I didn't want to make a whole copy of the object if I didn't want update or add all the metrics. About places in the codebase where naming would need to change, I'd argue the default shouldn't be to use a pre-computed value. I hadn't realized that `n_counts` fields were being stored or used until I started looking around. Since summing over a matrix is likely a pretty light computation compared to what follows, I don't think there's a strong performance argument for keeping it as the default.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-436161904
https://github.com/scverse/scanpy/pull/316#issuecomment-436161904:130,Energy Efficiency,reduce,reduced,130,"No problem!. * `features` sounds more natural to me, but `variables` is fine. Maybe we could do `vars` instead of `variables` for reduced verbosity?; * `expr_type` would work. Maybe `vars_type`?; * How about `n_genes_by_{exprs_type/vars_type}`? `n` works great for this, since it's integer valued. I might like `vars` over `genes` since the variables could be transcripts or surface markers, but I'm not sure on this. I like the `by_{vars_type}` convention for a couple reasons, which also apply to your last point:; * It allows recording at multiple steps in the process. You could imagine: `n_{vars/genes}_by_counts` and `n_{vars/genes}_by_imputed_counts` or `n_{vars/genes}_by_normed_expression`; * The convention allows for multi-omic measurements on a gene, `n_{vars/genes}_by_fluorescence` for example. This is a case where `genes` makes more sense than `vars`.; * `control_variables` does sound more natural. I'd possibly like to replace `control` as well, since these aren't necessarily controlled variables.; * Largely similar thoughts as the third point, e.g.; * Recording at multiple steps: `n_cells_by_counts` and `n_cells_by_imputed_counts`; * Multi-omic measurements: `n_cells_by_fluorescence`. I think `total` can be more widely used than `n`, allowing more consistency. To me, `total_cells` or `total_vars` make sense while `n_fluorescence` or `n_log_counts` don't. It's also totally fine to have a mix. Yeah, I figured I didn't want to make a whole copy of the object if I didn't want update or add all the metrics. About places in the codebase where naming would need to change, I'd argue the default shouldn't be to use a pre-computed value. I hadn't realized that `n_counts` fields were being stored or used until I started looking around. Since summing over a matrix is likely a pretty light computation compared to what follows, I don't think there's a strong performance argument for keeping it as the default.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-436161904
https://github.com/scverse/scanpy/pull/316#issuecomment-436161904:58,Modifiability,variab,variables,58,"No problem!. * `features` sounds more natural to me, but `variables` is fine. Maybe we could do `vars` instead of `variables` for reduced verbosity?; * `expr_type` would work. Maybe `vars_type`?; * How about `n_genes_by_{exprs_type/vars_type}`? `n` works great for this, since it's integer valued. I might like `vars` over `genes` since the variables could be transcripts or surface markers, but I'm not sure on this. I like the `by_{vars_type}` convention for a couple reasons, which also apply to your last point:; * It allows recording at multiple steps in the process. You could imagine: `n_{vars/genes}_by_counts` and `n_{vars/genes}_by_imputed_counts` or `n_{vars/genes}_by_normed_expression`; * The convention allows for multi-omic measurements on a gene, `n_{vars/genes}_by_fluorescence` for example. This is a case where `genes` makes more sense than `vars`.; * `control_variables` does sound more natural. I'd possibly like to replace `control` as well, since these aren't necessarily controlled variables.; * Largely similar thoughts as the third point, e.g.; * Recording at multiple steps: `n_cells_by_counts` and `n_cells_by_imputed_counts`; * Multi-omic measurements: `n_cells_by_fluorescence`. I think `total` can be more widely used than `n`, allowing more consistency. To me, `total_cells` or `total_vars` make sense while `n_fluorescence` or `n_log_counts` don't. It's also totally fine to have a mix. Yeah, I figured I didn't want to make a whole copy of the object if I didn't want update or add all the metrics. About places in the codebase where naming would need to change, I'd argue the default shouldn't be to use a pre-computed value. I hadn't realized that `n_counts` fields were being stored or used until I started looking around. Since summing over a matrix is likely a pretty light computation compared to what follows, I don't think there's a strong performance argument for keeping it as the default.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-436161904
https://github.com/scverse/scanpy/pull/316#issuecomment-436161904:115,Modifiability,variab,variables,115,"No problem!. * `features` sounds more natural to me, but `variables` is fine. Maybe we could do `vars` instead of `variables` for reduced verbosity?; * `expr_type` would work. Maybe `vars_type`?; * How about `n_genes_by_{exprs_type/vars_type}`? `n` works great for this, since it's integer valued. I might like `vars` over `genes` since the variables could be transcripts or surface markers, but I'm not sure on this. I like the `by_{vars_type}` convention for a couple reasons, which also apply to your last point:; * It allows recording at multiple steps in the process. You could imagine: `n_{vars/genes}_by_counts` and `n_{vars/genes}_by_imputed_counts` or `n_{vars/genes}_by_normed_expression`; * The convention allows for multi-omic measurements on a gene, `n_{vars/genes}_by_fluorescence` for example. This is a case where `genes` makes more sense than `vars`.; * `control_variables` does sound more natural. I'd possibly like to replace `control` as well, since these aren't necessarily controlled variables.; * Largely similar thoughts as the third point, e.g.; * Recording at multiple steps: `n_cells_by_counts` and `n_cells_by_imputed_counts`; * Multi-omic measurements: `n_cells_by_fluorescence`. I think `total` can be more widely used than `n`, allowing more consistency. To me, `total_cells` or `total_vars` make sense while `n_fluorescence` or `n_log_counts` don't. It's also totally fine to have a mix. Yeah, I figured I didn't want to make a whole copy of the object if I didn't want update or add all the metrics. About places in the codebase where naming would need to change, I'd argue the default shouldn't be to use a pre-computed value. I hadn't realized that `n_counts` fields were being stored or used until I started looking around. Since summing over a matrix is likely a pretty light computation compared to what follows, I don't think there's a strong performance argument for keeping it as the default.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-436161904
https://github.com/scverse/scanpy/pull/316#issuecomment-436161904:341,Modifiability,variab,variables,341,"No problem!. * `features` sounds more natural to me, but `variables` is fine. Maybe we could do `vars` instead of `variables` for reduced verbosity?; * `expr_type` would work. Maybe `vars_type`?; * How about `n_genes_by_{exprs_type/vars_type}`? `n` works great for this, since it's integer valued. I might like `vars` over `genes` since the variables could be transcripts or surface markers, but I'm not sure on this. I like the `by_{vars_type}` convention for a couple reasons, which also apply to your last point:; * It allows recording at multiple steps in the process. You could imagine: `n_{vars/genes}_by_counts` and `n_{vars/genes}_by_imputed_counts` or `n_{vars/genes}_by_normed_expression`; * The convention allows for multi-omic measurements on a gene, `n_{vars/genes}_by_fluorescence` for example. This is a case where `genes` makes more sense than `vars`.; * `control_variables` does sound more natural. I'd possibly like to replace `control` as well, since these aren't necessarily controlled variables.; * Largely similar thoughts as the third point, e.g.; * Recording at multiple steps: `n_cells_by_counts` and `n_cells_by_imputed_counts`; * Multi-omic measurements: `n_cells_by_fluorescence`. I think `total` can be more widely used than `n`, allowing more consistency. To me, `total_cells` or `total_vars` make sense while `n_fluorescence` or `n_log_counts` don't. It's also totally fine to have a mix. Yeah, I figured I didn't want to make a whole copy of the object if I didn't want update or add all the metrics. About places in the codebase where naming would need to change, I'd argue the default shouldn't be to use a pre-computed value. I hadn't realized that `n_counts` fields were being stored or used until I started looking around. Since summing over a matrix is likely a pretty light computation compared to what follows, I don't think there's a strong performance argument for keeping it as the default.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-436161904
https://github.com/scverse/scanpy/pull/316#issuecomment-436161904:1006,Modifiability,variab,variables,1006,"No problem!. * `features` sounds more natural to me, but `variables` is fine. Maybe we could do `vars` instead of `variables` for reduced verbosity?; * `expr_type` would work. Maybe `vars_type`?; * How about `n_genes_by_{exprs_type/vars_type}`? `n` works great for this, since it's integer valued. I might like `vars` over `genes` since the variables could be transcripts or surface markers, but I'm not sure on this. I like the `by_{vars_type}` convention for a couple reasons, which also apply to your last point:; * It allows recording at multiple steps in the process. You could imagine: `n_{vars/genes}_by_counts` and `n_{vars/genes}_by_imputed_counts` or `n_{vars/genes}_by_normed_expression`; * The convention allows for multi-omic measurements on a gene, `n_{vars/genes}_by_fluorescence` for example. This is a case where `genes` makes more sense than `vars`.; * `control_variables` does sound more natural. I'd possibly like to replace `control` as well, since these aren't necessarily controlled variables.; * Largely similar thoughts as the third point, e.g.; * Recording at multiple steps: `n_cells_by_counts` and `n_cells_by_imputed_counts`; * Multi-omic measurements: `n_cells_by_fluorescence`. I think `total` can be more widely used than `n`, allowing more consistency. To me, `total_cells` or `total_vars` make sense while `n_fluorescence` or `n_log_counts` don't. It's also totally fine to have a mix. Yeah, I figured I didn't want to make a whole copy of the object if I didn't want update or add all the metrics. About places in the codebase where naming would need to change, I'd argue the default shouldn't be to use a pre-computed value. I hadn't realized that `n_counts` fields were being stored or used until I started looking around. Since summing over a matrix is likely a pretty light computation compared to what follows, I don't think there's a strong performance argument for keeping it as the default.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-436161904
https://github.com/scverse/scanpy/pull/316#issuecomment-436161904:1882,Performance,perform,performance,1882,"No problem!. * `features` sounds more natural to me, but `variables` is fine. Maybe we could do `vars` instead of `variables` for reduced verbosity?; * `expr_type` would work. Maybe `vars_type`?; * How about `n_genes_by_{exprs_type/vars_type}`? `n` works great for this, since it's integer valued. I might like `vars` over `genes` since the variables could be transcripts or surface markers, but I'm not sure on this. I like the `by_{vars_type}` convention for a couple reasons, which also apply to your last point:; * It allows recording at multiple steps in the process. You could imagine: `n_{vars/genes}_by_counts` and `n_{vars/genes}_by_imputed_counts` or `n_{vars/genes}_by_normed_expression`; * The convention allows for multi-omic measurements on a gene, `n_{vars/genes}_by_fluorescence` for example. This is a case where `genes` makes more sense than `vars`.; * `control_variables` does sound more natural. I'd possibly like to replace `control` as well, since these aren't necessarily controlled variables.; * Largely similar thoughts as the third point, e.g.; * Recording at multiple steps: `n_cells_by_counts` and `n_cells_by_imputed_counts`; * Multi-omic measurements: `n_cells_by_fluorescence`. I think `total` can be more widely used than `n`, allowing more consistency. To me, `total_cells` or `total_vars` make sense while `n_fluorescence` or `n_log_counts` don't. It's also totally fine to have a mix. Yeah, I figured I didn't want to make a whole copy of the object if I didn't want update or add all the metrics. About places in the codebase where naming would need to change, I'd argue the default shouldn't be to use a pre-computed value. I hadn't realized that `n_counts` fields were being stored or used until I started looking around. Since summing over a matrix is likely a pretty light computation compared to what follows, I don't think there's a strong performance argument for keeping it as the default.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-436161904
https://github.com/scverse/scanpy/pull/316#issuecomment-436378119:1230,Modifiability,variab,variables,1230,"thank you for your thoughts and suggestions! great! :smile:. - `vars` is fine, `genes` is also fine as this is a function that will predominantly used for RNA-seq, I guess, `features` would be fine if we had adopted a different convention; - `expr_type` is better than `vars_type`; we could do `values_type`, which is suggestive and anticipates that at some point, `.X` might get deprecated and replaced by `.values`; but maybe a simple `suffix` parameter is better?; - why not simply `n_genes` and `n_genes_{suffix} if suffix is not None`? if people want to distinguish between different processing steps or parts of the data matrix? but right now, the canonical use would only complete these things once for the raw data; I hardly imagine computing this stuff on imputed counts or normed expression... if people want, ok, they have the suffix argument for... if people do multi-omics with anndata (we're starting to do this a lot), yes, they should also be able to do it, but a `suffix` would be fine in this case, too; regarding `by_{suffix}`: I usually associate conditioning on something when I read `by` and I guess many people do, are we sure we want this here?; - `control_variables` is more clear than the super-generic `variables` argument, which usually indicates in scanpy that you want a function to restrict to a set of variables; but here, it's different; - largely similar thoughts on `n_cells` vs `n_cells_{suffix}`; - `n_...` versus `total_...`; sure you're absolutely right, `total_vars` is much easier to swallow than `n_fluorescence`; so, I'm ok with `total_...` if you like to move forward with that; on the other hand, having `n_...` for things that are numbers and counted and `sum_...` for things that are simply sums within columns or rows would be even clearer, I imagine; but maybe confusing to implement; your decision! :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-436378119
https://github.com/scverse/scanpy/pull/316#issuecomment-436378119:1334,Modifiability,variab,variables,1334,"thank you for your thoughts and suggestions! great! :smile:. - `vars` is fine, `genes` is also fine as this is a function that will predominantly used for RNA-seq, I guess, `features` would be fine if we had adopted a different convention; - `expr_type` is better than `vars_type`; we could do `values_type`, which is suggestive and anticipates that at some point, `.X` might get deprecated and replaced by `.values`; but maybe a simple `suffix` parameter is better?; - why not simply `n_genes` and `n_genes_{suffix} if suffix is not None`? if people want to distinguish between different processing steps or parts of the data matrix? but right now, the canonical use would only complete these things once for the raw data; I hardly imagine computing this stuff on imputed counts or normed expression... if people want, ok, they have the suffix argument for... if people do multi-omics with anndata (we're starting to do this a lot), yes, they should also be able to do it, but a `suffix` would be fine in this case, too; regarding `by_{suffix}`: I usually associate conditioning on something when I read `by` and I guess many people do, are we sure we want this here?; - `control_variables` is more clear than the super-generic `variables` argument, which usually indicates in scanpy that you want a function to restrict to a set of variables; but here, it's different; - largely similar thoughts on `n_cells` vs `n_cells_{suffix}`; - `n_...` versus `total_...`; sure you're absolutely right, `total_vars` is much easier to swallow than `n_fluorescence`; so, I'm ok with `total_...` if you like to move forward with that; on the other hand, having `n_...` for things that are numbers and counted and `sum_...` for things that are simply sums within columns or rows would be even clearer, I imagine; but maybe confusing to implement; your decision! :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-436378119
https://github.com/scverse/scanpy/pull/316#issuecomment-436378119:430,Usability,simpl,simple,430,"thank you for your thoughts and suggestions! great! :smile:. - `vars` is fine, `genes` is also fine as this is a function that will predominantly used for RNA-seq, I guess, `features` would be fine if we had adopted a different convention; - `expr_type` is better than `vars_type`; we could do `values_type`, which is suggestive and anticipates that at some point, `.X` might get deprecated and replaced by `.values`; but maybe a simple `suffix` parameter is better?; - why not simply `n_genes` and `n_genes_{suffix} if suffix is not None`? if people want to distinguish between different processing steps or parts of the data matrix? but right now, the canonical use would only complete these things once for the raw data; I hardly imagine computing this stuff on imputed counts or normed expression... if people want, ok, they have the suffix argument for... if people do multi-omics with anndata (we're starting to do this a lot), yes, they should also be able to do it, but a `suffix` would be fine in this case, too; regarding `by_{suffix}`: I usually associate conditioning on something when I read `by` and I guess many people do, are we sure we want this here?; - `control_variables` is more clear than the super-generic `variables` argument, which usually indicates in scanpy that you want a function to restrict to a set of variables; but here, it's different; - largely similar thoughts on `n_cells` vs `n_cells_{suffix}`; - `n_...` versus `total_...`; sure you're absolutely right, `total_vars` is much easier to swallow than `n_fluorescence`; so, I'm ok with `total_...` if you like to move forward with that; on the other hand, having `n_...` for things that are numbers and counted and `sum_...` for things that are simply sums within columns or rows would be even clearer, I imagine; but maybe confusing to implement; your decision! :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-436378119
https://github.com/scverse/scanpy/pull/316#issuecomment-436378119:478,Usability,simpl,simply,478,"thank you for your thoughts and suggestions! great! :smile:. - `vars` is fine, `genes` is also fine as this is a function that will predominantly used for RNA-seq, I guess, `features` would be fine if we had adopted a different convention; - `expr_type` is better than `vars_type`; we could do `values_type`, which is suggestive and anticipates that at some point, `.X` might get deprecated and replaced by `.values`; but maybe a simple `suffix` parameter is better?; - why not simply `n_genes` and `n_genes_{suffix} if suffix is not None`? if people want to distinguish between different processing steps or parts of the data matrix? but right now, the canonical use would only complete these things once for the raw data; I hardly imagine computing this stuff on imputed counts or normed expression... if people want, ok, they have the suffix argument for... if people do multi-omics with anndata (we're starting to do this a lot), yes, they should also be able to do it, but a `suffix` would be fine in this case, too; regarding `by_{suffix}`: I usually associate conditioning on something when I read `by` and I guess many people do, are we sure we want this here?; - `control_variables` is more clear than the super-generic `variables` argument, which usually indicates in scanpy that you want a function to restrict to a set of variables; but here, it's different; - largely similar thoughts on `n_cells` vs `n_cells_{suffix}`; - `n_...` versus `total_...`; sure you're absolutely right, `total_vars` is much easier to swallow than `n_fluorescence`; so, I'm ok with `total_...` if you like to move forward with that; on the other hand, having `n_...` for things that are numbers and counted and `sum_...` for things that are simply sums within columns or rows would be even clearer, I imagine; but maybe confusing to implement; your decision! :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-436378119
https://github.com/scverse/scanpy/pull/316#issuecomment-436378119:1200,Usability,clear,clear,1200,"thank you for your thoughts and suggestions! great! :smile:. - `vars` is fine, `genes` is also fine as this is a function that will predominantly used for RNA-seq, I guess, `features` would be fine if we had adopted a different convention; - `expr_type` is better than `vars_type`; we could do `values_type`, which is suggestive and anticipates that at some point, `.X` might get deprecated and replaced by `.values`; but maybe a simple `suffix` parameter is better?; - why not simply `n_genes` and `n_genes_{suffix} if suffix is not None`? if people want to distinguish between different processing steps or parts of the data matrix? but right now, the canonical use would only complete these things once for the raw data; I hardly imagine computing this stuff on imputed counts or normed expression... if people want, ok, they have the suffix argument for... if people do multi-omics with anndata (we're starting to do this a lot), yes, they should also be able to do it, but a `suffix` would be fine in this case, too; regarding `by_{suffix}`: I usually associate conditioning on something when I read `by` and I guess many people do, are we sure we want this here?; - `control_variables` is more clear than the super-generic `variables` argument, which usually indicates in scanpy that you want a function to restrict to a set of variables; but here, it's different; - largely similar thoughts on `n_cells` vs `n_cells_{suffix}`; - `n_...` versus `total_...`; sure you're absolutely right, `total_vars` is much easier to swallow than `n_fluorescence`; so, I'm ok with `total_...` if you like to move forward with that; on the other hand, having `n_...` for things that are numbers and counted and `sum_...` for things that are simply sums within columns or rows would be even clearer, I imagine; but maybe confusing to implement; your decision! :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-436378119
https://github.com/scverse/scanpy/pull/316#issuecomment-436378119:1731,Usability,simpl,simply,1731,"thank you for your thoughts and suggestions! great! :smile:. - `vars` is fine, `genes` is also fine as this is a function that will predominantly used for RNA-seq, I guess, `features` would be fine if we had adopted a different convention; - `expr_type` is better than `vars_type`; we could do `values_type`, which is suggestive and anticipates that at some point, `.X` might get deprecated and replaced by `.values`; but maybe a simple `suffix` parameter is better?; - why not simply `n_genes` and `n_genes_{suffix} if suffix is not None`? if people want to distinguish between different processing steps or parts of the data matrix? but right now, the canonical use would only complete these things once for the raw data; I hardly imagine computing this stuff on imputed counts or normed expression... if people want, ok, they have the suffix argument for... if people do multi-omics with anndata (we're starting to do this a lot), yes, they should also be able to do it, but a `suffix` would be fine in this case, too; regarding `by_{suffix}`: I usually associate conditioning on something when I read `by` and I guess many people do, are we sure we want this here?; - `control_variables` is more clear than the super-generic `variables` argument, which usually indicates in scanpy that you want a function to restrict to a set of variables; but here, it's different; - largely similar thoughts on `n_cells` vs `n_cells_{suffix}`; - `n_...` versus `total_...`; sure you're absolutely right, `total_vars` is much easier to swallow than `n_fluorescence`; so, I'm ok with `total_...` if you like to move forward with that; on the other hand, having `n_...` for things that are numbers and counted and `sum_...` for things that are simply sums within columns or rows would be even clearer, I imagine; but maybe confusing to implement; your decision! :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-436378119
https://github.com/scverse/scanpy/pull/316#issuecomment-436378119:1780,Usability,clear,clearer,1780,"thank you for your thoughts and suggestions! great! :smile:. - `vars` is fine, `genes` is also fine as this is a function that will predominantly used for RNA-seq, I guess, `features` would be fine if we had adopted a different convention; - `expr_type` is better than `vars_type`; we could do `values_type`, which is suggestive and anticipates that at some point, `.X` might get deprecated and replaced by `.values`; but maybe a simple `suffix` parameter is better?; - why not simply `n_genes` and `n_genes_{suffix} if suffix is not None`? if people want to distinguish between different processing steps or parts of the data matrix? but right now, the canonical use would only complete these things once for the raw data; I hardly imagine computing this stuff on imputed counts or normed expression... if people want, ok, they have the suffix argument for... if people do multi-omics with anndata (we're starting to do this a lot), yes, they should also be able to do it, but a `suffix` would be fine in this case, too; regarding `by_{suffix}`: I usually associate conditioning on something when I read `by` and I guess many people do, are we sure we want this here?; - `control_variables` is more clear than the super-generic `variables` argument, which usually indicates in scanpy that you want a function to restrict to a set of variables; but here, it's different; - largely similar thoughts on `n_cells` vs `n_cells_{suffix}`; - `n_...` versus `total_...`; sure you're absolutely right, `total_vars` is much easier to swallow than `n_fluorescence`; so, I'm ok with `total_...` if you like to move forward with that; on the other hand, having `n_...` for things that are numbers and counted and `sum_...` for things that are simply sums within columns or rows would be even clearer, I imagine; but maybe confusing to implement; your decision! :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-436378119
https://github.com/scverse/scanpy/pull/316#issuecomment-436498986:123,Modifiability,variab,variable,123,"* How about a `var_type` argument which defaults to `""genes""`? Could even replace occurrences of `cells` with a `obs_type` variable; * Agreed, `expr_type` it is.; * I'd considered an option of a suffix argument, but I think there are some issues with it being a string. If `suffix=None, expr_type=""counts""` is it still `total_counts`? If `suffix=""_by_counts""` is it still `total_counts`? A possible solution to this is making suffix a boolean argument, where `True` adds `_by_{expr_type}` as appropriate.; * Additionally `n_genes` in particular is already an overloaded term, as it's already used with multiple different meanings in the codebase (typically number of genes to consider, or number of columns in the data matrix). ; * Could you say a bit more about the conditioning? I'm not sure I follow how it would be confusing here.; * `control_variables` definitely makes more sense here than `variables`, I was just wondering if there's a better word than `control`.; * If there was an equivalent for observations (something like `mean_counts_in_donor1`), what would be a good name for that argument?; * I think I'm happy with `n_...` for some things, `total_...` for others.; * On other convention I'm unsure about, should the range of the `pct_...` values be 0-100 or 0-1?. ---------------. Here's what I'm thinking it'll look like right now (pending `suffix` decision). For obs:. | current | proposed |; | ------- | -------- |; |`total_features_by_{expr_values}` | `n_{var_type}_by_{expr_type}`|; |`total_{expr_values}` | `total_{expr_type}`|; |`pct_{expr_values}_in_top_{n}_features` | `pct_{expr_type}_in_top_{n}_{var_type}`|; |`total_{expr_values}_{feature_control}` | `total_{expr_type}_{control_var}`|; |`pct_{expr_values}_{feature_control}` | `pct_{expr_type}_{control_var}`|. For var:. | current | proposed |; | ------- | -------- |; |`total_{expr_values}` | `total_{expr_type}`|; |`mean_{expr_values}` | `mean_{expr_type}`|; |`n_cells_by_{expr_values}` | `n_cells_by_{expr_type}`|; |`pc",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-436498986
https://github.com/scverse/scanpy/pull/316#issuecomment-436498986:897,Modifiability,variab,variables,897,"* How about a `var_type` argument which defaults to `""genes""`? Could even replace occurrences of `cells` with a `obs_type` variable; * Agreed, `expr_type` it is.; * I'd considered an option of a suffix argument, but I think there are some issues with it being a string. If `suffix=None, expr_type=""counts""` is it still `total_counts`? If `suffix=""_by_counts""` is it still `total_counts`? A possible solution to this is making suffix a boolean argument, where `True` adds `_by_{expr_type}` as appropriate.; * Additionally `n_genes` in particular is already an overloaded term, as it's already used with multiple different meanings in the codebase (typically number of genes to consider, or number of columns in the data matrix). ; * Could you say a bit more about the conditioning? I'm not sure I follow how it would be confusing here.; * `control_variables` definitely makes more sense here than `variables`, I was just wondering if there's a better word than `control`.; * If there was an equivalent for observations (something like `mean_counts_in_donor1`), what would be a good name for that argument?; * I think I'm happy with `n_...` for some things, `total_...` for others.; * On other convention I'm unsure about, should the range of the `pct_...` values be 0-100 or 0-1?. ---------------. Here's what I'm thinking it'll look like right now (pending `suffix` decision). For obs:. | current | proposed |; | ------- | -------- |; |`total_features_by_{expr_values}` | `n_{var_type}_by_{expr_type}`|; |`total_{expr_values}` | `total_{expr_type}`|; |`pct_{expr_values}_in_top_{n}_features` | `pct_{expr_type}_in_top_{n}_{var_type}`|; |`total_{expr_values}_{feature_control}` | `total_{expr_type}_{control_var}`|; |`pct_{expr_values}_{feature_control}` | `pct_{expr_type}_{control_var}`|. For var:. | current | proposed |; | ------- | -------- |; |`total_{expr_values}` | `total_{expr_type}`|; |`mean_{expr_values}` | `mean_{expr_type}`|; |`n_cells_by_{expr_values}` | `n_cells_by_{expr_type}`|; |`pc",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-436498986
https://github.com/scverse/scanpy/pull/316#issuecomment-437734137:187,Integrability,depend,depending,187,"> How about a var_type argument which defaults to ""genes""? Could even replace occurrences of cells with a obs_type variable. Sorry about the late response. The week passes so quickly and depending on how it goes, I can only go back to Scanpy during the weekends these days... I like your new table much better than the previous one. If you'd put that forward in a PR, I'd be super happy!. PS: I think `control_variables` is fine, maybe `focus_variables` better reflects that it's not only about ""control"", but just a subset of variables (or observations) of interest. Let's not move forward on the suffix argument, your imposed structure will make things cleaner for people! :smile: `pct` is ok if in 0 - 100. No `obs_type` needed right now...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-437734137
https://github.com/scverse/scanpy/pull/316#issuecomment-437734137:115,Modifiability,variab,variable,115,"> How about a var_type argument which defaults to ""genes""? Could even replace occurrences of cells with a obs_type variable. Sorry about the late response. The week passes so quickly and depending on how it goes, I can only go back to Scanpy during the weekends these days... I like your new table much better than the previous one. If you'd put that forward in a PR, I'd be super happy!. PS: I think `control_variables` is fine, maybe `focus_variables` better reflects that it's not only about ""control"", but just a subset of variables (or observations) of interest. Let's not move forward on the suffix argument, your imposed structure will make things cleaner for people! :smile: `pct` is ok if in 0 - 100. No `obs_type` needed right now...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-437734137
https://github.com/scverse/scanpy/pull/316#issuecomment-437734137:527,Modifiability,variab,variables,527,"> How about a var_type argument which defaults to ""genes""? Could even replace occurrences of cells with a obs_type variable. Sorry about the late response. The week passes so quickly and depending on how it goes, I can only go back to Scanpy during the weekends these days... I like your new table much better than the previous one. If you'd put that forward in a PR, I'd be super happy!. PS: I think `control_variables` is fine, maybe `focus_variables` better reflects that it's not only about ""control"", but just a subset of variables (or observations) of interest. Let's not move forward on the suffix argument, your imposed structure will make things cleaner for people! :smile: `pct` is ok if in 0 - 100. No `obs_type` needed right now...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-437734137
https://github.com/scverse/scanpy/pull/316#issuecomment-437745410:97,Availability,down,down,97,"No problem, gave a chance to optimize the code a bit (peak memory was about 3x AnnData size, now down to about 2x).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-437745410
https://github.com/scverse/scanpy/pull/316#issuecomment-437745410:29,Performance,optimiz,optimize,29,"No problem, gave a chance to optimize the code a bit (peak memory was about 3x AnnData size, now down to about 2x).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-437745410
https://github.com/scverse/scanpy/issues/317#issuecomment-431812136:44,Availability,toler,tolerance,44,The only solution so far is to increase the tolerance threshold of the tests! I don't know where those differences come from. Is always a problem. I will be very glad if you find a better solution.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317#issuecomment-431812136
https://github.com/scverse/scanpy/issues/317#issuecomment-431812136:71,Testability,test,tests,71,The only solution so far is to increase the tolerance threshold of the tests! I don't know where those differences come from. Is always a problem. I will be very glad if you find a better solution.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317#issuecomment-431812136
https://github.com/scverse/scanpy/issues/317#issuecomment-432047736:31,Availability,error,errors,31,"I don't remember getting these errors before. Are you not getting them now, and did you get them before?. Also, where did the images in the repo get generated?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317#issuecomment-432047736
https://github.com/scverse/scanpy/issues/317#issuecomment-432352461:8,Testability,test,tests,8,"Hm, the tests work for me. And I never set up a specific environment. Obviously, they also run through on Travis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317#issuecomment-432352461
https://github.com/scverse/scanpy/issues/317#issuecomment-434538616:26,Testability,test,tests,26,"I've been able to get the tests to pass on a different machine (this one running linux). I can get rid of most of the discrepancies between plots between the two machines by replacing:. ```python; mpl.use(""agg""); ```. with. ```python; from matplotlib.testing import setup; setup(); ```. However violin plots still get a large RMS value.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317#issuecomment-434538616
https://github.com/scverse/scanpy/issues/317#issuecomment-434538616:251,Testability,test,testing,251,"I've been able to get the tests to pass on a different machine (this one running linux). I can get rid of most of the discrepancies between plots between the two machines by replacing:. ```python; mpl.use(""agg""); ```. with. ```python; from matplotlib.testing import setup; setup(); ```. However violin plots still get a large RMS value.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317#issuecomment-434538616
https://github.com/scverse/scanpy/issues/317#issuecomment-435647914:67,Testability,test,testing,67,"For me, exactly the opposite happens; adding; ```; from matplotlib.testing import setup; setup(); ```; makes almost all tests fail. If I use it from the beginning and produce all test images with it, then, of course, it works.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317#issuecomment-435647914
https://github.com/scverse/scanpy/issues/317#issuecomment-435647914:120,Testability,test,tests,120,"For me, exactly the opposite happens; adding; ```; from matplotlib.testing import setup; setup(); ```; makes almost all tests fail. If I use it from the beginning and produce all test images with it, then, of course, it works.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317#issuecomment-435647914
https://github.com/scverse/scanpy/issues/317#issuecomment-435647914:179,Testability,test,test,179,"For me, exactly the opposite happens; adding; ```; from matplotlib.testing import setup; setup(); ```; makes almost all tests fail. If I use it from the beginning and produce all test images with it, then, of course, it works.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317#issuecomment-435647914
https://github.com/scverse/scanpy/issues/317#issuecomment-435650013:46,Testability,test,tests,46,"Hm, I've wanted to add a PAGA notebook to the tests and I'm struggling to get it to run on travis. The tests run through for me on a MacBook and on a remote linux machine, but travis seems even to be able to produce differently-shaped output images: https://travis-ci.org/theislab/scanpy/jobs/450416143. My current suspicion is that networkx does something strange as it really only affects the graph plot... I'll investigate further, but if you've seen this already, a hint would be very welcome! 🙂",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317#issuecomment-435650013
https://github.com/scverse/scanpy/issues/317#issuecomment-435650013:103,Testability,test,tests,103,"Hm, I've wanted to add a PAGA notebook to the tests and I'm struggling to get it to run on travis. The tests run through for me on a MacBook and on a remote linux machine, but travis seems even to be able to produce differently-shaped output images: https://travis-ci.org/theislab/scanpy/jobs/450416143. My current suspicion is that networkx does something strange as it really only affects the graph plot... I'll investigate further, but if you've seen this already, a hint would be very welcome! 🙂",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317#issuecomment-435650013
https://github.com/scverse/scanpy/issues/317#issuecomment-435728828:222,Availability,error,errors,222,"Ah, yeah that's what I meant. If I use `setup()`, the tests on the linux server fail. However, the images generated are similar (RMSD < 10) to images made on my MacBook after running `setup()`. On the PAGA notebook, I saw errors like that when I was playing around with the dpi. Maybe fig size or dpi is being changed?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317#issuecomment-435728828
https://github.com/scverse/scanpy/issues/317#issuecomment-435728828:54,Testability,test,tests,54,"Ah, yeah that's what I meant. If I use `setup()`, the tests on the linux server fail. However, the images generated are similar (RMSD < 10) to images made on my MacBook after running `setup()`. On the PAGA notebook, I saw errors like that when I was playing around with the dpi. Maybe fig size or dpi is being changed?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317#issuecomment-435728828
https://github.com/scverse/scanpy/issues/317#issuecomment-435729565:215,Availability,reliab,reliable,215,"OK! Thanks! @fidelram Should we simply regenerate all images using `matplotlib.testing.setup()`, which seems to be the most stable way to go and in the future restrict ourselves to that? I guess this is closer to a reliable test setup for all the images than the current solution via `mpl.use(""agg"")`. Also the name suggests that matplotlib does it this way. But you did some research at the time when introducing the first tests, right?. Thanks for the comment on the PAGA notebook, too, @ivirshup. I'll make sure that I didn't hard-code anything into the plotting functions that might collide with anything else happening on travis... but it's astonishing... In the meanwhile I work-around with a data-base test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317#issuecomment-435729565
https://github.com/scverse/scanpy/issues/317#issuecomment-435729565:79,Testability,test,testing,79,"OK! Thanks! @fidelram Should we simply regenerate all images using `matplotlib.testing.setup()`, which seems to be the most stable way to go and in the future restrict ourselves to that? I guess this is closer to a reliable test setup for all the images than the current solution via `mpl.use(""agg"")`. Also the name suggests that matplotlib does it this way. But you did some research at the time when introducing the first tests, right?. Thanks for the comment on the PAGA notebook, too, @ivirshup. I'll make sure that I didn't hard-code anything into the plotting functions that might collide with anything else happening on travis... but it's astonishing... In the meanwhile I work-around with a data-base test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317#issuecomment-435729565
https://github.com/scverse/scanpy/issues/317#issuecomment-435729565:224,Testability,test,test,224,"OK! Thanks! @fidelram Should we simply regenerate all images using `matplotlib.testing.setup()`, which seems to be the most stable way to go and in the future restrict ourselves to that? I guess this is closer to a reliable test setup for all the images than the current solution via `mpl.use(""agg"")`. Also the name suggests that matplotlib does it this way. But you did some research at the time when introducing the first tests, right?. Thanks for the comment on the PAGA notebook, too, @ivirshup. I'll make sure that I didn't hard-code anything into the plotting functions that might collide with anything else happening on travis... but it's astonishing... In the meanwhile I work-around with a data-base test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317#issuecomment-435729565
https://github.com/scverse/scanpy/issues/317#issuecomment-435729565:424,Testability,test,tests,424,"OK! Thanks! @fidelram Should we simply regenerate all images using `matplotlib.testing.setup()`, which seems to be the most stable way to go and in the future restrict ourselves to that? I guess this is closer to a reliable test setup for all the images than the current solution via `mpl.use(""agg"")`. Also the name suggests that matplotlib does it this way. But you did some research at the time when introducing the first tests, right?. Thanks for the comment on the PAGA notebook, too, @ivirshup. I'll make sure that I didn't hard-code anything into the plotting functions that might collide with anything else happening on travis... but it's astonishing... In the meanwhile I work-around with a data-base test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317#issuecomment-435729565
https://github.com/scverse/scanpy/issues/317#issuecomment-435729565:709,Testability,test,test,709,"OK! Thanks! @fidelram Should we simply regenerate all images using `matplotlib.testing.setup()`, which seems to be the most stable way to go and in the future restrict ourselves to that? I guess this is closer to a reliable test setup for all the images than the current solution via `mpl.use(""agg"")`. Also the name suggests that matplotlib does it this way. But you did some research at the time when introducing the first tests, right?. Thanks for the comment on the PAGA notebook, too, @ivirshup. I'll make sure that I didn't hard-code anything into the plotting functions that might collide with anything else happening on travis... but it's astonishing... In the meanwhile I work-around with a data-base test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317#issuecomment-435729565
https://github.com/scverse/scanpy/issues/317#issuecomment-435729565:32,Usability,simpl,simply,32,"OK! Thanks! @fidelram Should we simply regenerate all images using `matplotlib.testing.setup()`, which seems to be the most stable way to go and in the future restrict ourselves to that? I guess this is closer to a reliable test setup for all the images than the current solution via `mpl.use(""agg"")`. Also the name suggests that matplotlib does it this way. But you did some research at the time when introducing the first tests, right?. Thanks for the comment on the PAGA notebook, too, @ivirshup. I'll make sure that I didn't hard-code anything into the plotting functions that might collide with anything else happening on travis... but it's astonishing... In the meanwhile I work-around with a data-base test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317#issuecomment-435729565
https://github.com/scverse/scanpy/issues/317#issuecomment-435785931:21,Testability,test,testing,21,I didn't know about `testing.setup()` I will take a look. Seems very promising.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317#issuecomment-435785931
https://github.com/scverse/scanpy/issues/317#issuecomment-453901572:18,Availability,error,errors,18,No longer getting errors on plotting tests. Was this being actively worked on? I think it's ready to close otherwise.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317#issuecomment-453901572
https://github.com/scverse/scanpy/issues/317#issuecomment-453901572:37,Testability,test,tests,37,No longer getting errors on plotting tests. Was this being actively worked on? I think it's ready to close otherwise.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317#issuecomment-453901572
https://github.com/scverse/scanpy/issues/317#issuecomment-453920643:11,Testability,test,tests,11,Indeed the tests have been modified and now `testing.setup` is being used. Thanks for the tip. You can close this issue.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317#issuecomment-453920643
https://github.com/scverse/scanpy/issues/317#issuecomment-453920643:45,Testability,test,testing,45,Indeed the tests have been modified and now `testing.setup` is being used. Thanks for the tip. You can close this issue.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317#issuecomment-453920643
https://github.com/scverse/scanpy/issues/317#issuecomment-456033231:104,Testability,test,tests,104,"Yeah, @fidelram added it in this PR: https://github.com/theislab/scanpy/pull/369. :smile: It's awesome, tests run through for me, too, everywhere... . PS: Only thing that remains is the unnecessarily long runtime due to too-large test data in a couple of instances. I'll take care of it very soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317#issuecomment-456033231
https://github.com/scverse/scanpy/issues/317#issuecomment-456033231:230,Testability,test,test,230,"Yeah, @fidelram added it in this PR: https://github.com/theislab/scanpy/pull/369. :smile: It's awesome, tests run through for me, too, everywhere... . PS: Only thing that remains is the unnecessarily long runtime due to too-large test data in a couple of instances. I'll take care of it very soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317#issuecomment-456033231
https://github.com/scverse/scanpy/issues/318#issuecomment-431813153:15,Modifiability,variab,variable,15,"Very strange. `variable` is some assigned name after an internal `pandas.melt`. . First, I would not recommend to plot all `adata.var_names` unless they are fewer (<30). But that seems not to be the problem. To discard a problem with seaborn violin plot, can you try `sc.pl.matrixplot` instead?. Also, do you get the same output in both cases after. ```; adata.obs.head(); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/318#issuecomment-431813153
https://github.com/scverse/scanpy/issues/319#issuecomment-432357859:173,Deployability,update,updated,173,"UMAP also has no meaning attached when clusters are completely disconnected (Supplemental Figure 10 of [this](https://rawgit.com/falexwolf/paga_paper/master/paga.pdf), soon updated on [here](https://doi.org/10.1101/208819) on bioRxiv and finally in a journal...); and I'd tend to think that this is such a case. Then, UMAP's parameters have to be adjusted (mostly `min_disd` and `spread`). It's true that UMAP has less tendency to tear apart connected things than tSNE. Overall, it's more faithful to the global topology.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/319#issuecomment-432357859
https://github.com/scverse/scanpy/issues/319#issuecomment-432482743:23,Usability,feedback,feedback,23,"Thank you all for your feedback here - that was helpful. I'll close this so it doesn't look like an issue needs to be handled, but please, do continue any discussion.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/319#issuecomment-432482743
https://github.com/scverse/scanpy/issues/320#issuecomment-432491475:98,Deployability,update,update,98,"Hi Alex, thank you for your quick response! I contacted Sten over at loompy and he just pushed an update that allows for a little more flexibility when reading in loom files. It now works for me. The change he applied only applies to the loompy function `loompy.connect`, so I think I would still get this same problem when using scanpy function `read_loom`. I have the latest version of loompy. I don't think this is something that necessarily needs to be fixed on your end. It sounds like the loom format has changed a little bit, and maybe the people who made the loom file I was using did not follow all the rules when making the file. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/320#issuecomment-432491475
https://github.com/scverse/scanpy/pull/321#issuecomment-432347980:73,Deployability,update,updates,73,"Thank you!. One further thing to consider: with all these frequent image updates the repository will at some point explode in size. In all the image-based tests, we should use the smallest sizes possible. Images are already relatively small, but we can further reduce the size in the future. No necessary to remake all of them now, but something to keep in mind for future PRs. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/321#issuecomment-432347980
https://github.com/scverse/scanpy/pull/321#issuecomment-432347980:261,Energy Efficiency,reduce,reduce,261,"Thank you!. One further thing to consider: with all these frequent image updates the repository will at some point explode in size. In all the image-based tests, we should use the smallest sizes possible. Images are already relatively small, but we can further reduce the size in the future. No necessary to remake all of them now, but something to keep in mind for future PRs. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/321#issuecomment-432347980
https://github.com/scverse/scanpy/pull/321#issuecomment-432347980:155,Testability,test,tests,155,"Thank you!. One further thing to consider: with all these frequent image updates the repository will at some point explode in size. In all the image-based tests, we should use the smallest sizes possible. Images are already relatively small, but we can further reduce the size in the future. No necessary to remake all of them now, but something to keep in mind for future PRs. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/321#issuecomment-432347980
https://github.com/scverse/scanpy/issues/323#issuecomment-2041512845:4114,Availability,Error,Error,4114,"vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):; --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(; [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:125) filename,; [126](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:126) backed=backed,; [127](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:127) sheet=sheet,; [128](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:128) ext=ext,; ...; [65](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:65) anno = anno.copy(deep=False); [66](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:66) if not is_string_dtype(anno.index):. ValueError: Observations annot. `var` must have as many rows as `X` has columns (1), but has 33538 rows.; Error raised while reading key '' of <class 'h5py._hl.files.File'> from /; Output is truncated. View as a [scrollable element](command:cellOutput.enableScrolling?21de049a-219f-41e8-9d9d-5aabf61d8031) or open in a [text editor](command:workbench.action.openLargeOutput?21de049a-219f-41e8-9d9d-5aabf61d8031). Adjust cell output [settings](command:workbench.action.openSettings?%5B%22%40tag%3AnotebookOutputLayout%22%5D)...; ```; This is my version:; ```; print(sc.__version__); print(ad.__version__); 1.10.0; 0.10.6; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323#issuecomment-2041512845
https://github.com/scverse/scanpy/issues/323#issuecomment-2041512845:893,Integrability,wrap,wrapper,893,"I run into this bug too.; when i run:; ```; import scanpy as sc; import anndata as ad; adata = sc.read(filepath); ```; it turns out:; ```; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); Cell In[6], [line 8](vscode-notebook-cell:?execution_count=6&line=8); [6](vscode-notebook-cell:?execution_count=6&line=6) if filename.endswith('.h5ad'):; [7](vscode-notebook-cell:?execution_count=6&line=7) filepath = os.path.join(directory, filename); ----> [8](vscode-notebook-cell:?execution_count=6&line=8) adata = sc.read(filepath); [10](vscode-notebook-cell:?execution_count=6&line=10) # Rename columns with periods in `.obs` attribute; [11](vscode-notebook-cell:?execution_count=6&line=11) for col in adata.obs.columns:. File /data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); [77](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:77) @wraps(fn); [78](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:78) def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; [79](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:79) if len(args_all) <= n_positional:; ---> [80](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80) return fn(*args_all, **kw); [82](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args; [83](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323#issuecomment-2041512845
https://github.com/scverse/scanpy/issues/323#issuecomment-2041512845:1107,Integrability,wrap,wraps,1107,"; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); Cell In[6], [line 8](vscode-notebook-cell:?execution_count=6&line=8); [6](vscode-notebook-cell:?execution_count=6&line=6) if filename.endswith('.h5ad'):; [7](vscode-notebook-cell:?execution_count=6&line=7) filepath = os.path.join(directory, filename); ----> [8](vscode-notebook-cell:?execution_count=6&line=8) adata = sc.read(filepath); [10](vscode-notebook-cell:?execution_count=6&line=10) # Rename columns with periods in `.obs` attribute; [11](vscode-notebook-cell:?execution_count=6&line=11) for col in adata.obs.columns:. File /data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); [77](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:77) @wraps(fn); [78](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:78) def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; [79](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:79) if len(args_all) <= n_positional:; ---> [80](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80) return fn(*args_all, **kw); [82](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args; [83](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:83) args_pos, args_rest = args_all[:n_positional], args_al",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323#issuecomment-2041512845
https://github.com/scverse/scanpy/issues/323#issuecomment-2041512845:2311,Performance,cache,cache,2311,"rgs, **kw: P.kwargs) -> R:; [79](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:79) if len(args_all) <= n_positional:; ---> [80](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80) return fn(*args_all, **kw); [82](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args; [83](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:83) args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs); [122](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:122) filename = Path(filename) # allow passing strings; [123](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):; --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(; [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:125) filename,; [126](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:126) backed=backed,; [127](https://vscode-remote+ssh-002dremote-002b",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323#issuecomment-2041512845
https://github.com/scverse/scanpy/issues/324#issuecomment-433010405:129,Energy Efficiency,reduce,reduced,129,"So I've been sitting offline with @VolkerBergen trying to get to the bottom of this. It seems that the precision of `adata.X` is reduced after subsetting. This is the case when using either of:. ```; adata_hvg = adata_hvg[:, disp_filter['gene_subset']]; adata_hvg._inplace_subset_var(disp_filter['gene_subset']); ```. Either way `adata[:,disp_filter['gene_subset']].X` gives a higher precision than `adata_hvg.X`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/324#issuecomment-433010405
https://github.com/scverse/scanpy/issues/324#issuecomment-433383722:150,Testability,test,test,150,I have not set the precision to float64 manually anywhere. It may however be the case that ComBat batch correction automatically uses float64. I will test and let you know.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/324#issuecomment-433383722
https://github.com/scverse/scanpy/issues/324#issuecomment-433441386:110,Deployability,update,update,110,So it seems ComBat outputs np.float64 😐. I assume with the anndata fix that should be fine now though? I will update and rerun...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/324#issuecomment-433441386
https://github.com/scverse/scanpy/issues/325#issuecomment-433258438:85,Usability,learn,learn,85,"1. It gives exactly the same results when rerunning on the same platform, but scikit-learn's randomized PCA sometimes fails to reproduce exactly the same results when run on different platforms. That's why since some time, the warning is output.; 2. Setting `svd_solver='arpack'` resolves that problem.; 3. That's probably hard, as at the time, the results were produced using the randomized version expecting that setting the same seed on a different would reproduce this also elsewhere.; 4. What I uploaded for you at the time were the clustering results shown in the PAGA paper and the Scanpy paper, they are still linked in the issue in the PAGA repo: https://github.com/theislab/paga/issues/1#issuecomment-404263982; I can easily upload these clustering results also to https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells, that's no problem at all. There was one issue with right? The vector is shifted by one cell or something? Can I change something so that the problem you had at the time doesn't come up again?. Sorry about the whole issue!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325#issuecomment-433258438
https://github.com/scverse/scanpy/issues/325#issuecomment-433334926:496,Availability,error,error,496,"Starting from the end: I think if you could upload the clustering results from the Scanpy paper / PAGA preprint to the scanpy github repo, it would be great. I still have the dropbox link of course, but I guess in the long run it's better if that file was located here and linked from the https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells page. The issue with 1 cell missing was because I did not specify `header=None` when loading it with Pandas :) So my error, not yours. The file is correct as is. That said, I am worried about the influence the random seed in randomized PCA seems to give in this case. Let me show you how it looks:. ![mln-tsne-clustering-comparison](https://user-images.githubusercontent.com/8970231/47555195-71af9480-d90b-11e8-85fb-a3e8dcb7a66f.png). I would be fine with some cells getting into other clusters depending on the random seed, and it would even be okay if small clusters changed their identities, but what we see here is a very drastic change of the cluster structure. Are you sure that the only difference is the randomized PCA outcome? Can it be that some of the default parameters in `sc.pp.recipe_zheng17`, `sc.pp.neighbors`, or `sc.tl.louvain` changed since when you ran the clustering? The scanpy code I posted above is the full code I used, and I ran it yesterday after updating scanpy via pip. BTW, the visualization above is taken from https://www.biorxiv.org/content/early/2018/10/25/453449 which we posted yesterday. Any comments very welcome! I hope you don't mind being thanked in the acknowledgements!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325#issuecomment-433334926
https://github.com/scverse/scanpy/issues/325#issuecomment-433334926:874,Integrability,depend,depending,874,"Starting from the end: I think if you could upload the clustering results from the Scanpy paper / PAGA preprint to the scanpy github repo, it would be great. I still have the dropbox link of course, but I guess in the long run it's better if that file was located here and linked from the https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells page. The issue with 1 cell missing was because I did not specify `header=None` when loading it with Pandas :) So my error, not yours. The file is correct as is. That said, I am worried about the influence the random seed in randomized PCA seems to give in this case. Let me show you how it looks:. ![mln-tsne-clustering-comparison](https://user-images.githubusercontent.com/8970231/47555195-71af9480-d90b-11e8-85fb-a3e8dcb7a66f.png). I would be fine with some cells getting into other clusters depending on the random seed, and it would even be okay if small clusters changed their identities, but what we see here is a very drastic change of the cluster structure. Are you sure that the only difference is the randomized PCA outcome? Can it be that some of the default parameters in `sc.pp.recipe_zheng17`, `sc.pp.neighbors`, or `sc.tl.louvain` changed since when you ran the clustering? The scanpy code I posted above is the full code I used, and I ran it yesterday after updating scanpy via pip. BTW, the visualization above is taken from https://www.biorxiv.org/content/early/2018/10/25/453449 which we posted yesterday. Any comments very welcome! I hope you don't mind being thanked in the acknowledgements!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325#issuecomment-433334926
https://github.com/scverse/scanpy/issues/325#issuecomment-433334926:464,Performance,load,loading,464,"Starting from the end: I think if you could upload the clustering results from the Scanpy paper / PAGA preprint to the scanpy github repo, it would be great. I still have the dropbox link of course, but I guess in the long run it's better if that file was located here and linked from the https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells page. The issue with 1 cell missing was because I did not specify `header=None` when loading it with Pandas :) So my error, not yours. The file is correct as is. That said, I am worried about the influence the random seed in randomized PCA seems to give in this case. Let me show you how it looks:. ![mln-tsne-clustering-comparison](https://user-images.githubusercontent.com/8970231/47555195-71af9480-d90b-11e8-85fb-a3e8dcb7a66f.png). I would be fine with some cells getting into other clusters depending on the random seed, and it would even be okay if small clusters changed their identities, but what we see here is a very drastic change of the cluster structure. Are you sure that the only difference is the randomized PCA outcome? Can it be that some of the default parameters in `sc.pp.recipe_zheng17`, `sc.pp.neighbors`, or `sc.tl.louvain` changed since when you ran the clustering? The scanpy code I posted above is the full code I used, and I ran it yesterday after updating scanpy via pip. BTW, the visualization above is taken from https://www.biorxiv.org/content/early/2018/10/25/453449 which we posted yesterday. Any comments very welcome! I hope you don't mind being thanked in the acknowledgements!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325#issuecomment-433334926
https://github.com/scverse/scanpy/issues/325#issuecomment-435735671:212,Integrability,depend,depending,212,Sorry for the slow response! Congrats on the very nice preprint and thanks for thanking me! :wink:. I'll upload the file from dropbox to `scanpy_usage`!. Regarding the result: the high PCs can change drastically depending on the platform and the random seed. I've seen clustering results changing completely after I became aware of it. I'm very sure that non of the preprocessing and other code changed at any point. What do you think?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325#issuecomment-435735671
https://github.com/scverse/scanpy/issues/325#issuecomment-435797047:14,Availability,ping,ping,14,"Great. Please ping me here when you upload the file to `scanpy_usage` and feel free to close the issue then. I'll update my script to link directly to `scanpy_usage`. > Regarding the result: the high PCs can change drastically depending on the platform and the random seed. I've seen clustering results changing completely after I became aware of it. . I don't have much experience with randomized PCA, but this is very disturbing, no? Was your feeling that the PCs themselves changed strongly (as measured, I don't know, by the %% of total captured variance, or maybe angle between subspaces, etc.), or is it rather that clustering outcome is dangerously sensitive to small changes in the data? I think this is something worth investigating.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325#issuecomment-435797047
https://github.com/scverse/scanpy/issues/325#issuecomment-435797047:114,Deployability,update,update,114,"Great. Please ping me here when you upload the file to `scanpy_usage` and feel free to close the issue then. I'll update my script to link directly to `scanpy_usage`. > Regarding the result: the high PCs can change drastically depending on the platform and the random seed. I've seen clustering results changing completely after I became aware of it. . I don't have much experience with randomized PCA, but this is very disturbing, no? Was your feeling that the PCs themselves changed strongly (as measured, I don't know, by the %% of total captured variance, or maybe angle between subspaces, etc.), or is it rather that clustering outcome is dangerously sensitive to small changes in the data? I think this is something worth investigating.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325#issuecomment-435797047
https://github.com/scverse/scanpy/issues/325#issuecomment-435797047:227,Integrability,depend,depending,227,"Great. Please ping me here when you upload the file to `scanpy_usage` and feel free to close the issue then. I'll update my script to link directly to `scanpy_usage`. > Regarding the result: the high PCs can change drastically depending on the platform and the random seed. I've seen clustering results changing completely after I became aware of it. . I don't have much experience with randomized PCA, but this is very disturbing, no? Was your feeling that the PCs themselves changed strongly (as measured, I don't know, by the %% of total captured variance, or maybe angle between subspaces, etc.), or is it rather that clustering outcome is dangerously sensitive to small changes in the data? I think this is something worth investigating.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325#issuecomment-435797047
https://github.com/scverse/scanpy/issues/325#issuecomment-436013237:108,Availability,down,downstream,108,"@LuckyMD Thanks. This agrees with what I suspected: that randomized PCA itself should be pretty stable, but downstream clustering procedures can be very unstable. I have little experience with clustering so I cannot really comment further, but this certainly should be a big red flag for taking a clustering result seriously. It's especially impressive that float32 vs float64 can cause such a difference. Did you observe this influencing t-SNE/UMAP/etc equally drastically, or did it only affect clustering so strongly?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325#issuecomment-436013237
https://github.com/scverse/scanpy/issues/325#issuecomment-436028541:323,Integrability,bridg,bridge,323,"The reason I actually noticed it in the first place was a difference in t-SNE and UMAP... the difference wasn't huge, but noticeable. Here's the issue where you can see the UMAP plots (#324). The clustering differences were actually rather small... the more worrying thing was the UMAP at the time as it appeared to show a bridge between two clusters that I didn't really expect to be there and didn't see in the float64 data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325#issuecomment-436028541
https://github.com/scverse/scanpy/issues/325#issuecomment-436040174:16,Availability,ping,ping,16,> Great. Please ping me here when you upload the file to... I uploaded the file.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325#issuecomment-436040174
https://github.com/scverse/scanpy/issues/325#issuecomment-436041937:462,Availability,robust,robustly,462,"> This agrees with what I suspected: that randomized PCA itself should be pretty stable, . No, if you look into how higher PCs vary, you see that they vary drastically depending on the seed or computational platform. That also makes sense, it's a power-method that does the computation, that runs into some unstable stuff. > PCs were similar to within a couple of decimal points,. I'm very sure that you only observed this for the first couple of PCs, which you robustly estimate. Going higher, you end up in some local minima for a subsapce; I believe that it doesn't mean it doesn't capture important variation; it just means that it's a local minimum that the algorithm converges into... something we observe all the time when training models... in the context of Lanzcos and other algorithms powering SVD, PCA etc., it's usually a nuisance that you have that instability when you go higher in iterations; but also there, people just use the results even if they know they don't get the *exact* 50th eigen vector...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325#issuecomment-436041937
https://github.com/scverse/scanpy/issues/325#issuecomment-436041937:247,Energy Efficiency,power,power-method,247,"> This agrees with what I suspected: that randomized PCA itself should be pretty stable, . No, if you look into how higher PCs vary, you see that they vary drastically depending on the seed or computational platform. That also makes sense, it's a power-method that does the computation, that runs into some unstable stuff. > PCs were similar to within a couple of decimal points,. I'm very sure that you only observed this for the first couple of PCs, which you robustly estimate. Going higher, you end up in some local minima for a subsapce; I believe that it doesn't mean it doesn't capture important variation; it just means that it's a local minimum that the algorithm converges into... something we observe all the time when training models... in the context of Lanzcos and other algorithms powering SVD, PCA etc., it's usually a nuisance that you have that instability when you go higher in iterations; but also there, people just use the results even if they know they don't get the *exact* 50th eigen vector...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325#issuecomment-436041937
https://github.com/scverse/scanpy/issues/325#issuecomment-436041937:796,Energy Efficiency,power,powering,796,"> This agrees with what I suspected: that randomized PCA itself should be pretty stable, . No, if you look into how higher PCs vary, you see that they vary drastically depending on the seed or computational platform. That also makes sense, it's a power-method that does the computation, that runs into some unstable stuff. > PCs were similar to within a couple of decimal points,. I'm very sure that you only observed this for the first couple of PCs, which you robustly estimate. Going higher, you end up in some local minima for a subsapce; I believe that it doesn't mean it doesn't capture important variation; it just means that it's a local minimum that the algorithm converges into... something we observe all the time when training models... in the context of Lanzcos and other algorithms powering SVD, PCA etc., it's usually a nuisance that you have that instability when you go higher in iterations; but also there, people just use the results even if they know they don't get the *exact* 50th eigen vector...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325#issuecomment-436041937
https://github.com/scverse/scanpy/issues/325#issuecomment-436041937:168,Integrability,depend,depending,168,"> This agrees with what I suspected: that randomized PCA itself should be pretty stable, . No, if you look into how higher PCs vary, you see that they vary drastically depending on the seed or computational platform. That also makes sense, it's a power-method that does the computation, that runs into some unstable stuff. > PCs were similar to within a couple of decimal points,. I'm very sure that you only observed this for the first couple of PCs, which you robustly estimate. Going higher, you end up in some local minima for a subsapce; I believe that it doesn't mean it doesn't capture important variation; it just means that it's a local minimum that the algorithm converges into... something we observe all the time when training models... in the context of Lanzcos and other algorithms powering SVD, PCA etc., it's usually a nuisance that you have that instability when you go higher in iterations; but also there, people just use the results even if they know they don't get the *exact* 50th eigen vector...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325#issuecomment-436041937
https://github.com/scverse/scanpy/issues/328#issuecomment-435736335:299,Availability,error,error,299,"Sorry for the super-late response! I just worked through almost 60 issues starting with the most recent, this is the last one... Sorry about that. `paga_path` requires computing a pseudotime before-hand as one needs to order cells at single-cell resolution along the path. I added a more meaningful error message stating that. PS: Now, there is also a test for PAGA [here](https://github.com/theislab/scanpy/blob/master/scanpy/tests/notebooks/test_paga_paul15_subsampled.py), making sure that the canonical use ([here](https://nbviewer.jupyter.org/github/theislab/paga/blob/master/blood/paul15/paul15.ipynb)) remains unchanged.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/328#issuecomment-435736335
https://github.com/scverse/scanpy/issues/328#issuecomment-435736335:305,Integrability,message,message,305,"Sorry for the super-late response! I just worked through almost 60 issues starting with the most recent, this is the last one... Sorry about that. `paga_path` requires computing a pseudotime before-hand as one needs to order cells at single-cell resolution along the path. I added a more meaningful error message stating that. PS: Now, there is also a test for PAGA [here](https://github.com/theislab/scanpy/blob/master/scanpy/tests/notebooks/test_paga_paul15_subsampled.py), making sure that the canonical use ([here](https://nbviewer.jupyter.org/github/theislab/paga/blob/master/blood/paul15/paul15.ipynb)) remains unchanged.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/328#issuecomment-435736335
https://github.com/scverse/scanpy/issues/328#issuecomment-435736335:352,Testability,test,test,352,"Sorry for the super-late response! I just worked through almost 60 issues starting with the most recent, this is the last one... Sorry about that. `paga_path` requires computing a pseudotime before-hand as one needs to order cells at single-cell resolution along the path. I added a more meaningful error message stating that. PS: Now, there is also a test for PAGA [here](https://github.com/theislab/scanpy/blob/master/scanpy/tests/notebooks/test_paga_paul15_subsampled.py), making sure that the canonical use ([here](https://nbviewer.jupyter.org/github/theislab/paga/blob/master/blood/paul15/paul15.ipynb)) remains unchanged.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/328#issuecomment-435736335
https://github.com/scverse/scanpy/issues/328#issuecomment-435736335:427,Testability,test,tests,427,"Sorry for the super-late response! I just worked through almost 60 issues starting with the most recent, this is the last one... Sorry about that. `paga_path` requires computing a pseudotime before-hand as one needs to order cells at single-cell resolution along the path. I added a more meaningful error message stating that. PS: Now, there is also a test for PAGA [here](https://github.com/theislab/scanpy/blob/master/scanpy/tests/notebooks/test_paga_paul15_subsampled.py), making sure that the canonical use ([here](https://nbviewer.jupyter.org/github/theislab/paga/blob/master/blood/paul15/paul15.ipynb)) remains unchanged.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/328#issuecomment-435736335
https://github.com/scverse/scanpy/pull/330#issuecomment-434090862:200,Deployability,release,release,200,"The whole PR is pretty awesome already! I wrote some comments... Can you also add the function to the docs, cross reference the deprecated and the new function in the Notes section and add it to the [release notes](https://github.com/theislab/scanpy/blob/master/docs/release_notes.rst)?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/330#issuecomment-434090862
https://github.com/scverse/scanpy/issues/331#issuecomment-435733122:93,Modifiability,variab,variables,93,"Sorry for the late response, Joshua! Could it be that your dataset has less than 50 cells or variables or something like this?. I believe that you're stating this. Computing a 50 dimensional PCA with less than 50 observations is probably not possible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/331#issuecomment-435733122
https://github.com/scverse/scanpy/issues/332#issuecomment-433745600:20,Availability,error,errors,20,"Addendum: different errors are generated depending on which axis is first sliced. The data set I'm loading is a dense matrix. ```; >>> data.X.dtype; dtype('<f4'); >>> data[:,0][0,:]; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1303, in __getitem__; return self._getitem_view(index); File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1307, in _getitem_view; return AnnData(self, oidx=oidx, vidx=vidx, asview=True); File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 669, in __init__; self._init_as_view(X, oidx, vidx); File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 694, in _init_as_view; uns_new = deepcopy(self._adata_ref._uns); File ""/usr/lib/python3.6/copy.py"", line 180, in deepcopy; y = _reconstruct(x, memo, *rv); File ""/usr/lib/python3.6/copy.py"", line 307, in _reconstruct; y[key] = value; File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 444, in __setitem__; _init_actual_AnnData(adata_view); File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 367, in _init_actual_AnnData; adata_view._init_as_actual(adata_view.copy()); File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 880, in _init_as_actual; self._check_dimensions(); File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1879, in _check_dimensions; .format(self._n_obs, self._obs.shape[0])); ValueError: Observations annot. `obs` must have number of rows of `X` (1), but has 2638 rows.; >>> data[0,:][:,0]; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1303, in __getitem__; return self._getitem_view(index); File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1307, in _getitem_view; return AnnData(self, oidx=oidx, vidx=vidx, asview=True); File ""/cellxg",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332#issuecomment-433745600
https://github.com/scverse/scanpy/issues/332#issuecomment-433745600:41,Integrability,depend,depending,41,"Addendum: different errors are generated depending on which axis is first sliced. The data set I'm loading is a dense matrix. ```; >>> data.X.dtype; dtype('<f4'); >>> data[:,0][0,:]; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1303, in __getitem__; return self._getitem_view(index); File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1307, in _getitem_view; return AnnData(self, oidx=oidx, vidx=vidx, asview=True); File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 669, in __init__; self._init_as_view(X, oidx, vidx); File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 694, in _init_as_view; uns_new = deepcopy(self._adata_ref._uns); File ""/usr/lib/python3.6/copy.py"", line 180, in deepcopy; y = _reconstruct(x, memo, *rv); File ""/usr/lib/python3.6/copy.py"", line 307, in _reconstruct; y[key] = value; File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 444, in __setitem__; _init_actual_AnnData(adata_view); File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 367, in _init_actual_AnnData; adata_view._init_as_actual(adata_view.copy()); File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 880, in _init_as_actual; self._check_dimensions(); File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1879, in _check_dimensions; .format(self._n_obs, self._obs.shape[0])); ValueError: Observations annot. `obs` must have number of rows of `X` (1), but has 2638 rows.; >>> data[0,:][:,0]; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1303, in __getitem__; return self._getitem_view(index); File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1307, in _getitem_view; return AnnData(self, oidx=oidx, vidx=vidx, asview=True); File ""/cellxg",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332#issuecomment-433745600
https://github.com/scverse/scanpy/issues/332#issuecomment-433745600:99,Performance,load,loading,99,"Addendum: different errors are generated depending on which axis is first sliced. The data set I'm loading is a dense matrix. ```; >>> data.X.dtype; dtype('<f4'); >>> data[:,0][0,:]; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1303, in __getitem__; return self._getitem_view(index); File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1307, in _getitem_view; return AnnData(self, oidx=oidx, vidx=vidx, asview=True); File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 669, in __init__; self._init_as_view(X, oidx, vidx); File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 694, in _init_as_view; uns_new = deepcopy(self._adata_ref._uns); File ""/usr/lib/python3.6/copy.py"", line 180, in deepcopy; y = _reconstruct(x, memo, *rv); File ""/usr/lib/python3.6/copy.py"", line 307, in _reconstruct; y[key] = value; File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 444, in __setitem__; _init_actual_AnnData(adata_view); File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 367, in _init_actual_AnnData; adata_view._init_as_actual(adata_view.copy()); File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 880, in _init_as_actual; self._check_dimensions(); File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1879, in _check_dimensions; .format(self._n_obs, self._obs.shape[0])); ValueError: Observations annot. `obs` must have number of rows of `X` (1), but has 2638 rows.; >>> data[0,:][:,0]; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1303, in __getitem__; return self._getitem_view(index); File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1307, in _getitem_view; return AnnData(self, oidx=oidx, vidx=vidx, asview=True); File ""/cellxg",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332#issuecomment-433745600
https://github.com/scverse/scanpy/issues/332#issuecomment-434005191:19,Testability,test,test,19,"More comprehensive test, similar to the current AnnData slicing tests (no use of external data). Each try/except block will (currently) fail on its last assertion. ```; import sys, traceback; import numpy as np; import scanpy.api as sc. adata = sc.AnnData(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])). # integer indexing; print(""\n>>> integer indexing, obs first""); try:; assert adata[0:2, :][:, 0:2].X.tolist() == [[1,2], [4,5]]; assert adata[0, 0].X.tolist() == 1; assert adata[0:1, 0:1].X.tolist() == 1; assert adata[0, :][:, 0].X.tolist() == 1. except Exception as e:; traceback.print_exc(file=sys.stdout). print(""\n>>> integer indexing, vars first""); try:; assert adata[:, 0:2][0:2, :].X.tolist() == [[1,2], [4,5]]; assert adata[0, 0].X.tolist() == 1; assert adata[0:1, 0:1].X.tolist() == 1; assert adata[:, 0][0, :].X.tolist() == 1. except Exception as e:; traceback.print_exc(file=sys.stdout). # boolean indexing; print(""\n>>> boolean indexing, obs first""); try:; obs_selector = np.zeros(len(adata.obs), dtype=bool); vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]; vars_selector[:] = [True, True, False]; assert adata[obs_selector, :][:, vars_selector].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]; vars_selector[:] = [True, False, False]; assert adata[obs_selector, :][:, vars_selector].X.tolist() == 1. except Exception as e:; traceback.print_exc(file=sys.stdout). print(""\n>>> boolean indexing, vars first""); try:; obs_selector = np.zeros(len(adata.obs), dtype=bool); vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]; vars_selector[:] = [True, True, False]; assert adata[:, vars_selector][obs_selector, :].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]; vars_selector[:] = [True, False, False]; assert adata[:, vars_selector][obs_selector, :].X.tolist() == 1. except Exception as e:; traceback.print_exc(file=sys.stdout); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332#issuecomment-434005191
https://github.com/scverse/scanpy/issues/332#issuecomment-434005191:64,Testability,test,tests,64,"More comprehensive test, similar to the current AnnData slicing tests (no use of external data). Each try/except block will (currently) fail on its last assertion. ```; import sys, traceback; import numpy as np; import scanpy.api as sc. adata = sc.AnnData(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])). # integer indexing; print(""\n>>> integer indexing, obs first""); try:; assert adata[0:2, :][:, 0:2].X.tolist() == [[1,2], [4,5]]; assert adata[0, 0].X.tolist() == 1; assert adata[0:1, 0:1].X.tolist() == 1; assert adata[0, :][:, 0].X.tolist() == 1. except Exception as e:; traceback.print_exc(file=sys.stdout). print(""\n>>> integer indexing, vars first""); try:; assert adata[:, 0:2][0:2, :].X.tolist() == [[1,2], [4,5]]; assert adata[0, 0].X.tolist() == 1; assert adata[0:1, 0:1].X.tolist() == 1; assert adata[:, 0][0, :].X.tolist() == 1. except Exception as e:; traceback.print_exc(file=sys.stdout). # boolean indexing; print(""\n>>> boolean indexing, obs first""); try:; obs_selector = np.zeros(len(adata.obs), dtype=bool); vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]; vars_selector[:] = [True, True, False]; assert adata[obs_selector, :][:, vars_selector].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]; vars_selector[:] = [True, False, False]; assert adata[obs_selector, :][:, vars_selector].X.tolist() == 1. except Exception as e:; traceback.print_exc(file=sys.stdout). print(""\n>>> boolean indexing, vars first""); try:; obs_selector = np.zeros(len(adata.obs), dtype=bool); vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]; vars_selector[:] = [True, True, False]; assert adata[:, vars_selector][obs_selector, :].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]; vars_selector[:] = [True, False, False]; assert adata[:, vars_selector][obs_selector, :].X.tolist() == 1. except Exception as e:; traceback.print_exc(file=sys.stdout); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332#issuecomment-434005191
https://github.com/scverse/scanpy/issues/332#issuecomment-434005191:153,Testability,assert,assertion,153,"More comprehensive test, similar to the current AnnData slicing tests (no use of external data). Each try/except block will (currently) fail on its last assertion. ```; import sys, traceback; import numpy as np; import scanpy.api as sc. adata = sc.AnnData(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])). # integer indexing; print(""\n>>> integer indexing, obs first""); try:; assert adata[0:2, :][:, 0:2].X.tolist() == [[1,2], [4,5]]; assert adata[0, 0].X.tolist() == 1; assert adata[0:1, 0:1].X.tolist() == 1; assert adata[0, :][:, 0].X.tolist() == 1. except Exception as e:; traceback.print_exc(file=sys.stdout). print(""\n>>> integer indexing, vars first""); try:; assert adata[:, 0:2][0:2, :].X.tolist() == [[1,2], [4,5]]; assert adata[0, 0].X.tolist() == 1; assert adata[0:1, 0:1].X.tolist() == 1; assert adata[:, 0][0, :].X.tolist() == 1. except Exception as e:; traceback.print_exc(file=sys.stdout). # boolean indexing; print(""\n>>> boolean indexing, obs first""); try:; obs_selector = np.zeros(len(adata.obs), dtype=bool); vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]; vars_selector[:] = [True, True, False]; assert adata[obs_selector, :][:, vars_selector].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]; vars_selector[:] = [True, False, False]; assert adata[obs_selector, :][:, vars_selector].X.tolist() == 1. except Exception as e:; traceback.print_exc(file=sys.stdout). print(""\n>>> boolean indexing, vars first""); try:; obs_selector = np.zeros(len(adata.obs), dtype=bool); vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]; vars_selector[:] = [True, True, False]; assert adata[:, vars_selector][obs_selector, :].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]; vars_selector[:] = [True, False, False]; assert adata[:, vars_selector][obs_selector, :].X.tolist() == 1. except Exception as e:; traceback.print_exc(file=sys.stdout); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332#issuecomment-434005191
https://github.com/scverse/scanpy/issues/332#issuecomment-434005191:372,Testability,assert,assert,372,"More comprehensive test, similar to the current AnnData slicing tests (no use of external data). Each try/except block will (currently) fail on its last assertion. ```; import sys, traceback; import numpy as np; import scanpy.api as sc. adata = sc.AnnData(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])). # integer indexing; print(""\n>>> integer indexing, obs first""); try:; assert adata[0:2, :][:, 0:2].X.tolist() == [[1,2], [4,5]]; assert adata[0, 0].X.tolist() == 1; assert adata[0:1, 0:1].X.tolist() == 1; assert adata[0, :][:, 0].X.tolist() == 1. except Exception as e:; traceback.print_exc(file=sys.stdout). print(""\n>>> integer indexing, vars first""); try:; assert adata[:, 0:2][0:2, :].X.tolist() == [[1,2], [4,5]]; assert adata[0, 0].X.tolist() == 1; assert adata[0:1, 0:1].X.tolist() == 1; assert adata[:, 0][0, :].X.tolist() == 1. except Exception as e:; traceback.print_exc(file=sys.stdout). # boolean indexing; print(""\n>>> boolean indexing, obs first""); try:; obs_selector = np.zeros(len(adata.obs), dtype=bool); vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]; vars_selector[:] = [True, True, False]; assert adata[obs_selector, :][:, vars_selector].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]; vars_selector[:] = [True, False, False]; assert adata[obs_selector, :][:, vars_selector].X.tolist() == 1. except Exception as e:; traceback.print_exc(file=sys.stdout). print(""\n>>> boolean indexing, vars first""); try:; obs_selector = np.zeros(len(adata.obs), dtype=bool); vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]; vars_selector[:] = [True, True, False]; assert adata[:, vars_selector][obs_selector, :].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]; vars_selector[:] = [True, False, False]; assert adata[:, vars_selector][obs_selector, :].X.tolist() == 1. except Exception as e:; traceback.print_exc(file=sys.stdout); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332#issuecomment-434005191
https://github.com/scverse/scanpy/issues/332#issuecomment-434005191:431,Testability,assert,assert,431,"More comprehensive test, similar to the current AnnData slicing tests (no use of external data). Each try/except block will (currently) fail on its last assertion. ```; import sys, traceback; import numpy as np; import scanpy.api as sc. adata = sc.AnnData(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])). # integer indexing; print(""\n>>> integer indexing, obs first""); try:; assert adata[0:2, :][:, 0:2].X.tolist() == [[1,2], [4,5]]; assert adata[0, 0].X.tolist() == 1; assert adata[0:1, 0:1].X.tolist() == 1; assert adata[0, :][:, 0].X.tolist() == 1. except Exception as e:; traceback.print_exc(file=sys.stdout). print(""\n>>> integer indexing, vars first""); try:; assert adata[:, 0:2][0:2, :].X.tolist() == [[1,2], [4,5]]; assert adata[0, 0].X.tolist() == 1; assert adata[0:1, 0:1].X.tolist() == 1; assert adata[:, 0][0, :].X.tolist() == 1. except Exception as e:; traceback.print_exc(file=sys.stdout). # boolean indexing; print(""\n>>> boolean indexing, obs first""); try:; obs_selector = np.zeros(len(adata.obs), dtype=bool); vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]; vars_selector[:] = [True, True, False]; assert adata[obs_selector, :][:, vars_selector].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]; vars_selector[:] = [True, False, False]; assert adata[obs_selector, :][:, vars_selector].X.tolist() == 1. except Exception as e:; traceback.print_exc(file=sys.stdout). print(""\n>>> boolean indexing, vars first""); try:; obs_selector = np.zeros(len(adata.obs), dtype=bool); vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]; vars_selector[:] = [True, True, False]; assert adata[:, vars_selector][obs_selector, :].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]; vars_selector[:] = [True, False, False]; assert adata[:, vars_selector][obs_selector, :].X.tolist() == 1. except Exception as e:; traceback.print_exc(file=sys.stdout); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332#issuecomment-434005191
https://github.com/scverse/scanpy/issues/332#issuecomment-434005191:467,Testability,assert,assert,467,"More comprehensive test, similar to the current AnnData slicing tests (no use of external data). Each try/except block will (currently) fail on its last assertion. ```; import sys, traceback; import numpy as np; import scanpy.api as sc. adata = sc.AnnData(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])). # integer indexing; print(""\n>>> integer indexing, obs first""); try:; assert adata[0:2, :][:, 0:2].X.tolist() == [[1,2], [4,5]]; assert adata[0, 0].X.tolist() == 1; assert adata[0:1, 0:1].X.tolist() == 1; assert adata[0, :][:, 0].X.tolist() == 1. except Exception as e:; traceback.print_exc(file=sys.stdout). print(""\n>>> integer indexing, vars first""); try:; assert adata[:, 0:2][0:2, :].X.tolist() == [[1,2], [4,5]]; assert adata[0, 0].X.tolist() == 1; assert adata[0:1, 0:1].X.tolist() == 1; assert adata[:, 0][0, :].X.tolist() == 1. except Exception as e:; traceback.print_exc(file=sys.stdout). # boolean indexing; print(""\n>>> boolean indexing, obs first""); try:; obs_selector = np.zeros(len(adata.obs), dtype=bool); vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]; vars_selector[:] = [True, True, False]; assert adata[obs_selector, :][:, vars_selector].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]; vars_selector[:] = [True, False, False]; assert adata[obs_selector, :][:, vars_selector].X.tolist() == 1. except Exception as e:; traceback.print_exc(file=sys.stdout). print(""\n>>> boolean indexing, vars first""); try:; obs_selector = np.zeros(len(adata.obs), dtype=bool); vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]; vars_selector[:] = [True, True, False]; assert adata[:, vars_selector][obs_selector, :].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]; vars_selector[:] = [True, False, False]; assert adata[:, vars_selector][obs_selector, :].X.tolist() == 1. except Exception as e:; traceback.print_exc(file=sys.stdout); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332#issuecomment-434005191
https://github.com/scverse/scanpy/issues/332#issuecomment-434005191:507,Testability,assert,assert,507,"More comprehensive test, similar to the current AnnData slicing tests (no use of external data). Each try/except block will (currently) fail on its last assertion. ```; import sys, traceback; import numpy as np; import scanpy.api as sc. adata = sc.AnnData(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])). # integer indexing; print(""\n>>> integer indexing, obs first""); try:; assert adata[0:2, :][:, 0:2].X.tolist() == [[1,2], [4,5]]; assert adata[0, 0].X.tolist() == 1; assert adata[0:1, 0:1].X.tolist() == 1; assert adata[0, :][:, 0].X.tolist() == 1. except Exception as e:; traceback.print_exc(file=sys.stdout). print(""\n>>> integer indexing, vars first""); try:; assert adata[:, 0:2][0:2, :].X.tolist() == [[1,2], [4,5]]; assert adata[0, 0].X.tolist() == 1; assert adata[0:1, 0:1].X.tolist() == 1; assert adata[:, 0][0, :].X.tolist() == 1. except Exception as e:; traceback.print_exc(file=sys.stdout). # boolean indexing; print(""\n>>> boolean indexing, obs first""); try:; obs_selector = np.zeros(len(adata.obs), dtype=bool); vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]; vars_selector[:] = [True, True, False]; assert adata[obs_selector, :][:, vars_selector].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]; vars_selector[:] = [True, False, False]; assert adata[obs_selector, :][:, vars_selector].X.tolist() == 1. except Exception as e:; traceback.print_exc(file=sys.stdout). print(""\n>>> boolean indexing, vars first""); try:; obs_selector = np.zeros(len(adata.obs), dtype=bool); vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]; vars_selector[:] = [True, True, False]; assert adata[:, vars_selector][obs_selector, :].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]; vars_selector[:] = [True, False, False]; assert adata[:, vars_selector][obs_selector, :].X.tolist() == 1. except Exception as e:; traceback.print_exc(file=sys.stdout); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332#issuecomment-434005191
https://github.com/scverse/scanpy/issues/332#issuecomment-434005191:662,Testability,assert,assert,662,"More comprehensive test, similar to the current AnnData slicing tests (no use of external data). Each try/except block will (currently) fail on its last assertion. ```; import sys, traceback; import numpy as np; import scanpy.api as sc. adata = sc.AnnData(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])). # integer indexing; print(""\n>>> integer indexing, obs first""); try:; assert adata[0:2, :][:, 0:2].X.tolist() == [[1,2], [4,5]]; assert adata[0, 0].X.tolist() == 1; assert adata[0:1, 0:1].X.tolist() == 1; assert adata[0, :][:, 0].X.tolist() == 1. except Exception as e:; traceback.print_exc(file=sys.stdout). print(""\n>>> integer indexing, vars first""); try:; assert adata[:, 0:2][0:2, :].X.tolist() == [[1,2], [4,5]]; assert adata[0, 0].X.tolist() == 1; assert adata[0:1, 0:1].X.tolist() == 1; assert adata[:, 0][0, :].X.tolist() == 1. except Exception as e:; traceback.print_exc(file=sys.stdout). # boolean indexing; print(""\n>>> boolean indexing, obs first""); try:; obs_selector = np.zeros(len(adata.obs), dtype=bool); vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]; vars_selector[:] = [True, True, False]; assert adata[obs_selector, :][:, vars_selector].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]; vars_selector[:] = [True, False, False]; assert adata[obs_selector, :][:, vars_selector].X.tolist() == 1. except Exception as e:; traceback.print_exc(file=sys.stdout). print(""\n>>> boolean indexing, vars first""); try:; obs_selector = np.zeros(len(adata.obs), dtype=bool); vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]; vars_selector[:] = [True, True, False]; assert adata[:, vars_selector][obs_selector, :].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]; vars_selector[:] = [True, False, False]; assert adata[:, vars_selector][obs_selector, :].X.tolist() == 1. except Exception as e:; traceback.print_exc(file=sys.stdout); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332#issuecomment-434005191
https://github.com/scverse/scanpy/issues/332#issuecomment-434005191:721,Testability,assert,assert,721,"More comprehensive test, similar to the current AnnData slicing tests (no use of external data). Each try/except block will (currently) fail on its last assertion. ```; import sys, traceback; import numpy as np; import scanpy.api as sc. adata = sc.AnnData(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])). # integer indexing; print(""\n>>> integer indexing, obs first""); try:; assert adata[0:2, :][:, 0:2].X.tolist() == [[1,2], [4,5]]; assert adata[0, 0].X.tolist() == 1; assert adata[0:1, 0:1].X.tolist() == 1; assert adata[0, :][:, 0].X.tolist() == 1. except Exception as e:; traceback.print_exc(file=sys.stdout). print(""\n>>> integer indexing, vars first""); try:; assert adata[:, 0:2][0:2, :].X.tolist() == [[1,2], [4,5]]; assert adata[0, 0].X.tolist() == 1; assert adata[0:1, 0:1].X.tolist() == 1; assert adata[:, 0][0, :].X.tolist() == 1. except Exception as e:; traceback.print_exc(file=sys.stdout). # boolean indexing; print(""\n>>> boolean indexing, obs first""); try:; obs_selector = np.zeros(len(adata.obs), dtype=bool); vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]; vars_selector[:] = [True, True, False]; assert adata[obs_selector, :][:, vars_selector].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]; vars_selector[:] = [True, False, False]; assert adata[obs_selector, :][:, vars_selector].X.tolist() == 1. except Exception as e:; traceback.print_exc(file=sys.stdout). print(""\n>>> boolean indexing, vars first""); try:; obs_selector = np.zeros(len(adata.obs), dtype=bool); vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]; vars_selector[:] = [True, True, False]; assert adata[:, vars_selector][obs_selector, :].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]; vars_selector[:] = [True, False, False]; assert adata[:, vars_selector][obs_selector, :].X.tolist() == 1. except Exception as e:; traceback.print_exc(file=sys.stdout); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332#issuecomment-434005191
https://github.com/scverse/scanpy/issues/332#issuecomment-434005191:757,Testability,assert,assert,757,"More comprehensive test, similar to the current AnnData slicing tests (no use of external data). Each try/except block will (currently) fail on its last assertion. ```; import sys, traceback; import numpy as np; import scanpy.api as sc. adata = sc.AnnData(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])). # integer indexing; print(""\n>>> integer indexing, obs first""); try:; assert adata[0:2, :][:, 0:2].X.tolist() == [[1,2], [4,5]]; assert adata[0, 0].X.tolist() == 1; assert adata[0:1, 0:1].X.tolist() == 1; assert adata[0, :][:, 0].X.tolist() == 1. except Exception as e:; traceback.print_exc(file=sys.stdout). print(""\n>>> integer indexing, vars first""); try:; assert adata[:, 0:2][0:2, :].X.tolist() == [[1,2], [4,5]]; assert adata[0, 0].X.tolist() == 1; assert adata[0:1, 0:1].X.tolist() == 1; assert adata[:, 0][0, :].X.tolist() == 1. except Exception as e:; traceback.print_exc(file=sys.stdout). # boolean indexing; print(""\n>>> boolean indexing, obs first""); try:; obs_selector = np.zeros(len(adata.obs), dtype=bool); vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]; vars_selector[:] = [True, True, False]; assert adata[obs_selector, :][:, vars_selector].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]; vars_selector[:] = [True, False, False]; assert adata[obs_selector, :][:, vars_selector].X.tolist() == 1. except Exception as e:; traceback.print_exc(file=sys.stdout). print(""\n>>> boolean indexing, vars first""); try:; obs_selector = np.zeros(len(adata.obs), dtype=bool); vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]; vars_selector[:] = [True, True, False]; assert adata[:, vars_selector][obs_selector, :].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]; vars_selector[:] = [True, False, False]; assert adata[:, vars_selector][obs_selector, :].X.tolist() == 1. except Exception as e:; traceback.print_exc(file=sys.stdout); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332#issuecomment-434005191
https://github.com/scverse/scanpy/issues/332#issuecomment-434005191:797,Testability,assert,assert,797,"More comprehensive test, similar to the current AnnData slicing tests (no use of external data). Each try/except block will (currently) fail on its last assertion. ```; import sys, traceback; import numpy as np; import scanpy.api as sc. adata = sc.AnnData(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])). # integer indexing; print(""\n>>> integer indexing, obs first""); try:; assert adata[0:2, :][:, 0:2].X.tolist() == [[1,2], [4,5]]; assert adata[0, 0].X.tolist() == 1; assert adata[0:1, 0:1].X.tolist() == 1; assert adata[0, :][:, 0].X.tolist() == 1. except Exception as e:; traceback.print_exc(file=sys.stdout). print(""\n>>> integer indexing, vars first""); try:; assert adata[:, 0:2][0:2, :].X.tolist() == [[1,2], [4,5]]; assert adata[0, 0].X.tolist() == 1; assert adata[0:1, 0:1].X.tolist() == 1; assert adata[:, 0][0, :].X.tolist() == 1. except Exception as e:; traceback.print_exc(file=sys.stdout). # boolean indexing; print(""\n>>> boolean indexing, obs first""); try:; obs_selector = np.zeros(len(adata.obs), dtype=bool); vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]; vars_selector[:] = [True, True, False]; assert adata[obs_selector, :][:, vars_selector].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]; vars_selector[:] = [True, False, False]; assert adata[obs_selector, :][:, vars_selector].X.tolist() == 1. except Exception as e:; traceback.print_exc(file=sys.stdout). print(""\n>>> boolean indexing, vars first""); try:; obs_selector = np.zeros(len(adata.obs), dtype=bool); vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]; vars_selector[:] = [True, True, False]; assert adata[:, vars_selector][obs_selector, :].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]; vars_selector[:] = [True, False, False]; assert adata[:, vars_selector][obs_selector, :].X.tolist() == 1. except Exception as e:; traceback.print_exc(file=sys.stdout); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332#issuecomment-434005191
https://github.com/scverse/scanpy/issues/332#issuecomment-434005191:1157,Testability,assert,assert,1157,"More comprehensive test, similar to the current AnnData slicing tests (no use of external data). Each try/except block will (currently) fail on its last assertion. ```; import sys, traceback; import numpy as np; import scanpy.api as sc. adata = sc.AnnData(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])). # integer indexing; print(""\n>>> integer indexing, obs first""); try:; assert adata[0:2, :][:, 0:2].X.tolist() == [[1,2], [4,5]]; assert adata[0, 0].X.tolist() == 1; assert adata[0:1, 0:1].X.tolist() == 1; assert adata[0, :][:, 0].X.tolist() == 1. except Exception as e:; traceback.print_exc(file=sys.stdout). print(""\n>>> integer indexing, vars first""); try:; assert adata[:, 0:2][0:2, :].X.tolist() == [[1,2], [4,5]]; assert adata[0, 0].X.tolist() == 1; assert adata[0:1, 0:1].X.tolist() == 1; assert adata[:, 0][0, :].X.tolist() == 1. except Exception as e:; traceback.print_exc(file=sys.stdout). # boolean indexing; print(""\n>>> boolean indexing, obs first""); try:; obs_selector = np.zeros(len(adata.obs), dtype=bool); vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]; vars_selector[:] = [True, True, False]; assert adata[obs_selector, :][:, vars_selector].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]; vars_selector[:] = [True, False, False]; assert adata[obs_selector, :][:, vars_selector].X.tolist() == 1. except Exception as e:; traceback.print_exc(file=sys.stdout). print(""\n>>> boolean indexing, vars first""); try:; obs_selector = np.zeros(len(adata.obs), dtype=bool); vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]; vars_selector[:] = [True, True, False]; assert adata[:, vars_selector][obs_selector, :].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]; vars_selector[:] = [True, False, False]; assert adata[:, vars_selector][obs_selector, :].X.tolist() == 1. except Exception as e:; traceback.print_exc(file=sys.stdout); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332#issuecomment-434005191
https://github.com/scverse/scanpy/issues/332#issuecomment-434005191:1316,Testability,assert,assert,1316,"More comprehensive test, similar to the current AnnData slicing tests (no use of external data). Each try/except block will (currently) fail on its last assertion. ```; import sys, traceback; import numpy as np; import scanpy.api as sc. adata = sc.AnnData(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])). # integer indexing; print(""\n>>> integer indexing, obs first""); try:; assert adata[0:2, :][:, 0:2].X.tolist() == [[1,2], [4,5]]; assert adata[0, 0].X.tolist() == 1; assert adata[0:1, 0:1].X.tolist() == 1; assert adata[0, :][:, 0].X.tolist() == 1. except Exception as e:; traceback.print_exc(file=sys.stdout). print(""\n>>> integer indexing, vars first""); try:; assert adata[:, 0:2][0:2, :].X.tolist() == [[1,2], [4,5]]; assert adata[0, 0].X.tolist() == 1; assert adata[0:1, 0:1].X.tolist() == 1; assert adata[:, 0][0, :].X.tolist() == 1. except Exception as e:; traceback.print_exc(file=sys.stdout). # boolean indexing; print(""\n>>> boolean indexing, obs first""); try:; obs_selector = np.zeros(len(adata.obs), dtype=bool); vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]; vars_selector[:] = [True, True, False]; assert adata[obs_selector, :][:, vars_selector].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]; vars_selector[:] = [True, False, False]; assert adata[obs_selector, :][:, vars_selector].X.tolist() == 1. except Exception as e:; traceback.print_exc(file=sys.stdout). print(""\n>>> boolean indexing, vars first""); try:; obs_selector = np.zeros(len(adata.obs), dtype=bool); vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]; vars_selector[:] = [True, True, False]; assert adata[:, vars_selector][obs_selector, :].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]; vars_selector[:] = [True, False, False]; assert adata[:, vars_selector][obs_selector, :].X.tolist() == 1. except Exception as e:; traceback.print_exc(file=sys.stdout); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332#issuecomment-434005191
https://github.com/scverse/scanpy/issues/332#issuecomment-434005191:1680,Testability,assert,assert,1680,"More comprehensive test, similar to the current AnnData slicing tests (no use of external data). Each try/except block will (currently) fail on its last assertion. ```; import sys, traceback; import numpy as np; import scanpy.api as sc. adata = sc.AnnData(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])). # integer indexing; print(""\n>>> integer indexing, obs first""); try:; assert adata[0:2, :][:, 0:2].X.tolist() == [[1,2], [4,5]]; assert adata[0, 0].X.tolist() == 1; assert adata[0:1, 0:1].X.tolist() == 1; assert adata[0, :][:, 0].X.tolist() == 1. except Exception as e:; traceback.print_exc(file=sys.stdout). print(""\n>>> integer indexing, vars first""); try:; assert adata[:, 0:2][0:2, :].X.tolist() == [[1,2], [4,5]]; assert adata[0, 0].X.tolist() == 1; assert adata[0:1, 0:1].X.tolist() == 1; assert adata[:, 0][0, :].X.tolist() == 1. except Exception as e:; traceback.print_exc(file=sys.stdout). # boolean indexing; print(""\n>>> boolean indexing, obs first""); try:; obs_selector = np.zeros(len(adata.obs), dtype=bool); vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]; vars_selector[:] = [True, True, False]; assert adata[obs_selector, :][:, vars_selector].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]; vars_selector[:] = [True, False, False]; assert adata[obs_selector, :][:, vars_selector].X.tolist() == 1. except Exception as e:; traceback.print_exc(file=sys.stdout). print(""\n>>> boolean indexing, vars first""); try:; obs_selector = np.zeros(len(adata.obs), dtype=bool); vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]; vars_selector[:] = [True, True, False]; assert adata[:, vars_selector][obs_selector, :].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]; vars_selector[:] = [True, False, False]; assert adata[:, vars_selector][obs_selector, :].X.tolist() == 1. except Exception as e:; traceback.print_exc(file=sys.stdout); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332#issuecomment-434005191
https://github.com/scverse/scanpy/issues/332#issuecomment-434005191:1839,Testability,assert,assert,1839,"More comprehensive test, similar to the current AnnData slicing tests (no use of external data). Each try/except block will (currently) fail on its last assertion. ```; import sys, traceback; import numpy as np; import scanpy.api as sc. adata = sc.AnnData(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])). # integer indexing; print(""\n>>> integer indexing, obs first""); try:; assert adata[0:2, :][:, 0:2].X.tolist() == [[1,2], [4,5]]; assert adata[0, 0].X.tolist() == 1; assert adata[0:1, 0:1].X.tolist() == 1; assert adata[0, :][:, 0].X.tolist() == 1. except Exception as e:; traceback.print_exc(file=sys.stdout). print(""\n>>> integer indexing, vars first""); try:; assert adata[:, 0:2][0:2, :].X.tolist() == [[1,2], [4,5]]; assert adata[0, 0].X.tolist() == 1; assert adata[0:1, 0:1].X.tolist() == 1; assert adata[:, 0][0, :].X.tolist() == 1. except Exception as e:; traceback.print_exc(file=sys.stdout). # boolean indexing; print(""\n>>> boolean indexing, obs first""); try:; obs_selector = np.zeros(len(adata.obs), dtype=bool); vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]; vars_selector[:] = [True, True, False]; assert adata[obs_selector, :][:, vars_selector].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]; vars_selector[:] = [True, False, False]; assert adata[obs_selector, :][:, vars_selector].X.tolist() == 1. except Exception as e:; traceback.print_exc(file=sys.stdout). print(""\n>>> boolean indexing, vars first""); try:; obs_selector = np.zeros(len(adata.obs), dtype=bool); vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]; vars_selector[:] = [True, True, False]; assert adata[:, vars_selector][obs_selector, :].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]; vars_selector[:] = [True, False, False]; assert adata[:, vars_selector][obs_selector, :].X.tolist() == 1. except Exception as e:; traceback.print_exc(file=sys.stdout); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332#issuecomment-434005191
https://github.com/scverse/scanpy/issues/333#issuecomment-434297999:22,Availability,error,error,22,"I am getting the same error in a much more basic setting:; ```; paul=sc.datasets.paul15(); sc.pl.scatter(paul, x=paul.var_names[0], y=paul.var_names[1]); ```; ...; > TypeError: object of type 'numpy.int64' has no len()",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/333#issuecomment-434297999
https://github.com/scverse/scanpy/issues/333#issuecomment-434298485:114,Usability,learn,learn,114,Forgot to mention my versions: . > scanpy==1.3.2 anndata==0.6.11 numpy==1.14.6 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/333#issuecomment-434298485
https://github.com/scverse/scanpy/issues/333#issuecomment-435728872:193,Deployability,release,release,193,"Everything runs fine on the current master branch, I uploaded the current version of the notebook: https://nbviewer.jupyter.org/github/theislab/paga/blob/master/blood/paul15/paul15.ipynb. I'll release either 1.3.3 or 1.4 very soon and if there should have been a bug at some point, it seems to have been fixed at some point. Finally, PAGA is also in the continuous integration tests, so no bugs in the future anymore for this. ;)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/333#issuecomment-435728872
https://github.com/scverse/scanpy/issues/333#issuecomment-435728872:354,Deployability,continuous,continuous,354,"Everything runs fine on the current master branch, I uploaded the current version of the notebook: https://nbviewer.jupyter.org/github/theislab/paga/blob/master/blood/paul15/paul15.ipynb. I'll release either 1.3.3 or 1.4 very soon and if there should have been a bug at some point, it seems to have been fixed at some point. Finally, PAGA is also in the continuous integration tests, so no bugs in the future anymore for this. ;)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/333#issuecomment-435728872
https://github.com/scverse/scanpy/issues/333#issuecomment-435728872:365,Deployability,integrat,integration,365,"Everything runs fine on the current master branch, I uploaded the current version of the notebook: https://nbviewer.jupyter.org/github/theislab/paga/blob/master/blood/paul15/paul15.ipynb. I'll release either 1.3.3 or 1.4 very soon and if there should have been a bug at some point, it seems to have been fixed at some point. Finally, PAGA is also in the continuous integration tests, so no bugs in the future anymore for this. ;)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/333#issuecomment-435728872
https://github.com/scverse/scanpy/issues/333#issuecomment-435728872:365,Integrability,integrat,integration,365,"Everything runs fine on the current master branch, I uploaded the current version of the notebook: https://nbviewer.jupyter.org/github/theislab/paga/blob/master/blood/paul15/paul15.ipynb. I'll release either 1.3.3 or 1.4 very soon and if there should have been a bug at some point, it seems to have been fixed at some point. Finally, PAGA is also in the continuous integration tests, so no bugs in the future anymore for this. ;)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/333#issuecomment-435728872
https://github.com/scverse/scanpy/issues/333#issuecomment-435728872:377,Testability,test,tests,377,"Everything runs fine on the current master branch, I uploaded the current version of the notebook: https://nbviewer.jupyter.org/github/theislab/paga/blob/master/blood/paul15/paul15.ipynb. I'll release either 1.3.3 or 1.4 very soon and if there should have been a bug at some point, it seems to have been fixed at some point. Finally, PAGA is also in the continuous integration tests, so no bugs in the future anymore for this. ;)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/333#issuecomment-435728872
https://github.com/scverse/scanpy/issues/333#issuecomment-435784820:27,Availability,error,error,27,I updated to 1.3.3 but the error still persists. One important thing I didnt mention before: I am running python/scanpy on a Windows machine. @Donovan-CG do you also use Windows?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/333#issuecomment-435784820
https://github.com/scverse/scanpy/issues/333#issuecomment-435784820:2,Deployability,update,updated,2,I updated to 1.3.3 but the error still persists. One important thing I didnt mention before: I am running python/scanpy on a Windows machine. @Donovan-CG do you also use Windows?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/333#issuecomment-435784820
https://github.com/scverse/scanpy/issues/336#issuecomment-435730736:98,Security,expose,exposed,98,@fidelram are you calling an implicit function `summarize_categorical` or something that could be exposed to the user as a tool? . @wangjiawen2013 `sc.set_figure_params(vector_friendly=False)` does what you want: https://scanpy.readthedocs.io/en/latest/api/index.html#settings,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/336#issuecomment-435730736
https://github.com/scverse/scanpy/issues/336#issuecomment-435754069:47,Energy Efficiency,adapt,adapted,47,"I have got what I want with the following code adapted from dotplot():. gene_ids = adata.raw.var.index.values; clusters = adata.obs['louvain'].cat.categories; obs = adata.raw[:,gene_ids].X.toarray(); obs = pd.DataFrame(obs,columns=gene_ids,index=adata.obs['louvain']); average_obs = obs.groupby(level=0).mean(); obs_bool = obs.astype(bool); fraction_obs = obs_bool.groupby(level=0).sum()/obs_bool.groupby(level=0).count(); average_obs.T.to_csv(""average.csv""); fraction_obs.T.to_csv(""fraction.csv"")",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/336#issuecomment-435754069
https://github.com/scverse/scanpy/issues/336#issuecomment-435754069:47,Modifiability,adapt,adapted,47,"I have got what I want with the following code adapted from dotplot():. gene_ids = adata.raw.var.index.values; clusters = adata.obs['louvain'].cat.categories; obs = adata.raw[:,gene_ids].X.toarray(); obs = pd.DataFrame(obs,columns=gene_ids,index=adata.obs['louvain']); average_obs = obs.groupby(level=0).mean(); obs_bool = obs.astype(bool); fraction_obs = obs_bool.groupby(level=0).sum()/obs_bool.groupby(level=0).count(); average_obs.T.to_csv(""average.csv""); fraction_obs.T.to_csv(""fraction.csv"")",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/336#issuecomment-435754069
https://github.com/scverse/scanpy/issues/336#issuecomment-1334674713:681,Deployability,integrat,integrated,681,"> I have got what I want with the following code adapted from dotplot():; > ; > gene_ids = adata.raw.var.index.values clusters = adata.obs['louvain'].cat.categories obs = adata.raw[:,gene_ids].X.toarray() obs = pd.DataFrame(obs,columns=gene_ids,index=adata.obs['louvain']) average_obs = obs.groupby(level=0).mean() obs_bool = obs.astype(bool) fraction_obs = obs_bool.groupby(level=0).sum()/obs_bool.groupby(level=0).count() average_obs.T.to_csv(""average.csv"") fraction_obs.T.to_csv(""fraction.csv""). Love this! Thanks a lot!! ; Just one question, is there a way to get the average expression in different cell types (cluster label 1 ) in different sample (cluster label 2 ) from an integrated object?? ; to get something roughly like this:. Gene 1 Gene 2 ; sample1 sample2 sample3 sample1 sample2 sample3 ..... ....... ....; T-cell; B-cell ; .....; ..... I am not sure if this makes sense, but I have been trying to do this for a while and nothing worked!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/336#issuecomment-1334674713
https://github.com/scverse/scanpy/issues/336#issuecomment-1334674713:49,Energy Efficiency,adapt,adapted,49,"> I have got what I want with the following code adapted from dotplot():; > ; > gene_ids = adata.raw.var.index.values clusters = adata.obs['louvain'].cat.categories obs = adata.raw[:,gene_ids].X.toarray() obs = pd.DataFrame(obs,columns=gene_ids,index=adata.obs['louvain']) average_obs = obs.groupby(level=0).mean() obs_bool = obs.astype(bool) fraction_obs = obs_bool.groupby(level=0).sum()/obs_bool.groupby(level=0).count() average_obs.T.to_csv(""average.csv"") fraction_obs.T.to_csv(""fraction.csv""). Love this! Thanks a lot!! ; Just one question, is there a way to get the average expression in different cell types (cluster label 1 ) in different sample (cluster label 2 ) from an integrated object?? ; to get something roughly like this:. Gene 1 Gene 2 ; sample1 sample2 sample3 sample1 sample2 sample3 ..... ....... ....; T-cell; B-cell ; .....; ..... I am not sure if this makes sense, but I have been trying to do this for a while and nothing worked!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/336#issuecomment-1334674713
https://github.com/scverse/scanpy/issues/336#issuecomment-1334674713:681,Integrability,integrat,integrated,681,"> I have got what I want with the following code adapted from dotplot():; > ; > gene_ids = adata.raw.var.index.values clusters = adata.obs['louvain'].cat.categories obs = adata.raw[:,gene_ids].X.toarray() obs = pd.DataFrame(obs,columns=gene_ids,index=adata.obs['louvain']) average_obs = obs.groupby(level=0).mean() obs_bool = obs.astype(bool) fraction_obs = obs_bool.groupby(level=0).sum()/obs_bool.groupby(level=0).count() average_obs.T.to_csv(""average.csv"") fraction_obs.T.to_csv(""fraction.csv""). Love this! Thanks a lot!! ; Just one question, is there a way to get the average expression in different cell types (cluster label 1 ) in different sample (cluster label 2 ) from an integrated object?? ; to get something roughly like this:. Gene 1 Gene 2 ; sample1 sample2 sample3 sample1 sample2 sample3 ..... ....... ....; T-cell; B-cell ; .....; ..... I am not sure if this makes sense, but I have been trying to do this for a while and nothing worked!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/336#issuecomment-1334674713
https://github.com/scverse/scanpy/issues/336#issuecomment-1334674713:49,Modifiability,adapt,adapted,49,"> I have got what I want with the following code adapted from dotplot():; > ; > gene_ids = adata.raw.var.index.values clusters = adata.obs['louvain'].cat.categories obs = adata.raw[:,gene_ids].X.toarray() obs = pd.DataFrame(obs,columns=gene_ids,index=adata.obs['louvain']) average_obs = obs.groupby(level=0).mean() obs_bool = obs.astype(bool) fraction_obs = obs_bool.groupby(level=0).sum()/obs_bool.groupby(level=0).count() average_obs.T.to_csv(""average.csv"") fraction_obs.T.to_csv(""fraction.csv""). Love this! Thanks a lot!! ; Just one question, is there a way to get the average expression in different cell types (cluster label 1 ) in different sample (cluster label 2 ) from an integrated object?? ; to get something roughly like this:. Gene 1 Gene 2 ; sample1 sample2 sample3 sample1 sample2 sample3 ..... ....... ....; T-cell; B-cell ; .....; ..... I am not sure if this makes sense, but I have been trying to do this for a while and nothing worked!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/336#issuecomment-1334674713
https://github.com/scverse/scanpy/issues/337#issuecomment-726207918:272,Availability,error,error,272,"Hi I am having a similar issue where I would like to set the tick locations on the colorbar. Using similar code as above. ```; ax = sc.pl.tsne(adata, color = 'gene', show=False); fig = plt.gcf(); cbar_ax = fig.axes[-1]; cbar_ax.set_yticks([0,1]); ```; I get the following error 'UserWarning: Use the colorbar set_ticks() method instead'. How can I access the colorbar specifically?. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/337#issuecomment-726207918
https://github.com/scverse/scanpy/issues/337#issuecomment-726207918:348,Security,access,access,348,"Hi I am having a similar issue where I would like to set the tick locations on the colorbar. Using similar code as above. ```; ax = sc.pl.tsne(adata, color = 'gene', show=False); fig = plt.gcf(); cbar_ax = fig.axes[-1]; cbar_ax.set_yticks([0,1]); ```; I get the following error 'UserWarning: Use the colorbar set_ticks() method instead'. How can I access the colorbar specifically?. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/337#issuecomment-726207918
https://github.com/scverse/scanpy/issues/338#issuecomment-435641437:14,Usability,simpl,simply,14,"Hm, how about simply ranking things yourself, like ; ```; sort_idcs = np.argsort(adata.var['PCs'][:, 0]); genes_ranked_by_loading_in_PC1 = adata.var_names[sort_idcs]; ```; This is what the plotting functions do internally.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/338#issuecomment-435641437
https://github.com/scverse/scanpy/issues/339#issuecomment-435639112:174,Usability,Simpl,Simply,174,`set_figure_params` does a whole bunch of stuff is `scanpy=True`: https://scanpy.readthedocs.io/en/latest/api/scanpy.api.set_figure_params.html#scanpy.api.set_figure_params. Simply set your favorite `matplotlib.rcParams` directly after calling the function; or don't call the function at all (or with `scanpy=False`) and set all the rcParams yourself.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/339#issuecomment-435639112
https://github.com/scverse/scanpy/pull/340#issuecomment-435273679:175,Availability,down,downsampling,175,"Um, it wasn't me. ```; CondaHTTPError: HTTP 000 CONNECTION FAILED for url <https://conda.anaconda.org/conda-forge/linux-64/xorg-libxdmcp-1.1.2-h470a237_7.tar.bz2>; ```. Also, downsampling from 3785143 finished after an hour, but definitely had the wrong answer (all counts in one gene). I'm not sure what to make of this, since it's given reasonable results for smaller tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/340#issuecomment-435273679
https://github.com/scverse/scanpy/pull/340#issuecomment-435273679:370,Testability,test,tests,370,"Um, it wasn't me. ```; CondaHTTPError: HTTP 000 CONNECTION FAILED for url <https://conda.anaconda.org/conda-forge/linux-64/xorg-libxdmcp-1.1.2-h470a237_7.tar.bz2>; ```. Also, downsampling from 3785143 finished after an hour, but definitely had the wrong answer (all counts in one gene). I'm not sure what to make of this, since it's given reasonable results for smaller tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/340#issuecomment-435273679
https://github.com/scverse/scanpy/pull/340#issuecomment-435326551:328,Usability,clear,clearly,328,"Hey!; I wrote this function a while ago... it was definitely not the cleanest or quickest implementation. And it did take a while to run on ~5k cells at the time, but I thought it would be useful to have this functionality in scanpy. Just wanted to note that the intention was definitely to implement this without resampling. I clearly missed that the default was to use resampling in `np.random.choice`. Thanks for spotting this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/340#issuecomment-435326551
https://github.com/scverse/scanpy/pull/340#issuecomment-435638913:160,Deployability,release,release,160,"That's really cool, thank you!. I'll add a logging output about that `replace=False` is the more natural choice and we'll make it the default in the next major release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/340#issuecomment-435638913
https://github.com/scverse/scanpy/pull/340#issuecomment-435638913:43,Testability,log,logging,43,"That's really cool, thank you!. I'll add a logging output about that `replace=False` is the more natural choice and we'll make it the default in the next major release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/340#issuecomment-435638913
https://github.com/scverse/scanpy/pull/343#issuecomment-436045072:83,Testability,test,tests,83,"It seems to be a stalled build in CI. Something to do with a URL request... if the tests run through on your end, everything should be fine. Do they?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/343#issuecomment-436045072
https://github.com/scverse/scanpy/issues/344#issuecomment-436069713:16,Deployability,install,install,16,You may need to install from github rather than wait for new pypi versions.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/344#issuecomment-436069713
https://github.com/scverse/scanpy/issues/346#issuecomment-436337697:8,Usability,simpl,simply,8,"Why not simply as in the [tutorial](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb)?; ```; sc.tl.rank_genes_groups(adata, 'louvain', groups=['0'], reference='1'); ```; Or am I missing your problem? A few lines of code documenting your call wouldn't hurt.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/346#issuecomment-436337697
https://github.com/scverse/scanpy/issues/346#issuecomment-445219624:473,Availability,error,error,473,"Hi Alex!. Sorry for this long delay, I just forgot completely.; Maybe I wasn't clear enough in my original post, here is where the issue lies:; ; When I run . ```py; sc.tl.rank_genes_groups(adata_f, groupby = 'ClusterName', groups = 'all'); ```. everything is fine and I get my desired result. However, when I just change the reference setting. ```py; sc.tl.rank_genes_groups(adata_f, groupby = 'ClusterName', reference='CA', groups = 'all'); ```. then I get the following error. ```pytb; 100 groups_order = [str(n) for n in groups_order]; 101 if reference != 'rest' and reference not in set(groups_order):; --> 102 groups_order += [reference]; 103 if (reference != 'rest'; 104 and reference not in set(adata.obs[groupby].cat.categories)):. TypeError: must be str, not list; ```. I absolutely understand how to solve this - as you said, I can just use the tutorial call and select groups explicilty. ```py; sc.tl.rank_genes_groups(adata_f, groupby = 'ClusterName', reference='CA', groups = ['OPC', 'Granule']); ```. and then it works again. I was just wondering whether this is the desired behavior - most users will leave the `groups` attribute at it's default setting when they change the `reference` attribute and wonder why it does not work - at least that was my idea. Maybe I am wrong.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/346#issuecomment-445219624
https://github.com/scverse/scanpy/issues/346#issuecomment-445219624:79,Usability,clear,clear,79,"Hi Alex!. Sorry for this long delay, I just forgot completely.; Maybe I wasn't clear enough in my original post, here is where the issue lies:; ; When I run . ```py; sc.tl.rank_genes_groups(adata_f, groupby = 'ClusterName', groups = 'all'); ```. everything is fine and I get my desired result. However, when I just change the reference setting. ```py; sc.tl.rank_genes_groups(adata_f, groupby = 'ClusterName', reference='CA', groups = 'all'); ```. then I get the following error. ```pytb; 100 groups_order = [str(n) for n in groups_order]; 101 if reference != 'rest' and reference not in set(groups_order):; --> 102 groups_order += [reference]; 103 if (reference != 'rest'; 104 and reference not in set(adata.obs[groupby].cat.categories)):. TypeError: must be str, not list; ```. I absolutely understand how to solve this - as you said, I can just use the tutorial call and select groups explicilty. ```py; sc.tl.rank_genes_groups(adata_f, groupby = 'ClusterName', reference='CA', groups = ['OPC', 'Granule']); ```. and then it works again. I was just wondering whether this is the desired behavior - most users will leave the `groups` attribute at it's default setting when they change the `reference` attribute and wonder why it does not work - at least that was my idea. Maybe I am wrong.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/346#issuecomment-445219624
https://github.com/scverse/scanpy/issues/347#issuecomment-436379004:113,Modifiability,variab,variable,113,"Hey! So you used `restrict_to = ('louvain', ['1'])` if you wanted to recluster '1'? You'll have a new clustering variable `louvain_R` in this case, which you can rename anything you like... checkout the documentation: https://icb-scanpy.readthedocs-hosted.com/en/latest/api/scanpy.api.tl.louvain.html. I guess that this is what you want.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/347#issuecomment-436379004
https://github.com/scverse/scanpy/issues/348#issuecomment-745609837:63,Availability,avail,available,63,"Can we reopen this issue? I still don't see this functionality available. The ask is to be able to specify the number of rows or columns for the arrangement of the output panels from `sc.pl.violin`. Right now if I plot 8 genes, for example, they all show up on one row, yielding tiny plots. It would be nice to be able to pass in something like `ncols=4` so that the 8 panels will be arranged as a 2x4 instead of a 1x8.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/348#issuecomment-745609837
https://github.com/scverse/scanpy/issues/349#issuecomment-436548839:108,Availability,avail,available,108,"With respect to the heatmap, indeed it is possible to transpose the matrix.; Currently, this option is only available for `stacked_violin`. I thought; about adding this option to other plots like heatmap, matrixplot and; dotplot but I have not find the time and it is always possible to save the; figure and rotate it so it has low priority for me. The changes are not as; trivial as simply rotating the matrix as all other elements need to be; adjusted. On Wed, Nov 7, 2018 at 3:03 AM Alex Wolf <notifications@github.com> wrote:. > @fidelram <https://github.com/fidelram> should be the expert for this...; > 😄; >; > —; > You are receiving this because you were mentioned.; >; >; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/349#issuecomment-436477272>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1b_dlMN1mihuJIbXg2lPmMJvgqGgks5usj-FgaJpZM4YRQ7g>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/349#issuecomment-436548839
https://github.com/scverse/scanpy/issues/349#issuecomment-436548839:384,Usability,simpl,simply,384,"With respect to the heatmap, indeed it is possible to transpose the matrix.; Currently, this option is only available for `stacked_violin`. I thought; about adding this option to other plots like heatmap, matrixplot and; dotplot but I have not find the time and it is always possible to save the; figure and rotate it so it has low priority for me. The changes are not as; trivial as simply rotating the matrix as all other elements need to be; adjusted. On Wed, Nov 7, 2018 at 3:03 AM Alex Wolf <notifications@github.com> wrote:. > @fidelram <https://github.com/fidelram> should be the expert for this...; > 😄; >; > —; > You are receiving this because you were mentioned.; >; >; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/349#issuecomment-436477272>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1b_dlMN1mihuJIbXg2lPmMJvgqGgks5usj-FgaJpZM4YRQ7g>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/349#issuecomment-436548839
https://github.com/scverse/scanpy/issues/349#issuecomment-460428844:197,Availability,avail,availible,197,"As this issue is not closed I'll add a question here. . Is it possible, or if not could it be added, that the cells in the heatmaps are sorted within the groupby variables. Either by pseudotime if availible or just clustered simply by hierarchical clustering. This could add a more visually and intuitive pleasing ordering of cells. For example as in my figure above the groupby miss some property of the data with pattern over cells. If the cell ordering was random or default this pattern could not be seen.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/349#issuecomment-460428844
https://github.com/scverse/scanpy/issues/349#issuecomment-460428844:162,Modifiability,variab,variables,162,"As this issue is not closed I'll add a question here. . Is it possible, or if not could it be added, that the cells in the heatmaps are sorted within the groupby variables. Either by pseudotime if availible or just clustered simply by hierarchical clustering. This could add a more visually and intuitive pleasing ordering of cells. For example as in my figure above the groupby miss some property of the data with pattern over cells. If the cell ordering was random or default this pattern could not be seen.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/349#issuecomment-460428844
https://github.com/scverse/scanpy/issues/349#issuecomment-460428844:225,Usability,simpl,simply,225,"As this issue is not closed I'll add a question here. . Is it possible, or if not could it be added, that the cells in the heatmaps are sorted within the groupby variables. Either by pseudotime if availible or just clustered simply by hierarchical clustering. This could add a more visually and intuitive pleasing ordering of cells. For example as in my figure above the groupby miss some property of the data with pattern over cells. If the cell ordering was random or default this pattern could not be seen.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/349#issuecomment-460428844
https://github.com/scverse/scanpy/issues/349#issuecomment-460428844:295,Usability,intuit,intuitive,295,"As this issue is not closed I'll add a question here. . Is it possible, or if not could it be added, that the cells in the heatmaps are sorted within the groupby variables. Either by pseudotime if availible or just clustered simply by hierarchical clustering. This could add a more visually and intuitive pleasing ordering of cells. For example as in my figure above the groupby miss some property of the data with pattern over cells. If the cell ordering was random or default this pattern could not be seen.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/349#issuecomment-460428844
https://github.com/scverse/scanpy/issues/350#issuecomment-437026529:410,Integrability,depend,dependency,410,"Thanks for the information. Very interesting read. From the paper it seems that the Leiden method indeed is superior to the louvain method. However, instead of replacing the louvain function I think is better to explicitly create a leiden clustering function as you have done and you can add the n_iterations parameter. . I would suggest to add a PR with this function as part of tools, but because of the new dependency required I am not sure. @falexwolf any suggestions?. Meanwhile I will give it a try.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350#issuecomment-437026529
https://github.com/scverse/scanpy/issues/350#issuecomment-437039831:552,Availability,error,errors,552,"I mean, @vtraag is is the person I’d believe when asked which algorithm is superior, so we could. 1. add `sc.tl.leiden` as an alternative that doesn’t have a flavour argument.; 2. make `leidenalg` a dependency and `louvain-igraph` an optional one.; 3. when calling `sc.tl.louvain` (no matter the flavor used), emit a ``DeprecationWarning('We recommend to use `sc.tool.leiden` instead. Refer to its documentation for details')``. This meets the following goals:. - education: people will learn why we recommend the new function; - ease of use: no weird errors pop up suddenly; - reproducibility: If `louvain-igraph` is installed, the code works exactly as before (with an added warning), else it crashes. we could do the following within `sc.tl.louvain` to help users:. ```py; try:; import louvain; except ImportError:; raise ImportError(; 'The package “louvain-igraph“ is not installed. '; 'Try using `sc.tl.leiden` in case you do not need '; 'to reproduce results produced using `sc.tl.louvain`'; ); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350#issuecomment-437039831
https://github.com/scverse/scanpy/issues/350#issuecomment-437039831:618,Deployability,install,installed,618,"I mean, @vtraag is is the person I’d believe when asked which algorithm is superior, so we could. 1. add `sc.tl.leiden` as an alternative that doesn’t have a flavour argument.; 2. make `leidenalg` a dependency and `louvain-igraph` an optional one.; 3. when calling `sc.tl.louvain` (no matter the flavor used), emit a ``DeprecationWarning('We recommend to use `sc.tool.leiden` instead. Refer to its documentation for details')``. This meets the following goals:. - education: people will learn why we recommend the new function; - ease of use: no weird errors pop up suddenly; - reproducibility: If `louvain-igraph` is installed, the code works exactly as before (with an added warning), else it crashes. we could do the following within `sc.tl.louvain` to help users:. ```py; try:; import louvain; except ImportError:; raise ImportError(; 'The package “louvain-igraph“ is not installed. '; 'Try using `sc.tl.leiden` in case you do not need '; 'to reproduce results produced using `sc.tl.louvain`'; ); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350#issuecomment-437039831
https://github.com/scverse/scanpy/issues/350#issuecomment-437039831:876,Deployability,install,installed,876,"I mean, @vtraag is is the person I’d believe when asked which algorithm is superior, so we could. 1. add `sc.tl.leiden` as an alternative that doesn’t have a flavour argument.; 2. make `leidenalg` a dependency and `louvain-igraph` an optional one.; 3. when calling `sc.tl.louvain` (no matter the flavor used), emit a ``DeprecationWarning('We recommend to use `sc.tool.leiden` instead. Refer to its documentation for details')``. This meets the following goals:. - education: people will learn why we recommend the new function; - ease of use: no weird errors pop up suddenly; - reproducibility: If `louvain-igraph` is installed, the code works exactly as before (with an added warning), else it crashes. we could do the following within `sc.tl.louvain` to help users:. ```py; try:; import louvain; except ImportError:; raise ImportError(; 'The package “louvain-igraph“ is not installed. '; 'Try using `sc.tl.leiden` in case you do not need '; 'to reproduce results produced using `sc.tl.louvain`'; ); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350#issuecomment-437039831
https://github.com/scverse/scanpy/issues/350#issuecomment-437039831:199,Integrability,depend,dependency,199,"I mean, @vtraag is is the person I’d believe when asked which algorithm is superior, so we could. 1. add `sc.tl.leiden` as an alternative that doesn’t have a flavour argument.; 2. make `leidenalg` a dependency and `louvain-igraph` an optional one.; 3. when calling `sc.tl.louvain` (no matter the flavor used), emit a ``DeprecationWarning('We recommend to use `sc.tool.leiden` instead. Refer to its documentation for details')``. This meets the following goals:. - education: people will learn why we recommend the new function; - ease of use: no weird errors pop up suddenly; - reproducibility: If `louvain-igraph` is installed, the code works exactly as before (with an added warning), else it crashes. we could do the following within `sc.tl.louvain` to help users:. ```py; try:; import louvain; except ImportError:; raise ImportError(; 'The package “louvain-igraph“ is not installed. '; 'Try using `sc.tl.leiden` in case you do not need '; 'to reproduce results produced using `sc.tl.louvain`'; ); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350#issuecomment-437039831
https://github.com/scverse/scanpy/issues/350#issuecomment-437039831:487,Usability,learn,learn,487,"I mean, @vtraag is is the person I’d believe when asked which algorithm is superior, so we could. 1. add `sc.tl.leiden` as an alternative that doesn’t have a flavour argument.; 2. make `leidenalg` a dependency and `louvain-igraph` an optional one.; 3. when calling `sc.tl.louvain` (no matter the flavor used), emit a ``DeprecationWarning('We recommend to use `sc.tool.leiden` instead. Refer to its documentation for details')``. This meets the following goals:. - education: people will learn why we recommend the new function; - ease of use: no weird errors pop up suddenly; - reproducibility: If `louvain-igraph` is installed, the code works exactly as before (with an added warning), else it crashes. we could do the following within `sc.tl.louvain` to help users:. ```py; try:; import louvain; except ImportError:; raise ImportError(; 'The package “louvain-igraph“ is not installed. '; 'Try using `sc.tl.leiden` in case you do not need '; 'to reproduce results produced using `sc.tl.louvain`'; ); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350#issuecomment-437039831
https://github.com/scverse/scanpy/issues/350#issuecomment-437046926:636,Deployability,install,installation,636,"I just did a quick comparison between louvain and leiden algorithms using the pbmc68k_reduced dataset:. ```PYTHON; adata = sc.datasets.pbmc68k_reduced(); sc.pp.neighbors(adata); leiden(adata, use_weights=True); sc.tl.louvain(adata, use_weights=True); sc.pl.umap(adata, color=['louvain', 'leiden'], s=50, alpha=0.6, ncols=2); ```; ![image](https://user-images.githubusercontent.com/4964309/48210096-fb814800-e376-11e8-9cbc-b16490c9ead9.png). The results are almost identical. However, while in the `louvain` results some cells appear in the wrong cluster (red circle) this is not the case for the `leiden` method. I should note that the installation of `leidenalg` didn't go smooth and took me a while to set it up.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350#issuecomment-437046926
https://github.com/scverse/scanpy/issues/350#issuecomment-437067620:212,Deployability,install,installation,212,"Sorry to hear it took you some time to set it up. I've created a recipe for `conda-forge` channel (https://github.com/conda-forge/staged-recipes/pull/6911), once merged that should hopefully simplify some of the installation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350#issuecomment-437067620
https://github.com/scverse/scanpy/issues/350#issuecomment-437067620:191,Usability,simpl,simplify,191,"Sorry to hear it took you some time to set it up. I've created a recipe for `conda-forge` channel (https://github.com/conda-forge/staged-recipes/pull/6911), once merged that should hopefully simplify some of the installation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350#issuecomment-437067620
https://github.com/scverse/scanpy/issues/350#issuecomment-437074497:19,Deployability,install,install,19,"@vtraag FWIW, `pip install leidenalg` worked without a hitch for me (CentOS 6.5, Python 3.6.6 in a relatively empty conda env: scanpy + scikit stack).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350#issuecomment-437074497
https://github.com/scverse/scanpy/issues/350#issuecomment-437075453:26,Deployability,install,install,26,"Likewise, I just ran `pip install leidenalg` on an OSX machine which already had scanpy and louvain on it, and it set up effortlessly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350#issuecomment-437075453
https://github.com/scverse/scanpy/issues/350#issuecomment-437077960:27,Deployability,install,installation,27,Good to hear also positive installation results!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350#issuecomment-437077960
https://github.com/scverse/scanpy/issues/350#issuecomment-437728767:783,Deployability,release,release,783,"I'd definitely make a new function `tl.leiden`; then @vtraag 's package can also be taken credit of in an appropriate way, using the reference to the recent arxiv. . Also, backwards compat is guaranteed and @flying-sheep's two other points (eduction and ease-of-use) are met, too. So, @ktpolanski, would you make a PR for a function `tl.leiden`? Of course, it would be nice if didn't duplicate all code in the louvain function, but that's up to you. I would not yet make the `leidenalg` package a at this stage, but transition to that either in Scanpy `1.4` or later. People can definitely achieve decent results with the current setup and we don't want everyone to change everything. After the PR, those who want can slowly transition to the new clustering algorithm. After a major release, we can broadly advertise the package. @vtraag: Great to see your new preprint and package. I thought that your Louvain implementation already yielded very well-connected communities and even made a remark on that [here](https://doi.org/10.1101/208819) in the first version more than a year ago. But great, in hard cases, I'd expect better results using your new algorithm for partitioning the graph...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350#issuecomment-437728767
https://github.com/scverse/scanpy/issues/350#issuecomment-437812546:334,Deployability,upgrade,upgrade,334,"I think the disconnected communities in Louvain should have less of an effect in KNN graphs as the degree distribution is a lot more regular. This issue appears to occur a lot more frequently when the node degrees in a community are quite different (or at least this is what I found on PPI networks). Nonetheless, it's a good idea to upgrade I reckon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350#issuecomment-437812546
https://github.com/scverse/scanpy/issues/350#issuecomment-439105490:377,Usability,simpl,simply,377,"@LuckyMD Yes, I was recently reminded of that by Mason Porter. I saw you also wrote a small piece on it in your [thesis](https://ora.ox.ac.uk/objects/uuid:b49187be-8203-4aa0-abbd-bff1a507ff6f). I had already forgotten about our email exchange again. But it's nice we can continue the conversation 4 years later here :). The approach I took back then was quite straightforward: simply separate the different connected components. This of course ignores the larger issue that even when communities are not completely disconnected, they may still be quite badly connected. The approach taken in the Leiden algorithm is quite different, and has much nicer properties I think.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350#issuecomment-439105490
https://github.com/scverse/scanpy/issues/350#issuecomment-439188030:359,Availability,down,down,359,"@LuckyMD Thanks! Yes, the algorithm has been in development for quite some time. I presented it already in 2016 at a conference in Amsterdam, and after that in several other places. It kept changing in relatively minor ways, although that also affected the exact guarantees it could offer. Unfortunate that the section on disconnected communities got trimmed down! Would you have more extensive results described somewhere else?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350#issuecomment-439188030
https://github.com/scverse/scanpy/issues/350#issuecomment-441213626:30,Integrability,wrap,wrapped,30,"Well, “implemented”… I’d say “wrapped”.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350#issuecomment-441213626
https://github.com/scverse/scanpy/issues/350#issuecomment-441809122:66,Usability,clear,clear,66,"@falexwolf thanks! And no worries about it, I just wanted to make clear that `louvain-igraph` didn't contain that fix. Indeed @LuckyMD, I would have assumed the same myself :smile:. But alas, the problem got lost in the mists of time (or well, my mist of time: I forgot about it), until it resurfaced in another context.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350#issuecomment-441809122
https://github.com/scverse/scanpy/issues/351#issuecomment-437729769:82,Usability,learn,learn,82,"This is very interesting! It would be awesome if you linked to a small example to learn what you do exactly! I guess, a PR would then be more than welcome! 🙂",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351#issuecomment-437729769
https://github.com/scverse/scanpy/issues/351#issuecomment-437946116:113,Security,hash,hashing,113,"@falexwolf Mostly just following along with the Seurat vignettes here using scanpy:. * [Demultiplexing with cell hashing](https://satijalab.org/seurat/hashing_vignette.html); * [Multimodal analysis with CITE-seq ADTs](https://satijalab.org/seurat/multimodal_vignette.html). Mostly I'm using [CITE-seq-count](https://github.com/Hoohm/CITE-seq-Count) to generate counts matrices then storing the ADT counts in `adata.obsm[""X_adt""]` and HTO counts in `adata.obsm[""X_hto""]`. From there, I generate additional metadata about the cells which I store in either `adata.obs` (e.g, for HTOs, `adata.obs.global_classification` stores `[""singlet"", ""doublet"", ""negative""]` and `adata.obs.tag_class` stores `[""hashtag_1"", ""hashtag_2"", ...]`). I'd be happy to contribute a PR in the next couple weeks. A quick question though: the workflow involves some IO, normalization, classification, and plotting, so where would be the best entry point for this? I was thinking about stashing all related functionality in a `multimodal.py` module, but then should they be part of the `tools` api or a separate api like `sc.mm`? Or should I spread out the functionality across the existing `pp`, `tl`, `pl` apis?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351#issuecomment-437946116
https://github.com/scverse/scanpy/issues/351#issuecomment-542879027:839,Deployability,pipeline,pipeline,839,"I'm also interested in this since I'll be analyzing some HTO data soon. . As I wrote [here](https://github.com/theislab/scanpy/pull/797/files/8bcee13537d6353399f1722bac7f60bc943a482f#r335664372), I think we should also discuss the I/O and storage procedures for ADT/HTOs. . @wflynny it makes a lot of sense to use `adata.obsm[""X_adt""]` and `adata.obsm[""X_hto""]` for ADT and HTO counts. One caveat is that we cannot store ADT/HTO barcode strings in `adata.obsm` but I don't know how important this is. For I/O, we can define a `sc.read_antibody_tags(filename)` that reads HTO/ADTs into the `adata.obsm['X_hto']`. Then a simple `sc.pp.classify_hashtags()` method can determine classes and creates new fields like HTO_class in `adata.obs`. @wflynny @njbernstein what do you think? @wflynny what else do you think is needed for a nice HTO/ADT pipeline?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351#issuecomment-542879027
https://github.com/scverse/scanpy/issues/351#issuecomment-542879027:619,Usability,simpl,simple,619,"I'm also interested in this since I'll be analyzing some HTO data soon. . As I wrote [here](https://github.com/theislab/scanpy/pull/797/files/8bcee13537d6353399f1722bac7f60bc943a482f#r335664372), I think we should also discuss the I/O and storage procedures for ADT/HTOs. . @wflynny it makes a lot of sense to use `adata.obsm[""X_adt""]` and `adata.obsm[""X_hto""]` for ADT and HTO counts. One caveat is that we cannot store ADT/HTO barcode strings in `adata.obsm` but I don't know how important this is. For I/O, we can define a `sc.read_antibody_tags(filename)` that reads HTO/ADTs into the `adata.obsm['X_hto']`. Then a simple `sc.pp.classify_hashtags()` method can determine classes and creates new fields like HTO_class in `adata.obs`. @wflynny @njbernstein what do you think? @wflynny what else do you think is needed for a nice HTO/ADT pipeline?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351#issuecomment-542879027
https://github.com/scverse/scanpy/issues/351#issuecomment-543387900:1567,Deployability,pipeline,pipeline,1567,", cholesterol tags, etc.) we've been doing. However, most of the time I'll just load the tags matrix in as a pandas dataframe and run them through a demuxing function that'll modify `adata.obs`. A couple challenges/ideas to consider:. * at our facility, we're typically building the same Illumina i7 index (`ATTACTCG`) into all tag libraries. This leads to some tricky situations when using a NovaSeq for sequencing since the multiple tag libraries (with disjoint sets of tags) may be run on the same sequencing flowcell lane. This results in a single set of FASTQ files and thus a single barcode-tag matrix for all tag libraries on that lane. Therefore, the mapping between transcriptome AnnData objects <-> tag library matrices is not always 1-to-1.; * in my experience, HTO libraries have a large variance in quality, so for the most part I've been using the transcriptome as my ""ground truth"" as to what is a cell. However, I imagine others use HTOs to ""rescue"" cells that were not called by their pipeline of choice (and I hope to do this once I build enough trust in the data). In that case, one would want to intersect the HTO classifications with the raw cell-gene matrix.; * not all tags are antibody based, so I'd vote for naming all related functions `*hashtags()`. I'd therefore vote for something like the following design:; ```{python}; # htos is a AnnData object; htos = sc.read_hashtags(filename) . # classify_hashtags adds a classification to the hto AnnData object; # kwargs might involve things like `use_tags=[""tag1"", ""tag2"", ""tag3""]`; sc.pp.classify_hashtags(htos, **kwargs); print(htos.obs.classification) . # demuxing cell-gene matrix(es) could then be done like; rna1 = sc.read_10x_h5(...); rna2 = sc.read_10x_h5(...); # sc.pp.demux_by_hashtag(adata_hto, *adata_rna, tag_groups=None, ...); sc.pp.demux_by_hashtag(; htos, ; rna1, rna2, ; tag_groups=[(""tag1"", ""tag3"", ""tag5""), (""tag2"", ""tag4"", ""tag6"")]; ); ```; @gokceneraslan This is more complex than what you suggested, but I ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351#issuecomment-543387900
https://github.com/scverse/scanpy/issues/351#issuecomment-543387900:353,Modifiability,layers,layers,353,"@gokceneraslan Hey, sorry for my long silence on this. I've been using @Hoohm's [https://github.com/Hoohm/CITE-seq-Count](CITE-seq-Count) for ADT/HTO tag counting which produces (in recent versions) a 10X v3 style `mtx` directory for both reads and UMIs. In some cases, I'll load these in as their own AnnData object with reads and counts as different `layers` which is helpful in computing per-cell or per-tag ""sequencing saturation"" and other metrics involving both reads and counts. This is especially helpful for investigating some pilot experiments (lipid tags, cholesterol tags, etc.) we've been doing. However, most of the time I'll just load the tags matrix in as a pandas dataframe and run them through a demuxing function that'll modify `adata.obs`. A couple challenges/ideas to consider:. * at our facility, we're typically building the same Illumina i7 index (`ATTACTCG`) into all tag libraries. This leads to some tricky situations when using a NovaSeq for sequencing since the multiple tag libraries (with disjoint sets of tags) may be run on the same sequencing flowcell lane. This results in a single set of FASTQ files and thus a single barcode-tag matrix for all tag libraries on that lane. Therefore, the mapping between transcriptome AnnData objects <-> tag library matrices is not always 1-to-1.; * in my experience, HTO libraries have a large variance in quality, so for the most part I've been using the transcriptome as my ""ground truth"" as to what is a cell. However, I imagine others use HTOs to ""rescue"" cells that were not called by their pipeline of choice (and I hope to do this once I build enough trust in the data). In that case, one would want to intersect the HTO classifications with the raw cell-gene matrix.; * not all tags are antibody based, so I'd vote for naming all related functions `*hashtags()`. I'd therefore vote for something like the following design:; ```{python}; # htos is a AnnData object; htos = sc.read_hashtags(filename) . # classify_hashtags a",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351#issuecomment-543387900
https://github.com/scverse/scanpy/issues/351#issuecomment-543387900:275,Performance,load,load,275,"@gokceneraslan Hey, sorry for my long silence on this. I've been using @Hoohm's [https://github.com/Hoohm/CITE-seq-Count](CITE-seq-Count) for ADT/HTO tag counting which produces (in recent versions) a 10X v3 style `mtx` directory for both reads and UMIs. In some cases, I'll load these in as their own AnnData object with reads and counts as different `layers` which is helpful in computing per-cell or per-tag ""sequencing saturation"" and other metrics involving both reads and counts. This is especially helpful for investigating some pilot experiments (lipid tags, cholesterol tags, etc.) we've been doing. However, most of the time I'll just load the tags matrix in as a pandas dataframe and run them through a demuxing function that'll modify `adata.obs`. A couple challenges/ideas to consider:. * at our facility, we're typically building the same Illumina i7 index (`ATTACTCG`) into all tag libraries. This leads to some tricky situations when using a NovaSeq for sequencing since the multiple tag libraries (with disjoint sets of tags) may be run on the same sequencing flowcell lane. This results in a single set of FASTQ files and thus a single barcode-tag matrix for all tag libraries on that lane. Therefore, the mapping between transcriptome AnnData objects <-> tag library matrices is not always 1-to-1.; * in my experience, HTO libraries have a large variance in quality, so for the most part I've been using the transcriptome as my ""ground truth"" as to what is a cell. However, I imagine others use HTOs to ""rescue"" cells that were not called by their pipeline of choice (and I hope to do this once I build enough trust in the data). In that case, one would want to intersect the HTO classifications with the raw cell-gene matrix.; * not all tags are antibody based, so I'd vote for naming all related functions `*hashtags()`. I'd therefore vote for something like the following design:; ```{python}; # htos is a AnnData object; htos = sc.read_hashtags(filename) . # classify_hashtags a",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351#issuecomment-543387900
https://github.com/scverse/scanpy/issues/351#issuecomment-543387900:645,Performance,load,load,645,"@gokceneraslan Hey, sorry for my long silence on this. I've been using @Hoohm's [https://github.com/Hoohm/CITE-seq-Count](CITE-seq-Count) for ADT/HTO tag counting which produces (in recent versions) a 10X v3 style `mtx` directory for both reads and UMIs. In some cases, I'll load these in as their own AnnData object with reads and counts as different `layers` which is helpful in computing per-cell or per-tag ""sequencing saturation"" and other metrics involving both reads and counts. This is especially helpful for investigating some pilot experiments (lipid tags, cholesterol tags, etc.) we've been doing. However, most of the time I'll just load the tags matrix in as a pandas dataframe and run them through a demuxing function that'll modify `adata.obs`. A couple challenges/ideas to consider:. * at our facility, we're typically building the same Illumina i7 index (`ATTACTCG`) into all tag libraries. This leads to some tricky situations when using a NovaSeq for sequencing since the multiple tag libraries (with disjoint sets of tags) may be run on the same sequencing flowcell lane. This results in a single set of FASTQ files and thus a single barcode-tag matrix for all tag libraries on that lane. Therefore, the mapping between transcriptome AnnData objects <-> tag library matrices is not always 1-to-1.; * in my experience, HTO libraries have a large variance in quality, so for the most part I've been using the transcriptome as my ""ground truth"" as to what is a cell. However, I imagine others use HTOs to ""rescue"" cells that were not called by their pipeline of choice (and I hope to do this once I build enough trust in the data). In that case, one would want to intersect the HTO classifications with the raw cell-gene matrix.; * not all tags are antibody based, so I'd vote for naming all related functions `*hashtags()`. I'd therefore vote for something like the following design:; ```{python}; # htos is a AnnData object; htos = sc.read_hashtags(filename) . # classify_hashtags a",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351#issuecomment-543387900
https://github.com/scverse/scanpy/issues/351#issuecomment-543387900:1829,Security,hash,hashtags,1829,"hat'll modify `adata.obs`. A couple challenges/ideas to consider:. * at our facility, we're typically building the same Illumina i7 index (`ATTACTCG`) into all tag libraries. This leads to some tricky situations when using a NovaSeq for sequencing since the multiple tag libraries (with disjoint sets of tags) may be run on the same sequencing flowcell lane. This results in a single set of FASTQ files and thus a single barcode-tag matrix for all tag libraries on that lane. Therefore, the mapping between transcriptome AnnData objects <-> tag library matrices is not always 1-to-1.; * in my experience, HTO libraries have a large variance in quality, so for the most part I've been using the transcriptome as my ""ground truth"" as to what is a cell. However, I imagine others use HTOs to ""rescue"" cells that were not called by their pipeline of choice (and I hope to do this once I build enough trust in the data). In that case, one would want to intersect the HTO classifications with the raw cell-gene matrix.; * not all tags are antibody based, so I'd vote for naming all related functions `*hashtags()`. I'd therefore vote for something like the following design:; ```{python}; # htos is a AnnData object; htos = sc.read_hashtags(filename) . # classify_hashtags adds a classification to the hto AnnData object; # kwargs might involve things like `use_tags=[""tag1"", ""tag2"", ""tag3""]`; sc.pp.classify_hashtags(htos, **kwargs); print(htos.obs.classification) . # demuxing cell-gene matrix(es) could then be done like; rna1 = sc.read_10x_h5(...); rna2 = sc.read_10x_h5(...); # sc.pp.demux_by_hashtag(adata_hto, *adata_rna, tag_groups=None, ...); sc.pp.demux_by_hashtag(; htos, ; rna1, rna2, ; tag_groups=[(""tag1"", ""tag3"", ""tag5""), (""tag2"", ""tag4"", ""tag6"")]; ); ```; @gokceneraslan This is more complex than what you suggested, but I think is sufficiently general to cover my needs as listed above. Let me know what you think---I'll have some development time next week to possible contribute to this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351#issuecomment-543387900
https://github.com/scverse/scanpy/issues/351#issuecomment-561856270:108,Security,hash,hashsolo,108,"I'm happy to implement my method, when we have a consensus. https://github.com/calico/solo/blob/master/solo/hashsolo.py. https://www.biorxiv.org/content/10.1101/841981v1",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351#issuecomment-561856270
https://github.com/scverse/scanpy/issues/351#issuecomment-601407528:81,Deployability,install,install,81,@aditisk I'm the author of this method https://github.com/calico/solo. it should install relatively easily if you have any issues I'm happy to help. The main functionality it doesn't have is `tag_groups` so you'd have t manually create that if you have used multiple hashes per group.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351#issuecomment-601407528
https://github.com/scverse/scanpy/issues/351#issuecomment-601407528:267,Security,hash,hashes,267,@aditisk I'm the author of this method https://github.com/calico/solo. it should install relatively easily if you have any issues I'm happy to help. The main functionality it doesn't have is `tag_groups` so you'd have t manually create that if you have used multiple hashes per group.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351#issuecomment-601407528
https://github.com/scverse/scanpy/issues/351#issuecomment-698009524:66,Security,hash,hashing,66,I'd like to tackle this. Can someone tell me how we want to store hashing data in an `anndata` object?; @flying-sheep @fidelram . I'll take back up porting hashsolo to scanpy,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351#issuecomment-698009524
https://github.com/scverse/scanpy/issues/351#issuecomment-698009524:156,Security,hash,hashsolo,156,I'd like to tackle this. Can someone tell me how we want to store hashing data in an `anndata` object?; @flying-sheep @fidelram . I'll take back up porting hashsolo to scanpy,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351#issuecomment-698009524
https://github.com/scverse/scanpy/issues/351#issuecomment-758885784:622,Testability,test,tests,622,"I've been using a rough python implementation of Chris McGinnis's MULTIseq demuxing code for all my multiplexed experiments. This algorithm has been incorporated into Seurat as an alternative to their default `HTODemux` function. This [recent preprint](https://www.biorxiv.org/content/10.1101/2020.11.16.384222v1) suggests it's one of the better algorithms for sample demultiplexing. I recently put my implementation on GitHub here: https://github.com/wflynny/multiseq-py. Is there interest in including this in `scanpy.external` in addition to solo? If so, I can invest effort into cleaning up the implementation, adding tests, etc.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351#issuecomment-758885784
https://github.com/scverse/scanpy/issues/351#issuecomment-759510507:418,Testability,log,log,418,"If your implementation already uses scanpy, the best is to keep it in your repository and we can link to it from scanpy (see https://scanpy.readthedocs.io/en/stable/ecosystem.html). I did some work on HTOs in the past and for me what worked best was to fit a gaussian mixture but I had not followed the new methods. Something that helped was to visualize the results as follows (each row a different barcode, x axis = log HTO):. ![image](https://user-images.githubusercontent.com/4964309/104469555-edfc2400-55b8-11eb-9f47-580395b255a7.png). If you are interested I can share the code with you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351#issuecomment-759510507
https://github.com/scverse/scanpy/issues/351#issuecomment-759554768:29,Deployability,update,update,29,"@fidelram Sounds good---I'll update the code and then open a PR to get it added to the ecosystem docs. In my experience with HTOs (and now LMOs & CMOs), GMMs and even poisson/negative binomial mixture models don't work particularly well for all experiments as they tend to only call 50-70% of cells as singlets/multiplets. The remaining ""negatives"" or uncalled cells can really hamper some experimental designs (like when tags correspond to different conditions/perturbations). Anecdotally, multiplexing seems substantially more difficult to get right for tissues rather than blood or cell/organoid lines. . That said, I'd be interested in any plotting code you could share :). I very much appreciate all the plotting functionality you've implemented in scanpy!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351#issuecomment-759554768
https://github.com/scverse/scanpy/issues/351#issuecomment-759575072:353,Modifiability,variab,variability,353,"@wflynny `hashsolo` allows you to set a prior for your expected rate negatives, singlets, and doublets, which helps quite a bit with the issue you described despite modeling the log CMO counts as a normal distribution. Additionally, you can also add cell types if you've done cell-type annotation or even leiden clustering labels to help with cell type variability with CMO counts. This helped me quite a bit in kidney where NK cells had far fewer CMO counts than other cells despite being apparently live cells, e.g. good gene diversity and low mitochondrial gene percentage. @fidelram I'd be happy to add a visualization tool like you suggested if you have the code laying around.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351#issuecomment-759575072
https://github.com/scverse/scanpy/issues/351#issuecomment-759575072:10,Security,hash,hashsolo,10,"@wflynny `hashsolo` allows you to set a prior for your expected rate negatives, singlets, and doublets, which helps quite a bit with the issue you described despite modeling the log CMO counts as a normal distribution. Additionally, you can also add cell types if you've done cell-type annotation or even leiden clustering labels to help with cell type variability with CMO counts. This helped me quite a bit in kidney where NK cells had far fewer CMO counts than other cells despite being apparently live cells, e.g. good gene diversity and low mitochondrial gene percentage. @fidelram I'd be happy to add a visualization tool like you suggested if you have the code laying around.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351#issuecomment-759575072
https://github.com/scverse/scanpy/issues/351#issuecomment-759575072:178,Testability,log,log,178,"@wflynny `hashsolo` allows you to set a prior for your expected rate negatives, singlets, and doublets, which helps quite a bit with the issue you described despite modeling the log CMO counts as a normal distribution. Additionally, you can also add cell types if you've done cell-type annotation or even leiden clustering labels to help with cell type variability with CMO counts. This helped me quite a bit in kidney where NK cells had far fewer CMO counts than other cells despite being apparently live cells, e.g. good gene diversity and low mitochondrial gene percentage. @fidelram I'd be happy to add a visualization tool like you suggested if you have the code laying around.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351#issuecomment-759575072
https://github.com/scverse/scanpy/issues/351#issuecomment-759587209:149,Security,hash,hashsolo,149,"@njbernstein Probably not the right place for this discussion, but a couple of follow-up questions for you:; - Do you happen to have a benchmark of `hashsolo` vs the other demuxing algos? It's been on my todo list for ..a while.. but still haven't gotten around to doing it. I've seen the benchmarks of the doublet finding capabilities of `solo` and they look good. As a user of scrublet, it'd be nice to have one tool/codebase that handles both transcriptomic and tag multiplets.; - Are you open to PRs? I'd at least like to have functionality to generate the initial h5ad object containing the tag counts from the output of `CITE-seq-Count`.; - Regarding non-antibody tags, have you noticed celltype-specific preferential binding? I've had problems with LMO/CMOs where not tagging particular celltypes (like some epithelial subtypes where we had 2-3 orders of magnitude lower tag counts).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351#issuecomment-759587209
https://github.com/scverse/scanpy/issues/351#issuecomment-759587209:135,Testability,benchmark,benchmark,135,"@njbernstein Probably not the right place for this discussion, but a couple of follow-up questions for you:; - Do you happen to have a benchmark of `hashsolo` vs the other demuxing algos? It's been on my todo list for ..a while.. but still haven't gotten around to doing it. I've seen the benchmarks of the doublet finding capabilities of `solo` and they look good. As a user of scrublet, it'd be nice to have one tool/codebase that handles both transcriptomic and tag multiplets.; - Are you open to PRs? I'd at least like to have functionality to generate the initial h5ad object containing the tag counts from the output of `CITE-seq-Count`.; - Regarding non-antibody tags, have you noticed celltype-specific preferential binding? I've had problems with LMO/CMOs where not tagging particular celltypes (like some epithelial subtypes where we had 2-3 orders of magnitude lower tag counts).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351#issuecomment-759587209
https://github.com/scverse/scanpy/issues/351#issuecomment-759587209:289,Testability,benchmark,benchmarks,289,"@njbernstein Probably not the right place for this discussion, but a couple of follow-up questions for you:; - Do you happen to have a benchmark of `hashsolo` vs the other demuxing algos? It's been on my todo list for ..a while.. but still haven't gotten around to doing it. I've seen the benchmarks of the doublet finding capabilities of `solo` and they look good. As a user of scrublet, it'd be nice to have one tool/codebase that handles both transcriptomic and tag multiplets.; - Are you open to PRs? I'd at least like to have functionality to generate the initial h5ad object containing the tag counts from the output of `CITE-seq-Count`.; - Regarding non-antibody tags, have you noticed celltype-specific preferential binding? I've had problems with LMO/CMOs where not tagging particular celltypes (like some epithelial subtypes where we had 2-3 orders of magnitude lower tag counts).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351#issuecomment-759587209
https://github.com/scverse/scanpy/issues/351#issuecomment-759597008:385,Availability,recover,recover,385,"In supplementary figure 9 of our paper, I did a light comparison of tools using the demuxlet data as ground truth: https://www.cell.com/cms/10.1016/j.cels.2020.05.010/attachment/040c239d-1e70-42a4-8974-9fbd75c65551/mmc1.pdf; Which I think is a fine first stab at getting at this comparison, but it could be better. Hashsolo performance was comparable with other methods but is able to recover cell types with lower CMO counts. . I think that sounds great. That's an issue we had as well, but I noticed it occurring for NK cells in kidney; ![Screen Shot 2021-01-13 at 9 18 30 AM](https://user-images.githubusercontent.com/6864886/104486266-5d095680-5580-11eb-971e-c882063f2a45.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351#issuecomment-759597008
https://github.com/scverse/scanpy/issues/351#issuecomment-759597008:324,Performance,perform,performance,324,"In supplementary figure 9 of our paper, I did a light comparison of tools using the demuxlet data as ground truth: https://www.cell.com/cms/10.1016/j.cels.2020.05.010/attachment/040c239d-1e70-42a4-8974-9fbd75c65551/mmc1.pdf; Which I think is a fine first stab at getting at this comparison, but it could be better. Hashsolo performance was comparable with other methods but is able to recover cell types with lower CMO counts. . I think that sounds great. That's an issue we had as well, but I noticed it occurring for NK cells in kidney; ![Screen Shot 2021-01-13 at 9 18 30 AM](https://user-images.githubusercontent.com/6864886/104486266-5d095680-5580-11eb-971e-c882063f2a45.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351#issuecomment-759597008
https://github.com/scverse/scanpy/issues/351#issuecomment-759597008:385,Safety,recover,recover,385,"In supplementary figure 9 of our paper, I did a light comparison of tools using the demuxlet data as ground truth: https://www.cell.com/cms/10.1016/j.cels.2020.05.010/attachment/040c239d-1e70-42a4-8974-9fbd75c65551/mmc1.pdf; Which I think is a fine first stab at getting at this comparison, but it could be better. Hashsolo performance was comparable with other methods but is able to recover cell types with lower CMO counts. . I think that sounds great. That's an issue we had as well, but I noticed it occurring for NK cells in kidney; ![Screen Shot 2021-01-13 at 9 18 30 AM](https://user-images.githubusercontent.com/6864886/104486266-5d095680-5580-11eb-971e-c882063f2a45.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351#issuecomment-759597008
https://github.com/scverse/scanpy/issues/351#issuecomment-759597008:315,Security,Hash,Hashsolo,315,"In supplementary figure 9 of our paper, I did a light comparison of tools using the demuxlet data as ground truth: https://www.cell.com/cms/10.1016/j.cels.2020.05.010/attachment/040c239d-1e70-42a4-8974-9fbd75c65551/mmc1.pdf; Which I think is a fine first stab at getting at this comparison, but it could be better. Hashsolo performance was comparable with other methods but is able to recover cell types with lower CMO counts. . I think that sounds great. That's an issue we had as well, but I noticed it occurring for NK cells in kidney; ![Screen Shot 2021-01-13 at 9 18 30 AM](https://user-images.githubusercontent.com/6864886/104486266-5d095680-5580-11eb-971e-c882063f2a45.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351#issuecomment-759597008
https://github.com/scverse/scanpy/issues/351#issuecomment-788017481:4,Deployability,update,updates,4,Any updates on this thread?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351#issuecomment-788017481
https://github.com/scverse/scanpy/issues/351#issuecomment-788095696:13,Security,hash,hashsolo,13,@brianpenghe hashsolo is implementer in scanpy now,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351#issuecomment-788095696
https://github.com/scverse/scanpy/issues/351#issuecomment-788842420:15,Security,hash,hashsolo,15,> @brianpenghe hashsolo is implementer in scanpy now. That would be awesome. Which version of Scanpy includes hashsolo? Any Scanpy tutorials?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351#issuecomment-788842420
https://github.com/scverse/scanpy/issues/351#issuecomment-788842420:110,Security,hash,hashsolo,110,> @brianpenghe hashsolo is implementer in scanpy now. That would be awesome. Which version of Scanpy includes hashsolo? Any Scanpy tutorials?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351#issuecomment-788842420
https://github.com/scverse/scanpy/issues/351#issuecomment-1975375458:124,Security,hash,hashsolo,124,@brianpenghe solo is here at least: https://docs.scvi-tools.org/en/stable/api/reference/scvi.external.SOLO.html Don't think hashsolo is anywhere yet though,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351#issuecomment-1975375458
https://github.com/scverse/scanpy/issues/351#issuecomment-1975387276:9,Security,hash,hashsolo,9,@Zethson hashsolo is in scanpy already in the external api. Let me know if you have any questions about it !,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351#issuecomment-1975387276
https://github.com/scverse/scanpy/issues/351#issuecomment-2304281523:74,Security,hash,hashsholo,74,Hi @njbernstein . I'm using scanpy version 1.10.2 and when I tried to run hashsholo but it's not working and I'm not sure why. ![image](https://github.com/user-attachments/assets/3a542123-f7b1-432d-9898-cb25ed52bd51). Any suggestions? Thank you. Edit: htos is just the list with he two hashtag names shown in the obs,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351#issuecomment-2304281523
https://github.com/scverse/scanpy/issues/351#issuecomment-2304281523:286,Security,hash,hashtag,286,Hi @njbernstein . I'm using scanpy version 1.10.2 and when I tried to run hashsholo but it's not working and I'm not sure why. ![image](https://github.com/user-attachments/assets/3a542123-f7b1-432d-9898-cb25ed52bd51). Any suggestions? Thank you. Edit: htos is just the list with he two hashtag names shown in the obs,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351#issuecomment-2304281523
https://github.com/scverse/scanpy/issues/351#issuecomment-2305593426:99,Security,hash,hashsolo,99,"@Lucas-Maciel ; Can you try setting the `number_of_noise_barcodes = 1`? . e.g. `scanpy.external.pp.hashsolo(adata, cell_hashing_columns, *, priors=(0.01, 0.8, 0.19), pre_existing_clusters=None, number_of_noise_barcodes=1)`; https://scanpy.readthedocs.io/en/stable/generated/scanpy.external.pp.hashsolo.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351#issuecomment-2305593426
https://github.com/scverse/scanpy/issues/351#issuecomment-2305593426:293,Security,hash,hashsolo,293,"@Lucas-Maciel ; Can you try setting the `number_of_noise_barcodes = 1`? . e.g. `scanpy.external.pp.hashsolo(adata, cell_hashing_columns, *, priors=(0.01, 0.8, 0.19), pre_existing_clusters=None, number_of_noise_barcodes=1)`; https://scanpy.readthedocs.io/en/stable/generated/scanpy.external.pp.hashsolo.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351#issuecomment-2305593426
https://github.com/scverse/scanpy/issues/353#issuecomment-437726312:0,Usability,Simpl,Simply,0,"Simply use standard slicing. The most elegant way in this case would be something like [this](https://github.com/theislab/scanpy/blob/7de1f5159c91d6d2243bb7866d9495ee6747c750/scanpy/datasets/__init__.py#L108-L109), I guess. 🙂",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/353#issuecomment-437726312
https://github.com/scverse/scanpy/issues/355#issuecomment-437501315:0,Deployability,Update,Update,0,"Update: the issue is that `apt-get install -y python3-pip` installs Python version 3.5.2. You would need to upgrade to version 3.6, as the readme suggests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/355#issuecomment-437501315
https://github.com/scverse/scanpy/issues/355#issuecomment-437501315:35,Deployability,install,install,35,"Update: the issue is that `apt-get install -y python3-pip` installs Python version 3.5.2. You would need to upgrade to version 3.6, as the readme suggests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/355#issuecomment-437501315
https://github.com/scverse/scanpy/issues/355#issuecomment-437501315:59,Deployability,install,installs,59,"Update: the issue is that `apt-get install -y python3-pip` installs Python version 3.5.2. You would need to upgrade to version 3.6, as the readme suggests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/355#issuecomment-437501315
https://github.com/scverse/scanpy/issues/355#issuecomment-437501315:108,Deployability,upgrade,upgrade,108,"Update: the issue is that `apt-get install -y python3-pip` installs Python version 3.5.2. You would need to upgrade to version 3.6, as the readme suggests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/355#issuecomment-437501315
https://github.com/scverse/scanpy/pull/358#issuecomment-438006625:96,Deployability,update,update,96,"PS: Don't worry about the tutorial, I'll move that into the Scanpy docs without images soon and update it there. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/358#issuecomment-438006625
https://github.com/scverse/scanpy/pull/360#issuecomment-439746463:102,Deployability,install,installation,102,"Hm, yes it's nice that things are simpler now, but the point of the script before was to use the fast installation of the conda binaries... . Before your commit: 3 min 46 s test time (https://travis-ci.org/theislab/scanpy/builds/454438531?utm_source=github_status&utm_medium=notification). After your commit: 6 min 46 s test time (https://travis-ci.org/theislab/scanpy/builds/454487170?utm_source=github_status&utm_medium=notification). While the 3 min 46 s are way too long, there is still a good chance that you realize that your commit broke everything. After almost 7 min, you're almost always doing something else already. I also feel kind of bad about travis's servers. ;)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/360#issuecomment-439746463
https://github.com/scverse/scanpy/pull/360#issuecomment-439746463:173,Testability,test,test,173,"Hm, yes it's nice that things are simpler now, but the point of the script before was to use the fast installation of the conda binaries... . Before your commit: 3 min 46 s test time (https://travis-ci.org/theislab/scanpy/builds/454438531?utm_source=github_status&utm_medium=notification). After your commit: 6 min 46 s test time (https://travis-ci.org/theislab/scanpy/builds/454487170?utm_source=github_status&utm_medium=notification). While the 3 min 46 s are way too long, there is still a good chance that you realize that your commit broke everything. After almost 7 min, you're almost always doing something else already. I also feel kind of bad about travis's servers. ;)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/360#issuecomment-439746463
https://github.com/scverse/scanpy/pull/360#issuecomment-439746463:320,Testability,test,test,320,"Hm, yes it's nice that things are simpler now, but the point of the script before was to use the fast installation of the conda binaries... . Before your commit: 3 min 46 s test time (https://travis-ci.org/theislab/scanpy/builds/454438531?utm_source=github_status&utm_medium=notification). After your commit: 6 min 46 s test time (https://travis-ci.org/theislab/scanpy/builds/454487170?utm_source=github_status&utm_medium=notification). While the 3 min 46 s are way too long, there is still a good chance that you realize that your commit broke everything. After almost 7 min, you're almost always doing something else already. I also feel kind of bad about travis's servers. ;)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/360#issuecomment-439746463
https://github.com/scverse/scanpy/pull/360#issuecomment-439746463:34,Usability,simpl,simpler,34,"Hm, yes it's nice that things are simpler now, but the point of the script before was to use the fast installation of the conda binaries... . Before your commit: 3 min 46 s test time (https://travis-ci.org/theislab/scanpy/builds/454438531?utm_source=github_status&utm_medium=notification). After your commit: 6 min 46 s test time (https://travis-ci.org/theislab/scanpy/builds/454487170?utm_source=github_status&utm_medium=notification). While the 3 min 46 s are way too long, there is still a good chance that you realize that your commit broke everything. After almost 7 min, you're almost always doing something else already. I also feel kind of bad about travis's servers. ;)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/360#issuecomment-439746463
https://github.com/scverse/scanpy/pull/360#issuecomment-439747800:203,Deployability,install,install,203,"Well, so essentially, this PR reversed what I did quite some time ago to speed up the CI... But, let's leave it like this. Hopefully, at some point, we'll have a less hackish way than the previous conda install script of dealing with this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/360#issuecomment-439747800
https://github.com/scverse/scanpy/pull/360#issuecomment-439811200:242,Deployability,install,install,242,"> Well, so essentially, this PR reversed what I did quite some time ago to speed up the CI... Exactly! With the crucial difference that after the first build, binary packages for everything are being cached by pip, *and* now we don’t have to install conda every build. The one you linked to was just this one initial build. The real numbers are now:. &nbsp; | Runtime | Total | Link; --- | --- | --- | ---; Before (conda) | 4m47s | 8m33s | https://travis-ci.org/theislab/scanpy/builds/454438531 ; After (pip) | 3m34s | 5m 2s | https://travis-ci.org/theislab/scanpy/builds/456855724. > But, let's leave it like this. Hopefully, at some point, we'll have a less hackish way than the previous conda install script of dealing with this. The time is now, wheee!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/360#issuecomment-439811200
https://github.com/scverse/scanpy/pull/360#issuecomment-439811200:696,Deployability,install,install,696,"> Well, so essentially, this PR reversed what I did quite some time ago to speed up the CI... Exactly! With the crucial difference that after the first build, binary packages for everything are being cached by pip, *and* now we don’t have to install conda every build. The one you linked to was just this one initial build. The real numbers are now:. &nbsp; | Runtime | Total | Link; --- | --- | --- | ---; Before (conda) | 4m47s | 8m33s | https://travis-ci.org/theislab/scanpy/builds/454438531 ; After (pip) | 3m34s | 5m 2s | https://travis-ci.org/theislab/scanpy/builds/456855724. > But, let's leave it like this. Hopefully, at some point, we'll have a less hackish way than the previous conda install script of dealing with this. The time is now, wheee!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/360#issuecomment-439811200
https://github.com/scverse/scanpy/pull/360#issuecomment-439811200:200,Performance,cache,cached,200,"> Well, so essentially, this PR reversed what I did quite some time ago to speed up the CI... Exactly! With the crucial difference that after the first build, binary packages for everything are being cached by pip, *and* now we don’t have to install conda every build. The one you linked to was just this one initial build. The real numbers are now:. &nbsp; | Runtime | Total | Link; --- | --- | --- | ---; Before (conda) | 4m47s | 8m33s | https://travis-ci.org/theislab/scanpy/builds/454438531 ; After (pip) | 3m34s | 5m 2s | https://travis-ci.org/theislab/scanpy/builds/456855724. > But, let's leave it like this. Hopefully, at some point, we'll have a less hackish way than the previous conda install script of dealing with this. The time is now, wheee!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/360#issuecomment-439811200
https://github.com/scverse/scanpy/pull/360#issuecomment-439837732:182,Performance,cache,cache,182,"Oh, that's wonderful and exactly what I had hoped pip on the travis server would do! :smile: You mentioned that you might look into it at some point. I just didn't notice the ; ```; cache: pip; ```; line in the commit... Great that you figured this out! Test times now are really nice, in particular, as I can easily speed them up further... So cool! :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/360#issuecomment-439837732
https://github.com/scverse/scanpy/pull/360#issuecomment-439837732:254,Testability,Test,Test,254,"Oh, that's wonderful and exactly what I had hoped pip on the travis server would do! :smile: You mentioned that you might look into it at some point. I just didn't notice the ; ```; cache: pip; ```; line in the commit... Great that you figured this out! Test times now are really nice, in particular, as I can easily speed them up further... So cool! :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/360#issuecomment-439837732
https://github.com/scverse/scanpy/pull/361#issuecomment-438281756:53,Integrability,depend,dependencies,53,Think the Travis thing failing is because of the new dependencies,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361#issuecomment-438281756
https://github.com/scverse/scanpy/pull/361#issuecomment-438316053:4,Testability,test,tests,4,"The tests don’t fail, but you should still add the extra to setup.py",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361#issuecomment-438316053
https://github.com/scverse/scanpy/pull/361#issuecomment-438320501:137,Deployability,install,install,137,"> The tests don’t fail, but you should still add the extra to setup.py. Is the fact that you don't list `'docs'` in your pip thing (`pip install -e .[louvain,leiden,test]`) purposeful?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361#issuecomment-438320501
https://github.com/scverse/scanpy/pull/361#issuecomment-438320501:6,Testability,test,tests,6,"> The tests don’t fail, but you should still add the extra to setup.py. Is the fact that you don't list `'docs'` in your pip thing (`pip install -e .[louvain,leiden,test]`) purposeful?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361#issuecomment-438320501
https://github.com/scverse/scanpy/pull/361#issuecomment-438320501:165,Testability,test,test,165,"> The tests don’t fail, but you should still add the extra to setup.py. Is the fact that you don't list `'docs'` in your pip thing (`pip install -e .[louvain,leiden,test]`) purposeful?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361#issuecomment-438320501
https://github.com/scverse/scanpy/pull/361#issuecomment-438352107:23,Testability,test,tests,23,"Currently there are no tests, so those packages aren't actually needed. Looks good to me!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361#issuecomment-438352107
https://github.com/scverse/scanpy/pull/361#issuecomment-439104094:108,Usability,simpl,simply,108,"Hi! Sorry for frustrating you :( if you want I can fix and merge it manually. You're doing great work!. You simply need to import the things you're using in the annotations, then it'll work!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361#issuecomment-439104094
https://github.com/scverse/scanpy/pull/361#issuecomment-439331820:311,Testability,test,test,311,"I had to fix a few issues, if you want you can check it out: 66e64b40870035c3ee869e3baf34cf7110508d85. - I unified the parameter order with `louvain`; - I actually import it in `sc.tl`; - I added it to the docs here: https://scanpy.readthedocs.io/en/latest/api/#clustering-and-trajectory-inference; - I added a test; - I fixed the references (you had typos there: 2018 instead of 18 and a missing “L”); - You did this:. ```py; partition_kwargs['weights'] = None; if use_weights:; weights = np.array(g.es['weight']).astype(np.float64); # “weights” is never used then; ```. But I assume you meant this. Am I correct?. ```py; if use_weights:; partition_kwargs['weights'] = np.array(g.es['weight']).astype(np.float64); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361#issuecomment-439331820
https://github.com/scverse/scanpy/pull/361#issuecomment-439841050:112,Deployability,install,install,112,"```; try:; from bbknn import bbknn; except ImportError:; def bbknn(*args, **kwargs):; raise ImportError('Please install BBKNN: `pip3 install bbknn`'); ```. > I went that way since I didn’t want to make it look like we coded it (with the docs hosted on our page and so on). Do you think that’s a good solution or would you like it to be done differently?. This is great!. https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.bbknn.html#scanpy.api.pp.bbknn. But I actually don't see any docs there, I don't know why it doesn't find the original docstring... We'd like to have the reference to @ktpolanski preprint in the docstring in the first line together with a short summary of what it does and how it does it, just as for any other function. As this directly uses the implementation from @ktpolanski, we'd also want an explicit statement about that. > pp.bbknn is just an alias for bbknn.bbknn(). Refer to it for the documentation. ... can be removed. That should be evident... In the case of the `tl.leiden` wrapper, I'd also add an explicit statement that this wraps the leiden package of Traag (2018). Otherwise, all of this is great!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361#issuecomment-439841050
https://github.com/scverse/scanpy/pull/361#issuecomment-439841050:133,Deployability,install,install,133,"```; try:; from bbknn import bbknn; except ImportError:; def bbknn(*args, **kwargs):; raise ImportError('Please install BBKNN: `pip3 install bbknn`'); ```. > I went that way since I didn’t want to make it look like we coded it (with the docs hosted on our page and so on). Do you think that’s a good solution or would you like it to be done differently?. This is great!. https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.bbknn.html#scanpy.api.pp.bbknn. But I actually don't see any docs there, I don't know why it doesn't find the original docstring... We'd like to have the reference to @ktpolanski preprint in the docstring in the first line together with a short summary of what it does and how it does it, just as for any other function. As this directly uses the implementation from @ktpolanski, we'd also want an explicit statement about that. > pp.bbknn is just an alias for bbknn.bbknn(). Refer to it for the documentation. ... can be removed. That should be evident... In the case of the `tl.leiden` wrapper, I'd also add an explicit statement that this wraps the leiden package of Traag (2018). Otherwise, all of this is great!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361#issuecomment-439841050
https://github.com/scverse/scanpy/pull/361#issuecomment-439841050:1017,Integrability,wrap,wrapper,1017,"```; try:; from bbknn import bbknn; except ImportError:; def bbknn(*args, **kwargs):; raise ImportError('Please install BBKNN: `pip3 install bbknn`'); ```. > I went that way since I didn’t want to make it look like we coded it (with the docs hosted on our page and so on). Do you think that’s a good solution or would you like it to be done differently?. This is great!. https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.bbknn.html#scanpy.api.pp.bbknn. But I actually don't see any docs there, I don't know why it doesn't find the original docstring... We'd like to have the reference to @ktpolanski preprint in the docstring in the first line together with a short summary of what it does and how it does it, just as for any other function. As this directly uses the implementation from @ktpolanski, we'd also want an explicit statement about that. > pp.bbknn is just an alias for bbknn.bbknn(). Refer to it for the documentation. ... can be removed. That should be evident... In the case of the `tl.leiden` wrapper, I'd also add an explicit statement that this wraps the leiden package of Traag (2018). Otherwise, all of this is great!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361#issuecomment-439841050
https://github.com/scverse/scanpy/pull/361#issuecomment-439841050:1071,Integrability,wrap,wraps,1071,"```; try:; from bbknn import bbknn; except ImportError:; def bbknn(*args, **kwargs):; raise ImportError('Please install BBKNN: `pip3 install bbknn`'); ```. > I went that way since I didn’t want to make it look like we coded it (with the docs hosted on our page and so on). Do you think that’s a good solution or would you like it to be done differently?. This is great!. https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.bbknn.html#scanpy.api.pp.bbknn. But I actually don't see any docs there, I don't know why it doesn't find the original docstring... We'd like to have the reference to @ktpolanski preprint in the docstring in the first line together with a short summary of what it does and how it does it, just as for any other function. As this directly uses the implementation from @ktpolanski, we'd also want an explicit statement about that. > pp.bbknn is just an alias for bbknn.bbknn(). Refer to it for the documentation. ... can be removed. That should be evident... In the case of the `tl.leiden` wrapper, I'd also add an explicit statement that this wraps the leiden package of Traag (2018). Otherwise, all of this is great!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361#issuecomment-439841050
https://github.com/scverse/scanpy/pull/361#issuecomment-439855867:145,Deployability,install,installed,145,"> But I actually don't see any docs there, I don't know why it doesn't find the original docstring... That’s because on readthedocs, bbknn isn’t installed, so it uses the dummy version. > We'd like to have the reference to @ktpolanski preprint in the docstring in the first line together with a short summary of what it does and how it does it, just as for any other function. We could do that either by adding a docstring to the dummy version that links to https://bbknn.rtfd.io or by installing bbknn on rtd, and modifying the docstring programmatically. something like:. ```py; try:; from bbknn import bbknn; first_para, rest = bbknn.__doc__.split('\n\n', 1); bbknn.__doc__ =; '{}\n\nFor a graphical explanation, visit '; '`The bbknn project <https://github.com/Teichlab/bbknn>`__-\n\n{}'; .format(first_para, rest); except ImportError:; ...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361#issuecomment-439855867
https://github.com/scverse/scanpy/pull/361#issuecomment-439855867:486,Deployability,install,installing,486,"> But I actually don't see any docs there, I don't know why it doesn't find the original docstring... That’s because on readthedocs, bbknn isn’t installed, so it uses the dummy version. > We'd like to have the reference to @ktpolanski preprint in the docstring in the first line together with a short summary of what it does and how it does it, just as for any other function. We could do that either by adding a docstring to the dummy version that links to https://bbknn.rtfd.io or by installing bbknn on rtd, and modifying the docstring programmatically. something like:. ```py; try:; from bbknn import bbknn; first_para, rest = bbknn.__doc__.split('\n\n', 1); bbknn.__doc__ =; '{}\n\nFor a graphical explanation, visit '; '`The bbknn project <https://github.com/Teichlab/bbknn>`__-\n\n{}'; .format(first_para, rest); except ImportError:; ...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361#issuecomment-439855867
https://github.com/scverse/scanpy/issues/362#issuecomment-440912410:946,Availability,robust,robustness,946,"That is a wonderful solution! I will give it a shot shortly. Thank you so much for your help!. Here's some example code of how Seurat handles cluster scoring and merging with random forests and OOBE:. ```; pbmc <- ValidateClusters(pbmc, pc.use = 1:30, top.genes = 30). pbmc <- BuildClusterTree(pbmc, ; do.reorder = T, ; reorder.numeric = T). node.scores <- AssessNodes(pbmc). node.scores[order(node.scores$oobe,decreasing = T),] -> node.scores. nodes.merge <- node.scores[which(node.scores[,2] > 0.1),]; nodes.to.merge <- sort(nodes.merge$node) ; pbmc.merged <- pbmc. for (n in nodes.to.merge); {; pbmc.merged <- MergeNode(pbmc.merged, n); }. ```. Here's an explanation, as this code was derived from this recent (and awesome) publication:; ; From page 6 of the Supplementary Methods of Plass et al 2018: http://science.sciencemag.org/content/early/2018/04/18/science.aaq1723. To prevent obtaining spurious clusters result of overclustering, the robustness of the clusters was calculated using the function AssessNodes from Seurat. For each cluster, the average expression of all variable genes (4910) is computed and a phylogenetic tree based on the distance matrix in gene expression space is computed. Next, it computes an Out of Bag Error for a random forest classifier trained on each internal node split of the tree. We recursively build a tree and assessed all its nodes, merging all clusters with an out of bag error bigger than 0.1 until no such nodes were found.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/362#issuecomment-440912410
https://github.com/scverse/scanpy/issues/362#issuecomment-440912410:1237,Availability,Error,Error,1237,"That is a wonderful solution! I will give it a shot shortly. Thank you so much for your help!. Here's some example code of how Seurat handles cluster scoring and merging with random forests and OOBE:. ```; pbmc <- ValidateClusters(pbmc, pc.use = 1:30, top.genes = 30). pbmc <- BuildClusterTree(pbmc, ; do.reorder = T, ; reorder.numeric = T). node.scores <- AssessNodes(pbmc). node.scores[order(node.scores$oobe,decreasing = T),] -> node.scores. nodes.merge <- node.scores[which(node.scores[,2] > 0.1),]; nodes.to.merge <- sort(nodes.merge$node) ; pbmc.merged <- pbmc. for (n in nodes.to.merge); {; pbmc.merged <- MergeNode(pbmc.merged, n); }. ```. Here's an explanation, as this code was derived from this recent (and awesome) publication:; ; From page 6 of the Supplementary Methods of Plass et al 2018: http://science.sciencemag.org/content/early/2018/04/18/science.aaq1723. To prevent obtaining spurious clusters result of overclustering, the robustness of the clusters was calculated using the function AssessNodes from Seurat. For each cluster, the average expression of all variable genes (4910) is computed and a phylogenetic tree based on the distance matrix in gene expression space is computed. Next, it computes an Out of Bag Error for a random forest classifier trained on each internal node split of the tree. We recursively build a tree and assessed all its nodes, merging all clusters with an out of bag error bigger than 0.1 until no such nodes were found.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/362#issuecomment-440912410
https://github.com/scverse/scanpy/issues/362#issuecomment-440912410:1419,Availability,error,error,1419,"That is a wonderful solution! I will give it a shot shortly. Thank you so much for your help!. Here's some example code of how Seurat handles cluster scoring and merging with random forests and OOBE:. ```; pbmc <- ValidateClusters(pbmc, pc.use = 1:30, top.genes = 30). pbmc <- BuildClusterTree(pbmc, ; do.reorder = T, ; reorder.numeric = T). node.scores <- AssessNodes(pbmc). node.scores[order(node.scores$oobe,decreasing = T),] -> node.scores. nodes.merge <- node.scores[which(node.scores[,2] > 0.1),]; nodes.to.merge <- sort(nodes.merge$node) ; pbmc.merged <- pbmc. for (n in nodes.to.merge); {; pbmc.merged <- MergeNode(pbmc.merged, n); }. ```. Here's an explanation, as this code was derived from this recent (and awesome) publication:; ; From page 6 of the Supplementary Methods of Plass et al 2018: http://science.sciencemag.org/content/early/2018/04/18/science.aaq1723. To prevent obtaining spurious clusters result of overclustering, the robustness of the clusters was calculated using the function AssessNodes from Seurat. For each cluster, the average expression of all variable genes (4910) is computed and a phylogenetic tree based on the distance matrix in gene expression space is computed. Next, it computes an Out of Bag Error for a random forest classifier trained on each internal node split of the tree. We recursively build a tree and assessed all its nodes, merging all clusters with an out of bag error bigger than 0.1 until no such nodes were found.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/362#issuecomment-440912410
https://github.com/scverse/scanpy/issues/362#issuecomment-440912410:1080,Modifiability,variab,variable,1080,"That is a wonderful solution! I will give it a shot shortly. Thank you so much for your help!. Here's some example code of how Seurat handles cluster scoring and merging with random forests and OOBE:. ```; pbmc <- ValidateClusters(pbmc, pc.use = 1:30, top.genes = 30). pbmc <- BuildClusterTree(pbmc, ; do.reorder = T, ; reorder.numeric = T). node.scores <- AssessNodes(pbmc). node.scores[order(node.scores$oobe,decreasing = T),] -> node.scores. nodes.merge <- node.scores[which(node.scores[,2] > 0.1),]; nodes.to.merge <- sort(nodes.merge$node) ; pbmc.merged <- pbmc. for (n in nodes.to.merge); {; pbmc.merged <- MergeNode(pbmc.merged, n); }. ```. Here's an explanation, as this code was derived from this recent (and awesome) publication:; ; From page 6 of the Supplementary Methods of Plass et al 2018: http://science.sciencemag.org/content/early/2018/04/18/science.aaq1723. To prevent obtaining spurious clusters result of overclustering, the robustness of the clusters was calculated using the function AssessNodes from Seurat. For each cluster, the average expression of all variable genes (4910) is computed and a phylogenetic tree based on the distance matrix in gene expression space is computed. Next, it computes an Out of Bag Error for a random forest classifier trained on each internal node split of the tree. We recursively build a tree and assessed all its nodes, merging all clusters with an out of bag error bigger than 0.1 until no such nodes were found.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/362#issuecomment-440912410
https://github.com/scverse/scanpy/issues/362#issuecomment-440912410:214,Security,Validat,ValidateClusters,214,"That is a wonderful solution! I will give it a shot shortly. Thank you so much for your help!. Here's some example code of how Seurat handles cluster scoring and merging with random forests and OOBE:. ```; pbmc <- ValidateClusters(pbmc, pc.use = 1:30, top.genes = 30). pbmc <- BuildClusterTree(pbmc, ; do.reorder = T, ; reorder.numeric = T). node.scores <- AssessNodes(pbmc). node.scores[order(node.scores$oobe,decreasing = T),] -> node.scores. nodes.merge <- node.scores[which(node.scores[,2] > 0.1),]; nodes.to.merge <- sort(nodes.merge$node) ; pbmc.merged <- pbmc. for (n in nodes.to.merge); {; pbmc.merged <- MergeNode(pbmc.merged, n); }. ```. Here's an explanation, as this code was derived from this recent (and awesome) publication:; ; From page 6 of the Supplementary Methods of Plass et al 2018: http://science.sciencemag.org/content/early/2018/04/18/science.aaq1723. To prevent obtaining spurious clusters result of overclustering, the robustness of the clusters was calculated using the function AssessNodes from Seurat. For each cluster, the average expression of all variable genes (4910) is computed and a phylogenetic tree based on the distance matrix in gene expression space is computed. Next, it computes an Out of Bag Error for a random forest classifier trained on each internal node split of the tree. We recursively build a tree and assessed all its nodes, merging all clusters with an out of bag error bigger than 0.1 until no such nodes were found.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/362#issuecomment-440912410
https://github.com/scverse/scanpy/issues/363#issuecomment-439745461:87,Availability,error,error,87,"Can you call `del adata.uns['cell_ontology_class_colors']`? This should throw a better error message... I can do that soon, I wonder how you managed to produce the error... cannot be anything related to a recent update... Hm.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363#issuecomment-439745461
https://github.com/scverse/scanpy/issues/363#issuecomment-439745461:164,Availability,error,error,164,"Can you call `del adata.uns['cell_ontology_class_colors']`? This should throw a better error message... I can do that soon, I wonder how you managed to produce the error... cannot be anything related to a recent update... Hm.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363#issuecomment-439745461
https://github.com/scverse/scanpy/issues/363#issuecomment-439745461:212,Deployability,update,update,212,"Can you call `del adata.uns['cell_ontology_class_colors']`? This should throw a better error message... I can do that soon, I wonder how you managed to produce the error... cannot be anything related to a recent update... Hm.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363#issuecomment-439745461
https://github.com/scverse/scanpy/issues/363#issuecomment-439745461:93,Integrability,message,message,93,"Can you call `del adata.uns['cell_ontology_class_colors']`? This should throw a better error message... I can do that soon, I wonder how you managed to produce the error... cannot be anything related to a recent update... Hm.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363#issuecomment-439745461
https://github.com/scverse/scanpy/issues/363#issuecomment-440038619:1907,Modifiability,layers,layers,1907,"'; obsm: 'X_pca', 'X_umap', 'X_tsne'; varm: 'PCs'. tiss_facs[tiss_facs.obs['cell_ontology_class']=='keratinocyte stem cell']; ---------------------------------------------------------------------------; IndexError Traceback (most recent call last); <ipython-input-82-428532769794> in <module>(); ----> 1 tiss_facs[tiss_facs.obs['cell_ontology_class']=='keratinocyte stem cell']. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index); 1299 def __getitem__(self, index):; 1300 """"""Returns a sliced view of the object.""""""; -> 1301 return self._getitem_view(index); 1302 ; 1303 def _getitem_view(self, index):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index); 1303 def _getitem_view(self, index):; 1304 oidx, vidx = self._normalize_indices(index); -> 1305 return AnnData(self, oidx=oidx, vidx=vidx, asview=True); 1306 ; 1307 def _remove_unused_categories(self, df_full, df_sub, uns):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx); 662 if not isinstance(X, AnnData):; 663 raise ValueError('`X` has to be an AnnData object.'); --> 664 self._init_as_view(X, oidx, vidx); 665 else:; 666 self._init_as_actual(. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _init_as_view(self, adata_ref, oidx, vidx); 713 raise KeyError('Unknown Index type'); 714 # fix categories; --> 715 self._remove_unused_categories(adata_ref.obs, obs_sub, uns_new); 716 self._remove_unused_categories(adata_ref.var, var_sub, uns_new); 717 # set attributes. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _remove_unused_categories(self, df_full, df_sub, uns); 1318 uns[k + '_colors'] = np.array(uns[k + '_colors'])[; 1319 np.where(np.in1d(; -> 1320 all_categories, df_sub[k].cat.categories))[0]]; 1321 ; 1322 def rename_categories(self, key, categories):. IndexError: index 6 is out of bounds for axis 1 with size 6; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363#issuecomment-440038619
https://github.com/scverse/scanpy/issues/363#issuecomment-442366170:2240,Deployability,update,updated,2240,"(self, index):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index); 1303 def _getitem_view(self, index):; 1304 oidx, vidx = self._normalize_indices(index); -> 1305 return AnnData(self, oidx=oidx, vidx=vidx, asview=True); 1306 ; 1307 def _remove_unused_categories(self, df_full, df_sub, uns):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx); 662 if not isinstance(X, AnnData):; 663 raise ValueError('`X` has to be an AnnData object.'); --> 664 self._init_as_view(X, oidx, vidx); 665 else:; 666 self._init_as_actual(. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _init_as_view(self, adata_ref, oidx, vidx); 691 self._varm = ArrayView(adata_ref.varm[vidx_normalized], view_args=(self, 'varm')); 692 # hackish solution here, no copy should be necessary; --> 693 uns_new = deepcopy(self._adata_ref._uns); 694 # need to do the slicing before setting the updated self._n_obs, self._n_vars; 695 self._n_obs = self._adata_ref.n_obs # use the original n_obs here. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil); 178 y = x; 179 else:; --> 180 y = _reconstruct(x, memo, *rv); 181 ; 182 # If is its own copy, don't memoize. ~/anaconda3/lib/python3.6/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy); 278 if state is not None:; 279 if deep:; --> 280 state = deepcopy(state, memo); 281 if hasattr(y, '__setstate__'):; 282 y.__setstate__(state). ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil); 148 copier = _deepcopy_dispatch.get(cls); 149 if copier:; --> 150 y = copier(x, memo); 151 else:; 152 try:. ~/anaconda3/lib/python3.6/copy.py in _deepcopy_dict(x, memo, deepcopy); 238 memo[id(x)] = y; 239 for key, value in x.items():; --> 240 y[deepcopy(key, memo)] = deepcopy(value, memo); 241 return y; 242 d[dict] = _deepcopy_dict. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363#issuecomment-442366170
https://github.com/scverse/scanpy/issues/363#issuecomment-442366170:1651,Modifiability,layers,layers,1651," plot_boxplot_cell_fraction(adata, gene, label, title, ax, show); 1 def plot_boxplot_cell_fraction(adata, gene, label, title, ax, show=True):; ----> 2 gene_vals = np.asarray(adata[:, gene].X).flatten(); 3 ; 4 labels = ['3m','24m']; 5 # labels = list(set(adata.obs[label])). ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index); 1299 def __getitem__(self, index):; 1300 """"""Returns a sliced view of the object.""""""; -> 1301 return self._getitem_view(index); 1302 ; 1303 def _getitem_view(self, index):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index); 1303 def _getitem_view(self, index):; 1304 oidx, vidx = self._normalize_indices(index); -> 1305 return AnnData(self, oidx=oidx, vidx=vidx, asview=True); 1306 ; 1307 def _remove_unused_categories(self, df_full, df_sub, uns):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx); 662 if not isinstance(X, AnnData):; 663 raise ValueError('`X` has to be an AnnData object.'); --> 664 self._init_as_view(X, oidx, vidx); 665 else:; 666 self._init_as_actual(. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _init_as_view(self, adata_ref, oidx, vidx); 691 self._varm = ArrayView(adata_ref.varm[vidx_normalized], view_args=(self, 'varm')); 692 # hackish solution here, no copy should be necessary; --> 693 uns_new = deepcopy(self._adata_ref._uns); 694 # need to do the slicing before setting the updated self._n_obs, self._n_vars; 695 self._n_obs = self._adata_ref.n_obs # use the original n_obs here. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil); 178 y = x; 179 else:; --> 180 y = _reconstruct(x, memo, *rv); 181 ; 182 # If is its own copy, don't memoize. ~/anaconda3/lib/python3.6/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy); 278 if state is not None:; 279 if deep:; --> 280 state = deepcopy(state, memo);",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363#issuecomment-442366170
https://github.com/scverse/scanpy/issues/363#issuecomment-458386979:71,Modifiability,variab,variables,71,"hi @falexwolf, it still isn't working. The problem is with categorical variables, I'm currently doing this before subsetting:. ```; cat_columns = adata.obs.select_dtypes(['category']).columns; adata.obs[cat_columns] = adata.obs[cat_columns].astype(str); del cat_columns; ```; but it's really annoying, specially when using scvelo. Can you look into it? Also something problematic is that the `adata.uns['variable_color']` doesn't delete after you delete `adata.obs['variable']` so when you run into the subsetting problem my fix doesn't work if this is the situation and I've to manually delete the columns one by one... perhaps make this part of `sanitize_anndata`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363#issuecomment-458386979
https://github.com/scverse/scanpy/issues/363#issuecomment-458386979:466,Modifiability,variab,variable,466,"hi @falexwolf, it still isn't working. The problem is with categorical variables, I'm currently doing this before subsetting:. ```; cat_columns = adata.obs.select_dtypes(['category']).columns; adata.obs[cat_columns] = adata.obs[cat_columns].astype(str); del cat_columns; ```; but it's really annoying, specially when using scvelo. Can you look into it? Also something problematic is that the `adata.uns['variable_color']` doesn't delete after you delete `adata.obs['variable']` so when you run into the subsetting problem my fix doesn't work if this is the situation and I've to manually delete the columns one by one... perhaps make this part of `sanitize_anndata`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363#issuecomment-458386979
https://github.com/scverse/scanpy/issues/363#issuecomment-458658134:72,Availability,error,error,72,"@flying-sheep yes, it's exactly the same problem, with the exactly same error message that only happens when I (or the function) wanna subset an existing adata object",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363#issuecomment-458658134
https://github.com/scverse/scanpy/issues/363#issuecomment-458658134:78,Integrability,message,message,78,"@flying-sheep yes, it's exactly the same problem, with the exactly same error message that only happens when I (or the function) wanna subset an existing adata object",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363#issuecomment-458658134
https://github.com/scverse/scanpy/issues/363#issuecomment-513808772:270,Availability,error,errors,270,"Hi @aopisco ! @falexwolf I ran into the same problem but got everything to work by deleting all the unnecessary items in adata.uns. ```py; keep = ['neighbors', ]; keys = list(adata.uns.keys()); for key in keys:; if key not in keep:; del adata.uns[key]; ```; I don't get errors anymore but I fear that this might cause other problems I'm currently unaware of.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363#issuecomment-513808772
https://github.com/scverse/scanpy/pull/364#issuecomment-453448469:157,Testability,test,tests,157,"I’d advise to use the R package. The score is still off and I didn’t figure out why. Sorry. /edit: it works now. I’d like to have a good toy example for the tests that actually has a batch effect, then I’d merge this",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/364#issuecomment-453448469
https://github.com/scverse/scanpy/pull/364#issuecomment-1372378914:96,Testability,test,testing,96,"Yes, we did an attempt of implementing kBET in Python, but we did not follow up with sufficient testing whether the implementation reproduces the R package. The neighborhood search is slow indeed. For better comparability one should fix one neighborhood per dataset. That's also the reasons why we dropped it in the scIB package.; There was no pvalue adjustment in the original test, indeed. I created a statistic of 100 kBET results instead to compute a mean rejection rate and a range. There is a pvalue computation for the rejection rate, but it's almost always significant, and you rarely achieve rejection rates were the significance level is relevant. Moreover, it is a test statistic over a test statistic, so I am not quite sure how pvalues should be adjusted. . > I didn't realize there was a PR for this, there are some things here that we don't have, like adjusting the pvalues and the neighbor search. The former is fast to add (though I didn't see it in the R implementation?) and the latter could be really slow to have in practice",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/364#issuecomment-1372378914
https://github.com/scverse/scanpy/pull/364#issuecomment-1372378914:378,Testability,test,test,378,"Yes, we did an attempt of implementing kBET in Python, but we did not follow up with sufficient testing whether the implementation reproduces the R package. The neighborhood search is slow indeed. For better comparability one should fix one neighborhood per dataset. That's also the reasons why we dropped it in the scIB package.; There was no pvalue adjustment in the original test, indeed. I created a statistic of 100 kBET results instead to compute a mean rejection rate and a range. There is a pvalue computation for the rejection rate, but it's almost always significant, and you rarely achieve rejection rates were the significance level is relevant. Moreover, it is a test statistic over a test statistic, so I am not quite sure how pvalues should be adjusted. . > I didn't realize there was a PR for this, there are some things here that we don't have, like adjusting the pvalues and the neighbor search. The former is fast to add (though I didn't see it in the R implementation?) and the latter could be really slow to have in practice",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/364#issuecomment-1372378914
https://github.com/scverse/scanpy/pull/364#issuecomment-1372378914:676,Testability,test,test,676,"Yes, we did an attempt of implementing kBET in Python, but we did not follow up with sufficient testing whether the implementation reproduces the R package. The neighborhood search is slow indeed. For better comparability one should fix one neighborhood per dataset. That's also the reasons why we dropped it in the scIB package.; There was no pvalue adjustment in the original test, indeed. I created a statistic of 100 kBET results instead to compute a mean rejection rate and a range. There is a pvalue computation for the rejection rate, but it's almost always significant, and you rarely achieve rejection rates were the significance level is relevant. Moreover, it is a test statistic over a test statistic, so I am not quite sure how pvalues should be adjusted. . > I didn't realize there was a PR for this, there are some things here that we don't have, like adjusting the pvalues and the neighbor search. The former is fast to add (though I didn't see it in the R implementation?) and the latter could be really slow to have in practice",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/364#issuecomment-1372378914
https://github.com/scverse/scanpy/pull/364#issuecomment-1372378914:698,Testability,test,test,698,"Yes, we did an attempt of implementing kBET in Python, but we did not follow up with sufficient testing whether the implementation reproduces the R package. The neighborhood search is slow indeed. For better comparability one should fix one neighborhood per dataset. That's also the reasons why we dropped it in the scIB package.; There was no pvalue adjustment in the original test, indeed. I created a statistic of 100 kBET results instead to compute a mean rejection rate and a range. There is a pvalue computation for the rejection rate, but it's almost always significant, and you rarely achieve rejection rates were the significance level is relevant. Moreover, it is a test statistic over a test statistic, so I am not quite sure how pvalues should be adjusted. . > I didn't realize there was a PR for this, there are some things here that we don't have, like adjusting the pvalues and the neighbor search. The former is fast to add (though I didn't see it in the R implementation?) and the latter could be really slow to have in practice",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/364#issuecomment-1372378914
https://github.com/scverse/scanpy/pull/364#issuecomment-1372444139:425,Integrability,depend,dependence,425,"Yes, I totally agree that creating a fast implementation is probably not straightforward. The major bottleneck IMHO is computing this many neighbors to maximize the rejection rate, especially with 900k cells. In the original paper, we tried to find a range of neighborhood sizes K that return a maximal rejection rate, which is roughly between K = 50 to 0.5 * N where N denotes the number of cells, but there might be also a dependence on the number of batches, which we did not fully explore.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/364#issuecomment-1372444139
https://github.com/scverse/scanpy/pull/364#issuecomment-1372444139:100,Performance,bottleneck,bottleneck,100,"Yes, I totally agree that creating a fast implementation is probably not straightforward. The major bottleneck IMHO is computing this many neighbors to maximize the rejection rate, especially with 900k cells. In the original paper, we tried to find a range of neighborhood sizes K that return a maximal rejection rate, which is roughly between K = 50 to 0.5 * N where N denotes the number of cells, but there might be also a dependence on the number of batches, which we did not fully explore.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/364#issuecomment-1372444139
https://github.com/scverse/scanpy/issues/365#issuecomment-440391801:124,Availability,error,error,124,This is all looks fine and should work perfectly. I'd need an example with some data and the lines of code that produce the error.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/365#issuecomment-440391801
https://github.com/scverse/scanpy/issues/365#issuecomment-440420960:104,Availability,error,error,104,"I can do that. I have a pickled object that I can share with you (how?).; Here is how you reproduce the error:; ```; import scanpy.api as sc; import pickle. # Load the object; with open(""example.pkl"",""rb"") as handle:; adata = pickle.load(handle). # Run Scanpy; sc.tl.rank_genes_groups(adata,groupby=""celltype""); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/365#issuecomment-440420960
https://github.com/scverse/scanpy/issues/365#issuecomment-440420960:159,Performance,Load,Load,159,"I can do that. I have a pickled object that I can share with you (how?).; Here is how you reproduce the error:; ```; import scanpy.api as sc; import pickle. # Load the object; with open(""example.pkl"",""rb"") as handle:; adata = pickle.load(handle). # Run Scanpy; sc.tl.rank_genes_groups(adata,groupby=""celltype""); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/365#issuecomment-440420960
https://github.com/scverse/scanpy/issues/365#issuecomment-440420960:233,Performance,load,load,233,"I can do that. I have a pickled object that I can share with you (how?).; Here is how you reproduce the error:; ```; import scanpy.api as sc; import pickle. # Load the object; with open(""example.pkl"",""rb"") as handle:; adata = pickle.load(handle). # Run Scanpy; sc.tl.rank_genes_groups(adata,groupby=""celltype""); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/365#issuecomment-440420960
https://github.com/scverse/scanpy/issues/365#issuecomment-440424406:27,Usability,simpl,simply,27,"PS: You can of course also simply upload here on GitHub in a comment, as you want. ; PPS: The canonical way of saving AnnData's is via `.write('myfile.h5ad')`. 🙂",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/365#issuecomment-440424406
https://github.com/scverse/scanpy/issues/365#issuecomment-440431843:8,Availability,error,error,8,But the error comes from your variable names being tuples. The following fixes it.; ```; adata.var_names = [i[0] for i in adata.var_names]; ```. Let me think where it would be best to output a warning. I'm not quite sure where else it would lead to collisions. Do you need tuples in the index?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/365#issuecomment-440431843
https://github.com/scverse/scanpy/issues/365#issuecomment-440431843:30,Modifiability,variab,variable,30,But the error comes from your variable names being tuples. The following fixes it.; ```; adata.var_names = [i[0] for i in adata.var_names]; ```. Let me think where it would be best to output a warning. I'm not quite sure where else it would lead to collisions. Do you need tuples in the index?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/365#issuecomment-440431843
https://github.com/scverse/scanpy/issues/365#issuecomment-472686448:82,Testability,test,test,82,do you have any references on t-test_overestim_var ? I cannot find papers on this test method. what's the difference from t-test ? @falexwolf,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/365#issuecomment-472686448
https://github.com/scverse/scanpy/issues/365#issuecomment-472686448:124,Testability,test,test,124,do you have any references on t-test_overestim_var ? I cannot find papers on this test method. what's the difference from t-test ? @falexwolf,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/365#issuecomment-472686448
https://github.com/scverse/scanpy/issues/365#issuecomment-474304007:327,Testability,test,test,327,"No, there aren't any references. It's most easy to understand from this: https://github.com/theislab/scanpy/blob/662f66a4c2bc9a254990792f570cc971a444c575/scanpy/tools/_rank_genes_groups.py#L191. We had quite some material before (@tcallies, where did it go?), but we're now moving away from it and will set a different default test in the future.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/365#issuecomment-474304007
https://github.com/scverse/scanpy/issues/365#issuecomment-994325045:84,Testability,test,test,84,"> do you have any references on t-test_overestim_var ? I cannot find papers on this test method. what's the difference from t-test ? @falexwolf. I am also interested in formal descriptions about t-test_overestim_var or related publications, thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/365#issuecomment-994325045
https://github.com/scverse/scanpy/issues/365#issuecomment-994325045:126,Testability,test,test,126,"> do you have any references on t-test_overestim_var ? I cannot find papers on this test method. what's the difference from t-test ? @falexwolf. I am also interested in formal descriptions about t-test_overestim_var or related publications, thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/365#issuecomment-994325045
https://github.com/scverse/scanpy/pull/369#issuecomment-440740958:122,Deployability,update,updated,122,"Together with the suggested changes, I am also updating my usual notebook containing examples of all the plots (~~not yet updated:~~ https://gist.github.com/fidelram/2289b7a8d6da055fb058ac9a79ed485c). However, don't you think that this could be part of a the scanpy tutorials section?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369#issuecomment-440740958
https://github.com/scverse/scanpy/pull/369#issuecomment-441069177:108,Deployability,integrat,integrated,108,"@fidelram, I really like your plotting gallery! Would be cool to have that as part of the tutorials or even integrated in the main documentation (enhance each plotting function with an example image?)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369#issuecomment-441069177
https://github.com/scverse/scanpy/pull/369#issuecomment-441069177:108,Integrability,integrat,integrated,108,"@fidelram, I really like your plotting gallery! Would be cool to have that as part of the tutorials or even integrated in the main documentation (enhance each plotting function with an example image?)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369#issuecomment-441069177
https://github.com/scverse/scanpy/pull/369#issuecomment-441069177:146,Modifiability,enhance,enhance,146,"@fidelram, I really like your plotting gallery! Would be cool to have that as part of the tutorials or even integrated in the main documentation (enhance each plotting function with an example image?)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369#issuecomment-441069177
https://github.com/scverse/scanpy/pull/369#issuecomment-441216129:75,Testability,test,tests,75,"OK, my changes in 426f028708cdd203b7d97d48eb558e695090da82 didn’t make the tests break!. Do we currently not use the plotting test results?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369#issuecomment-441216129
https://github.com/scverse/scanpy/pull/369#issuecomment-441216129:126,Testability,test,test,126,"OK, my changes in 426f028708cdd203b7d97d48eb558e695090da82 didn’t make the tests break!. Do we currently not use the plotting test results?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369#issuecomment-441216129
https://github.com/scverse/scanpy/pull/369#issuecomment-441474788:555,Usability,simpl,simply,555,"> However, don't you think that this could be part of a the scanpy tutorials section?. Of course, this should become a part of the scanpy tutorials section! That would be awesome! I already asked you for that some time ago. :wink: I'd also start adding calls producing images to the doc pages. Let me build the core infrastructure for having the tutorials run on readthedocs and adding notebooks containing only code to the scanpy main repo. If you want, to shortcut, you can make a PR to scanpy_usage and upload your notebook there. Or, equally well, we simply link to your notebook from the tutorials page.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369#issuecomment-441474788
https://github.com/scverse/scanpy/pull/369#issuecomment-441549419:0,Energy Efficiency,Green,Green,0,Green light on my side to merge.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369#issuecomment-441549419
https://github.com/scverse/scanpy/pull/369#issuecomment-441571449:69,Deployability,update,update,69,"See my last comment. After fixing the colormaps in this PR, I didn’t update the images, but the tests still pass. What’s up with that?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369#issuecomment-441571449
https://github.com/scverse/scanpy/pull/369#issuecomment-441571449:96,Testability,test,tests,96,"See my last comment. After fixing the colormaps in this PR, I didn’t update the images, but the tests still pass. What’s up with that?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369#issuecomment-441571449
https://github.com/scverse/scanpy/pull/369#issuecomment-441619642:103,Testability,test,tests,103,"@flying-sheep I think that your changes should produce images that are almost equal to the ones on the tests as your changes simply introduce a different way to get the colormap. Btw, what is the advantage of using `ListedColormap` and `BoundaryNorm` instead of `LinearSegmentedColormap` ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369#issuecomment-441619642
https://github.com/scverse/scanpy/pull/369#issuecomment-441619642:125,Usability,simpl,simply,125,"@flying-sheep I think that your changes should produce images that are almost equal to the ones on the tests as your changes simply introduce a different way to get the colormap. Btw, what is the advantage of using `ListedColormap` and `BoundaryNorm` instead of `LinearSegmentedColormap` ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369#issuecomment-441619642
https://github.com/scverse/scanpy/pull/369#issuecomment-441704873:623,Deployability,continuous,continuous,623,"The important part is the `BoundaryNorm`. We got a really weird selection of colors without it, since the default is to treat the colormap as a linear space. `max(vec)` gets the last color, `min(vec)` the first one, and everything else some color between. Using the `BoundaryNorm` I defined, numbers in `[0, len(colors)-1]` get the color at the respective index, and everything smaller or bigger would get the first or last color (not ideal, but better than what we had). I don’t think it really makes a difference, but ListedColormap is a colormap for discrete uses like ours, LinearSegmentedColormap is for interpolating continuous values onto the map. Before | After; --- | ---; ![before](https://user-images.githubusercontent.com/291575/48907731-3ba8f600-ee60-11e8-9b87-8e095f6ed764.png) | ![after](https://user-images.githubusercontent.com/291575/49027776-e25f0080-f198-11e8-825e-1e98659cbc3a.png). In the before pic, we map `[0,1,2,3]` onto `[0;19]`, which results in `[0, 5.75, 10.5, 14.25, 19]`, and `[to_hex(tab20.colors[math.ceil(i)]) for i in [0, 5.75, 10.5, 14.25, 19]]` gives us [`#1f77b4`, `#98df8a`, `#8c564b`, `#c7c7c7`, `#9edae5`]",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369#issuecomment-441704873
https://github.com/scverse/scanpy/pull/369#issuecomment-443067468:35,Deployability,Release,Release,35,Thank you again! I'm merging this. Release prior to this is 1.3.4.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369#issuecomment-443067468
https://github.com/scverse/scanpy/issues/370#issuecomment-440522061:2,Deployability,upgrade,upgraded,2,I upgraded to 1.3.3 and the bug persists. . PS: I accidentally closed the issue for some reason.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/370#issuecomment-440522061
https://github.com/scverse/scanpy/issues/370#issuecomment-440556607:152,Deployability,upgrade,upgraded,152,"I can reproduce the problem. Very strange. I will submit a PR to fix it. On Wed, Nov 21, 2018 at 5:03 AM Andreas <notifications@github.com> wrote:. > I upgraded to 1.3.3 and the bug persists.; >; > PS: I accidentally closed the issue for some reason.; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/370#issuecomment-440522061>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1b3a4RVAX6v4o3oY_e3a1sh1Rnq2ks5uxNCCgaJpZM4YrmLi>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/370#issuecomment-440556607
https://github.com/scverse/scanpy/pull/371#issuecomment-441479438:202,Usability,simpl,simple,202,Also one problem is that i don't understand where to put `materialize_as_ndarray` as it used [here](https://github.com/theislab/scanpy/blob/44c038ad7b6488407958ab020858923b25368d97/scanpy/preprocessing/simple.py#L598) and in filtering.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/371#issuecomment-441479438
https://github.com/scverse/scanpy/pull/371#issuecomment-456647889:34,Deployability,release,release,34,"@tomwhite OK, I added this to the release notes (https://github.com/theislab/scanpy/commit/cee23dc13cf2b77d8e23ee0f91eb55fac0e35ed8, sorry confounded with some style change); it would be nice to have a link to your performance benchmarks... Let me know when we should announce it on twitter. I'm also happy to retweet...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/371#issuecomment-456647889
https://github.com/scverse/scanpy/pull/371#issuecomment-456647889:215,Performance,perform,performance,215,"@tomwhite OK, I added this to the release notes (https://github.com/theislab/scanpy/commit/cee23dc13cf2b77d8e23ee0f91eb55fac0e35ed8, sorry confounded with some style change); it would be nice to have a link to your performance benchmarks... Let me know when we should announce it on twitter. I'm also happy to retweet...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/371#issuecomment-456647889
https://github.com/scverse/scanpy/pull/371#issuecomment-456647889:227,Testability,benchmark,benchmarks,227,"@tomwhite OK, I added this to the release notes (https://github.com/theislab/scanpy/commit/cee23dc13cf2b77d8e23ee0f91eb55fac0e35ed8, sorry confounded with some style change); it would be nice to have a link to your performance benchmarks... Let me know when we should announce it on twitter. I'm also happy to retweet...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/371#issuecomment-456647889
https://github.com/scverse/scanpy/issues/373#issuecomment-440619581:624,Deployability,install,install,624,"Hi! I think we have a different focus here, and not all of what you stated as fact is correct, so I’ll do my best to clear this up:. 1. There is an advantage for type hints in common Scanpy usage. IPython should use Jedi to create autocompletions since this summer, but they forgot to reenable it. I sent them an issue to do so, ipython/ipython#11503 and a fix in ipython/ipython#11506. Jedi supports type hints, so with `c.Completer.use_jedi = True` now or by default in a month, people will profit from them. Furthermore, people are using scanpy in applications and scripts, not just in notebooks. When you use an IDE (or install the jedi extension in EMACS) you should profit from it. 2. The Jupyter shift-tab help being hard to read in the presence of type hints is what I consider a bug. I reported it in ipython/ipython#11504 and fixed it in ipython/ipython#11505. 3. The numpy is on it (see [here](https://github.com/numpy/numpy-stubs)) and will probably integrate it once there needs to be no Python 2 compat. e.g. scikit-learn waits for numpy: scikit-learn/scikit-learn#11170. I see your concern about entry hurdles, but I don’t agree. It’s super easy. `Union` is “or”, `Optional` is “or `None`”. If there’s questions, they can be answered. (or people click on the links in the docs and read like one sentence of explanation). 4. If you want we can change how all that is rendered. `Union[a, b]` could be done as ``` :class:`a` or :class:`b` ``` But it’s really not hard…. Honestly I think the `Callable[…]` is much better than the textual description that was there before: Until it was there, people (including me when i was writing that annotation) had to dive into the code to figure out what function signature is *really* expected there. Now they have to be able to parse what that `Callable[[a,b], c]` there means. If they have never encountered it before, they can click on it, read one sentence of explanation and know that `a` and `b` are parameters and `c` the return type. Done in",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-440619581
https://github.com/scverse/scanpy/issues/373#issuecomment-440619581:962,Deployability,integrat,integrate,962,"Hi! I think we have a different focus here, and not all of what you stated as fact is correct, so I’ll do my best to clear this up:. 1. There is an advantage for type hints in common Scanpy usage. IPython should use Jedi to create autocompletions since this summer, but they forgot to reenable it. I sent them an issue to do so, ipython/ipython#11503 and a fix in ipython/ipython#11506. Jedi supports type hints, so with `c.Completer.use_jedi = True` now or by default in a month, people will profit from them. Furthermore, people are using scanpy in applications and scripts, not just in notebooks. When you use an IDE (or install the jedi extension in EMACS) you should profit from it. 2. The Jupyter shift-tab help being hard to read in the presence of type hints is what I consider a bug. I reported it in ipython/ipython#11504 and fixed it in ipython/ipython#11505. 3. The numpy is on it (see [here](https://github.com/numpy/numpy-stubs)) and will probably integrate it once there needs to be no Python 2 compat. e.g. scikit-learn waits for numpy: scikit-learn/scikit-learn#11170. I see your concern about entry hurdles, but I don’t agree. It’s super easy. `Union` is “or”, `Optional` is “or `None`”. If there’s questions, they can be answered. (or people click on the links in the docs and read like one sentence of explanation). 4. If you want we can change how all that is rendered. `Union[a, b]` could be done as ``` :class:`a` or :class:`b` ``` But it’s really not hard…. Honestly I think the `Callable[…]` is much better than the textual description that was there before: Until it was there, people (including me when i was writing that annotation) had to dive into the code to figure out what function signature is *really* expected there. Now they have to be able to parse what that `Callable[[a,b], c]` there means. If they have never encountered it before, they can click on it, read one sentence of explanation and know that `a` and `b` are parameters and `c` the return type. Done in",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-440619581
https://github.com/scverse/scanpy/issues/373#issuecomment-440619581:962,Integrability,integrat,integrate,962,"Hi! I think we have a different focus here, and not all of what you stated as fact is correct, so I’ll do my best to clear this up:. 1. There is an advantage for type hints in common Scanpy usage. IPython should use Jedi to create autocompletions since this summer, but they forgot to reenable it. I sent them an issue to do so, ipython/ipython#11503 and a fix in ipython/ipython#11506. Jedi supports type hints, so with `c.Completer.use_jedi = True` now or by default in a month, people will profit from them. Furthermore, people are using scanpy in applications and scripts, not just in notebooks. When you use an IDE (or install the jedi extension in EMACS) you should profit from it. 2. The Jupyter shift-tab help being hard to read in the presence of type hints is what I consider a bug. I reported it in ipython/ipython#11504 and fixed it in ipython/ipython#11505. 3. The numpy is on it (see [here](https://github.com/numpy/numpy-stubs)) and will probably integrate it once there needs to be no Python 2 compat. e.g. scikit-learn waits for numpy: scikit-learn/scikit-learn#11170. I see your concern about entry hurdles, but I don’t agree. It’s super easy. `Union` is “or”, `Optional` is “or `None`”. If there’s questions, they can be answered. (or people click on the links in the docs and read like one sentence of explanation). 4. If you want we can change how all that is rendered. `Union[a, b]` could be done as ``` :class:`a` or :class:`b` ``` But it’s really not hard…. Honestly I think the `Callable[…]` is much better than the textual description that was there before: Until it was there, people (including me when i was writing that annotation) had to dive into the code to figure out what function signature is *really* expected there. Now they have to be able to parse what that `Callable[[a,b], c]` there means. If they have never encountered it before, they can click on it, read one sentence of explanation and know that `a` and `b` are parameters and `c` the return type. Done in",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-440619581
https://github.com/scverse/scanpy/issues/373#issuecomment-440619581:936,Testability,stub,stubs,936,"Hi! I think we have a different focus here, and not all of what you stated as fact is correct, so I’ll do my best to clear this up:. 1. There is an advantage for type hints in common Scanpy usage. IPython should use Jedi to create autocompletions since this summer, but they forgot to reenable it. I sent them an issue to do so, ipython/ipython#11503 and a fix in ipython/ipython#11506. Jedi supports type hints, so with `c.Completer.use_jedi = True` now or by default in a month, people will profit from them. Furthermore, people are using scanpy in applications and scripts, not just in notebooks. When you use an IDE (or install the jedi extension in EMACS) you should profit from it. 2. The Jupyter shift-tab help being hard to read in the presence of type hints is what I consider a bug. I reported it in ipython/ipython#11504 and fixed it in ipython/ipython#11505. 3. The numpy is on it (see [here](https://github.com/numpy/numpy-stubs)) and will probably integrate it once there needs to be no Python 2 compat. e.g. scikit-learn waits for numpy: scikit-learn/scikit-learn#11170. I see your concern about entry hurdles, but I don’t agree. It’s super easy. `Union` is “or”, `Optional` is “or `None`”. If there’s questions, they can be answered. (or people click on the links in the docs and read like one sentence of explanation). 4. If you want we can change how all that is rendered. `Union[a, b]` could be done as ``` :class:`a` or :class:`b` ``` But it’s really not hard…. Honestly I think the `Callable[…]` is much better than the textual description that was there before: Until it was there, people (including me when i was writing that annotation) had to dive into the code to figure out what function signature is *really* expected there. Now they have to be able to parse what that `Callable[[a,b], c]` there means. If they have never encountered it before, they can click on it, read one sentence of explanation and know that `a` and `b` are parameters and `c` the return type. Done in",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-440619581
https://github.com/scverse/scanpy/issues/373#issuecomment-440619581:117,Usability,clear,clear,117,"Hi! I think we have a different focus here, and not all of what you stated as fact is correct, so I’ll do my best to clear this up:. 1. There is an advantage for type hints in common Scanpy usage. IPython should use Jedi to create autocompletions since this summer, but they forgot to reenable it. I sent them an issue to do so, ipython/ipython#11503 and a fix in ipython/ipython#11506. Jedi supports type hints, so with `c.Completer.use_jedi = True` now or by default in a month, people will profit from them. Furthermore, people are using scanpy in applications and scripts, not just in notebooks. When you use an IDE (or install the jedi extension in EMACS) you should profit from it. 2. The Jupyter shift-tab help being hard to read in the presence of type hints is what I consider a bug. I reported it in ipython/ipython#11504 and fixed it in ipython/ipython#11505. 3. The numpy is on it (see [here](https://github.com/numpy/numpy-stubs)) and will probably integrate it once there needs to be no Python 2 compat. e.g. scikit-learn waits for numpy: scikit-learn/scikit-learn#11170. I see your concern about entry hurdles, but I don’t agree. It’s super easy. `Union` is “or”, `Optional` is “or `None`”. If there’s questions, they can be answered. (or people click on the links in the docs and read like one sentence of explanation). 4. If you want we can change how all that is rendered. `Union[a, b]` could be done as ``` :class:`a` or :class:`b` ``` But it’s really not hard…. Honestly I think the `Callable[…]` is much better than the textual description that was there before: Until it was there, people (including me when i was writing that annotation) had to dive into the code to figure out what function signature is *really* expected there. Now they have to be able to parse what that `Callable[[a,b], c]` there means. If they have never encountered it before, they can click on it, read one sentence of explanation and know that `a` and `b` are parameters and `c` the return type. Done in",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-440619581
https://github.com/scverse/scanpy/issues/373#issuecomment-440619581:1030,Usability,learn,learn,1030,"ll of what you stated as fact is correct, so I’ll do my best to clear this up:. 1. There is an advantage for type hints in common Scanpy usage. IPython should use Jedi to create autocompletions since this summer, but they forgot to reenable it. I sent them an issue to do so, ipython/ipython#11503 and a fix in ipython/ipython#11506. Jedi supports type hints, so with `c.Completer.use_jedi = True` now or by default in a month, people will profit from them. Furthermore, people are using scanpy in applications and scripts, not just in notebooks. When you use an IDE (or install the jedi extension in EMACS) you should profit from it. 2. The Jupyter shift-tab help being hard to read in the presence of type hints is what I consider a bug. I reported it in ipython/ipython#11504 and fixed it in ipython/ipython#11505. 3. The numpy is on it (see [here](https://github.com/numpy/numpy-stubs)) and will probably integrate it once there needs to be no Python 2 compat. e.g. scikit-learn waits for numpy: scikit-learn/scikit-learn#11170. I see your concern about entry hurdles, but I don’t agree. It’s super easy. `Union` is “or”, `Optional` is “or `None`”. If there’s questions, they can be answered. (or people click on the links in the docs and read like one sentence of explanation). 4. If you want we can change how all that is rendered. `Union[a, b]` could be done as ``` :class:`a` or :class:`b` ``` But it’s really not hard…. Honestly I think the `Callable[…]` is much better than the textual description that was there before: Until it was there, people (including me when i was writing that annotation) had to dive into the code to figure out what function signature is *really* expected there. Now they have to be able to parse what that `Callable[[a,b], c]` there means. If they have never encountered it before, they can click on it, read one sentence of explanation and know that `a` and `b` are parameters and `c` the return type. Done in 5 seconds. It’s a big improvement to no longer have ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-440619581
https://github.com/scverse/scanpy/issues/373#issuecomment-440619581:1060,Usability,learn,learn,1060,"ll of what you stated as fact is correct, so I’ll do my best to clear this up:. 1. There is an advantage for type hints in common Scanpy usage. IPython should use Jedi to create autocompletions since this summer, but they forgot to reenable it. I sent them an issue to do so, ipython/ipython#11503 and a fix in ipython/ipython#11506. Jedi supports type hints, so with `c.Completer.use_jedi = True` now or by default in a month, people will profit from them. Furthermore, people are using scanpy in applications and scripts, not just in notebooks. When you use an IDE (or install the jedi extension in EMACS) you should profit from it. 2. The Jupyter shift-tab help being hard to read in the presence of type hints is what I consider a bug. I reported it in ipython/ipython#11504 and fixed it in ipython/ipython#11505. 3. The numpy is on it (see [here](https://github.com/numpy/numpy-stubs)) and will probably integrate it once there needs to be no Python 2 compat. e.g. scikit-learn waits for numpy: scikit-learn/scikit-learn#11170. I see your concern about entry hurdles, but I don’t agree. It’s super easy. `Union` is “or”, `Optional` is “or `None`”. If there’s questions, they can be answered. (or people click on the links in the docs and read like one sentence of explanation). 4. If you want we can change how all that is rendered. `Union[a, b]` could be done as ``` :class:`a` or :class:`b` ``` But it’s really not hard…. Honestly I think the `Callable[…]` is much better than the textual description that was there before: Until it was there, people (including me when i was writing that annotation) had to dive into the code to figure out what function signature is *really* expected there. Now they have to be able to parse what that `Callable[[a,b], c]` there means. If they have never encountered it before, they can click on it, read one sentence of explanation and know that `a` and `b` are parameters and `c` the return type. Done in 5 seconds. It’s a big improvement to no longer have ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-440619581
https://github.com/scverse/scanpy/issues/373#issuecomment-440619581:1073,Usability,learn,learn,1073,"ll of what you stated as fact is correct, so I’ll do my best to clear this up:. 1. There is an advantage for type hints in common Scanpy usage. IPython should use Jedi to create autocompletions since this summer, but they forgot to reenable it. I sent them an issue to do so, ipython/ipython#11503 and a fix in ipython/ipython#11506. Jedi supports type hints, so with `c.Completer.use_jedi = True` now or by default in a month, people will profit from them. Furthermore, people are using scanpy in applications and scripts, not just in notebooks. When you use an IDE (or install the jedi extension in EMACS) you should profit from it. 2. The Jupyter shift-tab help being hard to read in the presence of type hints is what I consider a bug. I reported it in ipython/ipython#11504 and fixed it in ipython/ipython#11505. 3. The numpy is on it (see [here](https://github.com/numpy/numpy-stubs)) and will probably integrate it once there needs to be no Python 2 compat. e.g. scikit-learn waits for numpy: scikit-learn/scikit-learn#11170. I see your concern about entry hurdles, but I don’t agree. It’s super easy. `Union` is “or”, `Optional` is “or `None`”. If there’s questions, they can be answered. (or people click on the links in the docs and read like one sentence of explanation). 4. If you want we can change how all that is rendered. `Union[a, b]` could be done as ``` :class:`a` or :class:`b` ``` But it’s really not hard…. Honestly I think the `Callable[…]` is much better than the textual description that was there before: Until it was there, people (including me when i was writing that annotation) had to dive into the code to figure out what function signature is *really* expected there. Now they have to be able to parse what that `Callable[[a,b], c]` there means. If they have never encountered it before, they can click on it, read one sentence of explanation and know that `a` and `b` are parameters and `c` the return type. Done in 5 seconds. It’s a big improvement to no longer have ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-440619581
https://github.com/scverse/scanpy/issues/373#issuecomment-441140790:113,Modifiability,polymorphi,polymorphic,113,"Hey, this has been something that's been confusing me a bit when annotating my arguments. Since python is pretty polymorphic (until its not), I find it hard to capture the traits an object should have using types I'm familiar with. Some examples:. * If you need to provide a list of genes, this could be a finite (ordered?) iterable whose elements are coercible to the same type as `obs_names`. ; * An integer. Could be a numpy integer, could be a python integer. What's are the correct typings for these? Do I do a Union of everything I can think of that matches this? Is there a way to say: ""should behave right if I call `np.array` on it"" (limiting possible arguments types to pd.Series, list, tuple, np.array, dask array, and probably some others)?. I guess I'd like to so some information on best practices and common idioms in the contribution guide. I haven't seen too many scientific python packages use type annotations, so I'm not sure how set conventions are. If anyone has seen some good writing on type annotations for the scientific python stack, I'd love to take a look.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441140790
https://github.com/scverse/scanpy/issues/373#issuecomment-441140790:850,Usability,guid,guide,850,"Hey, this has been something that's been confusing me a bit when annotating my arguments. Since python is pretty polymorphic (until its not), I find it hard to capture the traits an object should have using types I'm familiar with. Some examples:. * If you need to provide a list of genes, this could be a finite (ordered?) iterable whose elements are coercible to the same type as `obs_names`. ; * An integer. Could be a numpy integer, could be a python integer. What's are the correct typings for these? Do I do a Union of everything I can think of that matches this? Is there a way to say: ""should behave right if I call `np.array` on it"" (limiting possible arguments types to pd.Series, list, tuple, np.array, dask array, and probably some others)?. I guess I'd like to so some information on best practices and common idioms in the contribution guide. I haven't seen too many scientific python packages use type annotations, so I'm not sure how set conventions are. If anyone has seen some good writing on type annotations for the scientific python stack, I'd love to take a look.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441140790
https://github.com/scverse/scanpy/issues/373#issuecomment-441207438:74,Integrability,interface,interfaces,74,"Hi!. There’s a series of abstract base classes (other languages call them interfaces) that can be used if you know specifically what you want (e.g. `typing.Sequence[T]` if it only needs to be iterable and indexable, but not necessarily a `list`). About your examples, generally I always have to dig into the code to figure such things out. Annoying, but it means that people after me can just use the type annotations instead of wasting their time doing the same. - Currently we only accept `str` for obs_names, so depending on the actual usage in the function, `Iterable[str]` or `Sequence[str]` would be it I guess.; - Usually single integers being passed around should be python `int`s, because they can be any size. But if you need to accept `int32` and `int64`, you can do `Union[int, np.integer]`. > Is there a way to say: ""should behave right if I call np.array on it"". In truth, `array` eats just about anything (with weird results), and for us this is probably a good idea:. ```py; Number = Union[float, int, np.integer, np.floating]; Num1DArrayLike = Sequence[Number]; Num2DArrayLike = Sequence[Num1DArrayLike]; Num3DArrayLike = Sequence[Num2DArrayLike]; NumNDArrayLike = Union[Num1DArrayLike, Num2DArrayLike, Num3DArrayLike]; ```. But if we want to be exact about `array_like`s, we’d need this ABC:. ```py; class ArrayLike(ABC):; """"""An array,; any object exposing the array interface,; an object whose __array__ method returns an array,; or any (nested) sequence.; """"""; @classmethod; def __subclasshook__(cls, C):; if issubclass(C, np.ndarray):; return True; if any('__array_interface__' in B.__dict__ for B in C.__mro__):; return True; if any('__array__' in B.__dict__ for B in C.__mro__):; return True; return Sequence.__subclasshook__(cls, C); ```. ----. Two thoughts here:. 1. It’s fine if you don’t know exactly. Just use your best guess. The worst case is that someone wastes a second of runtime converting their argument to the type you thought was needed, while the original type wo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441207438
https://github.com/scverse/scanpy/issues/373#issuecomment-441207438:515,Integrability,depend,depending,515,"Hi!. There’s a series of abstract base classes (other languages call them interfaces) that can be used if you know specifically what you want (e.g. `typing.Sequence[T]` if it only needs to be iterable and indexable, but not necessarily a `list`). About your examples, generally I always have to dig into the code to figure such things out. Annoying, but it means that people after me can just use the type annotations instead of wasting their time doing the same. - Currently we only accept `str` for obs_names, so depending on the actual usage in the function, `Iterable[str]` or `Sequence[str]` would be it I guess.; - Usually single integers being passed around should be python `int`s, because they can be any size. But if you need to accept `int32` and `int64`, you can do `Union[int, np.integer]`. > Is there a way to say: ""should behave right if I call np.array on it"". In truth, `array` eats just about anything (with weird results), and for us this is probably a good idea:. ```py; Number = Union[float, int, np.integer, np.floating]; Num1DArrayLike = Sequence[Number]; Num2DArrayLike = Sequence[Num1DArrayLike]; Num3DArrayLike = Sequence[Num2DArrayLike]; NumNDArrayLike = Union[Num1DArrayLike, Num2DArrayLike, Num3DArrayLike]; ```. But if we want to be exact about `array_like`s, we’d need this ABC:. ```py; class ArrayLike(ABC):; """"""An array,; any object exposing the array interface,; an object whose __array__ method returns an array,; or any (nested) sequence.; """"""; @classmethod; def __subclasshook__(cls, C):; if issubclass(C, np.ndarray):; return True; if any('__array_interface__' in B.__dict__ for B in C.__mro__):; return True; if any('__array__' in B.__dict__ for B in C.__mro__):; return True; return Sequence.__subclasshook__(cls, C); ```. ----. Two thoughts here:. 1. It’s fine if you don’t know exactly. Just use your best guess. The worst case is that someone wastes a second of runtime converting their argument to the type you thought was needed, while the original type wo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441207438
https://github.com/scverse/scanpy/issues/373#issuecomment-441207438:1385,Integrability,interface,interface,1385,"ecifically what you want (e.g. `typing.Sequence[T]` if it only needs to be iterable and indexable, but not necessarily a `list`). About your examples, generally I always have to dig into the code to figure such things out. Annoying, but it means that people after me can just use the type annotations instead of wasting their time doing the same. - Currently we only accept `str` for obs_names, so depending on the actual usage in the function, `Iterable[str]` or `Sequence[str]` would be it I guess.; - Usually single integers being passed around should be python `int`s, because they can be any size. But if you need to accept `int32` and `int64`, you can do `Union[int, np.integer]`. > Is there a way to say: ""should behave right if I call np.array on it"". In truth, `array` eats just about anything (with weird results), and for us this is probably a good idea:. ```py; Number = Union[float, int, np.integer, np.floating]; Num1DArrayLike = Sequence[Number]; Num2DArrayLike = Sequence[Num1DArrayLike]; Num3DArrayLike = Sequence[Num2DArrayLike]; NumNDArrayLike = Union[Num1DArrayLike, Num2DArrayLike, Num3DArrayLike]; ```. But if we want to be exact about `array_like`s, we’d need this ABC:. ```py; class ArrayLike(ABC):; """"""An array,; any object exposing the array interface,; an object whose __array__ method returns an array,; or any (nested) sequence.; """"""; @classmethod; def __subclasshook__(cls, C):; if issubclass(C, np.ndarray):; return True; if any('__array_interface__' in B.__dict__ for B in C.__mro__):; return True; if any('__array__' in B.__dict__ for B in C.__mro__):; return True; return Sequence.__subclasshook__(cls, C); ```. ----. Two thoughts here:. 1. It’s fine if you don’t know exactly. Just use your best guess. The worst case is that someone wastes a second of runtime converting their argument to the type you thought was needed, while the original type would have been OK.; 2. It’s good if someone thinks about all that because that means things don’t break unexpectedly!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441207438
https://github.com/scverse/scanpy/issues/373#issuecomment-441219613:210,Availability,error,errors,210,@flying-sheep Regarding your first thought... it may cause issues when interfacing with other functions that do not have type annotations on the arguments. And users may then find it difficult to interpret the errors.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441219613
https://github.com/scverse/scanpy/issues/373#issuecomment-441243396:99,Testability,test,tests,99,…no? why would it? type annotations are only used for people and IDEs (unless you use mypy in your tests to check if everything is sound),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441243396
https://github.com/scverse/scanpy/issues/373#issuecomment-441252542:90,Availability,error,errors,90,"How? As said, they’re just for people and IDEs. Scanpy doesn’t use them. It doesn’t throw errors in case something doesn’t fit. We could use https://pypi.org/project/typecheck-decorator/ to throw errors when something is passed that doesn’t fit the annotations. However, doing so has a performance hit and requires flawless annotations (because if the annotations were wrong, that *would* start suddenly throwing errors). I’m just adding type annotations to improve user friendliness by being more clear what functions accept, and because it makes writing documentation easier.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441252542
https://github.com/scverse/scanpy/issues/373#issuecomment-441252542:196,Availability,error,errors,196,"How? As said, they’re just for people and IDEs. Scanpy doesn’t use them. It doesn’t throw errors in case something doesn’t fit. We could use https://pypi.org/project/typecheck-decorator/ to throw errors when something is passed that doesn’t fit the annotations. However, doing so has a performance hit and requires flawless annotations (because if the annotations were wrong, that *would* start suddenly throwing errors). I’m just adding type annotations to improve user friendliness by being more clear what functions accept, and because it makes writing documentation easier.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441252542
https://github.com/scverse/scanpy/issues/373#issuecomment-441252542:413,Availability,error,errors,413,"How? As said, they’re just for people and IDEs. Scanpy doesn’t use them. It doesn’t throw errors in case something doesn’t fit. We could use https://pypi.org/project/typecheck-decorator/ to throw errors when something is passed that doesn’t fit the annotations. However, doing so has a performance hit and requires flawless annotations (because if the annotations were wrong, that *would* start suddenly throwing errors). I’m just adding type annotations to improve user friendliness by being more clear what functions accept, and because it makes writing documentation easier.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441252542
https://github.com/scverse/scanpy/issues/373#issuecomment-441252542:286,Performance,perform,performance,286,"How? As said, they’re just for people and IDEs. Scanpy doesn’t use them. It doesn’t throw errors in case something doesn’t fit. We could use https://pypi.org/project/typecheck-decorator/ to throw errors when something is passed that doesn’t fit the annotations. However, doing so has a performance hit and requires flawless annotations (because if the annotations were wrong, that *would* start suddenly throwing errors). I’m just adding type annotations to improve user friendliness by being more clear what functions accept, and because it makes writing documentation easier.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441252542
https://github.com/scverse/scanpy/issues/373#issuecomment-441252542:498,Usability,clear,clear,498,"How? As said, they’re just for people and IDEs. Scanpy doesn’t use them. It doesn’t throw errors in case something doesn’t fit. We could use https://pypi.org/project/typecheck-decorator/ to throw errors when something is passed that doesn’t fit the annotations. However, doing so has a performance hit and requires flawless annotations (because if the annotations were wrong, that *would* start suddenly throwing errors). I’m just adding type annotations to improve user friendliness by being more clear what functions accept, and because it makes writing documentation easier.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441252542
https://github.com/scverse/scanpy/issues/373#issuecomment-441254115:119,Availability,error,errors,119,"In that case, I don't fully understand this typing and will just continue reading quietly ;). I assumed it would throw errors as for example in C++.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441254115
https://github.com/scverse/scanpy/issues/373#issuecomment-441256142:638,Availability,error,error,638,"There’s a few uses:. 1. Humans. Once you understand the syntax ([very easy](https://docs.python.org/3/library/typing.html), i just get `Generator` wrong all the time) it improves your understanding what a function really accepts and returns; 2. IDEs. They’ll get better when inferring the types of variables and will show you more actual problems in the code and less false positives; 3. Testing. Some projects use mypy to check if all code in your repo typechecks properly, which can be integrated into a test suite; 4. Runtime type checking. Has a performance hit (as said) but given proper type hints, it makes your code safer and the error messages better (“Function blah excepted a parameter foo of type Bar, but you passed a foo of type Baz”). i’m not planning to do 3 and 4 (yet, and probably never)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441256142
https://github.com/scverse/scanpy/issues/373#issuecomment-441256142:488,Deployability,integrat,integrated,488,"There’s a few uses:. 1. Humans. Once you understand the syntax ([very easy](https://docs.python.org/3/library/typing.html), i just get `Generator` wrong all the time) it improves your understanding what a function really accepts and returns; 2. IDEs. They’ll get better when inferring the types of variables and will show you more actual problems in the code and less false positives; 3. Testing. Some projects use mypy to check if all code in your repo typechecks properly, which can be integrated into a test suite; 4. Runtime type checking. Has a performance hit (as said) but given proper type hints, it makes your code safer and the error messages better (“Function blah excepted a parameter foo of type Bar, but you passed a foo of type Baz”). i’m not planning to do 3 and 4 (yet, and probably never)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441256142
https://github.com/scverse/scanpy/issues/373#issuecomment-441256142:488,Integrability,integrat,integrated,488,"There’s a few uses:. 1. Humans. Once you understand the syntax ([very easy](https://docs.python.org/3/library/typing.html), i just get `Generator` wrong all the time) it improves your understanding what a function really accepts and returns; 2. IDEs. They’ll get better when inferring the types of variables and will show you more actual problems in the code and less false positives; 3. Testing. Some projects use mypy to check if all code in your repo typechecks properly, which can be integrated into a test suite; 4. Runtime type checking. Has a performance hit (as said) but given proper type hints, it makes your code safer and the error messages better (“Function blah excepted a parameter foo of type Bar, but you passed a foo of type Baz”). i’m not planning to do 3 and 4 (yet, and probably never)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441256142
https://github.com/scverse/scanpy/issues/373#issuecomment-441256142:644,Integrability,message,messages,644,"There’s a few uses:. 1. Humans. Once you understand the syntax ([very easy](https://docs.python.org/3/library/typing.html), i just get `Generator` wrong all the time) it improves your understanding what a function really accepts and returns; 2. IDEs. They’ll get better when inferring the types of variables and will show you more actual problems in the code and less false positives; 3. Testing. Some projects use mypy to check if all code in your repo typechecks properly, which can be integrated into a test suite; 4. Runtime type checking. Has a performance hit (as said) but given proper type hints, it makes your code safer and the error messages better (“Function blah excepted a parameter foo of type Bar, but you passed a foo of type Baz”). i’m not planning to do 3 and 4 (yet, and probably never)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441256142
https://github.com/scverse/scanpy/issues/373#issuecomment-441256142:298,Modifiability,variab,variables,298,"There’s a few uses:. 1. Humans. Once you understand the syntax ([very easy](https://docs.python.org/3/library/typing.html), i just get `Generator` wrong all the time) it improves your understanding what a function really accepts and returns; 2. IDEs. They’ll get better when inferring the types of variables and will show you more actual problems in the code and less false positives; 3. Testing. Some projects use mypy to check if all code in your repo typechecks properly, which can be integrated into a test suite; 4. Runtime type checking. Has a performance hit (as said) but given proper type hints, it makes your code safer and the error messages better (“Function blah excepted a parameter foo of type Bar, but you passed a foo of type Baz”). i’m not planning to do 3 and 4 (yet, and probably never)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441256142
https://github.com/scverse/scanpy/issues/373#issuecomment-441256142:550,Performance,perform,performance,550,"There’s a few uses:. 1. Humans. Once you understand the syntax ([very easy](https://docs.python.org/3/library/typing.html), i just get `Generator` wrong all the time) it improves your understanding what a function really accepts and returns; 2. IDEs. They’ll get better when inferring the types of variables and will show you more actual problems in the code and less false positives; 3. Testing. Some projects use mypy to check if all code in your repo typechecks properly, which can be integrated into a test suite; 4. Runtime type checking. Has a performance hit (as said) but given proper type hints, it makes your code safer and the error messages better (“Function blah excepted a parameter foo of type Bar, but you passed a foo of type Baz”). i’m not planning to do 3 and 4 (yet, and probably never)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441256142
https://github.com/scverse/scanpy/issues/373#issuecomment-441256142:624,Safety,safe,safer,624,"There’s a few uses:. 1. Humans. Once you understand the syntax ([very easy](https://docs.python.org/3/library/typing.html), i just get `Generator` wrong all the time) it improves your understanding what a function really accepts and returns; 2. IDEs. They’ll get better when inferring the types of variables and will show you more actual problems in the code and less false positives; 3. Testing. Some projects use mypy to check if all code in your repo typechecks properly, which can be integrated into a test suite; 4. Runtime type checking. Has a performance hit (as said) but given proper type hints, it makes your code safer and the error messages better (“Function blah excepted a parameter foo of type Bar, but you passed a foo of type Baz”). i’m not planning to do 3 and 4 (yet, and probably never)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441256142
https://github.com/scverse/scanpy/issues/373#issuecomment-441256142:388,Testability,Test,Testing,388,"There’s a few uses:. 1. Humans. Once you understand the syntax ([very easy](https://docs.python.org/3/library/typing.html), i just get `Generator` wrong all the time) it improves your understanding what a function really accepts and returns; 2. IDEs. They’ll get better when inferring the types of variables and will show you more actual problems in the code and less false positives; 3. Testing. Some projects use mypy to check if all code in your repo typechecks properly, which can be integrated into a test suite; 4. Runtime type checking. Has a performance hit (as said) but given proper type hints, it makes your code safer and the error messages better (“Function blah excepted a parameter foo of type Bar, but you passed a foo of type Baz”). i’m not planning to do 3 and 4 (yet, and probably never)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441256142
https://github.com/scverse/scanpy/issues/373#issuecomment-441256142:506,Testability,test,test,506,"There’s a few uses:. 1. Humans. Once you understand the syntax ([very easy](https://docs.python.org/3/library/typing.html), i just get `Generator` wrong all the time) it improves your understanding what a function really accepts and returns; 2. IDEs. They’ll get better when inferring the types of variables and will show you more actual problems in the code and less false positives; 3. Testing. Some projects use mypy to check if all code in your repo typechecks properly, which can be integrated into a test suite; 4. Runtime type checking. Has a performance hit (as said) but given proper type hints, it makes your code safer and the error messages better (“Function blah excepted a parameter foo of type Bar, but you passed a foo of type Baz”). i’m not planning to do 3 and 4 (yet, and probably never)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441256142
https://github.com/scverse/scanpy/issues/373#issuecomment-441414363:315,Availability,error,errors,315,"@flying-sheep Thanks for the thorough response! This is a topic I have a lot of thoughts on, though I'm not so sure how coherently I can communicate all of them. On your first thought:. The worst case scenario I see here me typing something so poorly a newbie trying to follow the documentation gets horrible numba errors they can't figure out. This could happen if I hadn't thought about `Set` being a subtype of `Collection`. It can be difficult to know the `abc`s in `typing` are supposed to mean without spending a while using them. For example, this is not what I was expecting:. ```python; >>> issubclass(np.ndarray, typing.Collection); True; >>> issubclass(np.ndarray, typing.Sequence); False; ```. On defining `abc`s for this package, I think it would have to be done in a way where it was easy to discover exactly what is meant by the annotated types. Personally, I read most of my documentation through the ipython repl, where it can be difficult to figure out where a type referenced in a doc string is defined. Otherwise, I'd be interested in seeing how other people are doing it, but like @falexwolf, most of the packages I use don't have type annotations. Again, I think these would be less of an issue if quality writing on type annotation usage, particularly for scientific python, was available. As a Julia user, I found [this blog post](https://white.ucc.asn.au/2018/10/03/Dispatch,-Traits-and-Metaprogramming-Over-Reflection.html) very helpful not just for understanding how to implement trait types in Julia, but also when they're useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441414363
https://github.com/scverse/scanpy/issues/373#issuecomment-441414363:1302,Availability,avail,available,1302,"@flying-sheep Thanks for the thorough response! This is a topic I have a lot of thoughts on, though I'm not so sure how coherently I can communicate all of them. On your first thought:. The worst case scenario I see here me typing something so poorly a newbie trying to follow the documentation gets horrible numba errors they can't figure out. This could happen if I hadn't thought about `Set` being a subtype of `Collection`. It can be difficult to know the `abc`s in `typing` are supposed to mean without spending a while using them. For example, this is not what I was expecting:. ```python; >>> issubclass(np.ndarray, typing.Collection); True; >>> issubclass(np.ndarray, typing.Sequence); False; ```. On defining `abc`s for this package, I think it would have to be done in a way where it was easy to discover exactly what is meant by the annotated types. Personally, I read most of my documentation through the ipython repl, where it can be difficult to figure out where a type referenced in a doc string is defined. Otherwise, I'd be interested in seeing how other people are doing it, but like @falexwolf, most of the packages I use don't have type annotations. Again, I think these would be less of an issue if quality writing on type annotation usage, particularly for scientific python, was available. As a Julia user, I found [this blog post](https://white.ucc.asn.au/2018/10/03/Dispatch,-Traits-and-Metaprogramming-Over-Reflection.html) very helpful not just for understanding how to implement trait types in Julia, but also when they're useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441414363
https://github.com/scverse/scanpy/issues/373#issuecomment-441472798:164,Energy Efficiency,sustainab,sustainable,164,"@flying-sheep As always, thank you for your thorough thoughts on the topic! And as always, my ""hacking-numerics"" perspective likely is not a path that is long term sustainable. With what I wrote at the very beginning of this thread, I simply wanted to express that I thought that we shouldn't transition quickly and immediately; for the cosmetic reasons and for the reason of staying away from creating entry hurdles. I still don't think that scanpy needs to precede major packages like numpy and many others in adapting type annotations. But, in essence, I trust you and if you want to push this further I'm fine if scanpy becomes somewhat a field of experimentation for how to deal with type annotations in scientific and numerics-centered software. . @ivirshup Thank you very much for your remarks, too! I agree with your concerns and examples, but wouldn't have been able to summarize them as neatly. *Conclusion:* @flying-sheep if you feel you have bandwidth for improving the cosmetics (thanks for what you did already, also the PR to ipython) that lead to more homogeneous docstrings (I'd say: `Union[a, b]` → `a, b`), of course, please go ahead. If people make PRs with old-school docstrings and without type annotations, I'd still not trouble them, for now. When we have converged on new docstrings and canonical type annotations so that at least people who really know what they're doing (@ivirshup) don't feel things are ambiguous anymore (say in a year), we can start to rigorously ask for them. PS: Thanks for the hints about Jedi etc. @flying-sheep. But likely, I'll keep playing around and reading documentation of packages using shift-tab in jupyter and develop using emacs relatively plain (there were times when I worked with quite some extensions, but these days, I'm back to almost plain for performance reasons - I know that's probably not smart, but anyways)...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441472798
https://github.com/scverse/scanpy/issues/373#issuecomment-441472798:512,Energy Efficiency,adapt,adapting,512,"@flying-sheep As always, thank you for your thorough thoughts on the topic! And as always, my ""hacking-numerics"" perspective likely is not a path that is long term sustainable. With what I wrote at the very beginning of this thread, I simply wanted to express that I thought that we shouldn't transition quickly and immediately; for the cosmetic reasons and for the reason of staying away from creating entry hurdles. I still don't think that scanpy needs to precede major packages like numpy and many others in adapting type annotations. But, in essence, I trust you and if you want to push this further I'm fine if scanpy becomes somewhat a field of experimentation for how to deal with type annotations in scientific and numerics-centered software. . @ivirshup Thank you very much for your remarks, too! I agree with your concerns and examples, but wouldn't have been able to summarize them as neatly. *Conclusion:* @flying-sheep if you feel you have bandwidth for improving the cosmetics (thanks for what you did already, also the PR to ipython) that lead to more homogeneous docstrings (I'd say: `Union[a, b]` → `a, b`), of course, please go ahead. If people make PRs with old-school docstrings and without type annotations, I'd still not trouble them, for now. When we have converged on new docstrings and canonical type annotations so that at least people who really know what they're doing (@ivirshup) don't feel things are ambiguous anymore (say in a year), we can start to rigorously ask for them. PS: Thanks for the hints about Jedi etc. @flying-sheep. But likely, I'll keep playing around and reading documentation of packages using shift-tab in jupyter and develop using emacs relatively plain (there were times when I worked with quite some extensions, but these days, I'm back to almost plain for performance reasons - I know that's probably not smart, but anyways)...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441472798
https://github.com/scverse/scanpy/issues/373#issuecomment-441472798:512,Modifiability,adapt,adapting,512,"@flying-sheep As always, thank you for your thorough thoughts on the topic! And as always, my ""hacking-numerics"" perspective likely is not a path that is long term sustainable. With what I wrote at the very beginning of this thread, I simply wanted to express that I thought that we shouldn't transition quickly and immediately; for the cosmetic reasons and for the reason of staying away from creating entry hurdles. I still don't think that scanpy needs to precede major packages like numpy and many others in adapting type annotations. But, in essence, I trust you and if you want to push this further I'm fine if scanpy becomes somewhat a field of experimentation for how to deal with type annotations in scientific and numerics-centered software. . @ivirshup Thank you very much for your remarks, too! I agree with your concerns and examples, but wouldn't have been able to summarize them as neatly. *Conclusion:* @flying-sheep if you feel you have bandwidth for improving the cosmetics (thanks for what you did already, also the PR to ipython) that lead to more homogeneous docstrings (I'd say: `Union[a, b]` → `a, b`), of course, please go ahead. If people make PRs with old-school docstrings and without type annotations, I'd still not trouble them, for now. When we have converged on new docstrings and canonical type annotations so that at least people who really know what they're doing (@ivirshup) don't feel things are ambiguous anymore (say in a year), we can start to rigorously ask for them. PS: Thanks for the hints about Jedi etc. @flying-sheep. But likely, I'll keep playing around and reading documentation of packages using shift-tab in jupyter and develop using emacs relatively plain (there were times when I worked with quite some extensions, but these days, I'm back to almost plain for performance reasons - I know that's probably not smart, but anyways)...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441472798
https://github.com/scverse/scanpy/issues/373#issuecomment-441472798:1812,Performance,perform,performance,1812,"@flying-sheep As always, thank you for your thorough thoughts on the topic! And as always, my ""hacking-numerics"" perspective likely is not a path that is long term sustainable. With what I wrote at the very beginning of this thread, I simply wanted to express that I thought that we shouldn't transition quickly and immediately; for the cosmetic reasons and for the reason of staying away from creating entry hurdles. I still don't think that scanpy needs to precede major packages like numpy and many others in adapting type annotations. But, in essence, I trust you and if you want to push this further I'm fine if scanpy becomes somewhat a field of experimentation for how to deal with type annotations in scientific and numerics-centered software. . @ivirshup Thank you very much for your remarks, too! I agree with your concerns and examples, but wouldn't have been able to summarize them as neatly. *Conclusion:* @flying-sheep if you feel you have bandwidth for improving the cosmetics (thanks for what you did already, also the PR to ipython) that lead to more homogeneous docstrings (I'd say: `Union[a, b]` → `a, b`), of course, please go ahead. If people make PRs with old-school docstrings and without type annotations, I'd still not trouble them, for now. When we have converged on new docstrings and canonical type annotations so that at least people who really know what they're doing (@ivirshup) don't feel things are ambiguous anymore (say in a year), we can start to rigorously ask for them. PS: Thanks for the hints about Jedi etc. @flying-sheep. But likely, I'll keep playing around and reading documentation of packages using shift-tab in jupyter and develop using emacs relatively plain (there were times when I worked with quite some extensions, but these days, I'm back to almost plain for performance reasons - I know that's probably not smart, but anyways)...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441472798
https://github.com/scverse/scanpy/issues/373#issuecomment-441472798:235,Usability,simpl,simply,235,"@flying-sheep As always, thank you for your thorough thoughts on the topic! And as always, my ""hacking-numerics"" perspective likely is not a path that is long term sustainable. With what I wrote at the very beginning of this thread, I simply wanted to express that I thought that we shouldn't transition quickly and immediately; for the cosmetic reasons and for the reason of staying away from creating entry hurdles. I still don't think that scanpy needs to precede major packages like numpy and many others in adapting type annotations. But, in essence, I trust you and if you want to push this further I'm fine if scanpy becomes somewhat a field of experimentation for how to deal with type annotations in scientific and numerics-centered software. . @ivirshup Thank you very much for your remarks, too! I agree with your concerns and examples, but wouldn't have been able to summarize them as neatly. *Conclusion:* @flying-sheep if you feel you have bandwidth for improving the cosmetics (thanks for what you did already, also the PR to ipython) that lead to more homogeneous docstrings (I'd say: `Union[a, b]` → `a, b`), of course, please go ahead. If people make PRs with old-school docstrings and without type annotations, I'd still not trouble them, for now. When we have converged on new docstrings and canonical type annotations so that at least people who really know what they're doing (@ivirshup) don't feel things are ambiguous anymore (say in a year), we can start to rigorously ask for them. PS: Thanks for the hints about Jedi etc. @flying-sheep. But likely, I'll keep playing around and reading documentation of packages using shift-tab in jupyter and develop using emacs relatively plain (there were times when I worked with quite some extensions, but these days, I'm back to almost plain for performance reasons - I know that's probably not smart, but anyways)...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441472798
https://github.com/scverse/scanpy/issues/373#issuecomment-441583940:142,Availability,error,errors,142,"@ivirshup. > The worst case scenario I see here me typing something so poorly a newbie trying to follow the documentation gets horrible numba errors they can't figure out. Well, that’s an improvement over the current situation of “the freeform text type annotations make me guess what I can pass and I get horrible numba errors”, right?. > `issubclass(np.ndarray, typing.Sequence) == False`. That looks like a bug. The docs to `Sequence` say: “Concrete subclasses must override `__new__` or `__init__`, `__getitem__`, and `__len__`”, and. ```py; >>> np.ndarray.__new__ ; <function ndarray.__new__(*args, **kwargs)>; >>> np.ndarray.__getitem__ ; <slot wrapper '__getitem__' of 'numpy.ndarray' objects>; >>> np.ndarray.__len__ ; <slot wrapper '__len__' of 'numpy.ndarray' objects>; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441583940
https://github.com/scverse/scanpy/issues/373#issuecomment-441583940:321,Availability,error,errors,321,"@ivirshup. > The worst case scenario I see here me typing something so poorly a newbie trying to follow the documentation gets horrible numba errors they can't figure out. Well, that’s an improvement over the current situation of “the freeform text type annotations make me guess what I can pass and I get horrible numba errors”, right?. > `issubclass(np.ndarray, typing.Sequence) == False`. That looks like a bug. The docs to `Sequence` say: “Concrete subclasses must override `__new__` or `__init__`, `__getitem__`, and `__len__`”, and. ```py; >>> np.ndarray.__new__ ; <function ndarray.__new__(*args, **kwargs)>; >>> np.ndarray.__getitem__ ; <slot wrapper '__getitem__' of 'numpy.ndarray' objects>; >>> np.ndarray.__len__ ; <slot wrapper '__len__' of 'numpy.ndarray' objects>; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441583940
https://github.com/scverse/scanpy/issues/373#issuecomment-441583940:651,Integrability,wrap,wrapper,651,"@ivirshup. > The worst case scenario I see here me typing something so poorly a newbie trying to follow the documentation gets horrible numba errors they can't figure out. Well, that’s an improvement over the current situation of “the freeform text type annotations make me guess what I can pass and I get horrible numba errors”, right?. > `issubclass(np.ndarray, typing.Sequence) == False`. That looks like a bug. The docs to `Sequence` say: “Concrete subclasses must override `__new__` or `__init__`, `__getitem__`, and `__len__`”, and. ```py; >>> np.ndarray.__new__ ; <function ndarray.__new__(*args, **kwargs)>; >>> np.ndarray.__getitem__ ; <slot wrapper '__getitem__' of 'numpy.ndarray' objects>; >>> np.ndarray.__len__ ; <slot wrapper '__len__' of 'numpy.ndarray' objects>; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441583940
https://github.com/scverse/scanpy/issues/373#issuecomment-441583940:733,Integrability,wrap,wrapper,733,"@ivirshup. > The worst case scenario I see here me typing something so poorly a newbie trying to follow the documentation gets horrible numba errors they can't figure out. Well, that’s an improvement over the current situation of “the freeform text type annotations make me guess what I can pass and I get horrible numba errors”, right?. > `issubclass(np.ndarray, typing.Sequence) == False`. That looks like a bug. The docs to `Sequence` say: “Concrete subclasses must override `__new__` or `__init__`, `__getitem__`, and `__len__`”, and. ```py; >>> np.ndarray.__new__ ; <function ndarray.__new__(*args, **kwargs)>; >>> np.ndarray.__getitem__ ; <slot wrapper '__getitem__' of 'numpy.ndarray' objects>; >>> np.ndarray.__len__ ; <slot wrapper '__len__' of 'numpy.ndarray' objects>; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441583940
https://github.com/scverse/scanpy/issues/373#issuecomment-441590874:431,Deployability,patch,patching,431,"@falexwolf . > we shouldn't transition quickly and immediately; for the cosmetic reasons and for the reason of staying away from creating entry hurdles. Got it! so no “move fast and break things” but instead to identify problems and fix them before they occur. I think the most painful issues here are. 1. the signature rendering in ipython. Fixed in ipython/ipython#11505, We might incorporate a fix right now ourselves by monkey-patching `inspect.Signature.__str__` if we want. 2. losing contributions because of an entry hurdle. Hard to measure if this happens. If we lose someone, they won’t announce it. So maybe friendly [PR/issue templates](https://help.github.com/articles/about-issue-and-pull-request-templates/) or [contributing guidelines](https://help.github.com/articles/setting-guidelines-for-repository-contributors/) might help prevent that!. ---. > if you feel you have bandwidth for improving the cosmetics (thanks for what you did already, also the PR to ipython) that lead to more homogeneous docstrings (I'd say: `Union[a, b] → a, b`), of course, please go ahead. Will do, but a comma is ambiguous, as it could mean union, intersection, or (in Python) tuple. I think `Union[a, b, c]` → `a, b, or c` would be clearer. I think we should leave everything else as is: `Option[...]`, is clear enough, and `Callable` is better than introducing our own syntax (Some other languages know things like `(a, b) -> c` as type for functions, but Python doesn’t). > When we have converged on new docstrings and canonical type annotations so that at least people who really know what they're doing (@ivirshup) don't feel things are ambiguous anymore (say in a year), we can start to rigorously ask for them. good call! I might just edit them in-PR as I did to fix the colormaps in @fidelram’s last PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441590874
https://github.com/scverse/scanpy/issues/373#issuecomment-441590874:739,Usability,guid,guidelines,739,"@falexwolf . > we shouldn't transition quickly and immediately; for the cosmetic reasons and for the reason of staying away from creating entry hurdles. Got it! so no “move fast and break things” but instead to identify problems and fix them before they occur. I think the most painful issues here are. 1. the signature rendering in ipython. Fixed in ipython/ipython#11505, We might incorporate a fix right now ourselves by monkey-patching `inspect.Signature.__str__` if we want. 2. losing contributions because of an entry hurdle. Hard to measure if this happens. If we lose someone, they won’t announce it. So maybe friendly [PR/issue templates](https://help.github.com/articles/about-issue-and-pull-request-templates/) or [contributing guidelines](https://help.github.com/articles/setting-guidelines-for-repository-contributors/) might help prevent that!. ---. > if you feel you have bandwidth for improving the cosmetics (thanks for what you did already, also the PR to ipython) that lead to more homogeneous docstrings (I'd say: `Union[a, b] → a, b`), of course, please go ahead. Will do, but a comma is ambiguous, as it could mean union, intersection, or (in Python) tuple. I think `Union[a, b, c]` → `a, b, or c` would be clearer. I think we should leave everything else as is: `Option[...]`, is clear enough, and `Callable` is better than introducing our own syntax (Some other languages know things like `(a, b) -> c` as type for functions, but Python doesn’t). > When we have converged on new docstrings and canonical type annotations so that at least people who really know what they're doing (@ivirshup) don't feel things are ambiguous anymore (say in a year), we can start to rigorously ask for them. good call! I might just edit them in-PR as I did to fix the colormaps in @fidelram’s last PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441590874
https://github.com/scverse/scanpy/issues/373#issuecomment-441590874:792,Usability,guid,guidelines-for-repository-contributors,792,"@falexwolf . > we shouldn't transition quickly and immediately; for the cosmetic reasons and for the reason of staying away from creating entry hurdles. Got it! so no “move fast and break things” but instead to identify problems and fix them before they occur. I think the most painful issues here are. 1. the signature rendering in ipython. Fixed in ipython/ipython#11505, We might incorporate a fix right now ourselves by monkey-patching `inspect.Signature.__str__` if we want. 2. losing contributions because of an entry hurdle. Hard to measure if this happens. If we lose someone, they won’t announce it. So maybe friendly [PR/issue templates](https://help.github.com/articles/about-issue-and-pull-request-templates/) or [contributing guidelines](https://help.github.com/articles/setting-guidelines-for-repository-contributors/) might help prevent that!. ---. > if you feel you have bandwidth for improving the cosmetics (thanks for what you did already, also the PR to ipython) that lead to more homogeneous docstrings (I'd say: `Union[a, b] → a, b`), of course, please go ahead. Will do, but a comma is ambiguous, as it could mean union, intersection, or (in Python) tuple. I think `Union[a, b, c]` → `a, b, or c` would be clearer. I think we should leave everything else as is: `Option[...]`, is clear enough, and `Callable` is better than introducing our own syntax (Some other languages know things like `(a, b) -> c` as type for functions, but Python doesn’t). > When we have converged on new docstrings and canonical type annotations so that at least people who really know what they're doing (@ivirshup) don't feel things are ambiguous anymore (say in a year), we can start to rigorously ask for them. good call! I might just edit them in-PR as I did to fix the colormaps in @fidelram’s last PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441590874
https://github.com/scverse/scanpy/issues/373#issuecomment-441590874:1229,Usability,clear,clearer,1229,"@falexwolf . > we shouldn't transition quickly and immediately; for the cosmetic reasons and for the reason of staying away from creating entry hurdles. Got it! so no “move fast and break things” but instead to identify problems and fix them before they occur. I think the most painful issues here are. 1. the signature rendering in ipython. Fixed in ipython/ipython#11505, We might incorporate a fix right now ourselves by monkey-patching `inspect.Signature.__str__` if we want. 2. losing contributions because of an entry hurdle. Hard to measure if this happens. If we lose someone, they won’t announce it. So maybe friendly [PR/issue templates](https://help.github.com/articles/about-issue-and-pull-request-templates/) or [contributing guidelines](https://help.github.com/articles/setting-guidelines-for-repository-contributors/) might help prevent that!. ---. > if you feel you have bandwidth for improving the cosmetics (thanks for what you did already, also the PR to ipython) that lead to more homogeneous docstrings (I'd say: `Union[a, b] → a, b`), of course, please go ahead. Will do, but a comma is ambiguous, as it could mean union, intersection, or (in Python) tuple. I think `Union[a, b, c]` → `a, b, or c` would be clearer. I think we should leave everything else as is: `Option[...]`, is clear enough, and `Callable` is better than introducing our own syntax (Some other languages know things like `(a, b) -> c` as type for functions, but Python doesn’t). > When we have converged on new docstrings and canonical type annotations so that at least people who really know what they're doing (@ivirshup) don't feel things are ambiguous anymore (say in a year), we can start to rigorously ask for them. good call! I might just edit them in-PR as I did to fix the colormaps in @fidelram’s last PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441590874
https://github.com/scverse/scanpy/issues/373#issuecomment-441590874:1303,Usability,clear,clear,1303,"@falexwolf . > we shouldn't transition quickly and immediately; for the cosmetic reasons and for the reason of staying away from creating entry hurdles. Got it! so no “move fast and break things” but instead to identify problems and fix them before they occur. I think the most painful issues here are. 1. the signature rendering in ipython. Fixed in ipython/ipython#11505, We might incorporate a fix right now ourselves by monkey-patching `inspect.Signature.__str__` if we want. 2. losing contributions because of an entry hurdle. Hard to measure if this happens. If we lose someone, they won’t announce it. So maybe friendly [PR/issue templates](https://help.github.com/articles/about-issue-and-pull-request-templates/) or [contributing guidelines](https://help.github.com/articles/setting-guidelines-for-repository-contributors/) might help prevent that!. ---. > if you feel you have bandwidth for improving the cosmetics (thanks for what you did already, also the PR to ipython) that lead to more homogeneous docstrings (I'd say: `Union[a, b] → a, b`), of course, please go ahead. Will do, but a comma is ambiguous, as it could mean union, intersection, or (in Python) tuple. I think `Union[a, b, c]` → `a, b, or c` would be clearer. I think we should leave everything else as is: `Option[...]`, is clear enough, and `Callable` is better than introducing our own syntax (Some other languages know things like `(a, b) -> c` as type for functions, but Python doesn’t). > When we have converged on new docstrings and canonical type annotations so that at least people who really know what they're doing (@ivirshup) don't feel things are ambiguous anymore (say in a year), we can start to rigorously ask for them. good call! I might just edit them in-PR as I did to fix the colormaps in @fidelram’s last PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441590874
https://github.com/scverse/scanpy/issues/373#issuecomment-441598257:426,Usability,simpl,simply,426,"Great! . One last thing: In docstrings, why would you interpret a comma separated list as intersection or a tuple? This is not code but for humans. I'm even having a hard time to imagine the case that gives rise to an intersection. Also, a tuple in a docstring should always be verbose with `(,)`, so that no confusion is possible; we'll enforce that in the docs. Right now, the convention across all the major packages is to simply print out a comma separated list of types if you are allowed to pass different types to a parameter. This produces the least amount of visual distraction and maintains consistency for how it's done in Scanpy in the manual docstrings and everywhere else. If there is a case where an intersection is relevant, I'd treat that separately. Finally: `a, b, or c` is pretty elegant, too... but if there are no good answers to my two remarks I'd go for `a, b, c`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441598257
https://github.com/scverse/scanpy/issues/373#issuecomment-441684460:22,Integrability,wrap,wrap,22,"I created [legacy-api-wrap](https://github.com/flying-sheep/legacy-api-wrap). Only caveat: Scanpy is still 3.5 compatible, and I’m using f string syntax in it and its dependency [get_version](https://github.com/flying-sheep/get_version), so it’s python 3.6 only (which could be circumvented via [future-fstrings](https://github.com/asottile/future-fstrings) or so)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441684460
https://github.com/scverse/scanpy/issues/373#issuecomment-441684460:71,Integrability,wrap,wrap,71,"I created [legacy-api-wrap](https://github.com/flying-sheep/legacy-api-wrap). Only caveat: Scanpy is still 3.5 compatible, and I’m using f string syntax in it and its dependency [get_version](https://github.com/flying-sheep/get_version), so it’s python 3.6 only (which could be circumvented via [future-fstrings](https://github.com/asottile/future-fstrings) or so)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441684460
https://github.com/scverse/scanpy/issues/373#issuecomment-441684460:167,Integrability,depend,dependency,167,"I created [legacy-api-wrap](https://github.com/flying-sheep/legacy-api-wrap). Only caveat: Scanpy is still 3.5 compatible, and I’m using f string syntax in it and its dependency [get_version](https://github.com/flying-sheep/get_version), so it’s python 3.6 only (which could be circumvented via [future-fstrings](https://github.com/asottile/future-fstrings) or so)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441684460
https://github.com/scverse/scanpy/issues/373#issuecomment-441693979:174,Testability,log,logical,174,"> In docstrings, why would you interpret a comma separated list as intersection or a tuple?. In natural language “a, b, c” usually means “a, b, and c” (i.e. a composite or a logical intersection). And an intersection type is one that has all the attributes of all the types, like in `class x(a, b, c): ...` (where commas are also used). In Python plain `a, b, c` constructs a tuple (a composite type): `tup = a, b, c`. It took me a long time to find a numpy function that uses commas for anything other than the “, optional”, but of course [you’re right](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.matrix_rank.html). They do it like that. Why don’t people think before establishing conventions…. A good example of that function’s docs is also how braindead the “optional” is: for `tol`, it means “or None”, for `hermetian` it means “has a default” (probably, no way to know for sure). Goddamn. > Ah, we already have a contributing sheet. oh, is this visible? or does it need to be uppercase for that? CONTRIBUTING.md? I don’t see it when creating an issue, but maybe because I’m an organization member?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441693979
https://github.com/scverse/scanpy/issues/373#issuecomment-441771308:609,Testability,log,logical,609,"> In natural language... ... I know all that. :wink: But numpy, pandas, scikit learn, tensorflow, seaborn all have the comma-separated list as a convention and I'd really like to stick to that convention. > A good example... . No, the `optional` keyword always means that a parameter has a default. Very often, people forget to append ""or None"" (`, None`) to the list of possible types. Btw: that's maybe a nice way of thinking about it for you: you use a ""tuple of possible types"" to denote that any of these types can be passed in the function. As mentioned before, there is no point in using set-theoretic/logical notions like union or intersection as the topic is so simple that it doesn't need it (no need for an intersection, it's not even clear what that would mean; if you're stringent about it, it's also not clear for union). So, let's simply take the comma-separated list.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441771308
https://github.com/scverse/scanpy/issues/373#issuecomment-441771308:79,Usability,learn,learn,79,"> In natural language... ... I know all that. :wink: But numpy, pandas, scikit learn, tensorflow, seaborn all have the comma-separated list as a convention and I'd really like to stick to that convention. > A good example... . No, the `optional` keyword always means that a parameter has a default. Very often, people forget to append ""or None"" (`, None`) to the list of possible types. Btw: that's maybe a nice way of thinking about it for you: you use a ""tuple of possible types"" to denote that any of these types can be passed in the function. As mentioned before, there is no point in using set-theoretic/logical notions like union or intersection as the topic is so simple that it doesn't need it (no need for an intersection, it's not even clear what that would mean; if you're stringent about it, it's also not clear for union). So, let's simply take the comma-separated list.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441771308
https://github.com/scverse/scanpy/issues/373#issuecomment-441771308:671,Usability,simpl,simple,671,"> In natural language... ... I know all that. :wink: But numpy, pandas, scikit learn, tensorflow, seaborn all have the comma-separated list as a convention and I'd really like to stick to that convention. > A good example... . No, the `optional` keyword always means that a parameter has a default. Very often, people forget to append ""or None"" (`, None`) to the list of possible types. Btw: that's maybe a nice way of thinking about it for you: you use a ""tuple of possible types"" to denote that any of these types can be passed in the function. As mentioned before, there is no point in using set-theoretic/logical notions like union or intersection as the topic is so simple that it doesn't need it (no need for an intersection, it's not even clear what that would mean; if you're stringent about it, it's also not clear for union). So, let's simply take the comma-separated list.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441771308
https://github.com/scverse/scanpy/issues/373#issuecomment-441771308:746,Usability,clear,clear,746,"> In natural language... ... I know all that. :wink: But numpy, pandas, scikit learn, tensorflow, seaborn all have the comma-separated list as a convention and I'd really like to stick to that convention. > A good example... . No, the `optional` keyword always means that a parameter has a default. Very often, people forget to append ""or None"" (`, None`) to the list of possible types. Btw: that's maybe a nice way of thinking about it for you: you use a ""tuple of possible types"" to denote that any of these types can be passed in the function. As mentioned before, there is no point in using set-theoretic/logical notions like union or intersection as the topic is so simple that it doesn't need it (no need for an intersection, it's not even clear what that would mean; if you're stringent about it, it's also not clear for union). So, let's simply take the comma-separated list.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441771308
https://github.com/scverse/scanpy/issues/373#issuecomment-441771308:818,Usability,clear,clear,818,"> In natural language... ... I know all that. :wink: But numpy, pandas, scikit learn, tensorflow, seaborn all have the comma-separated list as a convention and I'd really like to stick to that convention. > A good example... . No, the `optional` keyword always means that a parameter has a default. Very often, people forget to append ""or None"" (`, None`) to the list of possible types. Btw: that's maybe a nice way of thinking about it for you: you use a ""tuple of possible types"" to denote that any of these types can be passed in the function. As mentioned before, there is no point in using set-theoretic/logical notions like union or intersection as the topic is so simple that it doesn't need it (no need for an intersection, it's not even clear what that would mean; if you're stringent about it, it's also not clear for union). So, let's simply take the comma-separated list.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441771308
https://github.com/scverse/scanpy/issues/373#issuecomment-441771308:846,Usability,simpl,simply,846,"> In natural language... ... I know all that. :wink: But numpy, pandas, scikit learn, tensorflow, seaborn all have the comma-separated list as a convention and I'd really like to stick to that convention. > A good example... . No, the `optional` keyword always means that a parameter has a default. Very often, people forget to append ""or None"" (`, None`) to the list of possible types. Btw: that's maybe a nice way of thinking about it for you: you use a ""tuple of possible types"" to denote that any of these types can be passed in the function. As mentioned before, there is no point in using set-theoretic/logical notions like union or intersection as the topic is so simple that it doesn't need it (no need for an intersection, it's not even clear what that would mean; if you're stringent about it, it's also not clear for union). So, let's simply take the comma-separated list.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441771308
https://github.com/scverse/scanpy/issues/373#issuecomment-442007106:1266,Modifiability,Variab,Variables,1266,"> I know all that. :wink:. And I know that you know! I just like to be comprehensive when presenting my arguments!. > But numpy, pandas, scikit learn, tensorflow, seaborn all have the comma-separated list as a convention and I'd really like to stick to that convention. OK. I’d prefer “a, b, or c”, but I’ll concede. It would also be no problem to change it later since all will be automated :+1: . > No, the optional keyword always means that a parameter has a default. Very often, people forget to append ""or None"" (, None) to the list of possible types. Well, when I open scanpy in PyCharm and someone forgot that in a type annotation, it highlights that fact to me. Pretty nice. > As mentioned before, there is no point in using set-theoretic/logical notions like union or intersection as the topic is so simple that it doesn't need it (no need for an intersection, it's not even clear what that would mean; if you're stringent about it, it's also not clear for union). Oh, then you didn’t hear of type theory. It’s a branch of logic: Type systems are formal systems, and in most of them the terms I used are well defined. The kinds of composite types I mentioned are:. - `Union` of types / Sum Type / [Tagged Union](https://en.wikipedia.org/wiki/Tagged_union): Variables with one of those have one of several fixed types.; - Subtype / [Intersection Type](https://en.wikipedia.org/wiki/Type_system#Intersection_types): Variables have all the properties of the supertypes.; - `Tuple` / [Product Type](https://en.wikipedia.org/wiki/Product_type): Variables contain multiple entries that each have one corresponding type.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-442007106
https://github.com/scverse/scanpy/issues/373#issuecomment-442007106:1423,Modifiability,Variab,Variables,1423,"> I know all that. :wink:. And I know that you know! I just like to be comprehensive when presenting my arguments!. > But numpy, pandas, scikit learn, tensorflow, seaborn all have the comma-separated list as a convention and I'd really like to stick to that convention. OK. I’d prefer “a, b, or c”, but I’ll concede. It would also be no problem to change it later since all will be automated :+1: . > No, the optional keyword always means that a parameter has a default. Very often, people forget to append ""or None"" (, None) to the list of possible types. Well, when I open scanpy in PyCharm and someone forgot that in a type annotation, it highlights that fact to me. Pretty nice. > As mentioned before, there is no point in using set-theoretic/logical notions like union or intersection as the topic is so simple that it doesn't need it (no need for an intersection, it's not even clear what that would mean; if you're stringent about it, it's also not clear for union). Oh, then you didn’t hear of type theory. It’s a branch of logic: Type systems are formal systems, and in most of them the terms I used are well defined. The kinds of composite types I mentioned are:. - `Union` of types / Sum Type / [Tagged Union](https://en.wikipedia.org/wiki/Tagged_union): Variables with one of those have one of several fixed types.; - Subtype / [Intersection Type](https://en.wikipedia.org/wiki/Type_system#Intersection_types): Variables have all the properties of the supertypes.; - `Tuple` / [Product Type](https://en.wikipedia.org/wiki/Product_type): Variables contain multiple entries that each have one corresponding type.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-442007106
https://github.com/scverse/scanpy/issues/373#issuecomment-442007106:1549,Modifiability,Variab,Variables,1549,"> I know all that. :wink:. And I know that you know! I just like to be comprehensive when presenting my arguments!. > But numpy, pandas, scikit learn, tensorflow, seaborn all have the comma-separated list as a convention and I'd really like to stick to that convention. OK. I’d prefer “a, b, or c”, but I’ll concede. It would also be no problem to change it later since all will be automated :+1: . > No, the optional keyword always means that a parameter has a default. Very often, people forget to append ""or None"" (, None) to the list of possible types. Well, when I open scanpy in PyCharm and someone forgot that in a type annotation, it highlights that fact to me. Pretty nice. > As mentioned before, there is no point in using set-theoretic/logical notions like union or intersection as the topic is so simple that it doesn't need it (no need for an intersection, it's not even clear what that would mean; if you're stringent about it, it's also not clear for union). Oh, then you didn’t hear of type theory. It’s a branch of logic: Type systems are formal systems, and in most of them the terms I used are well defined. The kinds of composite types I mentioned are:. - `Union` of types / Sum Type / [Tagged Union](https://en.wikipedia.org/wiki/Tagged_union): Variables with one of those have one of several fixed types.; - Subtype / [Intersection Type](https://en.wikipedia.org/wiki/Type_system#Intersection_types): Variables have all the properties of the supertypes.; - `Tuple` / [Product Type](https://en.wikipedia.org/wiki/Product_type): Variables contain multiple entries that each have one corresponding type.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-442007106
https://github.com/scverse/scanpy/issues/373#issuecomment-442007106:747,Testability,log,logical,747,"> I know all that. :wink:. And I know that you know! I just like to be comprehensive when presenting my arguments!. > But numpy, pandas, scikit learn, tensorflow, seaborn all have the comma-separated list as a convention and I'd really like to stick to that convention. OK. I’d prefer “a, b, or c”, but I’ll concede. It would also be no problem to change it later since all will be automated :+1: . > No, the optional keyword always means that a parameter has a default. Very often, people forget to append ""or None"" (, None) to the list of possible types. Well, when I open scanpy in PyCharm and someone forgot that in a type annotation, it highlights that fact to me. Pretty nice. > As mentioned before, there is no point in using set-theoretic/logical notions like union or intersection as the topic is so simple that it doesn't need it (no need for an intersection, it's not even clear what that would mean; if you're stringent about it, it's also not clear for union). Oh, then you didn’t hear of type theory. It’s a branch of logic: Type systems are formal systems, and in most of them the terms I used are well defined. The kinds of composite types I mentioned are:. - `Union` of types / Sum Type / [Tagged Union](https://en.wikipedia.org/wiki/Tagged_union): Variables with one of those have one of several fixed types.; - Subtype / [Intersection Type](https://en.wikipedia.org/wiki/Type_system#Intersection_types): Variables have all the properties of the supertypes.; - `Tuple` / [Product Type](https://en.wikipedia.org/wiki/Product_type): Variables contain multiple entries that each have one corresponding type.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-442007106
https://github.com/scverse/scanpy/issues/373#issuecomment-442007106:1032,Testability,log,logic,1032,"> I know all that. :wink:. And I know that you know! I just like to be comprehensive when presenting my arguments!. > But numpy, pandas, scikit learn, tensorflow, seaborn all have the comma-separated list as a convention and I'd really like to stick to that convention. OK. I’d prefer “a, b, or c”, but I’ll concede. It would also be no problem to change it later since all will be automated :+1: . > No, the optional keyword always means that a parameter has a default. Very often, people forget to append ""or None"" (, None) to the list of possible types. Well, when I open scanpy in PyCharm and someone forgot that in a type annotation, it highlights that fact to me. Pretty nice. > As mentioned before, there is no point in using set-theoretic/logical notions like union or intersection as the topic is so simple that it doesn't need it (no need for an intersection, it's not even clear what that would mean; if you're stringent about it, it's also not clear for union). Oh, then you didn’t hear of type theory. It’s a branch of logic: Type systems are formal systems, and in most of them the terms I used are well defined. The kinds of composite types I mentioned are:. - `Union` of types / Sum Type / [Tagged Union](https://en.wikipedia.org/wiki/Tagged_union): Variables with one of those have one of several fixed types.; - Subtype / [Intersection Type](https://en.wikipedia.org/wiki/Type_system#Intersection_types): Variables have all the properties of the supertypes.; - `Tuple` / [Product Type](https://en.wikipedia.org/wiki/Product_type): Variables contain multiple entries that each have one corresponding type.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-442007106
https://github.com/scverse/scanpy/issues/373#issuecomment-442007106:144,Usability,learn,learn,144,"> I know all that. :wink:. And I know that you know! I just like to be comprehensive when presenting my arguments!. > But numpy, pandas, scikit learn, tensorflow, seaborn all have the comma-separated list as a convention and I'd really like to stick to that convention. OK. I’d prefer “a, b, or c”, but I’ll concede. It would also be no problem to change it later since all will be automated :+1: . > No, the optional keyword always means that a parameter has a default. Very often, people forget to append ""or None"" (, None) to the list of possible types. Well, when I open scanpy in PyCharm and someone forgot that in a type annotation, it highlights that fact to me. Pretty nice. > As mentioned before, there is no point in using set-theoretic/logical notions like union or intersection as the topic is so simple that it doesn't need it (no need for an intersection, it's not even clear what that would mean; if you're stringent about it, it's also not clear for union). Oh, then you didn’t hear of type theory. It’s a branch of logic: Type systems are formal systems, and in most of them the terms I used are well defined. The kinds of composite types I mentioned are:. - `Union` of types / Sum Type / [Tagged Union](https://en.wikipedia.org/wiki/Tagged_union): Variables with one of those have one of several fixed types.; - Subtype / [Intersection Type](https://en.wikipedia.org/wiki/Type_system#Intersection_types): Variables have all the properties of the supertypes.; - `Tuple` / [Product Type](https://en.wikipedia.org/wiki/Product_type): Variables contain multiple entries that each have one corresponding type.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-442007106
https://github.com/scverse/scanpy/issues/373#issuecomment-442007106:809,Usability,simpl,simple,809,"> I know all that. :wink:. And I know that you know! I just like to be comprehensive when presenting my arguments!. > But numpy, pandas, scikit learn, tensorflow, seaborn all have the comma-separated list as a convention and I'd really like to stick to that convention. OK. I’d prefer “a, b, or c”, but I’ll concede. It would also be no problem to change it later since all will be automated :+1: . > No, the optional keyword always means that a parameter has a default. Very often, people forget to append ""or None"" (, None) to the list of possible types. Well, when I open scanpy in PyCharm and someone forgot that in a type annotation, it highlights that fact to me. Pretty nice. > As mentioned before, there is no point in using set-theoretic/logical notions like union or intersection as the topic is so simple that it doesn't need it (no need for an intersection, it's not even clear what that would mean; if you're stringent about it, it's also not clear for union). Oh, then you didn’t hear of type theory. It’s a branch of logic: Type systems are formal systems, and in most of them the terms I used are well defined. The kinds of composite types I mentioned are:. - `Union` of types / Sum Type / [Tagged Union](https://en.wikipedia.org/wiki/Tagged_union): Variables with one of those have one of several fixed types.; - Subtype / [Intersection Type](https://en.wikipedia.org/wiki/Type_system#Intersection_types): Variables have all the properties of the supertypes.; - `Tuple` / [Product Type](https://en.wikipedia.org/wiki/Product_type): Variables contain multiple entries that each have one corresponding type.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-442007106
https://github.com/scverse/scanpy/issues/373#issuecomment-442007106:884,Usability,clear,clear,884,"> I know all that. :wink:. And I know that you know! I just like to be comprehensive when presenting my arguments!. > But numpy, pandas, scikit learn, tensorflow, seaborn all have the comma-separated list as a convention and I'd really like to stick to that convention. OK. I’d prefer “a, b, or c”, but I’ll concede. It would also be no problem to change it later since all will be automated :+1: . > No, the optional keyword always means that a parameter has a default. Very often, people forget to append ""or None"" (, None) to the list of possible types. Well, when I open scanpy in PyCharm and someone forgot that in a type annotation, it highlights that fact to me. Pretty nice. > As mentioned before, there is no point in using set-theoretic/logical notions like union or intersection as the topic is so simple that it doesn't need it (no need for an intersection, it's not even clear what that would mean; if you're stringent about it, it's also not clear for union). Oh, then you didn’t hear of type theory. It’s a branch of logic: Type systems are formal systems, and in most of them the terms I used are well defined. The kinds of composite types I mentioned are:. - `Union` of types / Sum Type / [Tagged Union](https://en.wikipedia.org/wiki/Tagged_union): Variables with one of those have one of several fixed types.; - Subtype / [Intersection Type](https://en.wikipedia.org/wiki/Type_system#Intersection_types): Variables have all the properties of the supertypes.; - `Tuple` / [Product Type](https://en.wikipedia.org/wiki/Product_type): Variables contain multiple entries that each have one corresponding type.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-442007106
https://github.com/scverse/scanpy/issues/373#issuecomment-442007106:956,Usability,clear,clear,956,"> I know all that. :wink:. And I know that you know! I just like to be comprehensive when presenting my arguments!. > But numpy, pandas, scikit learn, tensorflow, seaborn all have the comma-separated list as a convention and I'd really like to stick to that convention. OK. I’d prefer “a, b, or c”, but I’ll concede. It would also be no problem to change it later since all will be automated :+1: . > No, the optional keyword always means that a parameter has a default. Very often, people forget to append ""or None"" (, None) to the list of possible types. Well, when I open scanpy in PyCharm and someone forgot that in a type annotation, it highlights that fact to me. Pretty nice. > As mentioned before, there is no point in using set-theoretic/logical notions like union or intersection as the topic is so simple that it doesn't need it (no need for an intersection, it's not even clear what that would mean; if you're stringent about it, it's also not clear for union). Oh, then you didn’t hear of type theory. It’s a branch of logic: Type systems are formal systems, and in most of them the terms I used are well defined. The kinds of composite types I mentioned are:. - `Union` of types / Sum Type / [Tagged Union](https://en.wikipedia.org/wiki/Tagged_union): Variables with one of those have one of several fixed types.; - Subtype / [Intersection Type](https://en.wikipedia.org/wiki/Type_system#Intersection_types): Variables have all the properties of the supertypes.; - `Tuple` / [Product Type](https://en.wikipedia.org/wiki/Product_type): Variables contain multiple entries that each have one corresponding type.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-442007106
https://github.com/scverse/scanpy/issues/373#issuecomment-443072359:1156,Integrability,depend,dependent,1156,"> Oh, then you didn’t hear of type theory. It’s a branch of logic:. Indeed, I have never heard of that. But I doubt that it would be considered a branch of logic. 1. `Union type` is a pretty bad descriptor for a variable that can take _one_ of a set of fixed types. A union usually denotes a composition of multiple sets giving rise to a new set that contains all elements from these sets.; 2. `Subtype` is a great descriptor for a type that has properties of supertypes.; 3. `Intersection type` is an insanely bad descriptor for a variable that denotes the intersection of _properties_ of supertypes; the concept of such a subtype might be something useful in some languages and some cases and it might deserve a special name as it's the converse behavior of subclassing. But I have no idea how such a type would be useful in Python and in all cases that I've encountered. The [example on Wikipedia](https://en.wikipedia.org/wiki/Type_system#Intersection_types) already constructs a highly artificial case, whose relevance is opaque to me even though Scanpy features it in many instances: functions that overload parameters and have different overloading-dependent return types (standard example is passing an array instead of an AnnData, which triggers the automatic return of the computed annotation). What do you think about 3, @flying-sheep?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-443072359
https://github.com/scverse/scanpy/issues/373#issuecomment-443072359:212,Modifiability,variab,variable,212,"> Oh, then you didn’t hear of type theory. It’s a branch of logic:. Indeed, I have never heard of that. But I doubt that it would be considered a branch of logic. 1. `Union type` is a pretty bad descriptor for a variable that can take _one_ of a set of fixed types. A union usually denotes a composition of multiple sets giving rise to a new set that contains all elements from these sets.; 2. `Subtype` is a great descriptor for a type that has properties of supertypes.; 3. `Intersection type` is an insanely bad descriptor for a variable that denotes the intersection of _properties_ of supertypes; the concept of such a subtype might be something useful in some languages and some cases and it might deserve a special name as it's the converse behavior of subclassing. But I have no idea how such a type would be useful in Python and in all cases that I've encountered. The [example on Wikipedia](https://en.wikipedia.org/wiki/Type_system#Intersection_types) already constructs a highly artificial case, whose relevance is opaque to me even though Scanpy features it in many instances: functions that overload parameters and have different overloading-dependent return types (standard example is passing an array instead of an AnnData, which triggers the automatic return of the computed annotation). What do you think about 3, @flying-sheep?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-443072359
https://github.com/scverse/scanpy/issues/373#issuecomment-443072359:532,Modifiability,variab,variable,532,"> Oh, then you didn’t hear of type theory. It’s a branch of logic:. Indeed, I have never heard of that. But I doubt that it would be considered a branch of logic. 1. `Union type` is a pretty bad descriptor for a variable that can take _one_ of a set of fixed types. A union usually denotes a composition of multiple sets giving rise to a new set that contains all elements from these sets.; 2. `Subtype` is a great descriptor for a type that has properties of supertypes.; 3. `Intersection type` is an insanely bad descriptor for a variable that denotes the intersection of _properties_ of supertypes; the concept of such a subtype might be something useful in some languages and some cases and it might deserve a special name as it's the converse behavior of subclassing. But I have no idea how such a type would be useful in Python and in all cases that I've encountered. The [example on Wikipedia](https://en.wikipedia.org/wiki/Type_system#Intersection_types) already constructs a highly artificial case, whose relevance is opaque to me even though Scanpy features it in many instances: functions that overload parameters and have different overloading-dependent return types (standard example is passing an array instead of an AnnData, which triggers the automatic return of the computed annotation). What do you think about 3, @flying-sheep?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-443072359
https://github.com/scverse/scanpy/issues/373#issuecomment-443072359:60,Testability,log,logic,60,"> Oh, then you didn’t hear of type theory. It’s a branch of logic:. Indeed, I have never heard of that. But I doubt that it would be considered a branch of logic. 1. `Union type` is a pretty bad descriptor for a variable that can take _one_ of a set of fixed types. A union usually denotes a composition of multiple sets giving rise to a new set that contains all elements from these sets.; 2. `Subtype` is a great descriptor for a type that has properties of supertypes.; 3. `Intersection type` is an insanely bad descriptor for a variable that denotes the intersection of _properties_ of supertypes; the concept of such a subtype might be something useful in some languages and some cases and it might deserve a special name as it's the converse behavior of subclassing. But I have no idea how such a type would be useful in Python and in all cases that I've encountered. The [example on Wikipedia](https://en.wikipedia.org/wiki/Type_system#Intersection_types) already constructs a highly artificial case, whose relevance is opaque to me even though Scanpy features it in many instances: functions that overload parameters and have different overloading-dependent return types (standard example is passing an array instead of an AnnData, which triggers the automatic return of the computed annotation). What do you think about 3, @flying-sheep?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-443072359
https://github.com/scverse/scanpy/issues/373#issuecomment-443072359:156,Testability,log,logic,156,"> Oh, then you didn’t hear of type theory. It’s a branch of logic:. Indeed, I have never heard of that. But I doubt that it would be considered a branch of logic. 1. `Union type` is a pretty bad descriptor for a variable that can take _one_ of a set of fixed types. A union usually denotes a composition of multiple sets giving rise to a new set that contains all elements from these sets.; 2. `Subtype` is a great descriptor for a type that has properties of supertypes.; 3. `Intersection type` is an insanely bad descriptor for a variable that denotes the intersection of _properties_ of supertypes; the concept of such a subtype might be something useful in some languages and some cases and it might deserve a special name as it's the converse behavior of subclassing. But I have no idea how such a type would be useful in Python and in all cases that I've encountered. The [example on Wikipedia](https://en.wikipedia.org/wiki/Type_system#Intersection_types) already constructs a highly artificial case, whose relevance is opaque to me even though Scanpy features it in many instances: functions that overload parameters and have different overloading-dependent return types (standard example is passing an array instead of an AnnData, which triggers the automatic return of the computed annotation). What do you think about 3, @flying-sheep?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-443072359
https://github.com/scverse/scanpy/issues/373#issuecomment-443178140:1336,Integrability,interface,interfaces,1336,"> I doubt that it would be considered a branch of logic. What do you define as logic here? I was talking about the logic theory that encompasses formal systems and so on. > [Union and intersection are bad names]. I agree, wikipedia enumerates more names, and explains where “union” comes from:. > tagged union, variant, variant record, choice type, discriminated union, disjoint union, or sum type; > …; > Mathematically, tagged unions correspond to disjoint or discriminated unions, usually written using +. Given an element of a disjoint union A + B, it is possible to determine whether it came from A or B. If an element lies in both, there will be two effectively distinct copies of the value in A + B, one from A and one from B. . I think “discriminated union/intersection of types” would make sense here. leaving out the “discriminated/tagged/disjoint” here is the problem. in C there’s actual *untagged* unions, which simply means that C reserves the memory for the largest of the intersected types and you need to keep track yourself of which the type of the value is. In python you can always do `isinstance`, so a more correct name for `Union[A, B]` would be `TaggedUnion[A, B]`. I’d also like `OneOf[A, B]`, but that ship has sailed. And intersections are basically duck types or structural types (when anonymous) and traits/interfaces (when named). (i.e. `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. So it makes sense for python, it’s just defined more explicitly than by literally intersecting types.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-443178140
https://github.com/scverse/scanpy/issues/373#issuecomment-443178140:50,Testability,log,logic,50,"> I doubt that it would be considered a branch of logic. What do you define as logic here? I was talking about the logic theory that encompasses formal systems and so on. > [Union and intersection are bad names]. I agree, wikipedia enumerates more names, and explains where “union” comes from:. > tagged union, variant, variant record, choice type, discriminated union, disjoint union, or sum type; > …; > Mathematically, tagged unions correspond to disjoint or discriminated unions, usually written using +. Given an element of a disjoint union A + B, it is possible to determine whether it came from A or B. If an element lies in both, there will be two effectively distinct copies of the value in A + B, one from A and one from B. . I think “discriminated union/intersection of types” would make sense here. leaving out the “discriminated/tagged/disjoint” here is the problem. in C there’s actual *untagged* unions, which simply means that C reserves the memory for the largest of the intersected types and you need to keep track yourself of which the type of the value is. In python you can always do `isinstance`, so a more correct name for `Union[A, B]` would be `TaggedUnion[A, B]`. I’d also like `OneOf[A, B]`, but that ship has sailed. And intersections are basically duck types or structural types (when anonymous) and traits/interfaces (when named). (i.e. `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. So it makes sense for python, it’s just defined more explicitly than by literally intersecting types.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-443178140
https://github.com/scverse/scanpy/issues/373#issuecomment-443178140:79,Testability,log,logic,79,"> I doubt that it would be considered a branch of logic. What do you define as logic here? I was talking about the logic theory that encompasses formal systems and so on. > [Union and intersection are bad names]. I agree, wikipedia enumerates more names, and explains where “union” comes from:. > tagged union, variant, variant record, choice type, discriminated union, disjoint union, or sum type; > …; > Mathematically, tagged unions correspond to disjoint or discriminated unions, usually written using +. Given an element of a disjoint union A + B, it is possible to determine whether it came from A or B. If an element lies in both, there will be two effectively distinct copies of the value in A + B, one from A and one from B. . I think “discriminated union/intersection of types” would make sense here. leaving out the “discriminated/tagged/disjoint” here is the problem. in C there’s actual *untagged* unions, which simply means that C reserves the memory for the largest of the intersected types and you need to keep track yourself of which the type of the value is. In python you can always do `isinstance`, so a more correct name for `Union[A, B]` would be `TaggedUnion[A, B]`. I’d also like `OneOf[A, B]`, but that ship has sailed. And intersections are basically duck types or structural types (when anonymous) and traits/interfaces (when named). (i.e. `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. So it makes sense for python, it’s just defined more explicitly than by literally intersecting types.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-443178140
https://github.com/scverse/scanpy/issues/373#issuecomment-443178140:115,Testability,log,logic,115,"> I doubt that it would be considered a branch of logic. What do you define as logic here? I was talking about the logic theory that encompasses formal systems and so on. > [Union and intersection are bad names]. I agree, wikipedia enumerates more names, and explains where “union” comes from:. > tagged union, variant, variant record, choice type, discriminated union, disjoint union, or sum type; > …; > Mathematically, tagged unions correspond to disjoint or discriminated unions, usually written using +. Given an element of a disjoint union A + B, it is possible to determine whether it came from A or B. If an element lies in both, there will be two effectively distinct copies of the value in A + B, one from A and one from B. . I think “discriminated union/intersection of types” would make sense here. leaving out the “discriminated/tagged/disjoint” here is the problem. in C there’s actual *untagged* unions, which simply means that C reserves the memory for the largest of the intersected types and you need to keep track yourself of which the type of the value is. In python you can always do `isinstance`, so a more correct name for `Union[A, B]` would be `TaggedUnion[A, B]`. I’d also like `OneOf[A, B]`, but that ship has sailed. And intersections are basically duck types or structural types (when anonymous) and traits/interfaces (when named). (i.e. `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. So it makes sense for python, it’s just defined more explicitly than by literally intersecting types.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-443178140
https://github.com/scverse/scanpy/issues/373#issuecomment-443178140:925,Usability,simpl,simply,925,"> I doubt that it would be considered a branch of logic. What do you define as logic here? I was talking about the logic theory that encompasses formal systems and so on. > [Union and intersection are bad names]. I agree, wikipedia enumerates more names, and explains where “union” comes from:. > tagged union, variant, variant record, choice type, discriminated union, disjoint union, or sum type; > …; > Mathematically, tagged unions correspond to disjoint or discriminated unions, usually written using +. Given an element of a disjoint union A + B, it is possible to determine whether it came from A or B. If an element lies in both, there will be two effectively distinct copies of the value in A + B, one from A and one from B. . I think “discriminated union/intersection of types” would make sense here. leaving out the “discriminated/tagged/disjoint” here is the problem. in C there’s actual *untagged* unions, which simply means that C reserves the memory for the largest of the intersected types and you need to keep track yourself of which the type of the value is. In python you can always do `isinstance`, so a more correct name for `Union[A, B]` would be `TaggedUnion[A, B]`. I’d also like `OneOf[A, B]`, but that ship has sailed. And intersections are basically duck types or structural types (when anonymous) and traits/interfaces (when named). (i.e. `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. So it makes sense for python, it’s just defined more explicitly than by literally intersecting types.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-443178140
https://github.com/scverse/scanpy/issues/373#issuecomment-443255977:248,Energy Efficiency,adapt,adapt,248,"sure! in short: alex said he didn’t like the switch to type annotations at all, citing a few gripes. i went on to fix them at various places (fixes are now in) and argued against a few others. i convinced alex that we should (slowly and carefully) adapt type annotations. the only thing that was missing is a consensus on how to best pretty-print `typing.Union`, because alex was not a fan of the name and clumsiness. I preferred `a, b, or c`, he just `a, b, c`. i explained why `a, b, c` is a bad convention, but alex insisted to go with it because (sadly) everyone is doing it. from there on we went deeper into algebraic types and so on. without need really, as we already decided on what to do.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-443255977
https://github.com/scverse/scanpy/issues/373#issuecomment-443255977:248,Modifiability,adapt,adapt,248,"sure! in short: alex said he didn’t like the switch to type annotations at all, citing a few gripes. i went on to fix them at various places (fixes are now in) and argued against a few others. i convinced alex that we should (slowly and carefully) adapt type annotations. the only thing that was missing is a consensus on how to best pretty-print `typing.Union`, because alex was not a fan of the name and clumsiness. I preferred `a, b, or c`, he just `a, b, c`. i explained why `a, b, c` is a bad convention, but alex insisted to go with it because (sadly) everyone is doing it. from there on we went deeper into algebraic types and so on. without need really, as we already decided on what to do.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-443255977
https://github.com/scverse/scanpy/issues/373#issuecomment-443397973:776,Integrability,depend,dependent,776,"> What do you define as logic here?. [Logic](https://en.wikipedia.org/wiki/Logic) as the mother of all formal reasoning and its close relative set theory in mathematics. When you say type theory is a branch of logic then 90% of computer science is a branch of logic. In many contexts this might be a valid but not a very useful statement. > I’d also like `OneOf[A, B]`. I love `OneOf[A, B]`. This also doesn't pretend to be logic or set stuff. > `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. This is what I meant when I said _intersection of properties of supertypes_. But I still don't know when you'd need such a type in a practical context, given that we just keep overloading functions like crazy and simply treat passed arguments dependent on their type. Any example when intersection types are actually useful? In a function we might see in Scanpy (this was the whole beginning of this discussion; I cannot imagine a case in which we need to label something _intersection type_ in the docs).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-443397973
https://github.com/scverse/scanpy/issues/373#issuecomment-443397973:24,Testability,log,logic,24,"> What do you define as logic here?. [Logic](https://en.wikipedia.org/wiki/Logic) as the mother of all formal reasoning and its close relative set theory in mathematics. When you say type theory is a branch of logic then 90% of computer science is a branch of logic. In many contexts this might be a valid but not a very useful statement. > I’d also like `OneOf[A, B]`. I love `OneOf[A, B]`. This also doesn't pretend to be logic or set stuff. > `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. This is what I meant when I said _intersection of properties of supertypes_. But I still don't know when you'd need such a type in a practical context, given that we just keep overloading functions like crazy and simply treat passed arguments dependent on their type. Any example when intersection types are actually useful? In a function we might see in Scanpy (this was the whole beginning of this discussion; I cannot imagine a case in which we need to label something _intersection type_ in the docs).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-443397973
https://github.com/scverse/scanpy/issues/373#issuecomment-443397973:38,Testability,Log,Logic,38,"> What do you define as logic here?. [Logic](https://en.wikipedia.org/wiki/Logic) as the mother of all formal reasoning and its close relative set theory in mathematics. When you say type theory is a branch of logic then 90% of computer science is a branch of logic. In many contexts this might be a valid but not a very useful statement. > I’d also like `OneOf[A, B]`. I love `OneOf[A, B]`. This also doesn't pretend to be logic or set stuff. > `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. This is what I meant when I said _intersection of properties of supertypes_. But I still don't know when you'd need such a type in a practical context, given that we just keep overloading functions like crazy and simply treat passed arguments dependent on their type. Any example when intersection types are actually useful? In a function we might see in Scanpy (this was the whole beginning of this discussion; I cannot imagine a case in which we need to label something _intersection type_ in the docs).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-443397973
https://github.com/scverse/scanpy/issues/373#issuecomment-443397973:75,Testability,Log,Logic,75,"> What do you define as logic here?. [Logic](https://en.wikipedia.org/wiki/Logic) as the mother of all formal reasoning and its close relative set theory in mathematics. When you say type theory is a branch of logic then 90% of computer science is a branch of logic. In many contexts this might be a valid but not a very useful statement. > I’d also like `OneOf[A, B]`. I love `OneOf[A, B]`. This also doesn't pretend to be logic or set stuff. > `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. This is what I meant when I said _intersection of properties of supertypes_. But I still don't know when you'd need such a type in a practical context, given that we just keep overloading functions like crazy and simply treat passed arguments dependent on their type. Any example when intersection types are actually useful? In a function we might see in Scanpy (this was the whole beginning of this discussion; I cannot imagine a case in which we need to label something _intersection type_ in the docs).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-443397973
https://github.com/scverse/scanpy/issues/373#issuecomment-443397973:210,Testability,log,logic,210,"> What do you define as logic here?. [Logic](https://en.wikipedia.org/wiki/Logic) as the mother of all formal reasoning and its close relative set theory in mathematics. When you say type theory is a branch of logic then 90% of computer science is a branch of logic. In many contexts this might be a valid but not a very useful statement. > I’d also like `OneOf[A, B]`. I love `OneOf[A, B]`. This also doesn't pretend to be logic or set stuff. > `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. This is what I meant when I said _intersection of properties of supertypes_. But I still don't know when you'd need such a type in a practical context, given that we just keep overloading functions like crazy and simply treat passed arguments dependent on their type. Any example when intersection types are actually useful? In a function we might see in Scanpy (this was the whole beginning of this discussion; I cannot imagine a case in which we need to label something _intersection type_ in the docs).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-443397973
https://github.com/scverse/scanpy/issues/373#issuecomment-443397973:260,Testability,log,logic,260,"> What do you define as logic here?. [Logic](https://en.wikipedia.org/wiki/Logic) as the mother of all formal reasoning and its close relative set theory in mathematics. When you say type theory is a branch of logic then 90% of computer science is a branch of logic. In many contexts this might be a valid but not a very useful statement. > I’d also like `OneOf[A, B]`. I love `OneOf[A, B]`. This also doesn't pretend to be logic or set stuff. > `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. This is what I meant when I said _intersection of properties of supertypes_. But I still don't know when you'd need such a type in a practical context, given that we just keep overloading functions like crazy and simply treat passed arguments dependent on their type. Any example when intersection types are actually useful? In a function we might see in Scanpy (this was the whole beginning of this discussion; I cannot imagine a case in which we need to label something _intersection type_ in the docs).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-443397973
https://github.com/scverse/scanpy/issues/373#issuecomment-443397973:424,Testability,log,logic,424,"> What do you define as logic here?. [Logic](https://en.wikipedia.org/wiki/Logic) as the mother of all formal reasoning and its close relative set theory in mathematics. When you say type theory is a branch of logic then 90% of computer science is a branch of logic. In many contexts this might be a valid but not a very useful statement. > I’d also like `OneOf[A, B]`. I love `OneOf[A, B]`. This also doesn't pretend to be logic or set stuff. > `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. This is what I meant when I said _intersection of properties of supertypes_. But I still don't know when you'd need such a type in a practical context, given that we just keep overloading functions like crazy and simply treat passed arguments dependent on their type. Any example when intersection types are actually useful? In a function we might see in Scanpy (this was the whole beginning of this discussion; I cannot imagine a case in which we need to label something _intersection type_ in the docs).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-443397973
https://github.com/scverse/scanpy/issues/373#issuecomment-443397973:746,Usability,simpl,simply,746,"> What do you define as logic here?. [Logic](https://en.wikipedia.org/wiki/Logic) as the mother of all formal reasoning and its close relative set theory in mathematics. When you say type theory is a branch of logic then 90% of computer science is a branch of logic. In many contexts this might be a valid but not a very useful statement. > I’d also like `OneOf[A, B]`. I love `OneOf[A, B]`. This also doesn't pretend to be logic or set stuff. > `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. This is what I meant when I said _intersection of properties of supertypes_. But I still don't know when you'd need such a type in a practical context, given that we just keep overloading functions like crazy and simply treat passed arguments dependent on their type. Any example when intersection types are actually useful? In a function we might see in Scanpy (this was the whole beginning of this discussion; I cannot imagine a case in which we need to label something _intersection type_ in the docs).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-443397973
https://github.com/scverse/scanpy/issues/373#issuecomment-443655464:710,Integrability,wrap,wrap,710,"People are discussing intersections [here](https://github.com/python/typing/issues/213). And I agree with you @ivirshup: That’s a great example where an intersection would be needed. Unions are only useful if you accept several things and somewhere switch behavior based on what you got like `if isinstance(...):`. I think that Alex just means that something like that isn’t needed anywhere in scanpy. An aside about switching behavior based on types: Too bad Python hasn’t been designed with destructuring `match`/`switch`. Rust is beautiful because of it. However, Python would probably need to use something like [scala’s unappy](https://docs.scala-lang.org/tour/extractor-objects.html) which I could never wrap my brain around.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-443655464
https://github.com/scverse/scanpy/issues/373#issuecomment-443966884:973,Availability,error,error,973,"When I read. > like Union[OrderedDict, pd.Series], this is a fine substitute for Intersect[Mapping, Sequence]. in the context of. > An example I could think of is needing key value lookup which is also ordered could be thought of as the intersection of Mapping and Sequence types. then `Intersect[Mapping, Sequence]` expects a new ""intersection object"" (here just an `OrderedDict`), which is to me an ""intersection way"" of subclassing - and the first is `OneOf`. So these are different things. My point is (repeating what Philipp said): in practice (in all the numerical stuff that I've done so far, including Scanpy), I have never encountered the need for defining such an intersection object on the typing level. I just overload functions using `OneOf` and account for differences in the passed objects attributes via `if isinstance(...):`... If I need a function that only eats a ""weird intersection type object"", I'll go and define the corresponding class and throw an error if the function gets fed something different. Fortunately, that happens quite rarely; but yeah, I had cases where I only wanted an `OrderedDict` but neither a `dict` or a `list`. But I'd never call this an ""intersection type"". @ivirshup You didn't explain the ""type lattice"": but according to what I learned about `Union` and `Intersection` in this thread, the sets involved in the mentioned ""set operations on the type lattice"" should have elements that are ""properties"" of types (as they are not restricted to actual class attributes, this, unfortunately, doesn't tell you right away which ""property"" you are intersecting: ""being ordered"", ""having a key accesor"", ""having a certain numerical range""). Right? Union and Intersection then refer to the maximal set of properties of the objects you pass. As each passed object can be characterized by a set of properties, all that naming makes sense. But for someone reading the docs, who isn't expected to know about all the properties of all each object that comes along th",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-443966884
https://github.com/scverse/scanpy/issues/373#issuecomment-443966884:1279,Usability,learn,learned,1279,"e first is `OneOf`. So these are different things. My point is (repeating what Philipp said): in practice (in all the numerical stuff that I've done so far, including Scanpy), I have never encountered the need for defining such an intersection object on the typing level. I just overload functions using `OneOf` and account for differences in the passed objects attributes via `if isinstance(...):`... If I need a function that only eats a ""weird intersection type object"", I'll go and define the corresponding class and throw an error if the function gets fed something different. Fortunately, that happens quite rarely; but yeah, I had cases where I only wanted an `OrderedDict` but neither a `dict` or a `list`. But I'd never call this an ""intersection type"". @ivirshup You didn't explain the ""type lattice"": but according to what I learned about `Union` and `Intersection` in this thread, the sets involved in the mentioned ""set operations on the type lattice"" should have elements that are ""properties"" of types (as they are not restricted to actual class attributes, this, unfortunately, doesn't tell you right away which ""property"" you are intersecting: ""being ordered"", ""having a key accesor"", ""having a certain numerical range""). Right? Union and Intersection then refer to the maximal set of properties of the objects you pass. As each passed object can be characterized by a set of properties, all that naming makes sense. But for someone reading the docs, who isn't expected to know about all the properties of all each object that comes along the way, it's a really sophisticated concept. As a user, I want to characterize things with a simple name for a class, like `AnnData` or `OrderedDict`. And these are the things that I want to see in the docs. Don't you agree? By contrast, I'm fine with having the sophisticated in the code; even though I still think that a plain old school untyped function signature looks more beautiful and its content can immediately be grasped by a human.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-443966884
https://github.com/scverse/scanpy/issues/373#issuecomment-443966884:2093,Usability,simpl,simple,2093,"he first is `OneOf`. So these are different things. My point is (repeating what Philipp said): in practice (in all the numerical stuff that I've done so far, including Scanpy), I have never encountered the need for defining such an intersection object on the typing level. I just overload functions using `OneOf` and account for differences in the passed objects attributes via `if isinstance(...):`... If I need a function that only eats a ""weird intersection type object"", I'll go and define the corresponding class and throw an error if the function gets fed something different. Fortunately, that happens quite rarely; but yeah, I had cases where I only wanted an `OrderedDict` but neither a `dict` or a `list`. But I'd never call this an ""intersection type"". @ivirshup You didn't explain the ""type lattice"": but according to what I learned about `Union` and `Intersection` in this thread, the sets involved in the mentioned ""set operations on the type lattice"" should have elements that are ""properties"" of types (as they are not restricted to actual class attributes, this, unfortunately, doesn't tell you right away which ""property"" you are intersecting: ""being ordered"", ""having a key accesor"", ""having a certain numerical range""). Right? Union and Intersection then refer to the maximal set of properties of the objects you pass. As each passed object can be characterized by a set of properties, all that naming makes sense. But for someone reading the docs, who isn't expected to know about all the properties of all each object that comes along the way, it's a really sophisticated concept. As a user, I want to characterize things with a simple name for a class, like `AnnData` or `OrderedDict`. And these are the things that I want to see in the docs. Don't you agree? By contrast, I'm fine with having the sophisticated in the code; even though I still think that a plain old school untyped function signature looks more beautiful and its content can immediately be grasped by a human.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-443966884
https://github.com/scverse/scanpy/issues/373#issuecomment-444715545:466,Integrability,interface,interfaces,466,"@flying-sheep, I'm pretty sure the logical conclusion of any long discussion about types is that everything should be done in Haskell. I don't like the use of branches with `isinstance` because it breaks polymorphism, which is a key part of pythonic code to me. @falexwolf, I completely agree with you on ""what makes a good docstring"". The knowledge overhead for numeric python doesn't include type theory, so the docs should be interpretable without them. Ideally, interfaces are simple and the documentation makes the expected behavior clear. I'm still not sure I totally understand what the intent of the ""type"" vs. ""class"" system is in python, so I'm often a little unsure what to do with heavily typed code. That said, if expected behaviors could be encapsulated (both formally and intuitively) with some abstract types (representing interfaces or traits) that would be a nicer solution. I don't think we're near that point in python. ## Lattices. Sorry about not giving some info on lattices, I'd thought you didn't want to get into it. It's the [partially ordered set kind](https://en.wikipedia.org/wiki/Lattice_(order)) of lattice, where each type is an element or subset. I'll give a short python based example (ignoring that `Union[]` can't be instantiated). <details>. <summary>The code:</summary>. ```python; from typing import Any, Union. class A():; pass. class B(A):; pass. class C(A):; pass. class D():; pass. class E(D):; pass; ```. </details>. that defines a lattice, which can be represented as a DAG like this:. ```; Any; / \; A D; / \ |; B C E; \ | /; Union[]; ```. It's partially ordered in that you can't say A contains E or vice-versa, but you can say things like A is contains B, and `Any` is a supertype of (contains) everything else. I think that how you're viewing it is pretty close, except the elements are types instead of their properties. My mental model has types being a collection of properties, and being a subtype means an object inherits it's supertypes properti",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-444715545
https://github.com/scverse/scanpy/issues/373#issuecomment-444715545:839,Integrability,interface,interfaces,839,"@flying-sheep, I'm pretty sure the logical conclusion of any long discussion about types is that everything should be done in Haskell. I don't like the use of branches with `isinstance` because it breaks polymorphism, which is a key part of pythonic code to me. @falexwolf, I completely agree with you on ""what makes a good docstring"". The knowledge overhead for numeric python doesn't include type theory, so the docs should be interpretable without them. Ideally, interfaces are simple and the documentation makes the expected behavior clear. I'm still not sure I totally understand what the intent of the ""type"" vs. ""class"" system is in python, so I'm often a little unsure what to do with heavily typed code. That said, if expected behaviors could be encapsulated (both formally and intuitively) with some abstract types (representing interfaces or traits) that would be a nicer solution. I don't think we're near that point in python. ## Lattices. Sorry about not giving some info on lattices, I'd thought you didn't want to get into it. It's the [partially ordered set kind](https://en.wikipedia.org/wiki/Lattice_(order)) of lattice, where each type is an element or subset. I'll give a short python based example (ignoring that `Union[]` can't be instantiated). <details>. <summary>The code:</summary>. ```python; from typing import Any, Union. class A():; pass. class B(A):; pass. class C(A):; pass. class D():; pass. class E(D):; pass; ```. </details>. that defines a lattice, which can be represented as a DAG like this:. ```; Any; / \; A D; / \ |; B C E; \ | /; Union[]; ```. It's partially ordered in that you can't say A contains E or vice-versa, but you can say things like A is contains B, and `Any` is a supertype of (contains) everything else. I think that how you're viewing it is pretty close, except the elements are types instead of their properties. My mental model has types being a collection of properties, and being a subtype means an object inherits it's supertypes properti",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-444715545
https://github.com/scverse/scanpy/issues/373#issuecomment-444715545:204,Modifiability,polymorphi,polymorphism,204,"@flying-sheep, I'm pretty sure the logical conclusion of any long discussion about types is that everything should be done in Haskell. I don't like the use of branches with `isinstance` because it breaks polymorphism, which is a key part of pythonic code to me. @falexwolf, I completely agree with you on ""what makes a good docstring"". The knowledge overhead for numeric python doesn't include type theory, so the docs should be interpretable without them. Ideally, interfaces are simple and the documentation makes the expected behavior clear. I'm still not sure I totally understand what the intent of the ""type"" vs. ""class"" system is in python, so I'm often a little unsure what to do with heavily typed code. That said, if expected behaviors could be encapsulated (both formally and intuitively) with some abstract types (representing interfaces or traits) that would be a nicer solution. I don't think we're near that point in python. ## Lattices. Sorry about not giving some info on lattices, I'd thought you didn't want to get into it. It's the [partially ordered set kind](https://en.wikipedia.org/wiki/Lattice_(order)) of lattice, where each type is an element or subset. I'll give a short python based example (ignoring that `Union[]` can't be instantiated). <details>. <summary>The code:</summary>. ```python; from typing import Any, Union. class A():; pass. class B(A):; pass. class C(A):; pass. class D():; pass. class E(D):; pass; ```. </details>. that defines a lattice, which can be represented as a DAG like this:. ```; Any; / \; A D; / \ |; B C E; \ | /; Union[]; ```. It's partially ordered in that you can't say A contains E or vice-versa, but you can say things like A is contains B, and `Any` is a supertype of (contains) everything else. I think that how you're viewing it is pretty close, except the elements are types instead of their properties. My mental model has types being a collection of properties, and being a subtype means an object inherits it's supertypes properti",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-444715545
https://github.com/scverse/scanpy/issues/373#issuecomment-444715545:1968,Modifiability,inherit,inherits,1968,"ty sure the logical conclusion of any long discussion about types is that everything should be done in Haskell. I don't like the use of branches with `isinstance` because it breaks polymorphism, which is a key part of pythonic code to me. @falexwolf, I completely agree with you on ""what makes a good docstring"". The knowledge overhead for numeric python doesn't include type theory, so the docs should be interpretable without them. Ideally, interfaces are simple and the documentation makes the expected behavior clear. I'm still not sure I totally understand what the intent of the ""type"" vs. ""class"" system is in python, so I'm often a little unsure what to do with heavily typed code. That said, if expected behaviors could be encapsulated (both formally and intuitively) with some abstract types (representing interfaces or traits) that would be a nicer solution. I don't think we're near that point in python. ## Lattices. Sorry about not giving some info on lattices, I'd thought you didn't want to get into it. It's the [partially ordered set kind](https://en.wikipedia.org/wiki/Lattice_(order)) of lattice, where each type is an element or subset. I'll give a short python based example (ignoring that `Union[]` can't be instantiated). <details>. <summary>The code:</summary>. ```python; from typing import Any, Union. class A():; pass. class B(A):; pass. class C(A):; pass. class D():; pass. class E(D):; pass; ```. </details>. that defines a lattice, which can be represented as a DAG like this:. ```; Any; / \; A D; / \ |; B C E; \ | /; Union[]; ```. It's partially ordered in that you can't say A contains E or vice-versa, but you can say things like A is contains B, and `Any` is a supertype of (contains) everything else. I think that how you're viewing it is pretty close, except the elements are types instead of their properties. My mental model has types being a collection of properties, and being a subtype means an object inherits it's supertypes properties, and can have more.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-444715545
https://github.com/scverse/scanpy/issues/373#issuecomment-444715545:35,Testability,log,logical,35,"@flying-sheep, I'm pretty sure the logical conclusion of any long discussion about types is that everything should be done in Haskell. I don't like the use of branches with `isinstance` because it breaks polymorphism, which is a key part of pythonic code to me. @falexwolf, I completely agree with you on ""what makes a good docstring"". The knowledge overhead for numeric python doesn't include type theory, so the docs should be interpretable without them. Ideally, interfaces are simple and the documentation makes the expected behavior clear. I'm still not sure I totally understand what the intent of the ""type"" vs. ""class"" system is in python, so I'm often a little unsure what to do with heavily typed code. That said, if expected behaviors could be encapsulated (both formally and intuitively) with some abstract types (representing interfaces or traits) that would be a nicer solution. I don't think we're near that point in python. ## Lattices. Sorry about not giving some info on lattices, I'd thought you didn't want to get into it. It's the [partially ordered set kind](https://en.wikipedia.org/wiki/Lattice_(order)) of lattice, where each type is an element or subset. I'll give a short python based example (ignoring that `Union[]` can't be instantiated). <details>. <summary>The code:</summary>. ```python; from typing import Any, Union. class A():; pass. class B(A):; pass. class C(A):; pass. class D():; pass. class E(D):; pass; ```. </details>. that defines a lattice, which can be represented as a DAG like this:. ```; Any; / \; A D; / \ |; B C E; \ | /; Union[]; ```. It's partially ordered in that you can't say A contains E or vice-versa, but you can say things like A is contains B, and `Any` is a supertype of (contains) everything else. I think that how you're viewing it is pretty close, except the elements are types instead of their properties. My mental model has types being a collection of properties, and being a subtype means an object inherits it's supertypes properti",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-444715545
https://github.com/scverse/scanpy/issues/373#issuecomment-444715545:481,Usability,simpl,simple,481,"@flying-sheep, I'm pretty sure the logical conclusion of any long discussion about types is that everything should be done in Haskell. I don't like the use of branches with `isinstance` because it breaks polymorphism, which is a key part of pythonic code to me. @falexwolf, I completely agree with you on ""what makes a good docstring"". The knowledge overhead for numeric python doesn't include type theory, so the docs should be interpretable without them. Ideally, interfaces are simple and the documentation makes the expected behavior clear. I'm still not sure I totally understand what the intent of the ""type"" vs. ""class"" system is in python, so I'm often a little unsure what to do with heavily typed code. That said, if expected behaviors could be encapsulated (both formally and intuitively) with some abstract types (representing interfaces or traits) that would be a nicer solution. I don't think we're near that point in python. ## Lattices. Sorry about not giving some info on lattices, I'd thought you didn't want to get into it. It's the [partially ordered set kind](https://en.wikipedia.org/wiki/Lattice_(order)) of lattice, where each type is an element or subset. I'll give a short python based example (ignoring that `Union[]` can't be instantiated). <details>. <summary>The code:</summary>. ```python; from typing import Any, Union. class A():; pass. class B(A):; pass. class C(A):; pass. class D():; pass. class E(D):; pass; ```. </details>. that defines a lattice, which can be represented as a DAG like this:. ```; Any; / \; A D; / \ |; B C E; \ | /; Union[]; ```. It's partially ordered in that you can't say A contains E or vice-versa, but you can say things like A is contains B, and `Any` is a supertype of (contains) everything else. I think that how you're viewing it is pretty close, except the elements are types instead of their properties. My mental model has types being a collection of properties, and being a subtype means an object inherits it's supertypes properti",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-444715545
https://github.com/scverse/scanpy/issues/373#issuecomment-444715545:538,Usability,clear,clear,538,"@flying-sheep, I'm pretty sure the logical conclusion of any long discussion about types is that everything should be done in Haskell. I don't like the use of branches with `isinstance` because it breaks polymorphism, which is a key part of pythonic code to me. @falexwolf, I completely agree with you on ""what makes a good docstring"". The knowledge overhead for numeric python doesn't include type theory, so the docs should be interpretable without them. Ideally, interfaces are simple and the documentation makes the expected behavior clear. I'm still not sure I totally understand what the intent of the ""type"" vs. ""class"" system is in python, so I'm often a little unsure what to do with heavily typed code. That said, if expected behaviors could be encapsulated (both formally and intuitively) with some abstract types (representing interfaces or traits) that would be a nicer solution. I don't think we're near that point in python. ## Lattices. Sorry about not giving some info on lattices, I'd thought you didn't want to get into it. It's the [partially ordered set kind](https://en.wikipedia.org/wiki/Lattice_(order)) of lattice, where each type is an element or subset. I'll give a short python based example (ignoring that `Union[]` can't be instantiated). <details>. <summary>The code:</summary>. ```python; from typing import Any, Union. class A():; pass. class B(A):; pass. class C(A):; pass. class D():; pass. class E(D):; pass; ```. </details>. that defines a lattice, which can be represented as a DAG like this:. ```; Any; / \; A D; / \ |; B C E; \ | /; Union[]; ```. It's partially ordered in that you can't say A contains E or vice-versa, but you can say things like A is contains B, and `Any` is a supertype of (contains) everything else. I think that how you're viewing it is pretty close, except the elements are types instead of their properties. My mental model has types being a collection of properties, and being a subtype means an object inherits it's supertypes properti",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-444715545
https://github.com/scverse/scanpy/issues/373#issuecomment-444715545:787,Usability,intuit,intuitively,787,"@flying-sheep, I'm pretty sure the logical conclusion of any long discussion about types is that everything should be done in Haskell. I don't like the use of branches with `isinstance` because it breaks polymorphism, which is a key part of pythonic code to me. @falexwolf, I completely agree with you on ""what makes a good docstring"". The knowledge overhead for numeric python doesn't include type theory, so the docs should be interpretable without them. Ideally, interfaces are simple and the documentation makes the expected behavior clear. I'm still not sure I totally understand what the intent of the ""type"" vs. ""class"" system is in python, so I'm often a little unsure what to do with heavily typed code. That said, if expected behaviors could be encapsulated (both formally and intuitively) with some abstract types (representing interfaces or traits) that would be a nicer solution. I don't think we're near that point in python. ## Lattices. Sorry about not giving some info on lattices, I'd thought you didn't want to get into it. It's the [partially ordered set kind](https://en.wikipedia.org/wiki/Lattice_(order)) of lattice, where each type is an element or subset. I'll give a short python based example (ignoring that `Union[]` can't be instantiated). <details>. <summary>The code:</summary>. ```python; from typing import Any, Union. class A():; pass. class B(A):; pass. class C(A):; pass. class D():; pass. class E(D):; pass; ```. </details>. that defines a lattice, which can be represented as a DAG like this:. ```; Any; / \; A D; / \ |; B C E; \ | /; Union[]; ```. It's partially ordered in that you can't say A contains E or vice-versa, but you can say things like A is contains B, and `Any` is a supertype of (contains) everything else. I think that how you're viewing it is pretty close, except the elements are types instead of their properties. My mental model has types being a collection of properties, and being a subtype means an object inherits it's supertypes properti",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-444715545
https://github.com/scverse/scanpy/issues/373#issuecomment-444816086:240,Integrability,protocol,protocol,240,"> I don't like the use of branches with `isinstance` because it breaks polymorphism, which is a key part of pythonic code to me. That’s what ABCs are for. `isinstance(thing, Mapping)` works beautifully for everything that has the `Mapping` protocol, no matter if it’s a `dict` or not.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-444816086
https://github.com/scverse/scanpy/issues/373#issuecomment-444816086:71,Modifiability,polymorphi,polymorphism,71,"> I don't like the use of branches with `isinstance` because it breaks polymorphism, which is a key part of pythonic code to me. That’s what ABCs are for. `isinstance(thing, Mapping)` works beautifully for everything that has the `Mapping` protocol, no matter if it’s a `dict` or not.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-444816086
https://github.com/scverse/scanpy/issues/373#issuecomment-445102460:293,Availability,error,error,293,"Are you talking about the `collections.abc.Mapping` in this case? [From the 3.7 docs](; https://docs.python.org/3/library/typing.html#classes-functions-and-decorators):. > In general, `isinstance()` and `issubclass()` should not be used with types. Additionally, those functions just throw an error for subscripted generics, so you definitely can't do `isinstance(m, Mapping[str, int])`. I don't think I'm totally clear on the differences in intended use cases for `ABC`s vs `Type`s. Are types only for annotation? Should ABCs not be used for annotations?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-445102460
https://github.com/scverse/scanpy/issues/373#issuecomment-445102460:414,Usability,clear,clear,414,"Are you talking about the `collections.abc.Mapping` in this case? [From the 3.7 docs](; https://docs.python.org/3/library/typing.html#classes-functions-and-decorators):. > In general, `isinstance()` and `issubclass()` should not be used with types. Additionally, those functions just throw an error for subscripted generics, so you definitely can't do `isinstance(m, Mapping[str, int])`. I don't think I'm totally clear on the differences in intended use cases for `ABC`s vs `Type`s. Are types only for annotation? Should ABCs not be used for annotations?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-445102460
https://github.com/scverse/scanpy/issues/373#issuecomment-445181839:743,Integrability,protocol,protocol,743,"I only just now got the distinction between types and classes in python. So when they talk about “types”, they mean stuff in the typing module, got it. So. - Types (`typing.*`), ABCs, and regular classes can be used for type annotation; - ABCs and regular classes can be used for `isinstance` and `issubclass` checking; - ABCs and mixins can be mixed in to enhance a class you defined. where a mixin is simply a regular class that happens to rely on some properties of the class it can be mixed with, and a regular class being any class that’s not a type or an ABC. - `collections.abc.Mapping` is an ABC and can be mixed in to enhance your basic mapping class with some convenience methods, or used to check if something has the basic mapping protocol (no matter if it was mixed in or not). What’s the basic protocol and what will be mixed in is [nicely documented](https://docs.python.org/3/library/collections.abc.html).; - `typing.Mapping` is a generic type, to be used in annotations only. There’s a few projects implementing type checking using them, e.g. mypy or typecheck-decorator. Check out the [docs for abstract base classes](https://docs.python.org/3/library/abc.html), they explain how ABCs work. (namely by `register`ing virtual subclasses and/or implementing `__subclasshook__`). Mixin example:. ```py; class EnumerableMixin:; """"""silly mixin class for iterables""""""; def enumerate(self, start=0):; yield from enumerate(self, start). class EnumerableList(list, EnumerableMixin):; pass. for i, e in EnumerableList.enumerate(): print(i, e); ```. ABC example:. ```py; class PositiveNumbers(collections.abc.Set):; def __contains__(self, i):; return isinstance(i, int) and i >= 0; def __iter__(self): return itertools.count(); def __len__(self): return float('inf'). # __lt__ is mixed in!; print({0, 1, 10_000} < PositiveNumbers()). # `set` doesn’t inherit from collections.abc.Set, the __subclasshook__ does its magic here; isinstance({}, collections.abc.Set); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-445181839
https://github.com/scverse/scanpy/issues/373#issuecomment-445181839:808,Integrability,protocol,protocol,808,"I only just now got the distinction between types and classes in python. So when they talk about “types”, they mean stuff in the typing module, got it. So. - Types (`typing.*`), ABCs, and regular classes can be used for type annotation; - ABCs and regular classes can be used for `isinstance` and `issubclass` checking; - ABCs and mixins can be mixed in to enhance a class you defined. where a mixin is simply a regular class that happens to rely on some properties of the class it can be mixed with, and a regular class being any class that’s not a type or an ABC. - `collections.abc.Mapping` is an ABC and can be mixed in to enhance your basic mapping class with some convenience methods, or used to check if something has the basic mapping protocol (no matter if it was mixed in or not). What’s the basic protocol and what will be mixed in is [nicely documented](https://docs.python.org/3/library/collections.abc.html).; - `typing.Mapping` is a generic type, to be used in annotations only. There’s a few projects implementing type checking using them, e.g. mypy or typecheck-decorator. Check out the [docs for abstract base classes](https://docs.python.org/3/library/abc.html), they explain how ABCs work. (namely by `register`ing virtual subclasses and/or implementing `__subclasshook__`). Mixin example:. ```py; class EnumerableMixin:; """"""silly mixin class for iterables""""""; def enumerate(self, start=0):; yield from enumerate(self, start). class EnumerableList(list, EnumerableMixin):; pass. for i, e in EnumerableList.enumerate(): print(i, e); ```. ABC example:. ```py; class PositiveNumbers(collections.abc.Set):; def __contains__(self, i):; return isinstance(i, int) and i >= 0; def __iter__(self): return itertools.count(); def __len__(self): return float('inf'). # __lt__ is mixed in!; print({0, 1, 10_000} < PositiveNumbers()). # `set` doesn’t inherit from collections.abc.Set, the __subclasshook__ does its magic here; isinstance({}, collections.abc.Set); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-445181839
https://github.com/scverse/scanpy/issues/373#issuecomment-445181839:357,Modifiability,enhance,enhance,357,"I only just now got the distinction between types and classes in python. So when they talk about “types”, they mean stuff in the typing module, got it. So. - Types (`typing.*`), ABCs, and regular classes can be used for type annotation; - ABCs and regular classes can be used for `isinstance` and `issubclass` checking; - ABCs and mixins can be mixed in to enhance a class you defined. where a mixin is simply a regular class that happens to rely on some properties of the class it can be mixed with, and a regular class being any class that’s not a type or an ABC. - `collections.abc.Mapping` is an ABC and can be mixed in to enhance your basic mapping class with some convenience methods, or used to check if something has the basic mapping protocol (no matter if it was mixed in or not). What’s the basic protocol and what will be mixed in is [nicely documented](https://docs.python.org/3/library/collections.abc.html).; - `typing.Mapping` is a generic type, to be used in annotations only. There’s a few projects implementing type checking using them, e.g. mypy or typecheck-decorator. Check out the [docs for abstract base classes](https://docs.python.org/3/library/abc.html), they explain how ABCs work. (namely by `register`ing virtual subclasses and/or implementing `__subclasshook__`). Mixin example:. ```py; class EnumerableMixin:; """"""silly mixin class for iterables""""""; def enumerate(self, start=0):; yield from enumerate(self, start). class EnumerableList(list, EnumerableMixin):; pass. for i, e in EnumerableList.enumerate(): print(i, e); ```. ABC example:. ```py; class PositiveNumbers(collections.abc.Set):; def __contains__(self, i):; return isinstance(i, int) and i >= 0; def __iter__(self): return itertools.count(); def __len__(self): return float('inf'). # __lt__ is mixed in!; print({0, 1, 10_000} < PositiveNumbers()). # `set` doesn’t inherit from collections.abc.Set, the __subclasshook__ does its magic here; isinstance({}, collections.abc.Set); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-445181839
https://github.com/scverse/scanpy/issues/373#issuecomment-445181839:627,Modifiability,enhance,enhance,627,"I only just now got the distinction between types and classes in python. So when they talk about “types”, they mean stuff in the typing module, got it. So. - Types (`typing.*`), ABCs, and regular classes can be used for type annotation; - ABCs and regular classes can be used for `isinstance` and `issubclass` checking; - ABCs and mixins can be mixed in to enhance a class you defined. where a mixin is simply a regular class that happens to rely on some properties of the class it can be mixed with, and a regular class being any class that’s not a type or an ABC. - `collections.abc.Mapping` is an ABC and can be mixed in to enhance your basic mapping class with some convenience methods, or used to check if something has the basic mapping protocol (no matter if it was mixed in or not). What’s the basic protocol and what will be mixed in is [nicely documented](https://docs.python.org/3/library/collections.abc.html).; - `typing.Mapping` is a generic type, to be used in annotations only. There’s a few projects implementing type checking using them, e.g. mypy or typecheck-decorator. Check out the [docs for abstract base classes](https://docs.python.org/3/library/abc.html), they explain how ABCs work. (namely by `register`ing virtual subclasses and/or implementing `__subclasshook__`). Mixin example:. ```py; class EnumerableMixin:; """"""silly mixin class for iterables""""""; def enumerate(self, start=0):; yield from enumerate(self, start). class EnumerableList(list, EnumerableMixin):; pass. for i, e in EnumerableList.enumerate(): print(i, e); ```. ABC example:. ```py; class PositiveNumbers(collections.abc.Set):; def __contains__(self, i):; return isinstance(i, int) and i >= 0; def __iter__(self): return itertools.count(); def __len__(self): return float('inf'). # __lt__ is mixed in!; print({0, 1, 10_000} < PositiveNumbers()). # `set` doesn’t inherit from collections.abc.Set, the __subclasshook__ does its magic here; isinstance({}, collections.abc.Set); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-445181839
https://github.com/scverse/scanpy/issues/373#issuecomment-445181839:1857,Modifiability,inherit,inherit,1857,"I only just now got the distinction between types and classes in python. So when they talk about “types”, they mean stuff in the typing module, got it. So. - Types (`typing.*`), ABCs, and regular classes can be used for type annotation; - ABCs and regular classes can be used for `isinstance` and `issubclass` checking; - ABCs and mixins can be mixed in to enhance a class you defined. where a mixin is simply a regular class that happens to rely on some properties of the class it can be mixed with, and a regular class being any class that’s not a type or an ABC. - `collections.abc.Mapping` is an ABC and can be mixed in to enhance your basic mapping class with some convenience methods, or used to check if something has the basic mapping protocol (no matter if it was mixed in or not). What’s the basic protocol and what will be mixed in is [nicely documented](https://docs.python.org/3/library/collections.abc.html).; - `typing.Mapping` is a generic type, to be used in annotations only. There’s a few projects implementing type checking using them, e.g. mypy or typecheck-decorator. Check out the [docs for abstract base classes](https://docs.python.org/3/library/abc.html), they explain how ABCs work. (namely by `register`ing virtual subclasses and/or implementing `__subclasshook__`). Mixin example:. ```py; class EnumerableMixin:; """"""silly mixin class for iterables""""""; def enumerate(self, start=0):; yield from enumerate(self, start). class EnumerableList(list, EnumerableMixin):; pass. for i, e in EnumerableList.enumerate(): print(i, e); ```. ABC example:. ```py; class PositiveNumbers(collections.abc.Set):; def __contains__(self, i):; return isinstance(i, int) and i >= 0; def __iter__(self): return itertools.count(); def __len__(self): return float('inf'). # __lt__ is mixed in!; print({0, 1, 10_000} < PositiveNumbers()). # `set` doesn’t inherit from collections.abc.Set, the __subclasshook__ does its magic here; isinstance({}, collections.abc.Set); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-445181839
https://github.com/scverse/scanpy/issues/373#issuecomment-445181839:403,Usability,simpl,simply,403,"I only just now got the distinction between types and classes in python. So when they talk about “types”, they mean stuff in the typing module, got it. So. - Types (`typing.*`), ABCs, and regular classes can be used for type annotation; - ABCs and regular classes can be used for `isinstance` and `issubclass` checking; - ABCs and mixins can be mixed in to enhance a class you defined. where a mixin is simply a regular class that happens to rely on some properties of the class it can be mixed with, and a regular class being any class that’s not a type or an ABC. - `collections.abc.Mapping` is an ABC and can be mixed in to enhance your basic mapping class with some convenience methods, or used to check if something has the basic mapping protocol (no matter if it was mixed in or not). What’s the basic protocol and what will be mixed in is [nicely documented](https://docs.python.org/3/library/collections.abc.html).; - `typing.Mapping` is a generic type, to be used in annotations only. There’s a few projects implementing type checking using them, e.g. mypy or typecheck-decorator. Check out the [docs for abstract base classes](https://docs.python.org/3/library/abc.html), they explain how ABCs work. (namely by `register`ing virtual subclasses and/or implementing `__subclasshook__`). Mixin example:. ```py; class EnumerableMixin:; """"""silly mixin class for iterables""""""; def enumerate(self, start=0):; yield from enumerate(self, start). class EnumerableList(list, EnumerableMixin):; pass. for i, e in EnumerableList.enumerate(): print(i, e); ```. ABC example:. ```py; class PositiveNumbers(collections.abc.Set):; def __contains__(self, i):; return isinstance(i, int) and i >= 0; def __iter__(self): return itertools.count(); def __len__(self): return float('inf'). # __lt__ is mixed in!; print({0, 1, 10_000} < PositiveNumbers()). # `set` doesn’t inherit from collections.abc.Set, the __subclasshook__ does its magic here; isinstance({}, collections.abc.Set); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-445181839
https://github.com/scverse/scanpy/issues/373#issuecomment-445609843:42,Integrability,interface,interfaces,42,"I've played around with ABCs for defining interfaces, but had thought the `typing` module was more closely tied with them. Do you understand the rationale for `typing` classes being separate when ABCs and regular classes can be used for annotation?. As far I can tell, it's for subscripted (parametric?) annotations. For many python classes those would have to be a runtime check – e.g. `List[int]` – which `ABC`s don't do. Not that `typing` does these checks, but it allows a way to express those constraints.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-445609843
https://github.com/scverse/scanpy/issues/373#issuecomment-445749064:57,Testability,test,test,57,"the runtime checks would be too costly or impossible. to test for `List[int]`, you’d have to traverse the whole thing and check every element. And for e.g. `Iterator[int]`, you just can’t test it at all – if you’d iterate the thing to check the objects it yields, you exhaust it and it’s no longer usable.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-445749064
https://github.com/scverse/scanpy/issues/373#issuecomment-445749064:188,Testability,test,test,188,"the runtime checks would be too costly or impossible. to test for `List[int]`, you’d have to traverse the whole thing and check every element. And for e.g. `Iterator[int]`, you just can’t test it at all – if you’d iterate the thing to check the objects it yields, you exhaust it and it’s no longer usable.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-445749064
https://github.com/scverse/scanpy/issues/373#issuecomment-445749064:298,Usability,usab,usable,298,"the runtime checks would be too costly or impossible. to test for `List[int]`, you’d have to traverse the whole thing and check every element. And for e.g. `Iterator[int]`, you just can’t test it at all – if you’d iterate the thing to check the objects it yields, you exhaust it and it’s no longer usable.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-445749064
https://github.com/scverse/scanpy/issues/374#issuecomment-440727446:142,Integrability,depend,dependency,142,"This is caused by an adjustment that tries to keep the legends closer to; the figure compared to the default placing. Clearly, there is some; dependency with the font size that I was not aware of. I will prepare a fix; soon. On Wed, Nov 21, 2018 at 3:23 PM Artemis-R <notifications@github.com> wrote:. > Here is a minimal example to recreate the issue I am describing:; >; > import scanpy.api as sc; > adata = sc.datasets.pbmc68k_reduced(); > sc.tl.pca(adata); > sc.pp.neighbors(adata); > sc.tl.umap(adata); >; > when you plot the umap of the bulk labels contained in adata.obs without; > specifying any further settings (i.e. sc.pl.umap(adata, color =; > ['bulk_labels']) ) everything looks fine.; >; > [image: image]; > <https://user-images.githubusercontent.com/15019107/48847064-efe34780-eda0-11e8-8d51-b503d7912d1e.png>; >; > But as soon as you try and adjust the legend font size (sc.pl.umap(adata,; > color = ['bulk_labels'], legend_fontsize = 4)) to a value that is smaller; > than the default font size it selects for your legend, the legend overlaps; > with the right edge of the plot.; >; > [image: image]; > <https://user-images.githubusercontent.com/15019107/48847096-04274480-eda1-11e8-9bac-dceb31aba155.png>; >; > For me this sometimes leads to issues that I can no longer export figures; > with my desired fontsize for presentations, etc. without it overlapping the; > plot in an ugly way.; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/374>, or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1VijUK8tuCXIowEtUo1I45iOhNd-ks5uxWHfgaJpZM4YtOB0>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/374#issuecomment-440727446
https://github.com/scverse/scanpy/issues/374#issuecomment-440727446:118,Usability,Clear,Clearly,118,"This is caused by an adjustment that tries to keep the legends closer to; the figure compared to the default placing. Clearly, there is some; dependency with the font size that I was not aware of. I will prepare a fix; soon. On Wed, Nov 21, 2018 at 3:23 PM Artemis-R <notifications@github.com> wrote:. > Here is a minimal example to recreate the issue I am describing:; >; > import scanpy.api as sc; > adata = sc.datasets.pbmc68k_reduced(); > sc.tl.pca(adata); > sc.pp.neighbors(adata); > sc.tl.umap(adata); >; > when you plot the umap of the bulk labels contained in adata.obs without; > specifying any further settings (i.e. sc.pl.umap(adata, color =; > ['bulk_labels']) ) everything looks fine.; >; > [image: image]; > <https://user-images.githubusercontent.com/15019107/48847064-efe34780-eda0-11e8-8d51-b503d7912d1e.png>; >; > But as soon as you try and adjust the legend font size (sc.pl.umap(adata,; > color = ['bulk_labels'], legend_fontsize = 4)) to a value that is smaller; > than the default font size it selects for your legend, the legend overlaps; > with the right edge of the plot.; >; > [image: image]; > <https://user-images.githubusercontent.com/15019107/48847096-04274480-eda1-11e8-9bac-dceb31aba155.png>; >; > For me this sometimes leads to issues that I can no longer export figures; > with my desired fontsize for presentations, etc. without it overlapping the; > plot in an ugly way.; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/374>, or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1VijUK8tuCXIowEtUo1I45iOhNd-ks5uxWHfgaJpZM4YtOB0>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/374#issuecomment-440727446
https://github.com/scverse/scanpy/issues/375#issuecomment-441056129:307,Modifiability,variab,variables,307,"Of course! would be wild if the plotting would internally transpose the anndata object in case one of the provided `keys` exists in `.var`. `sc.pl.violin(adata.T, 'key')` is 100% the right thing to do. I think the docs are a bit improvable though:. > *keys* : str or list of str; > &emsp;Keys for accessing variables of .var_names or fields of .obs. The mention of `var_names` here means that you can select one or more genes to plot. How can we phrase that better? Maybe we should also add an example that uses transposing.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375#issuecomment-441056129
https://github.com/scverse/scanpy/issues/375#issuecomment-441056129:297,Security,access,accessing,297,"Of course! would be wild if the plotting would internally transpose the anndata object in case one of the provided `keys` exists in `.var`. `sc.pl.violin(adata.T, 'key')` is 100% the right thing to do. I think the docs are a bit improvable though:. > *keys* : str or list of str; > &emsp;Keys for accessing variables of .var_names or fields of .obs. The mention of `var_names` here means that you can select one or more genes to plot. How can we phrase that better? Maybe we should also add an example that uses transposing.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375#issuecomment-441056129
https://github.com/scverse/scanpy/issues/375#issuecomment-441214996:18,Availability,error,error,18,"ouch, it’s pretty error prone to just guess! What if a column is in both `.var` and `.obs`? People will never figure out what they need to do in order to get what they want. I don’t like replicating that or that it ever went into any function. Explicit is better than implicit. We could throw a nice error if the column isn’t in `.obs` but is in `.var` instead, like. > You specified column “dropout_per_gene” which is not in `.obs`, but in `.var`. Did you mean to call `sc.pl.violin(adata.T, ...)`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375#issuecomment-441214996
https://github.com/scverse/scanpy/issues/375#issuecomment-441214996:300,Availability,error,error,300,"ouch, it’s pretty error prone to just guess! What if a column is in both `.var` and `.obs`? People will never figure out what they need to do in order to get what they want. I don’t like replicating that or that it ever went into any function. Explicit is better than implicit. We could throw a nice error if the column isn’t in `.obs` but is in `.var` instead, like. > You specified column “dropout_per_gene” which is not in `.obs`, but in `.var`. Did you mean to call `sc.pl.violin(adata.T, ...)`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375#issuecomment-441214996
https://github.com/scverse/scanpy/issues/375#issuecomment-441244349:401,Usability,simpl,simpler,401,"you wanted to say “implicit” right?. and I disagree, transposing here is exactly the right thing to do: when you want to use your genes as observations in a plot you should transpose your AnnData so that they are the dimension made for observations!. also, there’s no “technical restriction”. it’s about API design; we could also introduce a `switch_var_and_obs = False` parameter, but i feel `.T` is simpler.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375#issuecomment-441244349
https://github.com/scverse/scanpy/issues/375#issuecomment-441253483:350,Modifiability,variab,variables,350,"No, I meant an argument like `over='cells'` that would default to `.obs` covariates. So explicitly telling the function. Implicit would be nice, but you pointed out that it would require guessing, which has its own issues. I'm on the fence about your solution of doing neither and requiring the user to use `adata.T`. I do see your rationale behind 'variables' and 'observations' though. I'm just not entirely sure that is clear to the user in the same way it is clear to the developer. As a user I see cells and genes in my dataset and may not be aware that one of them are treated as the variables that describe the other. Then the question is: do you want to be as user-friendly as possible (I'll call it 'the R way') or stick with consistent conventions that may not be clear to everyone ('the numpy way'?). Both can cause frustrations and both have benefits.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375#issuecomment-441253483
https://github.com/scverse/scanpy/issues/375#issuecomment-441253483:590,Modifiability,variab,variables,590,"No, I meant an argument like `over='cells'` that would default to `.obs` covariates. So explicitly telling the function. Implicit would be nice, but you pointed out that it would require guessing, which has its own issues. I'm on the fence about your solution of doing neither and requiring the user to use `adata.T`. I do see your rationale behind 'variables' and 'observations' though. I'm just not entirely sure that is clear to the user in the same way it is clear to the developer. As a user I see cells and genes in my dataset and may not be aware that one of them are treated as the variables that describe the other. Then the question is: do you want to be as user-friendly as possible (I'll call it 'the R way') or stick with consistent conventions that may not be clear to everyone ('the numpy way'?). Both can cause frustrations and both have benefits.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375#issuecomment-441253483
https://github.com/scverse/scanpy/issues/375#issuecomment-441253483:423,Usability,clear,clear,423,"No, I meant an argument like `over='cells'` that would default to `.obs` covariates. So explicitly telling the function. Implicit would be nice, but you pointed out that it would require guessing, which has its own issues. I'm on the fence about your solution of doing neither and requiring the user to use `adata.T`. I do see your rationale behind 'variables' and 'observations' though. I'm just not entirely sure that is clear to the user in the same way it is clear to the developer. As a user I see cells and genes in my dataset and may not be aware that one of them are treated as the variables that describe the other. Then the question is: do you want to be as user-friendly as possible (I'll call it 'the R way') or stick with consistent conventions that may not be clear to everyone ('the numpy way'?). Both can cause frustrations and both have benefits.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375#issuecomment-441253483
https://github.com/scverse/scanpy/issues/375#issuecomment-441253483:463,Usability,clear,clear,463,"No, I meant an argument like `over='cells'` that would default to `.obs` covariates. So explicitly telling the function. Implicit would be nice, but you pointed out that it would require guessing, which has its own issues. I'm on the fence about your solution of doing neither and requiring the user to use `adata.T`. I do see your rationale behind 'variables' and 'observations' though. I'm just not entirely sure that is clear to the user in the same way it is clear to the developer. As a user I see cells and genes in my dataset and may not be aware that one of them are treated as the variables that describe the other. Then the question is: do you want to be as user-friendly as possible (I'll call it 'the R way') or stick with consistent conventions that may not be clear to everyone ('the numpy way'?). Both can cause frustrations and both have benefits.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375#issuecomment-441253483
https://github.com/scverse/scanpy/issues/375#issuecomment-441253483:668,Usability,user-friendly,user-friendly,668,"No, I meant an argument like `over='cells'` that would default to `.obs` covariates. So explicitly telling the function. Implicit would be nice, but you pointed out that it would require guessing, which has its own issues. I'm on the fence about your solution of doing neither and requiring the user to use `adata.T`. I do see your rationale behind 'variables' and 'observations' though. I'm just not entirely sure that is clear to the user in the same way it is clear to the developer. As a user I see cells and genes in my dataset and may not be aware that one of them are treated as the variables that describe the other. Then the question is: do you want to be as user-friendly as possible (I'll call it 'the R way') or stick with consistent conventions that may not be clear to everyone ('the numpy way'?). Both can cause frustrations and both have benefits.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375#issuecomment-441253483
https://github.com/scverse/scanpy/issues/375#issuecomment-441253483:774,Usability,clear,clear,774,"No, I meant an argument like `over='cells'` that would default to `.obs` covariates. So explicitly telling the function. Implicit would be nice, but you pointed out that it would require guessing, which has its own issues. I'm on the fence about your solution of doing neither and requiring the user to use `adata.T`. I do see your rationale behind 'variables' and 'observations' though. I'm just not entirely sure that is clear to the user in the same way it is clear to the developer. As a user I see cells and genes in my dataset and may not be aware that one of them are treated as the variables that describe the other. Then the question is: do you want to be as user-friendly as possible (I'll call it 'the R way') or stick with consistent conventions that may not be clear to everyone ('the numpy way'?). Both can cause frustrations and both have benefits.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375#issuecomment-441253483
https://github.com/scverse/scanpy/issues/375#issuecomment-441263484:102,Availability,error,error,102,"what if i told you that we can have our cake and eat it too? as said before:. > We could throw a nice error if the column isn’t in `.obs` but is in `.var` instead, like; > ; > > You specified column “dropout_per_gene” which is not in `.obs`, but in `.var`. Did you mean to call `sc.pl.violin(adata.T, ...)`?. Near zero frustration, because people can just do what the error tells them.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375#issuecomment-441263484
https://github.com/scverse/scanpy/issues/375#issuecomment-441263484:368,Availability,error,error,368,"what if i told you that we can have our cake and eat it too? as said before:. > We could throw a nice error if the column isn’t in `.obs` but is in `.var` instead, like; > ; > > You specified column “dropout_per_gene” which is not in `.obs`, but in `.var`. Did you mean to call `sc.pl.violin(adata.T, ...)`?. Near zero frustration, because people can just do what the error tells them.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375#issuecomment-441263484
https://github.com/scverse/scanpy/issues/375#issuecomment-441473742:701,Availability,redundant,redundant,701,"I like @flying-sheep's very last solution. To enable this for truly large-scale data and AnnData's that are backed on disk we need a much more efficient transposition implementation, which will probably need to return a view. That's problematic as it will break backwards compat (`.T` returns a copy these days). But it's good as it will allow adding fields to `.var`. @LuckyMD: At the time, when you mentioned that you wanted to plot over genes in scatter, I was fine with with having the scatter wrapper and assuming no ambiguity in obs and var keys. Now, I'd advocate for @flying-sheep's solution. Of course, we'll maintain the feature in `pl.scatter` when refactoring its code (a lot of it became redundant after fidel introduced the completely rewritten scatter plots).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375#issuecomment-441473742
https://github.com/scverse/scanpy/issues/375#issuecomment-441473742:143,Energy Efficiency,efficient,efficient,143,"I like @flying-sheep's very last solution. To enable this for truly large-scale data and AnnData's that are backed on disk we need a much more efficient transposition implementation, which will probably need to return a view. That's problematic as it will break backwards compat (`.T` returns a copy these days). But it's good as it will allow adding fields to `.var`. @LuckyMD: At the time, when you mentioned that you wanted to plot over genes in scatter, I was fine with with having the scatter wrapper and assuming no ambiguity in obs and var keys. Now, I'd advocate for @flying-sheep's solution. Of course, we'll maintain the feature in `pl.scatter` when refactoring its code (a lot of it became redundant after fidel introduced the completely rewritten scatter plots).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375#issuecomment-441473742
https://github.com/scverse/scanpy/issues/375#issuecomment-441473742:498,Integrability,wrap,wrapper,498,"I like @flying-sheep's very last solution. To enable this for truly large-scale data and AnnData's that are backed on disk we need a much more efficient transposition implementation, which will probably need to return a view. That's problematic as it will break backwards compat (`.T` returns a copy these days). But it's good as it will allow adding fields to `.var`. @LuckyMD: At the time, when you mentioned that you wanted to plot over genes in scatter, I was fine with with having the scatter wrapper and assuming no ambiguity in obs and var keys. Now, I'd advocate for @flying-sheep's solution. Of course, we'll maintain the feature in `pl.scatter` when refactoring its code (a lot of it became redundant after fidel introduced the completely rewritten scatter plots).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375#issuecomment-441473742
https://github.com/scverse/scanpy/issues/375#issuecomment-441473742:660,Modifiability,refactor,refactoring,660,"I like @flying-sheep's very last solution. To enable this for truly large-scale data and AnnData's that are backed on disk we need a much more efficient transposition implementation, which will probably need to return a view. That's problematic as it will break backwards compat (`.T` returns a copy these days). But it's good as it will allow adding fields to `.var`. @LuckyMD: At the time, when you mentioned that you wanted to plot over genes in scatter, I was fine with with having the scatter wrapper and assuming no ambiguity in obs and var keys. Now, I'd advocate for @flying-sheep's solution. Of course, we'll maintain the feature in `pl.scatter` when refactoring its code (a lot of it became redundant after fidel introduced the completely rewritten scatter plots).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375#issuecomment-441473742
https://github.com/scverse/scanpy/issues/375#issuecomment-441473742:701,Safety,redund,redundant,701,"I like @flying-sheep's very last solution. To enable this for truly large-scale data and AnnData's that are backed on disk we need a much more efficient transposition implementation, which will probably need to return a view. That's problematic as it will break backwards compat (`.T` returns a copy these days). But it's good as it will allow adding fields to `.var`. @LuckyMD: At the time, when you mentioned that you wanted to plot over genes in scatter, I was fine with with having the scatter wrapper and assuming no ambiguity in obs and var keys. Now, I'd advocate for @flying-sheep's solution. Of course, we'll maintain the feature in `pl.scatter` when refactoring its code (a lot of it became redundant after fidel introduced the completely rewritten scatter plots).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375#issuecomment-441473742
https://github.com/scverse/scanpy/pull/376#issuecomment-441008995:761,Availability,Error,Error,761,"@falexwolf:. It was a mistake to use optional positional parameters for our APIs. The core python team designs all its APIs using almost exclusively keyword-only parameters, because this way, parameters can be added without having to go to the end. (And they’re *much* more conservative and thoughtful in adding parameters in the first place). E.g. in this case there’s only one central important parameter apart from `adata`, so a `*` should be after `n_top`, and @grst could add his parameter before `ax`, leaving `ax` at the end of the pameter list without breaking anything:. ```py; def highest_expr_genes(; adata: AnnData,; n_top: int = 30,; *,; show: Optional[bool] = None,; ...; ax: Optional[Axes] = None,; **kwds,; ). highest_expr_genes(ad, 12, True) # Error: show is a keyword-only param; highest_expr_genes(ad, 12, show=True) # Works; ```. I have an idea how to fix this without breaking everything: I think I can build a decorator that allows people to use parameters positionally, but sends a DeprecationWarning when they do, while the docs only report the new signature.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376#issuecomment-441008995
https://github.com/scverse/scanpy/pull/376#issuecomment-441017256:173,Availability,avail,available,173,"Thanks for the PR. . One concern that I have is that similar solutions do not exists for other plotting functions. For coherence, ideally the `annot_col` argument should be available for other cases. Thus, I think that a better and more generic approach would be to simply modify your genes names in the `AnnData` object and let all plotting functions use those names. For this, you simply do:. ```PYTHON; adata.var = adata.var.reset_index().set_index(annot_col); # adata.var_names is automatically updated; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376#issuecomment-441017256
https://github.com/scverse/scanpy/pull/376#issuecomment-441017256:499,Deployability,update,updated,499,"Thanks for the PR. . One concern that I have is that similar solutions do not exists for other plotting functions. For coherence, ideally the `annot_col` argument should be available for other cases. Thus, I think that a better and more generic approach would be to simply modify your genes names in the `AnnData` object and let all plotting functions use those names. For this, you simply do:. ```PYTHON; adata.var = adata.var.reset_index().set_index(annot_col); # adata.var_names is automatically updated; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376#issuecomment-441017256
https://github.com/scverse/scanpy/pull/376#issuecomment-441017256:266,Usability,simpl,simply,266,"Thanks for the PR. . One concern that I have is that similar solutions do not exists for other plotting functions. For coherence, ideally the `annot_col` argument should be available for other cases. Thus, I think that a better and more generic approach would be to simply modify your genes names in the `AnnData` object and let all plotting functions use those names. For this, you simply do:. ```PYTHON; adata.var = adata.var.reset_index().set_index(annot_col); # adata.var_names is automatically updated; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376#issuecomment-441017256
https://github.com/scverse/scanpy/pull/376#issuecomment-441017256:383,Usability,simpl,simply,383,"Thanks for the PR. . One concern that I have is that similar solutions do not exists for other plotting functions. For coherence, ideally the `annot_col` argument should be available for other cases. Thus, I think that a better and more generic approach would be to simply modify your genes names in the `AnnData` object and let all plotting functions use those names. For this, you simply do:. ```PYTHON; adata.var = adata.var.reset_index().set_index(annot_col); # adata.var_names is automatically updated; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376#issuecomment-441017256
https://github.com/scverse/scanpy/pull/376#issuecomment-441040160:483,Availability,avail,available,483,"@fidelram Actually, a similar solution does exist for other plotting functions. @falexwolf added the `gene_symbols` parameter to `sc.pl.rank_genes_groups()` and `sc.pl.rank_genes_groups_violin()` as I was having the same issue of using Ensembl gene IDs. I'm not sure if he regrets doing that now though or not... it's not consistently applied. In either case, Ensembl gene IDs are more specific than HGNC/MGI gene symbols. For some users it may be important that this specificity is available (and you don't keep having to make copies of the data to plot). Also, if alternative splicing is investigated with single-cell data, then you cannot keep gene symbols as `var_names` throughout your dataset as a default. So this is a future-proof approach.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376#issuecomment-441040160
https://github.com/scverse/scanpy/pull/376#issuecomment-441077219:398,Integrability,protocol,protocols,398,@flying-sheep I completely agree that it may not be the best naming convention. That was what it was used for at the time though. @grst The idea of using gene symbols as `.var_names` in scanpy was to make the software as user friendly as possible as far as I'm aware. Typically people care about the gene-level as the highest resolution. This is probably also due to the dominance of 3` enrichment protocols making it hard to detect signals at a higher resolution.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376#issuecomment-441077219
https://github.com/scverse/scanpy/pull/376#issuecomment-441077219:426,Safety,detect,detect,426,@flying-sheep I completely agree that it may not be the best naming convention. That was what it was used for at the time though. @grst The idea of using gene symbols as `.var_names` in scanpy was to make the software as user friendly as possible as far as I'm aware. Typically people care about the gene-level as the highest resolution. This is probably also due to the dominance of 3` enrichment protocols making it hard to detect signals at a higher resolution.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376#issuecomment-441077219
https://github.com/scverse/scanpy/pull/376#issuecomment-441598874:287,Modifiability,layers,layers,287,"> Let’s pay attention to not include any new function without * anymore, OK?. OK!. > Oh it’s definitely OK! I meant that it needs a bit of imagination to put other things in there than actual gene symbols, but I feel like it’s faster to see!. Yes, but I hardly imagine that you need two layers of annotations for variables outside the specific case of ENSids and gene symbols. So it's good to be verbose about this specific case and people wont miss such functionality in other cases.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376#issuecomment-441598874
https://github.com/scverse/scanpy/pull/376#issuecomment-441598874:313,Modifiability,variab,variables,313,"> Let’s pay attention to not include any new function without * anymore, OK?. OK!. > Oh it’s definitely OK! I meant that it needs a bit of imagination to put other things in there than actual gene symbols, but I feel like it’s faster to see!. Yes, but I hardly imagine that you need two layers of annotations for variables outside the specific case of ENSids and gene symbols. So it's good to be verbose about this specific case and people wont miss such functionality in other cases.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376#issuecomment-441598874
https://github.com/scverse/scanpy/issues/377#issuecomment-441092773:119,Availability,error,error,119,"@fidelram, just wanted to add that I still cannot use the ```use_raw=False``` parameter, for example I keep getting an error when try to visualise my data with ```sc.pl.rank_genes_groups_heatmap``` or ```sc.pl.rank_genes_groups_matrixplot```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/377#issuecomment-441092773
https://github.com/scverse/scanpy/issues/378#issuecomment-443073890:234,Usability,Simpl,Simply,234,The distance matrix you are passing might not be what sklearn wants: by densifying you'll get many zeros that sklearn probably assumes to be true zeros (it likely expects a dense distance matrix with all values actually be computed). Simply pass the data matrix `X_pca` to circumvent this.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/378#issuecomment-443073890
https://github.com/scverse/scanpy/pull/380#issuecomment-443101144:192,Deployability,pipeline,pipeline,192,"As an added note, it would be great to see this 'gene_symbol' argument used uniformly across the plotting functions. We've had to handle it in pretty hacky ways to make it work throughout the pipeline.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/380#issuecomment-443101144
https://github.com/scverse/scanpy/issues/381#issuecomment-443398454:320,Usability,clear,clear,320,"Did you recompute using `tl.paga` in between?. You're passing something from the 14 cluster calculation to the 4-cluster call, as is evident from ; > 'c' argument has 14 elements, which is not acceptable for use with 'x' with size 4, 'y' with size 4. It has nothing to do with anything in matplotlib. If the docs aren't clear enough, please let me know.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/381#issuecomment-443398454
https://github.com/scverse/scanpy/issues/381#issuecomment-456264112:273,Availability,error,error,273,"I think I find the reason. When run sc.tl.louvain(adata), louvain_colors will be saved in adata.uns, sc.pl.paga will use louvain_colors. But, when run sc.tl.louvain(adata) again with another resolution and then rerun sc.tl.paga, louvain_colors will not be updated, and the error occurs!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/381#issuecomment-456264112
https://github.com/scverse/scanpy/issues/381#issuecomment-456264112:256,Deployability,update,updated,256,"I think I find the reason. When run sc.tl.louvain(adata), louvain_colors will be saved in adata.uns, sc.pl.paga will use louvain_colors. But, when run sc.tl.louvain(adata) again with another resolution and then rerun sc.tl.paga, louvain_colors will not be updated, and the error occurs!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/381#issuecomment-456264112
https://github.com/scverse/scanpy/pull/382#issuecomment-443398154:114,Availability,avail,available,114,"Yes, you have the choice of either having 'gene_symbols' as your index or 'gene_ids', what is not in the index is available as a column in `.var`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/382#issuecomment-443398154
https://github.com/scverse/scanpy/pull/382#issuecomment-443398324:162,Deployability,update,update,162,"Thank you for the PR! It looks good to me. Also the function underlying, as far as I can tell. If there are performance problems, we can still address them in an update. 80 character lines would be nice also for the docstring. Then I could see whether they make sense. I'm seeing this on a 13-inch screen and the docstring looks like a mess through that. ;)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/382#issuecomment-443398324
https://github.com/scverse/scanpy/pull/382#issuecomment-443398324:108,Performance,perform,performance,108,"Thank you for the PR! It looks good to me. Also the function underlying, as far as I can tell. If there are performance problems, we can still address them in an update. 80 character lines would be nice also for the docstring. Then I could see whether they make sense. I'm seeing this on a 13-inch screen and the docstring looks like a mess through that. ;)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/382#issuecomment-443398324
https://github.com/scverse/scanpy/pull/382#issuecomment-443462331:33,Testability,log,logic,33,"Thanks! Using the most formal of logic :-), line length is now 80 chars.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/382#issuecomment-443462331
https://github.com/scverse/scanpy/issues/383#issuecomment-443276112:196,Modifiability,variab,variable,196,"Hi,. To make a pca plot you need the functions `scn.tl.pca(adata_needed)` and `scn.pl.pca(adata_needed)`. If you would like to visualize your CD4+ and CD8+ cells on this plot, you need to store a variable in `adata_needed.obs` which contains the information which cell is CD4+ and which cell is CD8+. For example:; ```; adata_needed.obs['cell_type'] = my_list_with_cd_labels; scn.tl.pca(adata_needed); scn.pl.pac(adata_needed, color='cell_type'); ```. Here the variable `my_list_with_cd_labels` should look someting like this:; `my_list_with_cd_labels = ['CD8+', 'CD4+', 'CD4+', 'CD4+', 'CD4+', 'CD8+', 'CD8+', ..., 'CD8+', 'CD4+']`; Where the length of the list is equal to the number of cells you have in `adata_needed` and the order is the same as well. It may be helpful to consult this tutorial as well: https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb. I hope that helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/383#issuecomment-443276112
https://github.com/scverse/scanpy/issues/383#issuecomment-443276112:461,Modifiability,variab,variable,461,"Hi,. To make a pca plot you need the functions `scn.tl.pca(adata_needed)` and `scn.pl.pca(adata_needed)`. If you would like to visualize your CD4+ and CD8+ cells on this plot, you need to store a variable in `adata_needed.obs` which contains the information which cell is CD4+ and which cell is CD8+. For example:; ```; adata_needed.obs['cell_type'] = my_list_with_cd_labels; scn.tl.pca(adata_needed); scn.pl.pac(adata_needed, color='cell_type'); ```. Here the variable `my_list_with_cd_labels` should look someting like this:; `my_list_with_cd_labels = ['CD8+', 'CD4+', 'CD4+', 'CD4+', 'CD4+', 'CD8+', 'CD8+', ..., 'CD8+', 'CD4+']`; Where the length of the list is equal to the number of cells you have in `adata_needed` and the order is the same as well. It may be helpful to consult this tutorial as well: https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb. I hope that helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/383#issuecomment-443276112
https://github.com/scverse/scanpy/issues/383#issuecomment-445461792:28,Usability,clear,clear,28,Thank you!; It was just not clear from the tutorial in the beginning that the same thing is used to define colors in plots :(,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/383#issuecomment-445461792
https://github.com/scverse/scanpy/issues/385#issuecomment-443412025:232,Availability,avail,available,232,"Personally, I don't like it because; * Explicit is better than implicit; * there could be cases where I would like to plot Ensembl/Entrez or whatever identifier I have in `var_names` directly, even if I have a `gene_symbols` column available.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/385#issuecomment-443412025
https://github.com/scverse/scanpy/issues/385#issuecomment-456026742:53,Deployability,update,update,53,"Yeah, I decided just to go for it 🤠. I'd be happy to update the code if there ends up being a `gene_symbols` flag.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/385#issuecomment-456026742
https://github.com/scverse/scanpy/issues/386#issuecomment-445273759:71,Deployability,update,update,71,"The variable y_axis is something I introduced in my latest PR. If you; update to the master branch you should see those changes. On Tue, Dec 4, 2018 at 2:49 AM pritykin <notifications@github.com> wrote:. > I would like to use stacked_violin plot with variable y-axis limits,; > particularly when swap_axes=True. Examples here; > <https://gist.github.com/fidelram/2289b7a8d6da055fb058ac9a79ed485c>,; > particularly code in line 7, show this. How do I do this? When I use it now; > with my code, it always chooses a uniform y-axis limit for all genes. Which; > option do I use for variable y-axis limits?; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/386>, or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1VtN8DWjBSDb-YjUImPvquAJapH3ks5u1dSzgaJpZM4Y_wfC>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/386#issuecomment-445273759
https://github.com/scverse/scanpy/issues/386#issuecomment-445273759:4,Modifiability,variab,variable,4,"The variable y_axis is something I introduced in my latest PR. If you; update to the master branch you should see those changes. On Tue, Dec 4, 2018 at 2:49 AM pritykin <notifications@github.com> wrote:. > I would like to use stacked_violin plot with variable y-axis limits,; > particularly when swap_axes=True. Examples here; > <https://gist.github.com/fidelram/2289b7a8d6da055fb058ac9a79ed485c>,; > particularly code in line 7, show this. How do I do this? When I use it now; > with my code, it always chooses a uniform y-axis limit for all genes. Which; > option do I use for variable y-axis limits?; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/386>, or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1VtN8DWjBSDb-YjUImPvquAJapH3ks5u1dSzgaJpZM4Y_wfC>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/386#issuecomment-445273759
https://github.com/scverse/scanpy/issues/386#issuecomment-445273759:251,Modifiability,variab,variable,251,"The variable y_axis is something I introduced in my latest PR. If you; update to the master branch you should see those changes. On Tue, Dec 4, 2018 at 2:49 AM pritykin <notifications@github.com> wrote:. > I would like to use stacked_violin plot with variable y-axis limits,; > particularly when swap_axes=True. Examples here; > <https://gist.github.com/fidelram/2289b7a8d6da055fb058ac9a79ed485c>,; > particularly code in line 7, show this. How do I do this? When I use it now; > with my code, it always chooses a uniform y-axis limit for all genes. Which; > option do I use for variable y-axis limits?; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/386>, or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1VtN8DWjBSDb-YjUImPvquAJapH3ks5u1dSzgaJpZM4Y_wfC>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/386#issuecomment-445273759
https://github.com/scverse/scanpy/issues/386#issuecomment-445273759:579,Modifiability,variab,variable,579,"The variable y_axis is something I introduced in my latest PR. If you; update to the master branch you should see those changes. On Tue, Dec 4, 2018 at 2:49 AM pritykin <notifications@github.com> wrote:. > I would like to use stacked_violin plot with variable y-axis limits,; > particularly when swap_axes=True. Examples here; > <https://gist.github.com/fidelram/2289b7a8d6da055fb058ac9a79ed485c>,; > particularly code in line 7, show this. How do I do this? When I use it now; > with my code, it always chooses a uniform y-axis limit for all genes. Which; > option do I use for variable y-axis limits?; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/386>, or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1VtN8DWjBSDb-YjUImPvquAJapH3ks5u1dSzgaJpZM4Y_wfC>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/386#issuecomment-445273759
https://github.com/scverse/scanpy/issues/386#issuecomment-921089934:58,Availability,error,error,58,"Hi,; I tried to set the y-axis limit, but failed with the error:. >>> axes = sc.pl.stacked_violin(adata, marker_genes, groupby='cell_types', rotation=90,swap_axes=True,row_palette='muted',yticklabels=True,show=False); >>> for ax in axes:; ... ax.set_ylim(0, 5); ...; Traceback (most recent call last):; File ""<stdin>"", line 2, in <module>; AttributeError: 'str' object has no attribute 'set_ylim'. I use scanpy 1.8.1.; Do you have any idea? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/386#issuecomment-921089934
https://github.com/scverse/scanpy/issues/386#issuecomment-921104209:759,Availability,error,error,759,"Look at the documentation before you ask questions. The object returned from the function you called doesn’t return a matplotlib object, it returns a dictionary, assuming that the ‘show’ parameter is off. You can’t loop through a dictionary like an array, you need to retrieve the keys access individual values and then use the ‘ylim’ property. Get Outlook for iOS<https://aka.ms/o0ukef>; ________________________________; From: ZxyChopcat ***@***.***>; Sent: Thursday, September 16, 2021 1:24:05 PM; To: theislab/scanpy ***@***.***>; Cc: Vekeria, Jai Patel ***@***.***>; Comment ***@***.***>; Subject: Re: [theislab/scanpy] How to use stacked_violin with variable y-axis limits between rows? (#386). Hi,; I tried to set the y-axis limit, but failed with the error:; `>>> axes = sc.pl.stacked_violin(adata, marker_genes, groupby='cell_types', rotation=90,swap_axes=True,row_palette='muted',yticklabels=True,show=False). for ax in axes:; ... ax.set_ylim(0, 5); ...; Traceback (most recent call last):; File """", line 2, in; AttributeError: 'str' object has no attribute 'set_ylim'; `; I use scanpy 1.8.1.; Do you have any idea? Thanks!. —; You are receiving this because you commented.; Reply to this email directly, view it on GitHub<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Ftheislab%2Fscanpy%2Fissues%2F386%23issuecomment-921089934&data=04%7C01%7Cjai.vekeria%40pitt.edu%7C4da79e06909d45b4b4e508d97936c8d4%7C9ef9f489e0a04eeb87cc3a526112fd0d%7C1%7C0%7C637674098542578553%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=t3jhsNr2Q3IlftHnubs6%2FWZyy%2FAijC2BWJ18Ih41Py0%3D&reserved=0>, or unsubscribe<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FAL6KD25HVPRX7SK4DD5UPE3UCIR3LANCNFSM4GH7A7BA&data=04%7C01%7Cjai.vekeria%40pitt.edu%7C4da79e06909d45b4b4e508d97936c8d4%7C9ef9f489e0a04eeb87cc3a526112fd0d%7C1%7C0%7C637674098542578553%7CUnknown%7CTWF",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/386#issuecomment-921104209
https://github.com/scverse/scanpy/issues/386#issuecomment-921104209:656,Modifiability,variab,variable,656,"Look at the documentation before you ask questions. The object returned from the function you called doesn’t return a matplotlib object, it returns a dictionary, assuming that the ‘show’ parameter is off. You can’t loop through a dictionary like an array, you need to retrieve the keys access individual values and then use the ‘ylim’ property. Get Outlook for iOS<https://aka.ms/o0ukef>; ________________________________; From: ZxyChopcat ***@***.***>; Sent: Thursday, September 16, 2021 1:24:05 PM; To: theislab/scanpy ***@***.***>; Cc: Vekeria, Jai Patel ***@***.***>; Comment ***@***.***>; Subject: Re: [theislab/scanpy] How to use stacked_violin with variable y-axis limits between rows? (#386). Hi,; I tried to set the y-axis limit, but failed with the error:; `>>> axes = sc.pl.stacked_violin(adata, marker_genes, groupby='cell_types', rotation=90,swap_axes=True,row_palette='muted',yticklabels=True,show=False). for ax in axes:; ... ax.set_ylim(0, 5); ...; Traceback (most recent call last):; File """", line 2, in; AttributeError: 'str' object has no attribute 'set_ylim'; `; I use scanpy 1.8.1.; Do you have any idea? Thanks!. —; You are receiving this because you commented.; Reply to this email directly, view it on GitHub<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Ftheislab%2Fscanpy%2Fissues%2F386%23issuecomment-921089934&data=04%7C01%7Cjai.vekeria%40pitt.edu%7C4da79e06909d45b4b4e508d97936c8d4%7C9ef9f489e0a04eeb87cc3a526112fd0d%7C1%7C0%7C637674098542578553%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=t3jhsNr2Q3IlftHnubs6%2FWZyy%2FAijC2BWJ18Ih41Py0%3D&reserved=0>, or unsubscribe<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FAL6KD25HVPRX7SK4DD5UPE3UCIR3LANCNFSM4GH7A7BA&data=04%7C01%7Cjai.vekeria%40pitt.edu%7C4da79e06909d45b4b4e508d97936c8d4%7C9ef9f489e0a04eeb87cc3a526112fd0d%7C1%7C0%7C637674098542578553%7CUnknown%7CTWF",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/386#issuecomment-921104209
https://github.com/scverse/scanpy/issues/386#issuecomment-921104209:1247,Safety,safe,safelinks,1247,"ay, you need to retrieve the keys access individual values and then use the ‘ylim’ property. Get Outlook for iOS<https://aka.ms/o0ukef>; ________________________________; From: ZxyChopcat ***@***.***>; Sent: Thursday, September 16, 2021 1:24:05 PM; To: theislab/scanpy ***@***.***>; Cc: Vekeria, Jai Patel ***@***.***>; Comment ***@***.***>; Subject: Re: [theislab/scanpy] How to use stacked_violin with variable y-axis limits between rows? (#386). Hi,; I tried to set the y-axis limit, but failed with the error:; `>>> axes = sc.pl.stacked_violin(adata, marker_genes, groupby='cell_types', rotation=90,swap_axes=True,row_palette='muted',yticklabels=True,show=False). for ax in axes:; ... ax.set_ylim(0, 5); ...; Traceback (most recent call last):; File """", line 2, in; AttributeError: 'str' object has no attribute 'set_ylim'; `; I use scanpy 1.8.1.; Do you have any idea? Thanks!. —; You are receiving this because you commented.; Reply to this email directly, view it on GitHub<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Ftheislab%2Fscanpy%2Fissues%2F386%23issuecomment-921089934&data=04%7C01%7Cjai.vekeria%40pitt.edu%7C4da79e06909d45b4b4e508d97936c8d4%7C9ef9f489e0a04eeb87cc3a526112fd0d%7C1%7C0%7C637674098542578553%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=t3jhsNr2Q3IlftHnubs6%2FWZyy%2FAijC2BWJ18Ih41Py0%3D&reserved=0>, or unsubscribe<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FAL6KD25HVPRX7SK4DD5UPE3UCIR3LANCNFSM4GH7A7BA&data=04%7C01%7Cjai.vekeria%40pitt.edu%7C4da79e06909d45b4b4e508d97936c8d4%7C9ef9f489e0a04eeb87cc3a526112fd0d%7C1%7C0%7C637674098542578553%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=NONHFLCjdRyCv1jUBuSyGgy4%2FX8do5WWWrrPPyLk5tw%3D&reserved=0>.; Triage notifications on the go with GitHub Mobile for iOS<https://nam12.safelinks.protection.outloo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/386#issuecomment-921104209
https://github.com/scverse/scanpy/issues/386#issuecomment-921104209:1704,Safety,safe,safelinks,1704,"ried to set the y-axis limit, but failed with the error:; `>>> axes = sc.pl.stacked_violin(adata, marker_genes, groupby='cell_types', rotation=90,swap_axes=True,row_palette='muted',yticklabels=True,show=False). for ax in axes:; ... ax.set_ylim(0, 5); ...; Traceback (most recent call last):; File """", line 2, in; AttributeError: 'str' object has no attribute 'set_ylim'; `; I use scanpy 1.8.1.; Do you have any idea? Thanks!. —; You are receiving this because you commented.; Reply to this email directly, view it on GitHub<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Ftheislab%2Fscanpy%2Fissues%2F386%23issuecomment-921089934&data=04%7C01%7Cjai.vekeria%40pitt.edu%7C4da79e06909d45b4b4e508d97936c8d4%7C9ef9f489e0a04eeb87cc3a526112fd0d%7C1%7C0%7C637674098542578553%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=t3jhsNr2Q3IlftHnubs6%2FWZyy%2FAijC2BWJ18Ih41Py0%3D&reserved=0>, or unsubscribe<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FAL6KD25HVPRX7SK4DD5UPE3UCIR3LANCNFSM4GH7A7BA&data=04%7C01%7Cjai.vekeria%40pitt.edu%7C4da79e06909d45b4b4e508d97936c8d4%7C9ef9f489e0a04eeb87cc3a526112fd0d%7C1%7C0%7C637674098542578553%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=NONHFLCjdRyCv1jUBuSyGgy4%2FX8do5WWWrrPPyLk5tw%3D&reserved=0>.; Triage notifications on the go with GitHub Mobile for iOS<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fapps.apple.com%2Fapp%2Fapple-store%2Fid1477376905%3Fct%3Dnotification-email%26mt%3D8%26pt%3D524675&data=04%7C01%7Cjai.vekeria%40pitt.edu%7C4da79e06909d45b4b4e508d97936c8d4%7C9ef9f489e0a04eeb87cc3a526112fd0d%7C1%7C0%7C637674098542588546%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=%2FXLG4aYkL8kN8XOgwQdaIBq3OoN%2FjjSAE984xAYIRDM%3D&reserved=0> or Android<https://nam12.safe",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/386#issuecomment-921104209
https://github.com/scverse/scanpy/issues/386#issuecomment-921104209:2225,Safety,safe,safelinks,2225,"n GitHub<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Ftheislab%2Fscanpy%2Fissues%2F386%23issuecomment-921089934&data=04%7C01%7Cjai.vekeria%40pitt.edu%7C4da79e06909d45b4b4e508d97936c8d4%7C9ef9f489e0a04eeb87cc3a526112fd0d%7C1%7C0%7C637674098542578553%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=t3jhsNr2Q3IlftHnubs6%2FWZyy%2FAijC2BWJ18Ih41Py0%3D&reserved=0>, or unsubscribe<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FAL6KD25HVPRX7SK4DD5UPE3UCIR3LANCNFSM4GH7A7BA&data=04%7C01%7Cjai.vekeria%40pitt.edu%7C4da79e06909d45b4b4e508d97936c8d4%7C9ef9f489e0a04eeb87cc3a526112fd0d%7C1%7C0%7C637674098542578553%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=NONHFLCjdRyCv1jUBuSyGgy4%2FX8do5WWWrrPPyLk5tw%3D&reserved=0>.; Triage notifications on the go with GitHub Mobile for iOS<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fapps.apple.com%2Fapp%2Fapple-store%2Fid1477376905%3Fct%3Dnotification-email%26mt%3D8%26pt%3D524675&data=04%7C01%7Cjai.vekeria%40pitt.edu%7C4da79e06909d45b4b4e508d97936c8d4%7C9ef9f489e0a04eeb87cc3a526112fd0d%7C1%7C0%7C637674098542588546%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=%2FXLG4aYkL8kN8XOgwQdaIBq3OoN%2FjjSAE984xAYIRDM%3D&reserved=0> or Android<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fplay.google.com%2Fstore%2Fapps%2Fdetails%3Fid%3Dcom.github.android%26referrer%3Dutm_campaign%253Dnotification-email%2526utm_medium%253Demail%2526utm_source%253Dgithub&data=04%7C01%7Cjai.vekeria%40pitt.edu%7C4da79e06909d45b4b4e508d97936c8d4%7C9ef9f489e0a04eeb87cc3a526112fd0d%7C1%7C0%7C637674098542588546%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=jfOeBNWAPuUo7LZ0gx6ArEZqlajNpIB1XHvuXjBYblo%3D&reserved=0>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/386#issuecomment-921104209
https://github.com/scverse/scanpy/issues/386#issuecomment-921104209:2705,Safety,safe,safelinks,2705,"n GitHub<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Ftheislab%2Fscanpy%2Fissues%2F386%23issuecomment-921089934&data=04%7C01%7Cjai.vekeria%40pitt.edu%7C4da79e06909d45b4b4e508d97936c8d4%7C9ef9f489e0a04eeb87cc3a526112fd0d%7C1%7C0%7C637674098542578553%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=t3jhsNr2Q3IlftHnubs6%2FWZyy%2FAijC2BWJ18Ih41Py0%3D&reserved=0>, or unsubscribe<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FAL6KD25HVPRX7SK4DD5UPE3UCIR3LANCNFSM4GH7A7BA&data=04%7C01%7Cjai.vekeria%40pitt.edu%7C4da79e06909d45b4b4e508d97936c8d4%7C9ef9f489e0a04eeb87cc3a526112fd0d%7C1%7C0%7C637674098542578553%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=NONHFLCjdRyCv1jUBuSyGgy4%2FX8do5WWWrrPPyLk5tw%3D&reserved=0>.; Triage notifications on the go with GitHub Mobile for iOS<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fapps.apple.com%2Fapp%2Fapple-store%2Fid1477376905%3Fct%3Dnotification-email%26mt%3D8%26pt%3D524675&data=04%7C01%7Cjai.vekeria%40pitt.edu%7C4da79e06909d45b4b4e508d97936c8d4%7C9ef9f489e0a04eeb87cc3a526112fd0d%7C1%7C0%7C637674098542588546%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=%2FXLG4aYkL8kN8XOgwQdaIBq3OoN%2FjjSAE984xAYIRDM%3D&reserved=0> or Android<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fplay.google.com%2Fstore%2Fapps%2Fdetails%3Fid%3Dcom.github.android%26referrer%3Dutm_campaign%253Dnotification-email%2526utm_medium%253Demail%2526utm_source%253Dgithub&data=04%7C01%7Cjai.vekeria%40pitt.edu%7C4da79e06909d45b4b4e508d97936c8d4%7C9ef9f489e0a04eeb87cc3a526112fd0d%7C1%7C0%7C637674098542588546%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=jfOeBNWAPuUo7LZ0gx6ArEZqlajNpIB1XHvuXjBYblo%3D&reserved=0>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/386#issuecomment-921104209
https://github.com/scverse/scanpy/issues/386#issuecomment-921104209:286,Security,access,access,286,"Look at the documentation before you ask questions. The object returned from the function you called doesn’t return a matplotlib object, it returns a dictionary, assuming that the ‘show’ parameter is off. You can’t loop through a dictionary like an array, you need to retrieve the keys access individual values and then use the ‘ylim’ property. Get Outlook for iOS<https://aka.ms/o0ukef>; ________________________________; From: ZxyChopcat ***@***.***>; Sent: Thursday, September 16, 2021 1:24:05 PM; To: theislab/scanpy ***@***.***>; Cc: Vekeria, Jai Patel ***@***.***>; Comment ***@***.***>; Subject: Re: [theislab/scanpy] How to use stacked_violin with variable y-axis limits between rows? (#386). Hi,; I tried to set the y-axis limit, but failed with the error:; `>>> axes = sc.pl.stacked_violin(adata, marker_genes, groupby='cell_types', rotation=90,swap_axes=True,row_palette='muted',yticklabels=True,show=False). for ax in axes:; ... ax.set_ylim(0, 5); ...; Traceback (most recent call last):; File """", line 2, in; AttributeError: 'str' object has no attribute 'set_ylim'; `; I use scanpy 1.8.1.; Do you have any idea? Thanks!. —; You are receiving this because you commented.; Reply to this email directly, view it on GitHub<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Ftheislab%2Fscanpy%2Fissues%2F386%23issuecomment-921089934&data=04%7C01%7Cjai.vekeria%40pitt.edu%7C4da79e06909d45b4b4e508d97936c8d4%7C9ef9f489e0a04eeb87cc3a526112fd0d%7C1%7C0%7C637674098542578553%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=t3jhsNr2Q3IlftHnubs6%2FWZyy%2FAijC2BWJ18Ih41Py0%3D&reserved=0>, or unsubscribe<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FAL6KD25HVPRX7SK4DD5UPE3UCIR3LANCNFSM4GH7A7BA&data=04%7C01%7Cjai.vekeria%40pitt.edu%7C4da79e06909d45b4b4e508d97936c8d4%7C9ef9f489e0a04eeb87cc3a526112fd0d%7C1%7C0%7C637674098542578553%7CUnknown%7CTWF",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/386#issuecomment-921104209
https://github.com/scverse/scanpy/issues/387#issuecomment-444197487:476,Deployability,update,update,476,"We're using a custom color map in scanpy by default, anyways: https://github.com/theislab/scanpy/blob/master/scanpy/plotting/palettes.py#L22. It would, of course, be easy to change this, but then everything changes for everyone and many people will wonder why everything looks different now (""where is my green cluster?""). If we do it, we only exchange green with another color, so that at least all other colors will be unaffected... I would have liked to wait until a major update, because I consider this breaking backward consistency, though...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387#issuecomment-444197487
https://github.com/scverse/scanpy/issues/387#issuecomment-444197487:305,Energy Efficiency,green,green,305,"We're using a custom color map in scanpy by default, anyways: https://github.com/theislab/scanpy/blob/master/scanpy/plotting/palettes.py#L22. It would, of course, be easy to change this, but then everything changes for everyone and many people will wonder why everything looks different now (""where is my green cluster?""). If we do it, we only exchange green with another color, so that at least all other colors will be unaffected... I would have liked to wait until a major update, because I consider this breaking backward consistency, though...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387#issuecomment-444197487
https://github.com/scverse/scanpy/issues/387#issuecomment-444197487:353,Energy Efficiency,green,green,353,"We're using a custom color map in scanpy by default, anyways: https://github.com/theislab/scanpy/blob/master/scanpy/plotting/palettes.py#L22. It would, of course, be easy to change this, but then everything changes for everyone and many people will wonder why everything looks different now (""where is my green cluster?""). If we do it, we only exchange green with another color, so that at least all other colors will be unaffected... I would have liked to wait until a major update, because I consider this breaking backward consistency, though...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387#issuecomment-444197487
https://github.com/scverse/scanpy/issues/387#issuecomment-444388795:663,Deployability,update,update,663,"I am partial color blind as well. So I second any initiative in this; direction. On Tue, Dec 4, 2018 at 7:03 PM Alex Wolf <notifications@github.com> wrote:. > We're using a custom color map in scanpy by default, anyways:; > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/palettes.py#L22; > .; >; > It would, of course, be easy to change this, but then everything changes; > for everyone and many people will wonder why everything looks different now; > (""where is my green cluster?""). If we do it, we only exchange green with; > another color, so that at least all other colors will be unaffected...; >; > I would have liked to wait until a major update, because I consider this; > breaking backward consistency, though...; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/387#issuecomment-444197487>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1aBQoQxEiqx5gNfgpj2-tJvQZ2Ssks5u1rjXgaJpZM4ZA5qf>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387#issuecomment-444388795
https://github.com/scverse/scanpy/issues/387#issuecomment-444388795:483,Energy Efficiency,green,green,483,"I am partial color blind as well. So I second any initiative in this; direction. On Tue, Dec 4, 2018 at 7:03 PM Alex Wolf <notifications@github.com> wrote:. > We're using a custom color map in scanpy by default, anyways:; > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/palettes.py#L22; > .; >; > It would, of course, be easy to change this, but then everything changes; > for everyone and many people will wonder why everything looks different now; > (""where is my green cluster?""). If we do it, we only exchange green with; > another color, so that at least all other colors will be unaffected...; >; > I would have liked to wait until a major update, because I consider this; > breaking backward consistency, though...; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/387#issuecomment-444197487>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1aBQoQxEiqx5gNfgpj2-tJvQZ2Ssks5u1rjXgaJpZM4ZA5qf>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387#issuecomment-444388795
https://github.com/scverse/scanpy/issues/387#issuecomment-444388795:531,Energy Efficiency,green,green,531,"I am partial color blind as well. So I second any initiative in this; direction. On Tue, Dec 4, 2018 at 7:03 PM Alex Wolf <notifications@github.com> wrote:. > We're using a custom color map in scanpy by default, anyways:; > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/palettes.py#L22; > .; >; > It would, of course, be easy to change this, but then everything changes; > for everyone and many people will wonder why everything looks different now; > (""where is my green cluster?""). If we do it, we only exchange green with; > another color, so that at least all other colors will be unaffected...; >; > I would have liked to wait until a major update, because I consider this; > breaking backward consistency, though...; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/387#issuecomment-444197487>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1aBQoQxEiqx5gNfgpj2-tJvQZ2Ssks5u1rjXgaJpZM4ZA5qf>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387#issuecomment-444388795
https://github.com/scverse/scanpy/issues/387#issuecomment-444418923:195,Energy Efficiency,green,green,195,"I don’t consider this breaking backwards compatibility. Everything is still in the same place, still has the same labels. Only the default color of the labels is different. If we only switch the green or red color with another, most people won’t even notice.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387#issuecomment-444418923
https://github.com/scverse/scanpy/issues/387#issuecomment-444420310:204,Energy Efficiency,green,green,204,"> I don’t consider this breaking backwards compatibility. Everything is still in the same place, still has the same labels. Only the default color of the labels is different.; > ; > If we only switch the green or red color with another, most people won’t even notice. how about making green just a bit brighter/less bright to make it discernible from red for color-blind people? should not break much; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub, or mute the thread.; > . ---; Fabian Theis; Institute of Computational Biology - http://icb.helmholtz-muenchen.de; Helmholtz Zentrum München and Depts. Mathematics&Life Sciences, TU München. Helmholtz Zentrum Muenchen; Deutsches Forschungszentrum fuer Gesundheit und Umwelt (GmbH); Ingolstaedter Landstr. 1; 85764 Neuherberg; www.helmholtz-muenchen.de; Aufsichtsratsvorsitzende: MinDirig.in Petra Steiner-Hoffmann; Stellv.Aufsichtsratsvorsitzender: MinDirig. Dr. Manfred Wolter; Geschaeftsfuehrer: Prof. Dr. med. Dr. h.c. Matthias Tschoep, Heinrich Bassler, Dr. rer. nat. Alfons Enhsen; Registergericht: Amtsgericht Muenchen HRB 6466; USt-IdNr: DE 129521671",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387#issuecomment-444420310
https://github.com/scverse/scanpy/issues/387#issuecomment-444420310:285,Energy Efficiency,green,green,285,"> I don’t consider this breaking backwards compatibility. Everything is still in the same place, still has the same labels. Only the default color of the labels is different.; > ; > If we only switch the green or red color with another, most people won’t even notice. how about making green just a bit brighter/less bright to make it discernible from red for color-blind people? should not break much; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub, or mute the thread.; > . ---; Fabian Theis; Institute of Computational Biology - http://icb.helmholtz-muenchen.de; Helmholtz Zentrum München and Depts. Mathematics&Life Sciences, TU München. Helmholtz Zentrum Muenchen; Deutsches Forschungszentrum fuer Gesundheit und Umwelt (GmbH); Ingolstaedter Landstr. 1; 85764 Neuherberg; www.helmholtz-muenchen.de; Aufsichtsratsvorsitzende: MinDirig.in Petra Steiner-Hoffmann; Stellv.Aufsichtsratsvorsitzender: MinDirig. Dr. Manfred Wolter; Geschaeftsfuehrer: Prof. Dr. med. Dr. h.c. Matthias Tschoep, Heinrich Bassler, Dr. rer. nat. Alfons Enhsen; Registergericht: Amtsgericht Muenchen HRB 6466; USt-IdNr: DE 129521671",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387#issuecomment-444420310
https://github.com/scverse/scanpy/issues/387#issuecomment-444463868:220,Availability,down,down,220,"> how about making green just a bit brighter/less bright to make it discernible from red for color-blind people? should not break much. I guess that would be best done by switching the green with another green somewhere down the list to not mess with the color map entirely. Not sure this is a general purpose fix for all types of colorblindness though, no?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387#issuecomment-444463868
https://github.com/scverse/scanpy/issues/387#issuecomment-444463868:19,Energy Efficiency,green,green,19,"> how about making green just a bit brighter/less bright to make it discernible from red for color-blind people? should not break much. I guess that would be best done by switching the green with another green somewhere down the list to not mess with the color map entirely. Not sure this is a general purpose fix for all types of colorblindness though, no?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387#issuecomment-444463868
https://github.com/scverse/scanpy/issues/387#issuecomment-444463868:185,Energy Efficiency,green,green,185,"> how about making green just a bit brighter/less bright to make it discernible from red for color-blind people? should not break much. I guess that would be best done by switching the green with another green somewhere down the list to not mess with the color map entirely. Not sure this is a general purpose fix for all types of colorblindness though, no?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387#issuecomment-444463868
https://github.com/scverse/scanpy/issues/387#issuecomment-444463868:204,Energy Efficiency,green,green,204,"> how about making green just a bit brighter/less bright to make it discernible from red for color-blind people? should not break much. I guess that would be best done by switching the green with another green somewhere down the list to not mess with the color map entirely. Not sure this is a general purpose fix for all types of colorblindness though, no?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387#issuecomment-444463868
https://github.com/scverse/scanpy/issues/387#issuecomment-444729710:109,Energy Efficiency,green,green,109,I'm fine with such rather small changes. This should indeed not bother people. Tell me when you decided on a green that satisfies @ftheis. We don't want to make this change multiple times... :wink:,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387#issuecomment-444729710
https://github.com/scverse/scanpy/issues/387#issuecomment-444803441:478,Energy Efficiency,green,green,478,"Some colors of that cycle are too close for people with deuteranomaly (by far the most common type):. `#1f77b4` `#ff7f0e` `#2ca02c` `#d62728` `#9467bd` `#8c564b` `#e377c2` `#7f7f7f` `#bcbd22` `#17becf`. Normal | Deuteranomaly; --- | ---; ![Normal](https://user-images.githubusercontent.com/291575/49573631-6605b380-f936-11e8-9629-68b177e59043.png) | ![Deuteranomaly](https://user-images.githubusercontent.com/291575/49573600-52f2e380-f936-11e8-9729-67ec0dde0aaf.png). - red and green; - blue and purple; - orange and kakhi. Playing around a bit, it’s easy to get a version that works, e.g. `#1f77b4` `#ff7f0e` `#279e68` `#d62728` `#aa40fc` `#8c564b` `#e377c2` `#7f7f7f` `#b5bd61` `#17becf`. Normal | Deuteranomaly; --- | ---; ![Normal](https://user-images.githubusercontent.com/291575/49574129-926dff80-f937-11e8-8fad-58e89cceebf0.png) | ![Deuteranomaly](https://user-images.githubusercontent.com/291575/49574155-a285df00-f937-11e8-84ba-c1b1ae28ee99.png). I think the changes are subtle enough that we *can* adopt it now and change it a little later if we want.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387#issuecomment-444803441
https://github.com/scverse/scanpy/issues/387#issuecomment-444805210:66,Energy Efficiency,green,green,66,"thanks Phil - as discussed before can you just change the red and green i.e. #ff7f0e #2ca02c which look virtually the same to people with deuteranomaly (incl me) by the two colors you suggested?. > Am 06.12.2018 um 10:17 schrieb Philipp A. <notifications@github.com>:; > ; > Some colors of that cycle are too close for people with deuteranomaly (by far the most common type):; > ; > #1f77b4 #ff7f0e #2ca02c #d62728 #9467bd #8c564b #e377c2 #7f7f7f #bcbd22 #17becf; > ; > Normal	Deuteranomaly; > 	; > 	• red and green; > 	• blue and purple; > 	• orange and kakhi; > Playing around a bit, it’s easy to get a version that works, e.g. #1f77b4 #ff7f0e #279e68 #d62728 #aa40fc #8c564b #e377c2 #7f7f7f #b5bd61 #17becf; > ; > Normal	Deuteranomaly; > 	; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub, or mute the thread.; > . ---; Fabian Theis; Institute of Computational Biology - http://icb.helmholtz-muenchen.de; Helmholtz Zentrum München and Depts. Mathematics&Life Sciences, TU München. Helmholtz Zentrum Muenchen; Deutsches Forschungszentrum fuer Gesundheit und Umwelt (GmbH); Ingolstaedter Landstr. 1; 85764 Neuherberg; www.helmholtz-muenchen.de; Aufsichtsratsvorsitzende: MinDirig.in Petra Steiner-Hoffmann; Stellv.Aufsichtsratsvorsitzender: MinDirig. Dr. Manfred Wolter; Geschaeftsfuehrer: Prof. Dr. med. Dr. h.c. Matthias Tschoep, Heinrich Bassler, Dr. rer. nat. Alfons Enhsen; Registergericht: Amtsgericht Muenchen HRB 6466; USt-IdNr: DE 129521671",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387#issuecomment-444805210
https://github.com/scverse/scanpy/issues/387#issuecomment-444805210:510,Energy Efficiency,green,green,510,"thanks Phil - as discussed before can you just change the red and green i.e. #ff7f0e #2ca02c which look virtually the same to people with deuteranomaly (incl me) by the two colors you suggested?. > Am 06.12.2018 um 10:17 schrieb Philipp A. <notifications@github.com>:; > ; > Some colors of that cycle are too close for people with deuteranomaly (by far the most common type):; > ; > #1f77b4 #ff7f0e #2ca02c #d62728 #9467bd #8c564b #e377c2 #7f7f7f #bcbd22 #17becf; > ; > Normal	Deuteranomaly; > 	; > 	• red and green; > 	• blue and purple; > 	• orange and kakhi; > Playing around a bit, it’s easy to get a version that works, e.g. #1f77b4 #ff7f0e #279e68 #d62728 #aa40fc #8c564b #e377c2 #7f7f7f #b5bd61 #17becf; > ; > Normal	Deuteranomaly; > 	; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub, or mute the thread.; > . ---; Fabian Theis; Institute of Computational Biology - http://icb.helmholtz-muenchen.de; Helmholtz Zentrum München and Depts. Mathematics&Life Sciences, TU München. Helmholtz Zentrum Muenchen; Deutsches Forschungszentrum fuer Gesundheit und Umwelt (GmbH); Ingolstaedter Landstr. 1; 85764 Neuherberg; www.helmholtz-muenchen.de; Aufsichtsratsvorsitzende: MinDirig.in Petra Steiner-Hoffmann; Stellv.Aufsichtsratsvorsitzender: MinDirig. Dr. Manfred Wolter; Geschaeftsfuehrer: Prof. Dr. med. Dr. h.c. Matthias Tschoep, Heinrich Bassler, Dr. rer. nat. Alfons Enhsen; Registergericht: Amtsgericht Muenchen HRB 6466; USt-IdNr: DE 129521671",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387#issuecomment-444805210
https://github.com/scverse/scanpy/issues/387#issuecomment-444805820:62,Energy Efficiency,green,green,62,"I just changed one color in each of the named pairs (i.e. the green, the purple, and the kakhi). Are the swatches in the lower pictures distinguishable for you? Then the mathematical model for color closeness matches your vision and we should adopt something like it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387#issuecomment-444805820
https://github.com/scverse/scanpy/issues/387#issuecomment-444852474:82,Energy Efficiency,green,green,82,"when you use the firefox extension I quoted above, you’ll see that the orange and green are very similar for people with *protanomaly*, not deuteranomaly. so i guess fabian has the former, more rare thing. confirmed by fabian and the simulation, my changed colors seem to work well for both types. so I assume the webtool can be used to design this. the only change from default in the parameters I made was that i removed the minimum lightness distance (making the colors not suited for completely color blind people, but having no cone cells, their vision is probably too bad to see our plots anyway.). PS: please use backticks around colors, like `#fe57a1` so we can see little swatches on github! I edited your post for this @LuckyMD",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387#issuecomment-444852474
https://github.com/scverse/scanpy/issues/388#issuecomment-444339817:10,Deployability,patch,patch,10,"Here is a patch that fixes the above problem... import matplotlib.colors. #if user defined, then use the vmax, vmin keywords, else use data to generate them...; if ('vmax' in kwds) and ('vmin' in kwds):; _vmax = kwds['vmax']; _vmin = kwds['vmin']; else: ; _vmax = max(mean_flat); _vmin = min(mean_flat) . #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat), vmax=max(mean_flat)) ; normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax). I'll submit a pull request.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388#issuecomment-444339817
https://github.com/scverse/scanpy/issues/388#issuecomment-444388428:145,Deployability,patch,patch,145,"The change is quite useful. Please go ahead and add a PR. On Wed, Dec 5, 2018 at 3:52 AM Tim Rand <notifications@github.com> wrote:. > Here is a patch that fixes the above problem...; >; > import matplotlib.colors; >; > #if user defined, then use the vmax, vmin keywords, else use data to generate them...; > if ('vmax' in kwds) and ('vmin' in kwds):; > _vmax = kwds['vmax']; > _vmin = kwds['vmin']; > else:; > _vmax = max(mean_flat); > _vmin = min(mean_flat); >; > #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat), vmax=max(mean_flat)); > normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax); >; > I'll submit a pull request.; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/388#issuecomment-444339817>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1WglYAlmHO-3DyNHUCRJwBtAOfskks5u1zT6gaJpZM4ZB23Z>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388#issuecomment-444388428
https://github.com/scverse/scanpy/issues/388#issuecomment-444592024:381,Deployability,patch,patch,381,"Hi Fidel,; Please note new pull request; dotplot can take vmin vmax arguments from user; <https://github.com/theislab/scanpy/pull/390>; Tim. On Tue, Dec 4, 2018 at 11:39 PM Fidel Ramirez <notifications@github.com>; wrote:. > The change is quite useful. Please go ahead and add a PR.; >; > On Wed, Dec 5, 2018 at 3:52 AM Tim Rand <notifications@github.com> wrote:; >; > > Here is a patch that fixes the above problem...; > >; > > import matplotlib.colors; > >; > > #if user defined, then use the vmax, vmin keywords, else use data to; > generate them...; > > if ('vmax' in kwds) and ('vmin' in kwds):; > > _vmax = kwds['vmax']; > > _vmin = kwds['vmin']; > > else:; > > _vmax = max(mean_flat); > > _vmin = min(mean_flat); > >; > > #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat),; > vmax=max(mean_flat)); > > normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax); > >; > > I'll submit a pull request.; > >; > > —; > > You are receiving this because you are subscribed to this thread.; > > Reply to this email directly, view it on GitHub; > > <https://github.com/theislab/scanpy/issues/388#issuecomment-444339817>,; > > or mute the thread; > > <; > https://github.com/notifications/unsubscribe-auth/AEu_1WglYAlmHO-3DyNHUCRJwBtAOfskks5u1zT6gaJpZM4ZB23Z; > >; > > .; > >; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/388#issuecomment-444388428>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AACez5ZEF7goRe3PYEixKaLT4f0cNthGks5u13gdgaJpZM4ZB23Z>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388#issuecomment-444592024
https://github.com/scverse/scanpy/issues/388#issuecomment-444632665:111,Availability,error,error,111,"mmmm...; looks like there are some difficulties here.; The decorator sitting ontop of dotplot() causes a weird error for kwds; dictionary lookups. If I leave the decorator in place, then I get a; keywords error when, vmin is left out as a parameter. If I take the; decorator off the method, it works. error is coming from the. @doc_params(). decorator. But 1. it looks like this function only purpose in life it to; ensure that the __doc__ string starts with a '\' character. And in the case; of dotplot() it already does. When I comment out the decorator, the code; works. This error is too strange for me to understand. I don't often use; decorators, and it seems to be the problem here.; Tim. On Tue, Dec 4, 2018 at 11:39 PM Fidel Ramirez <notifications@github.com>; wrote:. > The change is quite useful. Please go ahead and add a PR.; >; > On Wed, Dec 5, 2018 at 3:52 AM Tim Rand <notifications@github.com> wrote:; >; > > Here is a patch that fixes the above problem...; > >; > > import matplotlib.colors; > >; > > #if user defined, then use the vmax, vmin keywords, else use data to; > generate them...; > > if ('vmax' in kwds) and ('vmin' in kwds):; > > _vmax = kwds['vmax']; > > _vmin = kwds['vmin']; > > else:; > > _vmax = max(mean_flat); > > _vmin = min(mean_flat); > >; > > #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat),; > vmax=max(mean_flat)); > > normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax); > >; > > I'll submit a pull request.; > >; > > —; > > You are receiving this because you are subscribed to this thread.; > > Reply to this email directly, view it on GitHub; > > <https://github.com/theislab/scanpy/issues/388#issuecomment-444339817>,; > > or mute the thread; > > <; > https://github.com/notifications/unsubscribe-auth/AEu_1WglYAlmHO-3DyNHUCRJwBtAOfskks5u1zT6gaJpZM4ZB23Z; > >; > > .; > >; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388#issuecomment-444632665
