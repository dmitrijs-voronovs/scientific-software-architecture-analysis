quality_attribute,sentence,source,author,repo,version,id,keyword,matched_word,match_idx,wiki,url,total_similar,target_keywords,target_matched_words
Integrability,"### What kind of feature would you like to request?. Additional function parameters / changed functionality / changed defaults?. ### Please describe your wishes. #### Summary; Integration of the `polars` and `fast_matrix_market` libraries into Scanpy's data loading functions, specifically `scanpy.read_10x_mtx` and `scanpy.read_mtx`. This will improve the loading speed of `.mtx` and `.csv` files, which is crucial for handling large-scale single-cell datasets more efficiently. #### The problem; The current data loading mechanisms in Scanpy, while effective for small to medium datasets, could be substantially optimized for speed when dealing with larger datasets. #### Expected Impact; - Reduced loading times; - Improving the user experience; - Enhanced scalability. #### Code snipped. ```; import fast_matrix_market; import os; import scanpy as sc; import scipy as sp. def read_10x_faster(; path: str; )-> sc.AnnData:; """"""; Read a sparse matrix in Matrix Market format and two CSV files with gene and cell metadata; into an AnnData object.; ; Args:; path: Path to the directory containing the matrix.mtx, genes.tsv, and barcodes.tsv files.; ; Returns:; An AnnData object with the matrix, gene metadata, and cell metadata. """"""; mtx_file = os.path.join(path, ""matrix.mtx""); gene_info = os.path.join(path, ""genes.tsv""); cell_metadata = os.path.join(path, ""barcodes.tsv""); ; # Read the .mtx file into a sparse matrix using the fast_matrix_market package (faster than scanpy, uses multiprocessing); mtx = fast_matrix_market.mmread(mtx_file). # Convert the sparse matrix to a CSR matrix; # Otherwise you will not be able to use it with scanpy; if isinstance(mtx, sp.sparse.coo.coo_matrix):; mtx = mtx.tocsr(); ; # Create an AnnData object; adata = sc.AnnData(X=mtx.T). # Polars is faster than pandas reading csv files; # Read the gene names and cell names into the AnnData object; adata.var = pl.read_csv(gene_info, separator= '\t', has_header=False).to_pandas(); ; # Read the cell names and cell met",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2846:176,Integrat,Integration,176,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2846,1,['Integrat'],['Integration']
Integrability,"### What kind of feature would you like to request?. Additional function parameters / changed functionality / changed defaults?. ### Please describe your wishes. Currently sc.pp.subsample does not allow for sampling with replacement. When n_obs is provided, and it is larger than the size of the adata object, an error message from numpy.random.choice is given.; ""obs_indices = np.random.choice(old_n_obs, size=new_n_obs, replace=False)"". It seems like replace is automatically set to False. It would be great if sc.pp.subsample provided a paramater to change the np.random.choice's 'replace' parameter to True.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2854:319,message,message,319,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2854,1,['message'],['message']
Integrability,"### What kind of feature would you like to request?. Additional function parameters / changed functionality / changed defaults?. ### Please describe your wishes. Hello,. I am trying to integrate some datasets that were generated with SmartSeq2 and 10X platforms. For the SmartSeq2 datasets, I only got the TPM data, while for the 10x data,I got raw counts matrix. How to merge and integrate SmartSeq2 and 10X Datasets? Can the TPM and raw counts be merged for analysis?. Thank you in advance for your assistance.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2662:185,integrat,integrate,185,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2662,2,['integrat'],['integrate']
Integrability,### What kind of feature would you like to request?. Additional function parameters / changed functionality / changed defaults?. ### Please describe your wishes. Hello: . I have multiome snRNA+snATAC data and the snRNA seq data were analyzed with `scanpy`. ; How to integrate the snRNA seq data generated by `scanpy` with the snATAC seq data generated by `ArchR` ?; From the documents https://www.archrproject.com/bookdown/cross-platform-linkage-of-scatac-seq-cells-with-scrna-seq-cells.html; How to generate the `RangedSummarizedExperiment` data set ? Is it possible to convert the `anndata` used by `scanpy` to `RangedSummarizedExperiment` data?; Thanks a lot,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3273:266,integrat,integrate,266,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3273,1,['integrat'],['integrate']
Integrability,"### What kind of feature would you like to request?. Additional function parameters / changed functionality / changed defaults?. ### Please describe your wishes. It would be extremely helpful if the embedding manifold tools had scikit-learn style API. . For example, https://pydiffmap.readthedocs.io/en/master/reference/diffusion_map.html. Having the .fit, .transform, and .fit_transform would make the robust implementations in the backend of ScanPy a lot more accessible for users. Right now, the usage feels a bit restrictive and I'm having difficulty leveraging the power of the methods if it's not part of some similar workflow that is in the tutorials. . I'm trying to use the code in the backend of ScanPy implement this API myself but ScanPy is an extremely confusing package from an outside developer. There are nested functions and tests for even simple steps (many of which handle edge cases making the package robust). More specifically, I'm trying to use the ScanPy implementation of Diffusion Maps as I would use those from pyDiffMap or the spectral clustering from Sklearn. I would like to be able to fit a model with data. Pickle it. Then transform new samples based on the fitted model. This would provide a useful interface for users looking for a non linear alternative to pca.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3054:1232,interface,interface,1232,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3054,1,['interface'],['interface']
Integrability,### What kind of feature would you like to request?. Additional function parameters / changed functionality / changed defaults?. ### Please describe your wishes. The array api is somewhat mature to the point of being using in scipy for interoperability between dense CPU/GPU functions (see https://docs.scipy.org/doc/scipy/dev/api-dev/array_api.html). It is coming with the new sparse format that may come into existence: https://github.com/pydata/sparse/discussions/618. So it would be good to start experimenting with this in scanpy,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3136:236,interoperab,interoperability,236,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3136,1,['interoperab'],['interoperability']
Integrability,### What kind of feature would you like to request?. New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?. ### Please describe your wishes. I'm currently implementing a function that takes in an anndata and then subsamples a given representation using https://github.com/jmschrei/apricot. This generally serves the purpose of semi-optimally picking a reduced number of points that's still representative of the latent space. . Is this sth within the scope of scanpy? When it's done it wouldn't be too much effort to polish it up for a PR. The dependency load seems fairly low.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2862:583,depend,dependency,583,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2862,1,['depend'],['dependency']
Integrability,"### What kind of feature would you like to request?. New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?. ### Please describe your wishes. I've thought for a while that we should have NMF in scanpy (https://github.com/scverse/scanpy/pull/941). But it's always been pretty trivial to implement, so not that much work for someone to cover. But now that we're increasing the amount of out of core support in scanpy I think we can offer a lot more value here with out-of-core NMF support. I would suggest we start with a simple [`sklearn.decompositions.NMF`](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html) wrapper for in memory datasets. . For out of core implementations, it'll be a bit more work. Some thoughts:. * `sklearn` offers `MiniBatchNMF` which allows updating by batch. While this is out of core, it's effectively serial and may not scale well with increasing compute; * But there are [many distributed NMF implementations out there](https://www.google.com/search?client=safari&rls=en&q=distributed+nmf&ie=UTF-8&oe=UTF-8) (including GPU specific ones, which is relevant for rapids-singlecell); * It would be nice to upstream whatever we do to dask-ml (https://github.com/dask/dask-ml/issues/96), maybe cuml",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2939:678,wrap,wrapper,678,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2939,1,['wrap'],['wrapper']
Integrability,"### What kind of feature would you like to request?. Other?. ### Please describe your wishes. Another point that I'd like to throw into the scanpy 2.0 discussion: . Right now, many functions have the `inplace` argument, that determines if a function should write back to `adata` or return the result instead. . With this behavior it is hard to make correct type hints. While it is possible with `@overload`, it is cumbersome because it requires to type out the entire function signature twice. When I asked if it is possible to write these overloads in a more concise way [on stackoverflow](https://stackoverflow.com/questions/75757890/python-overload-single-argument?noredirect=1#comment133653817_75757890) several users argued that changing the return type based on an argument is an anti-pattern, and I think they convinced me. . ## Alternative approach; Have two API levels, e.g. ```py; def scanpy.tl.pca(adata: AnnData, **kwargs) -> None: ...; ```; and; ```py; def scanpy.lowlevel.tl.pca(data: np.ndarray | sp.spmatrix, n_pcs) -> np.ndarray: ...; ```. Where the former is a wrapper for the latter. This allows to separate the implementation of the actual method using only numpy/scipy data types from the scverse-specific behavior.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2583:1079,wrap,wrapper,1079,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2583,1,['wrap'],['wrapper']
Integrability,"### What kind of feature would you like to request?. Other?. ### Please describe your wishes. Depends on. - https://github.com/scientific-python/pytest-doctestplus/issues/229; - https://github.com/scientific-python/pytest-doctestplus/issues/231; - https://github.com/pytest-dev/pytest/issues/11475; - And after that one has been fixed, I’m sure doctestplus also needs to adjust to it.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2729:94,Depend,Depends,94,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2729,1,['Depend'],['Depends']
Integrability,"### What kind of feature would you like to request?. Other?. ### Please describe your wishes. Hi! I [tried](https://github.com/VPetukhov/ngschool2023-bayesian) running scanpy using [JupyterLite](https://jupyterlite.readthedocs.io/en/stable/) for teaching purposes. However, it fails, as only `noarch` packages are currently supported in [xeus](https://jupyterlite.readthedocs.io/en/stable/howto/xeus-python/preinstalled_packages.html). From my error log it seems the only non-`noarch` dependency is [h5py](https://beta.mamba.pm/channels/conda-forge/packages/h5py), which is not really needed for teaching. . Would it be possible to make `h5py` an optional dependency on conda-forge somehow? It would be really helpful for many students to be able to try scanpy without the need to install anything locally.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2667:485,depend,dependency,485,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2667,2,['depend'],['dependency']
Integrability,"#### Problem; File `tissue_positions_list.csv` not found when running `sc.read_visium`; This issue was caused by the file name change by `spaceranger`. . #### Solution; Make a copy of the `outs/spatial/tissue_positions.csv` and name it as `outs/spatial/tissue_positions_list.csv`. #### Problem; The following error messages appeared when running `sc.pl.spatial`:; ```python; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); Cell In [4], line 1; ----> 1 sc.pl.spatial(adata, color=adata.var_names.tolist()[0], img_key='hires', scale_factor=None). File scanpy/plotting/_tools/scatterplots.py:1003, in spatial(adata, basis, img, img_key, library_id, crop_coord, alpha_img, bw, size, scale_factor, spot_size, na_color, show, return_fig, save, **kwargs); 1000 cmap_img = None; 1001 circle_radius = size * scale_factor * spot_size * 0.5; -> 1003 axs = embedding(; 1004 adata,; 1005 basis=basis,; 1006 scale_factor=scale_factor,; 1007 size=circle_radius,; 1008 na_color=na_color,; 1009 show=False,; 1010 save=False,; 1011 **kwargs,; 1012 ); 1013 if not isinstance(axs, list):; 1014 axs = [axs]. File scanpy/plotting/_tools/scatterplots.py:392, in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, dimensions, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, colorbar_loc, vmax, vmin, vcenter, norm, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs); 389 # if user did not set alpha, set alpha to 0.7; 390 kwargs['alpha'] = 0.7 if alpha is None else alpha; --> 392 cax = scatter(; 393 coords[:, 0],; 394 coords[:, 1],; 395 marker=""."",; 396 c=color_vector,; 397 rasterized=settings._vector_friendly,; 398 norm=normalize,; 399 **kwargs,; 400 ); 402 # remo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2345:315,message,messages,315,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2345,1,['message'],['messages']
Integrability,"#### Versions. <details>. -----; anndata 0.8.0; scanpy 1.9.1; -----; PIL 9.1.1; aa8f2297d25b4dc6fd3d98411eb3ba53823c4f42 NA; absl NA; asttokens NA; astunparse 1.6.3; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; brotli NA; certifi 2022.12.07; cffi 1.15.1; charset_normalizer 2.1.0; colorama 0.4.5; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; debugpy 1.6.3; decorator 5.1.1; defusedxml 0.7.1; entrypoints 0.4; etils 0.7.1; executing 0.10.0; flatbuffers 22.11.23; fsspec 2022.7.1; gast NA; google NA; h5py 3.7.0; hypergeom_ufunc NA; idna 3.3; igraph 0.10.2; ipykernel 6.14.0; ipython_genutils 0.2.0; ipywidgets 7.7.1; jax 0.3.16; jaxlib 0.3.15; jedi 0.18.1; joblib 1.1.0; jupyter_server 1.18.1; keras 2.11.0; kiwisolver 1.4.3; leidenalg 0.9.0; llvmlite 0.38.1; louvain 0.8.0; lz4 4.0.2; matplotlib 3.5.3; mpl_toolkits NA; natsort 8.1.0; nbinom_ufunc NA; numba 0.55.2; numexpr 2.8.3; numpy 1.22.4; opt_einsum v3.3.0; packaging 21.3; pandas 1.4.2; parso 0.8.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.30; psutil 5.9.1; ptyprocess 0.7.0; pure_eval 0.2.2; pyarrow 9.0.0; pycparser 2.21; pydev_ipython NA; pydevconsole NA; pydevd 2.8.0; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.13.0; pynndescent 0.5.7; pyparsing 3.0.9; pytz 2022.2.1; requests 2.28.1; scipy 1.8.1; session_info 1.0.0; six 1.16.0; sklearn 1.1.1; socks 1.7.1; sphinxcontrib NA; stack_data 0.4.0; tensorboard 2.11.0; tensorflow 2.11.0; termcolor NA; texttable 1.6.7; threadpoolctl 3.1.0; tornado 6.2; tqdm 4.64.0; traitlets 5.3.0; typing_extensions NA; umap 0.5.3; unicodedata2 NA; urllib3 1.26.11; wcwidth 0.2.5; wrapt 1.14.1; yaml 6.0; zmq 23.2.1; zope NA; -----; IPython 8.4.0; jupyter_client 7.3.4; jupyter_core 4.11.1; jupyterlab 3.4.5; notebook 6.4.12; -----; Python 3.9.13 | packaged by conda-forge | (main, May 27 2022, 16:56:21) [GCC 10.3.0]; Linux-3.10.0-1160.66.1.el7.x86_64-x86_64-with-glibc2.17; -----; Session information updated at 2022-12-15 16:32. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2381:3597,wrap,wrapt,3597,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2381,1,['wrap'],['wrapt']
Integrability,#2210 is triggered by not having dask installed. Our current CI setup can't test for this. This should be addressed with a CI run that uses minimal dependencies.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2211:148,depend,dependencies,148,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2211,1,['depend'],['dependencies']
Integrability,"'Integrated.h5ad'); ```. So I did use make obs names unique after ad.concat(adatas). However, after I was finished with the integration and moving on to write_h5ad, it returns the following errors and tells me they can't write my h5ad cuz I have duplicated rows:. ```pytb; Feb 26 11:20:39 PM: Your dataset appears to contain duplicated items (rows); when embedding, you should typically have unique items.; Feb 26 11:20:39 PM: The following items have duplicates [60449 60452 60455 ... 70783 70784 70785]; Traceback (most recent call last):; File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_io/utils.py"", line 214, in func_wrapper; return func(elem, key, val, *args, **kwargs); File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_io/specs/registry.py"", line 171, in write_elem; _REGISTRY.get_writer(dest_type, (t, elem.dtype.kind), modifiers)(; File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_io/specs/registry.py"", line 24, in wrapper; result = func(g, k, *args, **kwargs); File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_io/specs/methods.py"", line 346, in write_vlen_string_array; f.create_dataset(k, data=elem.astype(str_dtype), dtype=str_dtype, **dataset_kwargs); File ""/home/joyzheng/.local/lib/python3.8/site-packages/h5py/_hl/group.py"", line 183, in create_dataset; dsid = dataset.make_new_dset(group, shape, dtype, data, name, **kwds); File ""/home/joyzheng/.local/lib/python3.8/site-packages/h5py/_hl/dataset.py"", line 168, in make_new_dset; dset_id.write(h5s.ALL, h5s.ALL, data); File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper; File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper; File ""h5py/h5d.pyx"", line 280, in h5py.h5d.DatasetID.write; File ""h5py/_proxy.pyx"", line 145, in h5py._proxy.dset_rw; File ""h5py/_conv.pyx"", line 444, in h5py._conv.str2vlen; File ""h5py/_conv.pyx"", line 95, in h5py._conv.generic_converter; File ""h5py/_conv.pyx"", line 249, in h5py._conv.conv_str2vlen; Type",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2432:1790,wrap,wrapper,1790,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2432,1,['wrap'],['wrapper']
Integrability,"(optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python; ncont = ncont[ncont.obs.pct_counts_mt < 5, :]; ncont.raw = ncont; ```. ```pytb; [TypeError: cannot unpack non-iterable NoneType object]; ```. #### Versions. scanpy==1.8.2 anndata==0.7.8 umap==0.5.2 numpy==1.21.5 scipy==1.7.1 pandas==1.3.2 scikit-learn==0.24.2 statsmodels==0.13.2 python-igraph==0.9.9 pynndescent==0.5.6. <details>. [WARNING: If you miss a compact list, please try `print_header`!; The `sinfo` package has changed name and is now called `session_info` to become more discoverable and self-explanatory. The `sinfo` PyPI package will be kept around to avoid breaking old installs and you can downgrade to 0.3.2 if you want to use it without seeing this message. For the latest features and bug fixes, please install `session_info` instead. The usage and defaults also changed slightly, so please review the latest README at https://gitlab.com/joelostblom/session_info.; -----; anndata 0.7.8; scanpy 1.8.2; sinfo 0.3.4; -----; PIL 8.3.1; anyio NA; attr 21.2.0; babel 2.9.1; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; celltypist 0.2.0; certifi 2021.05.30; cffi 1.14.6; charset_normalizer 2.0.4; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; decorator 5.0.9; defusedxml 0.7.1; entrypoints 0.3; h5py 3.3.0; idna 3.2; igraph 0.9.9; ipykernel 6.2.0; ipython_genutils 0.2.0; jedi 0.18.0; jinja2 3.0.1; joblib 1.0.1; json5 NA; jsonschema 3.2.0; jupyter_server 1.10.2; jupyterlab_server 2.7.1; kiwisolver 1.3.1; leidenalg 0.8.9; llvmlite 0.38.0; markupsafe 2.0.1; matplotlib 3.4.3; matplotlib_inline NA; mpl_toolkits NA; natsort 8.1.0; nbclassic NA; nbformat 5.1.3; nbinom_ufunc NA; numba 0.55.1; numexpr 2.8.1; numpy 1.21.5; packaging ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2188:1172,message,message,1172,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2188,1,['message'],['message']
Integrability,* fixed vmin/vmax for categorical data #800 ; * added error message when vmin is not valid to point out how to format it; * updated test to cover categorical data,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/804:60,message,message,60,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/804,1,['message'],['message']
Integrability,"**Set name for storing Umap coordinates explicitly in tl.umap and pl.umap**; tl.umap(..., x_umap = ""X_umap""); pl.umap(..., x_umap = ""X_umap""). Sometimes it would be helpful to specify the adata obsm field for storing the umap coordinates.; For example : ; -if I want to compute and plot the umap for the raw data and afterwards for the integrated or in any way modified data. The first x_umap is going to be overwritten and needs to be computed again, if I need to plot the first step again.; So it would be cool to enable a workflow like the following:. ```; tl.umap(adata, ..., x_umap = ""X_umap_raw""); # do some operations ...; tl.umap(adata, ..., x_umap = ""X_umap_mod). # now after computation I might need to take a look on both umaps again, or plot them in direct comparison; pl.umap(adata, ..., x_umap = ""X_umap_raw""); pl.umap(adata, ..., x_umap = ""X_umap_mod""). ```. I hope I was able to explain what I mean and did not oversee such feature or misunderstood the usage.; All the best ; maflot",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2245:336,integrat,integrated,336,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2245,1,['integrat'],['integrated']
Integrability,"**Update** Note sure if it is a problem with our script. However, if colors are not found, can it use default color-map and let us plot after raising a warning?. Log is following. ~Working on a PR.~ This issue is for reference. Could not upload source file since it depends on data file which are huge. ```; (Py36) pragati@wasabi-simons ~/Work/scanpy_exp $ python planaria.py ; scanpy==1.3.2+4.g7c9fb1a anndata==0.6.11 numpy==1.14.6 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 ; ... storing 'clusters' as categorical; computing tSNE; using data matrix X directly; using the 'MulticoreTSNE' package by Ulyanov (2017); finished (0:02:39.15); Traceback (most recent call last):; File ""/home/pragati/Py36/lib/python3.6/site-packages/matplotlib/colors.py"", line 158, in to_rgba; rgba = _colors_full_map.cache[c, alpha]; KeyError: ('grey80', None). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/home/pragati/Py36/lib/python3.6/site-packages/matplotlib/axes/_axes.py"", line 4210, in scatter; colors = mcolors.to_rgba_array(c); File ""/home/pragati/Py36/lib/python3.6/site-packages/matplotlib/colors.py"", line 259, in to_rgba_array; result[i] = to_rgba(cc, alpha); File ""/home/pragati/Py36/lib/python3.6/site-packages/matplotlib/colors.py"", line 160, in to_rgba; rgba = _to_rgba_no_colorcycle(c, alpha); File ""/home/pragati/Py36/lib/python3.6/site-packages/matplotlib/colors.py"", line 204, in _to_rgba_no_colorcycle; raise ValueError(""Invalid RGBA argument: {!r}"".format(orig_c)); ValueError: Invalid RGBA argument: 'grey80'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""planaria.py"", line 47, in <module>; sc.pl.tsne(adata, color='clusters', legend_loc='on data', legend_fontsize=5, save='_full'); File ""/home/pragati/Py36/lib/python3.6/site-packages/scanpy/plotting/tools/scatterplots.py"", line 47, in tsne; return plot_scatter(adata, basis='tsne', **k",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/286:266,depend,depends,266,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286,1,['depend'],['depends']
Integrability,"**failed resolving arguments***); 375 if not downsample or obs_chunk_size > downsample or adata.n_obs < downsample:; 376 logger.info(f""Running IncrementalPCA without downsampling""); --> 377 sc.tl.pca(adata, n_comps=ndim, chunked=True,; 378 chunk_size=obs_chunk_size); 379 else: # downsample; 380 logger.info(f""Running IncrementalPCA with downsample = {downsample}""). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/scanpy/preprocessing/_pca.py:255, in pca(***failed resolving arguments***); 253 for chunk, _, _ in adata_comp.chunked_X(chunk_size):; 254 chunk = chunk.toarray() if issparse(chunk) else chunk; --> 255 pca_.partial_fit(chunk); 257 for chunk, start, end in adata_comp.chunked_X(chunk_size):; 258 chunk = chunk.toarray() if issparse(chunk) else chunk. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/base.py:1473, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs); 1466 estimator._validate_params(); 1468 with config_context(; 1469 skip_parameter_validation=(; 1470 prefer_skip_nested_validation or global_skip_validation; 1471 ); 1472 ):; -> 1473 return fit_method(estimator, *args, **kwargs). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/decomposition/_incremental_pca.py:304, in IncrementalPCA.partial_fit(self, X, y, check_input); 298 raise ValueError(; 299 ""n_components=%r invalid for n_features=%d, need ""; 300 ""more rows than columns for IncrementalPCA ""; 301 ""processing"" % (self.n_components, n_features); 302 ); 303 elif not self.n_components <= n_samples:; --> 304 raise ValueError(; 305 ""n_components=%r must be less or equal to ""; 306 ""the batch number of samples ""; 307 ""%d."" % (self.n_components, n_samples); 308 ); 309 else:; 310 self.n_components_ = self.n_components. ValueError: n_components=100 must be less or equal to the batch number of samples 77; ```. To fix this bug, ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3227:1301,wrap,wrapper,1301,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3227,1,['wrap'],['wrapper']
Integrability,"*kwargs); 157 except Exception as e:. /opt/conda/lib/python3.7/site-packages/anndata/_io/h5ad.py in read_group(group); 531 if encoding_type:; --> 532 EncodingVersions[encoding_type].check(; 533 group.name, group.attrs[""encoding-version""]. /opt/conda/lib/python3.7/enum.py in __getitem__(cls, name); 356 def __getitem__(cls, name):; --> 357 return cls._member_map_[name]; 358 . KeyError: 'dict'. During handling of the above exception, another exception occurred:. AnnDataReadError Traceback (most recent call last); <ipython-input-20-38a594ec7d06> in <module>; ----> 1 adata_ast=sc.read_h5ad('../../data_processed/Leng_2020/adata_ast.h5ad'). /opt/conda/lib/python3.7/site-packages/anndata/_io/h5ad.py in read_h5ad(filename, backed, as_sparse, as_sparse_fmt, chunk_size); 424 d[k] = read_dataframe(f[k]); 425 else: # Base case; --> 426 d[k] = read_attribute(f[k]); 427 ; 428 d[""raw""] = _read_raw(f, as_sparse, rdasp). /opt/conda/lib/python3.7/functools.py in wrapper(*args, **kw); 838 '1 positional argument'); 839 ; --> 840 return dispatch(args[0].__class__)(*args, **kw); 841 ; 842 funcname = getattr(func, '__name__', 'singledispatch function'). /opt/conda/lib/python3.7/site-packages/anndata/_io/utils.py in func_wrapper(elem, *args, **kwargs); 161 parent = _get_parent(elem); 162 raise AnnDataReadError(; --> 163 f""Above error raised while reading key {elem.name!r} of ""; 164 f""type {type(elem)} from {parent}.""; 165 ). AnnDataReadError: Above error raised while reading key '/layers' of type <class 'h5py._hl.group.Group'> from /.; adata_ast=sc.read_h5ad('../../data_processed/Leng_2020/adata_ast.h5ad'); ```. <details>; <summary>Versions</summary>. Package Version; ----------------------- ------------; absl-py 1.1.0; aiohttp 3.8.1; aiosignal 1.2.0; anndata 0.7.5; anndata2ri 1.0.6; annoy 1.17.0; argon2-cffi 21.3.0; argon2-cffi-bindings 21.2.0; asn1crypto 1.4.0; async-timeout 4.0.2; asynctest 0.13.0; attrs 20.3.0; backcall 0.2.0; beautifulsoup4 4.11.1; bleach 5.0.0; boto3 1.17.66; botocore",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1351#issuecomment-1146346336:1367,wrap,wrapper,1367,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351#issuecomment-1146346336,2,['wrap'],['wrapper']
Integrability,", delimiter, first_column_names, backup_url, cache, cache_compression, suppress_cache_warning, **kwargs); 698 if ext in {'h5', 'h5ad'}:; 699 if sheet is None:; --> 700 return read_h5ad(filename, backed=backed); 701 else:; 702 logg.debug(f'reading sheet {sheet} from file {filename}'). ~/.miniconda3/envs/cellrank2/lib/python3.8/site-packages/anndata/_io/h5ad.py in read_h5ad(filename, backed, as_sparse, as_sparse_fmt, chunk_size); 427 _clean_uns(d) # backwards compat; 428 ; --> 429 return AnnData(**d); 430 ; 431 . TypeError: __init__() got an unexpected keyword argument 'batch.names'. ```. #### Versions. <details>. -----; anndata 0.7.4; scanpy 1.6.0; sinfo 0.3.1; -----; PIL 8.0.1; absl NA; anndata 0.7.4; autoreload NA; backcall 0.2.0; cellrank 1.0.0; cffi 1.14.3; cycler 0.10.0; cython_runtime NA; dateutil 2.8.1; decorator 4.4.2; docrep 0.3.1; future_fstrings NA; get_version 2.1; h5py 2.10.0; igraph 0.8.3; ipykernel 5.3.4; ipython_genutils 0.2.0; jax 0.2.5; jaxlib 0.1.56; jedi 0.17.2; joblib 0.17.0; kiwisolver 1.3.1; lapack NA; legacy_api_wrap 1.2; leidenalg 0.8.1; llvmlite 0.34.0; louvain 0.7.0; matplotlib 3.3.2; mpl_toolkits NA; natsort 7.0.1; numba 0.51.2; numexpr 2.7.1; numpy 1.19.4; opt_einsum v3.3.0; packaging 20.4; pandas 1.1.4; parso 0.7.1; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; progressbar 3.53.1; prompt_toolkit 3.0.8; psutil 5.7.3; ptyprocess 0.6.0; pygam 0.8.0; pygments 2.7.2; pyparsing 2.4.7; python_utils NA; pytz 2020.4; scanpy 1.6.0; scipy 1.5.3; scvelo 0.2.2; setuptools_scm NA; sinfo 0.3.1; six 1.15.0; sklearn 0.23.2; sphinxcontrib NA; storemagic NA; tables 3.6.1; texttable 1.6.3; threadpoolctl 2.1.0; tornado 6.1; traitlets 5.0.5; wcwidth 0.2.5; wrapt 1.12.1; zmq 19.0.2; -----; IPython 7.19.0; jupyter_client 6.1.7; jupyter_core 4.6.3; notebook 6.1.4; -----; Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]; Linux-5.8.0-3-amd64-x86_64-with-glibc2.10; 8 logical CPU cores; -----; Session information updated at 2020-11-03 13:36. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1480:3612,wrap,wrapt,3612,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1480,1,['wrap'],['wrapt']
Integrability,", uns_merge, label, keys, index_unique, fill_value, pairwise); 813 # Annotation for other axis; 814 alt_annot = merge_dataframes(; --> 815 [getattr(a, alt_dim) for a in adatas], alt_indices, merge; 816 ); 817 . ~/.local/lib/python3.7/site-packages/anndata/_core/merge.py in merge_dataframes(dfs, new_index, merge_strategy); 524 ; 525 def merge_dataframes(dfs, new_index, merge_strategy=merge_unique):; --> 526 dfs = [df.reindex(index=new_index) for df in dfs]; 527 # New dataframe with all shared data; 528 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/.local/lib/python3.7/site-packages/anndata/_core/merge.py in <listcomp>(.0); 524 ; 525 def merge_dataframes(dfs, new_index, merge_strategy=merge_unique):; --> 526 dfs = [df.reindex(index=new_index) for df in dfs]; 527 # New dataframe with all shared data; 528 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/.local/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs); 322 @wraps(func); 323 def wrapper(*args, **kwargs) -> Callable[..., Any]:; --> 324 return func(*args, **kwargs); 325 ; 326 kind = inspect.Parameter.POSITIONAL_OR_KEYWORD. ~/.local/lib/python3.7/site-packages/pandas/core/frame.py in reindex(self, *args, **kwargs); 4770 kwargs.pop(""axis"", None); 4771 kwargs.pop(""labels"", None); -> 4772 return super().reindex(**kwargs); 4773 ; 4774 @deprecate_nonkeyword_arguments(version=None, allowed_args=[""self"", ""labels""]). ~/.local/lib/python3.7/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs); 4817 # perform the reindex on the axes; 4818 return self._reindex_axes(; -> 4819 axes, level, limit, tolerance, method, fill_value, copy; 4820 ).__finalize__(self, method=""reindex""); 4821 . ~/.local/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy); 4596 if index is not None:; 4597 frame = frame._reindex_index(; -> 4598 index, method, copy, level, fill_value, limit, tolerance; ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2364:3331,wrap,wrapper,3331,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2364,3,['wrap'],"['wrapper', 'wraps']"
Integrability,", which gives scanpy users an obvious advantage. . I've learned so much using this package and it has allowed me to display my data in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to access the current version of this software? . I think that major partitions identified with CytOF or scRNA-seq can be linked together providing a coarse-grained mechanism to demonstrate how heterogeneity identified with each technique relates to each other based on a given experimental time point. . (3) Histogram integration in the plotting api for QC metrics would be helpful. While scatter plots and violin plots are effective, I find myself wanting to make cut",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/510:1366,Integrat,Integration,1366,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510,1,['Integrat'],['Integration']
Integrability,"- [ ] I have checked that this issue has not already been reported.; - [ ] I have confirmed this bug exists on the latest version of scanpy. ---; **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python; import scanpy as sc; ...; ...; test_data = sc.pp.regress_out(test_data,['n_count'], copy=True); sc.pp.scale(test_data); sc.tl.pca(test_data,n_comps=30, use_highly_variable=True); sc.pp.neighbors(test_data,n_neighbors=20). ```. ```output print. ...storing 'feature_types' as categorical; OMP: Info #276:omp_set_nested routine deprecated, please use omp_set_max_active_levels instead. ```. #### Versions. my python version is 3.8; and the versions of scanpy I have tried were 1.8.2 and 1.9.1. I guess that the reason inducing the errors was the version of python is too high. . How can I solve the problem?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2404:733,rout,routine,733,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2404,1,['rout'],['routine']
Integrability,"- [ ] I have checked that this issue has not already been reported.; - [ ] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). I am learning the example of [Integrating data using ingest and BBKNN](https://scanpy-tutorials.readthedocs.io/en/latest/integrating-data-using-ingest.html). When I run code ; ```python; sc.tl.umap(adata_ref); ```; I get; ```pytb; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-13-0e548e19df3a> in <module>; 1 sc.pp.pca(adata_ref); 2 sc.pp.neighbors(adata_ref); ----> 3 sc.tl.umap(adata_ref). ~/miniconda3/envs/tf/lib/python3.6/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key); 171 neigh_params.get('metric', 'euclidean'),; 172 neigh_params.get('metric_kwds', {}),; --> 173 verbose=settings.verbosity > 3,; 174 ); 175 elif method == 'rapids':. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'; ```. #### Versions. <details>. scanpy==1.6.0 anndata==0.7.5 umap==0.5.0 numpy==1.19.5 scipy==1.5.4 pandas==1.1.5 scikit-learn==0.24.0 statsmodels==0.12.1 python-igraph==0.8.3 leidenalg==0.8.3. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1579:518,Integrat,Integrating,518,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579,2,"['Integrat', 'integrat']","['Integrating', 'integrating-data-using-ingest']"
Integrability,"- [X ] I have checked that this issue has not already been reported.; - [X ] I have confirmed this bug exists on the latest version of scanpy.; - [X ] (optional) I have confirmed this bug exists on the master branch of scanpy. I'm working on comit `63b42e4b` (latest master).; I'm not sure if intended or not but it seems like it would be usefull if one were able to ingest data that don't share 100% of all features. ### Minimal code sample (that we can copy&paste without having any data). ```python; adata = sc.datasets.paul15(); sc.pp.pca(adata_ref); sc.pp.neighbors(adata_ref); sc.tl.leiden(adata_ref); adata = adata_ref[:, :1000].copy() # assume adata_ref has more than 1000 genes.; sc.tl.ingest(adata, adata_ref, obs='leiden'); ```. Error message; ```pytb; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-37-b3cd11e67810> in <module>; ----> 1 sc.tl.ingest(adata, adata_ref, obs='leiden'). ~/projects/scanpy/scanpy/tools/_ingest.py in ingest(adata, adata_ref, obs, embedding_method, labeling_method, neighbors_key, inplace, **kwargs); 125 ; 126 ing = Ingest(adata_ref, neighbors_key); --> 127 ing.fit(adata); 128 ; 129 for method in embedding_method:. ~/projects/scanpy/scanpy/tools/_ingest.py in fit(self, adata_new); 437 ; 438 if not ref_var_names.equals(new_var_names):; --> 439 raise ValueError(; 440 'Variables in the new adata are different '; 441 'from variables in the reference adata'. ValueError: Variables in the new adata are different from variables in the reference adata; ```. --- . #### Versions. <details>. sc.logging.print_header(); scanpy==1.8.0.dev78+gc488909a anndata==0.7.6 umap==0.5.0 numpy==1.19.4 scipy==1.5.4 pandas==1.1.4 scikit-learn==0.23.2 statsmodels==0.12.1 python-igraph==0.8.3 leidenalg==0.8.3 pynndescent==0.5.1. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2001:746,message,message,746,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2001,1,['message'],['message']
Integrability,"- [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. Hello, according to the documentation, the simulation of doublets with scrublet works best with un-normalized counts. The current wrapper implementation however uses a layer to hold the raw data, which then gets normalized anyway. . Please have in mind, that this is the first issue I have ever opened. If I made any grievous mistakes please inform me of such. . ---. ### Minimal code sample:. ```python; import scanpy; adata = scanpy.datasets.pbmc3k(); scanpy.external.pp.scrublet(adata); ```; and around line 177 in scanpy.external.pp.scrublet we need to 'observe' the values:; ```python; adata_obs.layers['raw'] = adata_obs.X; print(adata_obs.layers['raw']); pp.normalize_total(adata_obs) <--- currently normalizes all layers and X; print(adata_obs.layers['raw']); ```; ---; ### Impact; The 'raw' layer is later used in line 194 of scanpy.external.pp.scrublet to simulate doublets, however, it doesn't contain raw data anymore. ```python; adata_sim = scrublet_simulate_doublets(; adata_obs,; layer='raw',; sim_doublet_ratio=sim_doublet_ratio,; synthetic_doublet_umi_subsampling=synthetic_doublet_umi_subsampling,; ); ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1957:359,wrap,wrapper,359,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1957,1,['wrap'],['wrapper']
Integrability,"- [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---; # Introduction. Hi,. so this is a weird one and I could not track it down yet.; The Schillerlab people have a workstation known as ""agando"". On this workstation the full environment is installed globally and shared by all users. I am looking to change that. # The issue. When calculating the `sc.tl.marker_gene_overlap` I get the expected and reasonable results on the agando environment, but completely rubbish results when running the same code with a fresh Conda environment and the latest dependencies installed. ![image](https://user-images.githubusercontent.com/21954664/106739402-659dfb80-6619-11eb-84f1-e75abfa6167d.png). Top = new, trash results; Bottom = old=agando expected results. The old environment has:. ```; scanpy==1.6.1.dev110+gb4234d81 anndata==0.7.4 umap==0.4.6 numpy==1.19.0 scipy==1.5.1 pandas==1.1.5 scikit-learn==0.23.1 statsmodels==0.12.1 python-igraph==0.8.0 louvain==0.6.1 leidenalg==0.8.3; ```. The new environment has ; ```; scanpy==1.6.1 anndata==0.7.5 umap==0.4.6 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.1 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3; ```; Full new conda environment:; ```; name: single_cell_analysis; channels:; - defaults; dependencies:; - _libgcc_mutex=0.1=main; - argon2-cffi=20.1.0=py37h7b6447c_1; - async_generator=1.10=py37h28b3542_0; - attrs=20.3.0=pyhd3eb1b0_0; - backcall=0.2.0=pyhd3eb1b0_0; - bleach=3.3.0=pyhd3eb1b0_0; - ca-certificates=2021.1.19=h06a4308_0; - certifi=2020.12.5=py37h06a4308_0; - cffi=1.14.4=py37h261ae71_0; - dbus=1.13.18=hb2f20db_0; - decorator=4.4.2=pyhd3eb1b0_0; - defusedxml=0.6.0=py_0; - entrypoints=0.3=py37_0; - expat=2.2.10=he6710b0_2; - fontconfig=2.13.0=h9420a91_0; - freetype=2.10.4=h5ab3b9f_0; - glib=2.66.1=h92f7085_0; - gst-plugins",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1625:722,depend,dependencies,722,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625,1,['depend'],['dependencies']
Integrability,"- [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. This is probably a bug in my thinking, but naively I thought that `sc.pp.normalize_total()` normalizes counts per cell, thus allowing comparison of different cells by correcting for variable sequencing depth. However, the log transformation applied after normalisation seems to upset this relationship, example below. Why is this not problematic?. Incidentally, I first noticed this on my real biological dataset, not the toy example below. Edit: [relevant paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6215955/). > We can show, mathematically, that if we normalize expression profiles to have the same mean across cells, the mean after the equation [log] transformation used for RNA-Seq data will not be the same, and it will depend on the detection rate... And this [one](https://www.biorxiv.org/content/10.1101/404962v1.full):. > One issue of particular interest is that the mean of the log-counts is not generally the same as the log-mean count [1]. This is problematic in scRNA-seq contexts where the log-transformation is applied to normalized expression data. ---. ### Minimal code sample. ```python; >>> from anndata import AnnData; >>> import scanpy as sc; >>> import numpy as np; >>> adata = AnnData(np.array([[3, 3, 3, 6, 6],[1, 1, 1, 2, 2],[1, 22, 1, 2, 2], ])); >>> X_norm = sc.pp.normalize_total(adata, target_sum=1, inplace=False)['X']; >>> X_norm_log = np.log1p(X_norm); >>> X_norm_again = np.expm1(X_norm_log); >>> adata.X.sum(axis=1); array([21., 7., 28.], dtype=float32) # Different counts for each cell; >>> X_norm.sum(axis=1); array([1., 1., 1.], dtype=float32) # Normalisation means same counts for each cell; >>> X_norm_log.sum(axis=1); array([0.90322304, 0.90322304, 0.7879869 ], dtype=float32) # <<< Interested in this! Different counts for each ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1364:957,depend,depend,957,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364,1,['depend'],['depend']
Integrability,"- [x] Additional function parameters / changed functionality / changed defaults?. At the moment when we are plotting data points in e.g., `sc.pl.umap()` with `color='covariate'` we determine the plotting order in two ways:; 1. if `'covariate'` is continuous the highest values are plotted on top, to showcase the peaks of the distribution;; 2. if `'covariate'` is a categorical variable, the order of `adata.obs_names` is used (i believe). As we often concatenate datasets after integration or loading from multiple sources, covariates we plot are usually not randomly ordered here. I think the first case is fine (and it can be turned off), but we should probably not be doing case 2. Instead, it would be good if the default was to plot in a random order unless the covariate is ordered internally (I believe this is already taken into account, but not sure). I have come across this issue several times now, and we're not solving this in a good way imo. Fabian has mentioned this to me several times as well. What do you think @fidelram @ivirshup ?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1263:479,integrat,integration,479,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1263,1,['integrat'],['integration']
Integrability,"- [x] Additional function parameters / changed functionality / changed defaults?. I recently wrote up a parallelized implementation of the Mann-Whitney U test, for my own use ([gist is here](https://gist.github.com/jamestwebber/38ab26d281f97feb8196b3d93edeeb7b)). For the types of tests we tend to do in scRNAseq (lots of different features, 2d arrays) it basically scales with the number of cores you can throw at it. When you're doing a lot of tests this is very nice!. Given that `scanpy` already has a dependency on `numba` this would be a pretty simple thing to add, if you want to do so. Thought I would just point it out!. - James",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2060:506,depend,dependency,506,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2060,1,['depend'],['dependency']
Integrability,"- [x] Additional function parameters / changed functionality / changed defaults?; - [x] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [x] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. I'd really like a smarter d/e method to be accessible easily from Scanpy, one that allows proper treatment of covariates etc. MAST is obviously very popular, but fiddly to integrate from R. I see diffxpy mentioned about the place here, and see it's an in-house tool of yours. Is there a reason it's not been integrated already? If nobody's working on it, shall I take a crack at it?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1955:538,integrat,integrate,538,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1955,2,['integrat'],"['integrate', 'integrated']"
Integrability,"- [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample. RecursionError when using sc.tl.ingest with Python 3.10. Downgrading to Python 3.8 or 3.9 fixes the issue. . Recreate by just running scanpy's own [Integrating data using ingest and BBKNN](https://scanpy-tutorials.readthedocs.io/en/latest/integrating-data-using-ingest.html#Integrating-data-using-ingest-and-BBKNN) tutorial. ![image](https://user-images.githubusercontent.com/4848896/160018184-7609a89a-adc5-4094-9bc3-7f2d894355f4.png). ```pytb; ---------------------------------------------------------------------------; RecursionError Traceback (most recent call last); Input In [6], in <cell line: 1>(); ----> 1 sc.tl.ingest(adata, adata_ref, obs='louvain'). File ~/miniconda3/envs/test/lib/python3.10/site-packages/scanpy/tools/_ingest.py:133, in ingest(adata, adata_ref, obs, embedding_method, labeling_method, neighbors_key, inplace, **kwargs); 130 ing.map_embedding(method); 132 if obs is not None:; --> 133 ing.neighbors(**kwargs); 134 for i, col in enumerate(obs):; 135 ing.map_labels(col, labeling_method[i]). File ~/miniconda3/envs/test/lib/python3.10/site-packages/scanpy/tools/_ingest.py:472, in Ingest.neighbors(self, k, queue_size, epsilon, random_state); 469 if self._use_pynndescent:; 470 self._nnd_idx.search_rng_state = rng_state; --> 472 self._indices, self._distances = self._nnd_idx.query(test, k, epsilon); 474 else:; 475 from umap.utils import deheap_sort. File ~/miniconda3/envs/test/lib/python3.10/site-packages/pynndescent/pynndescent_.py:1595, in NNDescent.query(self, query_data, k, epsilon); 1564 """"""Query the training graph_data for the k nearest neighbors; 1565 ; 1566 Parameters; (...); 1592 training graph_data.; 1593 """"""; 1594 if not hasattr(self, ""_search_graph""):; -> 1595 self._init_search_graph(); 1597 if n",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2191:402,Integrat,Integrating,402,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2191,3,"['Integrat', 'integrat']","['Integrating', 'Integrating-data-using-ingest-and-BBKNN', 'integrating-data-using-ingest']"
Integrability,"- [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data); Write any anndata with pearson residuals in uns; ```python; ad_all.write(filename='output/10x_h5/ad_all_2cello.h5ad'); ```; The pearson_residual_df looks like this, with 38291 rows (obs) and 5000 columns (features) :; ```python; {'theta': 100,; 'clip': None,; 'computed_on': 'adata.X',; 'pearson_residuals_df': gene_name A2M AADACL2-AS1 AAK1 ABCA1 \; barcode ; GAACGTTCACACCGAC-1-placenta_81 -1.125285 -1.159130 -3.921314 -2.533474 ; TATACCTGTTAGCTAC-1-placenta_81 -1.091364 3.267127 -1.806667 -2.109586 ; CTCAAGAGTGACTGTT-1-placenta_81 -1.074943 12.272920 -1.948798 -2.735791 ; TTCATTGTCACGAACT-1-placenta_81 -1.098699 -1.131765 3.481171 4.472371 ; TATCAGGCAGCTCATA-1-placenta_81 -1.107734 -1.141064 -0.571775 -2.813671 ; ... ... ... ... ... ; CACAACATCGGCGATC-1-placenta_314 -0.115585 -0.119107 -0.434686 -0.303945 ; AGCCAGCGTGCCCAGT-1-placenta_314 -0.097424 -0.100394 -0.366482 -0.256219 ; CCGGTGAGTGTTCGAT-1-placenta_314 -0.110334 -0.113696 -0.414971 -0.290148 ; AGGTCATAGCCTGACC-1-placenta_314 -0.115585 -0.119107 -0.434686 -0.303945 ; TTTATGCCAAAGGGTC-1-placenta_314 -0.112876 -0.116316 -0.424515 -0.296827 ; ```. ```pytb; Unable to create attribute (object header message is too large). Above error raised while writing key 'pearson_residuals_df' of <class 'h5py._hl.group.Group'> to /; ```. #### Versions. <details>. scanpy==1.9.1 anndata==0.8.0 umap==0.5.2 numpy==1.21.5 scipy==1.8.0 pandas==1.4.1 scikit-learn==1.0.2 statsmodels==0.13.2 python-igraph==0.9.9 pynndescent==0.5.6. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2383:1659,message,message,1659,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2383,1,['message'],['message']
Integrability,"- [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. Hello, . This may be a problem outside the realm of scanpy functionality, but I thought it best to bring up in case it is relevant or in case anyone here has seen something before while trying to use scanpy. It looks like I can having trouble importing a dependency of the sc.pp.regress() function. I don't think the data here is relevant, just something in my set up. I tried updating all the libraries so that everything is up to date. This problem just started occurring today (2/10/21) and had no issue yesterday, so I figure it was a change on the scanpy end that I didn't keep up with proprely. ```python; sc.pp.regress_out(merged_adata, ['pct_counts_mt', 'pct_counts_rp']); ```. yields the following error. ```pytb; /broad/software/free/Linux/redhat_7_x86_64/pkgs/anaconda3_2020.07/lib/python3.8/site-packages/statsmodels/tsa/filters/filtertools.py in <module>; 16 import scipy.fftpack as fft; 17 from scipy import signal; ---> 18 from scipy.signal.signaltools import _centered as trim_centered; 19 ; 20 from statsmodels.tools.validation import array_like, PandasWrapper. ImportError: cannot import name '_centered' from 'scipy.signal.signaltools' (/home/unix/jjeang/.local/lib/python3.8/site-packages/scipy/signal/signaltools.py); ```. #### Versions. <details>. scanpy==1.6.0 anndata==0.7.8 umap==0.3.10 numpy==1.22.2 scipy==1.8.0 pandas==1.2.5 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.9.9 louvain==0.7.0 leidenalg==0.8.0. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2137:484,depend,dependency,484,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2137,1,['depend'],['dependency']
Integrability,"- [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. In short `sc.pp.scale()` throws an error when `adata.X` is a dask array solely because `sc.pp._simple.scale` doesn't have the `sc.pp._simple.scale_array` function registered for a `dask.array.Array` type. Adding `@scale.register(da.Array)` [here](https://github.com/scverse/scanpy/blob/f03c5b407412de447480733a1a1a0e33e0c871d2/scanpy/preprocessing/_simple.py#LL758C7-L758C7) would fix that, but considering `dask.array` is [only an optional dependency,](https://github.com/scverse/scanpy/blob/f03c5b407412de447480733a1a1a0e33e0c871d2/scanpy/preprocessing/_simple.py#L32) there would have to be a conditional to wrap the function decoration. This brings me to the larger issue, is scanpy supposed to or working toward supporting dask arrays completely? I'm new to scanpy and I'm not sure if this is a bug report or an enhancement request. I could submit a pull request for this one issue but I'm curious if I'll run into many such issues as I dive in further and trying to figure out if there's existing momentum in this direction or whether I should be following some other parallelization strategy. I got here by following [AnnData's guide on using dask and zarr](https://anndata.readthedocs.io/en/latest/tutorials/notebooks/%7Bread%2Cwrite%7D_dispatched.html) to try to parallelize processing of a large scRNA-seq file. I come from a microscopy data analysis and ML background where my image data is stored in S3 hosted zarr arrays (in an OME-NGFF schema), handled by using dask arrays wrapped in xarray DataArrays, and parallelized across compute using ray. It would be nice to use a similar stack (dropping in anndata for xarray) for sc-seq analyses. ### Minimal code sample (that we can copy&paste without having any data). ```python; import zarr; import anndata as a",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2491:670,depend,dependency,670,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2491,2,"['depend', 'wrap']","['dependency', 'wrap']"
Integrability,"- [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. I clone the scanpy repository and I am on commit b69015e. I follow the instructions for a developmental install here:; https://scanpy.readthedocs.io/en/stable/installation.html#dev-install-instructions . ### Minimal code sample (that we can copy&paste without having any data). ```bash; pip install beni; beni pyproject.toml > environment.yml; conda env create -f environment.yml; ```. this is the error I get. ```; Collecting package metadata (repodata.json): done; Solving environment: failed. ResolvePackageNotFound:; - seaborn-split; ```. #### `environment.yml`. Here is the content of `environment.yml` which contains the strange package `seaborn-split`. So maybe the issue is upstream with beni?. <details>. ```; channels:; - conda-forge; dependencies:; - pip:; - flit; - bbknn; - scanpydoc>=0.7.4; - harmonypy; - magic-impute>=2.0; - cudf>=0.9; - cuml>=0.9; - cugraph>=0.9; - scanorama; - scrublet; - python>=3.7; - pip; - anndata>=0.7.4; - numpy>=1.17.0; - matplotlib-base>=3.1.2; - pandas>=0.21; - scipy>=1.4; - seaborn-split; - h5py>=2.10.0; - pytables; - tqdm; - scikit-learn>=0.22; - statsmodels>=0.10.0rc2; - patsy; - networkx>=2.3; - natsort; - joblib; - numba>=0.41.0; - umap-learn>=0.3.10; - packaging; - sinfo; - setuptools-scm; - black>=20.8b1; - docutils; - sphinx<4.2,>=4.1; - sphinx_rtd_theme>=0.3.1; - python-igraph; - leidenalg; - louvain!=0.6.2,>=0.6; - scikit-misc>=0.1.3; - pytest>=4.4; - pytest-nunit; - dask-core!=2.17.0; - fsspec; - zappy; - - zarr; - profimp; - flit-core; name: scanpy; ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2144:974,depend,dependencies,974,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2144,1,['depend'],['dependencies']
Integrability,"- [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. When scanpy gets installed with the latest version of `importlib_metadata` (2.0), the ; command `sc.logging.print_versions()` fails with the following error: . ```pytb; WARNING: If you miss a compact list, please try `print_header`!; Traceback (most recent call last):; File ""/home/sturm/anaconda3/envs/scanpy_test/lib/python3.7/site-packages/sinfo/main.py"", line 195, in sinfo; mod_version = _find_version(mod.__version__); AttributeError: module 'importlib_metadata' has no attribute '__version__'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/home/sturm/anaconda3/envs/scanpy_test/lib/python3.7/site-packages/scanpy/logging.py"", line 161, in print_versions; sinfo(dependencies=True); File ""/home/sturm/anaconda3/envs/scanpy_test/lib/python3.7/site-packages/sinfo/main.py"", line 198, in sinfo; mod_version = _find_version(mod.version); File ""/home/sturm/anaconda3/envs/scanpy_test/lib/python3.7/site-packages/sinfo/main.py"", line 42, in _find_version; return mod_version_attr(); TypeError: version() missing 1 required positional argument: 'distribution_name'; ```. According to the `importlib_metadata` changelog, the `__version__` attribute has been removed from the package: . ```; =========================; importlib_metadata NEWS; =========================. v2.0.0; ======. * ``importlib_metadata`` no longer presents a; ``__version__`` attribute. Consumers wishing to; resolve the version of the package should query it; directly with; ``importlib_metadata.version('importlib-metadata')``.; Closes #71.; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1437:1000,depend,dependencies,1000,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1437,1,['depend'],['dependencies']
Integrability,"- [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [x] (optional) I have confirmed this bug exists on the master branch of scanpy. When running the `Integrating spatial data with scRNA-seq using scanorama` [tutorial](https://scanpy-tutorials.readthedocs.io/en/latest/spatial/integration-scanorama.html#Data-integration-and-label-transfer-from-scRNA-seq-dataset) with the provided sample data, the following error occurs. This may be related to the following warning I also see. `<string>:6: VisibleDeprecationWarning: Creating an ndarray from nested sequences exceeding the maximum number of dimensions of 32 is deprecated. If you mean to do this, you must specify 'dtype=object' when creating the ndarray.`. How can I overcome this issue?. Example code below has how I downloaded the data in the `Data integration and label transfer from scRNA-seq dataset` section of the tutorial and then just the code block where the error actually occurs. ---. ### Minimal code sample (that we can copy&paste without having any data). ![scanorama_error](https://user-images.githubusercontent.com/52245296/154322971-c45606d2-54d7-42da-8ac6-85f91359e3c8.png). ```python. import subprocess; from pathlib import Path. if Path('./downloaded_data/adata_processed.h5ad').is_file():; print(""Data previously downloaded, skipping to next step""); else:; subprocess.run(['wget', '-O', './downloaded_data/adata_processed.h5ad', 'https://hmgubox.helmholtz-muenchen.de/f/4ef254675e2a41f89835/?dl=1']). adata_cortex = sc.read(""./downloaded_data/adata_processed.h5ad""). embedding_anterior = np.concatenate(integrated_anterior, axis=0); adata_cortex_anterior.obsm[""scanorama_embedding""] = embedding_anterior. embedding_posterior = np.concatenate(integrated_posterior, axis=0); adata_cortex_posterior.obsm[""scanorama_embedding""] = embedding_posterior; ```. ```pytb. <string>:6: VisibleDeprecationWarning: Creating an ndarray from nested se",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2143:242,Integrat,Integrating,242,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2143,4,"['Integrat', 'integrat']","['Integrating', 'integration', 'integration-and-label-transfer-from-scRNA-seq-dataset', 'integration-scanorama']"
Integrability,"------------------; AxisError Traceback (most recent call last); <ipython-input-55-c0d016811ded> in <module>; ----> 1 sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). ~/anaconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/preprocessing/_simple.py in regress_out(adata, keys, n_jobs, copy); 817 # split the adata.X matrix by columns in chunks of size n_chunk; 818 # (the last chunk could be of smaller size than the others); --> 819 chunk_list = np.array_split(adata.X, n_chunks, axis=1); 820 if variable_is_categorical:; 821 regressors_chunk = np.array_split(regressors, n_chunks, axis=1). <__array_function__ internals> in array_split(*args, **kwargs). ~/anaconda3/envs/scanpy/lib/python3.6/site-packages/numpy/lib/shape_base.py in array_split(ary, indices_or_sections, axis); 782 ; 783 sub_arys = []; --> 784 sary = _nx.swapaxes(ary, axis, 0); 785 for i in range(Nsections):; 786 st = div_points[i]. <__array_function__ internals> in swapaxes(*args, **kwargs). ~/anaconda3/envs/scanpy/lib/python3.6/site-packages/numpy/core/fromnumeric.py in swapaxes(a, axis1, axis2); 595 ; 596 """"""; --> 597 return _wrapfunc(a, 'swapaxes', axis1, axis2); 598 ; 599 . ~/anaconda3/envs/scanpy/lib/python3.6/site-packages/numpy/core/fromnumeric.py in _wrapfunc(obj, method, *args, **kwds); 56 bound = getattr(obj, method, None); 57 if bound is None:; ---> 58 return _wrapit(obj, method, *args, **kwds); 59 ; 60 try:. ~/anaconda3/envs/scanpy/lib/python3.6/site-packages/numpy/core/fromnumeric.py in _wrapit(obj, method, *args, **kwds); 45 except AttributeError:; 46 wrap = None; ---> 47 result = getattr(asarray(obj), method)(*args, **kwds); 48 if wrap:; 49 if not isinstance(result, mu.ndarray):. AxisError: axis1: axis 1 is out of bounds for array of dimension 0; ```. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; > scanpy==1.4.5.post3 anndata==0.7.1 umap==0.3.10 numpy==1.18.1 scipy==1.4.1 pandas==0.25.3 scikit-learn==0.22.1 statsmodels==0.11.0 python-igraph==0.7.1 louvain==0.6.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1010:2089,wrap,wrap,2089,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1010,2,['wrap'],['wrap']
Integrability,"-4-5d47edb05ae7> in <module>; ----> 1 sc.pp.neighbors(adata). ~/Projects/scanpy/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy); 137 adata._init_as_actual(adata.copy()); 138 neighbors = Neighbors(adata); --> 139 neighbors.compute_neighbors(; 140 n_neighbors=n_neighbors,; 141 knn=knn,. ~/Projects/scanpy/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds); 806 # we need self._distances also for method == 'gauss' if we didn't; 807 # use dense distances; --> 808 self._distances, self._connectivities = _compute_connectivities_umap(; 809 knn_indices,; 810 knn_distances,. ~/Projects/scanpy/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity); 385 # umap 0.5.0; 386 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""); --> 387 from umap.umap_ import fuzzy_simplicial_set; 388 ; 389 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/__init__.py in <module>; 1 from warnings import warn, catch_warnings, simplefilter; ----> 2 from .umap_ import UMAP; 3 ; 4 try:; 5 with catch_warnings():. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/umap_.py in <module>; 30 import umap.distances as dist; 31 ; ---> 32 import umap.sparse as sparse; 33 ; 34 from umap.utils import (. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/sparse.py in <module>; 10 import numpy as np; 11 ; ---> 12 from umap.utils import norm; 13 ; 14 locale.setlocale(locale.LC_NUMERIC, ""C""). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/utils.py in <module>; 38 ; 39 @numba.njit(""i4(i8[:])""); ---> 40 def tau_rand_int(state):; 41 """"""A fast (pseudo)-random number generator.; 42 . ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-846931466:1339,message,message,1339,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-846931466,2,['message'],['message']
Integrability,-forge; debugpy 1.5.1 py39hc377ac9_0 anaconda; decorator 5.1.1 pyhd3eb1b0_0 anaconda; dunamai 1.18.0 pyhd8ed1ab_0 conda-forge; entrypoints 0.4 py39hca03da5_0 anaconda; executing 0.8.3 pyhd3eb1b0_0 anaconda; fonttools 4.41.0 py39h0f82c59_0 conda-forge; freetype 2.12.1 hd633e50_1 conda-forge; get_version 3.5.4 pyhd8ed1ab_0 conda-forge; gettext 0.21.1 h0186832_0 conda-forge; git 2.41.0 pl5321h46e2b6d_0 conda-forge; h5py 3.9.0 nompi_py39he9c2634_101 conda-forge; hdf5 1.14.1 nompi_h3aba7b3_100 conda-forge; idna 3.4 pyhd8ed1ab_0 conda-forge; importlib-metadata 6.8.0 pyha770c72_0 conda-forge; importlib_metadata 6.8.0 hd8ed1ab_0 conda-forge; ipykernel 6.9.1 py39hca03da5_0 anaconda; ipython 8.3.0 py39hca03da5_0 anaconda; jedi 0.18.1 py39hca03da5_1 anaconda; joblib 1.3.0 pyhd8ed1ab_1 conda-forge; jupyter_client 7.2.2 py39hca03da5_0 anaconda; jupyter_core 4.10.0 py39hca03da5_0 anaconda; kiwisolver 1.4.4 py39haaf3ac1_1 conda-forge; krb5 1.21.1 h92f50d5_0 conda-forge; lcms2 2.15 hd835a16_1 conda-forge; legacy-api-wrap 1.2 py_0 conda-forge; lerc 4.0.0 h9a09cb3_0 conda-forge; libaec 1.0.6 hb7217d7_1 conda-forge; libblas 3.9.0 17_osxarm64_openblas conda-forge; libbrotlicommon 1.0.9 h1a8c8d9_9 conda-forge; libbrotlidec 1.0.9 h1a8c8d9_9 conda-forge; libbrotlienc 1.0.9 h1a8c8d9_9 conda-forge; libcblas 3.9.0 17_osxarm64_openblas conda-forge; libcurl 8.1.2 hc52a3a8_1 conda-forge; libcxx 16.0.6 h4653b0c_0 conda-forge; libdeflate 1.18 h1a8c8d9_0 conda-forge; libedit 3.1.20191231 hc8eb9b7_2 conda-forge; libev 4.33 h642e427_1 conda-forge; libexpat 2.5.0 hb7217d7_1 conda-forge; libffi 3.4.2 h3422bc3_5 conda-forge; libgfortran 5.0.0 12_2_0_hd922786_31 conda-forge; libgfortran5 12.2.0 h0eea778_31 conda-forge; libiconv 1.17 he4db4b2_0 conda-forge; libjpeg-turbo 2.1.5.1 h1a8c8d9_0 conda-forge; liblapack 3.9.0 17_osxarm64_openblas conda-forge; libllvm14 14.0.6 hd1a9a77_3 conda-forge; libnghttp2 1.52.0 hae82a92_0 conda-forge; libopenblas 0.3.23 openmp_hc731615_0 conda-forge; libpng 1.6.39 h76d750c_,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2564:4016,wrap,wrap,4016,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564,1,['wrap'],['wrap']
Integrability,"-packages/setuptools/build_meta.py"", line 145, in run_setup; exec(compile(code, __file__, 'exec'), locals()) File ""setup.py"", line 17, in <module>; setup(; File ""/tmp/pip-build-env-wb9dh0v3/overlay/lib/python3.8/site-packages/setuptools/__init__.py"", line 153, in setup; return distutils.core.setup(**attrs); File ""/usr/lib/python3.8/distutils/core.py"", line 108, in setup; _setup_distribution = dist = klass(attrs); File ""/tmp/pip-build-env-wb9dh0v3/overlay/lib/python3.8/site-packages/setuptools/dist.py"", line 423, in __init__; _Distribution.__init__(self, {; File ""/usr/lib/python3.8/distutils/dist.py"", line 292, in __init__; self.finalize_options() File ""/tmp/pip-build-env-wb9dh0v3/overlay/lib/python3.8/site-packages/setuptools/dist.py"", line 695, in finalize_options; ep(self); File ""/tmp/pip-build-env-wb9dh0v3/overlay/lib/python3.8/site-packages/setuptools/dist.py"", line 702, in _finalize_setup_keywords; ep.load()(self, ep.name, value); File ""/usr/local/lib/python3.8/dist-packages/setuptools_scm/integration.py"", line 17, in version_keyword; dist.metadata.version = _get_version(config); File ""/usr/local/lib/python3.8/dist-packages/setuptools_scm/__init__.py"", line 148, in _get_version; parsed_version = _do_parse(config); File ""/usr/local/lib/python3.8/dist-packages/setuptools_scm/__init__.py"", line 110, in _do_parse raise LookupError(; LookupError: setuptools-scm was unable to detect version for '/home/ubuntu/code/scanpy'.; Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work.; ; For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj```; ```; #### Versions. <details>. scanpy; problem is with installation, so scanpy.logging.print_versions(); commit: ef5a8ee57c2aef7778a069a49101a8998718e6d5. python; 3.8.5. pip; 2",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1496:2245,integrat,integration,2245,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1496,1,['integrat'],['integration']
Integrability,". **random_state** : typing.Union[int, mtrand.RandomState, NoneType]. A numpy random seed. **method** : {'umap', 'gauss', `None`} (default: `'umap'`). Use 'umap' [McInnes18]_ or 'gauss' (Gauss kernel following [Coifman05]_; with adaptive width [Haghverdi16]_) for computing connectivities. **metric** : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], float]], optional (default: 'euclidean'). A known metric’s name or a callable that returns a distance. **metric_kwds** : Mapping. Options for the metric. **copy** : bool. Return a copy instead of writing to adata. :Returns:. Depending on `copy`, updates or returns `adata` with the following:. . **connectivities** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Weighted adjacency matrix of the neighborhood graph of data; points. Weights should be interpreted as connectivities. **distances** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Instead of decaying weights, this stores distances for each pair of; neighbors.; File: ~/_hholtz/01_projects/1512_scanpy/scanpy/scanpy/neighbors/__init__.py; Type: function; ```. PS: ; - Already the [docs](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.Neighbors.compute_neighbors.html) show that `Neighbors.compute_neighbors` has invalid numpydoc... this was the case in several instances and I'm slowly fixing all of them... It's just a matter of adding `\` at the line breaks.; - I completely agree that the redundency between signature and docstring information lead to a a very small number of errors in the docstrings. However, in several instances, I'm setting the default value in the signature to `None`. But in the docstring, I'm giving the value to which this `None` evaluates in the default case (depending on what is passed)... There is quite a number of such cases. Clearly, one could replace all of them with `'auto'` parameters, which is probably the better way of doing this. As the whole thing is backwards compat, this is not an immediate problem",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:7900,depend,depending,7900,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999,2,['depend'],['depending']
Integrability,".); 768 **kwds,; 769 ):; 770 """"""\; 771 Plot ranking of genes using dotplot plot (see :func:`~scanpy.pl.dotplot`); 772 ; (...); 872 tl.rank_genes_groups; 873 """"""; --> 874 return _rank_genes_groups_plot(; 875 adata,; 876 plot_type='dotplot',; 877 groups=groups,; 878 n_genes=n_genes,; 879 groupby=groupby,; 880 values_to_plot=values_to_plot,; 881 var_names=var_names,; 882 gene_symbols=gene_symbols,; 883 key=key,; 884 min_logfoldchange=min_logfoldchange,; 885 show=show,; 886 save=save,; 887 return_fig=return_fig,; 888 **kwds,; 889 ). File [/mnt/data1/liz/software/miniconda3/envs/scanpy/lib/python3.9/site-packages/scanpy/plotting/_tools/__init__.py:531](/mnt/data1/liz/software/miniconda3/envs/scanpy/lib/python3.9/site-packages/scanpy/plotting/_tools/__init__.py#line=530), in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, values_to_plot, var_names, min_logfoldchange, key, show, save, return_fig, gene_symbols, **kwds); 529 values_df = None; 530 if values_to_plot is not None:; --> 531 values_df = _get_values_to_plot(; 532 adata,; 533 values_to_plot,; 534 var_names_list,; 535 key=key,; 536 gene_symbols=gene_symbols,; 537 ); 538 title = values_to_plot; 539 if values_to_plot == 'logfoldchanges':. File [/mnt/data1/liz/software/miniconda3/envs/scanpy/lib/python3.9/site-packages/scanpy/plotting/_tools/__init__.py:1636](/mnt/data1/liz/software/miniconda3/envs/scanpy/lib/python3.9/site-packages/scanpy/plotting/_tools/__init__.py#line=1635), in _get_values_to_plot(adata, values_to_plot, gene_names, groups, key, gene_symbols); 1629 message = (; 1630 ""Please run `sc.tl.rank_genes_groups` with ""; 1631 ""'n_genes=adata.shape[1]' to save all gene ""; 1632 f""scores. Currently, only {df.shape[0]} ""; 1633 ""are found""; 1634 ); 1635 logg.error(message); -> 1636 raise ValueError(message); 1637 df['group'] = group; 1638 df_list.append(df). ValueError: Please run `sc.tl.rank_genes_groups` with 'n_genes=adata.shape[1]' to save all gene scores. Currently, only 2238 are found; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3049#issuecomment-2107618181:2484,message,message,2484,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3049#issuecomment-2107618181,3,['message'],['message']
Integrability,".0 | testpath | 0.5.0 | testpath | 0.5.0; texttable | 1.6.4 | texttable | 1.6.4 | texttable | 1.6.4; threadpoolctl | 3.0.0 | threadpoolctl | 3.0.0 | threadpoolctl | 3.0.0;   |   | tomli | 2.0.0 |   |  ; toolz | 0.11.1 | toolz | 0.11.1 | toolz | 0.11.1; tornado | 6.1 | tornado | 6.1 | tornado | 6.1; tqdm | 4.62.3 | tqdm | 4.62.3 | tqdm | 4.62.3; traitlets | 5.1.1 | traitlets | 5.1.1 | traitlets | 5.1.1; typing_extensions | 4.0.1 | typing_extensions | 4.0.1 | typing_extensions | 4.0.1; umap-learn | 0.5.2 | umap-learn | 0.5.2 | umap-learn | 0.5.2; urllib3 | 1.26.7 | urllib3 | 1.26.7 | urllib3 | 1.26.7; wcwidth | 0.2.5 | wcwidth | 0.2.5 | wcwidth | 0.2.5; webencodings | 0.5.1 | webencodings | 0.5.1 | webencodings | 0.5.1; wheel | 0.37.1 | wheel | 0.37.1 | wheel | 0.37.1; widgetsnbextension | 3.5.2 | widgetsnbextension | 3.5.2 | widgetsnbextension | 3.5.2; win-inet-pton | 1.1.0 | win-inet-pton | 1.1.0 | win-inet-pton | 1.1.0; wincertstore | 0.2 | wincertstore | 0.2 | wincertstore | 0.2; wrapt | 1.13.3 | wrapt | 1.13.3 | wrapt | 1.13.3; xlrd | 1.2.0 | xlrd | 1.2.0 | xlrd | 1.2.0; yarl | 1.7.2 | yarl | 1.7.2 | yarl | 1.7.2; zict | 2.0.0 | zict | 2.0.0 | zict | 2.0.0; zipp | 3.7.0 | zipp | 3.7.0 | zipp | 3.7.0. </body>. </html>. These packages are different among these 3 PCs :<html xmlns:v=""urn:schemas-microsoft-com:vml""; xmlns:o=""urn:schemas-microsoft-com:office:office""; xmlns:x=""urn:schemas-microsoft-com:office:excel""; xmlns=""http://www.w3.org/TR/REC-html40"">. <head>. <meta name=ProgId content=Excel.Sheet>; <meta name=Generator content=""Microsoft Excel 15"">; <link id=Main-File rel=Main-File; href=""file:///C:/Users/hyjfo/AppData/Local/Temp/msohtmlclip1/01/clip.htm"">; <link rel=File-List; href=""file:///C:/Users/hyjfo/AppData/Local/Temp/msohtmlclip1/01/clip_filelist.xml"">; <style>; <!--table; 	{mso-displayed-decimal-separator:""\."";; 	mso-displayed-thousand-separator:""\,"";}; @page; 	{margin:.75in .7in .75in .7in;; 	mso-header-margin:.3in;; 	mso-footer-margin:.3in;}; tr; 	{mso-",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2114:15098,wrap,wrapt,15098,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2114,1,['wrap'],['wrapt']
Integrability,".0.1; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2022.7.0; dateutil 2.8.2; debugpy 1.5.1; decorator 5.1.1; defusedxml 0.7.1; deprecate 0.3.2; dill 0.3.4; docrep 0.3.2; entrypoints 0.4; etils 0.8.0; flax 0.6.1; fsspec 2022.7.1; google NA; graphviz 0.20; h5py 3.7.0; idna 3.4; igraph 0.10.2; ipykernel 6.15.2; ipython_genutils 0.2.0; ipywidgets 7.6.5; jax 0.3.23; jaxlib 0.3.22; jedi 0.18.1; jinja2 2.11.3; jmespath 0.10.0; joblib 1.1.1; jupyter_server 1.18.1; kiwisolver 1.4.2; leidenalg 0.8.10; llvmlite 0.39.1; louvain 0.8.0; lz4 3.1.3; markupsafe 2.0.1; matplotlib 3.5.2; matplotlib_inline 0.1.6; ml_collections NA; mpl_toolkits NA; msgpack 1.0.3; mudata 0.2.0; multipledispatch 0.6.0; natsort 8.1.0; nbinom_ufunc NA; numba 0.56.3; numexpr 2.8.3; numpy 1.22.4; numpyro 0.10.1; opt_einsum v3.3.0; optax 0.1.3; packaging 21.3; pandas 1.4.4; parso 0.8.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; plotly 5.9.0; prompt_toolkit 3.0.20; psutil 5.9.0; ptyprocess 0.7.0; pydev_ipython NA; pydevconsole NA; pydevd 2.6.0; pydevd_concurrency_analyser NA; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.11.2; pyparsing 3.0.9; pyro 1.8.2; pytorch_lightning 1.7.7; pytz 2022.1; regex 2.5.116; requests 2.28.1; rich NA; scipy 1.7.3; scvi 0.18.0; session_info 1.0.0; setuptools 63.4.1; simplejson 3.17.6; six 1.16.0; sklearn 1.1.2; snappy NA; socks 1.7.1; sphinxcontrib NA; storemagic NA; tblib 1.7.0; tensorboard 2.9.1; texttable 1.6.4; threadpoolctl 2.2.0; tlz 0.11.0; toolz 0.11.2; torch 1.12.1; torchmetrics 0.10.0; torchvision 0.13.1; tornado 6.1; tqdm 4.64.1; traitlets 5.1.1; tree 0.1.7; typing_extensions NA; urllib3 1.26.12; wcwidth 0.2.5; wrapt 1.14.1; yaml 6.0; zipp NA; zmq 23.2.0; zope NA; -----; IPython 7.31.1; jupyter_client 7.3.4; jupyter_core 4.11.1; jupyterlab 3.4.4; notebook 6.4.12; -----; Python 3.9.12 (main, Jun 1 2022, 06:36:29) [Clang 12.0.0 ]; macOS-10.16-x86_64-i386-64bit; -----; Session information updated at 2022-10-22 15:12. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2359:4347,wrap,wrapt,4347,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2359,1,['wrap'],['wrapt']
Integrability,".0; opt_einsum v3.3.0; optax 0.1.4; packaging 23.0; pandas 1.5.3; parso 0.8.3; paste NA; patsy 0.5.3; petsc4py 3.19.0; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; platformdirs 3.0.0; progressbar 4.2.0; prometheus_client NA; prompt_toolkit 3.0.38; psutil 5.9.4; ptyprocess 0.7.0; pure_eval 0.2.2; pvectorc NA; pycparser 2.21; pydev_ipython NA; pydevconsole NA; pydevd 2.9.5; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pydot 1.4.2; pygam 0.8.0; pygments 2.14.0; pygpcca 1.0.4; pyparsing 3.0.9; pyro 1.8.4+9ed468d; pyrsistent NA; python_utils NA; pythonjsonlogger NA; pytorch_lightning 1.9.3; pytz 2022.7.1; pywt 1.4.1; requests 2.28.2; rfc3339_validator 0.1.4; rfc3986_validator 0.1.1; rich NA; scHPL NA; scarches 0.5.7; sccoda 0.1.9; scipy 1.10.1; scvelo 0.2.5; scvi 0.20.1; seaborn 0.12.2; send2trash NA; session_info 1.0.0; setuptools 67.4.0; six 1.16.0; skewnorm_ufunc NA; skimage 0.19.3; sklearn 1.2.1; slepc4py 3.19.0; sniffio 1.3.0; socks 1.7.1; squidpy 1.2.2; stack_data 0.6.2; statsmodels 0.13.5; tblib 1.7.0; tcr_embedding NA; tensorboard 2.11.2; tensorflow 2.11.0; tensorflow_probability 0.19.0; termcolor NA; texttable 1.6.7; threadpoolctl 3.1.0; tifffile 2023.2.28; tlz 0.12.0; toolz 0.12.0; torch 1.13.1; torch_cluster 1.6.0; torch_geometric 2.2.0; torch_scatter 2.1.0; torch_sparse 0.6.15; torchmetrics 0.11.3; torchvision 0.14.1; tornado 6.2; tqdm 4.64.1; traitlets 5.9.0; tree 0.1.7; typing_extensions NA; unicodedata2 NA; uri_template NA; urllib3 1.26.14; validators 0.20.0; wcwidth 0.2.6; webcolors 1.11.1; websocket 1.5.1; wrapt 1.15.0; xarray 2023.2.0; xarray_einstats 0.5.1; yaml 6.0; zarr 2.13.6; zipp NA; zmq 25.0.0; zoneinfo NA; zope NA; -----; IPython 8.11.0; jupyter_client 8.0.3; jupyter_core 5.2.0; jupyterlab 3.6.1; notebook 6.5.2; -----; Python 3.10.9 | packaged by conda-forge | (main, Feb 2 2023, 20:20:04) [GCC 11.3.0]; Linux-3.10.0-1160.83.1.el7.x86_64-x86_64-with-glibc2.17; -----; Session information updated at 2023-05-26 01:06. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2493:3786,wrap,wrapt,3786,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2493,1,['wrap'],['wrapt']
Integrability,.10.0 nompi_py38hf6831e1_105 conda-forge; hdf5 1.10.6 nompi_hc457bb4_1110 conda-forge; icu 67.1 hb1e8313_0 conda-forge; idna 2.10 pyh9f0ad1d_0 conda-forge; imagesize 1.2.0 py_0 conda-forge; importlib-metadata 2.0.0 py38h32f6830_0 conda-forge; importlib_metadata 2.0.0 1 conda-forge; intervaltree 3.1.0 py_0 ; ipykernel 5.3.4 py38h1cdfbd6_1 conda-forge; ipython 7.18.1 py38h1cdfbd6_1 conda-forge; ipython_genutils 0.2.0 py_1 conda-forge; isort 5.6.4 py_0 conda-forge; jedi 0.17.1 py38h32f6830_0 conda-forge; jinja2 2.11.2 pyh9f0ad1d_0 conda-forge; joblib 0.17.0 py_0 conda-forge; jpeg 9d h0b31af3_0 conda-forge; jsonschema 3.2.0 py38h32f6830_1 conda-forge; jupyter_client 6.1.7 py_0 conda-forge; jupyter_core 4.6.3 py38h32f6830_2 conda-forge; jupyterlab_pygments 0.1.2 pyh9f0ad1d_0 conda-forge; keyring 21.4.0 py38h32f6830_2 conda-forge; kiwisolver 1.3.0 py38h02bb52f_0 conda-forge; krb5 1.17.1 h75d18d8_3 conda-forge; lazy-object-proxy 1.4.3 py38h64e0658_2 conda-forge; lcms2 2.11 h174193d_0 conda-forge; legacy-api-wrap 1.2 py_0 conda-forge; libblas 3.9.0 2_openblas conda-forge; libcblas 3.9.0 2_openblas conda-forge; libclang 10.0.1 default_hf57f61e_1 conda-forge; libcurl 7.71.1 h9bf37e3_8 conda-forge; libcxx 11.0.0 h439d374_0 conda-forge; libedit 3.1.20191231 hed1e85f_2 conda-forge; libev 4.33 haf1e3a3_1 conda-forge; libffi 3.2.1 1 bioconda; libgfortran 4.0.0 h50e675f_13 conda-forge; libgfortran4 7.5.0 h50e675f_13 conda-forge; libglib 2.66.2 hdb5fb44_0 conda-forge; libiconv 1.16 haf1e3a3_0 conda-forge; liblapack 3.9.0 2_openblas conda-forge; libllvm10 10.0.1 h009f743_3 conda-forge; libnghttp2 1.41.0 h7580e61_2 conda-forge; libopenblas 0.3.12 openmp_h63d9170_1 conda-forge; libpng 1.6.37 hb0a8c7a_2 conda-forge; libpq 12.3 haa216e0_2 conda-forge; libsodium 1.0.18 haf1e3a3_1 conda-forge; libspatialindex 1.9.3 h4a8c4bd_3 conda-forge; libssh2 1.9.0 h39bdce6_5 conda-forge; libtiff 4.1.0 h2ae36a8_6 conda-forge; libwebp-base 1.1.0 h0b31af3_3 conda-forge; libxml2 2.9.10 h7fdee97_2 conda-for,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/953#issuecomment-719504684:3095,wrap,wrap,3095,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953#issuecomment-719504684,2,['wrap'],['wrap']
Integrability,.11.0 ; dataclasses 0.6 ; dataforest 0.0.2 /home/ubuntu/code/dataforest ; dbus-python 1.2.16 ; decorator 4.4.2 ; defusedxml 0.6.0 ; distro-info 0.23ubuntu1 ; entrypoints 0.3 ; fastcore 1.3.5 ; flake8 3.8.4 ; Flask 1.1.2 ; Flask-Compress 1.8.0 ; future 0.18.2 ; get-version 2.1 ; gitdb 4.0.5 ; GitPython 3.1.11 ; h5py 3.1.0 ; hyperopt 0.1.2 ; idna 2.8 ; importlib-metadata 2.0.0 ; iniconfig 1.1.1 ; ipdb 0.13.4 ; ipykernel 5.3.4 ; ipympl 0.5.8 ; ipython 7.19.0 ; ipython-genutils 0.2.0 ; ipywidgets 7.5.1 ; isort 5.6.4 ; itsdangerous 1.1.0 ; jedi 0.17.2 ; Jinja2 2.11.2 ; jmespath 0.10.0 ; joblib 0.17.0 ; json5 0.9.5 ; jsonschema 3.2.0 ; jupyter-client 6.1.7 ; jupyter-core 4.6.3 ; jupyter-dash 0.3.1 ; jupyter-lsp 0.9.2 ; jupyterlab 2.2.9 ; jupyterlab-code-formatter 1.3.6 ; jupyterlab-git 0.23.1 ; jupyterlab-latex 2.0.0 ; jupyterlab-pygments 0.1.2 ; jupyterlab-server 1.2.0 ; jupyterlab-sql 0.3.3 ; jupyterlab-templates 0.2.5 ; jupytext 1.6.0 ; kiwisolver 1.3.1 ; lazy-object-proxy 1.4.3 ; legacy-api-wrap 1.2 ; llvmlite 0.34.0 ; markdown-it-py 0.5.6 ; MarkupSafe 1.1.1 ; matplotlib 3.3.3 ; mccabe 0.6.1 ; mistune 0.8.4 ; mypy-extensions 0.4.3 ; natsort 7.0.1 ; nbclient 0.5.1 ; nbconvert 6.0.7 ; nbdime 2.1.0 ; nbformat 5.0.8 ; nbresuse 0.3.6 ; nest-asyncio 1.4.3 ; networkx 2.5 ; notebook 6.1.5 ; numba 0.51.2 ; numexpr 2.7.1 ; numpy 1.19.4 ; packaging 20.4 ; pandas 1.1.4 ; pandocfilters 1.4.3 ; parso 0.7.1 ; path 15.0.0 ; pathspec 0.8.1 ; pathtools 0.1.2 ; patsy 0.5.1 ; peepdis 0.1.13 ; pexpect 4.8.0 ; pickleshare 0.7.5 ; Pillow 8.0.1 ; pip 20.0.2 ; plotly 4.12.0 ; pluggy 0.13.1 ; prometheus-client 0.8.0 ; prompt-toolkit 3.0.8 ; psutil 5.7.3 ; ptvsd 4.3.2 ; ptyprocess 0.6.0 ; py 1.9.0 ; pycodestyle 2.6.0 ; pycparser 2.20 ; pydocstyle 5.1.1 ; pyflakes 2.2.0 ; Pygments 2.7.2 ; PyGObject 3.36.0 ; pylint 2.6.0 ; pymongo 3.11.0 ; pyparsing 2.4.7 ; pyrsistent 0.17.3 ; pytest 6.1.2 ; python-apt 2.0.0+ubuntu0.20.4.1 ; python-dateutil 2.8.1 ; python-debian 0.1.36ubuntu1 ; python-jsonrpc-ser,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1496:4982,wrap,wrap,4982,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1496,1,['wrap'],['wrap']
Integrability,.2.0; - executing=2.0.1; - filelock=3.14.0; - fonttools=4.53.0; - fqdn=1.5.1; - freetype=2.12.1; - get-annotations=0.1.2; - gffpandas=1.2.2; - gffutils=0.13; - glpk=5.0; - gmp=6.3.0; - gmpy2=2.1.5; - h11=0.14.0; - h2=4.1.0; - h5py=3.11.0; - hdf5=1.14.3; - hpack=4.0.0; - httpcore=1.0.5; - httpx=0.27.0; - hyperframe=6.0.1; - icu=73.2; - idna=3.7; - igraph=0.10.12; - importlib-metadata=7.1.0; - importlib_metadata=7.1.0; - importlib_resources=6.4.0; - ipykernel=6.29.4; - ipython=8.25.0; - isoduration=20.11.0; - jedi=0.19.1; - jinja2=3.1.4; - joblib=1.4.2; - json5=0.9.25; - jsonpointer=2.4; - jsonschema=4.22.0; - jsonschema-specifications=2023.12.1; - jsonschema-with-format-nongpl=4.22.0; - jupyter-lsp=2.2.5; - jupyter_client=8.6.2; - jupyter_core=5.7.2; - jupyter_events=0.10.0; - jupyter_server=2.14.1; - jupyter_server_terminals=0.5.3; - jupyterlab=4.2.2; - jupyterlab_pygments=0.3.0; - jupyterlab_server=2.27.2; - kaleido-core=0.2.1; - kiwisolver=1.4.5; - krb5=1.21.2; - lcms2=2.16; - legacy-api-wrap=1.4; - leidenalg=0.10.2; - lerc=4.0.0; - libabseil=20240116.2; - libaec=1.1.3; - libblas=3.9.0; - libbrotlicommon=1.1.0; - libbrotlidec=1.1.0; - libbrotlienc=1.1.0; - libcblas=3.9.0; - libcurl=8.8.0; - libcxx=17.0.6; - libdeflate=1.20; - libedit=3.1.20191231; - libev=4.33; - libexpat=2.6.2; - libffi=3.4.2; - libgfortran=5.0.0; - libgfortran5=13.2.0; - libhwloc=2.10.0; - libiconv=1.17; - libjpeg-turbo=3.0.0; - liblapack=3.9.0; - liblapacke=3.9.0; - libleidenalg=0.11.1; - libllvm14=14.0.6; - libnghttp2=1.58.0; - libopenblas=0.3.27; - libpng=1.6.43; - libprotobuf=4.25.3; - libsodium=1.0.18; - libsqlite=3.46.0; - libssh2=1.11.0; - libtiff=4.6.0; - libwebp-base=1.4.0; - libxcb=1.15; - libxml2=2.12.7; - libzlib=1.3.1; - llvm-openmp=18.1.7; - llvmlite=0.42.0; - louvain=0.8.2; - lz4-c=1.9.4; - markupsafe=2.1.5; - mathjax=2.7.7; - matplotlib=3.8.4; - matplotlib-base=3.8.4; - matplotlib-inline=0.1.7; - mistune=3.0.2; - mkl=2023.2.0; - mkl-devel=2023.2.0; - mkl-include=2023.2.0; - mpc=1,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3116:4535,wrap,wrap,4535,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3116,1,['wrap'],['wrap']
Integrability,".5.0 | testpath | 0.5.0; texttable | 1.6.4 | texttable | 1.6.4 | texttable | 1.6.4; threadpoolctl | 3.0.0 | threadpoolctl | 3.0.0 | threadpoolctl | 3.0.0;   |   | tomli | 2.0.0 |   |  ; toolz | 0.11.1 | toolz | 0.11.1 | toolz | 0.11.1; tornado | 6.1 | tornado | 6.1 | tornado | 6.1; tqdm | 4.62.3 | tqdm | 4.62.3 | tqdm | 4.62.3; traitlets | 5.1.1 | traitlets | 5.1.1 | traitlets | 5.1.1; typing_extensions | 4.0.1 | typing_extensions | 4.0.1 | typing_extensions | 4.0.1; umap-learn | 0.5.2 | umap-learn | 0.5.2 | umap-learn | 0.5.2; urllib3 | 1.26.7 | urllib3 | 1.26.7 | urllib3 | 1.26.7; wcwidth | 0.2.5 | wcwidth | 0.2.5 | wcwidth | 0.2.5; webencodings | 0.5.1 | webencodings | 0.5.1 | webencodings | 0.5.1; wheel | 0.37.1 | wheel | 0.37.1 | wheel | 0.37.1; widgetsnbextension | 3.5.2 | widgetsnbextension | 3.5.2 | widgetsnbextension | 3.5.2; win-inet-pton | 1.1.0 | win-inet-pton | 1.1.0 | win-inet-pton | 1.1.0; wincertstore | 0.2 | wincertstore | 0.2 | wincertstore | 0.2; wrapt | 1.13.3 | wrapt | 1.13.3 | wrapt | 1.13.3; xlrd | 1.2.0 | xlrd | 1.2.0 | xlrd | 1.2.0; yarl | 1.7.2 | yarl | 1.7.2 | yarl | 1.7.2; zict | 2.0.0 | zict | 2.0.0 | zict | 2.0.0; zipp | 3.7.0 | zipp | 3.7.0 | zipp | 3.7.0. </body>. </html>. These packages are different among these 3 PCs :<html xmlns:v=""urn:schemas-microsoft-com:vml""; xmlns:o=""urn:schemas-microsoft-com:office:office""; xmlns:x=""urn:schemas-microsoft-com:office:excel""; xmlns=""http://www.w3.org/TR/REC-html40"">. <head>. <meta name=ProgId content=Excel.Sheet>; <meta name=Generator content=""Microsoft Excel 15"">; <link id=Main-File rel=Main-File; href=""file:///C:/Users/hyjfo/AppData/Local/Temp/msohtmlclip1/01/clip.htm"">; <link rel=File-List; href=""file:///C:/Users/hyjfo/AppData/Local/Temp/msohtmlclip1/01/clip_filelist.xml"">; <style>; <!--table; 	{mso-displayed-decimal-separator:""\."";; 	mso-displayed-thousand-separator:""\,"";}; @page; 	{margin:.75in .7in .75in .7in;; 	mso-header-margin:.3in;; 	mso-footer-margin:.3in;}; tr; 	{mso-height-source:au",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2114:15115,wrap,wrapt,15115,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2114,1,['wrap'],['wrapt']
Integrability,".6.8/lib/python3.6/contextlib.py"", line 88, in __exit__; next(self.gen); File ""/home/docs/checkouts/readthedocs.org/user_builds/icb-scanpy/envs/latest/lib/python3.6/site-packages/sphinx/util/logging.py"", line 219, in pending_warnings; memhandler.flushTo(logger); File ""/home/docs/checkouts/readthedocs.org/user_builds/icb-scanpy/envs/latest/lib/python3.6/site-packages/sphinx/util/logging.py"", line 184, in flushTo; logger.handle(record); File ""/home/docs/.pyenv/versions/3.6.8/lib/python3.6/logging/__init__.py"", line 1454, in handle; self.callHandlers(record); File ""/home/docs/.pyenv/versions/3.6.8/lib/python3.6/logging/__init__.py"", line 1516, in callHandlers; hdlr.handle(record); File ""/home/docs/.pyenv/versions/3.6.8/lib/python3.6/logging/__init__.py"", line 861, in handle; rv = self.filter(record); File ""/home/docs/.pyenv/versions/3.6.8/lib/python3.6/logging/__init__.py"", line 720, in filter; result = f.filter(record); File ""/home/docs/checkouts/readthedocs.org/user_builds/icb-scanpy/envs/latest/lib/python3.6/site-packages/sphinx/util/logging.py"", line 404, in filter; raise SphinxWarning(location + "":"" + message); sphinx.errors.SphinxWarning: /home/docs/checkouts/readthedocs.org/user_builds/icb-scanpy/checkouts/latest/scanpy/preprocessing/_simple.py:docstring of scanpy.pp.downsample_counts:17:py:class reference target not found: numpy.random.RandomState. Warning, treated as error:; /home/docs/checkouts/readthedocs.org/user_builds/icb-scanpy/checkouts/latest/scanpy/preprocessing/_simple.py:docstring of scanpy.pp.downsample_counts:17:py:class reference target not found: numpy.random.RandomState; ```. </details>. It looks like readthedocs is failing due to `numpy.random.RandomState` not being rewritten to `numpy.random.mtrand.RandomState`. I think the transform isn't happening in the right order when the docs are built from scratch since I can reproduce the warnings locally with:. ```; make clean; make html; ```. But if I run `make html` again, I don't get the warnings.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1057:3055,message,message,3055,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1057,1,['message'],['message']
Integrability,".pyplot as plt; fig,ax=plt.subplots(1,1,figsize=(7,1)); path_data = sc.pl.paga_path(; adata,; [4, 5],; [""Elane""],; ax=ax,; show_node_names=False,; ytick_fontsize=12,; return_data=True,; #n_avg=1,; color_map=""Greys"",; groups_key=""leiden"",; color_maps_annotations={""dpt_pseudotime"": ""viridis""}; ); ```. ### Error output. ```pytb; TypeError Traceback (most recent call last); Cell In[1], line 15; 13 import matplotlib.pyplot as plt; 14 fig,ax=plt.subplots(1,1,figsize=(7,1)); ---> 15 path_data = sc.pl.paga_path(; 16 adata,; 17 [4, 5],; 18 [""Elane""],; 19 ax=ax,; 20 show_node_names=False,; 21 ytick_fontsize=12,; 22 return_data=True,; 23 #n_avg=1,; 24 color_map=""Greys"",; 25 groups_key=""leiden"",; 26 color_maps_annotations={""dpt_pseudotime"": ""viridis""}; 27 ). File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\scanpy\plotting\_tools\paga.py:1255, in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax); 1253 print(X.shape); 1254 if as_heatmap:; -> 1255 img = ax.imshow(X, aspect=""auto"", interpolation=""nearest"", cmap=color_map); 1256 if show_yticks:; 1257 ax.set_yticks(range(len(X))). File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3025:2137,wrap,wraps,2137,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3025,1,['wrap'],['wraps']
Integrability,".read(self.source, self.parser,; File ""/usr/local/lib/python3.8/site-packages/sphinx/io.py"", line 108, in read; self.parse(); File ""/usr/local/lib/python3.8/site-packages/docutils/readers/__init__.py"", line 77, in parse; self.parser.parse(self.input, document); File ""/usr/local/lib/python3.8/site-packages/sphinx/parsers.py"", line 100, in parse; self.statemachine.run(inputlines, document, inliner=self.inliner); File ""/usr/local/lib/python3.8/site-packages/docutils/parsers/rst/states.py"", line 170, in run; results = StateMachineWS.run(self, input_lines, input_offset,; File ""/usr/local/lib/python3.8/site-packages/docutils/statemachine.py"", line 241, in run; context, next_state, result = self.check_line(; File ""/usr/local/lib/python3.8/site-packages/docutils/statemachine.py"", line 459, in check_line; return method(match, context, next_state); File ""/usr/local/lib/python3.8/site-packages/docutils/parsers/rst/states.py"", line 2769, in underline; self.section(title, source, style, lineno - 1, messages); File ""/usr/local/lib/python3.8/site-packages/docutils/parsers/rst/states.py"", line 327, in section; self.new_subsection(title, lineno, messages); File ""/usr/local/lib/python3.8/site-packages/docutils/parsers/rst/states.py"", line 393, in new_subsection; newabsoffset = self.nested_parse(; File ""/usr/local/lib/python3.8/site-packages/docutils/parsers/rst/states.py"", line 281, in nested_parse; state_machine.run(block, input_offset, memo=self.memo,; File ""/usr/local/lib/python3.8/site-packages/docutils/parsers/rst/states.py"", line 196, in run; results = StateMachineWS.run(self, input_lines, input_offset); File ""/usr/local/lib/python3.8/site-packages/docutils/statemachine.py"", line 241, in run; context, next_state, result = self.check_line(; File ""/usr/local/lib/python3.8/site-packages/docutils/statemachine.py"", line 459, in check_line; return method(match, context, next_state); File ""/usr/local/lib/python3.8/site-packages/docutils/parsers/rst/states.py"", line 2344, in explicit_m",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1946#issuecomment-877995557:6897,message,messages,6897,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1946#issuecomment-877995557,1,['message'],['messages']
Integrability,"0 infer.propagate(raise_errors=raise_errors); 71 typemap, restype, calltypes = infer.unify(raise_errors=raise_errors); 72 . ~/.local/lib/python3.9/site-packages/numba/core/typeinfer.py in propagate(self, raise_errors); 1069 if isinstance(e, ForceLiteralArg)]; 1070 if not force_lit_args:; -> 1071 raise errors[0]; 1072 else:; 1073 raise reduce(operator.or_, force_lit_args). TypingError: Failed in nopython mode pipeline (step: nopython frontend); Failed in nopython mode pipeline (step: nopython frontend); Failed in nopython mode pipeline (step: nopython mode backend); Failed in nopython mode pipeline (step: nopython mode backend); Failed in nopython mode pipeline (step: nopython frontend); No implementation of function Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>) found for signature:; ; >>> run_quicksort(array(int32, 1d, C)); ; There are 2 candidate implementations:; - Of which 2 did not match due to:; Overload in function 'register_jitable.<locals>.wrap.<locals>.ov_wrap': File: numba/core/extending.py: Line 150.; With argument(s): '(array(int32, 1d, C))':; Rejected as the implementation raised a specific error:; UnsupportedError: Failed in nopython mode pipeline (step: analyzing bytecode); Use of unsupported opcode (LOAD_ASSERTION_ERROR) found; ; File ""../../../../.local/lib/python3.9/site-packages/numba/misc/quicksort.py"", line 180:; def run_quicksort(A):; <source elided>; while high - low >= SMALL_QUICKSORT:; assert n < MAX_STACK; ^; ; raised from /home/gabriel/.local/lib/python3.9/site-packages/numba/core/byteflow.py:269. During: resolving callee type: Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>); During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/numba/np/arrayobj.py (5007). File ""../../../../.local/lib/python3.9/site-packages/numba/np/arrayobj.py"", line 5007:; def array_sort_impl(arr):; <source elided>; # Note we clobber the return value; sort_func(arr); ^. During: lo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1652:8764,wrap,wrap,8764,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652,1,['wrap'],['wrap']
Integrability,"0.0; cython_runtime NA; dateutil 2.8.1; decorator 4.4.2; easydev 0.10.1; get_version 2.1; gseapy 0.10.1; h5py 2.10.0; idna 2.10; ipykernel 5.3.4; ipython_genutils 0.2.0; jedi 0.15.2; jinja2 2.11.2; joblib 0.16.0; jsonschema 3.2.0; kiwisolver 1.2.0; legacy_api_wrap 1.2; llvmlite 0.34.0; lxml 4.5.2; markupsafe 1.1.1; matplotlib 3.3.2; mpl_toolkits NA; natsort 7.0.1; nbformat 5.0.7; networkx 2.5; numba 0.51.2; numexpr 2.7.1; numpy 1.19.2; packaging 20.4; pandas 1.0.1; parso 0.5.2; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prometheus_client NA; prompt_toolkit 3.0.7; ptyprocess 0.6.0; pvectorc NA; pygments 2.7.0; pylab NA; pyparsing 2.4.7; pyrsistent NA; pytz 2020.1; requests 2.23.0; requests_cache 0.5.2; scanpy 1.6.0; scipy 1.5.2; seaborn 0.11.0; send2trash NA; setuptools_scm NA; sinfo 0.3.1; six 1.15.0; sklearn 0.23.2; socks 1.7.1; soupsieve 2.0.1; statsmodels 0.12.0; storemagic NA; tables 3.6.1; terminado 0.8.3; tornado 6.0.4; traitlets 5.0.4; urllib3 1.25.10; wcwidth 0.2.5; wrapt 1.12.1; xlsxwriter 1.3.3; zmq 19.0.2; -----; IPython 7.18.1; jupyter_client 6.1.7; jupyter_core 4.6.3; jupyterlab 2.2.8; notebook 6.1.4; -----; Python 3.8.5 | packaged by conda-forge | (default, Aug 29 2020, 01:22:49) [GCC 7.5.0]; Linux-4.4.0-142-generic-x86_64-with-glibc2.10; 64 logical CPU cores, x86_64; -----; Session information updated at 2020-09-16 11:03; ```. Here is the error message:. ```; ---------------------------------------------------------------------------; InvalidIndexError Traceback (most recent call last); <ipython-input-37-b22ada65a1cd> in <module>; 1 # Create Concatenated anndata object for all timepoints; 2 #alldays = e125.concatenate(e135, e145, e155, uns_merge=""unique""); ----> 3 alldays = e125.concatenate(e135). ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/anndata.py in concatenate(self, join, batch_key, batch_categories, uns_merge, index_unique, fill_value, *adatas); 1696 all_adatas = (self,) + tuple(adatas); 1697 ; -> 169",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1409#issuecomment-693478875:1464,wrap,wrapt,1464,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409#issuecomment-693478875,1,['wrap'],['wrapt']
Integrability,0.2.0; bleach | 4.0.0 | 4.0.0; brotlipy | 0.7.0 | 0.7.0; cached-property | 1.5.2 | 1.5.2; certifi | 2021.5.30 | 2021.5.30; cffi | 1.14.6 | 1.14.6; charset-normalizer | 2.0.4 | 2.0.4; colorama | 0.4.4 | 0.4.4; contextvars | 2.4 | 2.4; **cryptography | 3.4.7 | 35.0.0**; cycler | 0.10.0 | 0.11.0; dataclasses | 0.8 | 0.8; decorator | 4.4.2 | 4.4.2; defusedxml | 0.7.1 | 0.7.1; entrypoints | 0.3 | 0.3; get-version | 2.1 | 2.1; h5py | 3.1.0 | 3.1.0; idna | 3.2 | 3.2; igraph | 0.9.8 | 0.9.8; immutables | 0.16 | 0.16; importlib-metadata | 4.8.1 | 4.8.1; ipykernel | 5.3.4 | 5.3.4; ipython | 7.16.1 | 7.16.1; ipython-genutils | 0.2.0 | 0.2.0; jedi | 0.17.0 | 0.17.0; **Jinja2 | 3.0.1 | 3.0.2**; joblib | 1.1.0 | 1.1.0; json5 | 0.9.6 | 0.9.6; jsonschema | 3.2.0 | 3.2.0; jupyter-client | 7.0.1 | 7.0.1; jupyter-core | 4.8.1 | 4.8.1; jupyter-server | 1.4.1 | 1.4.1; **jupyterlab | 3.1.7 | 3.2.1**; jupyterlab-pygments | 0.1.2 | 0.1.2; jupyterlab-server | 2.8.2 | 2.8.2; kiwisolver | 1.3.1 | 1.3.1; legacy-api-wrap | 1.2 | 1.2; leidenalg | 0.8.8 | 0.8.8; llvmlite | 0.36.0 | 0.36.0; MarkupSafe | 2.0.1 | 2.0.1; matplotlib | 3.3.4 | 3.3.4; mistune | 0.8.4 | 0.8.4; **natsort | 7.1.1 | 8.0.0**; nbclassic | 0.2.6 | 0.2.6; nbclient | 0.5.3 | 0.5.3; nbconvert | 6.0.7 | 6.0.7; nbformat | 5.1.3 | 5.1.3; nest-asyncio | 1.5.1 | 1.5.1; networkx | 2.5.1 | 2.5.1; notebook | 6.4.3 | 6.4.3; numba | 0.53.1 | 0.53.1; numexpr | 2.7.3 | 2.7.3; numpy | 1.19.5 | 1.19.5; packaging | 21 | 21; pandas | 1.1.5 | 1.1.5; pandocfilters | 1.4.3 | 1.4.3; parso | 0.8.2 | 0.8.2; patsy | 0.5.2 | 0.5.2; pickleshare | 0.7.5 | 0.7.5; Pillow | 8.4.0 | 8.4.0; pip | 21.0.1 | 21.2.2; prometheus-client | 0.11.0 | 0.11.0; prompt-toolkit | 3.0.20 | 3.0.20; pycparser | 2.2 | 2.2; Pygments | 2.10.0 | 2.10.0; pynndescent | 0.5.5 | 0.5.5; pyOpenSSL | 20.0.1 | 21.0.0; **pyparsing | 2.4.7 | 3.0.4**; pyrsistent | 0.17.3 | 0.17.3; PySocks | 1.7.1 | 1.7.1; python-dateutil | 2.8.2 | 2.8.2; python-igraph | 0.9.8 | 0.9.8; pytz | 2021.3 | 2021.3; ,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045:4378,wrap,wrap,4378,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045,1,['wrap'],['wrap']
Integrability,"0:00:00); extracting highly variable genes; finished (0:00:00); --> added; 'highly_variable', boolean vector (adata.var); 'means', float vector (adata.var); 'dispersions', float vector (adata.var); 'dispersions_norm', float vector (adata.var); normalizing counts per cell; finished (0:00:00); normalizing counts per cell; finished (0:00:00); Embedding transcriptomes using PCA...; using data matrix X directly; Automatically set threshold at doublet score = 0.42; Detected doublet rate = 0.3%; Estimated detectable doublet fraction = 5.2%; Overall doublet rate:; 	Expected = 5.0%; 	Estimated = 6.6%; Scrublet finished (0:00:14); ```. I'm still not sure what actually caused the problem, but it seems that some dependency inconsistency occurred when performing PCA within the pipeline. Perhaps some package required for the `sc.pp.scrublet()` pipeline needs to be updated to a newer version?. Here are the details of the packages in the virtual environment when I ran the code on my desktop (failed case):; ```; channels:; - pytorch; - plotly; - conda-forge; - bioconda; - defaults; dependencies:; - anndata=0.10.7; - anyio=4.4.0; - appnope=0.1.4; - argcomplete=3.3.0; - argh=0.31.2; - argon2-cffi=23.1.0; - argon2-cffi-bindings=21.2.0; - arpack=3.8.0; - array-api-compat=1.7.1; - arrow=1.3.0; - asttokens=2.4.1; - async-lru=2.0.4; - attrs=23.2.0; - babel=2.14.0; - beautifulsoup4=4.12.3; - biopython=1.83; - blas=2.120; - blas-devel=3.9.0; - bleach=6.1.0; - blosc=1.21.5; - brotli=1.1.0; - brotli-bin=1.1.0; - brotli-python=1.1.0; - bzip2=1.0.8; - c-ares=1.28.1; - c-blosc2=2.14.4; - ca-certificates=2024.6.2; - cached-property=1.5.2; - cached_property=1.5.2; - certifi=2024.6.2; - cffi=1.16.0; - charset-normalizer=3.3.2; - colorama=0.4.6; - colorcet=3.1.0; - colorful=0.5.6; - comm=0.2.2; - contourpy=1.2.1; - cycler=0.12.1; - debugpy=1.8.1; - decorator=5.1.1; - defusedxml=0.7.1; - dill=0.3.8; - dnspython=2.6.1; - entrypoints=0.4; - et_xmlfile=1.1.0; - exceptiongroup=1.2.0; - executing=2.0.1; - ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3116:2639,depend,dependencies,2639,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3116,1,['depend'],['dependencies']
Integrability,1 ; jeepney 0.6.0 pyhd3eb1b0_0 ; jinja2 3.0.0 pyhd3eb1b0_0 ; joblib 1.0.1 pyhd3eb1b0_0 ; jpeg 9b h024ee3a_2 ; json5 0.9.5 py_0 ; jsonschema 3.2.0 py_2 ; jupyter 1.0.0 py38_7 ; jupyter-packaging 0.7.12 pyhd3eb1b0_0 ; jupyter_client 6.1.12 pyhd3eb1b0_0 ; jupyter_console 6.4.0 pyhd3eb1b0_0 ; jupyter_contrib_core 0.3.3 py_2 conda-forge; jupyter_contrib_nbextensions 0.5.1 pyhd8ed1ab_2 conda-forge; jupyter_core 4.7.1 py38h06a4308_0 ; jupyter_highlight_selected_word 0.2.0 py38h578d9bd_1002 conda-forge; jupyter_latex_envs 1.4.6 pyhd8ed1ab_1002 conda-forge; jupyter_nbextensions_configurator 0.4.1 py38h578d9bd_2 conda-forge; jupyter_server 1.4.1 py38h06a4308_0 ; jupyterlab 3.0.14 pyhd3eb1b0_1 ; jupyterlab_pygments 0.1.2 py_0 ; jupyterlab_server 2.4.0 pyhd3eb1b0_0 ; jupyterlab_widgets 1.0.0 pyhd3eb1b0_1 ; keyring 23.0.1 py38h06a4308_0 ; kiwisolver 1.3.1 py38h2531618_0 ; krb5 1.17.1 h173b8e3_0 ; lazy-object-proxy 1.6.0 py38h27cfd23_0 ; lcms2 2.12 h3be6417_0 ; ld_impl_linux-64 2.33.1 h53a641e_7 ; legacy-api-wrap 1.2 py_0 conda-forge; leidenalg 0.8.2 py38habedc41_0 conda-forge; libarchive 3.4.2 h62408e4_0 ; libcurl 7.69.1 h20c2e04_0 ; libedit 3.1.20210216 h27cfd23_1 ; libev 4.33 h7b6447c_0 ; libffi 3.2.1 hf484d3e_1007 ; libgcc-ng 9.3.0 h2828fa1_19 conda-forge; libgfortran-ng 7.3.0 hdf63c60_0 ; libgomp 9.3.0 h2828fa1_19 conda-forge; libiconv 1.15 h63c8f33_5 ; liblief 0.10.1 he6710b0_0 ; libllvm10 10.0.1 hbcb73fb_5 ; libllvm9 9.0.1 h4a3c616_1 ; libpng 1.6.37 hbc83047_0 ; libsodium 1.0.18 h7b6447c_0 ; libspatialindex 1.9.3 h2531618_0 ; libssh2 1.9.0 h1ba5d50_1 ; libstdcxx-ng 9.1.0 hdf63c60_0 ; libtiff 4.2.0 h85742a9_0 ; libtool 2.4.6 h7b6447c_1005 ; libuuid 1.0.3 h1bed415_2 ; libuv 1.40.0 h7b6447c_0 ; libwebp-base 1.2.0 h27cfd23_0 ; libxcb 1.14 h7b6447c_0 ; libxml2 2.9.10 hb55368b_3 ; libxslt 1.1.34 hc22bd24_0 ; llvmlite 0.36.0 py38h612dafd_4 ; locket 0.2.1 py38h06a4308_1 ; loompy 2.0.16 py_0 bioconda; lxml 4.6.3 py38h9120a33_0 ; lz4-c 1.9.3 h2531618_0 ; lzo 2.10 h7b6447c_2 ; magic-i,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:9905,wrap,wrap,9905,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310,2,['wrap'],['wrap']
Integrability,"1 X,; 392 n_neighbors,. /hps/software/users/marioni/Leah/miniconda3/envs/scvelo/lib/python3.8/site-packages/umap/umap_.py in fuzzy_simplicial_set(X, n_neighbors, random_state, metric, metric_kwds, knn_indices, knn_dists, angular, set_op_mix_ratio, local_connectivity, apply_set_operations, verbose); 600 knn_dists = knn_dists.astype(np.float32); 601 ; --> 602 sigmas, rhos = smooth_knn_dist(; 603 knn_dists, float(n_neighbors), local_connectivity=float(local_connectivity),; 604 ). SystemError: CPUDispatcher(<function smooth_knn_dist at 0x14a113bac160>) returned a result with an error set. time: 4.73 s (started: 2021-08-18 11:47:40 +01:00); ```. #### Versions. <details>. ```pytb; WARNING: If you miss a compact list, please try `print_header`!; The `sinfo` package has changed name and is now called `session_info` to become more discoverable and self-explanatory. The `sinfo` PyPI package will be kept around to avoid breaking old installs and you can downgrade to 0.3.2 if you want to use it without seeing this message. For the latest features and bug fixes, please install `session_info` instead. The usage and defaults also changed slightly, so please review the latest README at https://gitlab.com/joelostblom/session_info.; -----; anndata 0.7.6; scanpy 1.8.1; sinfo 0.3.4; -----; PIL 8.3.1; autotime 0.3.1; backcall 0.2.0; bottleneck 1.3.2; cffi 1.14.6; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; decorator 5.0.9; defusedxml 0.7.1; h5py 2.10.0; igraph 0.9.6; ipykernel 6.0.3; ipython_genutils 0.2.0; jedi 0.18.0; joblib 1.0.1; kiwisolver 1.3.1; leidenalg 0.8.7; llvmlite 0.33.0; loompy 3.0.6; louvain 0.7.0; matplotlib 3.4.2; matplotlib_inline NA; mkl 2.4.0; mpl_toolkits NA; natsort 7.1.1; numba 0.50.1; numexpr 2.7.3; numpy 1.20.3; numpy_groupies 0.9.13; packaging 21.0; pandas 1.3.0; parso 0.8.2; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.19; ptyprocess 0.7.0; pycparser 2.20; pygments 2.9.0; pyparsing 2.4.7; pytz 2021.1; scipy 1.6.2; scvelo 0.2.3; ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1983:4068,message,message,4068,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1983,1,['message'],['message']
Integrability,1 he4413a7_1000 conda-forge; freetype 2.10.2 he06d7ca_0 conda-forge; get-version 2.1 pypi_0 pypi; glib 2.63.1 h3eb4bd4_1 ; gst-plugins-base 1.14.0 hbbd80ab_1 ; gstreamer 1.14.0 hb31296c_0 ; h5py 2.10.0 pypi_0 pypi; hdf5 1.10.4 hb1b8bf9_0 ; icu 58.2 hf484d3e_1000 conda-forge; importlib-metadata 1.6.1 py36h9f0ad1d_0 conda-forge; importlib_metadata 1.6.1 0 conda-forge; intel-openmp 2020.1 217 ; ipyevents 0.7.1 py_0 conda-forge; ipykernel 5.3.0 py36h95af2a2_0 conda-forge; ipython 7.15.0 py36h9f0ad1d_0 conda-forge; ipython_genutils 0.2.0 py_1 conda-forge; ipywidgets 7.5.1 py_0 conda-forge; jedi 0.17.1 py36h9f0ad1d_0 conda-forge; jinja2 2.11.2 pyh9f0ad1d_0 conda-forge; joblib 0.15.1 py_0 ; jpeg 9d h516909a_0 conda-forge; jsonschema 3.2.0 py36h9f0ad1d_1 conda-forge; jupyter 1.0.0 py_2 conda-forge; jupyter_client 6.1.3 py_0 conda-forge; jupyter_console 6.1.0 py_1 conda-forge; jupyter_core 4.6.3 py36h9f0ad1d_1 conda-forge; kiwisolver 1.2.0 py36hfd86e86_0 ; ld_impl_linux-64 2.33.1 h53a641e_7 ; legacy-api-wrap 1.2 pypi_0 pypi; leidenalg 0.8.0 pypi_0 pypi; libedit 3.1.20191231 h7b6447c_0 ; libffi 3.3 he6710b0_1 ; libgcc-ng 9.1.0 hdf63c60_0 ; libgfortran-ng 7.3.0 hdf63c60_0 ; libpng 1.6.37 hed695b0_1 conda-forge; libsodium 1.0.17 h516909a_0 conda-forge; libstdcxx-ng 9.1.0 hdf63c60_0 ; libuuid 2.32.1 h14c3975_1000 conda-forge; libxcb 1.13 h14c3975_1002 conda-forge; libxml2 2.9.10 he19cac6_1 ; llvmlite 0.33.0 pypi_0 pypi; lz4-c 1.9.2 he6710b0_0 ; lzo 2.10 h7b6447c_2 ; markupsafe 1.1.1 py36h8c4c3a4_1 conda-forge; matplotlib 3.2.2 0 ; matplotlib-base 3.2.2 py36hef1b27d_0 ; mistune 0.8.4 py36h8c4c3a4_1001 conda-forge; mkl 2020.1 217 ; mkl-service 2.3.0 py36he904b0f_0 ; mkl_fft 1.1.0 py36h23d657b_0 ; mkl_random 1.1.1 py36h0573a6f_0 ; mock 4.0.2 py_0 ; natsort 7.0.1 pypi_0 pypi; nbconvert 5.6.1 py36h9f0ad1d_1 conda-forge; nbformat 5.0.6 py_0 conda-forge; ncurses 6.2 he6710b0_1 ; networkx 2.4 pypi_0 pypi; notebook 6.0.3 py36h9f0ad1d_0 conda-forge; numba 0.50.0 pypi_0 pypi; numexpr 2.7.1 ,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1293:7086,wrap,wrap,7086,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1293,1,['wrap'],['wrap']
Integrability,"1 try:; --> 202 return func(*args, **kwargs); 203 except Exception as e:. File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\registry.py:235, in Reader.read_elem(self, elem, modifiers); 234 if self.callback is not None:; --> 235 return self.callback(read_func, elem.name, elem, iospec=get_spec(elem)); 236 else:. File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:241, in read_h5ad.<locals>.callback(func, elem_name, elem, iospec); 240 return read_dataframe(elem); --> 241 return func(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\methods.py:323, in read_array(elem, _reader); 319 @_REGISTRY.register_read(H5Array, IOSpec(""array"", ""0.2.0"")); 320 @_REGISTRY.register_read(ZarrArray, IOSpec(""array"", ""0.2.0"")); 321 @_REGISTRY.register_read(ZarrArray, IOSpec(""string-array"", ""0.2.0"")); 322 def read_array(elem, _reader):; --> 323 return elem[()]. File h5py\_objects.pyx:54, in h5py._objects.with_phil.wrapper(). File h5py\_objects.pyx:55, in h5py._objects.with_phil.wrapper(). File D:\Python3.10.9\lib\site-packages\h5py\_hl\dataset.py:768, in Dataset.__getitem__(self, args, new_dtype); 767 try:; --> 768 return self._fast_reader.read(args); 769 except TypeError:. File h5py\_selector.pyx:368, in h5py._selector.Reader.read(). File h5py\_selector.pyx:342, in h5py._selector.Reader.make_array(). MemoryError: Unable to allocate 9.90 GiB for an array with shape (310385, 8563) and data type float32. The above exception was the direct cause of the following exception:. AnnDataReadError Traceback (most recent call last); Cell In[2], line 4; 2 import pandas as pd; 3 import scanpy as sc; ----> 4 annData = sc.read_h5ad(""ReplogleWeissman2022_K562_essential.h5ad""). File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:243, in read_h5ad(filename, backed, as_sparse, as_sparse_fmt, chunk_size); 240 return read_dataframe(elem); 241 return func(elem); --> 243 adata = read_dispatched(f, callback=callback); 245 # Backwards compat (should figure out which version); 246 i",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2551:2234,wrap,wrapper,2234,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551,1,['wrap'],['wrapper']
Integrability,"1. You could create a heuristic grid size depending on cell numbers, or it's probably easier to just put grid dimensions as a user parameter with some (low) default value.; 2. I've been approaching this from the perspective that you care about where the densities occur on the visualization. That's why you can change the `basis` for the calculations and plotting. From my perspective, calculating densities over clusters and comparing these is actually just a sub-optimal replacement for testing for differential compositions. This is a separate problem, where the data should be modeled statistically, accounting for the compositional nature of the data. So sticking to the visualization is probably the right way forward for this function. On that note... we could use a seaborn heatmap function to plot the differential grid points. Overall I reckon we are moving toward a new plotting function here which does some calculations on the backend. Something like `sc.pl.embedding_density_diff()` where you take the output from `sc.tl.embedding_density()` and interpolate to a grid layout, rescale to sum to 1 across each grid separately, take the diff of two conditions, and then plot everything in a heatmap. Doesn't seem as difficult as I thought. I will get onto this when (read: if) I (ever) have some spare time 😉.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/575#issuecomment-478914589:42,depend,depending,42,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575#issuecomment-478914589,1,['depend'],['depending']
Integrability,"1.1; easydev 0.12.0; entrypoints 0.3; fsspec 2021.08.1; get_version 3.5.4; google NA; gridfs NA; gseapy 0.10.8; h5py 3.6.0; html5lib 1.1; idna 3.2; igraph 0.9.11; importlib_metadata NA; ipykernel 6.4.1; ipython_genutils 0.2.0; ipywidgets 7.6.5; itsdangerous 2.0.1; jedi 0.18.0; jinja2 2.11.3; joblib 1.1.0; json5 NA; jsonschema 3.2.0; jupyter_server 1.4.1; jupyterlab_server 2.8.2; kiwisolver 1.3.1; legacy_api_wrap 1.2; leidenalg 0.8.9; llvmlite 0.36.0; lxml 4.6.3; markupsafe 1.1.1; matplotlib 3.4.3; matplotlib_inline NA; mpl_toolkits NA; natsort 8.1.0; nbclassic NA; nbformat 5.1.3; nbinom_ufunc NA; numba 0.53.0; numexpr 2.7.3; numpy 1.22.3; opt_einsum v3.3.0; packaging 21.0; pandas 1.4.3; parso 0.8.2; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prometheus_client NA; prompt_toolkit 3.0.20; psutil 5.8.0; ptyprocess 0.7.0; pvectorc NA; pyarrow 8.0.0; pycparser 2.20; pydev_ipython NA; pydevconsole NA; pydevd 2.4.1; pydevd_concurrency_analyser NA; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pyexpat NA; pygments 2.10.0; pylab NA; pymongo 4.1.0; pyparsing 3.0.4; pyro 1.8.1; pyrsistent NA; pytorch_lightning 1.5.10; pytz 2021.3; regex 2.5.97; requests 2.26.0; requests_cache 0.9.4; rich NA; scipy 1.7.1; scvelo 0.2.4; scvi 0.12.0; send2trash NA; setuptools_scm NA; six 1.16.0; sklearn 1.0.2; sniffio 1.2.0; socks 1.7.1; soupsieve 2.2.1; sphinxcontrib NA; storemagic NA; tables 3.6.1; tblib 1.7.0; tensorboard 2.8.0; texttable 1.6.4; threadpoolctl 2.2.0; tlz 0.11.0; toolz 0.11.1; torch 1.11.0; torchmetrics 0.8.1; tornado 6.1; tqdm 4.62.3; traitlets 5.1.0; typing_extensions NA; ujson 4.0.2; url_normalize 1.4.3; urllib3 1.26.7; utils NA; wcwidth 0.2.5; webencodings 0.5.1; wrapt 1.12.1; yaml 5.4.1; zipp NA; zmq 22.2.1; zope NA; -----; IPython 7.29.0; jupyter_client 6.1.12; jupyter_core 4.8.1; jupyterlab 3.2.1; notebook 6.4.5; -----; Python 3.9.7 (default, Sep 16 2021, 08:50:36) [Clang 10.0.0 ]; macOS-10.16-x86_64-i386-64bit; 10 logical CPU cores, i386. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2310:6689,wrap,wrapt,6689,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2310,1,['wrap'],['wrapt']
Integrability,1.4.2. 1.5.X seemed to break the dependencies.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1407#issuecomment-691137508:33,depend,dependencies,33,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407#issuecomment-691137508,1,['depend'],['dependencies']
Integrability,10X Visium-ScRNA-seq integrative analysis,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1386:21,integrat,integrative,21,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1386,1,['integrat'],['integrative']
Integrability,"1=h7b6447c_3; - pip:; - anndata==0.7.5; - cached-property==1.5.2; - click==7.1.2; - cycler==0.10.0; - get-version==2.1; - h5py==3.1.0; - importlib-metadata==3.4.0; - joblib==1.0.0; - kiwisolver==1.3.1; - legacy-api-wrap==1.2; - leidenalg==0.8.3; - llvmlite==0.35.0; - loompy==3.0.6; - louvain==0.7.0; - matplotlib==3.3.4; - natsort==7.1.1; - networkx==2.5; - numba==0.52.0; - numexpr==2.7.2; - numpy==1.20.0; - numpy-groupies==0.9.13; - pandas==1.2.1; - patsy==0.5.1; - pillow==8.1.0; - python-igraph==0.8.3; - pytz==2021.1; - scanpy==1.6.1; - scikit-learn==0.24.1; - scipy==1.6.0; - scvelo==0.2.2; - seaborn==0.11.1; - setuptools-scm==5.0.1; - sinfo==0.3.1; - statsmodels==0.12.1; - stdlib-list==0.8.0; - tables==3.6.1; - texttable==1.6.3; - threadpoolctl==2.1.0; - tqdm==4.56.0; - typing-extensions==3.7.4.3; - umap-learn==0.4.6; ```. I can reproduce the issue with a Docker container that only has the minimal conda environment above. Additionally, I already tried installing the exact same dependency versions in the new environment, but got the same results. ; If you need access to the data and the container please contact me and I will make it available to you.; The data is already at the ICB cluster. Code:. ```; from os import path; import numpy as np; import matplotlib.pyplot as plt; import scanpy as sc; import scanpy.external as sce; from os import listdir; import pandas as pd; import seaborn as sb; import datetime, time; import scvelo as scv. from matplotlib.colors import LinearSegmentedColormap. #Define a nice colour map for gene expression; from matplotlib import colors. def timestamp():; ts = time.time(); st = datetime.datetime.fromtimestamp(ts).strftime('%d-%m-%Y %H:%M:%S'); return st. # Exporting folder. folder = ""/output""; sc.settings.figdir = folder + ""Plots/""; sc.set_figure_params(vector_friendly = True, dpi=300). sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3); sc.logging.print_version_and_date(); sc.logging.print_header(). ad",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1625:5400,depend,dependency,5400,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625,1,['depend'],['dependency']
Integrability,2.9.0.20240316; - typing-extensions=4.12.2; - typing_extensions=4.12.2; - typing_utils=0.1.0; - tzdata=2024a; - umap-learn=0.5.5; - uri-template=1.3.0; - urllib3=2.2.1; - wcwidth=0.2.13; - webcolors=24.6.0; - webencodings=0.5.1; - websocket-client=1.8.0; - wheel=0.43.0; - xlrd=1.2.0; - xorg-libxau=1.0.11; - xorg-libxdmcp=1.1.3; - xz=5.2.6; - yaml=0.2.5; - zeromq=4.3.5; - zipp=3.19.2; - zlib-ng=2.0.7; - zstd=1.5.6; - pip:; - absl-py==2.1.0; - astunparse==1.6.3; - bcbio-gff==0.7.1; - flatbuffers==24.3.25; - gast==0.5.4; - google-pasta==0.2.0; - grpcio==1.64.1; - keras==3.3.3; - libclang==18.1.1; - markdown==3.6; - markdown-it-py==3.0.0; - mdurl==0.1.2; - ml-dtypes==0.3.2; - namex==0.0.8; - opt-einsum==3.3.0; - optree==0.11.0; - rich==13.7.1; - tensorboard==2.16.2; - tensorboard-data-server==0.7.2; - tensorflow==2.16.1; - tensorflow-io-gcs-filesystem==0.37.0; - termcolor==2.4.0; - werkzeug==3.0.3; - wrapt==1.16.0; ```. The virtual environment on my laptop (successful case):; ```; channels:; - pytorch; - bioconda; - conda-forge; dependencies:; - adjusttext=1.0.4; - anndata=0.10.5.post1; - anyio=3.7.1; - aom=3.5.0; - appnope=0.1.3; - argcomplete=3.3.0; - argh=0.31.2; - argon2-cffi=23.1.0; - argon2-cffi-bindings=21.2.0; - arpack=3.8.0; - array-api-compat=1.4.1; - arrow=1.2.3; - asttokens=2.2.1; - async-lru=2.0.4; - attrs=23.1.0; - babel=2.12.1; - backcall=0.2.0; - backports=1.0; - backports.functools_lru_cache=1.6.5; - beautifulsoup4=4.12.2; - bleach=6.0.0; - blosc=1.21.4; - brotli=1.0.9; - brotli-bin=1.0.9; - brotli-python=1.0.9; - bzip2=1.0.8; - c-ares=1.19.1; - c-blosc2=2.10.2; - ca-certificates=2024.6.2; - cached-property=1.5.2; - cached_property=1.5.2; - cairo=1.18.0; - certifi=2024.6.2; - cffi=1.15.1; - charset-normalizer=3.2.0; - colorama=0.4.6; - colorcet=3.0.1; - colorful=0.5.4; - comm=0.1.4; - contourpy=1.1.0; - cryptography=41.0.4; - cycler=0.11.0; - dav1d=1.2.1; - debugpy=1.6.8; - decorator=5.1.1; - defusedxml=0.7.1; - dill=0.3.7; - dnspython=2.4.2; - entrypoi,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3116:8667,depend,dependencies,8667,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3116,1,['depend'],['dependencies']
Integrability,"3 cb = cbar.colorbar_factory(cax, mappable, **cb_kw); 2345 self.sca(current_ax); 2346 self.stale = True. File ~/miniconda3/envs/scvi10j/lib/python3.8/site-packages/matplotlib/colorbar.py:1731, in colorbar_factory(cax, mappable, **kwargs); 1729 cb = ColorbarPatch(cax, mappable, **kwargs); 1730 else:; -> 1731 cb = Colorbar(cax, mappable, **kwargs); 1733 cid = mappable.callbacksSM.connect('changed', cb.update_normal); 1734 mappable.colorbar = cb. File ~/miniconda3/envs/scvi10j/lib/python3.8/site-packages/matplotlib/colorbar.py:1225, in Colorbar.__init__(self, ax, mappable, **kwargs); 1223 if isinstance(mappable, martist.Artist):; 1224 _add_disjoint_kwargs(kwargs, alpha=mappable.get_alpha()); -> 1225 ColorbarBase.__init__(self, ax, **kwargs). File ~/miniconda3/envs/scvi10j/lib/python3.8/site-packages/matplotlib/cbook/deprecation.py:451, in _make_keyword_only.<locals>.wrapper(*args, **kwargs); 445 if len(args) > idx:; 446 warn_deprecated(; 447 since, message=""Passing the %(name)s %(obj_type)s ""; 448 ""positionally is deprecated since Matplotlib %(since)s; the ""; 449 ""parameter will become keyword-only %(removal)s."",; 450 name=name, obj_type=f""parameter of {func.__name__}()""); --> 451 return func(*args, **kwargs). TypeError: __init__() got an unexpected keyword argument 'location'; ```; I was having this problem with scanpy 1.9.1 and matplotlib 3.3.2 I just updated to 1.9.2 and confirm the issue is unchanged; ```; scanpy==1.9.2 anndata==0.8.0 umap==0.5.2 numpy==1.21.5 scipy==1.8.0 pandas==1.4.1 scikit-learn==0.23.2 statsmodels==0.13.2 python-igraph==0.9.9 louvain==0.7.1 pynndescent==0.5.6; -----; anndata 0.8.0; scanpy 1.9.2; -----; PIL 9.0.1; absl NA; asttokens NA; attr 21.4.0; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; brotli NA; certifi 2022.06.15; cffi 1.14.5; charset_normalizer 2.0.12; chex 0.1.5; cloudpickle 2.2.0; colorama 0.4.4; contextlib2 NA; cycler 0.10.0; cython_runtime NA; cytoolz 0.12.0; dask 2022.11.1; dateutil 2.8.2; debugpy 1.5.1; decorator 5.1.1; defuse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2318#issuecomment-1445561483:3121,wrap,wrapper,3121,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2318#issuecomment-1445561483,4,"['message', 'wrap']","['message', 'wrapper']"
Integrability,"3.8/site-packages/anndata/_core/sparse_dataset.py in to_memory(self); 370 mtx = format_class(self.shape, dtype=self.dtype); --> 371 mtx.data = self.group[""data""][...]; 372 mtx.indices = self.group[""indices""][...]. h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/h5py/_hl/dataset.py in __getitem__(self, args); 572 fspace = selection.id; --> 573 self.id.read(mspace, fspace, arr, mtype, dxpl=self._dxpl); 574 . h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/h5d.pyx in h5py.h5d.DatasetID.read(). h5py/_proxy.pyx in h5py._proxy.dset_rw(). h5py/_proxy.pyx in h5py._proxy.H5PY_H5Dread(). OSError: Can't read data (file read failed: time = Sat Aug 1 13:27:54 2020; , filename = '/path.../filtered_gene_bc_matrices.h5ad', file descriptor = 47, errno = 5, error message = 'Input/output error', buf = 0x55ec782e9031, total read size = 7011, bytes this sub-read = 7011, bytes actually read = 18446744073709551615, offset = 0). During handling of the above exception, another exception occurred:. AnnDataReadError Traceback (most recent call last); <ipython-input-14-faac769583f8> in <module>; 17 #while True:; 18 #try:; ---> 19 adatas.append(sc.read_h5ad(file)); 20 file_diffs.append('_'.join([file.split('/')[i] for i in diff_path_idx])); 21 #break. ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/anndata/_io/h5ad.py in read_h5ad(filename, backed, as_sparse, as_sparse_fmt, chunk_size); 411 d[k] = read_dataframe(f[k]); 412 else: # Base case; --> 413 d[k] = read_attribute(f[k]); 414 ; 415 d[""raw""] = _read_raw(f, as_sparse, rdasp). ~/miniconda3/envs/rpy2_3/lib/python3.8/functools.py in wrapper(*args, **kw); 873 '1 positional argument'); 874 ; --> 875 return dispatch(args[0].__class__)(*args, **kw); 876 ; 877 funcname = getattr(func, '__name__', 'singledispatch function'). ~/miniconda3/envs/rpy2_3/lib/python3.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1351:1995,message,message,1995,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351,1,['message'],['message']
Integrability,"39 in batch_execute_tasks; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 64 in run; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 92 in _worker; File ""<venv>/lib/python3.12/threading.py"", line 1010 in run; File ""<venv>/lib/python3.12/threading.py"", line 1073 in _bootstrap_inner; File ""<venv>/lib/python3.12/threading.py"", line 1030 in _bootstrap. Thread 0x000000016ed23000 (most recent call first):; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 89 in _worker; File ""<venv>/lib/python3.12/threading.py"", line 1010 in run; File ""<venv>/lib/python3.12/threading.py"", line 1073 in _bootstrap_inner; File ""<venv>/lib/python3.12/threading.py"", line 1030 in _bootstrap. Current thread 0x000000016dd17000 (most recent call first):; File ""~/Dev/scanpy/src/scanpy/_compat.py"", line 133 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 109 in _; File ""<venv>/lib/python3.12/functools.py"", line 909 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 30 in func; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 157 in get; File ""<venv>/lib/python3.12/site-packages/dask/optimization.py"", line 1001 in __call__; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 225 in execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 239 in batch_execute_tasks; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 58 in run; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 92 in _worker; File ""<venv>/lib/python3.12/threading.py"", line 1010 in run; File ""<venv>/lib/python3.12/threading.py"", line 1073 in _bootstrap_inner; File ""<venv>/lib/python3.12/threading.py"", line 1030 in _bootstrap. Thread 0x000000016cd0b000 (most recent call first):; File ""<venv>/lib/pytho",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478:2889,wrap,wrapper,2889,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478,2,['wrap'],['wrapper']
Integrability,"3c4f42 NA; absl NA; astunparse 1.6.3; atomicwrites 1.4.1; autoreload NA; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; brotli NA; bs4 4.11.1; certifi 2022.09.14; cffi 1.15.1; chardet 5.0.0; charset_normalizer 2.1.1; cloudpickle 2.2.0; colorama 0.4.5; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; debugpy 1.6.3; decorator 5.1.1; defusedxml 0.7.1; dill 0.3.5.1; entrypoints 0.4; flatbuffers 2.0; gast 0.5.3; google NA; h5py 3.7.0; hypergeom_ufunc NA; idna 3.3; igraph 0.9.11; import_all NA; ipykernel 6.15.3; jedi 0.18.1; joblib 1.2.0; keras 2.8.0; keras_preprocessing 1.1.2; kiwisolver 1.4.4; leidenalg 0.8.10; llvmlite 0.38.1; louvain 0.7.1; lxml 4.9.1; matplotlib 3.5.3; matplotlib_inline 0.1.6; mpl_toolkits NA; natsort 8.2.0; nbinom_ufunc NA; ncf_ufunc NA; numba 0.55.2; numpy 1.22.4; opt_einsum v3.3.0; packaging 21.3; pandas 1.4.4; params NA; parso 0.8.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; plotly 5.10.0; prompt_toolkit 3.0.31; psutil 5.9.2; ptyprocess 0.7.0; pycparser 2.21; pydev_ipython NA; pydevconsole NA; pydevd 2.8.0; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.13.0; pynndescent 0.5.7; pyparsing 3.0.9; pytz 2022.2.1; requests 2.28.1; scipy 1.9.1; seaborn 0.12.0; session_info 1.0.0; setuptools 65.3.0; sip NA; six 1.16.0; sklearn 1.1.2; socks 1.7.1; soupsieve 2.3.2.post1; sphinxcontrib NA; spyder 5.3.3; spyder_kernels 2.3.3; spydercustomize NA; statsmodels 0.13.2; storemagic NA; tensorboard 2.8.0; tensorflow 2.8.0; termcolor 1.1.0; texttable 1.6.4; threadpoolctl 3.1.0; tornado 6.2; tqdm 4.64.1; traitlets 5.4.0; typing_extensions NA; umap 0.5.3; unicodedata2 NA; urllib3 1.26.11; wcwidth 0.2.5; wrapt 1.14.1; wurlitzer 3.0.2; yaml 6.0; zipp NA; zmq 24.0.0; -----; IPython 7.33.0; jupyter_client 7.3.5; jupyter_core 4.11.1; -----; Python 3.9.13 | packaged by conda-forge | (main, May 27 2022, 16:56:21) [GCC 10.3.0]; Linux-5.4.0-124-generic-x86_64-with-glibc2.31; -----; Session information updated at 2022-09-16 10:24]. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2330:4137,wrap,wrapt,4137,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2330,1,['wrap'],['wrapt']
Integrability,"4, in func_wrapper; return func(elem, key, val, *args, **kwargs); File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_io/specs/registry.py"", line 171, in write_elem; _REGISTRY.get_writer(dest_type, (t, elem.dtype.kind), modifiers)(; File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_io/specs/registry.py"", line 24, in wrapper; result = func(g, k, *args, **kwargs); File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_io/specs/methods.py"", line 346, in write_vlen_string_array; f.create_dataset(k, data=elem.astype(str_dtype), dtype=str_dtype, **dataset_kwargs); File ""/home/joyzheng/.local/lib/python3.8/site-packages/h5py/_hl/group.py"", line 183, in create_dataset; dsid = dataset.make_new_dset(group, shape, dtype, data, name, **kwds); File ""/home/joyzheng/.local/lib/python3.8/site-packages/h5py/_hl/dataset.py"", line 168, in make_new_dset; dset_id.write(h5s.ALL, h5s.ALL, data); File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper; File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper; File ""h5py/h5d.pyx"", line 280, in h5py.h5d.DatasetID.write; File ""h5py/_proxy.pyx"", line 145, in h5py._proxy.dset_rw; File ""h5py/_conv.pyx"", line 444, in h5py._conv.str2vlen; File ""h5py/_conv.pyx"", line 95, in h5py._conv.generic_converter; File ""h5py/_conv.pyx"", line 249, in h5py._conv.conv_str2vlen; TypeError: Can't implicitly convert non-string objects to strings. The above exception was the direct cause of the following exception:; Traceback (most recent call last):; File ""integration.py"", line 66, in <module>; adata.write_h5ad('Integrated.h5ad'); File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_core/anndata.py"", line 1918, in write_h5ad; _write_h5ad(; File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_io/h5ad.py"", line 98, in write_h5ad; write_elem(f, ""obs"", adata.obs, dataset_kwargs=dataset_kwargs); File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_io/utils.py"", line 214, ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2432:2430,wrap,wrapper,2430,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2432,1,['wrap'],['wrapper']
Integrability,"401, in _compile_for_args; error_rewrite(e, 'typing'); File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 344, in error_rewrite; reraise(type(e), e, None); File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/six.py"", line 668, in reraise; raise value.with_traceback(tb); numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend); Invalid use of type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)) with parameters (array(float64, 1d, C), array(float32, 1d, C)); Known signatures:; * (array(float32, 1d, A), array(float32, 1d, A)) -> float32; * parameterized; [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)); [2] During: typing of call at /home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py (795). File ""miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 795:; def optimize_layout(; <source elided>. dist_squared = rdist(current, other); ^. This is not usually a problem with Numba itself but instead often caused by; the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:; http://numba.pydata.org/numba-doc/latest/reference/pysupported.html; and; http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:; http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message; and traceback, along with a minimal reproducer at:; https://github.com/numba/numba/issues/new. ```. I saw a relevant [issue](https://github.com/lmcinnes/umap/issues/179) on the umap package and ; even changed line 1138 in [umap_.py](https://github.com/lmcinnes/umap/blob/80f1247de0d60eb60d7222a3cdf9aef9452ab38e/umap/umap_.py) from `embedding` to `embedding..astype(np.float32, copy=True)`, but no success. Any idea?. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/948:2337,message,message,2337,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/948,1,['message'],['message']
Integrability,"50 ); 251 else:; 252 # plot time series as gene expression vs time; 253 timeseries(; 254 adata.X[adata.obs[""dpt_order_indices""].values],; 255 var_names=adata.var_names,; (...); 258 marker=marker,; 259 ). File D:\anaconda\Lib\site-packages\scanpy\plotting\_utils.py:227, in timeseries_as_heatmap(X, var_names, highlights_x, color_map); 223 x_new[:, _hold:] = X[:, hold:]; 225 _, ax = plt.subplots(figsize=(1.5 * 4, 2 * 4)); 226 img = ax.imshow(; --> 227 np.array(X, dtype=np.float_),; 228 aspect=""auto"",; 229 interpolation=""nearest"",; 230 cmap=color_map,; 231 ); 232 plt.colorbar(img, shrink=0.5); 233 plt.yticks(range(X.shape[0]), var_names). ValueError: setting an array element with a sequence.; ```. Error in dpt_groups_pseudotime:. ```pytb; ValueError Traceback (most recent call last); Cell In[91], line 1; ----> 1 sc.pl.dpt_groups_pseudotime(a1). File D:\anaconda\Lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File D:\anaconda\Lib\site-packages\scanpy\plotting\_tools\__init__.py:276, in dpt_groups_pseudotime(adata, color_map, palette, show, save, marker); 274 """"""Plot groups and pseudotime.""""""; 275 _, (ax_grp, ax_ord) = plt.subplots(2, 1); --> 276 timeseries_subplot(; 277 adata.obs[""dpt_groups""].cat.codes,; 278 time=adata.obs[""dpt_order""].values,; 279 color=np.asarray(adata.obs[""dpt_groups""]),; 280 highlights_x=adata.uns[""dpt_changepoints""],; 281 ylabel=""dpt groups"",; 282 yticks=(; 283 np.arange(len(adata.obs[""dpt_groups""].cat.categories), dtype=int); 284 if len(adata.obs[""dpt_groups""].cat.categories) < 5; 285 else None; 286 ),; 287 palette=palette,; 288 ax=ax_grp,; 289 marker=marker,; 290 ); 291 timeseries_subplot(; 292 adata.obs[""dpt_pseudotime""].values,; ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3086:2928,wrap,wraps,2928,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3086,1,['wrap'],['wraps']
Integrability,"52 loess_outputs, loess_prediction,; 53 loess_confidence_intervals, loess_anova). ImportError: DLL load failed while importing _loess: The specified module could not be found.; ```; Step5: run `import skmisc; print(skmisc.__file__)`; ```python; import skmisc; print(skmisc.__file__); C:\Users\Park_Lab\AppData\Roaming\Python\Python38\site-packages\skmisc\__init__.py; ```; Step6: due to Step4, I follow the solution (https://github.com/has2k1/scikit-misc/issues/4) to install Numpy with mkl.; ```python; (base) C:\Users\Park_Lab>conda activate Python38; (Python38) C:\Users\Park_Lab>cd Downloads/; (Python38) C:\Users\Park_Lab\Downloads>pip install numpy-1.21.5+mkl-cp38-cp38-win_amd64.whl; Processing c:\users\park_lab\downloads\numpy-1.21.5+mkl-cp38-cp38-win_amd64.whl; Installing collected packages: numpy; Attempting uninstall: numpy; Found existing installation: numpy 1.21.5; Uninstalling numpy-1.21.5:; Successfully uninstalled numpy-1.21.5; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; numba 0.54.1 requires numpy<1.21,>=1.17, but you have numpy 1.21.5+mkl which is incompatible.; Successfully installed numpy-1.21.5+mkl; ```; Step7: check anaconda Python38 environment, numpy-1.21.5+mkl is successfully installed; ![image](https://user-images.githubusercontent.com/75048821/147306587-eb94c188-5c18-40f0-add0-3a899872d786.png). Step8: Scanpy import error. Numpy>v1.20 is conflicted with Scanpy; ```python; import numpy as np; import pandas as pd; import scanpy as sc; import scanpy.external as sce; import scipy; sc.settings.verbosity = 3; sc.logging.print_header(); sc.set_figure_params(dpi=100, dpi_save=600); ; import scvelo as scv; scv.settings.verbosity = 3; scv.settings.presenter_view = True; scv.logging.print_versions(). import cellrank as cr; cr.settings.verbosity = 3; cr.logging.print_versions(). import matplotlib.pyplot as pl; from matplotlib impor",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342:4721,depend,dependency,4721,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342,1,['depend'],['dependency']
Integrability,"7.4; beta_ufunc NA; binom_ufunc NA; bottleneck 1.3.4; certifi 2022.05.18.1; cffi 1.15.0; charset_normalizer 2.0.12; cloudpickle 2.1.0; colorama 0.4.5; cycler 0.10.0; cython_runtime NA; dask 2022.6.1; dateutil 2.8.2; debugpy 1.6.0; decorator 5.1.1; defusedxml 0.7.1; deprecated 1.2.13; diffxpy v0.7.4; entrypoints 0.4; executing 0.8.3; flatbuffers NA; fsspec 2022.5.0; future 0.18.2; gast NA; google NA; graphtools 1.5.2; h5py 3.7.0; hypergeom_ufunc NA; idna 3.3; ipykernel 6.15.0; ipython_genutils 0.2.0; ipywidgets 7.7.0; jedi 0.18.1; jinja2 3.1.2; joblib 1.1.0; keras 2.9.0; kiwisolver 1.4.3; llvmlite 0.38.1; magic 3.0.0; markupsafe 2.1.1; matplotlib 3.4.3; matplotlib_inline NA; mkl 2.4.0; mpl_toolkits NA; natsort 8.1.0; nbinom_ufunc NA; numba 0.55.2; numexpr 2.8.1; numpy 1.22.3; opt_einsum v3.3.0; packaging 21.3; pandas 1.4.2; parso 0.8.3; patsy 0.5.2; pcurve NA; pexpect 4.8.0; phate 1.0.7; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.29; psutil 5.9.1; ptyprocess 0.7.0; pure_eval 0.2.2; pydev_ipython NA; pydevconsole NA; pydevd 2.8.0; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.12.0; pygsp 0.5.1; pyparsing 3.0.9; pytz 2022.1; requests 2.28.0; s_gd2 1.8; scipy 1.8.1; scprep 1.2.0; seaborn 0.11.2; session_info 1.0.0; setuptools 62.6.0; six 1.16.0; sklearn 1.1.1; slingshot NA; sparse 0.13.0; stack_data 0.3.0; statsmodels 0.13.2; swig_runtime_data4 NA; tasklogger 1.1.2; tensorboard 2.9.1; tensorflow 2.9.1; termcolor 1.1.0; threadpoolctl 3.1.0; tlz 0.11.2; toolz 0.11.2; torch 1.11.0; tornado 6.1; tqdm 4.64.0; traitlets 5.3.0; typing_extensions NA; unicodedata2 NA; urllib3 1.26.9; wcwidth 0.2.5; wrapt 1.14.1; yaml 6.0; zipp NA; zmq 23.1.0; -----; IPython 8.4.0; jupyter_client 7.3.4; jupyter_core 4.10.0; notebook 6.4.12; -----; Python 3.9.12 | packaged by conda-forge | (main, Mar 24 2022, 23:25:59) [GCC 10.3.0]; Linux-5.10.16.3-microsoft-standard-WSL2-x86_64-with-glibc2.31; -----; Session information updated at 2022-06-28 16:19; </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2285:12324,wrap,wrapt,12324,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2285,1,['wrap'],['wrapt']
Integrability,"72 | 2.049016 | 0.0 | 0.0; NaN | 66.620399 | 1.637823 | 0.0 | 0.0; NaN | 66.236443 | 1.807056 | 0.0 | 0.0; NaN | 64.112152 | 2.446921 | 0.0 | 0.0; NaN | 64.083160 | 2.495992 | 0.0 | 0.0; HBA1 | 63.376114 | 4.161097 | 0.0 | 0.0; NaN | 63.009491 | 2.059168 | 0.0 | 0.0; NaN | 58.142750 | 2.201216 | 0.0 | 0. ```. #### Versions. <details>. -----; anndata 0.7.5; scanpy 1.7.0; sinfo 0.3.1; -----; PIL 7.2.0; anndata 0.7.5; appdirs 1.4.4; autoreload NA; backcall 0.2.0; bioservices 1.7.8; bs4 4.9.1; cairo 1.19.1; certifi 2020.12.05; cffi 1.14.4; chardet 3.0.4; colorama 0.4.3; colorlog NA; cycler 0.10.0; cython_runtime NA; dateutil 2.8.1; decorator 4.4.2; deprecated 1.2.10; easydev 0.9.38; future 0.18.2; future_fstrings NA; get_version 2.1; graphtools 1.5.2; gseapy 0.10.1; h5py 2.10.0; idna 2.10; igraph 0.8.2; ipykernel 5.3.4; ipython_genutils 0.2.0; ipywidgets 7.5.1; jedi 0.17.2; joblib 0.16.0; kiwisolver 1.2.0; legacy_api_wrap 1.2; leidenalg 0.8.1; llvmlite 0.34.0; louvain 0.7.0; lxml 4.5.2; magic 2.0.3; matplotlib 3.3.1; mkl 2.3.0; mpl_toolkits NA; natsort 7.1.1; numba 0.51.2; numexpr 2.7.1; numpy 1.19.1; packaging 20.8; pandas 1.2.1; parso 0.7.1; pexpect 4.8.0; phenograph 1.5.7; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.6; psutil 5.7.2; ptyprocess 0.6.0; pycparser 2.20; pygments 2.6.1; pygsp 0.5.1; pylab NA; pyparsing 2.4.7; pytz 2020.1; requests 2.24.0; requests_cache 0.5.2; sca NA; scanpy 1.7.0; scipy 1.6.1; scprep 1.0.5.post2; seaborn 0.10.1; setuptools_scm NA; sinfo 0.3.1; six 1.15.0; sklearn 0.23.2; soupsieve 2.0.1; statsmodels 0.11.1; storemagic NA; tables 3.6.1; tasklogger 1.0.0; texttable 1.6.2; threadpoolctl 2.1.0; tornado 6.0.4; traitlets 4.3.3; urllib3 1.25.10; wcwidth 0.2.5; wrapt 1.12.1; zmq 19.0.2; zope NA; -----; IPython 7.17.0; jupyter_client 6.1.6; jupyter_core 4.6.3; notebook 6.1.3; -----; Python 3.8.2 (default, May 7 2020, 20:00:49) [GCC 7.3.0]; Linux-3.10.0-957.12.2.el7.x86_64-x86_64-with-glibc2.10; 64 logical CPU cores, x86_64; </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1748:2766,wrap,wrapt,2766,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1748,1,['wrap'],['wrapt']
Integrability,"7251754bb/scanpy/neighbors/__init__.py#L105; https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/neighbors/__init__.py#L258. There is a chance that this can also be solved with an import from UMAP.; https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/tools/_umap.py#L107. As just discussed, @Koncopd, can you look into this and make a PR that gets rid of the umap legacy code?. Thank you so much!; Alex. PS: Just wrote an explanation for the reasons why I intorduced the duplicated code in the first place.; > The duplicated code in Scanpy came about as I wanted to very quickly move forward with a version 1.0 of Scanpy about a year ago. UMAP was just becoming available on GitHub and there wasn’t even a preprint, I think. It changed very quickly and there were dramatic bugs every now and then. Nonetheless it was clear that it’s a major improvement over existing solutions, both in terms of computational performance, quality of the result and ease of installation and use. I wanted to achieve two things: (i) I had to rewrite some parts of UMAP so that I could decompose it a neighbors computing and a dedicated embedding step; you know that in Scanpy, the neighborhood graph is used for many other things other than for the embedding (clustering and trajectory inference). I also added the Gaussian kernel solution that I had before switching to a “UMAP backend” for `pp.neighbors`; which was needed so that results for DPT could be reproduced. All of this would have been quite a discussion with Leland. Until we would have had settled on the “Scanpy needs” that certainly weren’t aligned with the development of an independent young package, PRs would have been integrated to much time would have been lost. Finally, I wanted absolute reproducibility for Scanpy users, which could only be achieved by “freezing the code”. So, I asked Leland whether he is OK if I add a frozen version of umap as an intermediate solution.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/522:2076,integrat,integrated,2076,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/522,1,['integrat'],['integrated']
Integrability,"8 return_data=True,; ---> 19 show=False); 20 data.to_csv(""C:/Users/Lin/write/paga_path_{}.csv"".format(descr)); 21 pl.savefig(""C:/Users/Lin/figures/paga_path_KTC.pdf""). ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax); 1037 if n_avg > 1:; 1038 old_len_x = len(x); -> 1039 x = moving_average(x); 1040 if ikey == 0:; 1041 for key in annotations:. ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in moving_average(a); 980 ; 981 def moving_average(a):; --> 982 return _sc_utils.moving_average(a, n_avg); 983 ; 984 ax = pl.gca() if ax is None else ax. ~\Miniconda3\envs\project\lib\site-packages\scanpy\_utils.py in moving_average(a, n); 374 An array view storing the moving average.; 375 """"""; --> 376 ret = np.cumsum(a, dtype=float); 377 ret[n:] = ret[n:] - ret[:-n]; 378 return ret[n - 1:] / n. <__array_function__ internals> in cumsum(*args, **kwargs). ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in cumsum(a, axis, dtype, out); 2421 ; 2422 """"""; -> 2423 return _wrapfunc(a, 'cumsum', axis=axis, dtype=dtype, out=out); 2424 ; 2425 . ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in _wrapfunc(obj, method, *args, **kwds); 56 bound = getattr(obj, method, None); 57 if bound is None:; ---> 58 return _wrapit(obj, method, *args, **kwds); 59 ; 60 try:. ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in _wrapit(obj, method, *args, **kwds); 45 except AttributeError:; 46 wrap = None; ---> 47 result = getattr(asarray(obj), method)(*args, **kwds); 48 if wrap:; 49 if not isinstance(result, mu.ndarray):. ValueError: setting an array element with a sequence.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1168#issuecomment-615878967:2464,wrap,wrap,2464,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168#issuecomment-615878967,2,['wrap'],['wrap']
Integrability,"92 reverse=True)):. File ~/anaconda3/envs/ml/lib/python3.9/site-packages/mpl_toolkits/mplot3d/axes3d.py:471, in Axes3D.draw.<locals>.do_3d_projection(artist); 458 """"""; 459 Call `do_3d_projection` on an *artist*, and warn if passing; 460 *renderer*.; (...); 464 *renderer* and raise a warning.; 465 """"""; 467 if artist.__module__ == 'mpl_toolkits.mplot3d.art3d':; 468 # Our 3D Artists have deprecated the renderer parameter, so; 469 # avoid passing it to them; call this directly once the; 470 # deprecation has expired.; --> 471 return artist.do_3d_projection(); 473 _api.warn_deprecated(; 474 ""3.4"",; 475 message=""The 'renderer' parameter of ""; 476 ""do_3d_projection() was deprecated in Matplotlib ""; 477 ""%(since)s and will be removed %(removal)s.""); 478 return artist.do_3d_projection(renderer). File ~/anaconda3/envs/ml/lib/python3.9/site-packages/matplotlib/_api/deprecation.py:431, in delete_parameter.<locals>.wrapper(*inner_args, **inner_kwargs); 421 deprecation_addendum = (; 422 f""If any parameter follows {name!r}, they should be passed as ""; 423 f""keyword, not positionally.""); 424 warn_deprecated(; 425 since,; 426 name=repr(name),; (...); 429 else deprecation_addendum,; 430 **kwargs); --> 431 return func(*inner_args, **inner_kwargs). File ~/anaconda3/envs/ml/lib/python3.9/site-packages/mpl_toolkits/mplot3d/art3d.py:599, in Path3DCollection.do_3d_projection(self, renderer); 597 @_api.delete_parameter('3.4', 'renderer'); 598 def do_3d_projection(self, renderer=None):; --> 599 xs, ys, zs = self._offsets3d; 600 vxs, vys, vzs, vis = proj3d.proj_transform_clip(xs, ys, zs,; 601 self.axes.M); 602 # Sort the points based on z coordinates; 603 # Performance optimization: Create a sorted index array and reorder; 604 # points and point properties according to the index array. AttributeError: 'Path3DCollection' object has no attribute '_offsets3d'. ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]; -----; an",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2285:9429,wrap,wrapper,9429,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2285,1,['wrap'],['wrapper']
Integrability,"99 dendro_info = self.adata.uns[key]; 900 if self.groupby != dendro_info[""groupby""]:. File ~/.local/share/hatch/env/virtual/scverse-tutorials/_YRPCeuX/basic-scrna/lib/python3.12/site-packages/scanpy/plotting/_anndata.py:2384, in _get_dendrogram_key(adata, dendrogram_key, groupby); 2377 from ..tools._dendrogram import dendrogram; 2379 logg.warning(; 2380 f""dendrogram data not found (using key={dendrogram_key}). ""; 2381 ""Running `sc.tl.dendrogram` with default parameters. For fine ""; 2382 ""tuning it is recommended to run `sc.tl.dendrogram` independently.""; 2383 ); -> 2384 dendrogram(adata, groupby, key_added=dendrogram_key); 2386 if ""dendrogram_info"" not in adata.uns[dendrogram_key]:; 2387 raise ValueError(; 2388 f""The given dendrogram key ({dendrogram_key!r}) does not contain ""; 2389 ""valid dendrogram information.""; 2390 ). File ~/.local/share/hatch/env/virtual/scverse-tutorials/_YRPCeuX/basic-scrna/lib/python3.12/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File ~/.local/share/hatch/env/virtual/scverse-tutorials/_YRPCeuX/basic-scrna/lib/python3.12/site-packages/scanpy/tools/_dendrogram.py:121, in dendrogram(adata, groupby, n_pcs, use_rep, var_names, use_raw, cor_method, linkage_method, optimal_ordering, key_added, inplace); 25 @old_positionals(; 26 ""n_pcs"",; 27 ""use_rep"",; (...); 49 inplace: bool = True,; 50 ) -> dict[str, Any] | None:; 51 """"""\; 52 Computes a hierarchical clustering for the given `groupby` categories.; 53 ; (...); 118 >>> sc.pl.dotplot(adata, markers, groupby='bulk_labels', dendrogram=True); 119 """"""; --> 121 raise_not_implemented_error_if_backed_type(adata.X, ""dendrogram""); 122 if isinstance(groupby, str):; 123 # if not a list, turn i",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3199:4322,wrap,wrapper,4322,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3199,1,['wrap'],['wrapper']
Integrability,":+1: to twine check :-1: to the python versions. Ultimately, we do have a limited amount of CI, so I think it's important to be a bit cautious adding many jobs. Of the dependencies I'm worried about being an issue: generally not newer python versions. Minimum python versions are important for catching us using newer features. . I am up for swapping python 3.7 with 3.8. I don't think 3.9 is going to work for now. Last time I tried to use 3.9 (a month ago) numpy builds weren't working. I believe numba currently isn't working: https://github.com/numba/numba/issues/6345. Higher priorities to me (roughly in order):. * Test on windows; * Test against lower bounds of our requirements; * Test on Mac; * Dev builds",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1602#issuecomment-763582191:168,depend,dependencies,168,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602#issuecomment-763582191,1,['depend'],['dependencies']
Integrability,::test_sparse_nanmean - AttributeError: 'csr_matrix' object has no attribute 'A'; FAILED scanpy/tests/test_score_genes.py::test_score_genes_sparse_vs_dense - AttributeError: 'csr_matrix' object has no attribute 'A'; FAILED scanpy/tests/test_score_genes.py::test_score_genes_deplete - AttributeError: 'csr_matrix' object has no attribute 'A'; FAILED scanpy/tests/test_score_genes.py::test_npnanmean_vs_sparsemean - AttributeError: 'csr_matrix' object has no attribute 'A'; ```. ### Minimal code sample. ```python; pip install scipy==1.14.0rc1; pytest; ```. ### Error output. _No response_. ### Versions. <details>. ```; + anndata==0.11.0.dev116+g1bff5fb (from git+https://github.com/scverse/anndata@1bff5fbf0894185c0759b61d78c6df66d6dfeeba); + annoy==1.17.3; + anyio==4.4.0; + array-api-compat==1.7.1; + pillow==10.3.0; + platformdirs==4.2.2; + pluggy==1.5.0; + pre-commit==3.7.1; + profimp==0.1.0; + psutil==5.9.8; + pyarrow==16.1.0; + pygments==2.18.0; + pygsp==0.5.1; + pynndescent==0.5.12; + pyparsing==3.1.2; + pytest==8.2.1; + pytest-cov==5.0.0; + pytest-memray==1.6.0; + pytest-mock==3.14.0; + pytest-nunit==1.0.7; + pytest-xdist==3.6.1; + python-dateutil==2.9.0.post0; + pytz==2024.1; + pyyaml==6.0.1; + rich==13.7.1; + scanorama==1.7.4; + scanpy==1.10.2.dev25+gf5a62eee (from file:///home/vsts/work/1/s); + scikit-image==0.23.2; + scikit-learn==1.5.0; + scikit-misc==0.3.1; + scipy==1.14.0rc1; + scprep==1.1.0; + seaborn==0.13.2; + session-info==1.0.0; + setuptools==70.0.0; + setuptools-scm==8.1.0; + six==1.16.0; + sniffio==1.3.1; + sortedcontainers==2.4.0; + sparse==0.16.0a7; + statsmodels==0.14.2; + stdlib-list==0.10.0; + tasklogger==1.2.0; + tblib==3.0.0; + texttable==1.7.0; + textual==0.63.6; + threadpoolctl==3.5.0; + tifffile==2024.5.22; + toolz==0.12.1; + tornado==6.4; + tqdm==4.66.4; + typing-extensions==4.12.0; + tzdata==2024.1; + uc-micro-py==1.0.3; + umap-learn==0.5.6; + urllib3==2.2.1; + virtualenv==20.26.2; + wrapt==1.16.0; + zarr==2.18.2; + zict==3.0.0; ```. </details>,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3083:3104,wrap,wrapt,3104,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3083,1,['wrap'],['wrapt']
Integrability,":; 434 if sheet is None:; --> 435 return read_h5ad(filename, backed=backed); 436 else:; 437 logg.msg('reading sheet', sheet, 'from file', filename, v=4). /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size); 442 else:; 443 # load everything into memory; --> 444 return AnnData(*_read_args_from_h5ad(filename=filename, chunk_size=chunk_size)); 445 ; 446 . /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size); 471 f = adata.file._file; 472 else:; --> 473 f = h5py.File(filename, 'r'); 474 for key in f.keys():; 475 if backed and key in AnnData._BACKED_ATTRS:. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/h5py/h5sparse.py in __init__(self, name, mode, driver, libver, userblock_size, swmr, force_dense, **kwds); 139 userblock_size=userblock_size,; 140 swmr=swmr,; --> 141 **kwds,; 142 ); 143 super().__init__(self.h5f, force_dense). /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/h5py/_hl/files.py in __init__(self, name, mode, driver, libver, userblock_size, swmr, **kwds); 267 with phil:; 268 fapl = make_fapl(driver, libver, **kwds); --> 269 fid = make_fid(name, mode, userblock_size, fapl, swmr=swmr); 270 ; 271 if swmr_support:. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/h5py/_hl/files.py in make_fid(name, mode, userblock_size, fapl, fcpl, swmr); 97 if swmr and swmr_support:; 98 flags |= h5f.ACC_SWMR_READ; ---> 99 fid = h5f.open(name, flags, fapl=fapl); 100 elif mode == 'r+':; 101 fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl). h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/h5f.pyx in h5py.h5f.open(). OSError: Unable to open file (truncated file: eof = 1241513984, sblock->base_addr = 0, stored_eof = 14011376022); ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/626:2839,wrap,wrapper,2839,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/626,2,['wrap'],['wrapper']
Integrability,:smile: Now this raises a proper error message: https://github.com/theislab/scanpy/commit/2490bec27c1c37e1388cb1da44369c81e176df6c,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/88#issuecomment-366287782:39,message,message,39,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/88#issuecomment-366287782,1,['message'],['message']
Integrability,; - freetype=2.12.1; - fribidi=1.0.10; - get-annotations=0.1.2; - gettext=0.21.1; - gffutils=0.13; - glpk=5.0; - gmp=6.3.0; - gmpy2=2.1.2; - gnutls=3.7.8; - graphite2=1.3.13; - h11=0.14.0; - h2=4.1.0; - h5py=3.9.0; - harfbuzz=7.3.0; - hdf5=1.14.1; - hpack=4.0.0; - httpcore=0.18.0; - hyperframe=6.0.1; - icu=73.2; - idna=3.4; - igraph=0.10.8; - importlib-metadata=6.8.0; - importlib_metadata=6.8.0; - importlib_resources=6.0.1; - ipykernel=6.25.1; - ipython=8.14.0; - isoduration=20.11.0; - jedi=0.19.0; - jinja2=3.1.2; - joblib=1.3.2; - jpeg=9e; - json5=0.9.14; - jsonpointer=2.0; - jsonschema=4.19.0; - jsonschema-specifications=2023.7.1; - jsonschema-with-format-nongpl=4.19.0; - jupyter-lsp=2.2.0; - jupyter_client=8.3.0; - jupyter_core=5.3.1; - jupyter_events=0.7.0; - jupyter_server=2.7.1; - jupyter_server_terminals=0.4.4; - jupyterlab=4.0.5; - jupyterlab_pygments=0.2.2; - jupyterlab_server=2.24.0; - kaleido-core=0.2.1; - kiwisolver=1.4.4; - krb5=1.21.2; - lame=3.100; - lcms2=2.15; - legacy-api-wrap=1.4; - leidenalg=0.10.2; - lerc=4.0.0; - libabseil=20240116.2; - libaec=1.0.6; - libass=0.17.1; - libblas=3.9.0; - libbrotlicommon=1.0.9; - libbrotlidec=1.0.9; - libbrotlienc=1.0.9; - libcblas=3.9.0; - libcurl=8.2.1; - libcxx=16.0.6; - libdeflate=1.17; - libedit=3.1.20191231; - libev=4.33; - libexpat=2.5.0; - libffi=3.4.2; - libgfortran=5.0.0; - libgfortran5=12.3.0; - libglib=2.80.0; - libhwloc=2.9.3; - libiconv=1.17; - libidn2=2.3.4; - libintl=0.22.5; - libjpeg-turbo=2.1.4; - liblapack=3.9.0; - libleidenalg=0.11.1; - libllvm14=14.0.6; - libnghttp2=1.52.0; - libopenblas=0.3.23; - libopus=1.3.1; - libpng=1.6.39; - libprotobuf=4.25.3; - libsodium=1.0.18; - libsqlite=3.42.0; - libssh2=1.11.0; - libtasn1=4.19.0; - libtiff=4.5.0; - libunistring=0.9.10; - libuv=1.48.0; - libvpx=1.13.0; - libwebp-base=1.3.1; - libxcb=1.13; - libxml2=2.11.6; - libzlib=1.2.13; - llvm-openmp=16.0.6; - llvmlite=0.40.1; - lz4-c=1.9.4; - markupsafe=2.1.3; - mathjax=2.7.7; - matplotlib=3.7.2; - matplotlib-,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3116:10978,wrap,wrap,10978,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3116,1,['wrap'],['wrap']
Integrability,"; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); Cell In[6], [line 8](vscode-notebook-cell:?execution_count=6&line=8); [6](vscode-notebook-cell:?execution_count=6&line=6) if filename.endswith('.h5ad'):; [7](vscode-notebook-cell:?execution_count=6&line=7) filepath = os.path.join(directory, filename); ----> [8](vscode-notebook-cell:?execution_count=6&line=8) adata = sc.read(filepath); [10](vscode-notebook-cell:?execution_count=6&line=10) # Rename columns with periods in `.obs` attribute; [11](vscode-notebook-cell:?execution_count=6&line=11) for col in adata.obs.columns:. File /data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); [77](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:77) @wraps(fn); [78](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:78) def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; [79](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:79) if len(args_all) <= n_positional:; ---> [80](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80) return fn(*args_all, **kw); [82](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args; [83](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:83) args_pos, args_rest = args_all[:n_positional], args_al",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/323#issuecomment-2041512845:1107,wrap,wraps,1107,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323#issuecomment-2041512845,1,['wrap'],['wraps']
Integrability,; cycler=0.10.0=pypi_0; decorator=4.4.2=py_0; defusedxml=0.6.0=py_0; entrypoints=0.3=py37hc8dfbb8_1001; fontconfig=2.13.1=h86ecdb6_1001; freetype=2.10.1=he06d7ca_0; get-version=2.1=pypi_0; gettext=0.19.8.1=hc5be6a0_1002; glib=2.64.2=h6f030ca_0; gmp=6.2.0=he1b5a44_2; h5py=2.10.0=pypi_0; icu=64.2=he1b5a44_1; idna=2.9=py_1; importlib-metadata=1.6.0=py37hc8dfbb8_0; importlib_metadata=1.6.0=0; ipykernel=5.2.1=py37h43977f1_0; ipython=7.13.0=py37hc8dfbb8_2; ipython_genutils=0.2.0=py_1; jedi=0.17.0=py37hc8dfbb8_0; jinja2=2.11.2=pyh9f0ad1d_0; joblib=0.14.1=pypi_0; json5=0.9.0=py_0; jsonschema=3.2.0=py37hc8dfbb8_1; jupyter_client=6.1.3=py_0; jupyter_contrib_core=0.3.3=py_2; jupyter_contrib_nbextensions=0.5.1=py37_0; jupyter_core=4.6.3=py37hc8dfbb8_1; jupyter_highlight_selected_word=0.2.0=py37_1000; jupyter_latex_envs=1.4.6=py37_1000; jupyter_nbextensions_configurator=0.4.1=py37_0; jupyterlab=2.1.1=py_0; jupyterlab_server=1.1.1=py_0; kiwisolver=1.2.0=pypi_0; ld_impl_linux-64=2.33.1=h53a641e_7; legacy-api-wrap=1.2=pypi_0; leidenalg=0.8.0=py37h43df1e8_0; libedit=3.1.20181209=hc058e9b_0; libffi=3.2.1=hd88cf55_4; libgcc-ng=9.1.0=hdf63c60_0; libgfortran-ng=7.3.0=hdf63c60_5; libiconv=1.15=h516909a_1006; libpng=1.6.37=hed695b0_1; libsodium=1.0.17=h516909a_0; libstdcxx-ng=9.1.0=hdf63c60_0; libuuid=2.32.1=h14c3975_1000; libxcb=1.13=h14c3975_1002; libxml2=2.9.10=hee79883_0; libxslt=1.1.33=h31b3aaa_0; llvmlite=0.32.0=pypi_0; lxml=4.5.0=py37he3881c9_1; markupsafe=1.1.1=py37h8f50634_1; matplotlib=3.2.1=pypi_0; mistune=0.8.4=py37h8f50634_1001; natsort=7.0.1=pypi_0; nbconvert=5.6.1=py37hc8dfbb8_1; nbformat=5.0.6=py_0; ncurses=6.2=he6710b0_0; networkx=2.4=pypi_0; notebook=6.0.3=py37_0; numba=0.49.0=pypi_0; numexpr=2.7.1=pypi_0; numpy=1.18.3=pypi_0; openssl=1.1.1g=h516909a_0; packaging=20.3=pypi_0; pandas=1.0.3=pypi_0; pandoc=2.9.2.1=0; pandocfilters=1.4.2=py_1; parso=0.7.0=pyh9f0ad1d_0; patsy=0.5.1=pypi_0; pcre=8.44=he1b5a44_0; pexpect=4.8.0=py37hc8dfbb8_1; pickleshare=0.7.5=py37hc8dfbb8_1001,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1183#issuecomment-620988575:1747,wrap,wrap,1747,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1183#issuecomment-620988575,2,['wrap'],['wrap']
Integrability,; packaging 24.0; pandas 1.5.3; pandocfilters 1.5.0; panel 1.3.8; param 2.0.2; parso 0.8.3; partd 1.4.1; patsy 0.5.6; pexpect 4.9.0; pickleshare 0.7.5; pillow 10.2.0; pip 24.0; pkgutil_resolve_name 1.3.10; platformdirs 4.2.0; pooch 1.8.1; prometheus_client 0.20.0; prompt-toolkit 3.0.42; protobuf 4.25.3; psutil 5.9.8; ptxcompiler 0.8.1; ptyprocess 0.7.0; pure-eval 0.2.2; pyarrow 14.0.2; pyarrow-hotfix 0.6; pycparser 2.21; pyct 0.5.0; pydantic 1.10.14; pyee 8.1.0; Pygments 2.17.2; pylibcugraph 24.2.0; pylibraft 24.2.0; pynndescent 0.5.11; pynvml 11.4.1; pyparsing 3.1.2; pyppeteer 1.0.2; pyproj 3.6.1; PySocks 1.7.1; python-dateutil 2.9.0; python-json-logger 2.0.7; pytometry 0.1.4; pytz 2024.1; pyviz_comms 3.0.1; PyWavelets 1.4.1; PyYAML 6.0.1; pyzmq 25.1.2; raft-dask 24.2.0; rapids_singlecell 0.9.6; readfcs 1.1.7; referencing 0.33.0; requests 2.31.0; rfc3339-validator 0.1.4; rfc3986-validator 0.1.1; rich 13.7.1; rmm 24.2.0; rpds-py 0.18.0; Rtree 1.2.0; scanpy 1.10.0rc2; scikit-image 0.22.0; scikit-learn 1.4.1.post1; scikit-misc 0.3.1; scipy 1.12.0; seaborn 0.13.2; Send2Trash 1.8.2; session-info 1.0.0; setuptools 69.1.1; shapely 2.0.3; simpervisor 1.0.0; single_cell_helper 0.0.1 ; six 1.16.0; sniffio 1.3.1; sortedcontainers 2.4.0; soupsieve 2.5; stack-data 0.6.2; statsmodels 0.14.1; stdlib-list 0.10.0; streamz 0.6.4; tblib 3.0.0; terminado 0.18.0; texttable 1.7.0; threadpoolctl 3.3.0; tifffile 2024.2.12; tinycss2 1.2.1; tomli 2.0.1; toolz 0.12.1; tornado 6.4; tqdm 4.66.2; traitlets 5.14.1; treelite 4.0.0; types-python-dateutil 2.8.19.20240311; typing_extensions 4.10.0; typing-utils 0.1.0; uc-micro-py 1.0.3; ucx-py 0.36.0; umap-learn 0.5.5; unicodedata2 15.1.0; uri-template 1.3.0; urllib3 1.26.18; wcwidth 0.2.13; webcolors 1.13; webencodings 0.5.1; websocket-client 1.7.0; websockets 10.4; wget 3.2; wheel 0.42.0; widgetsnbextension 4.0.10; wrapt 1.16.0; xarray 2024.2.0; xgboost 2.0.3; xyzservices 2023.10.1; yarl 1.9.4; zarr 2.17.1; zict 3.0.0; zipp 3.17.0; ```. </details>,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2964:6366,wrap,wrapt,6366,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2964,1,['wrap'],['wrapt']
Integrability,; rope 0.18.0 pyh9f0ad1d_0 conda-forge; rtree 0.9.4 py38h08f867b_1 conda-forge; scanpy 1.6.0 py_0 bioconda; scikit-learn 0.23.2 py38hc63f23e_1 conda-forge; scipy 1.5.2 py38hf17e0cf_2 conda-forge; seaborn 0.11.0 0 conda-forge; seaborn-base 0.11.0 py_0 conda-forge; setuptools 50.3.0 py38h0dc7051_1 ; setuptools-scm 4.1.2 pyh9f0ad1d_0 conda-forge; setuptools_scm 4.1.2 0 conda-forge; sinfo 0.3.1 py_0 conda-forge; six 1.15.0 pyh9f0ad1d_0 conda-forge; snowballstemmer 2.0.0 py_0 conda-forge; sortedcontainers 2.2.2 pyh9f0ad1d_0 conda-forge; sphinx 3.2.1 py_0 conda-forge; sphinxcontrib-applehelp 1.0.2 py_0 conda-forge; sphinxcontrib-devhelp 1.0.2 py_0 conda-forge; sphinxcontrib-htmlhelp 1.0.3 py_0 conda-forge; sphinxcontrib-jsmath 1.0.1 py_0 conda-forge; sphinxcontrib-qthelp 1.0.3 py_0 conda-forge; sphinxcontrib-serializinghtml 1.1.4 py_0 conda-forge; spyder 4.1.5 py38h32f6830_0 conda-forge; spyder-kernels 1.9.4 py38h32f6830_0 conda-forge; sqlite 3.33.0 h960bd1c_1 conda-forge; statsmodels 0.12.0 py38h174b24a_1 conda-forge; stdlib-list 0.7.0 py38h32f6830_1 conda-forge; tbb 2020.3 h879752b_0 ; testpath 0.4.4 py_0 conda-forge; texttable 1.6.3 pyh9f0ad1d_0 conda-forge; threadpoolctl 2.1.0 pyh5ca1d4c_0 conda-forge; tk 8.6.10 hb0a8c7a_1 conda-forge; toml 0.10.1 pyh9f0ad1d_0 conda-forge; tornado 6.0.4 py38h4d0b108_2 conda-forge; tqdm 4.51.0 pyh9f0ad1d_0 conda-forge; traitlets 5.0.5 py_0 conda-forge; ujson 4.0.1 py38h11c0d25_1 conda-forge; umap-learn 0.4.6 py38h32f6830_0 conda-forge; urllib3 1.25.11 py_0 conda-forge; watchdog 0.10.3 py38h4d0b108_2 conda-forge; wcwidth 0.2.5 pyh9f0ad1d_2 conda-forge; webencodings 0.5.1 py_1 conda-forge; wheel 0.35.1 pyh9f0ad1d_0 conda-forge; wrapt 1.11.2 py38h4d0b108_1 conda-forge; wurlitzer 2.0.1 py38_0 ; xz 5.2.5 haf1e3a3_1 conda-forge; yaml 0.2.5 haf1e3a3_0 conda-forge; yapf 0.30.0 pyh9f0ad1d_0 conda-forge; zeromq 4.3.3 hb1e8313_2 conda-forge; zipp 3.4.0 py_0 conda-forge; zlib 1.2.11 h7795811_1010 conda-forge; zstd 1.4.5 h0384e3a_2 conda-forge; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/953#issuecomment-719504684:8765,wrap,wrapt,8765,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953#issuecomment-719504684,2,['wrap'],['wrapt']
Integrability,"<!-- Please give a clear and concise description of what the bug is: -->. After I use function. ```py; adata = sc.read_visium(; './', count_file='V1_Human_Lymph_Node_filtered_feature_bc_matrix.h5',; genome=None, library_id=None, load_images=True,; ); ```. I use `sc.pp.calculate_qc_metrics(adata, qc_vars=[""mt""], inplace=True)` and got an error message:. ```pytb; RuntimeError Traceback (most recent call last); ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\errors.py in new_error_context(fmt_, *args, **kwargs); 744 try:; --> 745 yield; 746 except NumbaError as e:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\lowering.py in lower_block(self, block); 272 loc=self.loc, errcls_=defaulterrcls):; --> 273 self.lower_inst(inst); 274 self.post_block(block). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\lowering.py in lower_inst(self, inst); 485 if isinstance(inst, _class):; --> 486 func(self, inst); 487 return. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\parfors\parfor_lowering.py in _lower_parfor_parallel(lowerer, parfor); 239 lowerer, parfor, typemap, typingctx, targetctx, flags, {},; --> 240 bool(alias_map), index_var_typ, parfor.races); 241 finally:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\parfors\parfor_lowering.py in _create_gufunc_for_parfor_body(lowerer, parfor, typemap, typingctx, targetctx, flags, locals, has_aliases, index_var_typ, races); 1326 flags,; -> 1327 locals); 1328 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\compiler.py in compile_ir(typingctx, targetctx, func_ir, args, return_type, flags, locals, lifted, lifted_from, is_lifted_loop, library, pipeline_class); 666 return pipeline.compile_ir(func_ir=func_ir, lifted=lifted,; --> 667 lifted_from=lifted_from); 668 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\compiler.py in compile_ir(self, func_ir, lifted, lifted_from); 348 FixupArgs().run_pass(self.state); --> 349 return self._",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1341:345,message,message,345,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1341,1,['message'],['message']
Integrability,"<!-- Please give a clear and concise description of what the bug is: -->; Hi, I was trying to apply the SAM integration on a merged dataset of three example dataset from 10x homepage, in an attempt to compare the result to the PAGA and bbknn integrated umap.; I got a successful run on one of the PBMC dataset with no reprocessing. However in any other case I kept bump into an error:. TypeError: some keyword arguments unexpected. Here is the record. On the other hand, if I want to integrate bbknn with SAM, do I just apply bbknn after the run of SAM like this?. ////; import scanpy.external as sce. sam_obj = sce.tl.sam(adata); sc.pl.umap(sam_obj, color='Sample') . bbknn.bbknn(adata,batch_key='Sample'); #does this change the umap? or do I need to make another call of tl.umap?. sc.pl.umap(sam_obj, color='Sample') ; ////; i. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```python; ...; ```. import scanpy.external as sce; for adata in adatalist:; sam_obj = sce.tl.sam(adata); sce.pl.sam(adata,projection='X_umap'). <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```pytb; ... Self-assembling manifold; Running SAM; RUNNING SAM; Iteration: 0, Convergence: 0.016564277393631113; Iteration: 1, Convergence: 0.01278454723440345; Computing the UMAP embedding...; Elapsed time: 50.534051179885864 seconds; Self-assembling manifold; Running SAM; RUNNING SAM; Iteration: 0, Convergence: 0.022868878389371346. ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-17-4514ae92b370> in <module>; 1 import scanpy.external as sce; 2 for adata in adatalist:; ----> 3 sam_obj = sce.tl.sam(adata); 4 sce.pl.sam(adata,projection='X_umap'). ~/.local/lib/python3.7/site-packages/scanpy/external/tl/_sam.py in sam(adata, max_iter, num_norm_avg, k, distance, standardization, weight_pcs, sparse_pca, n_pcs, n_genes, projection, inplace, verbo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1157:108,integrat,integration,108,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1157,3,['integrat'],"['integrate', 'integrated', 'integration']"
Integrability,"<!-- Please give a clear and concise description of what the bug is: -->; I am trying to integrate data using ingest. I followed exactly the same steps and scripts as described in the scanpy tutorial. However, I got the error message that ‘UMAP’ object has no attribute ‘_input_distance_func’ every time when I ran the command of sc.tl.ingest(adata, adata_ref, obs=‘louvain’). I have the same problem even with using the example pbmc3k_processed and pbmc3k_processed as in the tutorial. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```; adata_ref = sc.datasets.pbmc3k_processed(); adata = sc.datasets.pbmc68k_reduced(); var_names = adata_ref.var_names.intersection(adata.var_names); adata_ref = adata_ref[:, var_names]; adata = adata[:, var_names]; sc.pp.pca(adata_ref); sc.pp.neighbors(adata_ref); sc.tl.umap(adata_ref). # problem occurs here; sc.tl.ingest(adata, adata_ref, obs='louvain'); ...; ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```; AttributeError Traceback (most recent call last); <ipython-input-12-27e22cc8f823> in <module>(); ----> 1 sc.tl.ingest(adata, adata_ref, obs='louvain'). 3 frames; /usr/local/lib/python3.6/dist-packages/umap/umap_.py in transform(self, X); 2006 try:; 2007 # sklearn pairwise_distances fails for callable metric on sparse data; -> 2008 _m = self.metric if self._sparse_data else self._input_distance_func; 2009 dmat = pairwise_distances(; 2010 X, self._raw_data, metric=_m, **self._metric_kwds. AttributeError: 'UMAP' object has no attribute '_input_distance_func'; ```. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; > scanpy==1.4.6 anndata==0.7.1 umap==0.4.1 numpy==1.18.2 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.22.2.post1 statsmodels==0.10.2 python-igraph==0.8.0 louvain==0.6.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1181:89,integrat,integrate,89,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1181,2,"['integrat', 'message']","['integrate', 'message']"
Integrability,"<!-- Please give a clear and concise description of what the bug is: -->; I'm using the ""scvelo"" https://scvelo.readthedocs.io/getting_started.html for scRNA data analysis. It underlying called ""scanpy"" function ""umap"" for calculating the coordinates. I tried the release version ""scanpy-1.4.4.post1-py_0"". It can not be imported to Python. Error message: ""ImportError: cannot import name '_Metric' from 'scanpy.neighbors' (/Users/shuzhe/anaconda3/lib/python3.7/site-packages/scanpy/neighbors/__init__.py)"". I finally switch to the developing version for ""scanpy"". When I run ""umap"", it gives me the error message below. I'm wondering what the "".obsp"" is and how it is generated.; ; <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```python; scv.tl.umap(adata); ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```pytb; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-22-391fc8667646> in <module>; ----> 1 scv.tl.umap(adata). ~/scanpy/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key); 125 start = logg.info('computing UMAP'); 126 ; --> 127 neighbors = NeighborsView(adata, neighbors_key); 128 ; 129 if ('params' not in neighbors. ~/scanpy/scanpy/_utils.py in __init__(self, adata, key); 667 self._dists_key = self._neighbors_dict['distances_key']; 668 ; --> 669 if self._conns_key in adata.obsp:; 670 self._connectivities = adata.obsp[self._conns_key]; 671 if self._dists_key in adata.obsp:. AttributeError: 'AnnData' object has no attribute 'obsp'; ```. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; > scanpy==1.4.7.dev26+gc255fa10 anndata==0.6.22.post1 umap==0.3.10 numpy==1.18.1 scipy==1.4.1 pandas==0.25.3 scikit-learn==0.21.2 statsmodels==0.11.0 python-igraph==0.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1125:347,message,message,347,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1125,2,['message'],['message']
Integrability,"<!-- Please give a clear and concise description of what the bug is: -->; I've had hard time in figuring this out. This is not a problem of scanpy directly but apparently is related to [scikit-learn 0.21 series](https://github.com/scikit-learn/scikit-learn/issues/14485) which is a dependency of latest scanpy version (1.4.6). Also related to [this comment in pytorch](https://github.com/pytorch/pytorch/issues/2575#issuecomment-523657178). My issue is that I'm using, in addition to scanpy, another library performing a dl_import with static TLS. ; So if I issue; ```python; import scanpy as sc; import graph_tool.all as gt; ```; I get. ```python; ImportError: dlopen: cannot load any more object with static TLS ; ```; error and I'm not able to use the second library. Reversing the order of the imports raises the same error and I'm not able to use `scanpy`. The issue is solved installing scikit-learn 0.20.4 (the last of 0.20 series). What are the exact scikit-learn 0.21.2 dependecies in scanpy?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1121:282,depend,dependency,282,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1121,2,['depend'],"['dependecies', 'dependency']"
Integrability,"<!-- Please give a clear and concise description of what the bug is: -->; `wx` appears to be a missing scanpy dependancy linked to matplotlib when installing on macOS. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```python; >>> import scanpy as sc; ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```pytb; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/miniconda3/envs/path/lib/python3.7/site-packages/scanpy/__init__.py"", line 38, in <module>; from . import plotting as pl; File ""/miniconda3/envs/path/lib/python3.7/site-packages/scanpy/plotting/__init__.py"", line 1, in <module>; from ._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot, dendrogram, correlation_matrix; File ""/miniconda3/envs/path/lib/python3.7/site-packages/scanpy/plotting/_anndata.py"", line 16, in <module>; from matplotlib import pyplot as pl; File ""/miniconda3/envs/path/lib/python3.7/site-packages/matplotlib/pyplot.py"", line 2282, in <module>; switch_backend(rcParams[""backend""]); File ""/miniconda3/envs/path/lib/python3.7/site-packages/matplotlib/pyplot.py"", line 221, in switch_backend; backend_mod = importlib.import_module(backend_name); File ""/miniconda3/envs/path/lib/python3.7/importlib/__init__.py"", line 127, in import_module; return _bootstrap._gcd_import(name[level:], package, level); File ""/miniconda3/envs/path/lib/python3.7/site-packages/matplotlib/backends/backend_wxagg.py"", line 1, in <module>; import wx; ModuleNotFoundError: No module named 'wx'; ```. The solution is simple, install `wxPython` https://pypi.org/project/wxPython/. However, it would be nice if scanpy could handle this OS-specific dependancy. #### Versions:; The latest scanpy version (1.5.1) installed via conda- of course I cannot print the versions since the scanpy import fails, other details;. ```; >>> import sys; print(sys.version); 3.7.6 | p",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1302:110,depend,dependancy,110,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1302,1,['depend'],['dependancy']
Integrability,"<!-- What kind of feature would you like to request? -->; - [ ] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [x] Other?. <!-- Please describe your wishes below: -->; ... I'm in the process of redesigning my most used workflows from https://github.com/jolespin/soothsayer to wrap around ScanPy. I come from microbial ecology but do a good amount of machine learning. In microbial ecology, the community is shifting towards a compositional data analysis (CoDA) approach which has fundamentals firmly rooted in mathematics. . Here is some literature about broad-scale applications across all NGS datasets: ; * [A field guide for the compositional analysis of any-omics data](https://academic.oup.com/gigascience/article/8/9/giz107/5572529); * [Understanding sequencing data as compositions: an outlook and review](https://academic.oup.com/bioinformatics/article/34/16/2870/4956011). it is even catching attention in scRNA-seq too: ; * [scCODA is a Bayesian model for compositional single-cell data analysis](https://www.nature.com/articles/s41467-021-27150-6). Anyways, off my soap box. As mentioned, I'm in the process of adapting my workflows to take advantage of ScanPy's power but I'm having a few difficulties. The first incorporating custom transformations. In future versions, would it be possible to create an API that is similar to [scanpy.pp.normalize_total)(https://scanpy.readthedocs.io/en/stable/generated/scanpy.pp.normalize_total.html) but allows for a custom metric? . For example: . ```python; import numpy as np; import pandas as pd; from typing import Union. def clr(x:Union[np.ndarray, pd.Series], multiplicative_replacement:Union[None,str,float,int]=""auto"") -> Union[np.ndarray, p",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2475:577,wrap,wrap,577,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2475,1,['wrap'],['wrap']
Integrability,"<!-- What kind of feature would you like to request? -->; - [ ] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [x] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python; import matplotlib.pyplot as plt; import scanpy as sc; import numpy as np. # given integrated object adata, clustered via the leiden algorithm and; # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID; count_series = adata.obs.groupby(['leiden', 'batch']).size(); new_df = count_series.to_frame(name = 'size').reset_index(); # convert from multi index to pivot; constitution = new_df.pivot(index='leiden', columns='batch')['size']; # convert to %batch (but could be modified to show different things instead; perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))); # keep track of the batch, cluster IDs so we can use them for plotting; clusters = adata.obs.leiden.cat.categories; batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots; # replace styling with scanpy defaults probably?; fig, ax = plt.subplots(); ax.grid(False); ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]); bottom = np.zeros(clusters.shape); for i, b in enumerate(batches):; ax.bar(clusters, perc",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1573:474,integrat,integrating,474,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573,2,['integrat'],"['integrated', 'integrating']"
Integrability,"<!-- What kind of feature would you like to request? -->; - [ ] Additional function parameters / changed functionality / changed defaults?; - [x] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->. PyMDE is a nice visualization method that to me seems to effectively serve the same purpose as UMAP in analyses (discussion about appropriateness of these methods can be in another issue :) ). It's super fast because after running pynndescent it puts the graph on the GPU optionally (using pytorch). I would love to see this in scanpy. There might be a way to use the scanpy neighbors graph from `sc.pp.neighbors` directly in pymde as the function below is a wrapper of some internal classes. <img width=""2431"" alt=""Screen Shot 2022-02-24 at 12 38 25 PM"" src=""https://user-images.githubusercontent.com/10859440/155603698-7f0e975e-2f4b-4a95-97eb-f119522c2510.png"">",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2154:928,wrap,wrapper,928,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2154,1,['wrap'],['wrapper']
Integrability,"<!-- What kind of feature would you like to request? -->; - [ ] Additional function parameters / changed functionality / changed defaults?; - [x] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; Adding multiplex community detection from Leiden: https://leidenalg.readthedocs.io/en/stable/multiplex.html#layer-multiplex. It seems very straightforward and would be the most simple way to integrate two modalities on the graph. We would make great use of it in Squidpy (rna counts+image), but I think it should live in Scanpy becasue it could be useful for other multi-modal data. This is a duplicate of #1107 and it has been extensively discussed in #1117 . In the latter however, lots of thought went into normalization/processing which is superfluous for this case as it is only specific for CITE-seq data. Here we'd just want to allow users to get partitions out of multiple graphs. This could be done in two ways:; - adding arguments to existing `tl.leiden`, so that it accepts multiple graphs and multiple resolutions params per graph.; - creating a separate function `sc.tl.leiden_multiplex`.; Any thoughts on this @ivirshup @Koncopd ?. I think @WeilerP also had some thoughts along these lines. Have you ever tried this out? is there any other analysis tool you explored with a simlar purpose? Would be interested to hear your thoughts!; worth mentioning that another approach, the WNN from seurat, was also mentioned here: https://github.com/theislab/scanpy/pull/1117#issuecomment-777020580; although am not sure how much work that requries.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1818:660,integrat,integrate,660,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818,1,['integrat'],['integrate']
Integrability,"<!-- What kind of feature would you like to request? -->; - [ ] Additional function parameters / changed functionality / changed defaults?; - [x] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; We are increasingly using ProjectR as a transfer learning technique from one dataset onto others. It would be great if this were part of the scanpy package, since most of the rest of what we do with single-cell uses scanpy. I'm going to start working on integration of the two but doubt I have the data science experience currently to submit a PR to this project for it. Are there any plans already in the works to pull in transfer-learning techniques such as this?. Relevant links:. - [Article in Bioinformatics](https://academic.oup.com/bioinformatics/advance-article-abstract/doi/10.1093/bioinformatics/btaa183/5804979?redirectedFrom=PDF); - [Article in Cell](https://www.cell.com/cell-systems/pdf/S2405-4712(19)30146-2.pdf); - [GitHub](https://github.com/genesofeve/projectR)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1205:723,integrat,integration,723,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1205,1,['integrat'],['integration']
Integrability,<!-- What kind of feature would you like to request? -->; - [X ] Additional function parameters / changed functionality / changed defaults?; <!-- Please describe your wishes below: -->; Could the function add a boolean parameter to make it work for non-log transformed data?. if [boolean depending on whether data is log transformed or not]:; foldchanges = (self.expm1_func(mean_group) + 1e-9) / (self.expm1_func(mean_rest) + 1e-9); else:; foldchanges = (mean_group+ 1e-9) / (mean_rest + 1e-9),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1454:288,depend,depending,288,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1454,1,['depend'],['depending']
Integrability,"<!-- What kind of feature would you like to request? -->; - [X] Additional function parameters / changed functionality / changed defaults?; - [X] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [X] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; t-SNE is an interative algorithm, and takes numerous iterations to converge, particularly on larger datasets. For example, if MulticoreTSNE is installed, it accepts `n_iter=30000` as opposed to the default `n_iter=1000`. It would be nice to have this parameter exposed. For larger datasets of say 200K cells, 1000 iterations isn't enough to fully converge to its final compact cluster shapes. . Alternatively, is it possible to pass in a kwargs to scanpy tools that wrap other algorithms, so that the advanced user can flexibly look up [additional MulticoreTSNE parameters](https://github.com/DmitryUlyanov/Multicore-TSNE/blob/62dedde52469f3a0aeb22fdd7bce2538f17f77ef/MulticoreTSNE/__init__.py#L55) to modify, without needing to exhaustively enumerate all parameters in the scanpy wrapper?. Finally, it would be even better to have the faster FFT-based tsne to generalize to millions of cells, the most recent re-implementation being https://github.com/pavlin-policar/openTSNE. In the mean time, one has to overwrite the `.X_tsne` attribute after running these other tools separately.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1150:935,wrap,wrap,935,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1150,2,['wrap'],"['wrap', 'wrapper']"
Integrability,"<!-- What kind of feature would you like to request? -->; - [x] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->. When we visualize gene expression, range of colorbar can be different. ![image](https://user-images.githubusercontent.com/30337666/89102749-a6d10b00-d43e-11ea-99f7-0867f3c31d13.png). Usually, it's fine. But if we want to compare gene expression in control-treat experiment in single-cell level, sometimes this happens. ![image](https://user-images.githubusercontent.com/30337666/89102998-08927480-d441-11ea-9068-3d7f09ab26a3.png)![image](https://user-images.githubusercontent.com/30337666/89103003-1942ea80-d441-11ea-8a6f-091a6cd2f19b.png). It's hard for us to estimate up or down regulation of this gene in different group of cells because colors with same value in the two figures are not consistent. Plotting the two figures with same colorbar range can solve it. So, I want to know that are there parameters solving it in scanpy.pl.umap or some matplotlib methods that can be integrated with Scanpy?. Thank you very much!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1352:1349,integrat,integrated,1349,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1352,1,['integrat'],['integrated']
Integrability,"<!-- What kind of feature would you like to request? -->; - [x] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; ... Hi!. We're considering implementing some of the t-SNE recommendations in https://www.nature.com/articles/s41467-019-13056-x for our single-cell analysis. They use a different t-SNE implementation (https://github.com/KlugerLab/FIt-SNE), and before I ran off doing my own wrapping and plumbing to integrate with Scanpy I thought I'd check: have you considered integrating this yourselves?. Thanks!. Jon",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/996:743,wrap,wrapping,743,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/996,3,"['integrat', 'wrap']","['integrate', 'integrating', 'wrapping']"
Integrability,"<!-- What kind of feature would you like to request? -->; - [x] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; ...; Hi scanpy develovepers,. A rotation student asked me what is `sc.pl.umap` showing if `sc.tl.umap` was not computed beforehand. To which I don't have the answer since I have never done it. If you know the answer I'd like to know it, but most importantly, I think it would be nice to have an error message in the UMAP plotting function if UMAP has not been computed. Unless there were meaning and a reason to use `sc.pl.umap` without running `sc.tl.umap` previously, and it was designed that way purposely. I assume this would apply to other plotting functions too. Thanks!; Alejandro",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1460:770,message,message,770,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1460,1,['message'],['message']
Integrability,"<!-- What kind of feature would you like to request? -->; - [x] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; As more and more technologies allow multimodal characterization of single cells it could be useful to exploit some functionalities of scanpy's toolkit to perform, at least, some rough integrative analysis. Assuming we have to modalities on different layers (say RNA and ATAC), one could create two knn graphs for both layers and use `leidenalg.find_partition_multiplex` to perform a joint call of partitions handling the two (or more) graphs as a multiplex. I have tested myself this approach, described in [leidenalg documentation](https://leidenalg.readthedocs.io/en/latest/multiplex.html), it works and it is highly configurable. ; We can take care of the implementation of enhancement (as `leiden_multiplex()` function?), I just want to be sure that it is not already on the development roadmap and that it is ok to have it into scanpy and not as an external tool.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1107:653,integrat,integrative,653,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1107,1,['integrat'],['integrative']
Integrability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. - Depends on #2814. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #2777, closes #2807; - [x] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:. TODO:. - [x] batched. <table>; <thead>; <tr>; <th scope=row>. `flavor=`. <th>. `""seurat""`. <th>. `""cell_ranger""`. <tbody>; <tr>; <th scope=row>. `n_top_genes=n`. <td>. - [x] &zwnj;. <td>. - [x] (https://github.com/dask/dask/issues/10853). <tr>; <th scope=row>. `{min,max}_{disp,mean}`. <td>. - [x] &zwnj;. <td>. - [x] &zwnj;. </table>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2809:236,Depend,Depends,236,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2809,1,['Depend'],['Depends']
Integrability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Only check the following box if you did not include release notes -->. ## TODO:. - [x] Fix tests; - [x] Figure out PCA test case with anndata 0.8.0; - [x] Add CI job; - [x] Rename CI job to be less similar to minimal dependencies, this will probably be `MinVer`; - [x] Bump anndata requirement back down to 0.7.3 (breaks dask tests); - Maybe 0.8 is low enough?; - [x] Bump pandas requirement back down to 1.5 (breaks grouped plots ordering). ## Some thoughts. * Sibling PR to: https://github.com/scverse/anndata/pull/1314; * Not completley sure what to do about plotting tests yet. Possible we just ignore any comparison failures, but ideally we could still know if these are broken.; * Metric consistency test failure is from https://github.com/scverse/scanpy/issues/2688; * Test updates in https://github.com/scverse/scanpy/pull/2705 (plus bumping one test a little lower) fixes it. <!-- Please check (“- [x]”) and fill in the following boxes -->",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816:456,depend,dependencies,456,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816,1,['depend'],['dependencies']
Integrability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Adresses #2088 and adresses #1733; <!-- Only check the following box if you did not include release notes -->; - [x] Tests included or not required because: They are required and some suggested; - [x] Release notes; - [x] Doc update - depending on feedback here; - [x] Doc update - guidance scanpy vs seurat. **Context**; As discussed in issues #2088 and #1733, `sc.pp.highly_variable_genes(adata, flavor=""seurat_v3"", batch_key=SOME_KEY)` potentially differs in the implementation of how HVGs are ranked from its Seurat counterpart:; - either by sorting by number-of-batches-in-which-genes-are-highly-variable and then breaking ties with median-rank-in-batches (this is described in [Stuart et al. 2019](https://www.cell.com/cell/pdf/S0092-8674(19)30559-8.pdf), and implemented in Seurat's [`SelectIntegrationFeatures`](https://satijalab.org/seurat/reference/selectintegrationfeatures)*).; - OR by sorting first by median-rank-in-batches and breaking ties with number-of-batches-in-which-genes-are-highly-variable (this is how `""seurat_v3""` in scanpy is currently implemented); ; causing quite some discrepancy in the results. *I am not an R expert, so this might not be correct: Digging into the code of `SelectIntegrationFeatures`, I suspect the genes _above_ a treshold level of batches in which they are HVGs are [ordered by their median rank](https://github.com/satijalab/seurat/blob/41d19a8a55350bff444340d6ae7d7e03417d4173/R/integration.R#L2988), in contrary to the textual description in Stuart et al.; and only the genes displaying this threshold of number of batches in which they are highly variable are ranked by their median rank - to decide which are kept as highly variable. This w",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2792:540,depend,depending,540,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2792,1,['depend'],['depending']
Integrability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #3226; - [x] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:. An alternative would be to subclass `PCA`, but that would involve erroring out or reimplementing all of its options. Ideally #3267 would be merged first and this one integrated into its improved decision tree.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3263:652,integrat,integrated,652,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3263,1,['integrat'],['integrated']
Integrability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes N/A; - [x] Tests included or not required because: dev process; <!-- Only check the following box if you did not include release notes -->; - [x] Release notes not necessary because: dev process. Filt resulted in the release workflow failing, as it tries to install the package’s runtime dependencies. Backporting the switch to hatch fixes that, now building only needs build deps",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2727:600,depend,dependencies,600,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2727,1,['depend'],['dependencies']
Integrability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes N/A; - [x] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [x] Release notes not necessary because: docs change. Also bump scanpydoc so the `[source]` links for wrapped functions work. E.g. `sc.pp.filter_cells` now links to the correct code lines):. ```diff; -<a href=""https://github.com/scverse/scanpy/tree/419c1a45aef26b5531a5b9cf1ec430e5ae67ce97/python3.11/site-packages/legacy_api_wrap/__init__.py#L49-L193"">[source]</a>; +<a href=""https://github.com/scverse/scanpy/tree/2d5bda1e45525354b9b751aa572c0b08175450cf/scanpy/preprocessing/_simple.py#L49-L193"">[source]</a>; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2800:544,wrap,wrapped,544,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2800,1,['wrap'],['wrapped']
Integrability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes N/A; - [x] Tests included; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:. Since today, there are some test breakages:. - `adata[:, [True, True]]` now behaves like `adata[:, np.array([1, 1])]` instead of `adata[:, np.array([True, True])]` (exposed through `read_10x_mtx`); - `sc.pl.violin` now creates slightly wider plots through some dependency change. This fixes everything that broke.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2801:720,depend,dependency,720,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2801,1,['depend'],['dependency']
Integrability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes https://github.com/scverse/scanpy/issues/2763; - [x] Tests included or not required because: manually checked using rtd PR build; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:. Adds [readthedocs-sphinx-search](https://readthedocs-sphinx-search.readthedocs.io/en/latest/) support via the scanpydoc theme, which contains JS and CSS customizations to make the search extension integrate with the theme. See. - https://github.com/theislab/scanpydoc/pull/121; - https://github.com/theislab/scanpydoc/pull/125. ### [rendered](https://icb-scanpy--2805.com.readthedocs.build/en/2805/). An alternative that looks nicer would be https://github.com/readthedocs/addons, but it’s still in alpha. PS: I didn’t add the same hack as in scanpydoc that makes the search work in PR builds, so you’ll only see “No results found” in the above. Check out https://icb-scanpydoc.readthedocs-hosted.com/en/latest/?rtd_search=scanpydoc to see rendered search results. You can see that the API works for scanpy:. ```console; $ http get https://icb-scanpy.readthedocs-hosted.com/_/api/v3/search/?q=project%3Aicb-scanpy%2Flatest+filter_cells; ╭──────────┬────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮; │ count │ 2 │; │ next │ │; │ previous │ │; │ │ ╭───┬────────────┬────────────────╮ │; │ projects │ │ # │ slug │ versions │ │; │ │ ├───┼────────────┼────────────────┤ │; │ │ │ 0 │ icb-scanpy │ ╭───┬────────╮ │ │;",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2805:759,integrat,integrate,759,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2805,1,['integrat'],['integrate']
Integrability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->; This PR fixes the case when `use_raw=None` in `scanpy.tl.score_genes`. It causes to first fetch `var_names` from `adata.var_names`, but later a subset on `adata.raw` can happen, which can have different gene names.; Also fixes the type of `use_raw` and adds a `ValueError` if `gene_pool` is empty (otherwise, crashes with non-informative error message). related issue: https://github.com/theislab/cellrank/issues/746",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1999:578,message,message,578,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1999,1,['message'],['message']
Integrability,<details>; <summary>Installed scanpy on jupyter notebook/ anaconda: </summary>. ```; pip install scanpy. Requirement already satisfied: scanpy in c:\users\charles\anaconda3\lib\site-packages (1.7.2); Requirement already satisfied: numba>=0.41.0 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (0.44.1); Requirement already satisfied: tables in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (3.7.0); Requirement already satisfied: anndata>=0.7.4 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (0.7.6); Requirement already satisfied: legacy-api-wrap in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (1.2); Requirement already satisfied: packaging in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (21.3); Requirement already satisfied: pandas>=0.21 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (1.3.4); Requirement already satisfied: scipy>=1.4 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (1.7.3); Requirement already satisfied: umap-learn>=0.3.10 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (0.5.1); Requirement already satisfied: h5py>=2.10.0 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (2.10.0); Requirement already satisfied: scikit-learn>=0.21.2 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (1.0.2); Requirement already satisfied: statsmodels>=0.10.0rc2 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (0.13.0); Requirement already satisfied: matplotlib>=3.1.2 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (3.5.1); Requirement already satisfied: numpy>=1.17.0 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (1.21.5); Requirement already satisfied: seaborn in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (0.11.2); Requirement already satisfied: tqdm in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (4.62.3); Requirement already satisfied: natsort in c:\users\charles\an,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2173#issuecomment-1063704626:585,wrap,wrap,585,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2173#issuecomment-1063704626,2,['wrap'],['wrap']
Integrability,<details>; <summary>pip list</summary>. ```; anndata 0.7.8; asttokens 2.0.5; bcrypt 3.2.0; Bottleneck 1.3.2; brotlipy 0.7.0; cached-property 1.5.2; certifi 2021.10.8; cffi 1.15.0; charset-normalizer 2.0.12; chart-studio 1.1.0; click 8.0.4; cmake 3.22.2; colorama 0.4.4; conda 4.11.0; conda-package-handling 1.7.3; cryptography 36.0.1; cycler 0.11.0; Cython 0.29.20; devtools 0.8.0; dunamai 1.9.0; executing 0.8.2; fa2 0.3.5; Fabric 1.6.1; fonttools 4.29.1; get_version 3.5.4; h5py 3.6.0; idna 3.3; igraph 0.9.9; install 1.3.5; joblib 1.1.0; kiwisolver 1.3.2; legacy-api-wrap 1.2; llvmlite 0.38.0; loom 0.0.18; loompy 3.0.6; mamba 0.15.3; matplotlib 3.5.1; mkl-fft 1.3.1; mkl-random 1.2.2; mkl-service 2.4.0; MulticoreTSNE 0.1; natsort 8.1.0; networkx 2.6.3; numba 0.55.1; numexpr 2.8.1; numpy 1.21.2; numpy-groupies 0.9.14; opt-einsum 3.3.0; packaging 21.3; pandas 1.4.1; paramiko 2.9.2; patsy 0.5.2; Pillow 9.0.1; pip 21.2.4; plotly 5.6.0; pycosat 0.6.3; pycparser 2.21; PyNaCl 1.5.0; pynndescent 0.5.6; pyOpenSSL 22.0.0; pyparsing 3.0.7; PyQt5 5.12.3; PyQt5_sip 4.19.18; PyQtChart 5.12; PyQtWebEngine 5.12.1; pyro-api 0.1.2; pyro-ppl 1.8.0; pysam 0.18.0; PySocks 1.7.1; python-dateutil 2.8.2; pytz 2021.3; requests 2.27.1; retrying 1.3.3; ruamel-yaml-conda 0.15.80; scanpy 1.7.0rc1; scikit-learn 1.0.2; scipy 1.7.3; seaborn 0.11.2; setuptools 58.0.4; sinfo 0.3.4; six 1.16.0; statsmodels 0.13.2; stdlib-list 0.8.0; tables 3.7.0; tenacity 8.0.1; texttable 1.6.4; threadpoolctl 3.1.0; torch 1.10.2; tornado 6.1; tqdm 4.62.3; umap-learn 0.4.6; unicodedata2 14.0.0; urllib3 1.26.8; velocyto 0.17.17; wheel 0.37.1; xlrd 1.2.0; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2169#issuecomment-1062402318:570,wrap,wrap,570,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2169#issuecomment-1062402318,2,['wrap'],['wrap']
Integrability,"<summary> from make html </summary>. ```sh; reading sources... [ 5%] generated/classes/scanpy.pl.DotPlot ; Exception occurred:; File ""/usr/local/lib/python3.8/site-packages/sphinx/util/docfields.py"", line 369, in transform; new_list += fieldtype.make_field(fieldtypes, self.directive.domain, items,; TypeError: make_field() got an unexpected keyword argument 'inliner'; The full traceback has been saved in /var/folders/bd/43q20k0n6z15tdfzxvd22r7c0000gn/T/sphinx-err-qbzn5se8.log, if you want to report the issue to the developers.; Please also report this if it was a user error, so that a better error message can be provided next time.; A bug report can be filed in the tracker at <https://github.com/sphinx-doc/sphinx/issues>. Thanks!; make: *** [html] Error 2; ```. </details>. <details>; <summary> contents of the referenced log file </summary>. ```python; # Sphinx version: 4.1.0; # Python version: 3.8.10 (CPython); # Docutils version: 0.16 release; # Jinja2 version: 2.11.2; # Last messages:; # reading sources... [ 2%] dev/documentation; # reading sources... [ 2%] dev/external-tools; # reading sources... [ 3%] dev/getting-set-up; # reading sources... [ 3%] dev/index; # reading sources... [ 3%] dev/release; # reading sources... [ 4%] dev/testing; # reading sources... [ 4%] dev/versioning; # reading sources... [ 4%] ecosystem; # reading sources... [ 5%] external; # reading sources... [ 5%] generated/classes/scanpy.pl.DotPlot; # Loaded extensions:; # sphinx.ext.mathjax (4.1.0) from /usr/local/lib/python3.8/site-packages/sphinx/ext/mathjax.py; # sphinxcontrib.applehelp (1.0.2) from /usr/local/lib/python3.8/site-packages/sphinxcontrib/applehelp/__init__.py; # sphinxcontrib.devhelp (1.0.2) from /usr/local/lib/python3.8/site-packages/sphinxcontrib/devhelp/__init__.py; # sphinxcontrib.htmlhelp (2.0.0) from /usr/local/lib/python3.8/site-packages/sphinxcontrib/htmlhelp/__init__.py; # sphinxcontrib.serializinghtml (1.1.5) from /usr/local/lib/python3.8/site-packages/sphinxcontrib/seri",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1946#issuecomment-877995557:1055,message,messages,1055,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1946#issuecomment-877995557,1,['message'],['messages']
Integrability,"> 2. Have the info in notebook. I think I'd be happy to recommend calling `session_info` directly for this. IIRC, we have these functions at all because a package which did a good job of displaying the imported packages and dependencies didn't really exist. . > and leave print_versions unchanged?. With the update to use `session_info`?. I'd even be fine to deprecate the `file` argument, since it's not super useful. Plus `session_info` provides this, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2089#issuecomment-998063447:224,depend,dependencies,224,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2089#issuecomment-998063447,1,['depend'],['dependencies']
Integrability,"> > > > > Should this argument be added to `sc.read_visium` as well? That is, add a path to the tiff if it would be in a standardized place?; > > > > ; > > > > ; > > > > yes that's a very good point actually. It could even make it easier for us to initialize the image container maybe?; > > > ; > > > ; > > > I thought about that already - standardised places for the tiff would be `sample_id""_image.tif` and `image.tif`. I don't think we need to add an extra argument to `sc.read_visium`, couldn't it automatically add the path if it exists?; > > ; > > ; > > good point, agree probably best solution, however one thing it's not given is the format of the file (could be tif or jpg, but potentially some else, depending on the lab). But an automatic search would be best, just not sure how extensive we can make it; > ; > do you have an example where the image is a jpg? I can account for both tif and jpg, but I'm wondering if its worth it. collaborators that generated visium data sent me jpg",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1506#issuecomment-734307032:710,depend,depending,710,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1506#issuecomment-734307032,1,['depend'],['depending']
Integrability,"> > > > Should this argument be added to `sc.read_visium` as well? That is, add a path to the tiff if it would be in a standardized place?; > > > ; > > > ; > > > yes that's a very good point actually. It could even make it easier for us to initialize the image container maybe?; > > ; > > ; > > I thought about that already - standardised places for the tiff would be `sample_id""_image.tif` and `image.tif`. I don't think we need to add an extra argument to `sc.read_visium`, couldn't it automatically add the path if it exists?; > ; > good point, agree probably best solution, however one thing it's not given is the format of the file (could be tif or jpg, but potentially some else, depending on the lab). But an automatic search would be best, just not sure how extensive we can make it. do you have an example where the image is a jpg? I can account for both tif and jpg, but I'm wondering if its worth it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1506#issuecomment-734306503:686,depend,depending,686,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1506#issuecomment-734306503,1,['depend'],['depending']
Integrability,"> > > Should this argument be added to `sc.read_visium` as well? That is, add a path to the tiff if it would be in a standardized place?; > > ; > > ; > > yes that's a very good point actually. It could even make it easier for us to initialize the image container maybe?; > ; > I thought about that already - standardised places for the tiff would be `sample_id""_image.tif` and `image.tif`. I don't think we need to add an extra argument to `sc.read_visium`, couldn't it automatically add the path if it exists?. good point, agree probably best solution, however one thing it's not given is the format of the file (could be tif or jpg, but potentially some else, depending on the lab). But an automatic search would be best, just not sure how extensive we can make it",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1506#issuecomment-733750814:662,depend,depending,662,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1506#issuecomment-733750814,1,['depend'],['depending']
Integrability,"> > @VladimirShitov can you give an example of how you use the gen_mpl_labels function? I tried it and got somewhat different results. For example it lacked the lines pointing to the cluster centers. Thanks!; > ; > Have you solved this problem? I still can't show the lines pointing to the cluster centers. Hi @nnnanchen. I apologize, I forgot to add the `adjust_text` call in the very last line of the `gen_mpl_labels` above. I edited the previous message. Can you try again?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1513#issuecomment-2048182372:449,message,message,449,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1513#issuecomment-2048182372,1,['message'],['message']
Integrability,"> > If we need multiple tools in the same container the place to add it would be [BioContainers/multi-package-containers](https://github.com/BioContainers/multi-package-containers); >; > We do make heavy use of optional dependencies, so this might be the way to go regardless. Just saw there's already a pr for this! . * BioContainers/multi-package-containers#2209",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2281#issuecomment-1160565536:220,depend,dependencies,220,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2281#issuecomment-1160565536,1,['depend'],['dependencies']
Integrability,"> > So I wrote a wrapper around scipy.sparse to implement NumPy's **array_function** protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular scipy.sparse.; > ; > Thoughts on adding this to scipy.sparse itself so that we can avoid the wrapper? cc @rgommers. See discussion in https://github.com/scipy/scipy/issues/10362 for this. The general sentiment is: probably not the best idea, because of the matrix/ndarray semantics not being compatible. More input on that SciPy issue is very welcome.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/921#issuecomment-557317682:17,wrap,wrapper,17,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-557317682,3,"['protocol', 'wrap']","['protocol', 'wrapper']"
Integrability,"> > ```python; > > scipy.io.mmwrite; > > ```; > ; > This code doesn't actually work - rows and columns are switched in the matrix, and it produces an error when you try to read in the output using either `Scanpy` or `Seurat` wrapper functions. Perhaps it's a package version thing though.. I was having the same issue as well. I ended up doing what was suggested above:. `adata.T.to_df().to_csv('matrix.csv')`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/262#issuecomment-1520696083:225,wrap,wrapper,225,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262#issuecomment-1520696083,1,['wrap'],['wrapper']
Integrability,"> @Brycealong could you also try this in a new isolated environment, please? There might be some dependency that's interfering. Would be glad to know which one, but it's tricky... Ofc. I can run the code on google colab and i'm stick to that. I think there's something interferring the process in my own computer...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2359#issuecomment-1298073650:97,depend,dependency,97,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2359#issuecomment-1298073650,1,['depend'],['dependency']
Integrability,"> @Zethson I believe that's an upstream issue. Looks like the docs broke when `sphinx-autodoc-typehints` bumped versions from `1.12.0` to `1.13.0`.; > ; > I can build the docs locally from `master` and from this branch with `sphinx-autodoc-typehints` v1.12, but not v1.13. (You'll also see an identical error in #2099, despite that just being a dependency bump for pre-commit.); > ; > I'll submit a PR to pin `sphinx-autodoc-typehints` to version 1.12.0 shortly. Thank you for taking the time to dig into this! Much appreciated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1828#issuecomment-1005073549:345,depend,dependency,345,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1828#issuecomment-1005073549,1,['depend'],['dependency']
Integrability,"> @Zethson thank you for the consideration and explanation. I am not sure Mellon would pass the criteria since it does not depend on or explicitly use AnnData although we do recommend using AnnData: https://mellon.readthedocs.io/en/latest/notebooks/basic_tutorial.html Additionally, it relies on [Palantir](https://github.com/dpeerlab/Palantir) which does also not qualify since it does not have a CI yet. Do you think we should try making a PR to https://github.com/scverse/ecosystem-packages regardless?. No, I'm afraid that these are hard criteria. However, I can highly recommend that you support AnnData first class :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2577#issuecomment-1656332039:123,depend,depend,123,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2577#issuecomment-1656332039,1,['depend'],['depend']
Integrability,"> A downside of this is it takes a really long time to compile on first run, which might be off-putting. Right, numba only compiles stuff when first run (because otherwise it can’t know the types) so this doesn’t slow down importing scanpy. > So... probably worth it?. We could wrap it in a function that checks the number of cells and only compiles this to faster code when necessary. If we do this in a generic way we could even defer importing numpy, saving on import duration (although there probably isn’t much functionality without running numpy-driven functions)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/844#issuecomment-534067279:278,wrap,wrap,278,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/844#issuecomment-534067279,1,['wrap'],['wrap']
Integrability,"> A lot of the motivation for wanting it in scanpy is to make it easier to try out 😸. We've definitely ended up with a number of features this way, but more so when we had fewer users... I think this would be a good candidate for an `experimental` module. > The umap developers adding it to the main package seems like a good argument that it's useful though. I should look through the paper on this more carefully, but from my initial skimming I wasn't particularly convinced the relative density was meaningful. > I hadn't thought about inputs but first guess is that PCA (or equivalent) should capture density relatively well?. I'm suspicious it could be dependent on dataset make-up, e.g. what the components represent and whether they are likely to be shared.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1619#issuecomment-831008286:658,depend,dependent,658,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1619#issuecomment-831008286,1,['depend'],['dependent']
Integrability,"> Ah I think I see the issue! Feature branches should be based off `master` and directing the pull request there! I think what's happening is that a pre-commit hook was installed, but the config only exists on the `master` branch.; > ; > I think this should largely be manageable by rebasing onto master (e.g. `git rebase --onto master 1.7.x`) and changing the branch the PR is targeting via the github interface:. Thanks a lot, I rebased and changed the PR target to `master` so I hope everything is on track now! ; The pre-commit style checks were working as expected now (auto-edits only in the files / parts I edited). > Side note: We're considering separating the highly_variable_genes interface into multiple functions, since the arguments to the different methods don't always overlap in meaningful or intuitive ways. There's nothing you need to do about this right now, but just a heads up to keep the logic for this method separate from the main function. Sounds good!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-795469189:403,interface,interface,403,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-795469189,4,['interface'],['interface']
Integrability,"> Also I don't think it returns a copy, so you would need to handle that. I've got a branch which implements cached datasets for testing as:. we could overcome this by simply updating anndata in the test then. > @cache is new in 3.8, but the implementation is:. what do you suggest to do? use your implementation or implement this wrapper?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-1053622705:331,wrap,wrapper,331,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1053622705,2,['wrap'],['wrapper']
Integrability,"> Also isn’t it cool that it points exactly to the problematic line?. Currently, I think the line number reported is the number of lines past the `:` in the function definition. It'd be really nice if it could tell you which line number in the file it was (which might be difficult for manipulated doc-strings). Also, from what the error message says, isn't the `any(broken)` check testing the same thing as assert lines[0], `f""{name} needs a single-line summary""`? Isn't the first one sufficient?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1492#issuecomment-725996519:338,message,message,338,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1492#issuecomment-725996519,1,['message'],['message']
Integrability,"> Also, I don't think I would mind `Pillow` too much as a test dependency (it doesn't have a ton of dependencies, right?); > ; > A more lightweight solution would be to call [`file`](https://en.wikipedia.org/wiki/File_(command)) on the path, which should be able to recognize it as a `tiff`. I think this would only look at the head of the file though, and wouldn't check if it was corrupted/ didn't finish downloading. I was not aware of `file`. I think it might be a good solution! `readwrite._download` should make sure that downloads are not incomplete, so just reading the header might be enough",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1506#issuecomment-733750462:63,depend,dependency,63,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1506#issuecomment-733750462,2,['depend'],"['dependencies', 'dependency']"
Integrability,"> And scipy is also some 100 MB right?. Scipy is actually under `~/.cache` on my mac, ¯\\\_(ツ)_/¯. > Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. > miniconda is somewhere else for me by default, and it contains everything. I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. > You'd not notice it much, because datasets are just being re-downloaded on demand. So the compute nodes on this HPC have limited internet connectivity. One of the use cases I'd had for adding the expression atlas was to be able to easily try a method across a bunch of test datasets. If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. > My favorite command line interfaces have the ability to query options and set options globally by writing to a config file. I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-478212804:1141,interface,interfaces,1141,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-478212804,4,['interface'],"['interface', 'interfaces']"
Integrability,"> Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. I've been wanting to use Xarray in the backend for AnnData, as AnnData objects are like a restricted `Dataset`. This is mainly blocked by not having CSC/ CSR sparse arrays compatible with Xarray, since we use those formats pretty heavily. @tomwhite's sparse wrapper could be a solution to this, as xarray will accept these if an `__array_function__` implementation is added. I tried a simple, broken in many cases, implementation which had promising results inside DataArrays. I'd definitely like to help fill this out a bit more. <details>; <summary><code>__array_function__</code> implementation</summary>. ```python; def __array_function__(self, func, types, args, kwargs):; result = func(*(x.value if isinstance(x, SparseArray) else x for x in args), **kwargs); if issparse(result):; result = SparseArray(result); elif isinstance(result, np.matrix):; result = np.asarray(result); return result; ```. </details>. @mrocklin would it make sense for this SparseArray class to live in pydata/sparse as a pair of CSR/ CSC classes? The internals could gradually be replaced with a more generic n-dimensional representation, but would get two very common use cases into the library.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/921#issuecomment-557721953:684,wrap,wrapper,684,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-557721953,2,['wrap'],['wrapper']
Integrability,"> As a general approach to this kind of problem, I write functions like this:; > ; > ```python; > def grouped_obs_mean(adata, group_key, layer=None, gene_symbols=None):; > if layer is not None:; > getX = lambda x: x.layers[layer]; > else:; > getX = lambda x: x.X; > if gene_symbols is not None:; > new_idx = adata.var[idx]; > else:; > new_idx = adata.var_names; > ; > grouped = adata.obs.groupby(group_key); > out = pd.DataFrame(; > np.zeros((adata.shape[1], len(grouped)), dtype=np.float64),; > columns=list(grouped.groups.keys()),; > index=adata.var_names; > ); > ; > for group, idx in grouped.indices.items():; > X = getX(adata[idx]); > out[group] = np.ravel(X.mean(axis=0, dtype=np.float64)); > return out; > ```; > ; > Swapping out the last 8 lines or so depending on what I'm calculating. To use a set of marker genes I'd call it as `grouped_obs_mean(adata[:, marker_genes], ...)`.; > ; > At some point we might have `groupby` for `AnnData`s, but that'll require figuring out how to be consistent about the returned type. Thanks. But need to make a tiny amendment to make it work now:; ```python; out[group] = np.ravel(X.mean(axis=0, dtype=np.float64)).tolist(); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/181#issuecomment-1871723078:760,depend,depending,760,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/181#issuecomment-1871723078,1,['depend'],['depending']
Integrability,"> As you can see, it's a pretty trivial wrapper anyway. Yes, makes sense. > Determining which affinity kernel to use would then be as simple as looking into adata.uns to find which parameter value sc.pp.neighbors was called with. Yes, I like this. > I added exaggeration=None, as is the default in openTSNE. But setting it to 1 instead of None is better, and I should change that in the next release. Ah, right, I somehow overlooked that you did add the exaggeration parameter. That's fine then!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-753621515:40,wrap,wrapper,40,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-753621515,2,['wrap'],['wrapper']
Integrability,"> Better: `pp.log1p` should write an attribute to `.uns`, say simply `.uns['log1p'] = True`. Depending on that attribute, log2fc is computed by rexponaniating or not. Fully agree. > ; > Also: If trying to call a t-test with non-logarithmized data, a warning should be written.; > . Also agree. > The overflow and 0 warnings: are you sure you used logarithmized data, Gökcen?. Oh true, it wasn't log transformed, true.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/519#issuecomment-478377325:93,Depend,Depending,93,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/519#issuecomment-478377325,1,['Depend'],['Depending']
Integrability,"> CCA does not have code in python, which will make it difficult to integrate, pySCENIC is probably easier but I would rather ask the developers.; > ; > @falexwolf We should consider a way to facilitate scanpy 'plugins'. A quick search shows me that this could be possible: https://packaging.python.org/guides/creating-and-discovering-plugins/ but honestly I don't know how it works. Nevertheless, given the number of tools that continue to appear we should consider a scheme that facilitate how developers can take advantage of scanpy preprocessing, storing, analysis and visualization tools. Pyscenic has been integrated into scanpy now! Here is the hyper link:; https://github.com/aertslab/pySCENIC/blob/master/notebooks/pySCENIC%20-%20Integration%20with%20scanpy.ipynb",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/265#issuecomment-509063881:68,integrat,integrate,68,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-509063881,4,['integrat'],"['integrate', 'integrated']"
Integrability,"> Can scale_factor be removed as an argument to embedding, and instead have that handling occur inside spatial?. no, otherwise I would have to modify the adata or pass a copy, and this would break other functionalities (first that come to mind, categorical colors saved in adata.uns). > Otherwise we assume it's already an array, and make sure it's the right shape. what do you mean by that>? What you are proposing is to pass the adata.obsm as array in question and not as a string basis right? Is that possible now>? Would be happy to do that but don't want to create and pass an adata copy. > Can an image be passed directly into spatial? I'd prefer this as being the ""expert users"" interface for plotting over an image, and think passing an image to embedding could be removed altogether in the future. done, also there is no image in embedding now. Everything is handled by spatial. It is possible to do something like this:. ```python; img = adata.uns[""spatial""][""V1_Adult_Mouse_Brain""][""images""][""hires""]; scalef = adata.uns[""spatial""][""V1_Adult_Mouse_Brain""][""scalefactors""][; ""tissue_hires_scalef""; ]; sc.pl.spatial(; adata,; color=""leiden"",; scale_factor=scalef,; img=img,; size=100,; basis=""spatial"",; groups=[""0""],; ); ```; ![image](https://user-images.githubusercontent.com/25887487/103009720-7eee5b00-4537-11eb-9bbf-39751493890f.png). I would still like to have this in 1.7 if possible, I can write docs and additional tests real quick tomorrow early morning",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1512#issuecomment-750338610:686,interface,interface,686,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1512#issuecomment-750338610,1,['interface'],['interface']
Integrability,"> Can we get a weighted graph out of the fit embedding object? . Yes, PyMDE can do that. The longer answer is that it depends on the type of embedding problem you set up --- some are specified using weighted graphs, others are not. But most embedding problems (including all problems specified using the `preserve_neighbors` function, which is the most commonly used recipe) have associated weighted graphs. > I'm also wondering about just how early the package is. I would like to be able to take advantage of any new features, and wouldn't want an early API decision to lock us out of those. Great question. The internals will very likely change over the coming months/year. But the interface to the `MDE` class, which is the central object in PyMDE, will likely be stable.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2154#issuecomment-1062027813:118,depend,depends,118,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2154#issuecomment-1062027813,2,"['depend', 'interface']","['depends', 'interface']"
Integrability,"> Completely agree, Gökcen!; > ; > How I just thought about dealing with this in the past couple of minutes: could we not make a submodule rtools? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful.; > . That'd make things a lot easier for many people (including myself 😃), I agree. However. 1) There are (and will be) so many R packages about single cell, so once we open the door, there might be so many requests about these packages so that it'd be difficult to decide what to include and what not to include. The decision might be a bit arbitrary. This is why I suggested a contrib repo, which will have everything users request (as soon as there is someone who is willing to maintain it), in a `use at your own risk` way... 2) There might be several bug reports about rpy2 itself or thin wrappers or R installation or R packages themselves. I was wondering whether this might introduce more maintenance burden, although supported packages will be limited. > The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a scanpy-contrib: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: anndata is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions.; > ; > What do you think?. Alternatively, we can just prepare jupyter notebooks with some Python 3 and some R cells in it (which is super easy via rpy2 magics anyway) for some R packages/functions like mnn or SIMLR and put those in scanpy_usage as a ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/125#issuecomment-382002901:175,wrap,wrapper,175,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-382002901,3,"['depend', 'wrap']","['dependencies', 'wrapper']"
Integrability,"> Cool... I haven't used the new plots yet. Looks very handy. Your wrapper is an automated use of `var_group_X` to label marker genes of particular clusters in these plots?. Just to clarify what I meant, current API:. ```python; sc.pl.stacked_violin(pbmc, marker_genes, groupby='bulk_labels', ; var_group_positions=[(7, 8)], var_group_labels=['NK']); ```. My suggestion:. ```python; markers = {'NK': ['GNLY', 'NKG7']}; sc.pl.stacked_violin(pbmc, marker_genes, groupby='bulk_labels', var_groups=markers); ```. It's ok to keep var_group_* parameters I think, for backward compatibility.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/646#issuecomment-493780398:67,wrap,wrapper,67,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/646#issuecomment-493780398,1,['wrap'],['wrapper']
Integrability,"> Do you get an exception message or something else? If you can also copy paste the error message here, we can debug it more easily. Many thanks for your quick reply!; Unfortunately , no visible exception... My code is as follows:. ```py; import velocyto as vcy; import numpy as np; import scanpy as sc; import anndata. vlm = vcy.VelocytoLoom(""path of DentateGyrus.loom""); S = vlm.S; S=S.transpose(); adata = anndata.AnnData(S); print(adata.X); print(adata.obs); print(adata.var). sc.pp.neighbors(adata, n_neighbors=100); adata.uns['iroot'] = 0; print(adata.uns); sc.tl.dpt(adata, n_branchings=2); sc.pl.diffmap(adata, color='dpt_pseudotime', projection='2d'); ```. error message (a number of warnings as well, taking up lots of lines and I have no idea of how to include all of them here...) :. <details><summary>numba warnings</summary>. ```pytb; WARNING: You’re trying to run this on 27998 dimensions of `.X`, if you really want this, set `use_rep='X'`.; Falling back to preprocessing with `sc.pp.pca` and default params.; /home/liz3/env/lib/python3.6/site-packages/umap/rp_tree.py:450: NumbaWarning: ; Compilation is falling back to object mode WITH looplifting enabled because Function ""make_euclidean_tree"" failed type inference due to: Cannot unify RandomProjectionTreeNode(array(int64, 1d, C), bool, none, none, none, none) and RandomProjectionTreeNode(none, bool, array(float32, 1d, C), float64, RandomProjectionTreeNode(array(int64, 1d, C), bool, none, none, none, none), RandomProjectionTreeNode(array(int64, 1d, C), bool, none, none, none, none)) for '$14.16', defined at /home/liz3/env/lib/python3.6/site-packages/umap/rp_tree.py (457). File ""env/lib/python3.6/site-packages/umap/rp_tree.py"", line 457:; def make_euclidean_tree(data, indices, rng_state, leaf_size=30):; <source elided>. left_node = make_euclidean_tree(data, left_indices, rng_state, leaf_size); ^. [1] During: resolving callee type: recursive(type(CPUDispatcher(<function make_euclidean_tree at 0x7f822dd05d08>))); [2] Du",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/749#issuecomment-515138442:26,message,message,26,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749#issuecomment-515138442,3,['message'],['message']
Integrability,"> Do you think you could provide me with some example objects that are giving you trouble?. you can take any `sc.datasets.visium_sge` and play around with inverting/not inverting second axis,and plotting using `sc.pl.spatial` or `sc.pl.embedding` where coordinates are in `adata.obsm[""coords""]`. I'll give another summary on current situation and goals:. **Type of spatial data**; 1. data with coordinates centered bottom left and no image (non visium); 2. data with coordinates centered top left and no image (visium); 3. data with coordinates centered top left and image (visium). `sc.pl.spatial` should support all of the above cases. In all cases it wraps embedding but in 1. it uses scatterplot, in 2. and 3. it uses circles (with a specified radius, present only in visium). The inversion is needed in case 2., because in case 3. this is already handled by the image axis plot.; My solution for this is to pass inverted coordinates for y axis in 2. to `embedding`, so that everything can be handled by the function independently. ; What do you think is a good way to go about this ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1512#issuecomment-740744921:654,wrap,wraps,654,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1512#issuecomment-740744921,1,['wrap'],['wraps']
Integrability,"> From a first glance, with seurat_v3 requiring count data, it is important that your .X (becoming the layer you refer to as counts) indeed contains counts, otherwise loess quickly runs into stability issues. I would expect there would be a warning here if this were the case, since `check_values` defaults to `True`. But at least this person had the same error caused by passing in normalized values:. * https://www.biostars.org/p/9535944/. The [author of scikit-misc says](https://github.com/has2k1/scikit-misc/issues/6#issuecomment-615304167):. > pass `surface=""direct""`. to the loess solver based only off the error message. So maybe we can enable that. I don't know enough about loess to be able to say why that would fix this. It would be interesting to see the data that caused this error. I would definitely want to have a reproducible case before attempting a fix.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2853#issuecomment-1997957671:620,message,message,620,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2853#issuecomment-1997957671,1,['message'],['message']
Integrability,"> From my error log it seems the only non-noarch dependency is [h5py](https://beta.mamba.pm/channels/conda-forge/packages/h5py). That’s surprising! I think numba is our most complex dependency, and umap’s dependency PyNNDescent is also compiled. I think if this isn’t a mistake and it’s really just about h5py, we can think about it. Trying to install scanpy and following JupyterLite’s debug instructions gives:. ![image](https://github.com/scverse/scanpy/assets/291575/07a30013-e78d-46af-80fd-fb48af71d45b). ```pytb; ValueError: Can't find a pure Python 3 wheel for: 'umap-learn>=0.3.10', 'session-info', 'numba>=0.41.0'; See: https://pyodide.org/en/stable/usage/faq.html#why-can-t-micropip-find-a-pure-python-wheel-for-a-package; ```. (session-info isn’t a problem, it’s just an old package that doesn’t publish wheels)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2667#issuecomment-1803434731:49,depend,dependency,49,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2667#issuecomment-1803434731,6,['depend'],['dependency']
Integrability,"> Given the file sizes nowadays and the number of ""groups"", this is getting fairly computationally intensive. It's one of those simple things your biologists will love (""this is so fast now!""). I agree it doesn't harm to have `rank_genes_groups` parallelized (given that it should be straightforward to implement). ; What @ivirshup was referring to though, is that `rank_genes_groups` on single cells in general isn't seen anymore as best practice for DE analysis because it doesn't account for pseudoreplication bias. Please take a look at @Zethson's [book chapter](https://www.sc-best-practices.org/conditions/differential_gene_expression.html). . > RE: pertpy; >; > Could does this relate to @davidsebfischer and diffxpy?. Diffxpy is currently being reimplemented. Once it is released, it would likely be included in pertpy as an additional method. I.e. pertpy is more general and strives to provide a consistent interface to multiple methods.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2390#issuecomment-1396521226:916,interface,interface,916,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390#issuecomment-1396521226,2,['interface'],['interface']
Integrability,"> Great!; > ; > I'm not sure what's going on with that conda build, but hopefully one of the maintainers there will know what to do.; > ; > On the topic of this PR, I do think this case should give a more helpful error message. Would you like to do that?. Sure- there you go. I reverted the above and just raised an error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1490#issuecomment-726651017:219,message,message,219,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1490#issuecomment-726651017,1,['message'],['message']
Integrability,"> Great, thank you!. hi, did you find the ""merge"" or ""integrate"" commond in scanpy?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/702#issuecomment-527330223:54,integrat,integrate,54,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702#issuecomment-527330223,1,['integrat'],['integrate']
Integrability,"> Hey @ywen1407!; > ; > The ideal case is that you don't pre-filter the gene sets before concatenating. Then, if you have aligned both sets of samples to the same genome, everything should be fine and you can filter out genes afterwards. Otherwise an outer join would only assume all values you filtered out were 0, which is probably not the way forward. That's why the only decent option you really have is an inner join. I assume you should have the unfiltered objects somewhere though.; > ; > Regarding memory use: ComBat is something we (actually, this was thanks to @Marius1311) just re-implemented from python and R code that was flying around. We do not generally optimize methods that were published elsewhere. How much RAM are you using that it's crashing? I think Marius even made ComBat usable for sparse matrices, so it's already using less memory than it was before. 38K cells doesn't sound like something that would require more than 16GB RAM. I can run datsasets with 50k locally. You can of course always try other batch correction/data integration methods that are less memory intensive such as BBKNN or scVI. We tested scalability of data integration tools (also BBKNN and ComBat memory use) here: https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2. However, ComBat is one of the least memory intensive methods out there... so maybe there is little room for optimization here... Thanks for the explanation. I tried concatenating all samples with inner join and it actually went well! The overall number of genes do drop from 45K to around 20K but after preprosessing, the clustering looks OK.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1431#issuecomment-699114229:1053,integrat,integration,1053,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1431#issuecomment-699114229,4,['integrat'],['integration']
Integrability,"> Hi @grimwoo,; > ; > The data integration methods MNN and BBKNN are implemented in scanpy externals, which you can find [here](https://scanpy.readthedocs.io/en/stable/external/index.html#batch-effect-correction). You can also use combat correction, which is a simpler, linear batch effect correction approach implemented as `sc.pp.combat()`. Thanks you so much~",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/702#issuecomment-527391920:31,integrat,integration,31,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702#issuecomment-527391920,2,['integrat'],['integration']
Integrability,"> Hi @grimwoo,; > ; > The data integration methods MNN and BBKNN are implemented in scanpy externals, which you can find [here](https://scanpy.readthedocs.io/en/stable/external/index.html#batch-effect-correction). You can also use combat correction, which is a simpler, linear batch effect correction approach implemented as `sc.pp.combat()`. sorry to bother you again. ; I want to merge adata001, adata002, and adata003 into adata.combined, with mark ""001"", ""002"", and ""003"" respectively. I looked into the help-information of ""help(combat)"", but still don't know how to do so. In Seurat (R), it can be done like: ; adata001$Sample <- ""001""; adata002$Sample <- ""002""; adata002$Sample <- ""003""; adata.anchors <- FindIntegrationAnchors(object.list = list(adata001, adata002, adata003), dims = 1:11); adata.combined <- IntegrateData(anchorset = adata.anchors, dims = 1:11)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/702#issuecomment-527731457:31,integrat,integration,31,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702#issuecomment-527731457,3,"['Integrat', 'integrat']","['IntegrateData', 'integration']"
Integrability,"> Hi @sygongcode,; > ; > Are you referring to differential expression testing between conditions? You can do that with `sc.tl.rank_genes_groups()` or in a more advanced way using `diffxpy`, which is easily integrated with `scanpy`. You can find it [here](https://github.com/theislab/diffxpy). Yes, that is what I want to do. Thank you so much.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/821#issuecomment-529218989:206,integrat,integrated,206,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/821#issuecomment-529218989,1,['integrat'],['integrated']
Integrability,"> Hi, It's not available in scanpy at the moment, but I wrote a wrapper for it via `rpy2` and `anndata2ri` which is available here:; > https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py. Hi,. I have been trying to use this wrapper, but seems like there's some error during the conversion process:. RRuntimeError: Error in validObject(.Object) : ; invalid class “dgCMatrix” object: 1: invalid object for slot ""i"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer""; invalid class “dgCMatrix” object: 2: invalid object for slot ""p"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer""; invalid class “dgCMatrix” object: 3: invalid object for slot ""Dim"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer""; invalid class “dgCMatrix” object: 4: invalid object for slot ""x"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""numeric"". Any pointers to get around this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1068#issuecomment-866121061:64,wrap,wrapper,64,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-866121061,2,['wrap'],['wrapper']
Integrability,"> Hi, please provide the data you use, otherwise this is not reproducible:; > ; > ```; > FileNotFoundError: [Errno 2] Unable to open file (unable to open file: name = '\external/CytAssist_FFPE_Human_Lung_Squamous_Cell_Carcinoma_filtered_feature_bc_matrix.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0); > ```. Hey, the data is publicly available under this link: https://www.10xgenomics.com/resources/datasets/human-lung-cancer-ffpe-2-standard. I simple copied the `curl` bash script to download all the files and then unzipped the file corresponding to the images to get the ""spatial"" folder",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2778#issuecomment-1845048906:277,message,message,277,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2778#issuecomment-1845048906,2,['message'],['message']
Integrability,"> Hi,; > ; > all things that are not transcriptomics should ideally live in our sister packages such as https://github.com/scverse/muon . If you remove all things related to ATAC-seq in this PR we could consider it, but honestly I think that this might better live outside of scanpy external and much rather in the [scverse ecosystem](https://scverse.org/packages/#ecosystem). Hi,; Thank you for your reply. I could remove the ATAC in the PR, actually, this commit [7f74d8c](https://github.com/scverse/scanpy/pull/2355/commits/7f74d8c47005dd630f691ab5926095f0ff277ce8) is the version without ATAC. Please let me know if this does not work, I will commit another PR.; I think scalex is very suitable to be included in pp.external since it was developed based-on scanpy system, which could provide more choices for scanpy users to do single-cell integration.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2355#issuecomment-1376280885:844,integrat,integration,844,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2355#issuecomment-1376280885,1,['integrat'],['integration']
Integrability,"> How about a var_type argument which defaults to ""genes""? Could even replace occurrences of cells with a obs_type variable. Sorry about the late response. The week passes so quickly and depending on how it goes, I can only go back to Scanpy during the weekends these days... I like your new table much better than the previous one. If you'd put that forward in a PR, I'd be super happy!. PS: I think `control_variables` is fine, maybe `focus_variables` better reflects that it's not only about ""control"", but just a subset of variables (or observations) of interest. Let's not move forward on the suffix argument, your imposed structure will make things cleaner for people! :smile: `pct` is ok if in 0 - 100. No `obs_type` needed right now...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/316#issuecomment-437734137:187,depend,depending,187,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-437734137,1,['depend'],['depending']
Integrability,"> I actually meant recreate the counts by reloading the data object ;). I guess I think about this because I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"" (with annotations, noisy genes, raw and normalized expression, cell/gene representations etc.). Imagine you upload a single h5ad file to GEO when you publish something and you're done without thinking about how much the users can ""go back"" from the h5ad file. Otherwise yeah, it's possible to either unnormalize things or load the original data file. > we would normally regard this as background noise anyway, no?. This depends on how the filtering is done I think. Some people keep only protein coding genes in adata.X, which makes adata.raw even more important since all non-coding gene expression goes to adata.raw. Or miro/ribo genes are filtered out sometimes, which might be needed later on e.g. to redo qc etc.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1798#issuecomment-819938442:660,depend,depends,660,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-819938442,2,['depend'],['depends']
Integrability,"> I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did.; > ; > First I tried to upgrade numba and umap as suggested by the other individuals in the thread:; > ; > ```shell; > pip install --upgrade numba; > pip install --upgrade umap-learn; > ```; > ; > Then I essentially reinstalled scanpy using the steps in their installation docs.; > ; > ```shell; > conda install seaborn scikit-learn statsmodels numba pytables; > conda install -c conda-forge python-igraph leidenalg; > pip install scanpy; > ```; > ; > I think I then ended up with a version of numpy that was incompatible with numba so I ran; > ; > ```shell; > pip install numpy==1.20; > ```; > ; > After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:; > ; > ```shell; > python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))""; > ```; > ; > This seemed to fix my problems; I hope it's able to help others!. I followed your instruction but it still threw errors:. <frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject. Segmentation fault",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1567#issuecomment-1063184606:1238,message,messagestream,1238,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567#issuecomment-1063184606,3,"['Message', 'message']","['MessageStream', 'messagestream']"
Integrability,"> I don't like the use of branches with `isinstance` because it breaks polymorphism, which is a key part of pythonic code to me. That’s what ABCs are for. `isinstance(thing, Mapping)` works beautifully for everything that has the `Mapping` protocol, no matter if it’s a `dict` or not.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-444816086:240,protocol,protocol,240,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-444816086,1,['protocol'],['protocol']
Integrability,"> I doubt that it would be considered a branch of logic. What do you define as logic here? I was talking about the logic theory that encompasses formal systems and so on. > [Union and intersection are bad names]. I agree, wikipedia enumerates more names, and explains where “union” comes from:. > tagged union, variant, variant record, choice type, discriminated union, disjoint union, or sum type; > …; > Mathematically, tagged unions correspond to disjoint or discriminated unions, usually written using +. Given an element of a disjoint union A + B, it is possible to determine whether it came from A or B. If an element lies in both, there will be two effectively distinct copies of the value in A + B, one from A and one from B. . I think “discriminated union/intersection of types” would make sense here. leaving out the “discriminated/tagged/disjoint” here is the problem. in C there’s actual *untagged* unions, which simply means that C reserves the memory for the largest of the intersected types and you need to keep track yourself of which the type of the value is. In python you can always do `isinstance`, so a more correct name for `Union[A, B]` would be `TaggedUnion[A, B]`. I’d also like `OneOf[A, B]`, but that ship has sailed. And intersections are basically duck types or structural types (when anonymous) and traits/interfaces (when named). (i.e. `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. So it makes sense for python, it’s just defined more explicitly than by literally intersecting types.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-443178140:1336,interface,interfaces,1336,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-443178140,2,['interface'],['interfaces']
Integrability,"> I for now would not try to differentiate between noqas that we want to keep and noqas that we want to get rid of. We want to get rid of all of them in the follow up issue and only when examining all of them we will figure out which ones we want to keep. After this merges new ignore messages can be added for reasons like ""this rule is generally good, but not in this specific case"". Each of these will go through PR review, so will be vetted. The ones added here largely have not been vetted, and are just being added so we don't get a failure. I would like to be able to distinguish between these cases. Once more `noqa` cases are added, it gets more complicated to find cases that haven't been vetted if they don't have some associated label. --------------------------. > What do you mean? How to ignore a single line? How to fully ignore whole checks?. How to disable flake8 errors for a line or file. > I would always refer to the flake8 documentation, because it will certainly maintained better than the dev documentation. A link to the section of the flake8 docs on this would be great. -------------------------. > I wish it were that easy. autopep8 does not take its configuration from the flake8 config file . `autopep8` says it does this: https://github.com/hhatto/autopep8#configuration. > It formats consistently, but not necessarily compatible with other tools. I would like changes that are automatically applicable to be automatically applied. I'm thinking of things like white space in docstrings. Is there another way to automate these you can suggest?. ---------. BTW, I've added a few more points to the checklist above. I would recommend trying to build the package and build the docs in the directory you're working in to see what files get generated so they can be added to the `ignore`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1689#issuecomment-787426782:285,message,messages,285,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689#issuecomment-787426782,1,['message'],['messages']
Integrability,"> I had the exact same issue and error message at that step in the tutorial. I installed scanpy using pip, because installing with conda was not working. Same here. I assume there is some issue with the implementation of the setter of adata.X, which prevents `adata.X = adata.X.toarray()` from updating X to its densified version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1010#issuecomment-578596689:39,message,message,39,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1010#issuecomment-578596689,1,['message'],['message']
Integrability,"> I have got what I want with the following code adapted from dotplot():; > ; > gene_ids = adata.raw.var.index.values clusters = adata.obs['louvain'].cat.categories obs = adata.raw[:,gene_ids].X.toarray() obs = pd.DataFrame(obs,columns=gene_ids,index=adata.obs['louvain']) average_obs = obs.groupby(level=0).mean() obs_bool = obs.astype(bool) fraction_obs = obs_bool.groupby(level=0).sum()/obs_bool.groupby(level=0).count() average_obs.T.to_csv(""average.csv"") fraction_obs.T.to_csv(""fraction.csv""). Love this! Thanks a lot!! ; Just one question, is there a way to get the average expression in different cell types (cluster label 1 ) in different sample (cluster label 2 ) from an integrated object?? ; to get something roughly like this:. Gene 1 Gene 2 ; sample1 sample2 sample3 sample1 sample2 sample3 ..... ....... ....; T-cell; B-cell ; .....; ..... I am not sure if this makes sense, but I have been trying to do this for a while and nothing worked!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/336#issuecomment-1334674713:681,integrat,integrated,681,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/336#issuecomment-1334674713,1,['integrat'],['integrated']
Integrability,"> I saw some of the github automated tests test are failing now, but I don't really understand the error messages tbh ;) Are they even related to the execution of the code provided by this PR?. yeah also don't understand them, it might be @cache in py 3.7 has issues? will investigate next week and report back!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-1050003610:105,message,messages,105,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1050003610,1,['message'],['messages']
Integrability,"> I think it would be sufficient for sc.tl.tsne to warn users if the graph it was passed looks unexpected. Yes, warning is a good idea. But the warning IMHO should not convey the message ""Do not do this!"". In my mind, it should convey the message ""What you are computing is not exactly t-SNE, but it is close enough to t-SNE that you can ignore this message. If you really want to get exactly t-SNE, run the following command instead: ..., but note that it can be slower and the result will likely look around the same"". > From an API point of view, we don't control weights at the sc.tl.umap call, so I think it would be strange to control weights at the sc.tl.tsne call. But we will *have* to control them anyway... Your suggested solution also controls them: namely, symmetrizes and normalizes. > I'm also not sure if binarizing the graph would be closer to ""t-SNE"". Maybe not, but; (1) it will be further away from UMAP, so that e.g. UMAP paper does not need to be cited when using such t-SNE. ; (2) There is citeable literature showing that binary affinities yield practically the same result as t-SNE proper. We can cite this in Scanpy docs. I am not aware of any such literature for normalized UMAP affinities in t-SNE. Stepping back, I am not sure I managed to convey my main point here. Which is: Scanpy is in a unique position to offer people t-SNE with k=15 binary affinities as a convenient, faster, *UMAP-independent*, and nearly equivalent replacement for k=90, perplexity=30 affinities. . I agree with Pavlin though that pretty much any decision would be better than the current situation :-)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-762763727:179,message,message,179,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-762763727,3,['message'],['message']
Integrability,"> I think this could be done more efficiently by using the index returned from `filter_genes(..., inplace=False)` in `_highly_variable_genes_single_batch` and instead of the whole data frame merging you add in the current version of your changes. I guess that would depend if you want to have a `filter_genes` call in the HVG selection function every time, or whether you only want it in there in a case where `filter_genes` normally doesn't work. You typically use it on the whole dataset, but not per batch. Another issue atm is that if you set the verbosity high, then the `filter_genes()` call gives you an output, which is not really intended as the user can't see the function call.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/824#issuecomment-529560265:266,depend,depend,266,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/824#issuecomment-529560265,1,['depend'],['depend']
Integrability,"> I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common scio package could be a way to achieve that. Agree, also R packages seem to be doing just fine with Seurat/Bioconductor representation?. > Also muon and scvi-tools read in different things from 10x atac data. This is not intentional at all, muon read atac data would work just fine in our package. > A solution to that would be versioned schemata. That could be good, the schema also don't have to be versioned, we can just have a few options and package devs wrap the method with their choice.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059798725:575,wrap,wrap,575,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059798725,1,['wrap'],['wrap']
Integrability,"> I think we should have a 'cookbook' where we can keep this and other information. I've been trying to be organized about keeping notebooks around for this ([here](https://github.com/ivirshup/notebooks/tree/master/plotting)). Of course, I rarely get the notebooks clean enough to push 😆. > > In the end it's about showing which cells are represented per pixel/pixel bin.; >; > I would argue that this would be fair. In the end it's about showing which cells are represented per pixel/pixel bin. Is it fair if coloring by batch and one dataset had fewer samples? Wouldn't you want to know that multiple batches were showing up in this region? I'm fairly convinced there is no good way to show this in one plot, other than telling users some information is hidden. > We could do a quick fix based on random order for now. I'm trying to think of the simplest way to implement this. I would like to keep the behaviour of `sort_order=False` just using the order from the anndata object. Some options:. * `sort_order=""random""`, this would make the order random, but we might need to add a seed argument. Also, do we still plot over null values?; * `sort_order=order_array` where `order_array: np.ndarray[1, int]`. Basically, the user can pass whatever order they like. For random order it would be `np.random.choice(adata.n_obs, adata.n_obs, repeat=False)`. This is pretty flexible since it allows whatever order you want to be used without sorting the object. > larger update that would have to do with updating scanpy plotting to larger cell numbers?. I think this might be worth a separate package, at least to start out. At least with how I'm handling it now, there would be a large number of dependencies. Plus, I think overplottting like this is an unsolved problem, so freedom to experiment in important.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1263#issuecomment-761745895:1692,depend,dependencies,1692,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1263#issuecomment-761745895,2,['depend'],['dependencies']
Integrability,"> I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". This makes more sense now. In that case however I would say that having just raw counts in `adata.raw.X` is fine, no? In the end you are distributing a data file. You can have your version of the normalized data in a layer... and you would be distributing your analysis code as well, so it's always clear how people should use this data file that is being deposited, no?. > Might be important for integration?. Integration works better with HVGs typically, so I don't think these super lowly expressed genes are so relevant here... I would often go with `min_cells=20` or even `50` for larger datasets. In the end I reason that this value will be approximately related to the size of the smallest unique cellular identity you expect to find. > This does run into memory usage problems if want do a densifying transform on the data. Don't understand this entirely... and not sure what a block sparse matrix type is... but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask... although i can imagine it might be slower than doing this on dense matrices. Based on above arguments the main issue I see is currently for the case @gokceneraslan mentioned about MT genes or non-coding genes being stored in `.raw`. In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. This clashes with the way raw is used in current scanpy pipeline. I think we could deprecate the way `.raw` is used at the moment, and use a `.layer` for this instead (maybe a designated ""raw"" layer?), but then introduce a new `.frozenraw` or sth like that where just the raw data is stored and it's essentially read-only after assignment?. I would be a bit hesitant to not have a replacement f",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1798#issuecomment-820336449:532,integrat,integration,532,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-820336449,3,"['Integrat', 'integrat']","['Integration', 'integration']"
Integrability,"> I wouldn’t call that situation fine, doing things at import time or even just requiring a certain value as configurable global state is bad behavior. It was fine in that it made UMAP work more places than it would have otherwise. > This means our solution for the second shouldn’t be that we hardcode a threading layer to use here. We could make it configurable on our end or something, but no import time global state change. I don't really want to have to touch it, especially since I believe it's immutable after any parallel code is called – so we can't actually control in. Our hands are a little tied by having a dependency do this. I would like to know why this is still considered necessary for pynndescent.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1931#issuecomment-874661678:621,depend,dependency,621,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1931#issuecomment-874661678,1,['depend'],['dependency']
Integrability,"> I'm not sure how you could get any python setup to install R dependencies for you. Maybe a conda package could include dependencies? I think getting a working environment would alleviate a large pain point for this stuff (for example, I currently have no working Seurat install.). Plus making sure packages are up to date for the wrapped functionality. > you'd have to have a a separate data structure that can move been languages. Sort of. The idea is that you could move arrays to R from python without making any copies, they'd just point to the same memory. This is already possible when passing data from R to python. The main idea is making these wrappers faster and take less memory.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1068#issuecomment-590215132:63,depend,dependencies,63,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-590215132,4,"['depend', 'wrap']","['dependencies', 'wrapped', 'wrappers']"
Integrability,"> I'm not sure i fully understand the point of caching. So you store the exact output of all the computations of a function so that it can be rerun exactly? How big do those objects become?. We've had problems in the past when running notebooks on different computers (by having different distros or just using the server) or just updating a library produced different results in terms of embedding/clustering... The other benefit is that if analyzing the data in multiple stages (or multiple times), you'd have to either store the adata object after each stage and then load it for the next one. Or just run it from scratch, which can take some time. Not to mention a forgotten parameter which affects reproducibility. The caching makes this convenient - just run the notebook. We only store the attributes generated by each function, therefore the size depends on what you cache and the dimensionality of the data. For ~8k cells, PCA takes upto 8MB (if I remember correctly).; Currently, there's no compression scheme in place, but I have it on my todo list.; The other thing would be to add more control to user during runtime about what needs to be cached.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/947#issuecomment-562544523:855,depend,depends,855,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/947#issuecomment-562544523,1,['depend'],['depends']
Integrability,"> I'm not sure what t-SNE implementation is currently used in scanpy, but would it make sense to switch it to openTSNE? It's a Cython re-implementation of FIt-SNE, it's available on conda and should be very easy to depend on. We use `MulticoreTSNE` if it's installed, but fall back to `sklearn`. > As far as I understand the scanpy architecture, it builds a kNN graph and then runs downstream analysis. Right now, we tend to use a connectivity graph built by UMAP, but are working on making this more generic. We're thinking about allowing the UMAP embedding to be generated on graphs we provide as well. > 1. switch scanpy to using openTSNE for tSNE, using already constructed kNN graph. I think I'd like to see this. That package is much more actively maintained than our current backend, and looks interesting. I would like it if the TSNE was flexible about the graph that was used. I'm not sure that I'll get to this, but a PR would be welcome. I'd have to see some performance/ results before thinking about changing the defaults, or whether this would go into a major or minor version change. > 2. add tSNE support for ingest using openTSNE functionality. @Koncopd do you have any thoughts on this?. > 3. change default tSNE parameters (n_iter, learning rate, initialization) following openTSNE defaults. Again, I'd have to think about backwards compatibility. Maybe this could start as a `sc.tl.opentsne` function?. > 4. add some tSNE ""recipes"". I'd be interested in this. Skimming that paper now, I really like the idea of showing regions of uncertainty for projection would be very useful. I'd be interested in how these ""recipes"" could be wrapped in a function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1233#issuecomment-631235395:215,depend,depend,215,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1233#issuecomment-631235395,4,"['depend', 'wrap']","['depend', 'wrapped']"
Integrability,"> I'm wondering if there might be a jax implementation as I'm a bit more keen on that as a dependency. Probably for another discussion -- I like jax as much as anyone, but it's not nearly as easy to install as pytorch, especially on windows and m1 mac.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2154#issuecomment-1062188081:91,depend,dependency,91,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2154#issuecomment-1062188081,1,['depend'],['dependency']
Integrability,"> I've observed that it may introduce unexpected changes (not shown here) in the shape of the distribution that is different from all of the other transforms mentioned, so these need to be validated biologically against some ""ground truth"". Here's some selected examples (skipping the raw and geometric mean for reasons stated earlier) of the additional aspect introduced by CLR, beyond linearization of the signal, which illustrate how one might want to decide on a case by case basis which is biologically true:. Some *potential* artifacts:; - discreteness at low values (reflected in the histograms earlier), and a ""kink"" near there in the contour that doesn't match with a 2D-gaussian; - skewing of the ""absence"" of a marker depending on presence of another marker; - a weird double-positive tail that extends along the diagonal. These types of effects are reminiscent of [flow cytometry artifacts](https://docs.flowjo.com/flowjo/graphs-and-gating/gw-transform-overview/gw-transform-digital/). However, without proving which one is ground truth, we don't know for sure which one is true. At least initially, I would think that the CLR plots look more plausible. ![image](https://user-images.githubusercontent.com/20694664/83360046-51985080-a34c-11ea-9ec0-2057301ae4fc.png). ![image](https://user-images.githubusercontent.com/20694664/83360065-74c30000-a34c-11ea-9e0b-d28cea53993e.png). ![image](https://user-images.githubusercontent.com/20694664/83360079-84dadf80-a34c-11ea-9026-4256d8a3199b.png). I used a neutral word earlier: that CLR ""injects"" additional changes, but now it seems that may be a positive thing because many of these empirical cases seem believable from a biological standpoint -- a more systematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1117#issuecomment-636513215:729,depend,depending,729,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-636513215,2,['depend'],['depending']
Integrability,> If our dependencies have dropped support we can too. You mean like anndata? :laughing: . > Is this ready for review? I think it mostly looks good. yeah!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2605#issuecomment-1772221723:9,depend,dependencies,9,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2605#issuecomment-1772221723,1,['depend'],['dependencies']
Integrability,"> If we need multiple tools in the same container the place to add it would be [BioContainers/multi-package-containers](https://github.com/BioContainers/multi-package-containers). We do make heavy use of optional dependencies, so this might be the way to go regardless. > Curious to know why and if it's something that can be overcome?. ### Practically. * The documentation for bioconda has been incomplete and out of date for years.; * conda-forge autoupdates recipes. When we make a pip release, a conda-forge release is automatically generated.; * bioconda packages can depend on conda-forge packages, but not the other way around (last I checked at least). If we go on bioconda all our dependents do too – *this could make it extremely painful to do a migration to bioconda*.; * All of our dependencies are on conda-forge; * Fewer channels to search means easier, faster environment solving. ### More philosophically. Why have separate package registries for biology vs everything else? Code for biology isn't particularly special, much of the tooling/ work here is duplicated effort. Why not just put all of bioconda onto conda-forge, but with a special tag saying they are bio packages? All the extra tooling/ maintenance consortiums can be developed orthogonally to the registry. I think there are very clear problems that come out of separate registries. It was a huge pain to install anything from BioJulia until they deprecated the BioJuliaRegistry. If bioconda didn't use it's own build system there wouldn't be out of date docs for that build system. It just seems like a lot of trouble to go through for unclear benefit. I will admit, I think there were more benefits to this model ~a decade ago. But I think these benefits have been mitigated by significantly improved tooling for developing, building, and distributing packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2281#issuecomment-1160555404:213,depend,dependencies,213,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2281#issuecomment-1160555404,8,['depend'],"['depend', 'dependencies', 'dependents']"
Integrability,"> If you have integrated embeddings (such ash X_pca_harmony) those will change every time you add new data. . This isn't always true though, e.g., if you use scArches or seurat (which also seems to use this umap transform). On the other hand, the umap transform visualization can be quite deceiving. It can be the case that it qualitatively appears to have no batch effects even when there definitely has been no integration/correction.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2259#issuecomment-1134250744:14,integrat,integrated,14,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2259#issuecomment-1134250744,2,['integrat'],"['integrated', 'integration']"
Integrability,"> In particular, I'm wondering if there might be a jax implementation as I'm a bit more keen on that as a dependency. I don't have any plans to switch from PyTorch to JAX. I did evaluate JAX when I started the project, but it wasn't mature enough back then. > I'd be interested in seeing how these graphs perform compared to the ones we get from UMAP. I'm not super clear on the semantics of the graphs obtained from UMAP. They might differ somewhat from the ones obtained from PyMDE. > Would this be the right way to retrieve the graphs for the object, or is distortions not the right field?. That's not quite right. Assuming that `mde` was constructed from `preserve_neighbors`, try this:. ```python3. weights = mde.distortion_function.weights.cpu().numpy(); edges = mde.edges.cpu().numpy(); n_items = mde.n_items. graph = pymde.Graph.from_edges(edges, weights, n_items).adjacency_matrix; ```. (API docs for `Graph` here: https://pymde.org/api/index.html#pymde.Graph. In the Graph class, distances/weights are used interchangeably.). I'll just mention however that with PyMDE, the weights and edges don't fully determine the embedding. The weights are parameters to distortion functions, which convey the extent to which two items are similar or dissimilar. Roughly speaking positive weights mean items are similar and should be close together, and negative weights mean that they're dissimilar and shouldn't be close (but need not be far). More details here:https: //pymde.org/mde/index.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2154#issuecomment-1062222262:106,depend,dependency,106,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2154#issuecomment-1062222262,2,['depend'],['dependency']
Integrability,"> In which I introduced that convention when helping Laleh to make it more efficient. Cool, I didn't know that! Should have made it a lot more efficient! :smile:. > The convention I know is to return two n × k matrices. Right, this is the default in sklearn. But yes, in the end, we want some sort of adjacency matrix for convenience and direct integration with all the graph stuff.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/441#issuecomment-460070455:345,integrat,integration,345,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/441#issuecomment-460070455,1,['integrat'],['integration']
Integrability,"> Is there anything like [clustree](https://github.com/lazappi/clustree) in python that integrates nicely with scanpy?. I have resorted to writing a small Rscript that takes a saved adata.h5ad file as input, loads it using `reticulate`, runs Clustree, saves it. I then run the script from a notebook using `invoke.run` from the `invoke` package as a function in a notebook and load the output figure as an image in the notebook. Here is the script I use in case it helps:. ```R. suppressPackageStartupMessages({; library(reticulate); library(SingleCellExperiment); library(glue); library(clustree); }); sc <- import(""scanpy""). args <- commandArgs(trailingOnly = TRUE); H5AD_PATH = args[1]; OUT_PATH = args[2]. print(glue(""H5AD_PATH: {H5AD_PATH}"")); print(glue(""OUT_PATH: {OUT_PATH}"")). load_adata = function(h5ad_path) {; adata <- sc$read_h5ad(h5ad_path). return(adata); }. count_clusterings = function(adata){; # Ryan suggests:; # length(grep(""leiden"",names(adata$obs))). clusterings = c(); for (x in adata$obs_keys()){; if (startsWith(x, ""leiden"")){; clusterings = append(clusterings, x); }; }; ; return(length(clusterings)); }. set_fig_dimensions = function(num_clusterings){; width = 10; height = (0.6 * num_clusterings); ; if (height < 8){; height = 8; }; ; png(width = width, height = height); options(repr.plot.width = width, repr.plot.height = height); ; return(list(width=width,height=height)); }. adata = load_adata(h5ad_path=H5AD_PATH). dims = set_fig_dimensions(num_clusterings = count_clusterings(adata)); # dims. # options(repr.plot.width = 10, repr.plot.height = 10). g = clustree(; x=adata$obs,; prefix=""leiden_"",; # suffix = NULL,; # metadata = NULL,; # count_filter = 0,; # prop_filter = 0.1,; # layout = ""sugiyama"",; # layout = ""tree"",; # use_core_edges = FALSE,; # highlight_core = FALSE,; # node_colour = prefix,; # node_colour_aggr = NULL,; # node_size = ""size"",; # node_size_aggr = NULL,; # node_size_range = c(4, 15),; # node_alpha = 1,; # node_alpha_aggr = NULL,; # node_text_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/670#issuecomment-785309409:88,integrat,integrates,88,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670#issuecomment-785309409,1,['integrat'],['integrates']
Integrability,"> It is mentioned in Seurat Pbmc3k example that best resolution parameter is 0.6-1.2 , but you used less and get more clusters. May be because i didn't explore random seed in leiden. I suspect this is because I processed the dataset with `sc.tl.diffmap`. The random seed tends to make only minor differences (+/- 1). As far as I can tell, the resolution parameter really is dataset dependent. But maybe someone with a better knowledge of the actual algorithm can comment on this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/670#issuecomment-498161514:382,depend,dependent,382,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670#issuecomment-498161514,1,['depend'],['dependent']
Integrability,"> Just to go back to your original problem, in your case you were using as mapping categories that were not present in your groupby key altogether. This is a different issue, and probably the function should have thrown an error saying var_group_labels are not present in categories. Since I just copied the example from the tutorial, I think it would be great fix the handling of heatmap there. There, the problem is sort of two-fold: 1) as you mentioned, the groupby labels NK/T-cell etc. hadn't been defined before, and 2) that only markers for only a subset of the clusters are used (5 out of 9 clusters have markers) are used with `dendrogram=True`. Both of them independently provoke the warning/issue about the reordering.; Maybe an error would be appropriate not only for undefined groupby labels (as you suggested), ; but also for the case where cluster markers for only a subset of clusters are supplied, instead of delivering a warning and a potentially incorrect ordering.; In the latter case, an error with a message suggesting to use `dendrogram=False` would be worthwhile. The danger with just printing a warning is that it might be ; missed/ignored by the user (e.g. if the function is used in a pipeline with lots of other outputs) ; and mismatching color codes might also not be apparent initially. ; For instance, it took me a while to even spot this issue, as i didn't notice the mismatching colors initially.; This confusion could be avoided if heatmap doesn't produce a plot at all when incorrect arguments are used. I agree that your workarounds also work, but they don't fully serve as substitutes to make the plots as in the tutorial yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1479#issuecomment-723071522:1022,message,message,1022,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1479#issuecomment-723071522,1,['message'],['message']
Integrability,"> Moving 10x reading functions to anndata. I haven't worked much with h5py or tables, is it time-consuming to refactor these functions? It seems like moving to anndata is the most straightforward solution at least logically to me. > scanpy as a requirement. I like scanpy, but the only thing we really *require* in scvi is the data loading part. A user could take their scvi outputs and go use Seurat if that makes them happy. And then like the data loading functions are simple enough that we could just implement them ourselves. I'm sure a lot of people are currently doing this, which inspired the idea to have a standalone package. > Splitting off new modules. Your questions are very valid. I don't really have good answers for them. I could just see a standalone package being widely used and community driven, especially if there is some scanpy backing + maybe optional dependencies/functionality to get your objects ready for R analysis pipelines.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1387#issuecomment-680188365:877,depend,dependencies,877,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-680188365,2,['depend'],['dependencies']
Integrability,"> My idea was to have print_header output very little, plus an expandable region (as it’s the one called in notebooks), and to revert print_versions to just copyable text output. Could we deprecate `print_header` and instead suggest a way to call `session_info` for the equivalent?. If all we're doing is calling `session_info.show` with a couple default arguments, I'm not sure it's worth keeping here. I'd like users to call it directly because:. * Users get access to all of the session_info options without us having to mediate that; * If something doesn't work, it's not our problem. > session_info has no file argument. It has `write_req_file`, which is the same intent – right?. I assume `file` was there from a time when this wrote something that you could `pip install` from?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2089#issuecomment-998837030:524,mediat,mediate,524,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2089#issuecomment-998837030,1,['mediat'],['mediate']
Integrability,"> New analysis tool: A simple analysis tool you have been using and are missing in sc.tools. What about alternative normalization tools like SCTransform? I read that they are supposed to be better for spatial data. As non-mathematician of course I'm not sure how big the difference will really be in the end but it would be great if there was a easy way to call and test them if it's worth it. > New plotting function: A kind of plot you would like to seein sc.pl?. I think a plot that shows the gene expression profile along a spatial axis would be nice if this is not planned yet. So to draw in e.g. a line in napari and get the gene expression of certain genes along this line. > External tools: Do you know an existing package that should go into sc.external.*?. A package I found very useful and easy to integrate with scanpy is SpatialDE. Are you planning to provide this in `sc.external.*`? And of course tools to integrate sc-RNA-seq and spatial data (like Stereoscope, cell2location,...) would be great! But I think you mentioned that there are plans for own tools, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1653#issuecomment-782699618:809,integrat,integrate,809,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1653#issuecomment-782699618,4,['integrat'],['integrate']
Integrability,"> Oh interesting, I thought it was clear :) I mean you even contributed to the function, no?; > ; > I think we also discussed why not to use intersection by default in the PR: [#614 (comment)](https://github.com/theislab/scanpy/pull/614#issuecomment-485875031); > ; > If intersection is not used by default, why would we write in the documentation that it acts as a lightweight batch correction method. I'm as surprised as you are :). Yes, I fixed sth and reorganized a bit. I also recall our disc on `highly_variable_intersection`. However, I thought your organization of HVGs was only for the ranking in `highly_variable_nbatches`. Didn't see it's also the default for `highly_variable`. I never really looked at the docs... that would have given a hint... I still feel as though I have sth slightly different though if I recall. Will look more carefully once this benchmarking data integration thing is out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1032#issuecomment-617120764:885,integrat,integration,885,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032#issuecomment-617120764,2,['integrat'],['integration']
Integrability,"> Oh, then you didn’t hear of type theory. It’s a branch of logic:. Indeed, I have never heard of that. But I doubt that it would be considered a branch of logic. 1. `Union type` is a pretty bad descriptor for a variable that can take _one_ of a set of fixed types. A union usually denotes a composition of multiple sets giving rise to a new set that contains all elements from these sets.; 2. `Subtype` is a great descriptor for a type that has properties of supertypes.; 3. `Intersection type` is an insanely bad descriptor for a variable that denotes the intersection of _properties_ of supertypes; the concept of such a subtype might be something useful in some languages and some cases and it might deserve a special name as it's the converse behavior of subclassing. But I have no idea how such a type would be useful in Python and in all cases that I've encountered. The [example on Wikipedia](https://en.wikipedia.org/wiki/Type_system#Intersection_types) already constructs a highly artificial case, whose relevance is opaque to me even though Scanpy features it in many instances: functions that overload parameters and have different overloading-dependent return types (standard example is passing an array instead of an AnnData, which triggers the automatic return of the computed annotation). What do you think about 3, @flying-sheep?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-443072359:1156,depend,dependent,1156,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-443072359,1,['depend'],['dependent']
Integrability,"> Or should we call it col_groups as you did in your sc.pl.heatmap pseudo code?. That could be up to you. It depends on what the user is trying to achieve, which makes more sense. For instance, I'm not sure if it makes sense to allow splitting the columns by both variables and groups, or if that's the wrong abstraction. > I'd be more than happy to make it more generalized, i.e., to sc.pl.heatmap, but I may need some time to understand sc.pl.heatmap first. The plotting functions are getting really complex- it took me some time to understand _dotplot and _baseplot :). This code could definitely be a lot more simple. Would definitely appreciate help here! I think some of the concepts used in `seaborn` could be quite useful here, though it looks like they're under heavy refactoring at the moment ([relevant seaborn branch](https://github.com/mwaskom/seaborn/tree/skunkworks/features)). Maybe a good first step would be to fix how so the dotplot would look right if the user provides the dot size and dot color dataframes? Would make these plots possible, and gives an interface to try later approaches with.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1876#issuecomment-988956524:109,depend,depends,109,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1876#issuecomment-988956524,4,"['depend', 'interface']","['depends', 'interface']"
Integrability,"> Removed 3.6. We should keep 3.6 as long as we support it. It's easy to accidentally add features which only work with 3.7+ otherwise. I'd be happy to drop 3.6 once numpy does (and in general roughly follow [NEP 29](https://numpy.org/neps/nep-0029-deprecation_policy.html) as soon as the ecosystem does). > is there any reason why we are currently not additionally using Github Actions?. Depends on the task. Also depends on the definition of github actions I think? We aren't using any of their runners for testing because we'd like the ability to integrate with hosted resources on azure. Also, azure seemed like much more of a standard for numeric python packages at the time we chose it. I'd be happy to have github actions for other things, like `precommit`. `twine check` could be another one, but I haven't looked in to how ""artifact"" type things are handled with github actions to know if we'd be able to recover the built objects. We'd talked about using codecov too, which I'd like to add a check for. I'm not totally clear on the distinction between checks and actions yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1602#issuecomment-763590019:389,Depend,Depends,389,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602#issuecomment-763590019,5,"['Depend', 'depend', 'integrat']","['Depends', 'depends', 'integrate']"
Integrability,"> Splitting off new modules; Finally, the idea of having IO functions go into their own package. I think this is a much bigger change, and I'd like to see a more fleshed out case for it. This would add a fair bit of complexity to development, so I'd want to be sure it's worth it. Some general questions I have:. > What are the advantages/ disadvantages of having smaller sub-packages?. method developer would just depend on those instead of (multiple) analysis package. > How does this impact users vs. developers?. user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. > Is IO special, or should more parts go into sub-packages?. it kind of is imho, it's all about having whatever data there is in an anndata/mudata shape. I must say that I'd also think plotting could be it's own separate package but it would probably require a lot of refactoring across packages (thinking about duplication of scanpy/scvelo code). > What gets re-exported from ""main"" modules?. didn't get this sorry. > Who manages the sub-packages?. the IO subpackage? everyone 😅 . so beside being in favour, it might also be that other issues arise. For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059346815:415,depend,depend,415,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059346815,1,['depend'],['depend']
Integrability,"> Thank you for this, @awnimo! I added a few small comments.; > ; > Could you move the whole code into `scanpy/external/_tools`, please? We'll transition to all wrapper code for external code to be in that directory. Thank you!. Hey @falexwolf ,; I have completed the changes you requestedI.; Please let me know if there are any other issues. Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/493#issuecomment-471725401:161,wrap,wrapper,161,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/493#issuecomment-471725401,1,['wrap'],['wrapper']
Integrability,"> Thanks for opening the issue.; > ; > It looks like a problem with pytables, which we are removing as a dependency since it's starting to have problems like this.; > ; > Are you able to update the installation of pytables? Otherwise, you could try a dev version of scanpy. Thank you for pointing out the issue with pytables. Tried a couple things and it works now.; I don't know how this matters. I uninstalled pytables > tried importing scanpy > doesn't work (says tables module not found, which is expected I guess). I reinstalled pytables - now it decides to work. I can't see how that makes a difference since I had the same pytables version before.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2138#issuecomment-1047851057:105,depend,dependency,105,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2138#issuecomment-1047851057,1,['depend'],['dependency']
Integrability,"> Thanks! So my understanding is that you are saying that neighbors function is ALREADY too complicated, so we should not complicate it any further (and rather the existing function could be eventually split by taking that gauss out of it, I guess?). Pretty much. I prefer more smaller, simpler functions with common APIs than fewer functions with larger APIs. > and rather the existing function could be eventually split by taking that gauss out of it, I guess?. I think I'd be pro that. I'd probably prefer exposing an interface for computing weights from KNN distances where methods like `gauss` could sit. > I think it's important that the following works and is actually the recommended way to run t-SNE within scanpy. (Using uniform affinities). Couple questions, first scientific: Why would you prefer uniform edge weights as input to your t-sne? I would think the information about relative distance is useful. Second API: I'm not sure I completely agree with this. I think it would be the most clear for `sc.pp.neighbors` to essentially mean ""build umap's connectivity graph"", and functions like `sc.tl.tsne` or `sc.tl.umap` to be ""find a 2d embedding using the passed connectivity graph"". This means whatever affinities you're passing through (e.g. via `connectivities_key`) are the weights that get used. Are there cases you think this disallows?. > One question here is maybe what should other downstream functions like Leiden clustering use, if somebody runs neighbors_tsne (or both neighbors and neighbors_tsne). The graph that's used is provided from arguments like `neighbors_key` or `obsp` from `leiden`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-759335128:521,interface,interface,521,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-759335128,2,['interface'],['interface']
Integrability,"> The Leiden algorithm is now [included](https://igraph.org/python/doc/igraph.Graph-class.html#community_leiden) in the latest release of `python-igraph`, version 0.8.0. I believe this alleviates the need to depend on the `leidenalg` packages. The Leiden algorithm provided in `python-igraph` is substantially faster than the `leidenalg` package. It is simpler though, providing fewer options, but I believe the more extensive options of the `leidenalg` package are not necessarily needed for the purposes of `scanpy`. We provide binary wheels on PyPI and binaries for conda are available from the conda-forge channel, also for Windows. I have now done a speed comparison with adata object of 1.85 million cells. igraph on adata as implemented [above](https://github.com/theislab/scanpy/issues/1053#issuecomment-1039424473) ran in **33 minutes** vs `sc.tl.leiden()` which took **~14 hours**",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1053#issuecomment-1039999011:208,depend,depend,208,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053#issuecomment-1039999011,2,['depend'],['depend']
Integrability,"> The `scipy.sparse` wrapper is actually interesting. I think it's tricky to add directly to SciPy, but it could be split out as a separate package that users could use and we could link to in the `scipy.sparse` docs. Any thoughts on where the `scipy.sparse` wrapper might live? It currently exposes just enough of the NumPy API for the purposes of ScanPy - i.e. it would need quite a bit more work to be more generally useful. Ditto for the `cupyx.scipy.sparse` wrapper.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/921#issuecomment-557464075:21,wrap,wrapper,21,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-557464075,3,['wrap'],['wrapper']
Integrability,"> The internals will very likely change over the coming months/year. Good to know, thanks! Any hints about what will change here? In particular, I'm wondering if there might be a `jax` implementation as I'm a bit more keen on that as a dependency. > But most embedding problems (including all problems specified using the preserve_neighbors function, which is the most commonly used recipe) have associated weighted graphs. I'd be interested in seeing how these graphs perform compared to the ones we get from UMAP. Would this be the right way to retrieve the graphs for the object, or is `distortions` not the right field?. ```python; from scipy import sparse. weights = mde.distortions().cpu().numpy(); edges = mde.edges.cpu().numpy(). graph = sparse.coo_matrix((weights, (edges[:, 0], edges[:, 1])), shape=(mde.n_items, mde.n_items)); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2154#issuecomment-1062106274:236,depend,dependency,236,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2154#issuecomment-1062106274,1,['depend'],['dependency']
Integrability,"> This agrees with what I suspected: that randomized PCA itself should be pretty stable, . No, if you look into how higher PCs vary, you see that they vary drastically depending on the seed or computational platform. That also makes sense, it's a power-method that does the computation, that runs into some unstable stuff. > PCs were similar to within a couple of decimal points,. I'm very sure that you only observed this for the first couple of PCs, which you robustly estimate. Going higher, you end up in some local minima for a subsapce; I believe that it doesn't mean it doesn't capture important variation; it just means that it's a local minimum that the algorithm converges into... something we observe all the time when training models... in the context of Lanzcos and other algorithms powering SVD, PCA etc., it's usually a nuisance that you have that instability when you go higher in iterations; but also there, people just use the results even if they know they don't get the *exact* 50th eigen vector...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/325#issuecomment-436041937:168,depend,depending,168,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325#issuecomment-436041937,1,['depend'],['depending']
Integrability,"> This is strange, i also tried to run the tests multiple times at the time of committing this and they failed every time. Maybe a dependency had a bugged release at the time?. > I am not sure what king of test. I don't want to add another save_and_compare_images test because plots seem to depend on the system at least sometimes. You could instead use `check_same_image`. Check that running `filter_rank_genes_group` then plotting is equivalent to manually passing those genes to `sc.pl.rank_genes_groups_*` plot on an object that hasn't had `filter_rank_genes_group` run on it. You can search the tests for examples of `check_same_image`. > (i have 3 failing plotting tests locally but they run fine here). Could you open an issue for this and note which tests they are? It would be good to make the tests as resilient as possible on other people's systems.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1942#issuecomment-878134649:131,depend,dependency,131,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1942#issuecomment-878134649,2,['depend'],"['depend', 'dependency']"
Integrability,"> This seems like pretty bad behavior for a development environment. We definitely don't want the dev install to be uninstalled when a new package gets downloaded. Well, scvelo depends on 1.7 and you have a release candidate of that one installed, so what happened is the only correct behavior: It uninstalled an incompatible version to install a compatible one. If your install’s metadata was outdated and it was in fact a compatible one, then you forgot to refresh the metadata by reinstalling it. That’s annoying but necessary as editable installs are nonstandard and therefore not well integrated into how package metadata works. > Why not just use `pip install -e` here?. Because development installs in general are nonstandard, and `pip install -e` in particular uses the deprecated `setup.py`. Tasks; -----. > - Exclude setup.py from sdist using the standard way, not via .gitignore. sounds good!. > I'm a bit concerned that pinning pip to an old version could lead to problems, especially while pip is going through a lot of changes.; > ...; > - flit mangles the build version part of wheel filenames, in a way that pip just started checking for. . No, as far as I can see, pip arbitrarily decided to not allow local version specifiers in wheel filenames. AFAIK nothing says there can’t be pluses in there, only that you can’t upload packages with local specifiers in their version to PyPI. Which we don’t do here, so pip should chill. If flit decides to work around that quirk, or pip relaxes, we can unpin pip. > - flit symlinked packages seem to be overwritten if a new package is installed which has the symlinked package as a dependency. Seee above. Has nothing to do with flit. What made you thing that anyway?. > - There is a fairly large workaround to make the package version available if the dependencies are not installed. Is it possible to use something more standard like versioneer here?. No. Either we hardcode a string constant in the `__init__.py` or we leave it like it is un",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1527#issuecomment-781992443:177,depend,depends,177,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-781992443,2,"['depend', 'integrat']","['depends', 'integrated']"
Integrability,"> This seems reasonable to me @flying-sheep . Does using the patched version change results over the unpatched @ashish615 i.e., for a given random seed, unpatched and patched are the same? If the two are the same for a given seed/state, then I think what @flying-sheep is proposing could be done separately (even if we make the dependency optional IMO). However, if the new version does change results, we will need the handling that @flying-sheep describes. @ilan-gold , I didn't check that. I will check that and let you guys know.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3061#issuecomment-2115403898:328,depend,dependency,328,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3061#issuecomment-2115403898,1,['depend'],['dependency']
Integrability,"> To me, italicizing the genes is more of a ""customizing plots for publication"" than ""looking at data"", so I think it's okay for it to have some extra steps as it happens less often. I personally have used illustrator for this kind of step. I disagree very much :) First of all I didn't say that these 10 dotplots are for ""looking at data,"" most of them are indeed for a paper (we can count the number of dotplots in sc papers, if we want to have a more accurate estimate of the time cost) and all of them need to be edited in a pdf editing software. Once the preprint is out, I can write in this post which dotplots I am talking about, how many there are in reality and how much time it took us to manually edit such minor things that can be easily addressed in the plotting code 😄 . What we are discussing is maybe a minor thing here, but if we can minimize the dependency on Illustrator (which is a pricey proprietary software with a highly unintuitive interface in my very subjective opinion) to make publication-ready figures, I think it's a HUGE plus for the community. I think this is related to the philosophy of scanpy. To sum it up in a broader context, I think enabling people to have high-quality, publication-ready figures without mastering matplotlib and/or Illustrator must be one of the top items of the `scanpy constitution` :). I know many colleagues who already nicely memorized the entire scanpy API but asking them also to know bits and pieces of a beast like mpl might be too much IMO. Based on your final suggestion, I can imagine myself trying to remember ""Was it `var_ticklabels_kwargs={""fontstyle"": ""italic""}` or `var_ticklabels_kwargs={""font_style"": ""italic""}` or `var_ticklabels_kwds={""fontstyle"": ""italic""}` or `ticklabels_var_kwargs={""fontstyle"": ""italic""}` etc. I even spend 45 seconds everyday to remember this damn thing here `plt.rcParams[""figure.figsize""]` :). > I don't really like that `set_figure_params` modifies plots not generated by scanpy. I totally understa",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1913#issuecomment-875885906:864,depend,dependency,864,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1913#issuecomment-875885906,4,"['depend', 'interface']","['dependency', 'interface']"
Integrability,"> Two options:; > ; > * bbknn < 1.5.0; > * use `bbknn.bbknn()` instead of the scanpy wrapper. Thank you for your rapid reply, bbknn 1.4 works for me (scanpy 1.7.2). The second option is good.; Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1873#issuecomment-872835230:85,wrap,wrapper,85,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1873#issuecomment-872835230,1,['wrap'],['wrapper']
Integrability,"> Unfortunately, I run into; > ; > ```; > __________________________________________________________________________________ test_scale[use_fastpp] ___________________________________________________________________________________; > ; > flavor = 'use_fastpp'; > ; > @pytest.mark.parametrize(""flavor"", [""default"", ""use_fastpp""]); > def test_scale(flavor):; > adata = pbmc68k_reduced(); > adata.X = adata.raw.X; > v = adata[:, 0 : adata.shape[1] // 2]; > # Should turn view to copy https://github.com/scverse/anndata/issues/171#issuecomment-508689965; > assert v.is_view; > with pytest.warns(Warning, match=""view""):; > > sc.pp.scale(v, flavor=flavor); > ; > scanpy/tests/test_preprocessing.py:127: ; > _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _; > ../../miniconda3/envs/scanpy/lib/python3.9/functools.py:888: in wrapper; > return dispatch(args[0].__class__)(*args, **kw); > scanpy/preprocessing/_simple.py:888: in scale_anndata; > X, adata.var[""mean""], adata.var[""std""] = do_scale(; > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args; > error_rewrite(e, 'typing'); > _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _; > ; > e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'); > issue_type = 'typing'; > ; > def error_rewrite(e, issue_type):; > """"""; > Rewrite and raise Exception `e` with help supplied based on the; > specified issue_type.; > """"""; > if config.SHOW_HELP:; > help_msg = errors.error_extras[issue_type]; > e.patch_message('\n'.join((str(e).rstrip(",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2457#issuecomment-1540006717:959,wrap,wrapper,959,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457#issuecomment-1540006717,1,['wrap'],['wrapper']
Integrability,"> We could wrap it in a function that checks the number of cells and only compiles this to faster code when necessary. So that's what this PR would replace. The reason I thought this could be replaced is that `numba` now allows on-disk cacheing of parallelized functions. This means that the function would only have to be compiled once per install. That cache only get's invalidated if function's source code get's modified, so this shouldn't cause too much pain for development testing times. I've added a note to the documentation mentioning this, so I think it's fine.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/844#issuecomment-534371715:11,wrap,wrap,11,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/844#issuecomment-534371715,1,['wrap'],['wrap']
Integrability,"> Well, as that issue says, it’s fixed in [lmcinnes/umap#261](https://github.com/lmcinnes/umap/pull/261), which means it’s in umap 0.3.10. @flying-sheep unfortunately `umap==3.10` does not fix this relative to the latest scanpy version on Bioconda (`1.4.4.post1`). The issue is that the UMAP fix does not address the branch of code that scanpy depends on (specifically the call follows [this branch in the UMAP code](https://github.com/lmcinnes/umap/blob/41205248fb48391d1f6e4effcb974307b7c229ce/umap/umap_.py#L1059)), which still just passes the `init_coords` in as is. . Of course, there has been [a workaround in scanpy since 1.4.5.post1](https://github.com/theislab/scanpy/commit/1400d1e35f908d6f5ab8a8681970ac4aba673565). However, I would caution against the advice in that commit's message, which assumed that once `umap==3.10` was released the workaround could be removed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/948#issuecomment-595355427:344,depend,depends,344,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/948#issuecomment-595355427,2,"['depend', 'message']","['depends', 'message']"
Integrability,"> Well, scvelo depends on 1.7 and you have a release candidate. `scvelo` depends on `scanpy>=1.5`, so I don't think this is the cause. Do you know why this is happening, and can you provide counter examples where it doesn't happen?. > If your install’s metadata was outdated and it was in fact a compatible one, then you forgot to refresh the metadata by reinstalling it. . I'm not sure what you mean by this. Does `flit install -s --deps=develop` not count as reinstalling? Are you counting `flit install -s` as a development install?. > Because development installs in general are nonstandard, and pip install -e in particular uses the deprecated `setup.py`. I think pinning pip to an old version is worse than using a common, even if non-standard, installation method. > No, as far as I can see, pip arbitrarily decided to not allow local version specifiers in wheel filenames. My reading of the PR in `flit` and the subsequent discussion in the `pip` and PEP threads suggests to me that the issue is `pip` validating the metadata name against the wheel, while a spec exists saying the wheel can't contain characters that are allowed by version specs. If anything, `pip` suddenly started expecting exact version specifiers in wheel filenames, while a spec exists that says how the filenames should be mangled. `flit` did the mangling, and `pip` now says that's wrong. It looks like the direction the discussion is headed is PEP 427 is wrong, and `pip` is right. I have no idea what sort of timeframe should be expected here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1527#issuecomment-782812159:15,depend,depends,15,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-782812159,2,['depend'],['depends']
Integrability,"> What do you define as logic here?. [Logic](https://en.wikipedia.org/wiki/Logic) as the mother of all formal reasoning and its close relative set theory in mathematics. When you say type theory is a branch of logic then 90% of computer science is a branch of logic. In many contexts this might be a valid but not a very useful statement. > I’d also like `OneOf[A, B]`. I love `OneOf[A, B]`. This also doesn't pretend to be logic or set stuff. > `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. This is what I meant when I said _intersection of properties of supertypes_. But I still don't know when you'd need such a type in a practical context, given that we just keep overloading functions like crazy and simply treat passed arguments dependent on their type. Any example when intersection types are actually useful? In a function we might see in Scanpy (this was the whole beginning of this discussion; I cannot imagine a case in which we need to label something _intersection type_ in the docs).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-443397973:776,depend,dependent,776,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-443397973,2,['depend'],['dependent']
Integrability,"> When I run [`conda install python=3.11` and `conda install -c conda-forge scanpy`] I get an error . Yeah no idea how to debug conda conflicts. I’ve often seen things like this: completely unrelated packages “conflicting” containing cryptic symbols like `feature:|@/osx-64::__osx==10.16=0`. No clue what that means. Conda seems to be unable to figure out which user-specified versions are in conflict with each other. Pip seems to do a better job these days:. > Running this with `pip -vv install scanpy` as you suggested indeed gives an error with numba. Exactly, so this is a numba issue. Please follow https://github.com/numba/numba/issues/8304. Once numba supports Python 3.11, you’ll be able to install any dependent project there (including scanpy)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369#issuecomment-1333418043:713,depend,dependent,713,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369#issuecomment-1333418043,1,['depend'],['dependent']
Integrability,"> Yeah, I can't reproduce it with a canned dataset either --- I'm doing something a bit weird and transforming imaging mass cytometry data into AnnData objects (hence the imctools dependency). I have an object that looks like:. thank you for reporting, this is very interesting use case! and thanks for the detailed evaluation. I would also try with different number of PCs to see whether that has an impact. if you open an issue on `pynndescent`, would you mind referencing this issue or pinging me there, would be interested to see what's the proposed solution/bug. @TiongSun let us know about your use case, thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1696#issuecomment-797652809:180,depend,dependency,180,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696#issuecomment-797652809,1,['depend'],['dependency']
Integrability,"> You said you used some automated tools to get faster compliance. What were these? In general, I would prefer to have a formatter that automatically ran than a tool that told me I formatted something wrong. https://pypi.org/project/autopep8/. Someone suggested to use it for pre-commit anyways. However, there is no full flake8 autofixer. > As a general point about this PR: to me, the fair amount of the work of turning on flake8 deciding on the rules. Perhaps we should start with a subset of files then? . Possible, but this will be an iterative process that will take a long time. If you are willing to do that I can close this PR and enable flake8 for example on the first folder of Scanpy. I cannot however fix all of them manually in like 20 PRs or something. > I'm pretty strongly against this. noqas just look like the formatter/ linter was wrong, and I'm not accepting that having no plan to address bugs. I think this should be a discussion with a broader set of the team. Depends on the view that you have. I see them more as TODOs for later since the ""bad code"" is in master at the moment anyways. But if you want to discuss that - sure.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1689#issuecomment-785875783:985,Depend,Depends,985,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689#issuecomment-785875783,1,['Depend'],['Depends']
Integrability,"> [39](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=38) def rdist(x, y):; [40](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=39) """"""Reduced Euclidean distance.; [41](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=40) ; [42](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=41) Parameters; (...); [49](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=48) The squared euclidean distance between x and y; [50](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=49) """"""; [51](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=50) result = 0.0. File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\decorators.py:219, in _jit.<locals>.wrapper(func); [217](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/decorators.py?line=216) with typeinfer.register_dispatcher(disp):; [218](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/decorators.py?line=217) for sig in sigs:; --> [219](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/decorators.py?line=218) disp.compile(sig); [220](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/decorators.py?line=219) disp.disable_compile(); [221](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/decorators.py?line=220) return disp. File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\dispatcher.py:965, in Dispatcher.compile(self, sig); [963](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/dispatcher.py?line=962) with ev.trigger_event(""numba:compile"", data=ev_details):; [964](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659:13278,wrap,wrapper,13278,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659,2,['wrap'],['wrapper']
Integrability,"> ```python; > scipy.io.mmwrite; > ```. This code doesn't actually work - rows and columns are switched in the matrix, and it produces an error when you try to read in the output using either `Scanpy` or `Seurat` wrapper functions. Perhaps it's a package version thing though..",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/262#issuecomment-1476035869:213,wrap,wrapper,213,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262#issuecomment-1476035869,1,['wrap'],['wrapper']
Integrability,"> `paul15` is downloaded automatically, very practical. Yeah, it’s really cool for interactive use, but not for automated testing / continuous integration I think.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/80#issuecomment-364372580:143,integrat,integration,143,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/80#issuecomment-364372580,1,['integrat'],['integration']
Integrability,"> can we just make finding out about this easier for users?. That’s what I feel like most of our plot options are. Kinda like an executable FAQ. Python’s `itertools` docs have a [recipes](https://docs.python.org/3/library/itertools.html#itertools-recipes) section instead, which are copyable code for less common use cases. I think going for docs here make sense because we’re not wrapping something complex, just increasing visibility for some useful option. Maybe we can add a table that shows what some of our plot options do with `rcParams` behind the scenes (and also some `rcParams` like this one which aren’t covered by our plot options)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1720#issuecomment-792304104:381,wrap,wrapping,381,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1720#issuecomment-792304104,1,['wrap'],['wrapping']
Integrability,"> it's annoy. Sounds like annoy is being … annoying :smile:. > In future, could you not force push while responding to review?. Okay! Hmm, generally IDK what the best approach is since I now know how I want to rebase the commits but I’ll probably forget later … Maybe indicate in the message which commit they “fixup”?. Also: can we reenable squash/rebase merges soon?. > Doc builds failures do seem related to this, however. The docs failure is ugly to fix, but I did it …. Since very shortly ago, (pypa/pip#9320) pip validates wheels and for some reason decided that pluses in wheel filenames are not valid (I couldn’t find that in any spec). I hope takluyver/flit#388 gets merged soon to circumvent/fix that. If we want to temporarily circumvent that we’d have to tell readthedocs to use pip 20.3.3 version (like I did in the pipelines yaml). And that’s ugly because we’d have to add a requirements file that contains just `pip==20.3.3`, since readthedocs doesn’t allow to specify a pip version or a literal list of requirements.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1527#issuecomment-777397179:284,message,message,284,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-777397179,1,['message'],['message']
Integrability,"> like pip install .[dev,test$(test_extras))], and run things once with test_extras='' and once with test_extras=',leiden,magic,harmony,scrublet,scanorama,skmisc'. Yeah, I was thinking something like this. Except we could just reduce `test` to include the barebones needed to make tests run, and separately have optional dependencies. The hard part here is structuring the tests so they can run without optional dependencies being present. We'd need to establish patterns for optional dependencies in fixtures and parameterized tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2211#issuecomment-1088721539:321,depend,dependencies,321,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2211#issuecomment-1088721539,3,['depend'],['dependencies']
Integrability,"> scvelo depends on scanpy>=1.5, so I don't think this is the cause. Do you know why this is happening, and can you provide counter examples where it doesn't happen?. Ah weird. Pip tries to resolve the dependencies, and for that purpose gets all the installed packages’ metadata, then tries to figure out a configuration of upgrades that makes things work. No idea why it sees “1.7.0rc2” and decides “I’ll update this even when not asked to update”. Maybe raise this issue with pip?. > I'm not sure what you mean by this. Does flit install -s --deps=develop not count as reinstalling? Are you counting flit install -s as a development install?. Yes, that’s a reinstall in some development mode. My point was that if a scanpy pre-1.7 version really was installed, maybe pip was correct to update to 1.7 for some reason. However since we’re past 1.7 now, unless you haven’t git-pulled yet, I assume your dev install’s metadata has gone stale. I guess pip wouldn’t uninstall your dev install if you had run `git pull && flit install -s`, therefore updating the metadata. But I could be wrong, as I have no idea why pip thought it necessary to touch scanpy when installing scvelo. > I think pinning pip to an old version is worse than using a common, even if non-standard, installation method. Our setup.py is a compatibility shim solely for fallback use, not something to be relied upon in any part of our process. Usually when something does an arbitrary change making our life harder, our approach is pinning it temporarily until it fixed that or the infrastructure has adapted to its whims, right?. > It looks like the direction the discussion is headed is PEP 427 is wrong, and pip is right. Accepted PEPs are specs, so only pip and flit can be right or wrong (as they implement it). If people decide that what pip does happens to be *better* than the currently spec-compliant behavior, the spec can be changed accordingly. Until then pip is wrong, so we should pin its version to one that accepts sp",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1527#issuecomment-783309298:9,depend,depends,9,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-783309298,2,['depend'],"['dependencies', 'depends']"
Integrability,"> some of the members of the nth ring have more than one neighbor in the nth-1. Damn... yeah, that won't work then. Back to differences of adjacency matrices then i guess. It looks as though your functions are replicating what I assume is going on in the backend of `networkx` anyway. Are you trying to avoid the heavy dependency or why replicate the effort? I would assume that once we need things like label propagation (maybe to denoise label assignment after deconvolving spatial spots?) then `networkx` might come in handy, no?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1383#issuecomment-705480776:319,depend,dependency,319,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383#issuecomment-705480776,1,['depend'],['dependency']
Integrability,"> sometimes we have math expressions like var = mean * mean^2 etc. in the docs. Is there a convention for scanpy docs if those should be in code format or just plain text?. I'm not sure, I think with math is nicer but not aware of any convention. @ivirshup ?. > I think the .._pca function is missing from the release note. should I add it there?; The ..pca function also did not use shared docs params yet. I started adding them and can commit tomorrow - is that okay if I just do it like that?. must say I missed those sorry, feel free to add and I'll take a look again tomorrow and wrap it up.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-1065373944:585,wrap,wrap,585,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1065373944,1,['wrap'],['wrap']
Integrability,"> the main hpc I'm on 1gb of space where appdirs would put these files. That's a misconfigured server, not a normal case. We should use appdirs as default, catch a IOError on write, and send a nice message like. > Error: Cannot write to your cache directory. Please make sure there's space in {cache_dir!r} or override the cache directory by setting one of the $SCANPY_CACHE_DIR or $XDG_CACHE_DIR environment variables. All linux-based systems should set $XDG_CACHE_DIR if there's a better place than ~/.cache for such files.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-476675808:198,message,message,198,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-476675808,1,['message'],['message']
Integrability,"> there is still a large amount of changes to the dataframe code here. Not really changes: it’s almost all refactoring, because the code was spaghetti with quite some duplication. I’m doing nothing more than. 1. I introduce helper functions so code gets more readable, e.g. a clean `disp_cut_off = _nth_highest(dispersion_norm, n_top_genes)` instead of a large inline code block that has to be decyphered line by line to figure out that it finds the nth highest value. This is especially necessary for the huge main pile of spaghetti that used to be the `if flavor == ""seurat"":`/`elif flavor == ""cell_ranger"":` branches. I simply put their contents into a `_get_mean_bins` helper and two helpers `_stats_seurat` and `_stats_cell_ranger` (while deduplicating shared code); 2. Making sure pandas indices match up while removing `.to_numpy()`. That way instead of having `.to_numpy()` potentially copy and and convert data in extension arrays, the series are just used directly. Not to mention that three `.to_numpy()` per line make things hard to read.; 3. refactor the 5 cutoff parameters into one value `cutoff` for clarity, less inline code, and better type information (we either have either `n_top_genes: int` or a `_Cutoffs` instance, never both. This way, the type system knows). and that’s it. <ins>potentially</ins> faster, much more maintainable, and almost dask-compatible. After my changes, it should be easier to further refactor things so the seurat_v3 flavor is integrated into the overall structure instead of doing its own thing.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2809#issuecomment-1929321140:1475,integrat,integrated,1475,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2809#issuecomment-1929321140,2,['integrat'],['integrated']
Integrability,>Could epiScanpy be used as a multi-modal analysis tool ? @falexwolf. I think this is a question that is best asked in the episcanpy forum:; https://github.com/colomemaria/epiScanpy/issues. They have used it for multiple epigenomics modalities. Not sure if integrated though.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/479#issuecomment-510453095:257,integrat,integrated,257,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/479#issuecomment-510453095,1,['integrat'],['integrated']
Integrability,"@Brycealong could you also try this in a new isolated environment, please? There might be some dependency that's interfering. Would be glad to know which one, but it's tricky...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2359#issuecomment-1291812216:95,depend,dependency,95,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2359#issuecomment-1291812216,1,['depend'],['dependency']
Integrability,"@Khalid-Usman Of course you can use silhouette coefficient for any kind of clustering in principal. you just need to choose the ""best option"" depending on your dataset which is again depending on what you are interested in and then you can validate it by looking into your clusters markers. I am actually very curious to see the T-cells case that was mentioned here....you can also have a look here: https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set; Again I would like to mention using such control parameters are mathematical methods to assess your clustering quality it might has nothing to do with the biological side and they can be actually helpful when you have no clue over the number of clusters you would look for so you would reply only on mathematics ! your question is really topic specific that what you look for and in which case you want to assess your data...if you already have an estimation or no.......there are also other ways to look for the quality of the data as grst mentioned but again it really depends on what you look into.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/670#issuecomment-498181103:142,depend,depending,142,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670#issuecomment-498181103,3,['depend'],"['depending', 'depends']"
Integrability,"@Koncopd has looked at refactoring the `rank_genes_groups` methods, but in the big picture we don't really love the output format that `rank_genes_groups` uses. Maybe an easier path forward would be to be able to directly pass values into the various plotting functions? You can already generate mostly similar plots from `sc.pl.rank_genes_groups_{plot_func}` and `sc.pl.{plot_func}` apart from using logfc and pvalues. If we allowed passing those in, it would be simple enough to make the same plots/ add a wrapper that generates the plots into `diffxpy`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1955#issuecomment-886408954:508,wrap,wrapper,508,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1955#issuecomment-886408954,2,['wrap'],['wrapper']
Integrability,@Koncopd it is the # of celltypes per each cohort or the relative_frequencies per each group:; ![image](https://user-images.githubusercontent.com/23288387/119840907-5ea23e00-bed3-11eb-9738-17b267889bb5.png). is it something researchers looking for? or do you think this not good approach as cells depends on how many cells per sample,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1845#issuecomment-849666917:297,depend,depends,297,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1845#issuecomment-849666917,1,['depend'],['depends']
Integrability,"@Koncopd this looks correct to me. For `compute_connectivities_umap`, it could be left in `scanpy/scanpy/neighbors/__init__.py` until UMAP's `fuzzy_simplicial_set` returns distances too. (In other words, the change to `fuzzy_simplicial_set` doesn't have to block depending on UMAP.)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/522#issuecomment-477199838:263,depend,depending,263,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/522#issuecomment-477199838,1,['depend'],['depending']
Integrability,"@Koncopd, could we have a meeting or VC on the progress here? Let's discuss via email. 🙂 . @tomwhite, any updates from @lmcinnes regarding integrating the distributed version of `pynndescent` into UMAP itself? I'm not 100% convinced to mingle with private functions within UMAP that might eventually break (another reason for why I copied over code from UMAP back in January/February 2018). It would be nice to have Leland's OK for adding distributed aspects to the package. Scanpy would then just make use of them... @tomwhite, by this, also many other single-cell packages (many of them use UMAP these days) would profit from the new distributed computing capabilities.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/522#issuecomment-476592722:139,integrat,integrating,139,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/522#issuecomment-476592722,1,['integrat'],['integrating']
Integrability,"@LuckyMD Dear Malter, Thanks, now I got your point. I just filtered my cells and genes again using ; `sc.pp.filter_cells(adata, min_genes=200); sc.pp.filter_genes(adata, min_cells=3)`; but I still get the same warning message",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/653#issuecomment-494332488:218,message,message,218,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653#issuecomment-494332488,1,['message'],['message']
Integrability,@LuckyMD I checked the commits. Between the Scanpy version on agando and the latest release the `marker_genes_overlap` was not changed. But maybe I am blind.; I'll go for the empirical route and try it out. Will report back!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1625#issuecomment-772439376:185,rout,route,185,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625#issuecomment-772439376,1,['rout'],['route']
Integrability,"@LuckyMD Mine was a general answer, I agree that at this scale it may not be an issue but it may indeed be for larger atlases. @mxposed about your first question: cell distances depend on the HVG in absolute terms, but the overall structure of your data is more ""relative"". If the kNN graph topology is overall conserved you'll end up with similar populations.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1531#issuecomment-738828346:178,depend,depend,178,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1531#issuecomment-738828346,1,['depend'],['depend']
Integrability,"@LuckyMD The thing is after imputation for sure I do get some negative values and I have observed it but such warning was not popping up before and NaN I am doubtful because otherwise I could see a warning message when I ran the imputation for all of my genes. . p.s. This is how I made my subset. > adata_magic_sub=adata_magic[(adata_magic.obs.louvain_04==""3"")|(adata_magic.obs.louvain_04==""7"")|(adata_magic.obs.louvain_04==""8"")|(adata_magic.obs.louvain_04==""11"")]",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/653#issuecomment-494342325:206,message,message,206,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653#issuecomment-494342325,1,['message'],['message']
Integrability,"@LuckyMD `ingest.to_adata` just returns a copy of `adata_new` (or the same object depending on inplace) with all mapped representations (pca, umap) and labels.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/651#issuecomment-515764313:82,depend,depending,82,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651#issuecomment-515764313,1,['depend'],['depending']
Integrability,"@LuckyMD, I definitely agree. . I personally find it kinda hard to work with version management and updating of notebooks, as well as keeping them clean with useful prose. This makes updating tutorials a pain. A lot of this just has to do with the interface, as I find this much easier with `.Rmd`. . I'm thinking this could be alleviated a bit with better automation around tutorials. Namely:. * Running + rendering notebooks through CI; * Save tutorials in a more git friendly format, maybe through something like [jupytext](https://jupytext.readthedocs.io). It looks to me like @michalk8 has set up some more extensive CI for tutorials with notebooks. @michalk8 do you have any recommendations here? How are you finding running CI against notebooks?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1357#issuecomment-669097668:248,interface,interface,248,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1357#issuecomment-669097668,1,['interface'],['interface']
Integrability,"@LuckyMD, I think you can get the docker environment travis uses. * [Docker image for travis python env](https://hub.docker.com/r/travisci/ci-python); * [Guide on running it](https://andy-carter.com/blog/setting-up-travis-locally-with-docker-to-test-continuous-integration). I did this a couple years ago, but I know travis has changed a bunch since then. Another good first step would be to figure out if it only fails on the first build, and if caches are being used in any way. Also, do the builds ever fail for forks? I don't think they've been failing [for me](https://travis-ci.org/ivirshup/scanpy/builds).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/580#issuecomment-478823933:261,integrat,integration,261,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580#issuecomment-478823933,1,['integrat'],['integration']
Integrability,"@Mr-Milk I see. I think right now, it's probably not going to go into scanpy pre 2.0.0 (as marked by the milestone). @flying-sheep is leading the new plotting effort so I'll step back. But roughly from my perspective, 1. seems useful perhaps for some internal stuff but 2. seems perhaps a point against integrating directly since; > this design doesn't make much difference compared to directly using Marsilea. Maybe there's some middle ground? Some definte API for integration into 2.0.0 that wouldn't require full integration but could give this sort of `return` functionality?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2444#issuecomment-2371719939:303,integrat,integrating,303,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444#issuecomment-2371719939,3,['integrat'],"['integrating', 'integration']"
Integrability,"@Zethson I believe that's an upstream issue. Looks like the docs broke when `sphinx-autodoc-typehints` bumped versions from `1.12.0` to `1.13.0`. I can build the docs locally from `master` and from this branch with `sphinx-autodoc-typehints` v1.12, but not v1.13. (You'll also see an identical error in #2099, despite that just being a dependency bump for pre-commit.). I'll submit a PR to pin `sphinx-autodoc-typehints` to version 1.12.0 shortly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1828#issuecomment-1005072811:336,depend,dependency,336,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1828#issuecomment-1005072811,1,['depend'],['dependency']
Integrability,"@Zethson thank you for the consideration and explanation. I am not sure Mellon would pass the criteria since it does not depend on or explicitly use AnnData although we do recommend using AnnData: https://mellon.readthedocs.io/en/latest/notebooks/basic_tutorial.html; Additionally, it relies on [Palantir](https://github.com/dpeerlab/Palantir) which does also not qualify since it does not have a CI yet.; Do you think we should try making a PR to https://github.com/scverse/ecosystem-packages regardless?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2577#issuecomment-1656331448:121,depend,depend,121,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2577#issuecomment-1656331448,1,['depend'],['depend']
Integrability,"@Zethson thanks 💯 ! looks really good! Tbh I was expecting much worse, the changes with flake8 are pretty conservative imho. > Depends on the view that you have. I see them more as TODOs for later since the ""bad code"" is in master at the moment anyways. But if you want to discuss that - sure. I totally share this view, I think it's fine to use noqa as flag and come back later. I admit you guys are 1000x more experts in this, but from what I can judge I think it's fine to merge this as first attempt and then in subsequent PRs improve and address flake 8 fails. . Maybe worth to merge #1527 first? otherwise there will be potential conflicts more for phil.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1689#issuecomment-786522462:127,Depend,Depends,127,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689#issuecomment-786522462,1,['Depend'],['Depends']
Integrability,@adamgayoso I don't think it fits under the other preprocessing tool headings of Data integration or Imputation. Maybe add a new one called Call hashing or Sample demultiplexing. @fidelram thoughts? Not sure who else to tag,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1483#issuecomment-722042260:86,integrat,integration,86,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1483#issuecomment-722042260,1,['integrat'],['integration']
Integrability,"@aditisk that depends on what you put in `adata.raw` ;). Initially `adata.raw` was used to store the full gene object when `adata.X` was filtered to only include HVGs or remove genes that aren't expressed in enough cells. Now, we just have a boolean mask in `adata.var['highly_variable']` for HVGs and so it's often not used anymore. I typically store my log-normalized expression data there if I do batch correction or regress anything out, as `adata.raw` is used as default to compute `rank_genes_groups` and to show expression values on an embedding plot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1039#issuecomment-617882284:14,depend,depends,14,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1039#issuecomment-617882284,1,['depend'],['depends']
Integrability,@brianpenghe Hi may I consult how you resolved the problem?. The comment says upgrade anndata to 0.8.0 but mine already is 0.8.0 and the error message remains.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2297#issuecomment-1447928394:143,message,message,143,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2297#issuecomment-1447928394,1,['message'],['message']
Integrability,"@cartal @SamueleSoraggi ; For some reason I decided to integrate Scrublet using Scanpy's functions where possible, rather than making a simple wrapper. The core functionality is up and running in [this fork](https://github.com/swolock/scanpy), and now I just need to add documentation, make some of the code more Scanpythonic(?), and add an example.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/173#issuecomment-492900457:55,integrat,integrate,55,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-492900457,4,"['integrat', 'wrap']","['integrate', 'wrapper']"
Integrability,"@cdpolt, is there are specific change (""new behavior"") you're referring to?. > Storing things in layers sequentially, I just end up with a bunch of layers that all are identically fully processed . Would the code in the previous message be helpful to understand why that happens and how to fix that?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2261#issuecomment-2071131186:229,message,message,229,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2261#issuecomment-2071131186,1,['message'],['message']
Integrability,"@dkobak @ivirshup @Koncopd This is a first stab #1233. Features. - [X] Construct t-SNE embeddings; - [ ] Recipes; - [ ] Ingest functionality. As discussed, this PR currently implements t-SNE with uniform affinity kernels, making it fit in nicely with the existing `sc.pp.neighbors` architecture. While this isn't technically t-SNE per se, it's visually almost impossible to tell them apart. It would also make sense to add a `tsne` option to `sc.pp.neighbors`, but from what I can tell, there's no direct way to change the existing code to do this. It looks like `sc.pp.neighbors` calls UMAP to calculate the nearest neighbors directly, calculating the UMAP weights. We'd probably have to do something similar to the `gauss` option and just overwrite the UMAP weights after the fact. Does this sound reasonable?. I like the API of calling `sc.tl.tsne.recipe_X(adata)`. Adding the recipes would be simple here; we can just add a simple class wrapper around `_tsne` which which would `__call__` tsne, and a bunch of static methods to the wrapper for recipes. This is kind-of messy and probably not something you guys do anywhere else throughout the code base, so I'd appreciate your feedback on this. Do you like this, or should we do it in a different way?. The `ingest` functionality should be fairly straightforward as well, just adding a `tsne` option to `embedding_method`. All of these things should be pretty easy to do, but I'd like to see that this is moving in a direction you like first.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561:941,wrap,wrapper,941,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561,2,['wrap'],['wrapper']
Integrability,@falexwolf ; I'm adding a wrapper for Palantir by [Setty et al. (2018)](https://doi.org/10.1101/385328); Please let me know if you have any comments,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/493:26,wrap,wrapper,26,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/493,1,['wrap'],['wrapper']
Integrability,"@falexwolf ; Is it caused by the parametter svd_solver ? Acorrding to the tl.pca() document ( svd_solver : str, optional (default: ‘auto’)SVD solver to use. Either ‘arpack’ for the ARPACK wrapper in SciPy (scipy.sparse.linalg.svds), or ‘randomized’ for the randomized algorithm due to Halko (2009). “auto” chooses automatically depending on the size of the problem.; The 'randomized' algorithm will give different result each time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/203#issuecomment-405780122:188,wrap,wrapper,188,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/203#issuecomment-405780122,2,"['depend', 'wrap']","['depending', 'wrapper']"
Integrability,"@falexwolf @willtownes @LuckyMD Valentine Svensson suggests that zero inflation does not exist in droplet protocols, but that log-transforming data could be responsible for the apparent zero inflation. Further, the high number of zeros can be accurately modeled with a non-zero-inflated model: https://www.nature.com/articles/s41587-019-0379-5. Since GLM-PCA doesn’t model zero inflation, it’s probably a really good base for distance calculations in scanpy in cases where its performance is sufficient. [From the paper](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6):. > The multinomial model adequately describes negative control data, and there is no need to model zero inflation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/868#issuecomment-592476723:106,protocol,protocols,106,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868#issuecomment-592476723,1,['protocol'],['protocols']
Integrability,"@falexwolf I agree with you about the `diffxpy` a `scanpy` dependencies, Tensorflow is a very important dependency!. > would you make a PR?. I did it, I pushed the code where I added the parameter `n_components` for `scanpy.tl.tsne` function. > Why not using `diffxpy` Volcano plots right away?. I wrote a function in which you can change the colour of the genes, you can add the names of the genes etc. > How did you write your tests?. I tried them on data coming from the lab in which I am working.; I can write a jupyter notebook using public dataset and push it on my copy of the `scanpy` repository.; Give me a couple of days.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/460#issuecomment-471324466:59,depend,dependencies,59,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460#issuecomment-471324466,2,['depend'],"['dependencies', 'dependency']"
Integrability,"@falexwolf I think it is worth to have a `scanpy.plugin` or `scanpy.extension` or something shorter like `scanpy.pg` or `scanpy.ext` that aggregates all plugins. First, this clarifies for the user that the tool he/she is using is not directly developed by scanpy. Second, this allows plugins to be installed separately without having to update scanpy's code. The idea is that scanpy will be able to discover any plugins installed. On the side of developers, this could facilitate integration with scanpy. We can get inspired by flask extensions: http://flask.pocoo.org/docs/1.0/extensiondev/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/265#issuecomment-423915827:480,integrat,integration,480,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-423915827,1,['integrat'],['integration']
Integrability,@falexwolf thanks very much for the extra information! I've copied the t-SNE API so it should fit much more cleanly into the scanpy interface. Let me know if there's anything else you need me to change. Thanks again!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/136#issuecomment-386075622:132,interface,interface,132,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/136#issuecomment-386075622,1,['interface'],['interface']
Integrability,"@falexwolf, realized I didn't change the AnnData dependency. I'm not totally sure what to do with that, since we've already got a requirement on 0.6.22 due to scipy and statsmodels.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/730#issuecomment-511276816:49,depend,dependency,49,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730#issuecomment-511276816,1,['depend'],['dependency']
Integrability,@falexwolf; I'm adding a wrapper for Harmony by [Setty et al. (2018)](https://doi.org/10.1101/471078); Please let me know if you have any comments,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/503:25,wrap,wrapper,25,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/503,1,['wrap'],['wrapper']
Integrability,"@fidelram . As noted in #1632, dotplot labels can often extend outside of the plotted area. Whether the full labels can be seen is dependent on how the plots are being output. It would be great if this always worked. <details>; <summary> Example from the docs: </summary>. ![](https://user-images.githubusercontent.com/8238804/107312688-122e2080-6ae5-11eb-8a7e-f61c51a8392c.png). </details>. Could possibly be solved by using `matplotlib`'s `constrained_layout` or `tight_layout`. I think these would require modifying how the grid spec and axes are created.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1705:131,depend,dependent,131,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1705,1,['depend'],['dependent']
Integrability,"@fidelram I guess you are the right person to ask for help with this... I'm struggling to work nicely with `plot_scatter()`. I am trying to generate a plot where density values for non-selected conditions are grey, while density values for the selected condition are on 'YlOrRd' or another color map. It seems this is not ideal with a single `plot_scatter()` call (which I was hoping to use as the facet wrapping is already done there). For the grey values I am using a color value of -1, while the others are between 0 and 1. However, when I define a color map that is symmetric around 0, positive values near 0 are mapped to grey instead of colours... any idea why?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/543#issuecomment-474979855:404,wrap,wrapping,404,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/543#issuecomment-474979855,1,['wrap'],['wrapping']
Integrability,"@fidelram It might be that the new plotting backend doesn't support the ""additional colors"" ([here](https://github.com/theislab/scanpy/blob/7c9fb1a5f2293956adda0673d47e7dec1b32ddfb/scanpy/plotting/utils.py#L166-L186)) anymore. These are colors that are standard in R and used for the Planaria example. We should try to integrate them for the sake of easily moving between python and R.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/286#issuecomment-427909754:319,integrat,integrate,319,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286#issuecomment-427909754,1,['integrat'],['integrate']
Integrability,"@fidelram Yes, makes sense. Let's see whether we manage to organize it this way. There will be a few plugins coming soon and I'll talk with the one doing it about this. @wangjiawen2013 The Seurat developers did a bit more than simply fitting a standard CCA. So I'd assume that it'd be some work to wrap sklearn's CCA or pyrcca so that it performs similar to Seurat's CCA on single cell data...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/265#issuecomment-424548158:298,wrap,wrap,298,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-424548158,2,['wrap'],['wrap']
Integrability,"@fidelram, I really like your plotting gallery! Would be cool to have that as part of the tutorials or even integrated in the main documentation (enhance each plotting function with an example image?)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/369#issuecomment-441069177:108,integrat,integrated,108,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369#issuecomment-441069177,1,['integrat'],['integrated']
Integrability,@flying-sheep ; Thank you so much for your reply!; Indeed quite a lot of packages are different between the two environments. ; I'm sorry for making this complicated. . The env on my desktop (where the `scrublet` function stopped) is actually newer and at first I thought that would not create huge problems (I recently switched to mamba instead of conda on my Intel-core desktop. ; I didn't use the yml from my M2-chip laptop to re-create the environment because of some dependency problems between the Intel/M2 computers).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3116#issuecomment-2187966638:472,depend,dependency,472,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3116#issuecomment-2187966638,1,['depend'],['dependency']
Integrability,@flying-sheep @gokceneraslan great! I agree it's hard to compare these algorithms as the performance of an imputation strategy often depends on the downstream use case. I'm looking forward to checking out the countae preprint. I find the [scVI](https://github.com/YosefLab/scVI) benchmark of imputation methods to be useful for now.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/45#issuecomment-367680111:133,depend,depends,133,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/45#issuecomment-367680111,1,['depend'],['depends']
Integrability,@flying-sheep I completely agree that it may not be the best naming convention. That was what it was used for at the time though. @grst The idea of using gene symbols as `.var_names` in scanpy was to make the software as user friendly as possible as far as I'm aware. Typically people care about the gene-level as the highest resolution. This is probably also due to the dominance of 3` enrichment protocols making it hard to detect signals at a higher resolution.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/376#issuecomment-441077219:398,protocol,protocols,398,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376#issuecomment-441077219,1,['protocol'],['protocols']
Integrability,"@flying-sheep To answer your question. Honestly, I am not so familiar with Plotnine, Plotly or Altair. However, after a quick revision I would say that Altair seems quite interesting and possibly were I could had reused/extended some code. Yet, at the moment in scanpy we use matplotlib extensively and I didn't even think about the other APIs. Looking closely at Altair I realized that I have a lot to catch up regarding Vega, Vega-lite and the idiosyncrasies specific to Altair before I could start using it. . Thus, the current effort only integrates the idea of 'chaining' seen in Altair (or in other context in Pandas). In Plotly or Plotnine the 'chaining' is achieved differently but I don't find it as nice or straightforward:. **Plotly:**; ```PYTHON; import plotly.graph_objects as go; fig = go.Figure(; data=[go.Bar(x=[1, 2, 3], y=[1, 3, 2])],; layout=go.Layout(; title=go.layout.Title(text=""A Bar Chart""); ); ); fig.show(); ```; **Plotnine:**; ```PYTHON. from plotnine import ggplot, geom_point, aes, stat_smooth, facet_wrap; from plotnine.data import mtcars. (ggplot(mtcars, aes('wt', 'mpg', color='factor(gear)')); + geom_point(); + stat_smooth(method='lm'); + facet_wrap('~gear')); ```; **Altair:**; ```PYTHON; import altair as alt; from vega_datasets import data. source = data.cars(). alt.Chart(source).mark_circle(size=60).encode(; x='Horsepower',; y='Miles_per_Gallon',; color='Origin',; tooltip=['Name', 'Origin', 'Horsepower', 'Miles_per_Gallon']; ).interactive(); ```; The current solution, although using method chaining, is very *ad hoc* for a specific type of graphs that have predetermined features, like 'dendrogram' or totals for categories or 'brackets' to highlight features.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1127#issuecomment-607888729:543,integrat,integrates,543,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127#issuecomment-607888729,1,['integrat'],['integrates']
Integrability,"@flying-sheep Yup, I think we need to at least print a better failure message than just returning -1 for the milestone check. It's not a good experience for people that are not familiar with this. @lazappi thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2657#issuecomment-1719024055:70,message,message,70,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2657#issuecomment-1719024055,1,['message'],['message']
Integrability,"@flying-sheep yes, it's exactly the same problem, with the exactly same error message that only happens when I (or the function) wanna subset an existing adata object",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/363#issuecomment-458658134:78,message,message,78,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363#issuecomment-458658134,1,['message'],['message']
Integrability,"@flying-sheep, I'm pretty sure the logical conclusion of any long discussion about types is that everything should be done in Haskell. I don't like the use of branches with `isinstance` because it breaks polymorphism, which is a key part of pythonic code to me. @falexwolf, I completely agree with you on ""what makes a good docstring"". The knowledge overhead for numeric python doesn't include type theory, so the docs should be interpretable without them. Ideally, interfaces are simple and the documentation makes the expected behavior clear. I'm still not sure I totally understand what the intent of the ""type"" vs. ""class"" system is in python, so I'm often a little unsure what to do with heavily typed code. That said, if expected behaviors could be encapsulated (both formally and intuitively) with some abstract types (representing interfaces or traits) that would be a nicer solution. I don't think we're near that point in python. ## Lattices. Sorry about not giving some info on lattices, I'd thought you didn't want to get into it. It's the [partially ordered set kind](https://en.wikipedia.org/wiki/Lattice_(order)) of lattice, where each type is an element or subset. I'll give a short python based example (ignoring that `Union[]` can't be instantiated). <details>. <summary>The code:</summary>. ```python; from typing import Any, Union. class A():; pass. class B(A):; pass. class C(A):; pass. class D():; pass. class E(D):; pass; ```. </details>. that defines a lattice, which can be represented as a DAG like this:. ```; Any; / \; A D; / \ |; B C E; \ | /; Union[]; ```. It's partially ordered in that you can't say A contains E or vice-versa, but you can say things like A is contains B, and `Any` is a supertype of (contains) everything else. I think that how you're viewing it is pretty close, except the elements are types instead of their properties. My mental model has types being a collection of properties, and being a subtype means an object inherits it's supertypes properti",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-444715545:466,interface,interfaces,466,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-444715545,4,['interface'],['interfaces']
Integrability,"@flying-sheep, do you know of a large package (ideally in our dependencies) which uses the directory structure you're advocating for? I'd ideally like to have another repo to look at/ crib from for test organization strategies.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2225#issuecomment-1090364103:62,depend,dependencies,62,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2225#issuecomment-1090364103,1,['depend'],['dependencies']
Integrability,"@flyingsheep I can assure you, that's the normal case in academic HPC; systems. On Tue, Mar 26, 2019 at 3:37 PM Philipp A. <notifications@github.com> wrote:. > the main hpc I'm on 1gb of space where appdirs would put these files; >; > That's a misconfigured server, not a normal case. We should use appdirs as; > default, catch a IOError on write, and send a nice message like; >; > Your cache directory is full. Please make sure there's space in; > {cache_dir} or override the cache directory by setting the; > $SCANPY_CACHE_DIR environment variable.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/558#issuecomment-476675808>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AAS-TQPrmr3LWdmwNL5O6XPnRdSAcl_1ks5vajC0gaJpZM4cKXC7>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-476677167:364,message,message,364,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-476677167,1,['message'],['message']
Integrability,"@gokceneraslan. > I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". As a point about this, I don't think `raw` completley solves this problem. There's two reasons for this:. ### Only a different set of variables. Raw only differs from the main object by variables. But we just as often want to remove observations (doublet detection for example). To account for this, I think it makes sense to just have two different anndata objects. ### absolutely everything. I don't think we really can expect to have everything. There are always going to be analyses that require going back to the BAM. If ""single file"" is the issue, we could definitely allow something like:. ```python; with h5py.File(""analysis.h5"") as f:; processed = ad.read_h5ad(f[""processed""]); raw = ad.read_h5ad(f[""raw""]); ```. -----------------------------. @LuckyMD . > Integration works better with HVGs typically. I'm thinking of the case where I have a few datasets saved as `h5ad` that I want to integrate. What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset?. I feel like it's helpful to have the all the measured genes present, so that when you do gather your datasets together you can select features from the full set. > > This does run into memory usage problems if want do a densifying transform on the data; > Don't understand this entirely... I was thinking about what happens if you do something like `sc.pp.scale`, where you don't have any 0s in your expression matrix anymore, so it has to be stored as a dense matrix. I believe this is why `raw` was even introduced originally, since the normalization workflow then was feature selection -> scale. It was wasteful to store the entire s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472:917,Integrat,Integration,917,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472,1,['Integrat'],['Integration']
Integrability,"@hhhh1230511, this PR is not part of any release yet (the latest version `scanpy==1.6` was released August 15, 2020). If you want to have the latest version from GitHub you can follow the instructions for a developer installation [here](https://scanpy.readthedocs.io/en/stable/installation.html) in the documentation, for example. Once a new release is available on `pip`, you can install it via `pip install --upgrade scanpy`; In general, you should avoid modifying the code by e.g. simply copying and pasting. This will either easily cause conflicts when updating the package or cause problems when functions from other files which depend on the content you changed but were not updated accordingly. Hope this helped and clarified things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1422#issuecomment-734460539:634,depend,depend,634,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422#issuecomment-734460539,2,['depend'],['depend']
Integrability,"@ilan-gold your minimal example causes the exact same error:. Exception ignored in: <class 'ValueError'>; Traceback (most recent call last):; File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint; File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32; ValueError: high is out of bounds for int32. if you are curious, it spits the error out 14.210 times (71050 lines of error message). EDIT: the random state does not seem to matter btw, also happens with different random states",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2969#issuecomment-2034445010:456,message,message,456,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2969#issuecomment-2034445010,1,['message'],['message']
Integrability,"@ivirshup ""One issue with the enrichment as is, is that gprofiler-official import name conflicts with the previous unofficial wrapper. I'm worried that this will break peoples environments if they're not aware of this. @liiskolb, do you have any thoughts on this?"". I'm not really sure I got it right, but if new version of scanpy includes new version of gprofiler-official, then it should work well. If people have old version of scanpy that uses old version of gprofiler, then it should also work but with data from archived release of gprofiler. . With this kind of updates it is inevitable that some environments break (we have the experience as you can see;)), these just need to be solved case by case if people with problems start to contact. They could be advised to update their packages to solve these issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/467#issuecomment-484043323:126,wrap,wrapper,126,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467#issuecomment-484043323,1,['wrap'],['wrapper']
Integrability,"@ivirshup -- I still can't tell why Travis is failing. For some reason on Travis, loess is outputting a zero for the gene mentioned in the error message, but this doesn't happen locally for me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1204#issuecomment-624831279:145,message,message,145,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1204#issuecomment-624831279,1,['message'],['message']
Integrability,"@ivirshup ; Yeah, it was the same data as the privious plot. I tried calling sc.tl.umap(sp, init_pos=""paga"") but meet an error. I just use the get_init_pos_from_paga function to solve this error as mention in #769 .Thanks!; ```; TypingError: Failed in nopython mode pipeline (step: nopython frontend); Invalid use of type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)); Known signatures:; * (array(float32, 1d, A), array(float32, 1d, A)) -> float32; * parameterized; [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)); [2] During: typing of call at /datc/dh_data/.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py (797). File ""../../../../.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py"", line 797:; def optimize_layout(; <source elided>. dist_squared = rdist(current, other); ^. This is not usually a problem with Numba itself but instead often caused by; the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:; http://numba.pydata.org/numba-doc/latest/reference/pysupported.html; and; http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:; http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message; and traceback, along with a minimal reproducer at:; https://github.com/numba/numba/issues/new",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/918#issuecomment-555516223:1502,message,message,1502,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918#issuecomment-555516223,1,['message'],['message']
Integrability,"@ivirshup @giovp I'm wondering whether you've had the time to look over this. If this PR is maybe too big a change, then perhaps it would make more sense to migrate to openTSNE in a more iterative approach. For instance, we could just replace the t-SNE implementation to openTSNE, ignoring the API discussion and ignoring the precomputed graphs. I think switching to openTSNE, regardless of integration, would make the t-SNE implementation faster. We could then go for tighter integration step by step. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-922497079:391,integrat,integration,391,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-922497079,2,['integrat'],['integration']
Integrability,@ivirshup I assume you caught that in your minimal-dependencies branch already? Maybe it’s a good time to push it!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2733#issuecomment-1798483923:51,depend,dependencies,51,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2733#issuecomment-1798483923,1,['depend'],['dependencies']
Integrability,"@ivirshup Indeed the problem is `use_raw=True` by default. In the test, I think what happens is that the raw data is being plotted and thus no error appears. The tolerance for the image difference may hide the problem if indeed the test image is correct. To avoid this confusion when plotting a layer I think it is better to override `use_raw`. This is how it was supposed to be working before the changes according to the documentation:. ```; layer : typing.Union[str, NoneType], optional (default: None); Name of the AnnData object layer that wants to be plotted. By default; adata.raw.X is plotted. If `use_raw=False` is set, then `adata.X` is plotted.; If `layer` is set to a valid layer name, then the layer is plotted. `layer`; takes precedence over `use_raw`.; ``` ; The current logic is around this lines https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L744. PS: the `use_raw` has been a source of many confusions for me. Now I know when raw is used by default but for new users this may not be obvious. One solution is to add a warning message everytime that `use_raw` is set to `True` by the code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/730#issuecomment-510785080:1083,message,message,1083,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730#issuecomment-510785080,1,['message'],['message']
Integrability,"@ivirshup Thank you for your super elaborate response and treatment of the topic. I completely understand that you're going for a more comprehensive solution than something like the simple bokeh wrapper that I pasted above. I'd really be interested in something that combines datashader and bokeh, for instance. If you're creating your own package for that, it would be awesome if it was somehow possible to use it also for Scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/253#issuecomment-420664106:195,wrap,wrapper,195,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/253#issuecomment-420664106,2,['wrap'],['wrapper']
Integrability,"@ivirshup, yes, your example works. However, I would not consider the issue as resolved as it still exists IMO.; Your example only works as you are working with a sparse matrix. If `X` is a `np.ndarray`, the method still fails:. ```bash; >>> adata = sc.AnnData(; np.ceil(np.abs(np.random.randn(10, 10))).astype('int64'),; dtype=int,; ); >>> sc.pp.log1p(adata); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/opt/anaconda3/lib/python3.7/functools.py"", line 840, in wrapper; return dispatch(args[0].__class__)(*args, **kw); File ""/opt/anaconda3/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py"", line 350, in log1p_anndata; X = log1p(X, copy=False, base=base); File ""/opt/anaconda3/lib/python3.7/functools.py"", line 840, in wrapper; return dispatch(args[0].__class__)(*args, **kw); File ""/opt/anaconda3/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py"", line 318, in log1p_array; np.log1p(X, out=X); TypeError: ufunc 'log1p' output (typecode 'd') could not be coerced to provided output parameter (typecode 'l') according to the casting rule ''same_kind''; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/435#issuecomment-683644812:497,wrap,wrapper,497,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435#issuecomment-683644812,2,['wrap'],['wrapper']
Integrability,"@ivirshup. > The worst case scenario I see here me typing something so poorly a newbie trying to follow the documentation gets horrible numba errors they can't figure out. Well, that’s an improvement over the current situation of “the freeform text type annotations make me guess what I can pass and I get horrible numba errors”, right?. > `issubclass(np.ndarray, typing.Sequence) == False`. That looks like a bug. The docs to `Sequence` say: “Concrete subclasses must override `__new__` or `__init__`, `__getitem__`, and `__len__`”, and. ```py; >>> np.ndarray.__new__ ; <function ndarray.__new__(*args, **kwargs)>; >>> np.ndarray.__getitem__ ; <slot wrapper '__getitem__' of 'numpy.ndarray' objects>; >>> np.ndarray.__len__ ; <slot wrapper '__len__' of 'numpy.ndarray' objects>; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-441583940:651,wrap,wrapper,651,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441583940,2,['wrap'],['wrapper']
Integrability,"@jlause Interesting work! It would indeed be nice to avoid having to learn bandwidths altogether. What would be the procedure for learning global theta from the data? Would you just flatten the expression matrix into one vector?. With regards to the clipping, I turned my brain off and copied the Seurat implementation as much as possible. `sqrt(n/30)` was the default parameter used by the SCTransform wrapper in Seurat. I also removed negative values to preserve sparsity structure of the data. Sorry I couldn't provide any insight about this!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1643#issuecomment-791871293:403,wrap,wrapper,403,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643#issuecomment-791871293,2,['wrap'],['wrapper']
Integrability,@kaushalprasadhial We internal discussed adding `scikit-learn-intelex` as a dependency. We came to the conclusion that we dont want it as such. Since this a patch that the user can do regardless we think tath the best thing would be to have a notebook that would show the speedup of the patch. We could host this in a new notebook acceleration category.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3279#issuecomment-2429335571:76,depend,dependency,76,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279#issuecomment-2429335571,2,['depend'],['dependency']
Integrability,"@kt6k, I think that's worth opening a separate issue for. I'm not too experienced with the velocyto tools, but I suspect the issue might be more appropriate for either [velocyto-team/velocyto.py](https://github.com/velocyto-team/velocyto.py) or [theislab/scvelo](https://github.com/theislab/scvelo) depending on when this filtering is occurring.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/567#issuecomment-489908092:299,depend,depending,299,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/567#issuecomment-489908092,1,['depend'],['depending']
Integrability,@maarten-hifibio in the mean time if you need it I have just made some GPU wrappers available on the following gist:; https://gist.github.com/LouisFaure/9302aa140d7989a25ed2a44b1ce741e8. It includes one for leiden that would exactly act like sc.tl.leiden (it is part of my PR referenced here),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1793#issuecomment-1106331321:75,wrap,wrappers,75,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793#issuecomment-1106331321,1,['wrap'],['wrappers']
Integrability,@maximilianh I think those messages are from your code? maybe you should improve the error message to include something like. > Try running sc.tl.rank_genes_groups(adata) to create the cluster annotation,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/262#issuecomment-478907896:27,message,messages,27,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262#issuecomment-478907896,2,['message'],"['message', 'messages']"
Integrability,"@maximilianh I was able to use the cell browser export function in the past but this time I am getting an error message:. INFO:root:Writing scanpy matrix to adata_cellbrowser_04_01_19_CD8_subclustered/exprMatrix.tsv.gz; INFO:root:Transposing matrix; INFO:root:Writing gene-by-gene, without using pandas; INFO:root:Writing 8068 genes in total; INFO:root:Wrote 0 genes; INFO:root:Wrote 2000 genes; INFO:root:Wrote 4000 genes; INFO:root:Wrote 6000 genes; INFO:root:Wrote 8000 genes; INFO:root:Writing UMAP coords to adata_cellbrowser_04_01_19_CD8_subclustered/umap_coords.tsv; ERROR:root:Couldnt find cluster markers list. I am using an h5ad file to import my ann data object. Is that why there is some issue with finding cluster markers ? I am able to plot the clusters in a UMAP plot so I know that the 'louvain' observation exists. Any thoughts on why this is happening ?. Thanks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/262#issuecomment-478685403:112,message,message,112,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262#issuecomment-478685403,1,['message'],['message']
Integrability,"@michalk8 thanks for the extensive recommendations!. I think I'd like to keep the number of tools used small. It's the worst when you want to fix a bug, but instead have to learn about configuring a linter. More tools means more configurations people need to be familiar with, and the goal is reducing cognitive load. > Also fixing types for `mypy` takes a while, I'd do it as last. Yeah, I figured this would be the case. Does `mypy` allow partial typing these days? Also, I haven't found the numpy or pandas type stubs to always be great. Have you run into problems around this?. I think this would also need to wait at least until we can drop python 3.6 for `anndata`, since adding types there currently means circular dependencies. > `rstcheck` to check the syntax of .rst files. I would particularly like a linter for `rst`. I noticed you also had `doc8`, but you'd recommend `rstcheck` check over this? I'm a little worried, considering its last release was over a year ago. Spell check for prose in doc-strings could also be great, but I could see this being overzealous (is there a good way to notify about misspelled words, while not being annoying about technical terms?). I'm a little worried about some custom sphinx extensions we have, and conflicting with this, any experience here?. --------------------------------------------. @Koncopd, I think I agree with your concern, as I said above: it's the worst when you want to fix a bug, but instead have to learn about configuring a linter. I also think it's very easy to add new checks, so someone complaining about new ones is valuable. Per commit, this should always be an option with `git commit --no-verify`, though you could also just not install `pre-commit`. I would like to keep the required checks limited, ideally formatting tasks that can be automated as opposed ""this is poor style"" warnings. I also know these tools can be wrong (e.g. `black` when expression's have many operators, sometimes with chaining) so it would be goo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1563#issuecomment-754352635:722,depend,dependencies,722,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563#issuecomment-754352635,2,['depend'],['dependencies']
Integrability,"@outlace Curiously, your change causes an error. Without your change I can run the tests correctly without a problem. I remember that I fixed a bug similar to this one that was recently integrated into master (see https://github.com/theislab/scanpy/pull/425/files#diff-b5175ed1415cdbf853646e523cbe8ae0L902). Could it be that you didn't have the latest pull from scanpy and that was causing the error?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/525#issuecomment-471592072:186,integrat,integrated,186,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/525#issuecomment-471592072,1,['integrat'],['integrated']
Integrability,@outlace I will check the problem with the test and integrate your changes in a new PR that addresses #512 and #524 if this is OK with you.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/525#issuecomment-471455638:52,integrat,integrate,52,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/525#issuecomment-471455638,1,['integrat'],['integrate']
Integrability,"@outlace, whoops, didn't see your message before I posted mine, chatroom model woulda stopped that. It looks to me like it's free to self host and theres (admittedly kinda high) educational price for discourse. I'll check out how easy it is to self host. An alternative for threaded conversations is a good old fashioned google group or a tag on stack overflow. (edit: we could probably even just use biostars). The main reason I'm pushing this, is because the more I think about it, the more how unhelpful most gitters have been for me. It might to do with being in a non-European or American timezone though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/542#issuecomment-476058909:34,message,message,34,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/542#issuecomment-476058909,1,['message'],['message']
Integrability,"@pati-ni ; I have the similar issue when installing CellRank as `conda install -c bioconda cellrank`. Problem is not all dependencies are on bioconda, some of them are on `conda-forge` - I've tested it and I have the same problem with scanpy.; Installing it as `conda install -c bioconda -c conda-forge scanpy` works. But @ivirshup is right, seems like conda issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1298#issuecomment-653900843:121,depend,dependencies,121,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1298#issuecomment-653900843,1,['depend'],['dependencies']
Integrability,"@pinin4fjords the tests aren't running since scrublet isn't actually being installed (I'm surprised the build still worked, apparently this is just a warning?). From the travis logs:. ```; 203$ pip install .[dev,test,louvain,leiden,magic,scvi,harmony,skmisc,scrublet]; 204Processing /home/travis/build/theislab/scanpy; 205 Installing build dependencies ... done; 206 Getting requirements to build wheel ... done; 207 Preparing wheel metadata ... done; 208 WARNING: scanpy 0.1.dev67+g3918588 does not provide the extra 'scrublet'; ```. You'll need to add a scrublet entry to `extras_require` here: https://github.com/theislab/scanpy/blob/d56d6beacdd951a010bb6a93078db26e1ac904b0/setup.py#L31-L56",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1476#issuecomment-734643707:340,depend,dependencies,340,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1476#issuecomment-734643707,1,['depend'],['dependencies']
Integrability,"@pranzatelli, could you open a new issue for this? In that issue, could you also report what versions of the dependencies you're using via `sc.logging.print_versions()`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1823#issuecomment-983618113:109,depend,dependencies,109,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1823#issuecomment-983618113,1,['depend'],['dependencies']
Integrability,"@rcannood,. I have the same issue while using PAGA_tree from the dyno wrapper.; Do you know where I can add the chunk of code that @flying-sheep suggested to remove the warnings?; My first guess would be to change the python files in the container built from PAGA_tree, however the latter (from dyno) builds a new container every time it is called. How should I procedd?. Best,; Andy",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/688#issuecomment-1180404535:70,wrap,wrapper,70,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/688#issuecomment-1180404535,1,['wrap'],['wrapper']
Integrability,@sjfleming Is there a GIST or repo url to use this code? Might take time to integrate into scanpy/anndata but people can benefit from the code if it already lives somewhere...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/950#issuecomment-1117994615:76,integrat,integrate,76,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/950#issuecomment-1117994615,1,['integrat'],['integrate']
Integrability,"@tomwhite, we're now fully depending on UMAP. :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/576#issuecomment-487798308:27,depend,depending,27,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576#issuecomment-487798308,1,['depend'],['depending']
Integrability,"@vitkl now multiple samples are supported, see [here](https://scanpy-tutorials.readthedocs.io/en/latest/spatial/integration-scanorama.html) for description on how to use the new concat strategy",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1158#issuecomment-640496084:112,integrat,integration-scanorama,112,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1158#issuecomment-640496084,1,['integrat'],['integration-scanorama']
Integrability,"A numba reimplementation of some of the metrics sounds pretty awesome actually. That's out of scope for `scIB` at the moment. We didn't bother with parallelization for most of the metrics (beyond what was already implemented in `sc.tl.louvain` and the sklearn dependencies) as the slowest ones were in R anyway (and now also C++ with our LISI update). Would really welcome that. I can help where I can, although not so familiar with numba.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/915#issuecomment-764146920:260,depend,dependencies,260,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-764146920,1,['depend'],['dependencies']
Integrability,"AC2_"", var_names='gene_symbols', cache=True)`. I get the following error:; `FileNotFoundError: [Errno 2] No such file or directory: 'GSE212966\\GSM6567159_PDAC2_features.tsv.gz'`. The thing is that the file exist here:; ![kép](https://github.com/user-attachments/assets/a3ee8f51-833d-4adb-ab9f-f6ff5b19387f). I have changed the *genes.tsv.gz file's name to *features.tsv.gz but still got the same error. Here is the full error log:; ```; ---------------------------------------------------------------------------; FileNotFoundError Traceback (most recent call last); Cell In[62], [line 1](vscode-notebook-cell:?execution_count=62&line=1); ----> [1](vscode-notebook-cell:?execution_count=62&line=1) data1 = sc.read_10x_mtx(""GSE212966"", prefix=""GSM6567159_PDAC2_"", var_names='gene_symbols', cache=True); [2](vscode-notebook-cell:?execution_count=62&line=2) data1.var_names_make_unique(). File ~\AppData\Roaming\Python\Python312\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); [77](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/legacy_api_wrap/__init__.py:77) @wraps(fn); [78](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/legacy_api_wrap/__init__.py:78) def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; [79](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/legacy_api_wrap/__init__.py:79) if len(args_all) <= n_positional:; ---> [80](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/legacy_api_wrap/__init__.py:80) return fn(*args_all, **kw); [82](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/sc",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:1441,wrap,wrapper,1441,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['wrap'],['wrapper']
Integrability,"AFAIK networkx and python-igraph do the same thing, only that python-igraph is faster. We also need python-igraph anyway for louvain and so on, so maybe it would be good to get rid of networkx. Downside: python-igraph and louvain-igraph is currently deliberately an optional dependency since it’s hard to install on windows. People need to build it themselves (A task that even I didn’t manage by now, and I got *many* things to compile!) or use Christoph Grohlke’s unofficial builds ([here](https://www.lfd.uci.edu/~gohlke/pythonlibs/#python-igraph) and [here](https://www.lfd.uci.edu/~gohlke/pythonlibs/#louvain-igraph))",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/97:275,depend,dependency,275,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97,1,['depend'],['dependency']
Integrability,"About the API, I still think it makes sense for TSNE weighted neighbor calculation to be separate, especially if it is going to have multiple weighting options that depend on the `openTSNE` package. If it turns out these methods don't have much in the way of parameters, then it might be reasonable for this to be a part of `sc.pp.neighbors`. How about this, the implementation here should be well factored out into:. 1. Getting nearest neighbors; 2. Weighting the graph; 3. Computing the layout. Once the available parameters are clear I think it'll be easier to make an informed decision about whether neighbor weighting for tsne should occur through `sc.pp.neighbors`. Additionally, I think it'll be easier to integrate cleanly separated code than to separate integrated code. > The weights constructed by UMAP in neighbors are not normalized. So if you run neighbors() and then tsne() then t-SNE should do something in order to be able to use this graph. For passing the umap connectivity matrix to tsne layout, I think I would expect the weights to be used. Something like this should accomplish that:. ```python; class WrappedAffinities(openTSNE.affinity.Affinities):; def __init__(self, neighbors, symmetrize=True, verbose=False):; self.verbose = verbose; P = neighbors; if symmetrize:; P = (P + P.T) / 2; total = P.sum(); if not np.isclose(total, 1.):; P = P / total; self.P = P; ```. That said, I'm not too familiar with the assumptions of tsne, or if this would be appropriate. I think binarizing the edge weights is a bit of a strong assumption unless specifically requested though. With `umap`, we throw a warning if it looks like the passed graph didn't come from `umap`. You could do the same here?. > From an implementation standpoint, the sc.pp.tsne_negihbors will inevitably have to call the UMAP KNNG construction, since I can see that it's not split out in the code-base. I would like nearest neighbor calculation and graph weighting to be split out eventually. Since it's already d",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-761950200:165,depend,depend,165,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-761950200,6,"['depend', 'integrat']","['depend', 'integrate', 'integrated']"
Integrability,"According to [scanorama MemoryError](https://github.com/brianhie/scanorama/blob/master/scanorama/scanorama.py#L768-L775), I think `batch_size` parameter doesn't work for `sc.external.pp.scanorama_integrate`. ### Minimal code sample (that we can copy&paste without having any data). ```python; sc.external.pp.scanorama_integrate(adata, key=""datasetID"", batch_size=500); ```. ```pytb; WARNING: Out of memory, consider turning on batched computation with batch_size parameter.; Traceback (most recent call last):; File ""/lustre1/shiq//02_igt/py/1_igt_scanorama.py"", line 26, in <module>; sc.external.pp.scanorama_integrate(adata, key=""datasetID"", batch_size=500); File ""/lustre1/shiq/app/miniconda3/envs/scanpy/lib/python3.10/site-packages/scanpy/external/pp/_scanorama_integrate.py"", line 121, in scanorama_integrate; integrated = scanorama.assemble(; File ""/lustre1/shiq/app/miniconda3/envs/scanpy/lib/python3.10/site-packages/scanorama/scanorama.py"", line 933, in assemble; bias = transform(curr_ds, curr_ref, ds_ind, ref_ind, sigma=sigma,; File ""/lustre1/shiq/app/miniconda3/envs/scanpy/lib/python3.10/site-packages/scanorama/scanorama.py"", line 762, in transform; avg_bias = batch_bias(curr_ds, match_ds, bias, sigma=sigma,; File ""/lustre1/shiq/app/miniconda3/envs/scanpy/lib/python3.10/site-packages/scanorama/scanorama.py"", line 723, in batch_bias; weights = rbf_kernel(curr_ds, match_ds, gamma=0.5*sigma); File ""/lustre1/shiq/app/miniconda3/envs/scanpy/lib/python3.10/site-packages/sklearn/metrics/pairwise.py"", line 1294, in rbf_kernel; X, Y = check_pairwise_arrays(X, Y); File ""/lustre1/shiq/app/miniconda3/envs/scanpy/lib/python3.10/site-packages/sklearn/metrics/pairwise.py"", line 155, in check_pairwise_arrays; X = check_array(; File ""/lustre1/shiq/app/miniconda3/envs/scanpy/lib/python3.10/site-packages/sklearn/utils/validation.py"", line 727, in check_array; warnings.warn(; FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy a",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2319:816,integrat,integrated,816,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2319,1,['integrat'],['integrated']
Integrability,Add CI job with minimal dependencies,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2222:24,depend,dependencies,24,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2222,1,['depend'],['dependencies']
Integrability,Add CI run with minimal dependencies,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2211:24,depend,dependencies,24,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2211,1,['depend'],['dependencies']
Integrability,Add DCA integration,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/186:8,integrat,integration,8,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/186,1,['integrat'],['integration']
Integrability,Add PHATE integration,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/136:10,integrat,integration,10,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/136,1,['integrat'],['integration']
Integrability,Add Scanorama integration to external API,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1332:14,integrat,integration,14,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1332,1,['integrat'],['integration']
Integrability,Add fsspec as a test dependency due to dask,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/765:21,depend,dependency,21,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/765,1,['depend'],['dependency']
Integrability,Add sparsificiation step before sparse-dependent Scrublet calls,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1707:39,depend,dependent,39,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1707,1,['depend'],['dependent']
Integrability,Add support for Harmony integration,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1306:24,integrat,integration,24,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1306,1,['integrat'],['integration']
Integrability,"Added DCA integration, as discussed in #142 . For denoising, uses can call:. `sc.pp.dca(adata)`. which replaces adata.X inplace. For latent representations:. `sc.pp.dca(adata, mode='latent')`. can be called, which adds 'X_dca' to adata.obsm. Fixes #142 . This integration uses new DCA API (>= 0.2.1). All DCA API arguments are exposed and usable in scanpy integration.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/186:10,integrat,integration,10,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/186,3,['integrat'],['integration']
Integrability,Added wrapper for mnnpy,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/131:6,wrap,wrapper,6,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/131,1,['wrap'],['wrapper']
Integrability,Added wrapper for mnnpy in pp,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/131:6,wrap,wrapper,6,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/131,1,['wrap'],['wrapper']
Integrability,"Addendum: different errors are generated depending on which axis is first sliced. The data set I'm loading is a dense matrix. ```; >>> data.X.dtype; dtype('<f4'); >>> data[:,0][0,:]; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1303, in __getitem__; return self._getitem_view(index); File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1307, in _getitem_view; return AnnData(self, oidx=oidx, vidx=vidx, asview=True); File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 669, in __init__; self._init_as_view(X, oidx, vidx); File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 694, in _init_as_view; uns_new = deepcopy(self._adata_ref._uns); File ""/usr/lib/python3.6/copy.py"", line 180, in deepcopy; y = _reconstruct(x, memo, *rv); File ""/usr/lib/python3.6/copy.py"", line 307, in _reconstruct; y[key] = value; File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 444, in __setitem__; _init_actual_AnnData(adata_view); File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 367, in _init_actual_AnnData; adata_view._init_as_actual(adata_view.copy()); File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 880, in _init_as_actual; self._check_dimensions(); File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1879, in _check_dimensions; .format(self._n_obs, self._obs.shape[0])); ValueError: Observations annot. `obs` must have number of rows of `X` (1), but has 2638 rows.; >>> data[0,:][:,0]; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1303, in __getitem__; return self._getitem_view(index); File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1307, in _getitem_view; return AnnData(self, oidx=oidx, vidx=vidx, asview=True); File ""/cellxg",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/332#issuecomment-433745600:41,depend,depending,41,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332#issuecomment-433745600,1,['depend'],['depending']
Integrability,Adding cell2location to the list of scRNA->spatial integration methods,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1574:51,integrat,integration,51,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1574,1,['integrat'],['integration']
Integrability,"After updating to the most recent version of scanpy, I had to separately update anndata, louvain, and leidenalg packages that are dependancies. Any dependent package updates of these types should just happen when the newest version is installed.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/518:130,depend,dependancies,130,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/518,2,['depend'],"['dependancies', 'dependent']"
Integrability,"Agreed. I don’t think we should rush and include everything into scanpy, especially when it would be a simple wrapper of something existing.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/95#issuecomment-369863247:110,wrap,wrapper,110,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/95#issuecomment-369863247,2,['wrap'],['wrapper']
Integrability,"Ah I think I see the issue! Feature branches should be based off `master` and directing the pull request there! I think what's happening is that a pre-commit hook was installed, but the config only exists on the `master` branch. I think this should largely be manageable by rebasing onto master (e.g. `git rebase --onto master 1.7.x`) and changing the branch the PR is targeting via the github interface:. <img width=""300"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/110570131-9093e600-81a9-11eb-9223-5b7bc233d75c.png"">. --------------. Side note: We're considering separating the `highly_variable_genes` interface into multiple functions, since the arguments to the different methods don't always overlap in meaningful or intuitive ways. There's nothing you need to do about this right now, but just a heads up to keep the logic for this method separate from the main function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-794790768:394,interface,interface,394,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-794790768,4,['interface'],['interface']
Integrability,"Ahh, I think this is just because of the way I've tried to translate into to the Scanpy workflow. There's a sparsing step [at the start of the basic Scrublet workflow](https://github.com/swolock/scrublet/blob/67f8ecbad14e8e1aa9c89b43dac6638cebe38640/src/scrublet/scrublet.py#L100), but I'm [injecting](https://github.com/theislab/scanpy/blob/76814588696d00183e5f6f02e64f145dbcf944a0/scanpy/external/pp/_scrublet.py#L360) the normalised matrix and effectively skipping that step. I'll PR a sparsing check and conversion (and yes @ivirshup , I'll add a test :-) ), but the workaround is perfectly valid for now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1645#issuecomment-788832663:291,inject,injecting,291,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1645#issuecomment-788832663,1,['inject'],['injecting']
Integrability,"All right, fair points. > Poetry is great! But i remember two problems:; > ; > no good way to editably install into some env: python-poetry/poetry#34; > doesn’t support plugins yet so only hardcoded versions in static metadata: python-poetry/poetry#140; > . I also stumbled upon the editably install issue. This is not an issue that Poetry can solve at the moment as explained in the thread. I do however understand that this is an issue for scanpy (considering the strong anndata dependency etc). Regarding plugins - they are on the roadmap and should appear at some point. Considering that the community is very active whereas the main developer is not anymore this may solve the editable install with some hack as well. I agree with your points and Poetry is not yet the solution that we should currently use, but I think it is the proper solution that we should aim for. > I'd also be worried using poetry would hamper contributions from people unfamiliar with it, and I don't think bioinformaticians are going to be familiar with it. I don't think that anybody is familiar with flit either, but it is slightly less intrusive and does not fundamentally change so many things like Poetry does. However, many things that Poetry does change make a lot of sense and solve other issues that we did not discuss here yet. So yeah, I personally would wait for Poetry to get it's plugin system and for the editable install issue to get a proper PEP. But if you don't feel like waiting Flit might be fun :). Cheers",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1527#issuecomment-765253434:481,depend,dependency,481,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-765253434,1,['depend'],['dependency']
Integrability,"Also I'm surprised to see I never left a note on your message @falexwolf : thanks! I'm working on the API now, will send in a PR when it's done or leave a note here if I think the DCA API could do with some modification.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/187#issuecomment-403501395:54,message,message,54,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/187#issuecomment-403501395,1,['message'],['message']
Integrability,"Also two api thoughts:. For `sc.metrics.gearys_c(a: ""array"", b: ""array"")`, where `b` is 2d is expected to have a shape like: `(variable, number_of_cells)` – the ufunc shape signature would be: `(m,m)(n,m)->(n,)`. This is because it needs fast access to each variable, so they correspond to rows. Also the length of the returned array depends on the first axis of the passed input. Is this intuitive, or should the input be transposed?. Second, for `confusion_matrix`, I'm thinking I should make it singly dispatched on the first argument. This way if a dataframe is passed, the next two arguments could correspond to keys in that dataframe. Otherwise, vectors can be passed directly. Under that, these calls would be equivalent:. ```python; sc.metrics.confusion_matrix(adata.obs, ""sample_labels"", ""leiden""); sc.metrics.confusion_matrix(adata.obs[""sample_labels""], adata.obs[""leiden""]); ```. Right now it has the seaborn style argument handling shown at the top of this PR. I'm not sure that's really caught on in other packages or fits with scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/915#issuecomment-559928610:334,depend,depends,334,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-559928610,2,['depend'],['depends']
Integrability,"Also, I don't think I would mind `Pillow` too much as a test dependency (it doesn't have a ton of dependencies, right?). A more lightweight solution would be to call [`file`](https://en.wikipedia.org/wiki/File_(command)) on the path, which should be able to recognize it as a `tiff`. I think this would only look at the head of the file though, and wouldn't check if it was corrupted/ didn't finish downloading.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1506#issuecomment-733652124:61,depend,dependency,61,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1506#issuecomment-733652124,2,['depend'],"['dependencies', 'dependency']"
Integrability,"Also, set up johnnydep and then do:. `johnnydep --output-format pinned scanpy_scripts; `; and after trundling for a very long time and emitting a lot of messages it gives up with:. ```. Given no hashes to check 0 links for project 'scipy': discarding no candidates; ERROR: Could not find a version that satisfies the requirement scipy<1.3.0,>=0.19.1,>=1.0,>=1.0.1,>=1.2.0,>=1.3.1,~=1.0 (from versions: 0.8.0, 0.9.0, 0.10.0, 0.10.1, 0.11.0, 0.12.0, 0.12.1, 0.13.0, 0.13.1, 0.13.2, 0.13.3, 0.14.0, 0.14.1, 0.15.0, 0.15.1, 0.16.0, 0.16.1, 0.17.0, 0.17.1, 0.18.0rc2, 0.18.0, 0.18.1, 0.19.0, 0.19.1, 1.0.0b1, 1.0.0rc1, 1.0.0rc2, 1.0.0, 1.0.1, 1.1.0rc1, 1.1.0, 1.2.0rc1, 1.2.0rc2, 1.2.0, 1.2.1, 1.2.2, 1.2.3, 1.3.0rc1, 1.3.0rc2, 1.3.0, 1.3.1, 1.3.2, 1.3.3, 1.4.0rc1, 1.4.0rc2, 1.4.0, 1.4.1, 1.5.0rc1, 1.5.0rc2, 1.5.0); ERROR: No matching distribution found for scipy<1.3.0,>=0.19.1,>=1.0,>=1.0.1,>=1.2.0,>=1.3.1,~=1.0. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1273#issuecomment-653294084:153,message,messages,153,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1273#issuecomment-653294084,1,['message'],['messages']
Integrability,"AnnData a large dependency given that the only interaction is; ```python; ad.obs[""mellon_log_density""] = mellon.DensityEstimator().fit_predict(ad.obsm[""DM_EigenVectors""]); ```; I understand the criteria though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2577#issuecomment-1656364181:16,depend,dependency,16,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2577#issuecomment-1656364181,1,['depend'],['dependency']
Integrability,"Anybody experience something similar? . I'm attempting to regress out cell cycle gene information from a single cell dataset. ```; # ; # # Part of the error message that probably matters most; # . Crashed Thread: 0. Exception Type: EXC_BAD_ACCESS (SIGSEGV); Exception Codes: KERN_INVALID_ADDRESS at 0x0000000000000110; Exception Note: EXC_CORPSE_NOTIFY. Termination Signal: Segmentation fault: 11; Termination Reason: Namespace SIGNAL, Code 0xb; Terminating Process: exc handler [0]. VM Regions Near 0x110:; --> ; __TEXT 00000001039d0000-00000001039d1000 [ 4K] r-x/rwx SM=COW /Library/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python. Application Specific Information:; *** multi-threaded process forked ***; crashed on child side of fork pre-exec. # ; # ; # ; ```. Any ideas on what the problem could be?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/194:157,message,message,157,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/194,1,['message'],['message']
Integrability,"Apparently, UMAP versions below 0.5 are affected (which is a strict dependency of `scvelo`). Versions 0.5+ are not affected.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1799#issuecomment-822498064:68,depend,dependency,68,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799#issuecomment-822498064,1,['depend'],['dependency']
Integrability,"As @fidelram said in https://github.com/theislab/scanpy/pull/661#issuecomment-496144015, we have to wait until matplotlib/matplotlib#14298 is fixed. It seems to be in matplotlib’s 3.1.2 milestone, so maybe we can just set the dependency to “matplotlib == 3.0.0 or matplotlib >= 3.1.2”. This originally came up in #663, and then later in e.g. #787",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/849:226,depend,dependency,226,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/849,1,['depend'],['dependency']
Integrability,"As a general approach to this kind of problem, I write functions like this:. ```python. def grouped_obs_mean(adata, group_key, layer=None, gene_symbols=None):; if layer is not None:; getX = lambda x: x.layers[layer]; else:; getX = lambda x: x.X; if gene_symbols is not None:; new_idx = adata.var[idx]; else:; new_idx = adata.var_names. grouped = adata.obs.groupby(group_key); out = pd.DataFrame(; np.zeros((adata.shape[1], len(grouped)), dtype=np.float64),; columns=list(grouped.groups.keys()),; index=adata.var_names; ). for group, idx in grouped.indices.items():; X = getX(adata[idx]); out[group] = np.ravel(X.mean(axis=0, dtype=np.float64)); return out; ```. Swapping out the last 8 lines or so depending on what I'm calculating. To use a set of marker genes I'd call it as `grouped_obs_mean(adata[:, marker_genes], ...)`. At some point we might have `groupby` for `AnnData`s, but that'll require figuring out how to be consistent about the returned type.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/181#issuecomment-534867254:698,depend,depending,698,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/181#issuecomment-534867254,1,['depend'],['depending']
Integrability,"As an alternative, I'd be up for just deprecating raw all together, as I think it causes more problems than it solves. I was talking about this recently with @falexwolf, who has come to a similar conclusion. This could be done on the `anndata` side, and just warn whenever `raw` is set. If no `raw` is present, then none of the weird behavior should come up. > I wonder how important it is to keep genes that are filtered out due to being expressed in too few cells anyway. Might be important for integration? But hopefully this could be solvable by just knowing what annotation was used so you can safely assume the missing values are 0. Also, what level of filtering are you doing here? I've tend to go `min_cells=1`. I think we do need to have a more general solution for having a ""feature-select-ed"" subset of the data, but think this can be done with `mask` argument. E.g. `sc.pp.pca(adata, mask=""highly_variable"")` (I believe we've talked about this before). This does run into memory usage problems if want do a densifying transform on the data, though I have doubts about whether this can be a good representation of the data. This can be technically solved by using a block sparse matrix type, but I'm not sure if any practically usable implementations of this are currently available.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1798#issuecomment-819998988:497,integrat,integration,497,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-819998988,2,['integrat'],['integration']
Integrability,"As an update, I've been using this helper function to consistently handle this:. ```python. def _choose_obs_rep(adata, *, use_raw=False, layer=None, obsm=None, obsp=None):; """"""; Choose array aligned with obs annotation.; """"""; is_layer = layer is not None; is_raw = use_raw is not False; is_obsm = obsm is not None; is_obsp = obsp is not None; choices_made = sum((is_layer, is_raw, is_obsm, is_obsp)); assert choices_made <= 1; if choices_made == 0:; return adata.X; elif is_layer:; return adata.layers[layer]; elif use_raw:; return adata.raw.X; elif is_obsm:; return adata.obsm[obsm]; elif is_obsp:; return adata.obsp[obsp]; else:; assert False, (; ""That was unexpected. Please report this bug at:\n\n\t""; "" https://github.com/theislab/scanpy/issues""; ); ```. This could use support for variable masks like `use_highly_variable`. Also the error message should be better. I think a collection of helper functions like this should go in to a utils module (`sc.utils.argutils`?) which could be public so it's easier to use in `scanpy`-like packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/828#issuecomment-560072919:845,message,message,845,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/828#issuecomment-560072919,1,['message'],['message']
Integrability,"As discussed, @Koncopd will try to integrate this into scikit-learn itself and not into Scanpy. :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/403#issuecomment-456032298:35,integrat,integrate,35,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/403#issuecomment-456032298,2,['integrat'],['integrate']
Integrability,"As expected the two other links here were of no use.; I also replictaed the problem in Python (not Jupyter) and get this unhepful message:. scanpy.pp.neighbors(adatas[1]); WARNING: You\u2019re trying to run this on 19151 dimensions of `.X`, if you really want this, set `use_rep='X'`.; Falling back to preprocessing with `sc.pp.pca` and default params.; Segmentation fault (core dumped). I know cpp is like this. But I can not even find the core.dump anywhere. Please help!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2361#issuecomment-1313461261:130,message,message,130,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2361#issuecomment-1313461261,1,['message'],['message']
Integrability,As it's scheduled to be removed in anndata 0.10.0. This will require a quick bug fix release. It may also require bumping the anndata dependency to >0.8.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2658:134,depend,dependency,134,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2658,1,['depend'],['dependency']
Integrability,"As said: `pip install scanpy[leiden]`, and use `scanpy.tl.leiden()` instead. See here for how to install scanpy and its dependencies: https://scanpy.readthedocs.io/en/stable/installation.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1283#issuecomment-1638210248:120,depend,dependencies,120,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1283#issuecomment-1638210248,1,['depend'],['dependencies']
Integrability,"As the clustering is an optimization of the modularity I would argue it makes little sense to use modularity to evaluate the clustering again. Especially at different resolutions, the modularity values you obtain are not really comparable (as the resolution parameter is introduced to not optimize pure modularity and get the same result you would otherwise get at resolution 1). A comparison between knn-graph modularity at the same resolution would tell you how inherently modular the graph is. Is that what you want to know? Or what does an 'improved' graph look like to you?. I agree with Alex that using the silhouette coefficient wouldn't be much more informative. It would then just be an assessment of the approach of using a KNN graph and modularity optimization. And as that approach has been shown to work quite well, evaluating it based on something that works less well (clustering in the feature space directly) feels a bit uninformative. I would go with what you suggested: evaluating based on marker gene expression. In the end the graph is a tool to describe the biology, so any graph structure means little without it. . If you want to evaluate how well the graph represents the biology, maybe the best way forward would be to infer cell-type labels (or use a dataset with labels) and look at the normalized mutual information between clusters and the labels. The clusters would have to be obtained in the same way for each graph (e.g. modularity optimization at a fixed resolution). Depending on how specific the labels are you will get a different result though ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/223#issuecomment-409960942:1502,Depend,Depending,1502,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223#issuecomment-409960942,1,['Depend'],['Depending']
Integrability,"At the moment scanpy seems to be compatible with only python >=3.7,<3.11, and it took me quite long to realise that the installation problem was due to having python 3.11 which is not yet compatible with scanpy. This appears as an error when running `pip install scanpy`, but not when running `conda install -c conda-forge scanpy`. Maybe compatibility with python versions could be mentioned on the installation manual, or be displayed as an error message when installing through conda.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369:448,message,message,448,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369,1,['message'],['message']
Integrability,"At the moment we're trying to clean up `scIB` that it becomes easier to use. We're still not certain how to best deal with metrics that rely on R and C++ code though. The current plan is to make a more usable pypi package where some metrics give you a warning on additional requirements/manual C++ compilation. Apologies for the usability mess that a package that also assesses usability has become ^^. I'd prefer to keep it separate for now to facilitate maintenance and citation though. That being said, maybe we could think about an optional requirement for scIB to integrate them? At least when we've cleaned up our side of things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/915#issuecomment-763835114:569,integrat,integrate,569,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-763835114,2,['integrat'],['integrate']
Integrability,"At the stage of finding neighbors, my jupyter kept showing this error:; <img width=""1103"" alt=""Screen Shot 2022-10-22 at 2 51 46 PM"" src=""https://user-images.githubusercontent.com/99854950/197325988-9a22e635-43df-4461-9d22-81f160fa652b.png"">. the error:; ```; OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.; And it killed the kernel entirely. ; ```. I try to make this work by running this in Linux but it got killed again. ; <img width=""281"" alt=""Screen Shot 2022-10-22 at 3 13 47 PM"" src=""https://user-images.githubusercontent.com/99854950/197326001-b8dbd92d-332a-40c6-a9b1-e6c3c0f68a6f.png"">. Below is my basic workflow:; ```python; def pp(adata):; sc.pp.filter_cells(adata, min_genes=200) #get rid of cells with fewer than 200 genes; sc.pp.filter_genes(adata, min_cells=3) #get rid of genes that are found in fewer than 3 cells; adata.var['mt'] = adata.var_names.str.startswith('MT-') # annotate the group of mitochondrial genes as 'mt'; sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True); upper_lim = np.quantile(adata.obs.n_genes_by_counts.values, .98); lower_lim = np.quantile(adata.obs.n_genes_by_counts.values, .02); adata = adata[(adata.obs.n_genes_by_counts < upper_lim) & (adata.obs.n_genes_by_counts > lower_lim)]; adata = adata[adata.obs.pct_counts_mt < 25]; sc.pp.normalize_total(adata, target_sum=1e4) #normalize every cell to 10,000 UMI; sc.pp.log1p(adata) #change to log counts; sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5) #these are default values; adata.raw = adata #save raw data before processing values and further filtering; adata = adata[:, adata.var.highly_variable] #filter highly variable; sc.pp.regress_out(adata, ['total_counts', 'pct_counts_mt']) #Regress out effects of total counts per cell and the percentage of mitochondrial genes expressed; sc.pp.scale(adata, max_value=10) #scale each gene to unit variance; sc.tl.pca(adata, svd_solver=",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2359:291,rout,routine,291,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2359,1,['rout'],['routine']
Integrability,Backport PR #1608 on branch 1.7.x (Remove dependency on scvelo for doc builds),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1609:42,depend,dependency,42,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1609,1,['depend'],['dependency']
Integrability,Backport PR #1608: Remove dependency on scvelo for doc builds,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1609:26,depend,dependency,26,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1609,1,['depend'],['dependency']
Integrability,Backport PR #1659 on branch 1.7.x (Fix passing of arguments between scrublet routines),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1674:77,rout,routines,77,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1674,1,['rout'],['routines']
Integrability,Backport PR #1659: Fix passing of arguments between scrublet routines,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1674:61,rout,routines,61,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1674,1,['rout'],['routines']
Integrability,Backport PR #1707 on branch 1.7.x (Add sparsificiation step before sparse-dependent Scrublet calls),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1711:74,depend,dependent,74,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1711,1,['depend'],['dependent']
Integrability,Backport PR #1707: Add sparsificiation step before sparse-dependent Scrublet calls,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1711:58,depend,dependent,58,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1711,1,['depend'],['dependent']
Integrability,Backport PR #2222 on branch 1.9.x (Add CI job with minimal dependencies),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2226:59,depend,dependencies,59,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2226,1,['depend'],['dependencies']
Integrability,Backport PR #2222: Add CI job with minimal dependencies,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2226:43,depend,dependencies,43,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2226,1,['depend'],['dependencies']
Integrability,"Based on my experience setting a single cutoff for all datasets will not work, as I've used a lot of different cutoffs depending on the distributions. I would echo @ivirshup's suggestion of looking at distributions. Joint distributions being a lot more important than individual histograms. There's a small discussion about it in our [best practices paper](https://www.embopress.org/lookup/doi/10.15252/msb.20188746)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/718#issuecomment-507264814:119,depend,depending,119,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/718#issuecomment-507264814,1,['depend'],['depending']
Integrability,"Blues"", ncols=5); 27 ; 28 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/scatterplots.py in umap(adata, **kwargs); 27 If `show==False` a `matplotlib.Axis` or a list of it.; 28 """"""; ---> 29 return plot_scatter(adata, basis='umap', **kwargs); 30 ; 31 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/scatterplots.py in plot_scatter(adata, color, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, basis, groups, components, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs); 280 if sort_order is True and value_to_plot is not None and categorical is False:; 281 order = np.argsort(color_vector); --> 282 color_vector = color_vector[order]; 283 _data_points = data_points[component_idx][order, :]; 284 . h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/h5py/_hl/dataset.py in __getitem__(self, args); 474 ; 475 # Perform the dataspace selection.; --> 476 selection = sel.select(self.shape, args, dsid=self.id); 477 ; 478 if selection.nselect == 0:. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/h5py/_hl/selections.py in select(shape, args, dsid); 70 elif isinstance(arg, np.ndarray):; 71 sel = PointSelection(shape); ---> 72 sel[arg]; 73 return sel; 74 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/h5py/_hl/selections.py in __getitem__(self, arg); 210 """""" Perform point-wise selection from a NumPy boolean array """"""; 211 if not (isinstance(arg, np.ndarray) and arg.dtype.kind == 'b'):; --> 212 raise TypeError(""PointSelection __getitem__ only works with bool arrays""); 213 if not arg.shape == self.shape:; 214 raise TypeError(""Boolean indexing array has incompatible shape""). TypeError: PointSelection __getitem__ only works with bool arrays. <Figure size 1978.56x288 with 0 Axes",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/440:1828,wrap,wrapper,1828,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/440,1,['wrap'],['wrapper']
Integrability,Building the docs fails ever since the version of `sphinx-autodoc-typehints` increased from v1.12.0 to v1.13.0. This PR temporarily pins this dependency's version to v1.12.0. See https://github.com/theislab/scanpy/pull/1828#issuecomment-1005072811 and cc @Zethson,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2100:142,depend,dependency,142,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2100,1,['depend'],['dependency']
Integrability,But I think it's a little different. It's probably easier to implement since we still have all dependencies available at collection time.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2211#issuecomment-1088708723:95,depend,dependencies,95,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2211#issuecomment-1088708723,1,['depend'],['dependencies']
Integrability,"But I think scanpydoc is very confused now for some reason. Documentation build is broken, it's visible in ~all~ some recent PRs too and there is not much we can do without the help of @falexwolf or @flying-sheep or @ivirshup, because we cannot even see the error message. My local builds are just fine 🤷",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1204#issuecomment-645460915:264,message,message,264,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1204#issuecomment-645460915,1,['message'],['message']
Integrability,"CCA does not have code in python, which will make it difficult to integrate, pySCENIC is probably easier but I would rather ask the developers. @falexwolf We should consider a way to facilitate scanpy 'plugins'. A quick search shows me that this could be possible: https://packaging.python.org/guides/creating-and-discovering-plugins/ but honestly I don't know how it works. Nevertheless, given the number of tools that continue to appear we should consider a scheme that facilitate how developers can take advantage of scanpy preprocessing, storing, analysis and visualization tools.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/265#issuecomment-423514211:66,integrat,integrate,66,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-423514211,2,['integrat'],['integrate']
Integrability,"Can we use Dask to speed up the preprocessing phase of Scanpy by taking advantage of multiple CPUs (or GPUs)?. **TLDR**: Dask can speed up Zheng17, but it needs lots of cores and a rewritten implementation. CuPy (for GPUs) has missing operations required by Zheng17, so more work is needed for Dask with GPUs. ### Investigation. Dask is mainly used with dense arrays, however the arrays in Scanpy are sparse (for most of the preprocessing phase at least). I tried looking at [pydata sparse](https://sparse.pydata.org/en/latest/) with Dask, but it ran a lot slower than regular [`scipy.sparse`](https://docs.scipy.org/doc/scipy/reference/sparse.html) (which is what Scanpy uses). So I wrote a [wrapper](https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/sparsearray/_scipy_sparse.py) around `scipy.sparse` to implement NumPy's `__array_function__` protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular `scipy.sparse`. However, when I first tried running the whole Zheng17 recipe, `scipy.sparse` was always faster than Dask with `scipy.sparse`, even with many cores (e.g. 64). It turned out that by using Anndata arrays, Dask has to materialize intermediate data more than is necessary in order to populate the Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). To avoid this complication I rewrote the Zheng17 recipe to do all the NumPy array computations and then construct an Anndata representation at the end,; to take advantage of Dask's deferred processing of lazy values. (See https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/preprocessing/_dask_optimized.py#L115 for the code.). With this change, running on the 1M neurons dataset with 64 cores `scipy.sparse` tak",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/921:693,wrap,wrapper,693,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921,2,"['protocol', 'wrap']","['protocol', 'wrapper']"
Integrability,"Can you call `del adata.uns['cell_ontology_class_colors']`? This should throw a better error message... I can do that soon, I wonder how you managed to produce the error... cannot be anything related to a recent update... Hm.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/363#issuecomment-439745461:93,message,message,93,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363#issuecomment-439745461,1,['message'],['message']
Integrability,"Can you point to a package whose test organization you would like our tests to emulate?. I find pytests docs rather hard to navigate and would really prefer to see an example of what you're advocating for. From your description above I had thought you didn't want to emulate [pandas use of conftest](https://github.com/pandas-dev/pandas/blob/main/pandas/conftest.py). -----------. > Would you accept a PR that simply moves the test utils into private submodules of scanpy.testing. I'd lean towards it, but I fully expect issues like #685 to come up. This is why I'd like to see a working example of what you want to work towards. ------------. > switches the import mode to (future default, drawback-less) importlib?. Is it definitely the future default? It looks like they are walking that back. Current versions of pytest docs say:. > [We intend to make importlib the default in future releases, depending on feedback.](https://docs.pytest.org/en/latest/explanation/pythonpath.html#import-modes). Where it previously said:. > [We intend to make importlib the default in future releases.](https://docs.pytest.org/en/6.2.x/pythonpath.html#import-modes)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2225#issuecomment-1096718863:898,depend,depending,898,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2225#issuecomment-1096718863,2,['depend'],['depending']
Integrability,"Can you show us the values of `combined_bbknn.obs['scNym']`?. Also, if you create a conda environment, does your problems still occur? I'm wondering if some dependencies like `pandas` could be out of date.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1701#issuecomment-787864572:157,depend,dependencies,157,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1701#issuecomment-787864572,1,['depend'],['dependencies']
Integrability,"Caught it. I had forgotten that arguments only get evaluated once, so if you mutate them, there is state which is maintained to other calls. I think the unhelpful `abort` message is from `louvain-igraph` expecting a weight vector of the right shape, which ended up with the error:. ```; libc++abi.dylib: terminating with uncaught exception of type char const*; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/248#issuecomment-419698370:171,message,message,171,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248#issuecomment-419698370,1,['message'],['message']
Integrability,"Changing it to a property throws a different error:. <details>; <summary> from make html </summary>. ```sh; reading sources... [ 5%] generated/classes/scanpy.pl.DotPlot ; Exception occurred:; File ""/usr/local/lib/python3.8/site-packages/sphinx/util/docfields.py"", line 369, in transform; new_list += fieldtype.make_field(fieldtypes, self.directive.domain, items,; TypeError: make_field() got an unexpected keyword argument 'inliner'; The full traceback has been saved in /var/folders/bd/43q20k0n6z15tdfzxvd22r7c0000gn/T/sphinx-err-qbzn5se8.log, if you want to report the issue to the developers.; Please also report this if it was a user error, so that a better error message can be provided next time.; A bug report can be filed in the tracker at <https://github.com/sphinx-doc/sphinx/issues>. Thanks!; make: *** [html] Error 2; ```. </details>. <details>; <summary> contents of the referenced log file </summary>. ```python; # Sphinx version: 4.1.0; # Python version: 3.8.10 (CPython); # Docutils version: 0.16 release; # Jinja2 version: 2.11.2; # Last messages:; # reading sources... [ 2%] dev/documentation; # reading sources... [ 2%] dev/external-tools; # reading sources... [ 3%] dev/getting-set-up; # reading sources... [ 3%] dev/index; # reading sources... [ 3%] dev/release; # reading sources... [ 4%] dev/testing; # reading sources... [ 4%] dev/versioning; # reading sources... [ 4%] ecosystem; # reading sources... [ 5%] external; # reading sources... [ 5%] generated/classes/scanpy.pl.DotPlot; # Loaded extensions:; # sphinx.ext.mathjax (4.1.0) from /usr/local/lib/python3.8/site-packages/sphinx/ext/mathjax.py; # sphinxcontrib.applehelp (1.0.2) from /usr/local/lib/python3.8/site-packages/sphinxcontrib/applehelp/__init__.py; # sphinxcontrib.devhelp (1.0.2) from /usr/local/lib/python3.8/site-packages/sphinxcontrib/devhelp/__init__.py; # sphinxcontrib.htmlhelp (2.0.0) from /usr/local/lib/python3.8/site-packages/sphinxcontrib/htmlhelp/__init__.py; # sphinxcontrib.serializinghtml (1.1.5",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1946#issuecomment-877995557:668,message,message,668,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1946#issuecomment-877995557,1,['message'],['message']
Integrability,"Checks if current axis is colorbar before trying to set the name, see #2681.; This might not be the best solution and does not yet integrate a unit test. <!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2682:131,integrat,integrate,131,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2682,1,['integrat'],['integrate']
Integrability,"Closing in favor of #1116, where I integrated the commit",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1109#issuecomment-600094624:35,integrat,integrated,35,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1109#issuecomment-600094624,1,['integrat'],['integrated']
Integrability,Cluster content plot for integrated data,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1573:25,integrat,integrated,25,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573,1,['integrat'],['integrated']
Integrability,"Companion PR to https://github.com/theislab/anndata/pull/434. Basically, I would like to deprecate the `dtype` argument of `AnnData._init_as_actual`, since it mostly just makes unexpected copies of `X`. Since other elements of an AnnData object are passed by reference, it makes sense for this to happen with `X` as well. Right now, this PR will fail CI. What I've done so far is remove all uses of that argument from the scanpy code base, while keeping the tests passing. I'm trying to figure out how to best preserve compatibility with older versions of `anndata`, without throwing too many warnings. I think the thing to do will be make code work with both (`AnnData(X.astype(dtype), dtype=dtype))` should only make one copy) and catch warnings. This can be removed once scanpy depends on `anndata 0.8`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1430:781,depend,depends,781,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1430,1,['depend'],['depends']
Integrability,"CompilerBase._compile_core(self); 461 res = None; 462 try:; --> 463 pm.run(self.state); 464 if self.state.cr is not None:; 465 break. File ~/miniconda3/envs/test/lib/python3.10/site-packages/numba/core/compiler_machinery.py:353, in PassManager.run(self, state); 350 msg = ""Failed in %s mode pipeline (step: %s)"" % \; 351 (self.pipeline_name, pass_desc); 352 patched_exception = self._patch_error(msg, e); --> 353 raise patched_exception. File ~/miniconda3/envs/test/lib/python3.10/site-packages/numba/core/compiler_machinery.py:341, in PassManager.run(self, state); 339 pass_inst = _pass_registry.get(pss).pass_inst; 340 if isinstance(pass_inst, CompilerPass):; --> 341 self._runPass(idx, pass_inst, state); 342 else:; 343 raise BaseException(""Legacy pass in use""). File ~/miniconda3/envs/test/lib/python3.10/site-packages/numba/core/compiler_lock.py:35, in _CompilerLock.__call__.<locals>._acquire_compile_lock(*args, **kwargs); 32 @functools.wraps(func); 33 def _acquire_compile_lock(*args, **kwargs):; 34 with self:; ---> 35 return func(*args, **kwargs). File ~/miniconda3/envs/test/lib/python3.10/site-packages/numba/core/compiler_machinery.py:296, in PassManager._runPass(self, index, pss, internal_state); 294 mutated |= check(pss.run_initialization, internal_state); 295 with SimpleTimer() as pass_time:; --> 296 mutated |= check(pss.run_pass, internal_state); 297 with SimpleTimer() as finalize_time:; 298 mutated |= check(pss.run_finalizer, internal_state). File ~/miniconda3/envs/test/lib/python3.10/site-packages/numba/core/compiler_machinery.py:269, in PassManager._runPass.<locals>.check(func, compiler_state); 268 def check(func, compiler_state):; --> 269 mangled = func(compiler_state); 270 if mangled not in (True, False):; 271 msg = (""CompilerPass implementations should return True/False. ""; 272 ""CompilerPass with name '%s' did not.""). File ~/miniconda3/envs/test/lib/python3.10/site-packages/numba/core/typed_passes.py:306, in ParforPass.run_pass(self, state); 295 assert state.fun",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2191:7734,wrap,wraps,7734,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2191,1,['wrap'],['wraps']
Integrability,"Completely agree, Gökcen!. How I just thought about dealing with this in the past couple of minutes: could we not make a submodule *rtools*? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a `scanpy-contrib`: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: `anndata` is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/125#issuecomment-381984759:169,wrap,wrapper,169,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-381984759,5,"['depend', 'interface', 'wrap']","['dependencies', 'interfaces', 'wrapper', 'wrappers']"
Integrability,"Cool! . > * Mask out genes which aren't expressed in the compared groups (since there's not too much point in getting and correcting a pvalue for them). I think masking out might be problematic because, `n_genes=adata.n_vars` should return all genes in any case. . > * Revert change (would bring back issue of genes with variance of 0). I feel like using scipy function will slightly increase the maintainability (and simplicity) of the code, so I'm fine with keeping the scipy switch. > * Wrap the t-test with something like `np.errstate` to hide the warning. This sounds good. Replacing weird scipy warning with a proper scanpy warning would also make sense.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/629#issuecomment-489105754:490,Wrap,Wrap,490,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/629#issuecomment-489105754,1,['Wrap'],['Wrap']
Integrability,"Cool! I'm of course happy to work together! :smile: And, of course, a Python implementation is way cooler than an R one. :wink:. The last addition to Scanpy was an interface for [pypairs](https://github.com/theislab/scanpy/blob/master/scanpy/tools/pypairs.py). It's still not in the docs but will be very soon, I'm right now missing a link to a short example, there. I'm guess you're planning to put your own package on PyPI, right? If yes, than it's not so crucial: for Scanpy, I changed from cython to numba. Many people currently do that, if you try numba, you'll realize why. So, I'd recommend to use numba if you need to speedup something.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/125#issuecomment-384310269:164,interface,interface,164,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-384310269,1,['interface'],['interface']
Integrability,"Cool! Very happy to get another pull request for an interface to `mnnpy`. :smile:. Regarding writing it in C: I disagree, numba-boosted Python code is much nicer for these type of ""relatively simple"" algorithms... . Regarding `rtools`. I'll remove it from the `api` but leave it in scanpy as an example for how one could wrap other r packages... No user will notice that...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/125#issuecomment-384463485:52,interface,interface,52,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-384463485,4,"['interface', 'wrap']","['interface', 'wrap']"
Integrability,"Cool!. Initially, Scanpy came with a built-in command-line interface. But I threw it out after a couple of months as it was hard to maintain both the interface and the library at the same time, I also expected a bit too much from the command-line interface at the time, I guess. I'd really like to support your much better efforts for generating command line interfaces. But I'm a bit hesitant to maintain it here in the main repo - already the library is already so much to take care of and other libraries typically don't involve a ""script-wrapper"", too. What about a PyPI package `scanpy-scripts`, which directly advertise from the Scanpy main docs, we could also have a little tutorial there. Similar to that, there will be interactive visual interfaces to scanpy, again as separate packages. I tend to think that this would be the better option. But if I miss something fundamental, please convince me of the opposite.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/281#issuecomment-427044578:59,interface,interface,59,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281#issuecomment-427044578,6,"['interface', 'wrap']","['interface', 'interfaces', 'wrapper']"
Integrability,"Cool, @Koncopd! Looks good! @tomwhite, thank you for your comments!. @lmcinnes: are the functions like `nearest_neighbors` and `fuzzy_simplicial_set` stable enough to be used externally? They are not re-exported to the user API; a year ago, I was afraid that code that depended on them might break if changes were made, even if it's just renaming parameters etc. Do you announce chances that might break backwards compat for these functions?. @Koncopd ; > But this fuzzy_simplicial_set doesn't calculate distances, only connectivities. What is the right approach to solve this? Writing PR to umap that adds distances as a return value for fuzzy_simplicial_set?. I'd guess that @lmcinnes wouldn't want to blow this up with another return value. Why can't we just use the return value for `nearest_neighbors` and construct the distance graph as it's done right now in Scanpy? In any case, it doesn't block depending on UMAP...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/522#issuecomment-477325247:269,depend,depended,269,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/522#issuecomment-477325247,2,['depend'],"['depended', 'depending']"
Integrability,Cool... I haven't used the new plots yet. Looks very handy. Your wrapper is an automated use of `var_group_X` to label marker genes of particular clusters in these plots?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/646#issuecomment-492693756:65,wrap,wrapper,65,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/646#issuecomment-492693756,1,['wrap'],['wrapper']
Integrability,Could you suggest some error handling behavior here? I think there could definitely be a more helpful error message.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1504#issuecomment-732724704:108,message,message,108,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1504#issuecomment-732724704,1,['message'],['message']
Integrability,"Could you throw a more informative error message for `copy=False`? Maybe:. `NotImplementedError(""Inplace subsampling is not implemented for backed objects"")`. ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2624#issuecomment-1691514632:41,message,message,41,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2624#issuecomment-1691514632,1,['message'],['message']
Integrability,Critical updates to Palantir external tool wrapper. Includes updated DocString.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1245:43,wrap,wrapper,43,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1245,1,['wrap'],['wrapper']
Integrability,"Currently there's only a wrapper for `scanorama.integrate_scanpy`. From https://github.com/brianhie/scanorama: . > The function integrate_scanpy() will simply add an entry into adata.obsm called 'X_scanorama' for each adata in adatas. obsm['X_scanorama'] contains the low dimensional embeddings as a result of integration, which can be used for KNN graph construction, visualization, and other downstream analysis. ; > The function correct_scanpy() is a little more involved -- it will create new AnnData objects and replace adata.X with the Scanorama-transformed cell-by-gene matrix, while keeping the other metadata in adata as well.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2323:25,wrap,wrapper,25,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2323,2,"['integrat', 'wrap']","['integration', 'wrapper']"
Integrability,"Currently, `sc.tl.louvain` etc return cluster assignments as a Categorical with dtype `str` resulting in incompatibility with matplotlib color sequences. For example, the following code raises a ValueError:. ```python; import numpy as np; import scanpy as sc; import matplotlib.pyplot as plt. adata = sc.AnnData(np.random.normal(size=(100,2))); sc.pp.neighbors(adata); sc.tl.louvain(adata); plt.scatter(adata.X[:,0], adata.X[:,1], c=adata.obs['louvain']); ```. The error is: `ValueError: RGBA values should be within 0-1 range`. Funnily enough, this used to work due to a bug in matplotlib that was fixed in https://github.com/matplotlib/matplotlib/pull/13913. Note, the following code works as intended:. ```python; plt.scatter(adata.X[:,0], adata.X[:,1], c=adata.obs['louvain'].astype(int)); ```. I would have submitted a PR changing this behavior had I not noticed that returning cluster assignments as `str` is explicitly checked here:. https://github.com/theislab/scanpy/blob/78125e6355c0cd2c4ae930495829282eea6f4a52/scanpy/tools/_utils_clustering.py#L11-L23. This brings up a larger design question in scanpy / anndata: *Why are arrays of numerics routinely converted to strings representing numbers?*. In `https://github.com/theislab/anndata/issues/311` I found a case where converting arrays of numerics to strings creates a bug when assigning to AnnData `obsm` with DataFrames with a RangeIndex. In that case, I understand there's a desire to avoid ambiguity in positional vs label indexing, but that issue was solved in pandas with the `.loc` and `.iloc` conventions. Why not carry that forward?. In this case, why not just return cluster assignments as arrays of numerics as is done in `sklearn.cluster`? . I think following these conventions will make both tools much more accessible to the general Python data science community.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030:1154,rout,routinely,1154,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030,1,['rout'],['routinely']
Integrability,DCA integration,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/142:4,integrat,integration,4,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/142,1,['integrat'],['integration']
Integrability,Dask no longer having fsspec as a required dependency means the dask tests fail. This should fix that.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/765:43,depend,dependency,43,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/765,1,['depend'],['dependency']
Integrability,"Data object for each sample; 2 for sample in sample_list:; ----> 3 sample_adata = adata[:, adata.obs['sample_name'] == sample]. File ~/opt/anaconda3/lib/python3.9/site-packages/anndata/_core/anndata.py:1113, in AnnData.__getitem__(self, index); 1111 def __getitem__(self, index: Index) -> ""AnnData"":; 1112 """"""Returns a sliced view of the object.""""""; -> 1113 oidx, vidx = self._normalize_indices(index); 1114 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ~/opt/anaconda3/lib/python3.9/site-packages/anndata/_core/anndata.py:1094, in AnnData._normalize_indices(self, index); 1093 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:; -> 1094 return _normalize_indices(index, self.obs_names, self.var_names). File ~/opt/anaconda3/lib/python3.9/site-packages/anndata/_core/index.py:36, in _normalize_indices(index, names0, names1); 34 ax0, ax1 = unpack_index(index); 35 ax0 = _normalize_index(ax0, names0); ---> 36 ax1 = _normalize_index(ax1, names1); 37 return ax0, ax1. File ~/opt/anaconda3/lib/python3.9/site-packages/anndata/_core/index.py:90, in _normalize_index(indexer, index); 88 elif issubclass(indexer.dtype.type, np.bool_):; 89 if indexer.shape != index.shape:; ---> 90 raise IndexError(; 91 f""Boolean index does not match AnnData’s shape along this ""; 92 f""dimension. Boolean index has shape {indexer.shape} while ""; 93 f""AnnData index has shape {index.shape}.""; 94 ); 95 positions = np.where(indexer)[0]; 96 return positions # np.ndarray[int]. IndexError: Boolean index does not match AnnData’s shape along this dimension. Boolean index has shape (5258,) while AnnData index has shape (17143,). ```; I would appreciate any insights. Thank you so much! ; #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. this code had a error message because scanpy wasn't defined, but when I ran . from importlib.metadata import version; version('scanpy'). I got an output: '1.9.1'. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2402:3241,message,message,3241,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2402,1,['message'],['message']
Integrability,"Dear @wflynny. You're completely right, I added to the documentation a note that the whole topic is under debate ([here](http://scanpy.readthedocs.io/en/latest/api/index.html#preprocessing-pp)). Generally, Scanpy aims to enable access to different tools via the same data object and consistent interfaces so that users can conveniently try out different tools. The threshold for including an interface in Scanpy is low and only requires that a preprint/paper together with a solid GitHub repository exist.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/189#issuecomment-405181668:294,interface,interfaces,294,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189#issuecomment-405181668,2,['interface'],"['interface', 'interfaces']"
Integrability,"Dear author,. Can bbknn integrate multiple variables？such as Platform and Individual. Looking forward your reply; Siyu. >>> sc.external.pp.bbknn(mydata, batch_key=[""Platform"",""Individual_2""],n_pcs=50,set_op_mix_ratio=ratio). computing batch balanced neighbors; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/home/scCell2/miniconda3/lib/python3.8/site-packages/scanpy/external/pp/_bbknn.py"", line 134, in bbknn; return bbknn(; File ""/home/scCell2/miniconda3/lib/python3.8/site-packages/bbknn/__init__.py"", line 110, in bbknn; if batch_key not in adata.obs:; File ""/home/scCell2/miniconda3/lib/python3.8/site-packages/pandas/core/generic.py"", line 1721, in __contains__; return key in self._info_axis; File ""/home/scCell2/miniconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py"", line 4071, in __contains__; hash(key); TypeError: unhashable type: 'list",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2004:24,integrat,integrate,24,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2004,1,['integrat'],['integrate']
Integrability,"Dear both, . correlation matrices are available now. Following our usual split into tools and plotting, you can call . `sc.tl.correlation_matrix(adata,name_list, n_genes=20, annotation_key=None, method='pearson')`. for correlation matrix calculation. ; I have left out a few parameters because I wrote the function actually to conveniently plot results from DE testing, but the basic functionality is the following: . _adata_ is the usual AnnData object you are working with. ; _name_list_ is a string containing gene names and should be specified. ; _n_genes_ cuts the name_list if the number specified is smaller then the length of the list, so set this high enough if you want to work with large data ; _annotation_key_ allows you to specify a string that works as the key in the AnnData object where results are stored. By default, the key is ""Correlation_matrix"". The method basically wraps the pd.DataFrame.corr method, which allows you to specify the correlation method ('pearson', 'spearman', 'kendall'). . I use it for smaller data so it has not been optimized for performance (yet), but I tested the method for 3k cells and 600 genes and ended up with a runtime of ~8 seconds. I hope that is conveniently fast enough for you (if not let us know). . After calling the tool, you can plot correlation matrices (using a wrapper for seaborn heatmap) by calling. `sc.pl.correlation_matrix(adata, annotation_key=None)`. This function searches basically only the AnnData annotation (again, if no key specified, ""Correlation_matrix"" is the default). Hope this does the job!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/72#issuecomment-361891662:890,wrap,wraps,890,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/72#issuecomment-361891662,2,['wrap'],"['wrapper', 'wraps']"
Integrability,"Dear professor&nbsp;Philipp A. Thank you of your patience.; I've attached my Anndata and codes in the attachment. Kind regards. ; Original Email; ; . Sender:""Philipp A.""< ***@***.*** &gt;;. Sent Time:2024/6/7 19:28. To:""scverse/scanpy""< ***@***.*** &gt;;. Cc recipient:""FessenSimon""< ***@***.*** &gt;;""Author""< ***@***.*** &gt;;. Subject:Re: [scverse/scanpy] unexpected error in sc.pl.dpt_timeseries anddpt_groups_pseudotime (Issue #3086). ; Please create a fully reproducible example. I can’t help if I don’t have an AnnData object that doesn’t behave like yours.; ; —; Reply to this email directly, view it on GitHub, or unsubscribe.; You are receiving this because you authored the thread.Message ID: ***@***.***&gt;; . 	; 	 		 			从QQ邮箱发来的超大附件 	; 	 		 				 					 						 							 						 					; 					 						 							repro.zip 							 (321.0MB, 2024年7月8日 16:26) 						 						 							进入下载页面 							：https://wx.mail.qq.com/ftn/download?func=3&k=ca9c3c356e51f263f8ef48353a6462397a35692138646239104c1e410b56560c535b4b025c050314505550571501005a034e055008575700575252015d546b3947061647574a1850457756363f313de49719e585e08cd2a2c51d8b742d5c&key=ca9c3c356e51f263f8ef48353a6462397a35692138646239104c1e410b56560c535b4b025c050314505550571501005a034e055008575700575252015d546b3947061647574a1850457756363f313de49719e585e08cd2a2c51d8b742d5c&code=5cf58db9&from=",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3086#issuecomment-2155872130:692,Message,Message,692,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3086#issuecomment-2155872130,1,['Message'],['Message']
Integrability,"Dear, . Is it possible to integrate scanpy with CCA and pyscenic?; CCA (canonical correlation analysis to alignment different datasets and batch effect correction):; https://satijalab.org/seurat/immune_alignment.html. pyscenic (single-cell regulatory network inference and clustering):; https://github.com/aertslab/pySCENIC",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/265:26,integrat,integrate,26,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265,1,['integrat'],['integrate']
Integrability,"Definitely a heavy dependency, you should see the size of the conda environment you need to test it. I think it'd be useful for playing around with ideas on how you'd like to aggregate and scale the values, since they've already got a bunch of methods implemented. Plus the plots often look pretty good.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/575#issuecomment-479510188:19,depend,dependency,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575#issuecomment-479510188,1,['depend'],['dependency']
Integrability,Depend on session-info2,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3202:0,Depend,Depend,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3202,1,['Depend'],['Depend']
Integrability,Dependencies missing in bioconda package,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2000:0,Depend,Dependencies,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2000,1,['Depend'],['Dependencies']
Integrability,Dependency on `legacy-api-wrap` prevents 1.10 conda release,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2966:0,Depend,Dependency,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2966,2,"['Depend', 'wrap']","['Dependency', 'wrap']"
Integrability,"Depends on how we're feeling about semantic versioning. 2.0 for a complete switch, but we could internally switch over with a compatibility layer and deprecation warnings anytime before then.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1340#issuecomment-666331545:0,Depend,Depends,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1340#issuecomment-666331545,1,['Depend'],['Depends']
Integrability,Depends on how you calculate your neighbors graph: https://scanpy.readthedocs.io/en/stable/generated/scanpy.pp.neighbors.html#scanpy.pp.neighbors,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2498#issuecomment-1576510979:0,Depend,Depends,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2498#issuecomment-1576510979,1,['Depend'],['Depends']
Integrability,"Didn’t you rephrase the message?. > scanpy/tests/test_read_10x.py: +3 -1; > ; > This above file has < {thresh} changes to black formatting. Please black format it and afterwards remove it from “tool.black.exclude"" in pyproject.toml. Anyway, it should be “remove it from ‘tool.black.exclude’ *and then* black-format it”, as black won’t run on it if it’s excluded.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/989#issuecomment-577155563:24,message,message,24,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/989#issuecomment-577155563,1,['message'],['message']
Integrability,Do you also mean minimal dependency versions? Because Rust’s cargo e.g. has `carg update -Z minimum-versions` which allows people to figure out if their minimum version bounds are truthful or lies (i.e. to be raised),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2211#issuecomment-1088701942:25,depend,dependency,25,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2211#issuecomment-1088701942,1,['depend'],['dependency']
Integrability,"Do you get an exception message or something else? If you can also copy paste the error message here, we can debug it more easily.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/749#issuecomment-515127872:24,message,message,24,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749#issuecomment-515127872,2,['message'],['message']
Integrability,"Do you mean that If I want to do use `scanpy.tl.louvain`, I can use `scanpy.tl.leiden` instead? I can `pip install scanpy[leiden]` but it will not change the error message in `scanpy.tl.louvain` with option `flavor='vtraag'`. When I try `pip install scanpy[louvain]`, it has the error `legacy-install-failure`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1283#issuecomment-1638255295:164,message,message,164,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1283#issuecomment-1638255295,1,['message'],['message']
Integrability,"Does this cause the same issue?. ```python; import numpy as np; import umap. umap.UMAP().fit_transform(np.random.randn(10_000, 20)); ```. And when you say ""dies"", is there a segfault message, or are you seeing a jupyter kernel failure message?. In general, this sounds like a numba issue. I'd recommend taking searching the `umap-learn` or `numba` repositories for similar issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1567#issuecomment-754547843:183,message,message,183,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567#issuecomment-754547843,4,['message'],['message']
Integrability,"Don't know what this ""review"" process is , but basically it is ready. . De: ""Lukas Heumos"" ***@***.***> ; À: ""theislab/scanpy"" ***@***.***> ; Cc: ""Yves Le Feuvre"" ***@***.***>, ""Mention"" ***@***.***> ; Envoyé: Jeudi 6 Janvier 2022 20:11:35 ; Objet: Re: [theislab/scanpy] Pca loadings n points patch (PR #2075) . [ https://github.com/Yves33 | @Yves33 ] is this ready for review? . — ; Reply to this email directly, [ https://github.com/theislab/scanpy/pull/2075#issuecomment-1006847333 | view it on GitHub ] , or [ https://github.com/notifications/unsubscribe-auth/ACEYIQUT75OTZC3MUGVAT3DUUXSOPANCNFSM5JWG2IZQ | unsubscribe ] . ; Triage notifications on the go with GitHub Mobile for [ https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675 | iOS ] or [ https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub | Android ] . ; You are receiving this because you were mentioned. Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2075#issuecomment-1007917047:1005,Message,Message,1005,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2075#issuecomment-1007917047,1,['Message'],['Message']
Integrability,"Dot sizes now work, this is ready to be merged... assuming you are happy with how I integrated it into the documentation (I added a separate ""density"" subsection for the plotting tools as I felt it shouldn't really be put in the same category as embeddings.)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/543#issuecomment-475595551:84,integrat,integrated,84,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/543#issuecomment-475595551,1,['integrat'],['integrated']
Integrability,"E.g. matplotlib is only necessary when plotting, and for e.g. Docker images, it would be useful to have a slim scanpy core. An idea would be to do it like Jupyter:. - A `scanpy-core` PyPI package with just the essentials.; - A `scanpy` metapackage, which depends on `scanpy-core` and most (or all) of the optional dependencies. Users doing `pip install scanpy` will get the full package, with no annoying runtime errors, and packagers needing flexibility get `scanpy-core` and can slim everything down as needed. cc @hensing",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/59:255,depend,depends,255,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/59,2,['depend'],"['dependencies', 'depends']"
Integrability,"Emm, the code is just from the scanpy tutorial.; https://scanpy-tutorials.readthedocs.io/en/latest/integrating-data-using-ingest.html. I run ingest on pbmc dataset, then meet this problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1951#issuecomment-883853578:99,integrat,integrating-data-using-ingest,99,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1951#issuecomment-883853578,1,['integrat'],['integrating-data-using-ingest']
Integrability,Error message in tl.diffmap / why n_comps must be > 2,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/668:6,message,message,6,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/668,1,['message'],['message']
Integrability,Even cooler!. Do you recall whether the memory usage for a million cells was anything prohibitive?. I was actually thinking of utilizing distribution across machines as a way to scale out if memory usage started becoming an issue (or even just using the same batching strategy on one machine). This could be very useful for large scale dataset integration. Though I'm curious if using nn-descent could introduce some bias towards merging data.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/659#issuecomment-495306812:344,integrat,integration,344,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659#issuecomment-495306812,1,['integrat'],['integration']
Integrability,"Everything runs fine on the current master branch, I uploaded the current version of the notebook: https://nbviewer.jupyter.org/github/theislab/paga/blob/master/blood/paul15/paul15.ipynb. I'll release either 1.3.3 or 1.4 very soon and if there should have been a bug at some point, it seems to have been fixed at some point. Finally, PAGA is also in the continuous integration tests, so no bugs in the future anymore for this. ;)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/333#issuecomment-435728872:365,integrat,integration,365,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/333#issuecomment-435728872,1,['integrat'],['integration']
Integrability,"Exactly the same error message pops up when inputting `np.int64` data into `sc.pp.log1p()`. This is with the latest scanpy, and using data that has otherwise worked well when not using `sc.pp.downsample_counts()`. I thus wouldn't consider this resolved, although I can open another issue as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/435#issuecomment-475722334:23,message,message,23,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435#issuecomment-475722334,1,['message'],['message']
Integrability,"Exactly, this is why I think it’s a great solution to advertise that repo in the scanpy docs and make scanpy provide a wrapper binary. I added ebi-gene-expression-group/scanpy-scripts#24, let me hear what you think",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/281#issuecomment-437812095:119,wrap,wrapper,119,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281#issuecomment-437812095,1,['wrap'],['wrapper']
Integrability,"Expanding on this a bit, here's an example discourse forum I've found useful: [https://discourse.julialang.org](). Good, relevant threads show up all the time in google searches, while I've spent a lot of time trying to find help on various `pyviz` and `conda` gitters with a pretty low success rate. I think something like gitter/ single slack channel works for about 20 people, but after that it gets a bit rough. Having a single stream of messages means keeping track of a conversation over days becomes a huge task. Topics get dropped not based on the value of the discussion, but because of the conversation's overhead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/542#issuecomment-476057005:442,message,messages,442,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/542#issuecomment-476057005,1,['message'],['messages']
Integrability,"External has outlived its usefulness with the emergence of the scverse ecosystem, and should be removed. ^ This text should be expanded. # TODO. - [ ] Finalize methods to keep/ merge into main namespace; - Currently: `scanorama`, `hashsolo`, `scrublet` or equivalent, maybe `bbknn`; - [ ] Make removal plan (probably deprecation message + maybe email to authors) for; - [ ] `harmony`; - [ ] `dca`; - [ ] `magic`; - [ ] `phate`; - [ ] `palantir`; - [ ] `trimap`; - [ ] `sam`; - [ ] `phenograph`; - [ ] `wishbone`; - [ ] `sandbag`; - [ ] `cyclone`; - [ ] Export functionality for; - [ ] `spring_project`; - [ ] `cellbrowser`. ```[tasklist]; ### Sub-issues; - [ ] #173; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2717:329,message,message,329,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2717,1,['message'],['message']
Integrability,FIt-SNE integration?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/996:8,integrat,integration,8,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/996,1,['integrat'],['integration']
Integrability,"FWIW, I stumbled upon a related issue this morning where my kernel just crashes/restarts computing neighbors. . For me it appears to crop up when the number of neighbors is <15, metric doesn't appear to matter. I've been upgrading/downgrading various dependencies, and I'm fairly certain this has to do with the call to [`NNDescent` in `umap.umap_.py`](https://github.com/lmcinnes/umap/blob/b1223505ca56ae104feb35e4196227277d1e8058/umap/umap_.py#L328) as if I import that directly, it raises the same errors. Currently have `numba=0.52` `llvmlite=0.35.0` `scanpy=1.7.1` `pynndescent=0.5.2` `umap-learn=0.5.1`. Rebuilding my environment from scratch and will update with a complete package list.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1696#issuecomment-797603893:251,depend,dependencies,251,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696#issuecomment-797603893,2,['depend'],['dependencies']
Integrability,"Faced exactly the same problem with file ""GSE185477_GSM3178784_C41_SC_raw_counts.zip"" from [GSE185477](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE185477). The advice from @hurleyLi helped, thanks a lot! But the error is quite confusing. It would be great to read files without extra actions for the user, but is it possible to at least change the error message? E.g.; ```python; try:; ....; except KeyError:; raise KeyError(""Unexpected error, probably due to Cellranger version. Make sure to unarchive gzipped file coming from Cellranger v2""); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1916#issuecomment-1296918392:364,message,message,364,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1916#issuecomment-1296918392,1,['message'],['message']
Integrability,Facing the same issue! Any guidance would be appreciated. Was trying to install using Anaconda Navigator for Windows but i guess I will try the Miniconda route,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-581828751:154,rout,route,154,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-581828751,2,['rout'],['route']
Integrability,"Feature selection refers to excluding uninformative genes such as those which exhibit no meaningful biological variation across samples. Since scRNA-Seq experiments usually examine cells within a single tissue, only a small fraction of genes are expected to be informative since many genes are biologically variable only across different tissues (adopted from https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6).; But, in fact some experimental design are very complex, such single-cell RNAseq of tissues from different development stage. The tissues can vary a log along the development timeline.; I find that the number of HVGs can affect data integration and batch effects correction. I've integrated seven cell samples collected at different development stage(1day, 2 day, 3 day, 4day, 5 day, 6 day, 7day after fertilization) with SCVI-tools, using 2000 HVGs, which then shows no ""batch effect"" (cells were mixed with no correlation among samples) left; on the other hand, using all genes, which shows still some extent of ""batch effect"" (some cells were clustered by time obviously) left. This could definitely affect the biological explaination, because the ""batch effect"" can be regarded as the difference of true biological difference at different development stage. The tissues are undergoing intensive differentiation process, so that the cell population are changing a lot during this process. Using only HVGs might lost these development process. ; In sum, HVGs are good for batch effect correction. The ""batch effects"" become less obvious when using less genes and more obvious when using more genes. However, more genes are good for discovery of new cell population. Does this make sense ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1578#issuecomment-764494020:670,integrat,integration,670,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578#issuecomment-764494020,2,['integrat'],"['integrated', 'integration']"
Integrability,"File ""/home/joyzheng/.local/lib/python3.8/site-packages/h5py/_hl/group.py"", line 183, in create_dataset; dsid = dataset.make_new_dset(group, shape, dtype, data, name, **kwds); File ""/home/joyzheng/.local/lib/python3.8/site-packages/h5py/_hl/dataset.py"", line 168, in make_new_dset; dset_id.write(h5s.ALL, h5s.ALL, data); File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper; File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper; File ""h5py/h5d.pyx"", line 280, in h5py.h5d.DatasetID.write; File ""h5py/_proxy.pyx"", line 145, in h5py._proxy.dset_rw; File ""h5py/_conv.pyx"", line 444, in h5py._conv.str2vlen; File ""h5py/_conv.pyx"", line 95, in h5py._conv.generic_converter; File ""h5py/_conv.pyx"", line 249, in h5py._conv.conv_str2vlen; TypeError: Can't implicitly convert non-string objects to strings. The above exception was the direct cause of the following exception:; Traceback (most recent call last):; File ""integration.py"", line 66, in <module>; adata.write_h5ad('Integrated.h5ad'); File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_core/anndata.py"", line 1918, in write_h5ad; _write_h5ad(; File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_io/h5ad.py"", line 98, in write_h5ad; write_elem(f, ""obs"", adata.obs, dataset_kwargs=dataset_kwargs); File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_io/utils.py"", line 214, in func_wrapper; return func(elem, key, val, *args, **kwargs); File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_io/specs/registry.py"", line 175, in write_elem; _REGISTRY.get_writer(dest_type, t, modifiers)(f, k, elem, *args, **kwargs); File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_io/specs/registry.py"", line 24, in wrapper; result = func(g, k, *args, **kwargs); File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_io/specs/methods.py"", line 514, in write_dataframe; write_elem(group, colname, series._values, dataset_kwargs=dataset_kwargs); File ""/home/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2432:3048,Integrat,Integrated,3048,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2432,1,['Integrat'],['Integrated']
Integrability,Fix passing of arguments between scrublet routines,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1659:42,rout,routines,42,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1659,1,['rout'],['routines']
Integrability,"Fix plotting and harmony, depend on anndata 0.7 for obsp (#1004, #1007)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1004:26,depend,depend,26,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1004,1,['depend'],['depend']
Integrability,Fixed https://anaconda.org/conda-forge/legacy-api-wrap,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2966#issuecomment-2023259405:50,wrap,wrap,50,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2966#issuecomment-2023259405,1,['wrap'],['wrap']
Integrability,"Fixes #1697. I'll leave any major backwards compatibility changes (i.e. updating to use python 3.7+ features) to the future. CI took a really long time to install the dependencies on the first run. Hopefully that will be cached? Not sure what triggers a saved cache. At the moment it looks like building `louvain` is taking a while, which we should actually just get around to deprecating.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1897:167,depend,dependencies,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1897,1,['depend'],['dependencies']
Integrability,"Fixes #435. Breaking case:. ```python; import scanpy as sc, numpy as np; sc.pp.log1p(; sc.AnnData(np.ones((100, 100)), dtype=int); ); ```. ```pytb; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-3-f3ae2b50ac50> in <module>; ----> 1 sc.pp.log1p(a). /usr/local/Cellar/python@3.8/3.8.5/Frameworks/Python.framework/Versions/3.8/lib/python3.8/functools.py in wrapper(*args, **kw); 873 '1 positional argument'); 874 ; --> 875 return dispatch(args[0].__class__)(*args, **kw); 876 ; 877 funcname = getattr(func, '__name__', 'singledispatch function'). ~/github/scanpy/scanpy/preprocessing/_simple.py in log1p_anndata(adata, base, copy, chunked, chunk_size, layer, obsm); 348 else:; 349 X = _get_obs_rep(adata, layer=layer, obsm=obsm); --> 350 X = log1p(X, copy=False, base=base); 351 _set_obs_rep(adata, X, layer=layer, obsm=obsm); 352 . /usr/local/Cellar/python@3.8/3.8.5/Frameworks/Python.framework/Versions/3.8/lib/python3.8/functools.py in wrapper(*args, **kw); 873 '1 positional argument'); 874 ; --> 875 return dispatch(args[0].__class__)(*args, **kw); 876 ; 877 funcname = getattr(func, '__name__', 'singledispatch function'). ~/github/scanpy/scanpy/preprocessing/_simple.py in log1p_array(X, base, copy); 316 else:; 317 X = X.copy(); --> 318 np.log1p(X, out=X); 319 if base is not None:; 320 np.divide(X, np.log(base), out=X). TypeError: ufunc 'log1p' output (typecode 'd') could not be coerced to provided output parameter (typecode 'l') according to the casting rule ''same_kind''; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1400:444,wrap,wrapper,444,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1400,2,['wrap'],['wrapper']
Integrability,"Fixes #993. This is an approximate implementation of the Seurat v3 hvg method. The only difference should be the use of lowess instead of loess (which is not available in python as far as I know). This method takes the UMI counts as input. The way HVGs from batches are merged is also different from the other flavors. As such, I didn't see a straightforward way to integrate this in the existing HVG code.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1182:366,integrat,integrate,366,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1182,1,['integrat'],['integrate']
Integrability,"Follow up to #2420, which let the tests run, but didn't actually install anndata-dev. Current version was quoted due to my error in reading a `pre-commit` complaint about yaml syntax. Tbf, yaml syntax is complicated, and the pre-commit check gives bad error messages. ----. Additionally apply other fixes from yesterdays bug fix release to main branch",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2421:258,message,messages,258,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2421,1,['message'],['messages']
Integrability,"Follow up to #703, this should take import times down another second. It does add a dependency, but that dependency will be in the stdlib from 3.8+. After this it looks like the next biggest contributors to import times are caused by `numba` (which would be hard to factor out) and `sklearn.metrics`, but these are on a smaller scale. Evidence of speedup:. ```sh; isaac@Mimir ~/github/scanpy$ time python3 -c ""import scanpy as sc"" ✹ ✭master ; python3 -c ""import scanpy as sc"" 3.10s user 0.41s system 116% cpu 3.010 total; isaac@Mimir ~/github/scanpy$ git checkout defer-umap ✹ ✭master ; Switched to branch 'defer-umap'; Your branch is up to date with 'origin/defer-umap'.; isaac@Mimir ~/github/scanpy$ time python3 -c ""import scanpy as sc"" ✹ ✭defer-umap ; python3 -c ""import scanpy as sc"" 2.18s user 0.42s system 131% cpu 1.981 total; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/704:84,depend,dependency,84,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/704,2,['depend'],['dependency']
Integrability,"Following up on #242. Here's my solution to the current queries being pretty unreliable for me (due to issue with bioservices module). It's all a pretty thin wrapper around `pybiomart`, which has a nice API and is well tested but has maintenance issues. . Currently I've replaced the `gene_coordinates` query with a more generic `biomart_annotations` – the example covers the functionality of `gene_coordinates`. I'm debating how to add tests given that they're network based (could fail when nothing is wrong with the code) and can take a while.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/467:158,wrap,wrapper,158,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467,1,['wrap'],['wrapper']
Integrability,"For me sounds interesting, especially if you add circle pathes. But I guess @ivirshup should say if this should be added as scanpy dependency potentially.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2194#issuecomment-1145328823:131,depend,dependency,131,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2194#issuecomment-1145328823,1,['depend'],['dependency']
Integrability,"For some reason I don't see the figures here on the Github page (and get an error message when I click on the link), but they showed fine in the email notification I received. Looks good!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-822057782:82,message,message,82,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-822057782,1,['message'],['message']
Integrability,"For those interested in using the GPU accelerated functions leiden, draw_graph_fa, I have made them available on the following gist:; https://gist.github.com/LouisFaure/9302aa140d7989a25ed2a44b1ce741e8. I have also included in that code `load_mtx`, which reads and convert mtx files into anndata using cudf. I tested on a 654Mo mtx containing 56621 cells x 20222 genes, I can obtain a 13X speedup (using RTX8000)! . ![image](https://user-images.githubusercontent.com/27488782/164707560-30c0c9fe-6bfe-4fcb-ac2c-0d8a503081b6.png). I expect this to scale even better with higher number of cells. I could also add this wrapper into scanpy once CI is ready.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1533#issuecomment-1106431960:615,wrap,wrapper,615,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1533#issuecomment-1106431960,1,['wrap'],['wrapper']
Integrability,"From the error message, you may want to try to convert the dense matrix to sparse matrix format as follows:. ```python; from scipy.sparse impor csr_matrix; adata.X = csr_matrix(adata.X); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1645#issuecomment-778224048:15,message,message,15,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1645#issuecomment-778224048,1,['message'],['message']
Integrability,"Given the old function now raises an error, could you at least add a; FutureWarning (or np.exceptions.VisibleDeprecationWarning) indicating the; new function to be used? Thanks!. On Wed, 7 June 2023, 05:35 Philipp A., ***@***.***> wrote:. > Closed #2500; > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500&g=ZWVjM2FjODk0ZjdmMTI1Nw==&h=N2Y4NmFmODU2ZTBlYjI1NzEzZDVlY2M3ZDQxMmVkMGVkZjY2OGMxZjEzMjZiMjNlODhmMGFhMTkwYjFmNGVjOQ==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>; > as not planned.; >; > —; > Reply to this email directly, view it on GitHub; > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500%23event-9456493371&g=ZGJmZGZhMzNmOTM5ZTgzYQ==&h=Y2JmZjM5MDc2MjMzNjM3MGQwMzk1MDYxZmE3MDZlYzBiNWEzYzdjMTMwNWY5MjgxNTU5YmQ3NDI0ZDBjNWRhZg==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>,; > or unsubscribe; > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/notifications/unsubscribe-auth/AUHCMAWZ7ISUHUBPHQDLLCDXKBDOHANCNFSM6AAAAAAY3HAO3E&g=NDU3YTZlZTA4ZDE0MzNhZQ==&h=OWQzOWMxNDgxMjZkZGM3ZWUxMmQ1ZTFlN2UwNjI5ZDI4YjFmMDA3OGVmYjc5MTljZDVkMDlhMTE1YjRiODBmNg==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>; > .; > You are receiving this because you authored the thread.Message ID:; > ***@***.***>; >. -- ; PLEASE NOTE: The information contained in this message is privileged and ; confidential, and is intended only for the use of the individual to whom it ; is addressed and others who have been specifically authorized to receive ; it. If you are not the intended recipient, you are hereby notified that any ; dissemination, distribution, or copying of this communication is strictly ; prohibited. If you have received this communication in error, or if any ; problems occur with the transmission, please contact the sender.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2500#issuecomment-1580655895:1379,Message,Message,1379,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2500#issuecomment-1580655895,2,"['Message', 'message']","['Message', 'message']"
Integrability,"Glad to see this discussion going on. Integrating openTSNE into scanpy should be fairly straightforward but may require some thought. I think Dmitry has already pointed out the most important things such as improved defaults, which other t-SNE implementations are lagging behind in. Apart from that, we package prebuilt binaries, so adding openTSNE as a dependency would be incredibly easy. The main thing we'd have to agree on is how to deal with the KNN graphs. UMAP by default calculates 15 nearest neighbors, and from what I can tell, louvain and leiden clustering both use those 15 neighbors as well by default. t-SNE, on the other hand, calculates 90 nearest neighbors by default. This is every single t-SNE implementation, not just openTSNE. Dmitry suggested using a uniform kernel with 15 neighbors, which would fit elegantly, but then again, this isn't truly t-SNE anymore, but rather something very close. The same goes for the `ingest` functionality. openTSNE does something similar to UMAP for adding new samples to existing embeddings, and then we'd again have to figure out how to calculate nearest neighbors from the new data to the reference data. I don't know how you do this currently for UMAP. I'm not exactly sure how these neighbors are meant to be used in scanpy, since there are several different algorithms that use them. Graph-based clustering uses the KNNG, UMAP uses it, forceatlas2 uses it, PAGA probably as well? Is relying on a single k=15 from UMAP for everything really ok? For example, seurat defaults to using 30 neighbors for clustering.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1233#issuecomment-637563724:38,Integrat,Integrating,38,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1233#issuecomment-637563724,2,"['Integrat', 'depend']","['Integrating', 'dependency']"
Integrability,"Good point! I think the idea might be that they’re actually different versions of a gene that just have been mapped to the same gene name (and have e.g. different ENSEMBL IDs). But of course that depends on how the data was generated, and your method might be more appropriate for some datasets. Example from the tutorial:. ```py; import pooch; import scanpy as sc. EXAMPLE_DATA = pooch.create(; path=pooch.os_cache(""scverse_tutorials""),; base_url=""doi:10.6084/m9.figshare.22716739.v1/"",; ); EXAMPLE_DATA.load_registry_from_doi(). samples = {; ""s1d1"": ""s1d1_filtered_feature_bc_matrix.h5"",; ""s1d3"": ""s1d3_filtered_feature_bc_matrix.h5"",; }; adatas = {}. for sample_id, filename in samples.items():; path = EXAMPLE_DATA.fetch(filename); sample_adata = sc.read_10x_h5(path); #sample_adata.var_names_make_unique(); adatas[sample_id] = sample_adata. adatas[""s1d1""].var[adatas[""s1d1""].var.index.duplicated(keep=False)]; ```. > | | gene_ids | feature_types | genome | pattern | read | sequence |; > | --- | --- | --- | --- | --- | --- | --- |; > | TBCE | ENSG00000285053 | Gene Expression | GRCh38 | | | |; > | TBCE | ENSG00000284770 | Gene Expression | GRCh38 | | | |; > | LINC01238 | ENSG00000237940 | Gene Expression | GRCh38 | | | |; > | LINC01238 | ENSG00000261186 | Gene Expression | GRCh38 | | | |; > | CYB561D2 | ENSG00000114395 | Gene Expression | GRCh38 | | | |; > | CYB561D2 | ENSG00000271858 | Gene Expression | GRCh38 | | | |; > | MATR3 | ENSG00000280987 | Gene Expression | GRCh38 | | | |; > | MATR3 | ENSG00000015479 | Gene Expression | GRCh38 | | | |; > | LINC01505 | ENSG00000234323 | Gene Expression | GRCh38 | | | |; > | LINC01505 | ENSG00000234229 | Gene Expression | GRCh38 | | | |; > | HSPA14 | ENSG00000284024 | Gene Expression | GRCh38 | | | |; > | HSPA14 | ENSG00000187522 | Gene Expression | GRCh38 | | | |; > | GOLGA8M | ENSG00000188626 | Gene Expression | GRCh38 | | | |; > | GOLGA8M | ENSG00000261480 | Gene Expression | GRCh38 | | | |; > | GGT1 | ENSG00000286070 | Gene Express",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3241#issuecomment-2360283300:196,depend,depends,196,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3241#issuecomment-2360283300,1,['depend'],['depends']
Integrability,"Great to hear from both of you. I'd really love to have better Dask integration with AnnData and am excited to see these progress!. @ryan-williams, it'd great if you could open an issue over on anndata about this! I think that'd be a good place to discuss design considerations.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1663#issuecomment-783235345:68,integrat,integration,68,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1663#issuecomment-783235345,1,['integrat'],['integration']
Integrability,"Great! I'll check it out when I have a chance. If this is close to ready, could it also start getting some tests?. Just to clarify, would a notebook with the pancreas integration stuff be useful to you?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/651#issuecomment-519798857:167,integrat,integration,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651#issuecomment-519798857,1,['integrat'],['integration']
Integrability,"Great!. I'll replace the dataset in the tests in that case. > It would be good to have tests that actually hit the parts of neighbors where non-pairwise distances are found (>4096 cells I think). We're just completely migrating to a shallow wrapper of umap there, where this is tested. I talked to Leland and he said it should be stable. At some point, we might move to `pynndescent` (when it get's introduced into umap). Long story short, I don't think we need to test the neighbors module within scanpy beyond testing the interface. > I've been pretty successful at speeding up the tests by just running them in parallel. Stuff like this might be good to have in some dev docs. Is there a place for that kind of thing right now?. No, happy to have you put some dev docs in a location that you find sensible. :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/581#issuecomment-479472437:241,wrap,wrapper,241,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/581#issuecomment-479472437,2,"['interface', 'wrap']","['interface', 'wrapper']"
Integrability,"Great!. I'm not sure what's going on with that conda build, but hopefully one of the maintainers there will know what to do. On the topic of this PR, I do think this case should give a more helpful error message. Would you like to do that?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1490#issuecomment-726498381:204,message,message,204,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1490#issuecomment-726498381,1,['message'],['message']
Integrability,"Great, thank you for summarizing this so neatly. We could even link from the PBMC3k tutorial to this. Quick answer: yes, there are many situations in which `pp.regress_out` can do more harm than good. In most cases, I personally don't correct for mitochondrial gene expression, for instance. Quite a few people will agree with that. Whether a certain processing step makes sense or not depends on the data. You should choose with subject knowledge. Because, of that, I found it hard to come up with a best practice tutorial; what came into life as Benchmark with Seurat, remained Seurat's way of defining best practice. Many papers stick to this. But I'm sure that also many Seurat users won't always regress out mitochondrial genes and number of counts per cell. I'm sure @LuckyMD has a lot to say on this. :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/526#issuecomment-471317931:386,depend,depends,386,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/526#issuecomment-471317931,1,['depend'],['depends']
Integrability,"Great, thank you, @andrea-tango and @Koncopd!. @andrea-tango, would you make a PR? We can then look at how you solved this. In principle, I'm very hesitant to add `diffxpy` as a dependency of Scanpy. It depends on Tensorflow itself, which is a large dependency. What would be OK would be to have a wrapper in `scanpy.external`, but I don't know whether this makes sense. Why not using `diffxpy`s Volcano plots right away?. Regarding the discrepancy between `wilxocon` in `diffxpy` and `scanpy`. There obviously shouldn't be any and there also shouldn't be duplicated code, here, at all. The only reason that Scanpy has its own Wilcoxon implementation was that there was no implementation available that would scale to large sparse data. That's why @tcallies wrote the present implementation about 1.5 years ago. He benchmarked with scipy's Wilcoxon test. @davidsebfischer, can you shed light on why and how you implemented your Wilcoxon? Shouldn't we have a comparison? At the time, @tcallies wrote [this](https://github.com/theislab/scanpy_usage/blob/master/171106_t-test_wilcoxon_comparison/Generic%20Comparison%20T-Test%20Wilcoxon-Rank-Sum%20Test.ipynb) and these [tests](https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_rank_genes_groups.py). How did you write your tests?. We were just talking about `log2FC`, which is such a simple quantity and should evidently be properly computed by `rank_genes_groups`. We just had this other PR on it (https://github.com/theislab/scanpy/pull/519). @tcallies, any thoughts from your side?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/460#issuecomment-471322809:178,depend,dependency,178,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460#issuecomment-471322809,8,"['depend', 'wrap']","['dependency', 'depends', 'wrapper']"
Integrability,"Great. Please ping me here when you upload the file to `scanpy_usage` and feel free to close the issue then. I'll update my script to link directly to `scanpy_usage`. > Regarding the result: the high PCs can change drastically depending on the platform and the random seed. I've seen clustering results changing completely after I became aware of it. . I don't have much experience with randomized PCA, but this is very disturbing, no? Was your feeling that the PCs themselves changed strongly (as measured, I don't know, by the %% of total captured variance, or maybe angle between subspaces, etc.), or is it rather that clustering outcome is dangerously sensitive to small changes in the data? I think this is something worth investigating.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/325#issuecomment-435797047:227,depend,depending,227,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325#issuecomment-435797047,1,['depend'],['depending']
Integrability,"Had same problem. It seems it tries to look for all gene names present in the anndata object,; rather than the cell cycle genes that are requested?. I have previously checked that the s_genes and g2m_genes in call; sc.tl.score_genes_cell_cycle(gdata, s_genes=s_found, g2m_genes=g2m_found); are in the data. use_raw=False ; makes it run without error. Error message below. Note that printout of gene names in data at the beginning matches the list of keyErrors it spits out. ```; Gene_names_in_data:; Index(['HES4', 'C1orf159', 'TNFRSF18', 'TNFRSF4', 'ATAD3C', 'PRKCZ',; 'AL365255.1', 'GPR153', 'TNFRSF25', 'DNAJC11',; ...; 'MCF2', 'SPANXA2-OT1', 'AFF2', 'LINC00894', 'MAMLD1', 'PDZD4', 'F8',; 'TMLHE-AS1', 'PRKY', 'UTY'],; dtype='object', length=3658). calculating cell cycle phase; computing score 'S_score'; ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); <ipython-input-82-30815e6b6381> in <module>; 20 print(gdata.var.index); 21 ; ---> 22 sc.tl.score_genes_cell_cycle(gdata, s_genes=s_found, g2m_genes=g2m_found); 23 ; 24 gdata.obs[""cellcycle""] = gdata.obs[""phase""]. ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/scanpy/tools/_score_genes.py in score_genes_cell_cycle(adata, s_genes, g2m_genes, copy, **kwargs); 247 ctrl_size = min(len(s_genes), len(g2m_genes)); 248 # add s-score; --> 249 score_genes(adata, gene_list=s_genes, score_name='S_score', ctrl_size=ctrl_size, **kwargs); 250 # add g2m-score; 251 score_genes(adata, gene_list=g2m_genes, score_name='G2M_score', ctrl_size=ctrl_size, **kwargs). ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/scanpy/tools/_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw); 128 _adata = adata.raw if use_raw else adata; 129 ; --> 130 _adata_subset = _adata[:, gene_pool] if len(gene_pool) < len(_adata.var_names) else _adata; 131 if issparse(_adata_subset.X):; 132 obs_avg = pd.Series(. ~/.pyenv/version",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1599#issuecomment-767782350:357,message,message,357,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599#issuecomment-767782350,1,['message'],['message']
Integrability,"Hah, so I wasn't aware of the ecosystem page yet. This looks very cool, and could really be built upon nicely. I think a more clear tutorial integration into the page would be useful.... and I guess some tools don't really have any brief explanations there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1443#issuecomment-703726683:141,integrat,integration,141,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1443#issuecomment-703726683,2,['integrat'],['integration']
Integrability,"Happy new year! And thanks for opening this PR @pavlin-policar. -----------------. First a general question. What is the scope of this PR? Will this just be single dataset TSNE calculation, with integration/ `ingest` functionality happening separately, or would you like to do it all at once?. -----------------. In terms of workflow, I think I'd like it to look similar to UMAP. * One function for calculating the graph/ manifold; * One function for computing the embedding. If possible, I would like it if the user could specify an arbitrary manifold (e.g. the umap weighted one) to pass to the embedding step, but this is icing. > It would also make sense to add a tsne option to sc.pp.neighbors. I would prefer for this to be a separate function, maybe `neighbors_tsne`? This could use the entire neighbor calculating workflow from `openTSNE`. How different are the arguments to the various `affinity` methods? At first glance they look pretty similar. I'd like to have the option of choosing which one, but does it make sense to have all the methods available through one function?. > noticed that sc.tl.umap and now sc.tl.tsne add their parameters to adata.uns. ... Determining which affinity kernel to use would then be as simple as looking into adata.uns to find which parameter value sc.pp.neighbors was called with. +1. Do you need to know what the affinity method was if you're just calculating an embeddings? Or does that only become important when you want to add new data?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-758355448:195,integrat,integration,195,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-758355448,2,['integrat'],['integration']
Integrability,"Happy to discuss what can be integrated from scvelo's `pl.scatter` into scanpy or how scvelo's codebase can be used.. just to mention some of the features that may also be interesting for scanpy:; - (`x`, `y`) is `str` key of (var_names, var_names), (var, var), (obs, obs), (array, array), (obs, var_names), where I find particularly passing arrays to be very convenient.; - `basis` from obsm (what is the reason for having an additional `pl.embedding`?) or var_names (on layer1 vs layer2, e.g. spliced vs. unspliced).; - `color` is `str` key of obs, var, layers or directly pass an array (which I find very convenient); while each of these can also be a list/tuple of `str` or arrays. . Further, we 'beautified' the colorbar, ticks etc. and added some functionality such as plotting a lin.reg line or polynomial fit of any degree directly on top of the scatterplot, show histogram/density along x and y axes, added `dpi` and `figsize` attributes and **kwargs for all other matplotlib-specific attributes such as `vmin`/`vmax`. ; Apart from these it entails all functionality of scanpy's `pl.scatter`. It turned out to be very convenient to have pretty much everything within one single `pl.scatter` module, not matter whether you want to visualize an embedding, any user-specified arrays colored by clusters, or visualize a gene trend along a pseudotime. I'd start of with the general question of whether incorporating some of these functionalities into scanpy's `pl.scatter` that may be useful, or whether re-implementing it based on scvelo's `pl.scatter` codebase makes more sense.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/617#issuecomment-553948802:29,integrat,integrated,29,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617#issuecomment-553948802,1,['integrat'],['integrated']
Integrability,"Has anyone found a solution for this? I run into segfault with the same message when trying to run `sc.pp.calculate_qc_metrics` on my M2. Latest clean installation. I have the core dump as well, but I don't know how to get useful information from there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2359#issuecomment-1345470076:72,message,message,72,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2359#issuecomment-1345470076,1,['message'],['message']
Integrability,"Has the format changed for all assays, or is this specific to visium HD?. And is there significant analysis that can be done on these without taking into account extra spatial information?. I would be inclined to say that the visium IO functions in scanpy should be deprecated/ replaced with a light wrapper around: `spatialdata_io.experimental.to_legacy_anndata(spatialdata_io.visium(*args, **kwargs))` or just accessing the AnnData from `spatialdata_io.visium_hd`. But if there is significant burden involved in the dependencies of `spatialdata`, then maybe we could implement something lighter here for now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2973#issuecomment-2033981290:300,wrap,wrapper,300,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2973#issuecomment-2033981290,2,"['depend', 'wrap']","['dependencies', 'wrapper']"
Integrability,"Haven't tried again but I have a suggestion. Since umap (or pynndescent) is a critical component of scanpy, I think it'd be great to run our tests against both ""stable"" and ""development"" branches of umap. However in order for this to happen, umap needs proper naming for the development and stable branches. Right now, there are master, 0.3dev and 0.4dev, therefore the names are version-dependent. . Does it make sense to file a bug report in umap repo? It'd be a lot easier to run test against two major branches of umap without changing the names in every major release. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/779#issuecomment-524128498:388,depend,dependent,388,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/779#issuecomment-524128498,1,['depend'],['dependent']
Integrability,"Having run into the limitations of constraining to 10 categories for quite some time, and now seeing @sunnysun515 write a wrapper that brings the functionality and produces meaningful plots: I'd strongly advocate for letting the user decide!. To address concerns raised above, I added a warning to the docstring that enough cells per category are needed for a meaningful estimate: https://github.com/theislab/scanpy/pull/1936/commits/760a967b57b93b0da3296d98430509627f3a80d7. [Imagining a warning showing up whenever the function is called seems more disruptive.]. Otherwise, it'd be great if @LuckyMD and @sunnysun515 could take a look in addition to Isaac: https://github.com/theislab/scanpy/pull/1936",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/719#issuecomment-875362308:122,wrap,wrapper,122,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/719#issuecomment-875362308,1,['wrap'],['wrapper']
Integrability,"Hej all,. I am using a backed dataset because, when I run the umap scatterplot, the RAM go pretty much crazy on our server. When I use not-backed data, I can do UMAP scatterplots without any problem apart from the memory usage. But now that the data is backed, when running the following:; ```; sc.plotting.tools.scatterplots.umap(all_data_flt_clst, color=markers, cmap=""Blues"", ncols=5); ```. I get an error message that seems related to the `h5py` package. Here is the whole trace back. ```; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-3-a78575d924b7> in <module>; 24 if len(markers) > 0:; 25 print(""Expression plots of "", names, "" markers: "", markers); ---> 26 sc.plotting.tools.scatterplots.umap(all_data_flt_clst, color=markers, cmap=""Blues"", ncols=5); 27 ; 28 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/scatterplots.py in umap(adata, **kwargs); 27 If `show==False` a `matplotlib.Axis` or a list of it.; 28 """"""; ---> 29 return plot_scatter(adata, basis='umap', **kwargs); 30 ; 31 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/scatterplots.py in plot_scatter(adata, color, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, basis, groups, components, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs); 280 if sort_order is True and value_to_plot is not None and categorical is False:; 281 order = np.argsort(color_vector); --> 282 color_vector = color_vector[order]; 283 _data_points = data_points[component_idx][order, :]; 284 . h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/h5py/_hl/dataset.py in __getitem__(self, args); 474 ; 475 # Perform the dataspace selection.; --> 476 selectio",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/440:409,message,message,409,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/440,1,['message'],['message']
Integrability,"Hej all. it seems there is a problem on the batch correction with bbknn. It gives an error at the pca step of bbknn, but I have problem understanding if this is due to the bbknn package itself or the wrapper of scanpy around it, or if it is due to my data, even though it worked when I used it previously. ```python; sc.external.pp.bbknn(all_data_flt, batch_key='batch', n_pcs=15,); ```. gives the error. ```python; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-23-a9dd619ada2e> in <module>; 1 #sc.neighbors.neighbors(all_data_flt, n_neighbors=40, n_pcs=15); ----> 2 sc.external.pp.bbknn(all_data_flt, n_pcs=15); 3 #sc.tools.umap(all_data_flt). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_bbknn.py in bbknn(adata, batch_key, save_knn, copy, **kwargs); 82 params = locals(); 83 kwargs = params.pop('kwargs'); ---> 84 return bbknn(**params, **kwargs). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/bbknn/__init__.py in bbknn(adata, batch_key, save_knn, copy, **kwargs); 215 batch_list = adata.obs[batch_key].values; 216 #call BBKNN proper; --> 217 bbknn_out = bbknn_pca_matrix(pca=pca,batch_list=batch_list,save_knn=save_knn,**kwargs); 218 #optionally save knn_indices; 219 if save_knn:. TypeError: bbknn_pca_matrix() got an unexpected keyword argument 'bbknn'; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/514:200,wrap,wrapper,200,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/514,1,['wrap'],['wrapper']
Integrability,"Hello ; I am also facing the same problem.; I would like to get gene name, log fold change, pval_adj, pts.pts_rest in a single output CSV file but i couldn't able to do that; ` ; sc.tl.rank_genes_groups(adata,""leiden_0.6"", method='t-test',pts=True,corr_method='benjamini-hochberg'); pd.DataFrame(adata.uns['rank_genes_groups']['names']); result = adata.uns['rank_genes_groups']; groups = result['names'].dtype.names; df= pd.DataFrame(; {group + '_' + key[:1]: result[key][group]; for group in groups for key in ['names','logfoldchanges','pts','pts_rest','pvals','pvals_adj']}); df.to_csv(""/home/Akila/integration/harmony/subset/celltype/find_markergenes.csv"")`; ; Any idea how to get in the single file along with pts??; ; Thanks; Akila",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1455#issuecomment-1164848375:601,integrat,integration,601,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1455#issuecomment-1164848375,1,['integrat'],['integration']
Integrability,"Hello everyone,; I have faced the below warning messages during running `sc.tl.rank_genes_groups` with parameters t-test and t-test overestimated. > RuntimeWarning: invalid value encountered in greater; return (self.a < x) & (x < self.b). > RuntimeWarning: invalid value encountered in less; return (self.a < x) & (x < self.b). > RuntimeWarning: invalid value encountered in less_equal; cond2 = cond0 & (x <= self.a). I found exactly the same warning message in a forum but there the warning message was coming from auto-sklearn package and since there is no auto-sklearn used in scanpy and instead we got scikit-learn I am wondering is it a problem with the version that I am using or is it a bug from scikit or scanpy it self. my versions are:. > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760 louvain==0.6.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/674:48,message,messages,48,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/674,3,['message'],"['message', 'messages']"
Integrability,"Hello there,. Thank you for the great work you're doing building ScanPy! . I am currently learning about open-source licenses and the intricacies of copyright. Especially regarding management of GPL dependencies and when the viral copyleft clause is triggerred or not.; It looks like the ScanPy team explored this question already as the projet is licensed under BSD while it is leveraging GPLed dependencies like `leidenalg`, `python-igraph` or `louvain`. Understanding how you handled this question would greatly help me, could you tell me?; Maybe there are discussions recorded somewhere?. Best,; Fabien",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3272:199,depend,dependencies,199,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3272,2,['depend'],['dependencies']
Integrability,"Hello world!; I've read in many papers that when performing a re-clustering of some populations, like T cells or B cells, prior to the step of integration and so on, they re-calculate the HVGs but excluding the TCR- or BCR-related genes, because they are donor-specific, especially when talking about BCR. Can you help me how to remove the TCR- or BCR-related genes before computing the HVGs selection, but without removing them from the .var of the anndata, since I want to evaluate their expression during the step of cell annotation?. The code that I use to calculate the HVGs is the following:; sc.pp.highly_variable_genes(adata,; n_top_genes = 4000, flavor = ""seurat_v3"",; layer = ""raw"", batch_key = 'sample_id',; subset = False). Thanks a lot!; Paolo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2895:143,integrat,integration,143,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2895,1,['integrat'],['integration']
Integrability,Hello! I thought it might be useful to add [Scanorama ](https://github.com/brianhie/scanorama) to the set of integration methods in scanpy.external.pp. I've followed the example laid out in https://github.com/theislab/scanpy/pull/1306 pretty closely. I've also done some small scale integration testing on a local machine just to verify that batch effect correction indeed works when called with this API. Let me know what I can do to help get this merged!,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1332:109,integrat,integration,109,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1332,2,['integrat'],['integration']
Integrability,"Hello, . I have managed to get my Seurat object converted into Loom and then read into Scanpy. Now my main objective is to use the clusters identified using Seurat in order to create a PAGA trajectory map. I was able to do a similar thing for Seurat -> Monocle by integrating the Seurat clusters and allow Monocle to perform a trajectory analysis on them. . I have the following Scanpy object:; ![scanpy_adata](https://user-images.githubusercontent.com/11708268/58907732-7a075380-86d4-11e9-9f2a-4c539ea58c80.png). All the cluster information along with cell ids are present in the obs part of the Scanpy object. Is there anyway to use that information in order to perform a PAGA trajectory analysis?. Thank you,; Behram",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/680:264,integrat,integrating,264,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/680,1,['integrat'],['integrating']
Integrability,"Hello, ; I have run this command again in the fresh conda environment. Again I get the same error as before. AttributeError Traceback (most recent call last); c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\sinfo\main.py in sinfo(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes); 194 try:; --> 195 mod_version = _find_version(mod.__version__); 196 except AttributeError:. AttributeError: module 'importlib_metadata' has no attribute '__version__'. During handling of the above exception, another exception occurred:. TypeError Traceback (most recent call last); <ipython-input-3-c71c26e11b3b> in <module>; ----> 1 sc.logging.print_versions(). c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\scanpy\logging.py in print_versions(file); 159 try:; 160 buf = sys.stdout = io.StringIO(); --> 161 sinfo(dependencies=True); 162 finally:; 163 sys.stdout = stdout. c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\sinfo\main.py in sinfo(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes); 196 except AttributeError:; 197 try:; --> 198 mod_version = _find_version(mod.version); 199 except AttributeError:; 200 try:. c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\sinfo\main.py in _find_version(mod_version_attr); 40 return joined_tuple; 41 elif callable(mod_version_attr):; ---> 42 return mod_version_attr(); 43 else:; 44 # print(f'Does not support module version of type {type(mod_ver_attr)}'). TypeError: version() missing 1 required positional argument: 'distribution_name'",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1932#issuecomment-883208028:280,depend,dependencies,280,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1932#issuecomment-883208028,3,['depend'],['dependencies']
Integrability,"Hello, I am trying to use the visualize marker genes tutorial to make some plots. I am importing scanpy in the new way (import scanpy as sc) as suggested in the tutorial but I am getting an error message:. AttributeError Traceback (most recent call last); <ipython-input-5-dfc1e4d9ed06> in <module>(); ----> 1 ax = sc.pl.correlation_matrix(adata, 'cell_types'). AttributeError: module 'scanpy.plotting' has no attribute 'correlation_matrix'. Here are the versions of all the packages I am using:; scanpy==1.4 anndata==0.6.17 numpy==1.16.0 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . Am I missing something ?. Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/544:196,message,message,196,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/544,1,['message'],['message']
Integrability,"Hello, I am unable to import scanpy and the error message shows below:. ```python; import scanpy as sc; ```. ```pytb; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/opt/anaconda3/lib/python3.9/site-packages/scanpy/__init__.py"", line 6, in <module>; from ._utils import check_versions; File ""/opt/anaconda3/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 29, in <module>; from .compute.is_constant import is_constant; File ""/opt/anaconda3/lib/python3.9/site-packages/scanpy/_utils/compute/is_constant.py"", line 5, in <module>; from numba import njit; ImportError: cannot import name 'njit' from 'numba' (unknown location)```; ```. Scanpy has been working well, but today it reports an issue. I have tried to uninstall and reinstall numba and scanpy, but it still did not work. Can you help? Thank you very much.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2438:50,message,message,50,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2438,1,['message'],['message']
Integrability,"Hello, I am using the folder where I store the raw data for the analysis. There is no error message when I run the command but it does not generate any file or object with this name or any name.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1795#issuecomment-817682376:92,message,message,92,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1795#issuecomment-817682376,1,['message'],['message']
Integrability,"Hello,. I am a beginner in the bioinformatics field. I have been used scanpy for one year, and I really like it. But yesterday after I downloaded and installed the new developmental version from scanpy github, when I ran the first command ""import scanpy as sc"", it displayed errors :AttributeError: module 'tables' has no attribute 'which_lib_version'. I don't know how to deal with this error. Does the ""module 'tables' "" in the error message mean pytables? After I saw the message, I reinstalled both pytables and scanpy through conda channel. But the error is not fixed. I thus wish to get some suggestions. Thank you so much!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/853:436,message,message,436,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853,2,['message'],['message']
Integrability,"Hello,. I am having the same issue as issue #1246 but my version of scipy being used with scanpy is not updating. I don't know if this is related to my using an ubuntu server or what's causing this but I was wondering if there is a workaround to make scanpy use a more updated version? I have scipy 1.4.1 installed when I check the version but for some reason scanpy is using 1.01 and I don't know how to change this. I'm a bit new to python so I'm sorry if this is a novice question. I appreciate any help you can offer. I am using an ubuntu server running python 3.6 with the following versions:; sc.logging.print_versions() ; scanpy==1.5.1 anndata==0.7.3 umap==0.4-dev numpy==1.15.0 scipy==1.0.1 pandas==0.23.3 scikit-learn==0.23.1 statsmodels==0.11.1. This is the error message:. ```pytb; computing tSNE; WARNING: You’re trying to run this on 16872 dimensions of `.X`, if you really want this, set `use_rep='X'`.; Falling back to preprocessing with `sc.pp.pca` and default params.; computing PCA; with n_comps=50; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-65-c244be664e51> in <module>(); ----> 1 sc.tl.tsne(adata, n_pcs = 50); 2 # UMAP, first with neighbor calculation; 3 sc.pp.neighbors(adata, n_pcs = 50, n_neighbors = 20); 4 sc.tl.umap(adata). ~/.local/lib/python3.6/site-packages/scanpy/tools/_tsne.py in tsne(adata, n_pcs, use_rep, perplexity, early_exaggeration, learning_rate, random_state, use_fast_tsne, n_jobs, copy); 78 start = logg.info('computing tSNE'); 79 adata = adata.copy() if copy else adata; ---> 80 X = _choose_representation(adata, use_rep=use_rep, n_pcs=n_pcs); 81 # params for sklearn; 82 params_sklearn = dict(. ~/.local/lib/python3.6/site-packages/scanpy/tools/_utils.py in _choose_representation(adata, use_rep, n_pcs, silent); 41 'Falling back to preprocessing with `sc.pp.pca` and default params.'; 42 ); ---> 43 X = pca(adata.X); 44 adata.obsm['X_pca'] = X[:, :n_pcs]; 45 ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1252:774,message,message,774,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1252,1,['message'],['message']
Integrability,"Hello,. I am trying to use the SAM algorithm in my single-cell analysis. I run the SAM function like so:. ```py; sam_obj = sce.tl.sam(adata,inplace=True); ```. The function runs fine and appears to finish training however it crashes when computing the UMAP with the following error:. ```pytb; TypeError: a bytes-like object is required, not 'list'; ```. I'm not sure where this problem is coming from and I have spent the past day installing different versions of python and other dependencies to see if that solves the issue. Maybe naive but I know conda can sometimes be behind in their updates. I installed scanpy following the anaconda instructions here: https://scanpy.readthedocs.io/en/stable/installation.html; And I installed sam-algorithm using pip. Below is the entire output from the function call above. Below this I have included the output of ""conda list"" in case this information is helpful. . Any help would be greatly appreciated. Thank you, Hasan. ```pytb; Self-assembling manifold; Running SAM; RUNNING SAM; Iteration: 0, Convergence: 0.6008695832027542; /wynton/home/state/alkhairohr/miniconda3/envs/python_env/lib/python3.6/site-packages/samalg/__init__.py:1165: FutureWarning: This location for 'connectivities' is deprecated. It has been moved to .obsp[connectivities], and will not be accesible here in a future version of anndata.; self.adata.uns[""neighbors""][""connectivities""] = EDM; Iteration: 1, Convergence: 0.3743130193917588; /wynton/home/state/alkhairohr/miniconda3/envs/python_env/lib/python3.6/site-packages/samalg/__init__.py:1165: FutureWarning: This location for 'connectivities' is deprecated. It has been moved to .obsp[connectivities], and will not be accesible here in a future version of anndata.; self.adata.uns[""neighbors""][""connectivities""] = EDM; Iteration: 2, Convergence: 0.029142717058066172; /wynton/home/state/alkhairohr/miniconda3/envs/python_env/lib/python3.6/site-packages/samalg/__init__.py:1165: FutureWarning: This location for 'connectivities'",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1293:481,depend,dependencies,481,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1293,1,['depend'],['dependencies']
Integrability,"Hello,. I just type ""import scanpy"", and it still shows the error message. . Here is my code. . ```py; import scanpy; ```. Here is what the computer showed after I ran this code:. ```pytb; AttributeError Traceback (most recent call last); <ipython-input-2-135279188441> in <module>; ----> 1 import scanpy. ~/Documents/scanpy/scanpy/scanpy/__init__.py in <module>; 34 # the actual API; 35 from ._settings import settings, Verbosity # start with settings as several tools are using it; ---> 36 from . import tools as tl; 37 from . import preprocessing as pp; 38 from . import plotting as pl. ~/Documents/scanpy/scanpy/scanpy/tools/__init__.py in <module>; 15 from ._leiden import leiden; 16 from ._louvain import louvain; ---> 17 from ._sim import sim; 18 from ._score_genes import score_genes, score_genes_cell_cycle; 19 from ._dendrogram import dendrogram. ~/Documents/scanpy/scanpy/scanpy/tools/_sim.py in <module>; 23 ; 24 from .. import _utils; ---> 25 from .. import readwrite; 26 from .._settings import settings; 27 from .. import logging as logg. ~/Documents/scanpy/scanpy/scanpy/readwrite.py in <module>; 7 import numpy as np; 8 import pandas as pd; ----> 9 import tables; 10 import anndata; 11 from anndata import (. ~/anaconda3/lib/python3.6/site-packages/tables/__init__.py in <module>; 91 ; 92 # Necessary imports to get versions stored on the cython extension; ---> 93 from .utilsextension import (; 94 get_pytables_version, get_hdf5_version, blosc_compressor_list,; 95 blosc_compcode_to_compname_ as blosc_compcode_to_compname,. tables/utilsextension.pyx in init tables.utilsextension(). ~/anaconda3/lib/python3.6/site-packages/tables/tables/__init__.py in <module>; 122 from .flavor import restrict_flavors; 123 from .description import *; --> 124 from .filters import Filters; 125 ; 126 # Import the user classes from the proper modules. ~/anaconda3/lib/python3.6/site-packages/tables/tables/filters.py in <module>; 27 from tables.req_versions import min_blosc_bitshuffle_version; 28 ;",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/853#issuecomment-539798622:66,message,message,66,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853#issuecomment-539798622,1,['message'],['message']
Integrability,"Hello,. Thank you for developing and maintaining such a useful tool!; I'm trying to integrate two data sets, they're replicates of the same condition. . ```; var_names = adata_002.var_names.intersection(adata_003.var_names); adata_002 = adata_002[:, var_names]; adata_003 = adata_003[:, var_names]. sc.pp.pca(adata_002); sc.pp.neighbors(adata_002); sc.tl.umap(adata_002). sc.tl.ingest(adata_003, adata_002, obs='louvain'); ```. And I got the following error:. ```; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-42-b3f5427509ba> in <module>; ----> 1 sc.tl.ingest(adata_003, adata_002, obs='louvain'). AttributeError: module 'scanpy.tools' has no attribute 'ingest'; ```. scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.18.1 scipy==1.4.1 pandas==1.0.1 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.7.1 louvain==0.6.1. I've already tried installing the package both with conda and pip and I continue having the same issue. . I would really appreciate your comments and suggestions. . Sara",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1092:84,integrat,integrate,84,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1092,1,['integrat'],['integrate']
Integrability,"Hello,; I am trying to use the wrapper class and I am getting error; RRuntimeError: Error in `[.data.frame`(meta.data, , ii, drop = FALSE) : ; undefined columns selected; Could you please suggest me what should I do; Its on line ro.r('seurat_obj = as.Seurat(adata, counts=""X"",data=NULL)'); Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1068#issuecomment-982525338:31,wrap,wrapper,31,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-982525338,1,['wrap'],['wrapper']
Integrability,"Hello,; This command (sc.logging.print_versions()) gives me the error pasted below:; AttributeError Traceback (most recent call last); c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\sinfo\main.py in sinfo(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes); 194 try:; --> 195 mod_version = _find_version(mod.__version__); 196 except AttributeError:. AttributeError: module 'importlib_metadata' has no attribute '__version__'. During handling of the above exception, another exception occurred:. TypeError Traceback (most recent call last); <ipython-input-19-c71c26e11b3b> in <module>; ----> 1 sc.logging.print_versions(). c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\scanpy\logging.py in print_versions(file); 159 try:; 160 buf = sys.stdout = io.StringIO(); --> 161 sinfo(dependencies=True); 162 finally:; 163 sys.stdout = stdout. c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\sinfo\main.py in sinfo(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes); 196 except AttributeError:; 197 try:; --> 198 mod_version = _find_version(mod.version); 199 except AttributeError:; 200 try:. c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\sinfo\main.py in _find_version(mod_version_attr); 40 return joined_tuple; 41 elif callable(mod_version_attr):; ---> 42 return mod_version_attr(); 43 else:; 44 # print(f'Does not support module version of type {type(mod_ver_attr)}'). TypeError: version() missing 1 required positional argument: 'distribution_name'",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1932#issuecomment-874660246:256,depend,dependencies,256,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1932#issuecomment-874660246,3,['depend'],['dependencies']
Integrability,"Here an example for the plot with NA:. The used obs has 2 categories (Levitas and none) and for the individual plots the groupby argument were „Levitas“ and „none“. Sent from my iPad. On 5. Jan 2023, at 15:11, Lukas Heumos ***@***.***> wrote:. ﻿. By a minimal working example I mean something that we can copy and paste and reproduce your result directly. The easier you make it for us the more likely we can dedicate some time to look at your issue. Thanks. —; Reply to this email directly, view it on GitHub<https://github.com/scverse/scanpy/issues/1988#issuecomment-1372263677>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AOGNBODKUC3S3SZC32VD3TLWQ3JBVANCNFSM5DB5VLAQ>.; You are receiving this because you authored the thread.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1988#issuecomment-1372299354:750,Message,Message,750,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1988#issuecomment-1372299354,1,['Message'],['Message']
Integrability,"Here we go. ~~It’s not done yet~~:. - [x] The timing stuff isn’t yet implemented. I think we just do timing stuff when doing “info” level logging, correct?. If yes, we probably just need to override `RootLogger.info` so it sets `self.handlers[0].formatter.passed_time = kwargs.get('t', False)`. - [x] All the places hackily using `_settings_verbosity_greater_or_equal_than(2|3)` have to work differently. I propose that we just add a kwarg `deepinfo: str` or so which only adds the passed string to the message if the active verbosity is higher than the function’s (i.e. calling `logging.warn('foo', deepinfo='bar')` and the loglevel/verbosity is 'INFO' or noisier adds 'bar'). - [x] We now use vanilla log function syntax, so no more using it like `print`. We switched to Python 3.6+ though, so I propose f-strings everywhere! That looks better and works. Fixes #256",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/676:503,message,message,503,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676,1,['message'],['message']
Integrability,"Here's a paper that compares various integration methods, with some nice figures showing which ones output corrected counts vs. corrected projections:. https://www.nature.com/articles/s41592-021-01336-8. You could use one of the methods that outputs corrected counts if you need corrected counts for some reason.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2314#issuecomment-1240118673:37,integrat,integration,37,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2314#issuecomment-1240118673,1,['integrat'],['integration']
Integrability,"Here's pretty much what I've tried:. ```python; import scanpy as sc; from functools import wraps, partial. pca = wraps(sc.pl.scatter)(partial(sc.pl.scatter, basis=""pca"")); pca( #tried tab completion here; ```. Tab completion shows:. <img width=""597"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/54574990-8e796f80-4a46-11e9-8ad4-ae5aeead9604.png"">. As opposed to:. <img width=""598"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/54580161-fdf95a00-4a5a-11e9-9b51-8eeec17babfc.png"">",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/535#issuecomment-474168347:91,wrap,wraps,91,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/535#issuecomment-474168347,2,['wrap'],['wraps']
Integrability,"Hey @adamgayoso ,. I really love this HVG method, but sometimes I get the following error from the loess fit:. ```pytb; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-244-764977f87ce6> in <module>; ----> 1 sc.pp.highly_variable_genes(ad_sub, n_top_genes=1500, flavor='seurat_v3', layer='counts'); 2 sc.pp.pca(ad_sub); 3 sc.pp.neighbors(ad_sub); 4 sc.tl.umap(ad_sub); 5 sc.tl.leiden(ad_sub, resolution=2.0). ~/.miniconda3/lib/python3.8/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key); 413 ; 414 if flavor == 'seurat_v3':; --> 415 return _highly_variable_genes_seurat_v3(; 416 adata,; 417 layer=layer,. ~/.miniconda3/lib/python3.8/site-packages/scanpy/preprocessing/_highly_variable_genes.py in _highly_variable_genes_seurat_v3(adata, layer, n_top_genes, batch_key, span, subset, inplace); 82 x = np.log10(mean[not_const]); 83 model = loess(x, y, span=span, degree=2); ---> 84 model.fit(); 85 estimat_var[not_const] = model.outputs.fitted_values; 86 reg_std = np.sqrt(10 ** estimat_var). _loess.pyx in _loess.loess.fit(). ValueError: b'reciprocal condition number 7.4971e-16\n'; ```. This is due to the very low but non-zero variance genes, I think. It goes away when I run `sc.pp.filter_genes(ad_sub, min_cells=5)` but not when I run only `sc.pp.filter_genes(ad_sub, min_cells=1)`. Maybe we can make the variance check more stringent, or we can print a better error message for the users?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1182#issuecomment-708677512:1611,message,message,1611,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1182#issuecomment-708677512,1,['message'],['message']
Integrability,"Hey @giovp & @ivirshup,; hope you had a good start into 2022! I was getting a twitter request recently asking about when this PR will be merged - are there any news on the timeline yet?. For the PR itself I made suggestions for the few remaining points (see my previous post) - just ping me here if you have feedback on that or if there is anything else to do!. Looking forward to wrap this up :); Best, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-1030169133:381,wrap,wrap,381,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1030169133,2,['wrap'],['wrap']
Integrability,"Hey @gokceneraslan,. I'm surprised at how you describe the contents of `adata.var['highly_variable']` when `batch_key` is set. I wrote a function that does pretty much exactly the same thing building upon use of `batch_key` for our data integration benchmarking, as I thought this wasn't available in scanpy. I recall looking through the code and thinking this was missing. Maybe we can compare functions for that to see if we're doing exactly the same thing or not?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1032#issuecomment-616820714:237,integrat,integration,237,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032#issuecomment-616820714,1,['integrat'],['integration']
Integrability,"Hey @victorlga are you still interested in fixing this? :); I think deleting the line order=keys where catplot is used should do the trick, then also the seaborn dependency could be loosened to allow for `0.13.0`...; What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2680#issuecomment-1791425380:162,depend,dependency,162,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2680#issuecomment-1791425380,1,['depend'],['dependency']
Integrability,"Hey @ywen1407!. The ideal case is that you don't pre-filter the gene sets before concatenating. Then, if you have aligned both sets of samples to the same genome, everything should be fine and you can filter out genes afterwards. Otherwise an outer join would only assume all values you filtered out were 0, which is probably not the way forward. That's why the only decent option you really have is an inner join. I assume you should have the unfiltered objects somewhere though. Regarding memory use: ComBat is something we (actually, this was thanks to @Marius1311) just re-implemented from python and R code that was flying around. We do not generally optimize methods that were published elsewhere. How much RAM are you using that it's crashing? I think Marius even made ComBat usable for sparse matrices, so it's already using less memory than it was before. 38K cells doesn't sound like something that would require more than 16GB RAM. I can run datsasets with 50k locally. You can of course always try other batch correction/data integration methods that are less memory intensive such as BBKNN or scVI. We tested scalability of data integration tools (also BBKNN and ComBat memory use) here: https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2. However, ComBat is one of the least memory intensive methods out there... so maybe there is little room for optimization here...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1431#issuecomment-698818414:1038,integrat,integration,1038,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1431#issuecomment-698818414,4,['integrat'],['integration']
Integrability,"Hey Dmitry, happy New Year's to you too!. > Can one use openTSNE code for computing perplexity-based weights or would one need to copy the binary search in here? [...] I noticed that you implemented `UniformAffinities ` in here, but isn't it part of openTSNE already?. No, I think we should be able to call the existing machinery. But we'd need to do something like I do with the Uniform affinities here. The reason I had to write separate classes is that the ones in openTSNE calculate the KNNG internally, and don't really offer a way to pass an existing KNNG. In openTSNE that makes sense, since otherwise, the API would be pretty complicated. But here, we have to deal with that. As you can see, it's a pretty trivial wrapper anyway. > How would tsne function know if it should use the uniform kernel or the weights constructed by the neighbors function?. I noticed that `sc.tl.umap` and now `sc.tl.tsne` add their parameters to `adata.uns`. I would imagine `sc.pp.neighbors` probably do the same, and if not, that seems like an easy addition, which is in line with the scanpy architecture. Determining which affinity kernel to use would then be as simple as looking into `adata.uns` to find which parameter value `sc.pp.neighbors` was called with. > I would definitely suggest to add `exaggeration=1` argument to `tsne()`. I added `exaggeration=None`, as is the default in openTSNE. But setting it to 1 instead of None is better, and I should change that in the next release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-753617428:722,wrap,wrapper,722,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-753617428,2,['wrap'],['wrapper']
Integrability,"Hey Phil! I'm still a bit hesitant to adopt this option. As discussed before, I'm mainly planning on integrating c++ extensions. Then doing everything via Cython seems overhead. Even the latest Cython distributions recommends **not** doing it. See [here](http://cython.readthedocs.io/en/latest/src/reference/compilation.html) and search for *distributing cython modules*. They provide a solution very similar to the one fom stackoverflow that we have adopted right now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/20#issuecomment-304808803:101,integrat,integrating,101,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/20#issuecomment-304808803,1,['integrat'],['integrating']
Integrability,"Hey again,. I addressed many of @ivirshup's comments by now and think I am almost done - will finish up the rest next week if all goes well. What is left todo:; - [ ] Make tests faster (re-use results where possible); - [ ] Make tests more code-efficient by code-sharing between functions where possible. Where I could use your / @giovp's input to continue:; - on the keyword/positional argument issue; - on the the ""is median rank a good way to do HVG selection across batches""-issue; - on the question what the final names of the functions should be - your suggestions were:; > `normalize_pearson_residuals` -> `pearson_residuals`; > It's a bit more like log1p; >; > `sc.experimental.pp.highly_variable_genes` -> something else; > I think using an already used function name (highly_variable_genes) and giving it a different API can be confusing. Would calling this pearson_deviant_genes or something like that be better? I do generally dislike how many methods highly_variable_genes wraps already though. Looking forward to the last bits :) ; Cheers, Jan. PS: I'm sorry for the problems that my dirty force-pushing caused before, I hope now everything works fine! I was not aware of the consequences for the comment history back then, but will now take care not to do it again",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-902985395:986,wrap,wraps,986,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-902985395,1,['wrap'],['wraps']
Integrability,"Hey everyone, thanks for the discussion so far! I don't have much to add to what @dkobak said earlier, so let me summarize a bit from my perspective:. I am motivated to contribute the method here because people were interested to use it with scanpy after seeing the preprint, and scanpy devs reached out to us to implement it here. For that it does not matter if it ends up in `external` or `core`, but as @giovp mentioned, the code is easy to integrate into the existing normalize/hvg-selection workflow and the method itself is well connected to established workflows. @adamgayoso raised the question if new preprint methods should be allowed in `core` at all, had several suggestions how this PR could be handled (halt until peer review publication/put in `external` for now/extend method to support also e.g. deviance residuals and others), and some open questions about the exact workflow integration. I would like to clarify with everyone how to proceed now. @ivirshup @LuckyMD, could you help us a bit to decide how to move forward?. In terms of development, I answered all of your code review comments @giovp, so maybe you can briefly check & resolve those you are happy with..?! I am also ready to finally write tests once we are decided on where this PR is going.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-801883490:444,integrat,integrate,444,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-801883490,2,['integrat'],"['integrate', 'integration']"
Integrability,"Hey everyone, thanks for your feedback! In the latest commit, I have tried to include all of your comments, including the more stylistic comments, the references, the numba integration, the unit tests and so on. Have a look and see what you think. I won't be able to work on this any more this year because I am going on holidays. Merry Christmas everyone!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/398#issuecomment-448304646:173,integrat,integration,173,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398#issuecomment-448304646,2,['integrat'],['integration']
Integrability,Hey! I looked at multiplex louvain a bit a few years ago (and put it in a grant that didn't get funded in the end ^^)... i guess one of the difficult things to actually using this is tuning the inter layer weight. I reckon this should actually be regarded as a new approach to multi-modal data integration. And it would require quite a bit of parameter tuning to understand how these edge weights need to be tuned. Hence I'm not sure if we just want to add it like this...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1818#issuecomment-828389504:294,integrat,integration,294,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818#issuecomment-828389504,1,['integrat'],['integration']
Integrability,"Hey! I thought plotting `.var` columns in `sc.pl.violin()` worked previously (and the error message seems to suggest this as well). I am doing the following:; ```; adata_counts.var['dropout_per_gene'] = (adata_counts.X > 0).mean(0); adata_counts.obs['dropout_per_cell'] = (adata_counts.X > 0).mean(1). sc.pl.violin(adata_counts, keys='dropout_per_gene'); ```. and I get this error:; ```; ---------------------------------------------------------------------------; IndexError Traceback (most recent call last); <ipython-input-8-463060c90a0b> in <module>(); ----> 1 sc.pl.violin(adata_counts, keys='dropout_per_gene'). ~/scanpy/scanpy/plotting/anndata.py in violin(adata, keys, groupby, log, use_raw, stripplot, jitter, size, scale, order, multi_panel, show, xlabel, rotation, save, ax, **kwds); 630 X_col = adata.raw[:, key].X; 631 else:; --> 632 X_col = adata[:, key].X; 633 obs_df[key] = X_col; 634 if groupby is None:. ~/anndata/anndata/base.py in __getitem__(self, index); 1303 def __getitem__(self, index):; 1304 """"""Returns a sliced view of the object.""""""; -> 1305 return self._getitem_view(index); 1306 ; 1307 def _getitem_view(self, index):. ~/anndata/anndata/base.py in _getitem_view(self, index); 1306 ; 1307 def _getitem_view(self, index):; -> 1308 oidx, vidx = self._normalize_indices(index); 1309 return AnnData(self, oidx=oidx, vidx=vidx, asview=True); 1310 . ~/anndata/anndata/base.py in _normalize_indices(self, index); 1283 obs, var = super(AnnData, self)._unpack_index(index); 1284 obs = _normalize_index(obs, self.obs_names); -> 1285 var = _normalize_index(var, self.var_names); 1286 return obs, var; 1287 . ~/anndata/anndata/base.py in _normalize_index(index, names); 261 return slice(start, stop, step); 262 elif isinstance(index, (int, str)):; --> 263 return name_idx(index); 264 elif isinstance(index, (Sequence, np.ndarray, pd.Index)):; 265 # here, we replaced the implementation based on name_idx with this. ~/anndata/anndata/base.py in name_idx(i); 248 raise IndexError(; 249 ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/375:92,message,message,92,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375,1,['message'],['message']
Integrability,"Hey!. I hope you are doing well. I run into the error AttributeError: module 'umap' has no attribute '__version__' when running this code:. ```; import pandas as pd; #pd.set_option(""display.max_columns"", None); import numpy as np; import anndata; import scanpy as sc. %store -r df. adata = anndata.AnnData(df); sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40); sc.tl.paga(adata); sc.pl.paga(adata, plot=False); sc.tl.umap(adata, init_pos='paga'); ```. I am using Python 3.8 on Jupyter Notebooks on Mac M1. Please find below the error in a more detailed fashion:. ```; AttributeError Traceback (most recent call last); <ipython-input-7-7cfb2fb3103e> in <module>; ----> 1 sc.tl.umap(adata, init_pos='paga'); 2 sc.pl.umap(adata). ~/opt/anaconda3/lib/python3.8/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key); 141 import umap; 142 ; --> 143 if version.parse(umap.__version__) >= version.parse(""0.5.0""):; 144 ; 145 def simplicial_set_embedding(*args, **kwargs):. AttributeError: module 'umap' has no attribute '__version__'; ```. Would you have any idea on how to solve this? I think it is a package dependency problem. Thank you in advance for your help!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1978:1234,depend,dependency,1234,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1978,1,['depend'],['dependency']
Integrability,"Hey!. You used the data integration methods incorrectly as far as I can see. Please read the documentation for the functions first. `mnnpy.mnn_correct()` takes all the batches as separate anndata objects as positional argument. So you need to do:; `mnnpy.mnn_correct(adata_batch1, adata_batch2, adata_batch3,...)` to run it. For `sc.pp.combat()` you didn't specify where your batch information was stored. And I'm surprised BBKNN didn't work for you. Did you run a `sc.pp.pca()` first?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/873#issuecomment-542686602:24,integrat,integration,24,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873#issuecomment-542686602,1,['integrat'],['integration']
Integrability,"Hey!; > * What methods/ tools?; I am mainly thinking about normalization and data integration methods. For example scran pooling, sctransform, scNorm, Seurat data integration, LIGER... etc. I have most of those already... But anyone is welcome to contribute for anything they regularly use. > * How would you handle R depencies?; So far I've been ignoring this problem and just assuming people have an R environment installed that has the relevant packages. You could just stick a `require(package)` in the function called by `rpy2` and then if would give you an `R` error you can interpret. The plan would be to make this a set of convenience functions, but not a cleanly installable module I guess... I'm not sure how you could get any python setup to install R dependencies for you... > * And (probably hard and definitely not necessary at first) could we use [arrow](https://arrow.apache.org/docs/python/) to speed up data transfer?; This looks interesting... but I don't entirely understand it... you'd have to have a a separate data structure that can move been languages, and be interpreted as an R data structure or `AnnData` depending on where it's used? Most methods are designed to run on a particular class of object. How would this help if you always have to convert to that type of object? So far I've just been using `anndata2ri` to ensure we have an `SCE` object which can be converted to other `R` data structures.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1068#issuecomment-590143256:82,integrat,integration,82,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-590143256,4,"['depend', 'integrat']","['dependencies', 'depending', 'integration']"
Integrability,"Hey!; I'm a member of g:Profiler (biit.cs.ut.ee/gprofiler) development team and was scanning through web to find services that might depend on us. . We recently went live with an extensive update which might break some of the previous pipelines and wrappers. . All the existing Python and R packages should work, however they are linking to an archived data version and they don't access the most up-to-date data from g:Profiler due to the new API etc. . We have already created a new R package that corresponds to the new API, Python package is still in the progress. . I just wanted to let you know and please feel free to contact me if I can be of any help. All the best,; Liis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/467#issuecomment-466359205:133,depend,depend,133,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467#issuecomment-466359205,2,"['depend', 'wrap']","['depend', 'wrappers']"
Integrability,"Hey, thanks for your reply!. I looked a bit around, and here is what the Seurat 3.1.4 docs say:. > Choose the features to use when integrating multiple datasets. This function ranks features by the number of datasets they appear in, breaking ties by the median rank across datasets. It returns the highest features by this ranking. from https://www.rdocumentation.org/packages/Seurat/versions/3.1.4/topics/SelectIntegrationFeatures. From this, I'd conclude that the current docs are correct, but in the sorting order of `_highly_variable_genes_seurat_v3` has it the wrong way around. Also, the test for the `_highly_variable_genes_seurat_v3()` method seems to assume that the method sorts the other way around than it currently does:. From within the method:. https://github.com/theislab/scanpy/blob/ca07fc12bbcd87e4cf67da56f52525a1e519711b/scanpy/preprocessing/_highly_variable_genes.py#L139-L144. From the test:. https://github.com/theislab/scanpy/blob/ca07fc12bbcd87e4cf67da56f52525a1e519711b/scanpy/tests/test_highly_variable_genes.py#L138-L151. So from this it seems save to say that the sorting order should be reversed in `_highly_variable_genes_seurat_v3()`..?!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1733#issuecomment-802052402:131,integrat,integrating,131,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1733#issuecomment-802052402,1,['integrat'],['integrating']
Integrability,"Hey,; while writing tests for #1715 I noted the following behavior:. `output = sc.pp.highly_variable(adata,inplace=True,subset=False,n_top_genes=100)`; --> Returns nothing :heavy_check_mark: ; --> `adata.var` fields are updated but shape stays the same :heavy_check_mark: . `output = sc.pp.highly_variable(adata,inplace=True,subset=True,n_top_genes=100)`; --> Returns nothing :heavy_check_mark: ; --> `adata.var` fields are updated and shape is subsetted correctly :heavy_check_mark: . `output = sc.pp.highly_variable(adata,inplace=False,subset=False,n_top_genes=100)`; --> output is a dataframe with the original number of genes as rows :heavy_check_mark: ; --> `adata` is unchanged :heavy_check_mark: . `output = sc.pp.highly_variable(adata,inplace=False,subset=True,n_top_genes=100)`; --> Returns nothing :x: ; --> `adata` shape is changed an `var` fields are updated :x: . I think the last case is unexpected, assuming that `inplace=False` should protect `adata` from changes and results in an output. I think these parts of code are the cause (depending on the flavor):. https://github.com/theislab/scanpy/blob/4dd8de9e355ce8d59009c15522670a3d0970462c/scanpy/preprocessing/_highly_variable_genes.py#L148-L174. https://github.com/theislab/scanpy/blob/4dd8de9e355ce8d59009c15522670a3d0970462c/scanpy/preprocessing/_highly_variable_genes.py#L528-L553. To fix this, one could change `if subset or inplace:` to `if inplace:`. Then one would need to add another `if subet:` in the `else` block, like so (half-pseudocode):. ```; if inplace: ; ; #update adata; ; if batch_key is not None:; #drop batch related keys; if subset:; adata._inplace_subset_var(df['highly_variable'].values); else:; if batch_key is None:; #drop batch related keys; if subset: ; df=df.iloc[df.highly_variable.values,:]; ; return df; ```. I can make a quick PR if this is the intended behavior. :slightly_smiling_face: ; best, jan.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1867:1049,depend,depending,1049,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1867,1,['depend'],['depending']
Integrability,"Hi @JonathanShor,. you don't need to create a custom API. One point of Scanpy is to provide convenient access via `anndata` to many single-cell packages around. The only thing needed for that is to provide a very simple interface like [this](https://github.com/theislab/scanpy/blob/master/scanpy/tools/phate.py#L8-L145) or [this](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/mnn_correct.py#L4-L104) or several of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/173#issuecomment-399367409:220,interface,interface,220,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-399367409,2,['interface'],['interface']
Integrability,"Hi @LuckyMD - thanks for your reply! Yeah that makes sense. I'm performing these corrections using a subset of highly variable genes, so I guess to ""make up"" for the loss of ""true"" HVGs in the new subclusters of cells I could select a higher number of HVGs to perform the original alignment? As well as maybe using a larger number of components for downstream applications from the low-dimensional embedding outputted by the original alignment. Does that make sense to you?. One more question - when performing differential gene expression analysis, what is your preferred pipeline/method when using aligned datasets? I generally do not perform the correction on the gene expression matrix when aligning, and I think doing DE with corrected matrices is not as common. So maybe other methods that use batch as a covariate would be preferable (e.g. diffxpy or others?) Would really appreciate any suggestions here!. PS. many congratulations on the benchmarking integration paper in Nature Methods - excellent work and very useful resource for the field!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2162#issuecomment-1061085766:959,integrat,integration,959,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2162#issuecomment-1061085766,1,['integrat'],['integration']
Integrability,"Hi @LuckyMD ; Thank you for the fast reply. Yes to FastMNN, as I understand from using align_cds – when you specify discretely what you want to remove e.g. sample-sample variation it calls FastMNN from batchelor. Thanks for the recommendation – I will check out Scanorama, been meaning to read the review on integration techniques. . > you will only get an integrated graph structure with this for scvelo, which may help a little, but won't remove the batch effect for RNA velocity calculation. scvelo doesn't currently have any batch removal in its pipeline as it is quite difficult to add as it works directly from the normalized count data and fits a model to these. Ahh okay, I misunderstood the process then – my understanding was that some of the mnn correction would be carried over when performing velocity analysis. I will check out the scvelo forum for info on comparing samples. . Thank you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1289#issuecomment-735661916:308,integrat,integration,308,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1289#issuecomment-735661916,2,['integrat'],"['integrated', 'integration']"
Integrability,"Hi @LuckyMD,. Many thanks for your comments. . The route via PCA followed by clustering & embedding (UMAP/tSNE) works perfectly fine for me. I have also got some interesting results from the analysis. Now, I want to try clustering cells with specific gene sets instead of the conventional dimensional reduction. Yes, I tried the following lines before:. ```; adata.obsm['X_geneset1`] = adata[:, ['gene1', 'gene2', 'gene3', 'gene4']].X; ```; It still says, KeyError: 'Indices ""[\'Ada\', \'Mustn1\', \'Mlc1\', \'Gfra\', \'Gm765\', \'Csrp2\', \'Socs2\', \'Dnajb9\']"" contain invalid observation/variables names/indices.'. All of these genes are present in my dataset. I am still trying to figure out why this is happening :/ ; Maybe, I will paste the short code snippet later. . P.S: Sorry for getting off the subject. Is there an alternative normalization step included apart from the log-normalization method? For example, TMM in edgeR & SCnorm- that uses quantile regression to calculate the dependence of read counts on sequencing depth for each gene (when count-depth relationship varies among genes).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/510#issuecomment-488001552:51,rout,route,51,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510#issuecomment-488001552,2,"['depend', 'rout']","['dependence', 'route']"
Integrability,"Hi @LuckyMD,. Sure, I'll work on it, as time allows. Before however, I have a couple of questions. . 1. Do you want it as a separate .py file in the tools module (similar to _dendrogram.py)?; 2. I also found interesting to look at exclusive expression of one gene and not the other. Would you be interested in adding a function for that as well and if so, should be a separate one or somehow integrated with coexpression?; 3. Turning values into categorical works, however now I have problem that the True (coexpressing) cells are not always plotted on top. Do you know how to do it in scanpy? I tried by setting `pd.Categorical(ordered = True)`, however, that doesn't help. ; 4. Could you elucidate on how you want to implement the imputation methods? I've never used them myself. Is there anything available in scanpy already?. And thanks @flying-sheep for showing how to remove the colourbar. I wanted to do it for some of my other plots, so that really helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/490#issuecomment-588132560:392,integrat,integrated,392,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/490#issuecomment-588132560,1,['integrat'],['integrated']
Integrability,"Hi @Olivia117,. Let's see if I can help. I think there are a few misunderstandings here. It appears that you are mixing the `adata.var['highly_variable']` approach with the `adata.obsm['X_geneset1']` approach Alex suggested. Firstly, there is a typo in Alex' code above. It should read:; ```; adata.obsm['X_geneset1'] = adata[:,['gene1', 'gene2', 'gene3', 'gene4']].X; sc.pp.neighbors(adata, use_rep='X_geneset1'); ```; I believe. Your error is due to this typo. The command is interpreting `'Map7d1'` as a cell index rather than a gene index. However, there are also a few other things.; 1. `adata.var['highly_variable']` takes a boolean list, so you should assign e.g., `[True, True, False, False]` if you are interested in only the first two genes out of a total of 4 genes in the dataset. This can be trivially extended to select your Gene1, Gene,... Gene500 that you are interested in. When using this approach you will need to run `sc.pp.pca(adata, svd_solver='arpack', use_highly_variable=True)` and `sc.pp.neighbors(adata)` before clustering with louvain or leiden. This approach subsets to your genes of interest, then performs PCA on this gene subset, and builds a KNN graph based on Euclidean distances in this PCA space, which is then used for clustering.; 2. If you don't want to use the route via PCA, you need to assign to `adata.obsm` as Alex suggests (with my typo correction above). Even if you do not have anything in `adata.obsm`, it should still work. If you want to put something in `adata.obsm`, just run `sc.pp.pca(adata, svd_solver='arpack')` and you will see `adata.obsm['X_pca']` appear. Hope this helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/510#issuecomment-487980089:1301,rout,route,1301,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510#issuecomment-487980089,1,['rout'],['route']
Integrability,"Hi @PauBadiaM,. I have always viewed Dorothea and Progeny as methods to aid in the interpretation of my data. Hence, I would assume this might be most useful as a targeted approach to plot activity of a particular TF or pathway. This is something I would probably find most useful as a function where i can either ask for the activity of a single TF/pathway or to get the activity score that explains most variation/correlates with a particular PC. Hence I would err on the side of storing the activities in `.obsm` and then have some functionality around analysing which activity scores are most useful to a user. It will be hard for users to go through all of the data in the end for further analysis. You can always write a wrapper around things like `sc.tl.rank_genes_groups` where the `.obsm` data is copied into a new `adata_tmp.X` for rank genes groups output.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1724#issuecomment-794274079:727,wrap,wrapper,727,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1724#issuecomment-794274079,1,['wrap'],['wrapper']
Integrability,"Hi @VladimirShitov . Thank you for the help (and the information about leiden vs UMAP). I think the code provided shows something slightly different. You are plotting False vs True here, but we would want something like False vs all. So, the True violin plot would be a little different. Regardless, I am just going to down this route :D",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2485#issuecomment-1542320460:329,rout,route,329,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2485#issuecomment-1542320460,1,['rout'],['route']
Integrability,"Hi @Zethson I am the creator of Cirun.io, ""GPU"" and ""CI"" caught my eye. FWIW I'll share my two cents. I created a service for problems like these, which is basically running custom machines (including GPUs) in GitHub Actions: https://cirun.io/. It is used in multiple open source projects needing GPU support like the following:. https://github.com/pystatgen/sgkit/; https://github.com/qutip/qutip-cupy. It is fairly simple to setup, all you need is a cloud account (AWS or GCP) and a simple yaml file describing what kind of machines you need and Cirun will spin up ephemeral machines on your cloud for GitHub Actions to run. It's native to GitHub ecosystem, which mean you can see logs/trigger in the Github's interface itself, just like any Github Action run. Also, note that Cirun is free for Open source projects. (You only pay to your cloud provider for machine usage)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1793#issuecomment-881043172:712,interface,interface,712,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793#issuecomment-881043172,2,['interface'],['interface']
Integrability,"Hi @adamgayoso , thanks for the comment, your raised very fair points. I disagree on couple of them but I think it's a very healthy discussion: . > then it does belong in scanpy more formally I think. In that sense, it sets a strange precedent about what belongs inside the main scanpy, versus external. the discussion on whether to include this in `scanpy.external` or `scanpy.core` was carried out here: https://github.com/berenslab/umi-normalization/issues/1 , two key take home messages from that were (imho):; - the simplicity of the method, in terms of codebase, and its scalability makes it suitable to be hosted in `core`.; - it is not strictly a new method, but has several connections with previous [sctransform](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1874-1) and [glm-pca](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6) (also, not sure on what basis you said that ""`glm-pca` is supposed to be better"", would be genuinely curious to see some evaluations). > This is just a general comment, but is it a bit rushed to include the analytic pearson residuals method in the main scanpy module given that the method has only been described in a preprint?. I also disagree about peer-review being a gold standard about legitimacy of the method: I find it a bit unusual in light of the ever-lasting discussion of peer-review flaws in academia, and I personally use non-peer-reviewed computational tools all the time. Beside that, I think you raise 2 very important points here (that are possibly flaws on Scanpy side):; - Should there be a transparent and more thorough vetting process about what is added in Scanpy, especially if it's something fundamental like normalization? (imho yes); - Should this process be somehow formalized, e.g. a common issue title like `[new method] My new method` ?. With such a system in place, I think it would have enabled you (and others) to express your disagreement at earlier stage (as you wouldn't poss",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-799276115:482,message,messages,482,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-799276115,2,['message'],['messages']
Integrability,"Hi @all,; Thanks to develop the great tools,; I encounter a pecular problem on bbknn integrated data.; i follow the workflow code to run the data integrated,the code showing below,; #; sc.pp.highly_variable_genes(adata); adata = adata[:, adata.var['highly_variable']]; sc.tl.pca(adata, svd_solver='arpack'); sc.tl.tsne(adata); ///data integrated; sc.external.pp.bbknn(adata, batch_key='orig.ident'). sc.tl.umap(adata); adata; sc.pl.umap(adata, color=['orig.ident']); showing the well integrated, picture below,; ![image](https://user-images.githubusercontent.com/41668708/90249921-df74d980-de6d-11ea-8283-3830dd3c6ad2.png); But,when i want to see the tsne picture, the batch from different sample showing up on the tsne but umap like above picture; ,i runing ,; sc.tl.tsne(adata); sc.pl.tsne(adata, color=['orig.ident']); the picture show below, indicating that the integrated can not be worked on tsne.; ![image](https://user-images.githubusercontent.com/41668708/90250154-3c708f80-de6e-11ea-8d43-7a597c5693a2.png); So, why this tsne showing significantly different with the object just running over the integrated process.; any advice would be appreciated; Best,; hanhuihong",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1370:85,integrat,integrated,85,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1370,6,['integrat'],['integrated']
Integrability,"Hi @brianhie,. It's great that you're contributing to Scanpy to make the interoperability even easier (I guess it was already quite good given you built on `AnnData`). We have been evaluating data integration methods and in which Scanorama performed quite well (you may have seen the [preprint](https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2)). One aspect that would make it even easier to use the tool that we were missing in the comparison is a small tutorial. The example in the function docstring is already very helpful, but do you think it would be possible to add a quite jupyter notebook in this direction? This is obviously a request outside of this PR. On the topic of the PR, I wonder if `adata.obsm['X_pca_scanorama']` is a good default name for the generated embedding, and not just `adata.obsm['X_scanorama']` as the standard user may not have delved into the methodology as much.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1332#issuecomment-665592723:73,interoperab,interoperability,73,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1332#issuecomment-665592723,2,"['integrat', 'interoperab']","['integration', 'interoperability']"
Integrability,"Hi @chansigit,; If you want to store the raw counts before filtering out cells/genes you can also do this in `adata.raw`. We're trying to reduce the use of this... but it will allow you to store data in a different dimension. @ivirshup I guess use of scaling is up in the air. Some people like it, some people don't. I find it can be helpful for data integration/batch correction.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1089#issuecomment-596466943:351,integrat,integration,351,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1089#issuecomment-596466943,1,['integrat'],['integration']
Integrability,"Hi @cornhundred,. While I can't speak for the intention of the authors of MNN, typically one would use 3000-6000 highly variable genes in scRNA-seq data analysis. That tends to cover the most important sources of variation. If you have a very deeply sequenced dataset from a sensitive scRNA-seq protocol (Smart-seq2/mcSCRB-seq?) with a lot of heterogeneity, you could make an argument for using more. Generally, 10,000 is a lot though. I would probably use fewer genes. The new `highly_variable_genes()` function does not subset the genes anymore, but instead creates a `.var['highly_variable']` column which stores a boolean variable indicating which genes are highly variable and which are not. You should be able to use this column to subset adata.var_names as an input to `sce.mnn_correct()` via the `var_index` and `var_subset` parameters. Using these inputs should not subset your `AnnData` object. Batch correction can create negative gene expression levels. People tend to deal with this differently. Some people force pre-batch-correction zeros to remain zero, others cast negative values to zero, and others again ignore it. I don't think there's a best approach to this. In the end you will probably get similar results in terms of embedding and trajectory inference. You just have to be careful how you interpret the gene expression values themselves. I have so far ignored it. By the way, I've written a bit about these topics in my best practices tutorial. The case study that goes with the manuscript (currently under review) is publicly available [here](https://github.com/theislab/single-cell-tutorial)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/449#issuecomment-458072946:295,protocol,protocol,295,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/449#issuecomment-458072946,1,['protocol'],['protocol']
Integrability,"Hi @falexwolf . Added a small wrapper for phenograph clustering, similar to phate.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/292:30,wrap,wrapper,30,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/292,1,['wrap'],['wrapper']
Integrability,"Hi @falexwolf, thanks for the solution you provided above for reading multiple files. I tried it and it worked when I had just 2 files. I am trying the same code with 23 files and I am getting an error message in the concatenation step. Any idea on how to fix this ? Thanks. ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-20-662b857d9182> in <module>; 12 adatas.obs['cell_names'] = pd.read_csv(path + sample + 'barcodes.tsv.gz', header=None)[0].values; 13 ; ---> 14 adata = adatas[0].concatenate(adatas[1:]). /Applications/anaconda3/lib/python3.7/site-packages/anndata/core/anndata.py in concatenate(self, join, batch_key, batch_categories, index_unique, *adatas); 1908 ; 1909 if any_sparse:; -> 1910 sparse_format = all_adatas[0].X.getformat(); 1911 X = X.asformat(sparse_format); 1912 . AttributeError: 'numpy.ndarray' object has no attribute 'getformat'",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/267#issuecomment-602964900:202,message,message,202,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267#issuecomment-602964900,1,['message'],['message']
Integrability,"Hi @falexwolf,. 10X is releasing a new version of CellRanger that is changing the output format. This pull request makes Scanpy forward compatible with the new version. In particular, the following changes are made:. Updated `read_10x_h5`:; - Renamed the original `read_10x_h5` as `_read_legacy_10x_h5`;; - Added `_read_v3_10x_h5` to read the new Cell Ranger output format;; - The new `read_10x_h5` determines the version of HDF5 input by the presence of the matrix key, and wraps the above two functions. In addition, it takes a `gex_only` argument which filters out feature barcoding counts from the outcome object when it is True (default). Otherwise, the full matrix will be retained.; - For CR-v3, `feature_types` and `genome` were added into the outcome object as new attributes. Updated `read_10x_mtx`:; - Renamed the original `read_10x_mtx` as `_read_legacy_10x_mtx`;; - Added `_read_v3_10x_mtx` to read the new Cell Ranger output format;; - The new `read_10x_mtx` determines the version of matrix input by the presence of the `genes.tsv` file under the input directory, and wraps the above two functions. In addition, it takes a `gex_only` argument which filters out feature barcoding counts from the outcome object when it is `True` (default). Otherwise, the full matrix will be retained.; - For CR-v3, `feature_types` was added into the outcome object as a new attribute. Added small test datasets and code for the revised functions to verify the expected behavior. Note for the `genome` argument:; - There is a genome argument in Scanpy's `read_10x_h5` function but not in `read_10x_mtx` as the genome was already specified by the path of input directory. The outcome object of the two functions should be the same which always take one genome at a time.; - In this PR, when there are multiple genomes (e.g. Barnyard), `read_10x_mtx` always read them all, whereas `read_10x_h5` always need to specify one of them (mm10 by default). However, when `gex_only == False`, the `genome` argument ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/334:475,wrap,wraps,475,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/334,1,['wrap'],['wraps']
Integrability,"Hi @fidelram, good to see you :smile:. I was working on the Galaxy integration. I tested that with the `1.3.2` version from Bioconda. I tested with adata from krumsiek11. - For colors, I tried with `sc.pl.scatter(adata=adata, x='EKLF', y='Cebpa', color=['EgrNab', 'cJun']) and I got the error:. ```; ...; and (color is None or color in adata.obs.keys() or color in adata.var.index)):; File ""path/to/lib/python3.6/site-packages/pandas/core/indexes/base.py"", line 2035, in __contains__; hash(key); TypeError: unhashable type: 'list'; ```. - For components: the command was . ```; sc.pl.scatter(; adata=adata,; x='EKLF',; y='Cebpa',; color='EgrNab',; layers=('X', 'X', 'X'),; use_raw=False,; sort_order=True,; components='all',; projection='2d',; legend_loc='right margin',; legend_fontsize=1,; legend_fontweight='normal',; palette='viridis',; frameon=True,; right_margin=1.0,; size=1.0,; show=False,; save='.png'); ```; and the error:. ```; components = np.array(components).astype(int) - 1; ValueError: invalid literal for int() with base 10: 'all'; ```. Did I put the parameters in a wrong way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/311#issuecomment-431284136:67,integrat,integration,67,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311#issuecomment-431284136,1,['integrat'],['integration']
Integrability,"Hi @flying-sheep! is there a fix for this issue? (besides pinning scanpy version) ; as i understand it, matplotlib went back to legend_handles ([API](https://matplotlib.org/stable/api/prev_api_changes/api_changes_3.9.0.html#:~:text=Legend.legendHandles%20was%20undocumented%20and%20has%20been%20renamed%20to%20legend_handles.)) and now the legacy wrapper is breaking.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3102#issuecomment-2162507912:347,wrap,wrapper,347,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3102#issuecomment-2162507912,1,['wrap'],['wrapper']
Integrability,"Hi @flying-sheep, just saw your updates here. I was slowly working on a re-structured scanpy-scripts that has a sub-command interface i.e `scanpy-cli read`, `scanpy-cli filter` etc and with some added functionality for convenience (https://github.com/ebi-gene-expression-group/scanpy-scripts/pull/40). I'll try change the interface back to make it compatible with yours.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/281#issuecomment-484475390:124,interface,interface,124,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281#issuecomment-484475390,2,['interface'],['interface']
Integrability,"Hi @freeman-lab, we have now quite a number of modules:. https://github.com/ebi-gene-expression-group/scanpy-scripts. And, as our other seurat-scripts, sc3-scripts and scater-scripts, it is bioconda installable (or in the way to be). We would be happy to accept your module, although it would be good to see how much it overlaps or not with existing parts already there, to find the best way to integrate it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/281#issuecomment-431712261:395,integrat,integrate,395,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281#issuecomment-431712261,1,['integrat'],['integrate']
Integrability,"Hi @giovp @ivirshup, (also CCing @dkobak). as we discussed at #1715 and in https://github.com/theislab/scanpy-tutorials/pull/43, I prepared a function `pearson_residuals_hvg_scatter()` that wraps `sc.pl.scatter()` to reproduce the ""gene selection plot"" from the tutorial. It can be used as a sanity check for both the HVG selection and the appropriateness of the used Pearson residual null model (as explained also in the tutorial notebook). I also added a feature to show known marker genes on the plot, change plot aesthetics and which fields are used for plotting. Looking forward to your thoughts on this one :); Best,; Jan. PS: I prepared this PR on an independent branch from #1715 - hope that is the correct way in this situation!; ```; import scanpy as sc; sc.settings.set_figure_params(dpi=80, facecolor=""white""). #run pearson residuals gene selection; adata=sc.datasets.pbmc3k(); sc.pp.filter_genes(adata, min_cells=1); sc.experimental.pp.highly_variable_genes(adata, flavor=""pearson_residuals"", n_top_genes=2000). #basic plot; sc.experimental.pl.pearson_residuals_hvg_scatter(adata); ```; ![image](https://user-images.githubusercontent.com/34481813/158408471-5d661d60-11f3-482d-981b-da3a507b64b4.png). ```; #modify some aesthetics; sc.experimental.pl.pearson_residuals_hvg_scatter(adata,kwargs_sc_pl_scatter=dict(size=30)); ```. ![image](https://user-images.githubusercontent.com/34481813/158408564-9ca8b476-c0e6-4159-8c4e-cc4b542f43c4.png). ```; #highlight some marker genes; markers = [""IL7R"", ""LYZ"", ""CD14"", ""MS4A1"", ""CD8A"", ""GNLY""]; sc.experimental.pl.pearson_residuals_hvg_scatter(adata,marker_names=markers,kwargs_sc_pl_scatter=dict(size=30)); ```; ![image](https://user-images.githubusercontent.com/34481813/158408630-f973cb16-0165-42ee-95e6-55e9e6de676e.png). ```; #use custom fields in `adata` for x and y; #(there is also a similar option to use a different field for where HVG flag is stored); sc.experimental.pl.pearson_residuals_hvg_scatter(adata,x='means',y='variances',return",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2176:190,wrap,wraps,190,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2176,1,['wrap'],['wraps']
Integrability,"Hi @giovp,; no worries, I hope you had a good TAC meeting! And thanks a lot for picking this up again, fixing the docs and also for starting the new issue on batch integration. I saw some of the github automated tests test are failing now, but I don't really understand the error messages tbh ;) Are they even related to the execution of the code provided by this PR?. If there is anything I should look into, let me know - I have some time for this next week!; Best, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-1049902277:164,integrat,integration,164,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1049902277,2,"['integrat', 'message']","['integration', 'messages']"
Integrability,"Hi @grimwoo,. The data integration methods MNN and BBKNN are implemented in scanpy externals, which you can find [here](https://scanpy.readthedocs.io/en/stable/external/index.html#batch-effect-correction). You can also use combat correction, which is a simpler, linear batch effect correction approach implemented as `sc.pp.combat()`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/702#issuecomment-527337268:23,integrat,integration,23,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702#issuecomment-527337268,2,['integrat'],['integration']
Integrability,"Hi @grst, I had a superficial look at the functionalities and setup and it does look very nice!. - BCR makes sense to add, there seems to be generally less happening in this space in single-cell though right now, compared to TCR. Would be good to have somebody on board who actually works on this data.; - [tcellmatch](https://github.com/theislab/tcellmatch)'s primary purpose is specificity prediction, this could be easily added ontop of this, I will look into your data structure and will think about the necessary changes. I am in the process of making this code public anyway, hopefully next week or so.; - You mentioned distance metrics, this is definitely an interesting and relevant area, in [tcellmatch](https://github.com/theislab/tcellmatch), we implicitly use 1. manhatten distances, 2. euclidian distances in BLOSUM embedding and 3. learned embedding distances, 2. and maybe 3. could be potentially integrated, would be worth discussing in any case.; - Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? These anticipated use cases would determine how and whether this makes sense i think.; - Potentially additionally relevant: An integration with dextramer counts to ""stain"" TCR specificity? There is the purely numeric, standard multi-modal single-cell, nature to this data that can be covered by standard scanpy work flows. This data is especially useful in the context of clonotypes etc which then would require additional functionalities, which could be built on what you have here. I have been looking into this type of analysis a lot in context of tcellmatch. Would be to contribute but also happy to see what other people do here, too!. Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Grea",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1163#issuecomment-613297254:912,integrat,integrated,912,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163#issuecomment-613297254,2,['integrat'],['integrated']
Integrability,"Hi @grst,. We've been working on a best-practices workflow/tutorial for scRNA-sesq data for the past year. The tutorial is based on scanpy, but it also integrates tools from R. You can take a look at a case study which implements the workflow [here](https://github.com/theislab/single-cell-tutorial). The revised manuscript should be submitted this week as well, so I hope it will be out soon. If you like, I can send it to you, if you pass me your email address. Regarding regressing out covariates. @falexwolf has already mentioned that this is very dataset dependent. It is also dependent on the downstream analysis. This is especially true for the covariates you suggest. MT gene expression and cell cycle are both biological, rather than technical covariates. Regressing out biological covariates is generally done to isolate particular processes in the data that you are interested in, while losing global structure in the data. This is helpful especially for trajectory inference, but maybe less so for global exploratory analysis with clustering. For example, cell cycle stage can be a major determinant in the difference between two cell types (e.g. stem cells and proliferating cells like transit amplifying cells). Removing this effect, hides the distinction. MT gene expression is also a biological covariate (as well as a technical indicator of cell stress). Higher levels of MT gene expression can indicate increased respiration in e.g., asthmatic conditions. . An additional unwanted effect to regressing out biological covariates is that biological processes are not independent. Thus, regressing out one process, will partially remove effects of other, downstream processes as well. If you're interested, we can discuss this in more detail. Hope this helps a bit,",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/526#issuecomment-471488594:152,integrat,integrates,152,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/526#issuecomment-471488594,3,"['depend', 'integrat']","['dependent', 'integrates']"
Integrability,"Hi @honghh2018,. Whatever is output depends on what you store in `adata.X`. If you don't want to output scaled data, then you can avoid calling `sc.pp.scale()` on your data. An alternative would be to save a version of your data before scaling in a different adata layer. For example, before scaling, you can just store a copy of your data by e.g., calling `adata.layers['normalized_unscaled] = adata.X`. You can export this data matrix by calling `adata.layers['normalized_unscaled'].to_csv(FILENAME)`. Hope this helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1650#issuecomment-779132748:36,depend,depends,36,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1650#issuecomment-779132748,1,['depend'],['depends']
Integrability,"Hi @ivirshup!. We've discussed this in Aptos a couple of months ago. Adding an `interactive` parameter to all the scatter plots would be really useful for working with notebooks. Would you consider adding that functionality as you have a lot of experience with it? Importantly, it should be based on the restructured plotting code that @fidelram is currently working on in https://github.com/theislab/scanpy/pull/244 (we could move that branch to the scanpy repo?). Hence, this would be for post-Scanpy 1.3 and there is no great hurry. A solution that takes an `AnnData` and creates an interactive plot but totally ignores the current way scatter plots are generated and Fidel's restructured way would be what follows below (due to @NDKoehler). Hence, the task is to think about a good way of integrating this with how scatter plots are done in Scanpy (after Fidel's changes).; ```; from bokeh.plotting import figure, show, output_notebook, save#, output_file; from bokeh.models import HoverTool, value, LabelSet, Legend, ColumnDataSource; from bokeh.palettes import viridis; output_notebook(). import matplotlib as mpl. def plot_interactive(data):. colors = [; ""#%02x%02x%02x"" % (int(r), int(g), int(b)) for r, g, b, _ in 255*mpl.cm.viridis(mpl.colors.Normalize()(data.obs['CCS'].values)); ]. source = ColumnDataSource(dict(; x=data.obsm['X_umap'][:,0],; y=data.obsm['X_umap'][:,1],; color=colors,#data.obs['CCS'],; label=data.obs['Charge'],; #msize= p_df['marker_size'],; #topic_key= p_df['clusters'],; #title= p_df[u'Title'],; #content = p_df['Text_Rep']; seq=data.obs['seq'],; ccs=data.obs['CCS'],; charge=data.obs['Charge'],; )); #ax = sc.pl.umap(data, color=['Charge','CCS']); #sc.pl.umap(data, color=['CCS'], save='ccs'). title = 'T-SNE visualization of sequences'. plot_lda = figure(plot_width=800, plot_height=600,; title=title, tools=""pan,wheel_zoom,box_zoom,reset,hover,previewsave"",; x_axis_type=None, y_axis_type=None, min_border=1). plot_lda.scatter(x='x', y='y', legend='label', source=",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/253:793,integrat,integrating,793,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/253,1,['integrat'],['integrating']
Integrability,"Hi @mrocklin, you might be interested in this work, especially the Dask-compatible wrapper around `scipy.sparse`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/921#issuecomment-557095212:83,wrap,wrapper,83,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-557095212,1,['wrap'],['wrapper']
Integrability,"Hi @pinin4fjords! I understand by integration, you mean access under the scanpy api. We try to advance the scanpy environment by modular extensions, which are packages with their own API, that also work on adata instances. This is currently what diffxpy is and there are no plans to collect all scanpy-related packages under `sc.*` as far as I am aware.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1955#issuecomment-884979935:34,integrat,integration,34,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1955#issuecomment-884979935,1,['integrat'],['integration']
Integrability,"Hi @r-reeves,; Maybe this is indeed a separate issue. `mnnpy` is indeed working on the gene expression matrix, and not on a low dimensional embedding like `FastMNN` (which is what I assume you might have been using?). You could try [Scanorama](https://github.com/brianhie/scanorama) which is a method similar to FastMNN, using a sped up algorithm and no iterative merging of batches, but a method they call ""panoramic stitching"". It has performed quite well in our [benchmark of data integration methods](https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2), and is in the scanpy ecosystem and therefore should work seamlessly in a Scanpy workflow. All of this being said, you will only get an integrated graph structure with this for scvelo, which may help a little, but won't remove the batch effect for RNA velocity calculation. scvelo doesn't currently have any batch removal in its pipeline as it is quite difficult to add as it works directly from the normalized count data and fits a model to these. @VolkerBergen has been thinking a bit about how to perform batch correction in an scvelo model, maybe he could chime in, or you could post an issue in the scvelo repo.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1289#issuecomment-734426157:484,integrat,integration,484,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1289#issuecomment-734426157,2,['integrat'],"['integrated', 'integration']"
Integrability,"Hi @saiteja-danda,. I cannot reproduce your code above as I don't have your data. Could you try to generate a minimal reproducible example with data from e.g., `sc.datasets.pbmc68k_reduced()`?. In general, could you check the output of `adata.obs['km'].value_counts()` to check whether the covariate you added was correctly stored? What output are you getting? You didn't share an error message.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1452#issuecomment-707656885:387,message,message,387,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1452#issuecomment-707656885,1,['message'],['message']
Integrability,"Hi @sygongcode,. Are you referring to differential expression testing between conditions? You can do that with `sc.tl.rank_genes_groups()` or in a more advanced way using `diffxpy`, which is easily integrated with `scanpy`. You can find it [here](https://github.com/theislab/diffxpy)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/821#issuecomment-529213147:198,integrat,integrated,198,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/821#issuecomment-529213147,1,['integrat'],['integrated']
Integrability,"Hi Ilan, thanks for your interest. One of the main advantages of Marsilea is the flexibility for layout plots and adding/removing components compared to the pre-defined visualization APIs in Scanpy. From my perspective, there could be two ways of integration: . 1) Reimplement some of the visualization APIs in scanpy using Marsilea, but we don't expose Marsilea to the user. You will always have a plot with known rendered size and fixed layout compared to directly using matplotlib. This could significantly reduce the code base complexities on the scanpy side, so less maintenance work. 2) Exposing the Marsilea API, we can create a visualization object that simulates the Marsilea API but is tailored specifically for `AnnData`. Maybe include some data transformation and aggregation functions that the user could directly apply during visualization. But this design doesn't make much difference compared to directly using Masilea as shown in the [notebook](https://scanpy.readthedocs.io/en/stable/how-to/plotting-with-marsilea.html).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2444#issuecomment-2352637693:247,integrat,integration,247,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444#issuecomment-2352637693,1,['integrat'],['integration']
Integrability,"Hi Isaac,. When I try to set n_comps equal to 2 (trying to do a diffmap in 2; components), I get an error message saying that it must be greater than 2.; I was wondering why?. On Sun, Jul 7, 2019 at 4:25 AM Isaac Virshup <notifications@github.com>; wrote:. > I'm not sure what you're asking about here. Could you provide a little; > more context?; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/675?email_source=notifications&email_token=AKIOHVNZFKCE63C4KLO45KTP6GSAPA5CNFSM4HSIFHXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODZLG7PQ#issuecomment-508981182>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AKIOHVL7254C36JSP374JOTP6GSAPANCNFSM4HSIFHXA>; > .; >; -- ; Harvard-MIT MD-PhD Student; G1, Biophysics; Lander Lab",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/675#issuecomment-508997047:106,message,message,106,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/675#issuecomment-508997047,1,['message'],['message']
Integrability,"Hi Lukas,. I am sorry, but what is a PR?. Best; Jin. ᐧ. On Fri, Mar 10, 2023 at 5:02 AM Lukas Heumos ***@***.***>; wrote:. > Would you be willing to file a PR for this? Thank you!; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/scverse/scanpy/issues/2436#issuecomment-1463561536>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AFOKIW345IGREVLPLR7F6BLW3L32JANCNFSM6AAAAAAVQOJ5FE>; > .; > You are receiving this because you authored the thread.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2436#issuecomment-1465318104:502,Message,Message,502,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2436#issuecomment-1465318104,1,['Message'],['Message']
Integrability,"Hi Malte and Isaac, many thanks for this! Ah, yes that other issue was; opened after I opened this one. I did search for the error message before I; opened the ticket, but I didn't search again while the ticket was open. The easiest workaround for me is simply to not use .raw anymore, for a; pipeline, it's not really needed anyways. Yes, I can see why it's important for file backed data, I just cannot see a; use case for file backed mode either. Any useful operations on file backed; data will be too slow anyways for practical use, and anyone can get a; high-RAM machine these days on Amazon for a few hours, so I've always; wondered file backed mode exists. (sidenote: File backed data is again a; feature that sounds rather complicated to implement. As a user I love; libraries that are small, stable and don't change a lot, especially for; very foundational things like anndata. I guess it's a matter of development; philosophy here). Also, yes, it's because I don't use scanpy interactively; that I don't see the use case for views. anyhow, thanks again, also for all your work on Scanpy!. On Wed, Jul 31, 2019 at 6:27 AM Isaac Virshup <notifications@github.com>; wrote:. > I've just spent a while trying to replicate, before realizing I've seen; > this issue before over on AnnData (theislab/anndata#182; > <https://github.com/theislab/anndata/issues/182>). I've got some good and; > bad news about this. It's fixed on master, but that fix is slated to be; > release in v0.7, which has intentionally breaking changes.; >; > I find views very useful when dealing with large datasets interactively.; > They're also important for file backed data, since copies are extremely; > expensive in that case.; >; > Unlike numpy, AnnData objects should always return a view when subset. If; > you'd like to get copies, you could add a .copy() to the end of your; > subsetting statement.; >; > —; > You are receiving this because you modified the open/close state.; > Reply to this email directly, view ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/728#issuecomment-516740578:131,message,message,131,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728#issuecomment-516740578,2,['message'],['message']
Integrability,"Hi Pavlin, Happy New Year!. Great to see this going forward!. > We'd probably have to do something similar to the gauss option and just overwrite the UMAP weights after the fact. Does this sound reasonable?. Yes, I also thought we'd need to do it similar to the `gauss` option. Can one use openTSNE code for computing perplexity-based weights or would one need to copy the binary search in here? I think it may be possible to use openTSNE's function to compute the affinities and then get the weights out of there?. One question here though: how would `tsne` function know if it should use the uniform kernel or the weights constructed by the `neighbors` function? Can some flag be switched if `neighbors` is run with `mode='tsne'` so that the `tsne()` function later on uses those weights? Not sure what's the best interface here. Alternatively the `tsne()` function could check if the weights conform to what t-SNE expects (sum to 1). Maybe that's better actually. Apart from that, I noticed that you implemented ; ```; class UniformAffinities(openTSNE.affinity.Affinities):; ``` ; in here, but isn't it part of `openTSNE` already?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-753488211:816,interface,interface,816,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-753488211,1,['interface'],['interface']
Integrability,"Hi Scanpy team!. After facing the issue with duplicated gene symbols again for the n-th time, I realised that one of the best solutions for renaming duplicates would likely be to do the following `'DuplicatedName-ENSEMBL_ID'` rather than just adding an order-dependent number `'DuplicatedName-1'` that can differ between dataset from different papers - preventing correct matching when integrating datasets which in turn essentially requires deleting duplicated genes. . What do you think in general?. Would it be possible to add support for this with interface like `var_names_make_unique(unique_column='ENSEMBL')`?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1719:259,depend,dependent,259,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1719,3,"['depend', 'integrat', 'interface']","['dependent', 'integrating', 'interface']"
Integrability,"Hi Scott!. Sorry that I didn't see this before writing the following email this moring:. > Sorry about forgetting to mention this in the first email below (just thought I'd send you the link to the most recent wrapper). The tSNE tool is a wrapper, too, and also has an example for a visualization function; > https://github.com/theislab/scanpy/blob/master/scanpy/tools/tsne.py; > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/tools/__init__.py#L426-L518; > You'll find the corresponding pair in the docs. If your wrapper parallelizes this, this will make it very easy for Scanpy users to use out PHATE. Scanpy's toplevel API does - on purpose - not feature objects, but convenience functions. If people want to do more complicated stuff, they should directly use the PHATE package, in this case. So, it would be nice if you'd only provide the convenience function - which is already very similar to what I pasted about tSNE. Also, please remove phate from the requirements... and raise a corresponding ImportError in your convenience function. Scanpy has quite some optional dependencies and Phate will be one of them. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/136#issuecomment-385959811:210,wrap,wrapper,210,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/136#issuecomment-385959811,4,"['depend', 'wrap']","['dependencies', 'wrapper']"
Integrability,"Hi Sergei,. Thank you very much for your fast reply. Do you mean I can still use the latest version of scanpy but installing a lower version of umap?. I tried different versions of scanpy, including pastiest, stable, 1.4.5, 1.4.5post3, 1.4.4post1... They seem to either have different error messages or packages not compatible. Do you know which version of the scanpy has it fixed?. Thank you for your kind help. Best regards,. Lirong. 获取 Outlook for iOS<https://aka.ms/o0ukef>; ________________________________; 发件人: Sergei R. <notifications@github.com>; 发送时间: Wednesday, April 22, 2020 12:44:36 PM; 收件人: theislab/scanpy <scanpy@noreply.github.com>; 抄送: plrlhb12 <lrpeng@hotmail.com>; Mention <mention@noreply.github.com>; 主题: Re: [theislab/scanpy] Issue with ingest (#1181). Hi, @plrlhb12<https://github.com/plrlhb12> .; Yes, this is a known problem. The easiest fix for now is to use umap 0.3.9 instead of 0.4.1.; You can also install scanpy from github where it is fixed or just wait for a new scanpy release. ―; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub<https://github.com/theislab/scanpy/issues/1181#issuecomment-617895856>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AKHCBZKH4TYT5672QNGFW33RN4NHJANCNFSM4MNX44PA>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1181#issuecomment-617911861:291,message,messages,291,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1181#issuecomment-617911861,1,['message'],['messages']
Integrability,"Hi Sidney,. Thanks for the pull request. igraph and louvain are kind of heavy dependencies (e.g. takes long time to compile them and they're not easily available via PyPI for all platforms etc.), this is why they are excluded from requirements file. It's written in the [installation document](https://scanpy.readthedocs.io/en/latest/installation.html) that these need to be installed manually. Also, there should be proper error messages stating that these must be installed separately when their functionality is needed for a function in scanpy and cannot be found. Did you get any other error regarding these packages?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/176#issuecomment-397543101:78,depend,dependencies,78,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/176#issuecomment-397543101,2,"['depend', 'message']","['dependencies', 'messages']"
Integrability,"Hi all! I wanted to make you aware of a caching extension for scanpy and scvelo that @michalk8 and myself have developed called [scachepy](https://github.com/theislab/scachepy) and to kick off a discussion about caching in scanpy. From my point of view, there are currently two main ways to cache your results in scanpy, please correct me if I'm wrong:; - write the AnnData object; - manually write the attributes, e.g. adata.X to file, e.g. pickle. The idea of scachepy is to offer the possibility to cache all fields of an AnnData object associated with a certain function call, e.g. `sc.pp.pca`. It allows you to globally define a caching directory and a backend (default is pickle) that the cached objects will be written to. In the case of PCA, this would amount to calling. ```python; import scachepy; c = scachepy.Cache(<directory>) ; c.pp.pca(adata); ```; where `c.pp.pca` wraps around `sc.pp.pca` but takes additional caching arguments like `force`. So in short, our aim with scachepy is to....; - ...have a flexible and easy to use way to cache variables associated with scanpy/scvelo function calls.; - ... speed up individual steps in a scanpy/scvelo analysis by caching them, without having to save the entire AnnData object; - ... be able to share jupyter notebooks with someone else who can run them on a different machine, possibly on a different OS and yet get the exactly the same results because the critical computations are cached. @michalk8 is the main developer and will be able to tell you much more about it. I would appreciate any input, and would love to discuss caching in scanpy/scvelo.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/947:881,wrap,wraps,881,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/947,1,['wrap'],['wraps']
Integrability,"Hi all, thanks for the mention @andrea-tango! If have been making multiple changes in diffxpy and batchglm recently, the following refers to the branch diffxpy dev, I haven' merged all of this into master yet as I am waiting for some last issues to be fixed. . 1. Are diffxpy's tests correct: We are using unit tests to check a) that all tests fullfill the null model. b) for standard tests that do not require model fits (Welch's t-test and rank sum test) we check that we produce the same results as scipy. We vectorise where possible but we do actually directly call scipy in the rank sum test right now. Both of these tests check out on the dev branch for the rank sum test, so this is working correctly. @falexwolf this might be a unit test that you could also introduce? I did not see this in the ones that you linked but I just glanced over. @andrea-tango please use dev right now. 2. Nature of the rank sum test. We previously called the Wilcoxon rank sum test de.test.wilcoxon, note that this is also alternatively named Mann-Whitney U test (MWU). Importantly, MWU is for independent samples, which we always have in scRNAseq, the ""wilcoxon"" test in scipy is for paired samples. We therefore renamed this to de.test.rank_sum to avoid confusion. Which one are you using @falexwolf? . 3. Comparison scanpy vs diffxpy (in unit tests). I would discourage this and compare against scipy because I would consider scipy the gold standard for statistical testing. I can run comparisons if the above comments do not resolve all issues discussed here, which would imply that we might differ in our wrapping in the tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/460#issuecomment-471487617:1597,wrap,wrapping,1597,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460#issuecomment-471487617,1,['wrap'],['wrapping']
Integrability,"Hi all,. I am trying to use ScanPy for integrating multiple scRNA-Seq samples (~20). Doing so that I can look at RNA Velocity with SCVelo, and want to use MNN as I got good batch effect removal previously in monocle using MNN. Is it true - as stated above, that the current implementation of mnncorrect with ScanPy is only operating on expression values? I have run through a ScanPy MNN [tutorial ](https://nbisweden.github.io/workshop-scRNAseq/labs/compiled/scanpy/scanpy_03_integration.html) provided by NBI Sweden. The results are improved, but it doesn't appear to work as well as in monocle - some separation by batch is still going on. . I'm wondering what the difference might be? Whether it could be due to the difference in PCA (multi-batch), or the actual MNN / batch effect removal step. Alternatively, I could use the corrected expression matrix, and add the UMAP coordinates/clusters from monocle, although I wonder if this is advisable. . If you have any info please let me know, or if I should raise a separate issue etc.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1289#issuecomment-734319967:39,integrat,integrating,39,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1289#issuecomment-734319967,1,['integrat'],['integrating']
Integrability,"Hi all,. I am trying to use `ingest` to integrate different datasets.; I found a couple of issues. - `ingest` requires that the `var_names` are the same in the reference and the new object. I can select the intersection between the datasets; however, it requires that the genes are in the same order `if not ref_var_names.equals(new_var_names)`. I think this `if` could be modified using `set` (e.g., `len(set(ref_var_names).difference(set(new_var_names))) == 0`). I tried to order the `.var` dataframe, but the `.X` remains the same. In such a way, the expression of the genes does not correspond to the correct one. I can generate a dataframe and recreate the `.X`, but it could be very nice that the `.X` will be modified according to `.var` or `.obs` modifications (i.e., ordering). . - although it is possible to set `embedding_method=umap`, `ingest` requires the PCA components. I used autoencoders instead of PCA, and I cannot run `ingest` only considering the UMAP. Can you fix it? . Thank you in advance.; Best,; Andrea",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1128:40,integrat,integrate,40,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1128,1,['integrat'],['integrate']
Integrability,"Hi all,. Sorry I sent a PR(https://github.com/theislab/scanpy/pull/1271) without reading any of these, it's my bad. Some thoughts are as follows:. - I think it's fairly straightforward to check for R dependencies in runtime, please see the PR for more info. - For Travis, I used Ubuntu packages for base R installation and then rest of the R deps are installed by the Travis user in home directory, which is cached. apt-install R installation takes around a minute. This is really hard to reduce, I think. . - After the caching, the installation of sctransform itself take around 15-20sec. This can even be reduced to zero if I check whether it's already installed. See https://travis-ci.org/github/theislab/scanpy/jobs/697070834 for a better breakdown. You can compare this with an existing test run e.g. https://travis-ci.org/github/theislab/scanpy/jobs/696758553. - sctransform test overhead is around 30sec, which can also be reduced. Overall, it adds 4 minutes to the travis test time. I don't know exactly where the remaining difference comes from. - However, if we keep adding more Ubuntu and/or R packages in the scanpy travis, it can get a bit bloated. Even if things are cached, for some reason, there is a 45-50 second cache upload overhead which is not negligible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1068#issuecomment-642835553:200,depend,dependencies,200,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-642835553,1,['depend'],['dependencies']
Integrability,"Hi everyone,. Seeing how many new single cell and spatial tools are being developed in Python, and how we are increasingly using it in general and scanpy in particular, at saezlab we decided to re-implement our tools to estimate pathways and Transcription factor (TF) activity ([Dorothea](https://saezlab.github.io/dorothea/) and [Progeny](https://saezlab.github.io/progeny/)) in it. Here's a first draft in Python of our tools:; https://github.com/saezlab/dorothea-py; https://github.com/saezlab/progeny-py. Our tools take gene expression as input and generate matrices of TF and pathway activities. They can be understood as: ; 1) Prior-knowledge dimensionality reduction methods (`obsm`). Examples of usage:; 	* Used as input for NN; 	* Used as input for integration methods; 2) New data assays (`X`). Examples of usage:; 	* Plot feature activities in projections such as PCA or UMAP; 	* Plot feature activities in heat-maps, clustermaps, violin plots, etc; 	* Differences between groups can be modeled to find significant differences. Because of this duality, the integration of our tools into scanpy is not straightforward. If we store the activities in `obsm` they can be used as a dimensonality reduction embedding but then we lose acces to all the fantastic plotting functions based on `X`. Then if we add add our activities to `X`, they have a very different distribution than gene expression plus there would be an overlap of names between genes and TFs. A solution to this would be to have a separate `.layer` to store this matrices but layers must contain the same dimensions as `X`. Another workaround would be to store it in `.raw` but then we force the user to use remove its previous contents, plus it is used in some methods as default which could cause problems. . What would be a smart solution to integrate our tools in your universe?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1724:758,integrat,integration,758,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1724,3,['integrat'],"['integrate', 'integration']"
Integrability,"Hi everyone,. a while back, @giovp asked me to create a pull request that integrates analytical Pearson residuals in scanpy. We already discussed a bit with @LuckyMD and @ivirshup over at berenslab/umi-normalization#1 how to structure it, and now I made a version that should be ready for review. As discussed earlier, this pull request implements two core methods:; - `sc.pp.normalize_pearson_residuals()`, which applies the method to `adata.X`. Overall, the function is very similar in structure to `sc.pp.normalize_total()` (support for layers, inplace operation etc).; - `sc.pp.highly_variable_genes(flavor='pearson_residuals')`, which selects genes based on Pearson residual variance. The ""inner"" function `_highly_variable_pearson_residuals()` is structured similarly to `_highly_variable_seurat_v3()` (support for multiple batches, median ranks for tie breaking). It includes the `chunksize` argument to allow for memory-efficient computation of the residual variance. We discussed quite a lot how to implement a third function that would bundle gene selection, normalization by analytical residuals and PCA. This PR includes the two options that emerged at the end of that discussion, so now we have to choose ;). - `sc.pp.recipe_pearson_residuals()` which does HVG selection and normalization both via Pearson residuals prior to PCA; - `sc.pp.normalize_pearson_residuals_pca()` which applies any HVG selection if the user previously added one to the `adata` object, and then normalizes via Pearson residuals and does PCA. Both functions retain the raw input counts as `adata.X` and add fields for PCA/Normalization/HVG selection results (or return them) as applicable, most importantly the `X_pca` in `adata.obsm['pearson_residuals_X_pca']`. I hope this addresses some of the issues we discussed over at the other repo in a scanpy-y way. Let me know what you think and where you think improvements are needed!. Cheers, Jan.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715:74,integrat,integrates,74,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715,1,['integrat'],['integrates']
Integrability,"Hi everyone,; I am using pip3 to install scanpy, this is the error message:. <details>. ```pytb; File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/site-packages/setuptools-18.3.2-py3.6.egg/setuptools/command/build_py.py"", line 94, in find_data_files; TypeError: must be str, not list. ----------------------------------------; Failed building wheel for scanpy; Running setup.py clean for scanpy; Failed to build scanpy; Installing collected packages: scanpy, decorator; Running setup.py install for scanpy ... error; Complete output from command /global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/bin/python -u -c ""import setuptools, tokenize;__file__='/tmp/xs-ttgump/pip-install-xpuhp0co/scanpy/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record /tmp/xs-ttgump/pip-record-v4z7bmhr/install-record.txt --single-version-externally-managed --compile --user --prefix=:; running install; running build; running build_py; creating build; creating build/lib; creating build/lib/scanpy; copying scanpy/logging.py -> build/lib/scanpy; copying scanpy/exporting.py -> build/lib/scanpy; copying scanpy/_version.py -> build/lib/scanpy; copying scanpy/utils.py -> build/lib/scanpy; copying scanpy/__init__.py -> build/lib/scanpy; copying scanpy/settings.py -> build/lib/scanpy; copying scanpy/readwrite.py -> build/lib/scanpy; creating build/lib/scanpy/tools; copying scanpy/tools/dpt.py -> build/lib/scanpy/tools; copying scanpy/tools/paga.py -> build/lib/scanpy/tools; copying scanpy/tools/louvain.py -> build/lib/scanpy/tools; copying scanpy/tools/_utils.py -> build/lib/scanpy/tools; copying scanpy/tools/pca.py -> build/lib/scanpy/tools; copying scanpy/tools/umap.py -> build/lib/scanpy/tools; copying scanpy/tools/sim.py -> build/lib/scanpy/tools; copying scanpy/tools/tsne.py -> build/lib/scanpy/tools; copying scanpy/tools/__init__.py -> build/lib/scanpy/tools; copyi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/148:67,message,message,67,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/148,1,['message'],['message']
Integrability,"Hi scanpy folks,. This PR adds our cell identity classification model [`scnym`](https://github.com/calico/scnym) as a tool in `sc.external.tl.scnym`. The `scnym` API was inspired by `scanpy` and intended to be compatible, so the implementation in `external/_scnym.py` is simply a wrapper.; I also added a test in `tests/external/test_scnym.py` that passes.; Everything was linted with `black,flake8,autopep8` through `pre-commit`. Please let me know if there are any issues or changes you'd like to see.; Thanks for building a great ecosystem!. Best,; Jacob",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1775:280,wrap,wrapper,280,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1775,1,['wrap'],['wrapper']
Integrability,"Hi there! Thanks for adding the ingest method to scanpy!; I was wondering what would be the suggested approach when the reference data contains batch effects that should be removed before actually performing the asymmetric integration with a query dataset. I tried to use the neighbors structure returned by BBKNN (which correctly adjusts for batch effects), but the 'metric' object is missing. Here the error:. ```; KeyError Traceback (most recent call last); <ipython-input-22-a805d117788e> in <module>; ----> 1 sc.tl.ingest(adata, adata_ref, obs='time', embedding_method='umap'). /opt/conda/lib/python3.7/site-packages/scanpy/tools/_ingest.py in ingest(adata, adata_ref, obs, embedding_method, labeling_method, inplace, **kwargs); 115 labeling_method = labeling_method * len(obs); 116 ; --> 117 ing = Ingest(adata_ref); 118 ing.fit(adata); 119 . /opt/conda/lib/python3.7/site-packages/scanpy/tools/_ingest.py in __init__(self, adata); 268 ; 269 if 'neighbors' in adata.uns:; --> 270 self._init_neighbors(adata); 271 ; 272 if 'X_umap' in adata.obsm:. /opt/conda/lib/python3.7/site-packages/scanpy/tools/_ingest.py in _init_neighbors(self, adata); 229 else:; 230 dist_args = (); --> 231 dist_func = named_distances[adata.uns['neighbors']['params']['metric']]; 232 self._random_init, self._tree_init = make_initialisations(dist_func, dist_args); 233 self._search = make_initialized_nnd_search(dist_func, dist_args). KeyError: 'metric'. ```; I'm running scanpy version 1.4.5.post2. Any help would be highly appreciated!! Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1108:223,integrat,integration,223,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1108,1,['integrat'],['integration']
Integrability,"Hi there. Everytime I run the code _sc.pp.neighbors_ the kernel dies. Unfortunately, there is no error message or error code. It just dies while computing neighbors. Other scanpy codes like _sc.pp.filter_cells_ and _sc.pp.filter_genes_ work without a problem. I'm using:. - windows 10 64-bit 24 gb ram; - python 3.8.5 in jupyter notebook; - numpy 1.19.4; - scanpy 1.6.0. Is there someone who would be able to solve this issue?; Thank you very much!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1567:103,message,message,103,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567,1,['message'],['message']
Integrability,"Hi! Harmony does not adjust the counts matrix at all, or even read it. Rather, it takes principal components as input, adjusts them to correct for batch, and outputs a modified set of PCs. Here is the paper that explains how it works: https://www.nature.com/articles/s41592-019-0619-0. (I wasn't involved in coming up with this method or implementing it, just making it interoperable with scanpy, for the record)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2314#issuecomment-1240108377:370,interoperab,interoperable,370,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2314#issuecomment-1240108377,1,['interoperab'],['interoperable']
Integrability,"Hi! I am wondering why `loompy` and `pybiomart` are not in the list of dependencies. As of now ; the package (at least in bioconda) is not fully functional and requieres some extra installation; steps if one wants to use certain functionalities provided. - [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### Minimal code sample (that we can copy&paste without having any data); ```bash; conda install -c bioconda scanpy; ````. ```python; import scanpy as sc; annot = sc.queries.biomart_annotations(""mmusculus"", [""ensembl_gene_id"", ""external_gene_name""]); ```; ```pytb; line 108, in biomart_annotations; return simple_query(org=org, attrs=attrs, host=host, use_cache=use_cache); File ""/Users/jfnavarro/opt/anaconda3/envs/nf-core/lib/python3.7/site-packages/scanpy/queries/_queries.py"", line 64, in simple_query; ""This method requires the `pybiomart` module to be installed.""; ImportError: This method requires the `pybiomart` module to be installed.; ```. ```python; import scanpy as sc; adata = sc.datasets.pbmc3k(); adata.write_loom('dummy.loom'); ```; ```pytb; write_loom(filename, self, write_obsm_varm=write_obsm_varm); File ""/Users/jfnavarro/opt/anaconda3/envs/nf-core/lib/python3.7/site-packages/anndata/_io/write.py"", line 112, in write_loom; from loompy import create; ModuleNotFoundError: No module named 'loompy'; ```. #### Versions. <details>. WARNING: If you miss a compact list, please try `print_header`!; -----; anndata 0.7.6; scanpy 1.7.2; sinfo 0.3.1; -----; PIL 8.3.1; anndata 0.7.6; beta_ufunc NA; binom_ufunc NA; bottleneck 1.3.2; cffi 1.14.6; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; dunamai 1.6.0; get_version 3.5; h5py 2.10.0; joblib 1.0.1; kiwisolver 1.3.1; legacy_api_wrap 0.0.0; llvmlite 0.36.0; matplotlib 3.4.2; mkl 2.4.0; mpl_toolkits NA; natsort 7.1.1; nbinom_ufunc NA; n",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2000:71,depend,dependencies,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2000,1,['depend'],['dependencies']
Integrability,"Hi! I think we have a different focus here, and not all of what you stated as fact is correct, so I’ll do my best to clear this up:. 1. There is an advantage for type hints in common Scanpy usage. IPython should use Jedi to create autocompletions since this summer, but they forgot to reenable it. I sent them an issue to do so, ipython/ipython#11503 and a fix in ipython/ipython#11506. Jedi supports type hints, so with `c.Completer.use_jedi = True` now or by default in a month, people will profit from them. Furthermore, people are using scanpy in applications and scripts, not just in notebooks. When you use an IDE (or install the jedi extension in EMACS) you should profit from it. 2. The Jupyter shift-tab help being hard to read in the presence of type hints is what I consider a bug. I reported it in ipython/ipython#11504 and fixed it in ipython/ipython#11505. 3. The numpy is on it (see [here](https://github.com/numpy/numpy-stubs)) and will probably integrate it once there needs to be no Python 2 compat. e.g. scikit-learn waits for numpy: scikit-learn/scikit-learn#11170. I see your concern about entry hurdles, but I don’t agree. It’s super easy. `Union` is “or”, `Optional` is “or `None`”. If there’s questions, they can be answered. (or people click on the links in the docs and read like one sentence of explanation). 4. If you want we can change how all that is rendered. `Union[a, b]` could be done as ``` :class:`a` or :class:`b` ``` But it’s really not hard…. Honestly I think the `Callable[…]` is much better than the textual description that was there before: Until it was there, people (including me when i was writing that annotation) had to dive into the code to figure out what function signature is *really* expected there. Now they have to be able to parse what that `Callable[[a,b], c]` there means. If they have never encountered it before, they can click on it, read one sentence of explanation and know that `a` and `b` are parameters and `c` the return type. Done in",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-440619581:962,integrat,integrate,962,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-440619581,2,['integrat'],['integrate']
Integrability,"Hi! I've added two possible APIs for PHATE: I imagine you'll only want to include one of them. . The first, `sc.tl.PHATE`, is an object-oriented interface that matches `phate.PHATE`, which looks like an `sklearn` estimator class. The second is a functional interface which is equivalent to running `phate.PHATE().fit_transform(data)`. The second is more simple, but less powerful as any changes in parameters will require the entire operator to be recomputed, where the object-oriented interface allows partial recomputation. Please let me know which you prefer (or if you would like to include both!) If we only include the functional version I would probably rename it to `phate` rather than `run_phate`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/136:145,interface,interface,145,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/136,3,['interface'],['interface']
Integrability,"Hi! I've tried first the 4th point from @atarashansky and yeah, multiplying the distance, connectivities, or both with a normal distribution with std 0.0001 (which makes roughly a 0.008% of absolute difference in the matrix) and the results are different. . Python code:; ```python; c0 = adata.uns['neighbors']['connectivities']; connect = adata.uns['neighbors']['connectivities'].todense(); connect = np.multiply(connect, np.random.normal(1, 0.0001, connect.shape)); adata.uns['neighbors']['connectivities'] = spr.csr_matrix(connect); print(100*np.sum(np.abs(c0 - connect))/min(np.sum(c0), np.sum(connect))); ```. ```python; c0 = adata.uns['neighbors']['distances']; csum = adata.uns['neighbors']['distances'].todense().sum(); connect = adata.uns['neighbors']['distances'].todense(); connect = np.multiply(connect, np.random.normal(1, 0.0001, connect.shape)); adata.uns['neighbors']['distances'] = spr.csr_matrix(connect); print(100*np.sum(np.abs(c0 - connect))/min(np.sum(c0), np.sum(connect))); ```. Before; ![image](https://user-images.githubusercontent.com/35657291/73247960-52f51900-41b2-11ea-937c-c09e301c5e7a.png). After; ![image](https://user-images.githubusercontent.com/35657291/73247968-55f00980-41b2-11ea-9d44-8e4cef0149d5.png). I mean, the results (hopefully) are not drastically different, but there are minor rearrangements and clustering assignments that might make the analysis be rearranged depending on the computer. . I've realized that, for some reason, the differences between points are not associated to different random seeds, that is, after setting the seed, the PCA look equal but are different in less than 0.001 when doing a pairwise comparison. This happens as well to the neighbors graph, so I believe that randomness is not really an issue. However I am puzzled as to why the differences in PCA / neighbour graphs is so small. Shouldn't two different computers do calculations equally at least at 3 or 4 decimals?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1009#issuecomment-579254312:1410,depend,depending,1410,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1009#issuecomment-579254312,1,['depend'],['depending']
Integrability,Hi! I’ve wanted to introduce https://github.com/flying-sheep/legacy-api-wrap for some time. do you think that would be sufficient or should we take the deprecation of kwargs into account?. There’s also tantale/deprecated#8,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/474#issuecomment-470910941:72,wrap,wrap,72,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474#issuecomment-470910941,1,['wrap'],['wrap']
Integrability,"Hi! We didn't look at the method yet, but superficially:. - uppercase letters are only allowed in CONSTANTS and ClassNames, not module names, parameter names or function names; - preprocessing goes into sce.pp. This way there's way less keyword arguments and no need to prefix them. If the preprocessed data isn't useful for other applications, you can put it into .layers; - Scanpy functions should either mutate or return something, depending on the value of the `inplace` parameter",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/903#issuecomment-555603337:435,depend,depending,435,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/903#issuecomment-555603337,1,['depend'],['depending']
Integrability,"Hi!. > > > for normalize_pearson_residual, i think it makes sense to keep normalize in, as it's not the same type of transformation compared to log1p.; > ; > > Isn't this quite similar to what log1p does though? In that it's a transformation of the matrix?; > ; > I think it should stay `normalize_pearson_residuals` because it mirrors `normalize_total`. I agree. > ; > for the rest, I think we are at a good stage, I'd ask @jlause to build docs locally `cd scanpy/docs` and then `make clean` and `make html` see https://scanpy.readthedocs.io/en/stable/dev/documentation.html#building-the-docs and check that:; > ; > * arguments and doc params match; > ; > * typo and other minor issues still present (e.g. difficult phrasing). I started doing that and will finish up tomorrow - there Qs in advance if you happen to look at this before:. - sometimes we have math expressions like var = mean * mean^2 etc. in the docs. Is there a convention for scanpy docs if those should be in `code` format or just plain text? e.g. in the adata docstring the matrix shape is described as `n_obs` x `n_var`, but elsewhere we say ""clipping is done by sqrt(n). I can consistently format them into `code` if you agree.; - I think the `.._pca` function is missing from the release note. should I add it there?; - The `..pca` function also did not use shared docs params yet. I started adding them and can commit tomorrow - is that okay if I just do it like that?. > ; > ; > if this gets approval, before merging to master todo:; > ; > * [x] add release note; > ; > * [ ] go over scanpy_tutorials and re run tutorial and merge. I've looked at that and commented in the respective github :). Very happy we are getting this wrapped up now :); Best! Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-1065345395:1701,wrap,wrapped,1701,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1065345395,1,['wrap'],['wrapped']
Integrability,Hi!. I am wondering if you could add https://github.com/BayraktarLab/cell2location to your list of scRNA->spatial integration methods (https://scanpy-tutorials.readthedocs.io/en/latest/spatial/integration-scanorama.html). Thanks!,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1574:114,integrat,integration,114,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1574,2,['integrat'],"['integration', 'integration-scanorama']"
Integrability,Hi!. If you used a neural network approach you could use scArches to leverage transfer learning to map things across without re-integrating (only minimal additional training done there). You could also map into the embedded space using `sc.tl.ingest` for example. But there is always the danger that there is a residual batch effect that cannot be removed without de-novo integration.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2162#issuecomment-1055535384:128,integrat,integrating,128,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2162#issuecomment-1055535384,4,['integrat'],"['integrating', 'integration']"
Integrability,"Hi!. There’s a series of abstract base classes (other languages call them interfaces) that can be used if you know specifically what you want (e.g. `typing.Sequence[T]` if it only needs to be iterable and indexable, but not necessarily a `list`). About your examples, generally I always have to dig into the code to figure such things out. Annoying, but it means that people after me can just use the type annotations instead of wasting their time doing the same. - Currently we only accept `str` for obs_names, so depending on the actual usage in the function, `Iterable[str]` or `Sequence[str]` would be it I guess.; - Usually single integers being passed around should be python `int`s, because they can be any size. But if you need to accept `int32` and `int64`, you can do `Union[int, np.integer]`. > Is there a way to say: ""should behave right if I call np.array on it"". In truth, `array` eats just about anything (with weird results), and for us this is probably a good idea:. ```py; Number = Union[float, int, np.integer, np.floating]; Num1DArrayLike = Sequence[Number]; Num2DArrayLike = Sequence[Num1DArrayLike]; Num3DArrayLike = Sequence[Num2DArrayLike]; NumNDArrayLike = Union[Num1DArrayLike, Num2DArrayLike, Num3DArrayLike]; ```. But if we want to be exact about `array_like`s, we’d need this ABC:. ```py; class ArrayLike(ABC):; """"""An array,; any object exposing the array interface,; an object whose __array__ method returns an array,; or any (nested) sequence.; """"""; @classmethod; def __subclasshook__(cls, C):; if issubclass(C, np.ndarray):; return True; if any('__array_interface__' in B.__dict__ for B in C.__mro__):; return True; if any('__array__' in B.__dict__ for B in C.__mro__):; return True; return Sequence.__subclasshook__(cls, C); ```. ----. Two thoughts here:. 1. It’s fine if you don’t know exactly. Just use your best guess. The worst case is that someone wastes a second of runtime converting their argument to the type you thought was needed, while the original type wo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-441207438:74,interface,interfaces,74,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441207438,2,"['depend', 'interface']","['depending', 'interfaces']"
Integrability,"Hi!; As someone else posted on [stackoverflow](https://stackoverflow.com/questions/54366505/importerror-dll-load-failed-while-file-is-in-working-directory/54441575#54441575), there seem to be problems with the tables dependencies for windows users resulting in the following error when importing scanpy:. ```pytb; >>> import scanpy; ...; File ""C:\Miniconda3\envs\py36\lib\site-packages\scanpy\readwrite.py"", line 9, in; import tables; File ""C:\Miniconda3\envs\py36\lib\site-packages\tables__init__.py"", line 131, in; from .file import File, open_file, copy_file; File ""C:\Miniconda3\envs\py36\lib\site-packages\tables\file.py"", line 35, in; from . import hdf5extension; ImportError: DLL load failed: The specified procedure could not be found.; ```. I've also posted an answer suggestion there. Maybe you could require h5py to have a fixed older version like 2.8 to avoid this problem for other windows users. Downgrading to that version worked for me.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/454:217,depend,dependencies,217,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454,1,['depend'],['dependencies']
Integrability,"Hi!; Sorry for that, the command-line interface got a bit behind. I fixed everything, simply pull again.; Cheers,; Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/30#issuecomment-322022113:38,interface,interface,38,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/30#issuecomment-322022113,2,['interface'],['interface']
Integrability,"Hi!; Thank you for tutorials, they're very helpful. ; Do you have a spatial/sc-rna seq integrative analysis tutorial? . Thanks in advance!. ...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1386:87,integrat,integrative,87,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1386,1,['integrat'],['integrative']
Integrability,"Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using ‘cellranger’ software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In ‘Seurat’, I can read the data from my three treatments separated, do quality control, and then integrate them using ‘FindIntegrationAnchors’ and ‘IntegrateData’ functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in ‘Scanpy’ to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that?. In ‘Scanpy’ I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type?. Thank you very much!. Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/859:576,integrat,integrate,576,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859,7,"['Integrat', 'integrat']","['IntegrateData', 'integrate', 'integrated', 'integrating']"
Integrability,"Hi, . I was wondering what is a good/accepted way to calculate differential gene expression after batch alignment of multiple datasets?. After reading into it, it seems to me that the DEG are calculated on the raw (=non batch corrected values) and after all some batch correction algorithms don't even transform the data matrix (I don't understand why). See: [Mnn correct docs](https://icb-scanpy.readthedocs-hosted.com/en/stable/api/scanpy.external.pp.mnn_correct.html), [Seurat issue](https://github.com/satijalab/seurat/issues/1224#issuecomment-473416336), [Harmony preprint](https://www.biorxiv.org/content/biorxiv/early/2018/11/05/461954.full.pdf). But that means I would need to include _batch_ as an interaction in the DEG calculation, therefore I could use _logistic regression_ in scanpy with:; `scanpy.tl.rank_genes_groups(adata, use_raw=True, method='logreg')`; I am struggling though to find out how to add interactions to sklearns logistic regression via scanpy. When using sklearn directly it should work through [patsy or PolynomialFeatures()](https://stackoverflow.com/questions/45828964/how-to-add-interaction-term-in-python-sklearn). [Others](https://github.com/theislab/scanpy/issues/95) seem to use sklearn without the wrapper.; Or maybe I don't need to add interactions if the biological difference between the samples is bigger than the batch effect?. Do you think this is the right way to do this and could you point me in the right direction to solve this?; I think this might actually be not an _issue_ of scanpy but more a matter of understanding how to properly do this and how to use the tool so no worries if you decide to close this. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/669:1239,wrap,wrapper,1239,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/669,1,['wrap'],['wrapper']
Integrability,"Hi, . This is a minor point. Thanks for linking to the scVI repo in your ecosystem page. However, would it be possible to put scVI in another category than ""data integration"" ? Since scVI can also do differential expression for example. . Thanks, ; Romain; <!--; ⚠ If you need help using Scanpy, please ask in https://scanpy.discourse.group/ instead ⚠; If you want to know about design decisions and the like, please ask below:; -->; ...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1072:162,integrat,integration,162,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1072,1,['integrat'],['integration']
Integrability,"Hi, . we developed [scirpy](https://github.com/icbi-lab/scirpy), a scanpy extension to analyse single-cell TCR data. You can learn more about the project in our [preprint](https://www.biorxiv.org/content/10.1101/2020.04.10.035865v1), the [apidocs](https://icbi-lab.github.io/scirpy/api.html) and the [tutorial](https://icbi-lab.github.io/scirpy/tutorials/tutorial_3k_tcr.html). . Since I heard that some people around here (@davidsebfischer, @b-schubert) are really interested in this topic, I created this issue to coordinate efforts. I would love to work together with you guys to take this to the next level. . Currently, some ideas of mine are; * extension to BCR data; * integration with @davidsebfischer's [tcellmatch](https://github.com/theislab/tcellmatch) as a distance metric for clonotype networks; * integration with epitope databases. Let me know what you think! . Best, ; Gregor . CC @ffinotello, @szabogtamas, @mlist",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1163:676,integrat,integration,676,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163,2,['integrat'],['integration']
Integrability,"Hi, ; I was wondering, if you can synchronize the functionality of the louvain and leiden clustering algorithm implementations. ; `sc.tl.louvain` has the `restrict_to` parameter, which allows subclustering of a specific cluster (set), while `sc.tl.leiden` does not (Note: I have `scanpy==1.4+18.gaabe446`). ; I'd be happy to have that. . Best,; M",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/582:34,synchroniz,synchronize,34,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/582,1,['synchroniz'],['synchronize']
Integrability,"Hi, ; I'm having similar issues in using sc.tl.ingest(adata, adata_ref, obs='louvain').; I have updated my Scanpy 1.4.6 and anndata to 0.7.1.; I'm getting the following error message.; ```pytb; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-71-27e22cc8f823> in <module>; ----> 1 sc.tl.ingest(adata, adata_ref, obs='louvain'). ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in ingest(adata, adata_ref, obs, embedding_method, labeling_method, inplace, **kwargs); 119 ; 120 for method in embedding_method:; --> 121 ing.map_embedding(method); 122 ; 123 if obs is not None:. ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in map_embedding(self, method); 407 """"""; 408 if method == 'umap':; --> 409 self._obsm['X_umap'] = self._umap_transform(); 410 elif method == 'pca':; 411 self._obsm['X_pca'] = self._pca(). ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in _umap_transform(self); 396 ; 397 def _umap_transform(self):; --> 398 return self._umap.transform(self._obsm['rep']); 399 ; 400 def map_embedding(self, method):. ~/opt/anaconda3/lib/python3.7/site-packages/umap/umap_.py in transform(self, X); 2006 try:; 2007 # sklearn pairwise_distances fails for callable metric on sparse data; -> 2008 _m = self.metric if self._sparse_data else self._input_distance_func; 2009 dmat = pairwise_distances(; 2010 X, self._raw_data, metric=_m, **self._metric_kwds. AttributeError: 'UMAP' object has no attribute '_input_distance_func'; ```. Appreciate your comments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1092#issuecomment-623064541:175,message,message,175,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1092#issuecomment-623064541,1,['message'],['message']
Integrability,"Hi, ; Thank you very much for such a detailed explanation. It really helps. I've two more questions: . 1). Can we do this gene subsetting with Logistic regression (where no multiple testing correction is involved)? . 2). Since you nicely pointed out sc.tl_rank_genes_groups doesn't tell about the contribution of genes in the clustering- are there tools that can be integrated with ScanPy to do this job? (for example, diffxpy or MAST). I'm really interested in the differential gene testing to predict the markers (from a gene subset used for clustering). . I shall be grateful if you can suggest a method.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/748#issuecomment-515114575:366,integrat,integrated,366,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748#issuecomment-515114575,1,['integrat'],['integrated']
Integrability,"Hi, ; we want to add SCALEX, an online integration method that integrate different single-cell experiments including scRNA-seq and scATAC-seq, which also enable accurate projecting new incoming datasets onto the existing cell space. This method is published on Nature Communications. SCALEX is developed based-on scanpy. We appreciate and will be honored to contribute to the scanpy community. Thank you!; Lei . <!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2353:39,integrat,integration,39,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2353,2,['integrat'],"['integrate', 'integration']"
Integrability,"Hi, I have a Seruat processed dataset, of which I wanted to use scVI for integration. I stored the raw count and cell information then assembled them in scanpy as anndata via method mentioned: `https://smorabit.github.io/tutorials/8_velocyto/`. . So I could get batch HVG function to work without specifying flavor, however, I couldn't get it to work with specifying `flavor=""seurat_v3""`, I wonder how important it is to set this parameter and why does it not work for me?. Thanks a lot!. ```; adata; AnnData object with n_obs × n_vars = 73998 × 13639; obs: 'orig.ident', 'nCount_RNA', 'nFeature_RNA', 'barcode', 'Celltype2', 'Clusters', 'Sample'; uns: 'log1p'; layers: 'counts'. for i in adatas:; i.layers['counts'] = i.X; adata = ad.concat(adatas); adata.obs_names_make_unique; sc.pp.log1p(adata); sc.pp.highly_variable_genes(; adata,; flavor=""seurat_v3"",; layer=""counts"",; batch_key=""Sample"",; subset=True; ); ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); Cell In [197], line 1; ----> 1 sc.pp.highly_variable_genes(; 2 adata_new,; 3 flavor=""seurat_v3"",; 4 layer=""counts"",; 5 batch_key=""Sample"",; 6 subset=True; 7 ); 8 adata_new. File ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_highly_variable_genes.py:422, in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key, check_values); 416 raise ValueError(; 417 '`pp.highly_variable_genes` expects an `AnnData` argument, '; 418 'pass `inplace=False` if you want to return a `pd.DataFrame`.'; 419 ); 421 if flavor == 'seurat_v3':; --> 422 return _highly_variable_genes_seurat_v3(; 423 adata,; 424 layer=layer,; 425 n_top_genes=n_top_genes,; 426 batch_key=batch_key,; 427 check_values=check_values,; 428 span=span,; 429 subset=subset,; 430 inplace=inplace,; 431 ); 433 if batch_key is None:; 434 df = _highly_variable_genes_single_batch(; 435 adata,; 436 layer=layer,; (...); 443",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2427:73,integrat,integration,73,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2427,1,['integrat'],['integration']
Integrability,"Hi, I was doing a dataset integration on quite some datasets. . ```py; adatas = [AT1,AT2,AT3,AT4,AT5,AT6,AT7] #each is an adata object from scRNA-seq. for i in adatas:; i.layers['counts'] = i.X. adatas = [AT1,AT2,AT3,AT4,AT5,AT6,AT7]. adata = ad.concat(adatas); adata.obs_names_make_unique. sc.pp.log1p(adata); sc.pp.highly_variable_genes(; adata,; layer=""logcounts"",; batch_key=""Sample"",; subset=True; ). scvi.model.SCVI.setup_anndata(adata, layer=""counts"", batch_key=""Sample""). vae = scvi.model.SCVI(adata, n_hidden=256); vae.train(); adata.obsm[""X_scVI""] = vae.get_latent_representation(); sc.pp.neighbors(adata, use_rep=""X_scVI""); from scvi.model.utils import mde; import pymde; adata.obsm[""X_mde""] = mde(adata.obsm[""X_scVI""]); adata.obsm[""X_normalized_scVI""] = vae.get_normalized_expression(); adata.write_h5ad('Integrated.h5ad'); ```. So I did use make obs names unique after ad.concat(adatas). However, after I was finished with the integration and moving on to write_h5ad, it returns the following errors and tells me they can't write my h5ad cuz I have duplicated rows:. ```pytb; Feb 26 11:20:39 PM: Your dataset appears to contain duplicated items (rows); when embedding, you should typically have unique items.; Feb 26 11:20:39 PM: The following items have duplicates [60449 60452 60455 ... 70783 70784 70785]; Traceback (most recent call last):; File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_io/utils.py"", line 214, in func_wrapper; return func(elem, key, val, *args, **kwargs); File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_io/specs/registry.py"", line 171, in write_elem; _REGISTRY.get_writer(dest_type, (t, elem.dtype.kind), modifiers)(; File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_io/specs/registry.py"", line 24, in wrapper; result = func(g, k, *args, **kwargs); File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_io/specs/methods.py"", line 346, in write_vlen_string_array; f.create_dataset(k, data=elem.astype(st",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2432:26,integrat,integration,26,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2432,3,"['Integrat', 'integrat']","['Integrated', 'integration']"
Integrability,"Hi, I wrapped a R function from scran (mnnCorrect) in the preprocessing module. It's a new algorithm for batch effect removal ([Batch effects in single-cell RNA-sequencing data are corrected by matching mutual nearest neighbors](https://www.nature.com/articles/nbt.4091)) and has not been implemented in Python, so I thought it would be useful. (Extremely useful for myself 😁 )",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/125:6,wrap,wrapped,6,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125,1,['wrap'],['wrapped']
Integrability,"Hi, I've now implemented many scanpy plots using Marsilea in the notebook. Please let me know what you think of it and if you have any further questions or requests that could be useful for the community. Looking forward to integrating this!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2444#issuecomment-2084906007:224,integrat,integrating,224,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444#issuecomment-2084906007,1,['integrat'],['integrating']
Integrability,"Hi, It's not available in scanpy at the moment, but I wrote a wrapper for it via `rpy2` and `anndata2ri` which is available here:; https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1068#issuecomment-590009483:62,wrap,wrapper,62,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-590009483,1,['wrap'],['wrapper']
Integrability,"Hi, please create a [minimal reproducible example][mre], including a complete stack trace of the error. Mentioning which data set you used and the error message alone does not help us to figure out what happened, as we lack the context of both the steps you took that lead up to the error and where exactly in the code the error happened. [mre]: https://stackoverflow.com/help/minimal-reproducible-example",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3261#issuecomment-2376233635:153,message,message,153,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3261#issuecomment-2376233635,1,['message'],['message']
Integrability,"Hi, please provide the data you use, otherwise this is not reproducible:. ```pytb; FileNotFoundError: [Errno 2] Unable to open file (unable to open file: name = '\external/CytAssist_FFPE_Human_Lung_Squamous_Cell_Carcinoma_filtered_feature_bc_matrix.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2778#issuecomment-1845023488:271,message,message,271,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2778#issuecomment-1845023488,1,['message'],['message']
Integrability,"Hi, thanks for the report!. Note that the plots in the documentation are generated on the fly when building the documentation. The plot you currently see on https://scanpy.readthedocs.io/en/stable/generated/scanpy.pl.dotplot.html has therefore been created with scanpy 1.10.1. Must be a dependency issue, I’ll try to reproduce with the environment you provided. /edit: I can reproduce it with that environment:. <details><summary>environment.yml</summary>. ```yaml; name: scanpy-3062; channels:; - conda-forge; dependencies:; - ipykernel. - python==3.10.10; - anndata==0.10.7; - scanpy==1.10.1; - IPython==8.13.2; - pillow==10.0.0; - astunparse==1.6.3; - backcall==0.2.0; - cffi==1.15.1; - cloudpickle==2.2.1; - colorama==0.4.4; - cycler==0.10.0; - cytoolz==0.12.0; - dask==2023.10.1; #- dateutil==2.8.2; - decorator==5.1.1; - defusedxml==0.7.1; - dill==0.3.6; - entrypoints==0.4; - exceptiongroup==1.1.1; - executing==1.2.0; - fasteners==0.17.3; - gmpy2==2.1.2; - h5py==3.8.0; #- icu==2.11; - python-igraph==0.11.2; - jedi==0.19.1; - jinja2==3.1.2; - joblib==1.2.0; - kiwisolver==1.4.4; - leidenalg==0.10.2; - llvmlite==0.42.0; - lz4==4.3.2; - markupsafe==2.1.2; - matplotlib==3.8.3; - mpmath==1.3.0; #- msgpack==1.0.5; - natsort==8.3.1; - numba==0.59.1; - numcodecs==0.11.0; - numexpr==2.7.3; - numpy==1.26.4; - packaging==23.1; - pandas==1.5.3; - parso==0.8.3; - pexpect==4.8.0; - pickleshare==0.7.5; - plotly==5.14.1; - prompt_toolkit==3.0.38; - psutil==5.9.5; - ptyprocess==0.7.0; - pure_eval==0.2.2; - pyarrow==10.0.1; - pydot==1.4.2; - pygments==2.15.1; - pyparsing==3.0.9; - pytz==2023.3.post1; - scipy==1.13.0; #- session_info==1.0.0; #- setuptools==67.7.2; - six==1.16.0; - scikit-learn==1.2.2; - stack_data==0.6.2; - sympy==1.11.1; - tblib==1.7.0; - texttable==1.6.7; - threadpoolctl==3.1.0; #- tlz==0.12.0; - toolz==0.11.2; #- pytorch==2.1.1; - tqdm==4.65.0; - traitlets==5.9.0; - wcwidth==0.2.6; #- yaml==5.4.1; - zarr==2.14.2; ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3062#issuecomment-2114986516:287,depend,dependency,287,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3062#issuecomment-2114986516,4,['depend'],"['dependencies', 'dependency']"
Integrability,"Hi,. I am writing the Galaxy integration for `pl.paga_path`. From the documentation, I understand that if we can provide an annotation (other than `dpt_pseudotime`) that are keys of `adata.obs`.; I tested with a dataset in which `dpt_pseudotime` is not a key of `adata.obs` and I got the error:. ```; Traceback (most recent call last):; File ""/miniconda3/envs/mulled-v1-9b7030900b5a2e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/scanpy/plotting/tools/paga.py"", line 930, in paga_path; idcs_group = np.argsort(adata.obs['dpt_pseudotime'].values[; File ""/miniconda3/envs/mulled-v1-9b7030900b5a2e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/pandas/core/frame.py"", line 2688, in __getitem__; return self._getitem_column(key); File ""/miniconda3/envs/mulled-v1-9b7030900b5a2e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/pandas/core/frame.py"", line 2695, in _getitem_column; return self._get_item_cache(key); File ""/miniconda3/envs/mulled-v1-9b7030900b5a2e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/pandas/core/generic.py"", line 2489, in _get_item_cache; values = self._data.get(item); File ""/miniconda3/envs/mulled-v1-9b7030900b5a2e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/pandas/core/internals.py"", line 4115, in get; loc = self.items.get_loc(item); File ""/miniconda3/envs/mulled-v1-9b7030900b5a2e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/pandas/core/indexes/base.py"", line 3080, in get_loc; return self._engine.get_loc(self._maybe_cast_indexer(key)); File ""pandas/_libs/index.pyx"", line 140, in pandas._libs.index.IndexEngine.get_loc; File ""pandas/_libs/index.pyx"", line 162, in pandas._libs.index.IndexEngine.get_loc; File ""pandas/_libs/hashtable_class_helper.pxi"", line 1492, in pandas._libs.hashtable.PyObjectHashTable.get_item; File ""pandas/_libs/hashtable_class_helper.pxi"", line 1500, i",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/328:29,integrat,integration,29,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/328,1,['integrat'],['integration']
Integrability,"Hi,. I can't get the ordinal regression test case to run on my MacBook. The single cpu case works fine, but if I ask for multiple processes, they launch, but activity monitor has them all at 0% cpu, with the main thread locking while waiting. Sometimes (routinely if I switch out `map_async` with `map`) I will get a crash log telling me:. ```; Crashed Thread: 0 Dispatch queue: com.apple.main-thread. Exception Type: EXC_BAD_ACCESS (SIGSEGV); Exception Codes: KERN_INVALID_ADDRESS at 0x0000000000000110; Exception Note: EXC_CORPSE_NOTIFY. Termination Signal: Segmentation fault: 11; Termination Reason: Namespace SIGNAL, Code 0xb; Terminating Process: exc handler [0]. VM Regions Near 0x110:; --> ; __TEXT 0000000102a16000-0000000102a18000 [ 8K] r-x/rwx SM=COW j [/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python]. Application Specific Information:; *** multi-threaded process forked ***; crashed on child side of fork pre-exec. Thread 0 Crashed:: Dispatch queue: com.apple.main-thread; 0 libdispatch.dylib 	0x00007fff572df8e1 _dispatch_root_queue_push + 108; 1 libBLAS.dylib 	0x00007fff2bfd0c9a rowMajorTranspose + 546; 2 libBLAS.dylib 	0x00007fff2bfd0a65 cblas_dgemv + 757; 3 multiarray.cpython-36m-darwin.so	0x00000001035c4f86 gemv + 182; 4 multiarray.cpython-36m-darwin.so	0x00000001035c4527 cblas_matrixproduct + 2807; 5 multiarray.cpython-36m-darwin.so	0x000000010358ab27 PyArray_MatrixProduct2 + 215; 6 multiarray.cpython-36m-darwin.so	0x000000010358626c array_dot + 188; 7 org.python.python 	0x0000000102a5d12e _PyCFunction_FastCallDict + 463; 8 org.python.python 	0x0000000102ac30e6 call_function + 491; 9 org.python.python 	0x0000000102abb621 _PyEval_EvalFrameDefault + 1659; 10 org.python.python 	0x0000000102ac3866 _PyEval_EvalCodeWithName + 1747; ```. Here's what I was running to cause that:. ```python; import numpy as np; import scanpy.api as sc; from anndata import AnnData; from scipy.sparse import random. adata ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/182:254,rout,routinely,254,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182,1,['rout'],['routinely']
Integrability,"Hi,. I corrected these small mistakes while checking the documentation to write Galaxy wrapper. Bérénice",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/282:87,wrap,wrapper,87,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/282,1,['wrap'],['wrapper']
Integrability,"Hi,. I found that in some tutorial documents, they does not use sc.pp.scale before sc.tl.pca. https://scanpy-tutorials.readthedocs.io/en/latest/integrating-data-using-ingest.html. But for some documents, they used. https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html. This thing also happened in several tools' analysis codes. https://docs.scvi-tools.org/en/stable/tutorials/notebooks/api_overview.html. Therefore, I wonder if the scale function is a key step for PCA analysis or not. Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2164:144,integrat,integrating-data-using-ingest,144,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2164,1,['integrat'],['integrating-data-using-ingest']
Integrability,"Hi,. I'm trying to follow the [Dahlin18 PAGA tutorial](https://nbviewer.jupyter.org/github/theislab/paga/blob/master/blood/dahlin18/dahlin18.ipynb). And in the part where it calls the UMAP function providing it with the PAGA initial points (line 28 in the notebook: `sc.tl.umap(adata, init_pos='paga')`), I'm getting this error message:. ```; computing UMAP; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/home/nr/miniconda3/lib/python3.7/site-packages/scanpy/tools/_umap.py"", line 145, in umap; verbose=settings.verbosity > 3,; File ""/home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 1005, in simplicial_set_embedding; verbose=verbose,; File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 401, in _compile_for_args; error_rewrite(e, 'typing'); File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 344, in error_rewrite; reraise(type(e), e, None); File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/six.py"", line 668, in reraise; raise value.with_traceback(tb); numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend); Invalid use of type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)) with parameters (array(float64, 1d, C), array(float32, 1d, C)); Known signatures:; * (array(float32, 1d, A), array(float32, 1d, A)) -> float32; * parameterized; [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)); [2] During: typing of call at /home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py (795). File ""miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 795:; def optimize_layout(; <source elided>. dist_squared = rdist(current, other); ^. This is not usually a problem with Numba itself but instead often caused by; the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:; http://numba.pydata.org/numba-doc/latest/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/948:328,message,message,328,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/948,1,['message'],['message']
Integrability,"Hi,. If `adata` is your anndata object, you can get the expression data from `adata.X`. Depending on what you did to the object before, that will contain differently pre-processed data. If you want it for ""Gene A"", then you can use `adata[:,'Gene A'].X`to get the expression value of Gene A in all cells. You can of course put that into a pandas dataframe via `pd.DataFrame()`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/870#issuecomment-541467843:88,Depend,Depending,88,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/870#issuecomment-541467843,1,['Depend'],['Depending']
Integrability,"Hi,. If you solely `pip install scanpy` it will use the latest versions of both, Scanpy and umap-learn. These are certainly compatible. Scanpy 1.7.2 not being perfectly compatible with the latest umap-learn package is an artifact of us not pinning the dependencies too hard and being more on the lenient side. I'd suggest to simply use the latest versions of both packages and you should not run into any problems.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2101#issuecomment-1005944568:252,depend,dependencies,252,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2101#issuecomment-1005944568,2,['depend'],['dependencies']
Integrability,"Hi,. The issues I was mostly running into were that when saving the anndata; variable as a h5ad file, 'pheno_jaccard_ig' was not compatible with this; action. So, I had to either remove pheno_jaccard_ig from the anndata object; and then save it as h5ad or convert it to a sparse matrix. This also; happened with a few other functions I tried on the anndata object, and I; kept getting the error ""this function is not compatible with COO matrix; format"", always talking about pheno_jaccard_ig. Therefore, since a sparse; matrix object does not have any problems with the functions I was running; on adata, changing pheno_jaccard_ig to a sparse matrix from the start makes; sense to circumvent any of those issues I was getting before. I hope this makes sense.; Thank you,; Deena Shefter. On Wed, Jul 27, 2022 at 6:10 PM Lukas Heumos ***@***.***>; wrote:. > Hi,; >; > could you please provide more details? What issues did you run into?; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/scverse/scanpy/pull/2295#issuecomment-1197424392>, or; > unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AMILUOEK7J64GU3YOR7DB53VWGXULANCNFSM534YT5ZA>; > .; > You are receiving this because you authored the thread.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2295#issuecomment-1274299180:1249,Message,Message,1249,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2295#issuecomment-1274299180,1,['Message'],['Message']
Integrability,"Hi,. You are correct that DE testing should be performed on raw or normalized data, but not on batch-corrected data. `sc.tl.rank_genes_groups()` doesn't let you include covariates, but there are plenty of methods that do. You could look into `diffxpy` for this, which is also based on AnnData and is easily integrated into a scanpy script. Otherwise, I have a case study for a best practices workflow, which uses MAST. You could reuse code from there as well. You can find the case study [here](https://www.github.com/theislab/single-cell-tutorial).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/669#issuecomment-497118928:307,integrat,integrated,307,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/669#issuecomment-497118928,1,['integrat'],['integrated']
Integrability,"Hi,. different versions of dependencies can lead to such changes. This is nothing major, don't worry about this. The general embedding structure will always be very similar though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2471#issuecomment-1513322610:27,depend,dependencies,27,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2471#issuecomment-1513322610,1,['depend'],['dependencies']
Integrability,"Hi,. thanks for the input and the nice description. Agree that this would be nice to have within scanpy itself. And agree, as you’ve shown in your example, that this should be the case when setting `copy=True`. I have set up a draft PR for the moment, with a suggestion where the error message especially persists when `copy=False`; I’d suggest to keep it this way:. - Overwriting the backed file seems not to be the expected behaviour to me; - Writing a new file for backing would occur in a rather hidden manner, confusing the user or even unexpectedly further fill the disk at worst over time",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2495#issuecomment-1683604054:286,message,message,286,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2495#issuecomment-1683604054,1,['message'],['message']
Integrability,"Hi,. when using the command ; ```python; sc.preprocessing.highly_variable_genes.highly_variable_genes(all_data,n_top_genes=5000); ```. I get this informative message:; ```; --> added; 'highly_variable', boolean vector (adata.var); 'means', boolean vector (adata.var); 'dispersions', boolean vector (adata.var); 'dispersions_norm', boolean vector (adata.var); ```. Even though not all of them are booleans! I guess it is just a copy-paste residue from implementation :); Cheers,; Samuele",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/411:158,message,message,158,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/411,1,['message'],['message']
Integrability,"Hi,; I find in scanpy 1.9.1 sc.pl.umap(adata,color=['leiden','batch']) can't show the color correctly. The color of ""leiden"" and ""batch"" are different, It seems that sc.pl.umap set the colors randomly, and the batch_color depends on leiden_color. There are some conflicts between leiden_color and batch_color. If I plot leiden and batch at the same time, then leiden will be the same as louvain color in https://github.com/scverse/scanpy/issues/156, and batch color is the same as the leiden color in https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html. I remeber that this maybe an old issue, but I cannot find it. This bug was fixed in some versions, but now 1.9.1 introduces it again.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2299:222,depend,depends,222,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2299,1,['depend'],['depends']
Integrability,"Hi,; I have some problems running Louvain clustering.; The first time I tried to run, it complains about missing library `igraph`.; I installed `igraph` but now this same library throws a `DeprecationWarning` when calling Louvain clustering. ```; ---------------------------------------------------------------------------; DeprecationWarning Traceback (most recent call last); <ipython-input-17-329d7c2ac26c> in <module>(); ----> 1 sc.tl.louvain(adata). ~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/scanpy/tools/louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, n_jobs, copy); 79 directed = False; 80 if not directed: logg.m(' using the undirected graph', v=4); ---> 81 g = utils.get_igraph_from_adjacency(adjacency, directed=directed); 82 if flavor == 'vtraag':; 83 import louvain. ~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/scanpy/utils.py in get_igraph_from_adjacency(adjacency, directed); 92 def get_igraph_from_adjacency(adjacency, directed=None):; 93 """"""Get igraph graph from adjacency matrix.""""""; ---> 94 import igraph as ig; 95 sources, targets = adjacency.nonzero(); 96 weights = adjacency[sources, targets]. ~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/igraph/__init__.py in <module>(); 6 __license__ = ""MIT""; 7 ; ----> 8 raise DeprecationWarning(""To avoid name collision with the igraph project, ""; 9 ""this visualization library has been renamed to ""; 10 ""'jgraph'. Please upgrade when convenient.""). DeprecationWarning: To avoid name collision with the igraph project, this visualization library has been renamed to 'jgraph'. Please upgrade when convenient.; ```. I can work on a PR and change line 94 of `scanpy/utils.py` to:. ```; import jgraph as ig; ```. This should solve the issue afaik (and `jgraph` should be listed in the dependencies I think). Let me know if you accept PRs. Thanks,; Francesco",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/138:1825,depend,dependencies,1825,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138,1,['depend'],['dependencies']
Integrability,"Hi,; I was trying to run the quick example described in the magic api cmd using datasets.paul15 but it keeps on giving me the same error. See below the code I used and the error it gives. . import numpy as np; import pandas as pd; import scanpy.api as sc; import matplotlib.pyplot as pl; import phate; import magic. sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3); sc.settings.set_figure_params(dpi=80) # low dpi (dots per inch) yields small inline figures; sc.logging.print_version_and_date(); # we will soon provide an update with more recent dependencies; sc.logging.print_versions_dependencies_numerics(). Running Scanpy 1.2.2+72.gbc6661c on 2018-07-18 19:40.; Dependencies: anndata==0.6.5 numpy==1.14.3 scipy==1.1.0 pandas==0.23.0 scikit-learn==0.19.1 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . adata = sc.datasets.paul15(). WARNING: In Scanpy 0.*, this returned logarithmized data. Now it returns non-logarithmized data.; ... storing 'paul15_clusters' as categorical. sc.pp.normalize_per_cell(adata); sc.pp.sqrt(adata); adata_magic = sc.pp.magic(adata, name_list=['Mpo', 'Klf1', 'Ifitm1'], k=5); adata_magic.shape. computing PHATE. ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-79-129f35d34dbd> in <module>(); 2 sc.pp.normalize_per_cell(adata); 3 sc.pp.sqrt(adata); ----> 4 adata_magic = sc.pp.magic(adata.X, name_list=['Mpo', 'Klf1', 'Ifitm1'], k=5); 5 adata_magic.shape. ~/software/scanpy/scanpy/preprocessing/magic.py in magic(adata, name_list, k, a, t, n_pca, knn_dist, random_state, n_jobs, verbose, copy, **kwargs); 131 n_jobs=n_jobs,; 132 verbose=verbose,; --> 133 **kwargs).fit_transform(adata,; 134 genes=name_list); 135 logg.info(' finished', time=True,. TypeError: 'module' object is not callable",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/208:582,depend,dependencies,582,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/208,2,"['Depend', 'depend']","['Dependencies', 'dependencies']"
Integrability,"Hi,; I'd like to plot a bunch of figures using the sc.pl.xx functions.; Is there some solution to suppress the Warning message during saving the figure?; the warning looks like that:; ```; WARNING: saving figure to file /home/test/figure/umap.marker1.png; WARNING: saving figure to file /home/test/figure/umap.marker2.png; WARNING: saving figure to file /home/test/figure/umap.marker3.png; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2238:119,message,message,119,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2238,1,['message'],['message']
Integrability,"Hi,; Is it necessary to use only high variable genes for the downstream analysis ?; If an examperiment includes many batches, then each batch will give a different set of high variable genes, how to determine the shared high variable genes (intersection or union) when integrating the batches ? Does scany have any fucntion to get the shared genes ?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1578:269,integrat,integrating,269,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578,1,['integrat'],['integrating']
Integrability,"Hi,; It looks like this code comes from the [single-cell-tutorial github](https://github.com/theislab/single-cell-tutorial). It might be best to report the issue there. It looks like you haven't filtered out genes that are not expressed in your dataset via `sc.pp.filter_genes()`. If you filter the dataset (maybe with `min_cells` set to 5-50, depending on the size of your dataset), then this shouldn't happen. Here, you have too many genes with the same mean, which is very low.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1560#issuecomment-753982630:344,depend,depending,344,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1560#issuecomment-753982630,1,['depend'],['depending']
Integrability,"Hi,; Would it be possible to create a panel of plots using both rows and columns when plotting tsne?; I did something similar to this:; ![tsne markers](https://user-images.githubusercontent.com/697622/39486721-19f44a02-4d4b-11e8-986e-d78079a06e0c.png). Basically the code calls the matplotlib subplots method based on the number of plots:. ```py; def _build_subplots(n):; '''; Build subplots grid; n: number of subplots; '''; nrow = int(np.sqrt(n)); ncol = int(np.ceil(n / nrow)); fig, axs = plt.subplots(nrow, ncol, dpi=100, figsize=(ncol*5, nrow*5)). return fig, axs, nrow, ncol; ```. Then the plots are drawn:. ```py; genes = [...list of gene symbols...]; fig, axs, nrow, ncol = _build_subplots(len(genes)). if type(axs) != np.ndarray:; axs = [axs]; else:; axs = axs.ravel(). for i in range(nrow*ncol):; if i < len(genes):; gene = genes[i]; # df is the numpy array containing tSNE; axs[i].scatter(df[:, 0], df[:, 1], ...); ```. Is it something that is already done, planned or that you don't want to integrate?. Thanks,; Francesco",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/137:1003,integrat,integrate,1003,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/137,1,['integrat'],['integrate']
Integrability,"Hi,; Your question is actually quite difficult to answer. It depends on how you define cell identity. For example, depending on how you set the resolution parameter for your clustering you can get a very different number of clusters. As clusters often have super- and sub-structure, you cannot easily say when a sub- or superstructure is not meaningful (e.g., T-cells vs CD4+ and CD8+ T-cells). Unfortunately many clustering techniques do not incorporate an assessment of uncertainty to tell you when you are fitting noise. And even when they do, this is based on a model of what a cell cluster should look like, which does not have to conform to the biological reality. The heuristic that tends to be used is that if you can biologically interpret your clusters based on marker genes and other signatures, then they are meaningful. That does however not mean that sub- or superstructures involving these clusters are not also meaningful. Sorry, I can't give a clearer answer than that. In terms of assessing differences between clusters, this is a somewhat related problem. An approach that you could use is to look at the distribution of Euclidean distances in gene- or PCA-space. However, that comes with the caveat that Euclidean distance does not take into account the biological manifold on which the cells lie (and on which distances will be more informative). You can try to use pseudotime as a measure for distance over this manifold, however that would require you to have sampled enough of the manifold to fit it in your data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/491#issuecomment-464778874:61,depend,depends,61,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/491#issuecomment-464778874,4,['depend'],"['depending', 'depends']"
Integrability,"Hi,; we want to add SCALEX, an online integration method that integrate different single-cell experiments including scRNA-seq and scATAC-seq, which also enable accurate projecting new incoming datasets onto the existing cell space. This method is published on Nature Communications. SCALEX is developed based-on scanpy. We appreciate and will be honored to contribute to the scanpy community. Thank you!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2355:38,integrat,integration,38,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2355,2,['integrat'],"['integrate', 'integration']"
Integrability,"Hi,; we want to add SCALEX, an online integration method that integrate different single-cell experiments including scRNA-seq and scATAC-seq, which also enable accurate projecting new incoming datasets onto the existing cell space. This method is published on Nature Communications. SCALEX is developed based-on scanpy. We appreciate and will be honored to contribute to the scanpy community. Thank you!; Lei",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2354:38,integrat,integration,38,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2354,2,['integrat'],"['integrate', 'integration']"
Integrability,"Hi. After I performed ingest, I need to concatenate the two datasets. But when followed the tutorial, used concatenated but this function doesn't;t concatenate the .obsm, therefore the UMAP coordinates are not merged. How did you manage to performed UMAP on the integrated/concatenated dataset?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/985:262,integrat,integrated,262,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/985,1,['integrat'],['integrated']
Integrability,"Hi. I have a problem with reproducing my analysis using scanpy 1.4.3. I have used two different machines using the same software version (all dependencies as well), and I keep having different results using the same commands and seed numbers. In one machine I get this:. <img width=""340"" alt=""Screenshot 2019-06-05 at 15 44 31"" src=""https://user-images.githubusercontent.com/3297906/58965800-13a03500-87a9-11e9-8a7c-bec104f4718e.png"">. and in the other I get this:. <img width=""334"" alt=""Screenshot 2019-06-05 at 15 44 14"" src=""https://user-images.githubusercontent.com/3297906/58965827-1e5aca00-87a9-11e9-95e5-90c09303a1c2.png"">. Can you recommend which aspects to check? or how to deal with this issue?. Thanks in advance!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/681:142,depend,dependencies,142,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/681,1,['depend'],['dependencies']
Integrability,"Hi. I have a question. I am using the scanorama for integrating multiple datasets. In my use-case, I will have a new dataset again. The question is, is there any way to transfer the results of scanorama to the new dataset? Or should I retrain everything again. This is also important if one has a train and test set. Ideally, you do not want to use the test set to use in the data integration training and only use the already trained transformation on the test data. . I am wondering if there is a solution for these use-cases? Would you please help us with that. Cheers; Ali",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2162:52,integrat,integrating,52,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2162,2,['integrat'],"['integrating', 'integration']"
Integrability,"Hi. Maybe I can help a little as well. Typically batch correction or data integration methods would be used to obtain good clustering of the data, however once differential testing is performed it is still unclear whether the corrected data can or should be used (no batch correction method is perfect and may overcorrect). The standard strategy would be to correct for batch, and any other covariates that you are not interested in for the clustering process. Once you have the clusters, it is standard practice to go back to the raw data and use a differential testing algorithm that allows you to account for batch and other technical covariates in the model (e.g. MAST).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/168#issuecomment-395726806:74,integrat,integration,74,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168#issuecomment-395726806,1,['integrat'],['integration']
Integrability,"Hi. While @flying-sheep is right that this is a question for the discourse group, I have a quick response here. Check out the tutorials here:; https://scanpy.readthedocs.io/en/stable/tutorials.html. You can merge anndata objects with `anndata.concatenate()`, and if you're looking for data integration methods, there are a couple of them in `scanpy.external`. For example MNN (works better in `mnnpy` though), and BBKNN. Scanorama also has a nice scanpy interface... and I think `SCVI` also has a scanpy integration tutorial (but can be harder to use than scanorama).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/859#issuecomment-545962554:290,integrat,integration,290,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859#issuecomment-545962554,3,"['integrat', 'interface']","['integration', 'interface']"
Integrability,"Hi; I am trying to concatenate two anndata objects(adata = adata1.concatenate(adata3, join='inner')), but get an error message informing me that concatenate is not found. Any idea what might cause this behavior? Here is what I have installed:scanpy==1.4.3 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.2 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1. and here is the error:; AttributeError Traceback (most recent call last); <ipython-input-187-32c3eda3cdc8> in <module>; ----> 1 adata = adata1.concatenate(adata3, join='inner'). ~/anaconda3/envs/scenv/lib/python3.6/site-packages/scipy/sparse/base.py in __getattr__(self, attr); 687 return self.getnnz(); 688 else:; --> 689 raise AttributeError(attr + "" not found""); 690 ; 691 def transpose(self, axes=None, copy=False):. AttributeError: concatenate not found; Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/760:119,message,message,119,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/760,1,['message'],['message']
Integrability,"Hm, it is still required to follow the internals of umap to reconstruct UMAP object from anndata info. Also this adds dependency on pynndescent.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1038#issuecomment-585107629:118,depend,dependency,118,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1038#issuecomment-585107629,1,['depend'],['dependency']
Integrability,"Hmm, doesn’t seem to work with `functools.partial`. It works if I do almost the same, but with a lambda:. ```py; import scanpy as sc; from functools import wraps, partial. pca = wraps(sc.pl.scatter)(lambda *args, **kw: sc.pl.scatter(*args, basis=""pca"", **kw)); ```. But I think the custom solution above is better anyway! `wraps` is if you really want to pose as the wrapped function, while we only want to inherit its signature.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/535#issuecomment-474255934:156,wrap,wraps,156,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/535#issuecomment-474255934,4,['wrap'],"['wrapped', 'wraps']"
Integrability,"Hmm, you're right. I think it must have been the that the ordering of cells in the `adata` object was also non-random. We had this quite a bit in the benchmarking data integration project while plotting batch. In several methods (e.g., scanorama), individual batch anndata objects are concatenated to generate the final output, which results in batch-ordered anndata objects. . Maybe instead of just having `sort_order=False` it would be better to have randomized ordering for plotting categorical variables? Unless it is an ordered categorical I guess.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1588#issuecomment-760249638:168,integrat,integration,168,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1588#issuecomment-760249638,1,['integrat'],['integration']
Integrability,"Hopefully last update on this PR. What I did:; - I noticed a regression on the method `rank_genes_groups_violin`, therefore I reverted back the code to the original one and I added an additional method `genes_groups_violin` which should be used if we want to pass the list of genes directly to the violin plot. The code is just a POC, but maybe it can be integrated; - Within the same method `rank_genes_groups_violin`, I found a bug: the ax variable was overwritten for each group (I don't know if it gave you error before). In my case, all the plots were merged into a single figure, every one on top of the previous ones; - Additionally, the parameters `gene_symbols` and `computed_distribution` were not defined within the method `rank_genes_groups_violin`. I added a default parameter (`None`) for `gene_symbols`, since it was defined in the docstring. With `computed_distribution` I didn't know what you wanted to do so I temporarily commented the line that used it",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/141#issuecomment-387106636:355,integrat,integrated,355,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/141#issuecomment-387106636,1,['integrat'],['integrated']
Integrability,"How is this work going? We'd love to integrate Scrublet into our workflows, which are currently quite Scanpy-centric.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/173#issuecomment-545010991:37,integrat,integrate,37,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-545010991,1,['integrat'],['integrate']
Integrability,How to integrate the snRNA seq data generated by `scanpy` with the snATAC seq data generated by `ArchR` ?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3273:7,integrat,integrate,7,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3273,1,['integrat'],['integrate']
Integrability,"How would you change it? I'd probably just wrap the section that says ""paste here"" with a `<details>` tag.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1343#issuecomment-666268228:43,wrap,wrap,43,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1343#issuecomment-666268228,1,['wrap'],['wrap']
Integrability,"How would you suggest doing the API for this? Another `kwarg` for backend?. The additional dependencies aren't so bad. They are `xarray`, `dask`, and `pillow`. But still, I probably wouldn't be up for data shader as a required dependency.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2656#issuecomment-1709960901:91,depend,dependencies,91,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2656#issuecomment-1709960901,2,['depend'],"['dependencies', 'dependency']"
Integrability,"Huh, that is weird. Also weird that it's including every optional dependency by default. Any chance you know if there's a way to not do that?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2144#issuecomment-1046710380:66,depend,dependency,66,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2144#issuecomment-1046710380,1,['depend'],['dependency']
Integrability,"I agree that it's bad behavior to modify state on import. I think it's worse to modify state after a function is called, save a few cases where it's obvious that will happen. I think it takes less time to figure out why my plot suddenly looks different if it's based on imports than which functions were called prior. I think if we could make all of our plots without importing `pyplot` that would be great. I'm not sure how feasible this is. Not only do we use `pyplot` a lot, but libraries we depend on for plots (like `seaborn`) import `pyplot`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/756#issuecomment-523738527:495,depend,depend,495,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/756#issuecomment-523738527,1,['depend'],['depend']
Integrability,"I agree that using the intersection is too harsh if the only data integration/batch correction you do is this HVG filtering, but for `mnn_correct` for example you only use these HVGs to calculate the technical batch vector. In that case you ideally don't want to capture the biological variation between batch samples. For that I reckon having an option of getting the intersection would be good. And for point 2... definitely agreed... but again, a different use case for me. The same approach (intersection of HVGs) is suggested for the new CCA in Seurat v3 btw.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/614#issuecomment-485875031:66,integrat,integration,66,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614#issuecomment-485875031,1,['integrat'],['integration']
Integrability,"I also experienced this a few times, and took me some time to understand what is going on. I fully agree with @ivirshup, we should improve the error message.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1504#issuecomment-748161925:149,message,message,149,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1504#issuecomment-748161925,1,['message'],['message']
Integrability,"I am also experiencing this issue. Running the following code:; ```; import pandas as pd; import scanpy as sc; import anndata. print(pd.__version__); print(sc.__version__); print(anndata.__version__); adata = sc.datasets.pbmc68k_reduced(); adata.obs[""single_cat""] = 1; adata.obs['single_cat'] = pd.Categorical(adata.obs['single_cat']); adata.write('/tmp/adata.h5ad'); sc.read('/tmp/adata.h5ad'); ```. Returns this error message:; ```; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-5-adde38d13544> in <module>; ----> 1 sc.read('/tmp/adata.h5ad'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 95 filename, backed=backed, sheet=sheet, ext=ext,; 96 delimiter=delimiter, first_column_names=first_column_names,; ---> 97 backup_url=backup_url, cache=cache, **kwargs,; 98 ); 99 # generate filename and read to dict. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 497 if ext in {'h5', 'h5ad'}:; 498 if sheet is None:; --> 499 return read_h5ad(filename, backed=backed); 500 else:; 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size); 445 else:; 446 # load everything into memory; --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size); 448 X = constructor_args[0]; 449 dtype = None. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size); 500 if not backed:; 501 f.close(); --> 502 return AnnData._args_from_dict(d); 503 ; 504 . /usr/local/anaconda3/envs/d",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/102#issuecomment-566126409:420,message,message,420,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/102#issuecomment-566126409,1,['message'],['message']
Integrability,"I am following the SCENIC protocol (https://github.com/aertslab/SCENICprotocol/blob/master/notebooks/PBMC10k_SCENIC-protocol-CLI.ipynb) with an admittedly different data set, but still using 10x data of similar type. I have to admit that I am relatively new to the python world and don't know yet where to turn to... ; haven't found any way to debug any further I conclude that this is a scanpy issue, at a minimal level at the error reporting stage as the information doesn't help me track down what is wrong. thanks for your time! . adata. ```; AnnData object with n_obs × n_vars = 4578 × 3389; obs: 'nGene', 'nUMI', 'n_genes', 'percent_mito', 'n_counts', 'louvain', 'leiden'; var: 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'mean', 'std'; uns: 'log1p', 'hvg', 'pca', 'neighbors', 'umap', 'louvain', 'leiden', 'louvain_colors', 'rank_genes_groups'; obsm: 'X_pca', 'X_umap', 'X_tsne'; varm: 'PCs'; obsp: 'distances', 'connectivities'; ```. my adata.raw.X looks like this:. ```; <4578x18247 sparse matrix of type '<class 'numpy.float32'>'; 	with 9236127 stored elements in Compressed Sparse Row format>; ```. adatat.X. ```; array([[ 0.23202083, 0.07064813, -0.05003222, ..., 1.4681866 ,; -0.21488723, 2.620106 ],; [ 0.09879599, 0.03607919, -0.08120057, ..., -0.3384455 ,; -0.19780253, 2.0771198 ],; [-0.5213845 , -0.1292537 , -0.1755099 , ..., -0.23126683,; -0.10592338, 0.02626752],; ...,; [ 2.4987383 , -0.14190508, -0.20776471, ..., -0.20877847,; -0.10354204, 0.14313072],; [ 0.1960011 , 0.06290449, -0.07691702, ..., -0.34954828,; 2.5718384 , 2.468825 ],; [-0.53571457, -0.13106212, -0.20085879, ..., -0.21621887,; -0.10943384, 0.05686853]], dtype=float32); ```. ### Minimal code sample (that we can copy&paste without having any data). ```python; # find marker genes; sc.tl.rank_genes_groups(adata, 'louvain', method='t-test', reference = 'rest'); ```. ```pytb; ranking genes; ---------------------------------------------------------------------------; AttributeEr",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2121:26,protocol,protocol,26,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2121,2,['protocol'],"['protocol', 'protocol-CLI']"
Integrability,"I am more and more convinced about having a single package for the reasons @adamgayoso mentioned. To address a few concerns from above: . ---. > > Who manages the sub-packages?; > ; > Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). Scverse core developers could take turns (e.g. every 6 months) in being ""lead maintainer"", i.e. in charge of releases and first-responders to issues (delegating them to the most appropriate people). This has the additional advantage that everything needs to be documented to a point that there can't be a single point of failure. . ---. > Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. ```; pip install scio[all]; ```. could be broadly advertised in the README. Packages could still use the slimmer version, e.g. in scirpy, I could depend on ; `scio[vdj]`. . ---. > I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult. I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common `scio` package could be a way to achieve that. . > I know squidpy will be changing its representation and I think muon should have changes to the ATAC representation. Also muon and scvi-tools read in different things from 10x atac data. A solution to that would be versioned schemata. E.g. whatever squidpy uses now is the ""spatial schema `v1`"". When we come up with a better way it becomes the ""spatial schema `v2`"". Old schemata will be deprecated but can stick around for a while. If a schema is experimental and subject to active changes it can be `v0.1`. . ```python; scio.spatial.read_vis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059727261:916,depend,dependencies,916,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059727261,1,['depend'],['dependencies']
Integrability,"I am not sure if it has been already addressed.; This should fix the following import error of scanpy from master, due to missing `__init__.py` in external. Probably a more clean solution would be to wrap the import for external in a try/except block. ```python; ---------------------------------------------------------------------------; ModuleNotFoundError Traceback (most recent call last); in ; ----> 1 import scanpy as sc; 2 sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3); 3 sc.settings.set_figure_params(dpi=200) # low dpi (dots per inch) yields small inline figures; 4 sc.settings.figdir = out('fig_supp'). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/__init__.py in ; 31 from . import preprocessing as pp; 32 from . import plotting as pl; ---> 33 from . import datasets, logging, queries, settings, external; 34 ; 35 from anndata import AnnData. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/__init__.py in ; ----> 1 from . import tl; 2 from . import pl; 3 from . import pp; 4 ; 5 from .. import _exporting as exporting. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/tl.py in ; 2 from ..tools._phate import phate; 3 from ..tools._phenograph import phenograph; ----> 4 from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/585:200,wrap,wrap,200,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585,1,['wrap'],['wrap']
Integrability,I am not sure what king of test. I don't want to add another `save_and_compare_images` test because plots seem to depend on the system at least sometimes (i have 3 failing plotting tests locally but they run fine here).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1942#issuecomment-878118310:114,depend,depend,114,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1942#issuecomment-878118310,1,['depend'],['depend']
Integrability,I am using 1.4.3. I tried to upgrade to 1.4.4 but I was having a lot of problem with dependencies. Wonder it 1.4.3 is recent enough? Thank you so much.; scanpy 1.4.3 py_0 bioconda,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/871#issuecomment-544294477:85,depend,dependencies,85,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/871#issuecomment-544294477,1,['depend'],['dependencies']
Integrability,"I am using Anaconda/Jupyter in my PC. When I try to downgrade numba, I run into issues of numba dependency packages in Anaconda so I am stuck!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1341#issuecomment-670191957:96,depend,dependency,96,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1341#issuecomment-670191957,1,['depend'],['dependency']
Integrability,I believe everything is in place. We should have now a generic `add_score` which scores cells according to expression of gene lists. That is wrapped twice in `cell_cycle_score`.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/76:141,wrap,wrapped,141,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/76,1,['wrap'],['wrapped']
Integrability,"I can apply pretty easily scrublet from the original python package, so I; guess a wrapper should be something fast to implement :) I was thinking; about doing it last week, but I am no expert in this kind of stuff :/. Den tir. 14. maj 2019 kl. 20.18 skrev Alex Wolf <notifications@github.com>:. > I guess, we should ask @swolock <https://github.com/swolock>. 🙂; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/173?email_source=notifications&email_token=ACC66UJZJVVP2VIQXNRHEITPVL7A3A5CNFSM4FE4LIF2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVMK5PQ#issuecomment-492351166>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACC66UN3NQVYPI4KPDUAOK3PVL7A3ANCNFSM4FE4LIFQ>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/173#issuecomment-492392283:83,wrap,wrapper,83,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-492392283,1,['wrap'],['wrapper']
Integrability,"I can confirm this as a working workaround. Thank you @michalk8 . > @pati-ni; > I have the similar issue when installing CellRank as `conda install -c bioconda cellrank`. Problem is not all dependencies are on bioconda, some of them are on `conda-forge` - I've tested it and I have the same problem with scanpy.; > Installing it as `conda install -c bioconda -c conda-forge scanpy` works. But @ivirshup is right, seems like conda issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1298#issuecomment-662450011:190,depend,dependencies,190,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1298#issuecomment-662450011,1,['depend'],['dependencies']
Integrability,"I can understand your thought process behind facilitating the integration of anndata into the broader ecosystem and I can also understand the frustration. I don't think the integration is quite as bad as you suggest though. `adata.X` is still a `numpy.ndarray` and can be used as such, exactly as `adata.var` and `adata.obs` are dataframes. The only issue is when you require the object to work as a whole data structure in a particular function. I'm not the most experienced `numpy` user, but from what I've seen, you would typically expect any `numpy` function that you apply to an `anndata` object to be applied to `adata.X` and don't require information in other parts of the object. Or am I missing a use case here? So the only change would then be that `adata = np.srqt(adata)` would need to become `adata.X = np.sqrt(adata.X)`. Furthermore, it's not entirely clear what a `numpy` function applied to an `AnnData` object should do. `np.min()` could be on `adata.X` or any column in `.obs` or `.var`. You can call it on the columns in the `pandas` dataframes already via `pandas` conventions... which makes a bit more sense to me. Regarding the slicing conventions... @ivirshup has mentioned a few reasons why things are sliced as they are in `scanpy`. What would your suggestion look like? `loc` and `iloc` work for `adata.obs` and `adata.var` atm. Would you forbid an `adata['Cell A',:]`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-584118922:62,integrat,integration,62,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-584118922,4,['integrat'],['integration']
Integrability,"I can't install MulticoreTSNE, and it may not even work on python >3.6. Since we want to drop 3.6 support (#1697), it would be good to stop recommending it, and pass the n_jobs parameter to sklearn's tsne. This PR attempts to do that, along with a bunch of deprecation warnings. I've also bumped the sklearn dependency to make sure TSNE is multithreaded. Metric was added to test if n_jobs was working. Either way, it seems to be using all the cpu on my laptop. Not sure what's up with that. ## TODO:. - ~~[ ] Figure out how to get n_jobs to actually limit cpu usage~~ Leaving this up to sklearn; - [x] Test metric; - [x] Test deprecations",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1854:308,depend,dependency,308,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1854,1,['depend'],['dependency']
Integrability,I can’t reproduce this locally and would like to get the dependencies unlocked. Where’s the bug report at h5py?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/822#issuecomment-577743290:57,depend,dependencies,57,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/822#issuecomment-577743290,1,['depend'],['dependencies']
Integrability,"I completely agree that including the R/scran requirements will be troublesome and harms user experience. The reason I used a R-py interface is that there's no decent MNN correct on python yet, and scran's implementation is already fast and efficient enough, and I think this is meant to be an optional feature that provides a handy fix for those in need. Personally I would prefer if you guys create a submodule _rtools_, and put wrappers inside. This is going to be awesome to use and easy to maintain.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/125#issuecomment-382002082:131,interface,interface,131,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-382002082,4,"['interface', 'wrap']","['interface', 'wrappers']"
Integrability,"I completely understand but I’m travailing and have no access to my data. but it’s the basic function and you can use any anndata object and the groupby argument on any observation. Sent from my iPad. On 5. Jan 2023, at 15:11, Lukas Heumos ***@***.***> wrote:. ﻿. By a minimal working example I mean something that we can copy and paste and reproduce your result directly. The easier you make it for us the more likely we can dedicate some time to look at your issue. Thanks. —; Reply to this email directly, view it on GitHub<https://github.com/scverse/scanpy/issues/1988#issuecomment-1372263677>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AOGNBODKUC3S3SZC32VD3TLWQ3JBVANCNFSM5DB5VLAQ>.; You are receiving this because you authored the thread.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1988#issuecomment-1372293439:767,Message,Message,767,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1988#issuecomment-1372293439,1,['Message'],['Message']
Integrability,"I copied your code to a google colabs instance and ran into a Type Error similar to the one above:; https://colab.research.google.com/drive/1LYxOAuNqaJHGfRjNjyluUHk9BFsmkWa4?usp=sharing . Error message:; ```; TypeError Traceback (most recent call last); <ipython-input-3-9abce68d1753> in <module>(); 4 sc.tl.dpt(adata); 5 sc.tl.paga(adata, groups='paul15_clusters'); ----> 6 sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). 5 frames; /usr/local/lib/python3.6/dist-packages/matplotlib/image.py in set_data(self, A); 697 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):; 698 raise TypeError(""Invalid shape {} for image data""; --> 699 .format(self._A.shape)); 700 ; 701 if self._A.ndim == 3:. TypeError: Invalid shape (3, 43, 1) for image data; ```. Versions: ; ```; scanpy==1.7.0 ; anndata==0.7.5 ; umap==0.5.0 ; numpy==1.19.5 ; scipy==1.4.1 ; pandas==1.1.5 ; scikit-learn==0.22.2.post1 ; statsmodels==0.10.2 ; python-igraph==0.8.3 ; leidenalg==0.8.3; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/953#issuecomment-778212671:194,message,message,194,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953#issuecomment-778212671,2,['message'],['message']
Integrability,"I created [legacy-api-wrap](https://github.com/flying-sheep/legacy-api-wrap). Only caveat: Scanpy is still 3.5 compatible, and I’m using f string syntax in it and its dependency [get_version](https://github.com/flying-sheep/get_version), so it’s python 3.6 only (which could be circumvented via [future-fstrings](https://github.com/asottile/future-fstrings) or so)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-441684460:22,wrap,wrap,22,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441684460,3,"['depend', 'wrap']","['dependency', 'wrap']"
Integrability,I did successfully import all three dependencies and (just to make sure) still cannot import scanpy.; ```; >>> import numba; >>> import pynndescent; >>> import umap; >>> import scanpy; Illegal instruction (core dumped); ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2062#issuecomment-985144190:36,depend,dependencies,36,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2062#issuecomment-985144190,1,['depend'],['dependencies']
Integrability,"I didn't use any filtering before the pipeline, but I read the barcode and gene names from index and columns of the data frame. and i have a cuff-off of the cell for my own. update——————————————; The calculation is depends on module ""patsy"" and it is not in the dependency list of scanpy or I have some problems with installing that package. ; After I reinstalled and updated ""pasty"", problem fixed",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/212#issuecomment-407454918:215,depend,depends,215,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/212#issuecomment-407454918,2,['depend'],"['dependency', 'depends']"
Integrability,"I don't have `pytest` installed locally (will change that), and the plan was to emulate the Travis python 3.5 environment, but I'm not sure what versions of all the dependencies are in there. I've been debugging in a notebook, but it always works there... at least with python 3.6. I'll try just creating a conda python 3.5 env to see what happens when I do that. Chances are it will always work locally as well though... hence my remote debugging. Sorry for that... Previous print statements have shown that the order of covariates is just different sometimes in the recarrays. So I thought it would all be fixed with 5b602f5.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/583#issuecomment-479462140:165,depend,dependencies,165,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/583#issuecomment-479462140,1,['depend'],['dependencies']
Integrability,"I don't know how to fix this in the code, but I do find a workaround to specify a color palette. Turns out both `sc.pl.umap()` and `sc.pl.scatter()` accept a color palette if it's from `seaborn` color palette:. ```py; sc.pl.scatter(adata, 'n_genes', 'n_counts', color='louvain', palette='Set3'); ```. throws the above error message. But if I use. ```py; sc.pl.scatter(adata, 'n_genes', 'n_counts', color='louvain', palette=sns.color_palette('Set3')); ```. if works as expected. Also you can manually assign a color list as :. ```py; sc.pl.scatter(adata, 'n_genes', 'n_counts', color='louvain', palette=[; '#000000', '#575757', '#AD2323', '#2A4BD7', '#1D6914', ; '#814A19', '#8126C0', '#A0A0A0', '#81C57A', '#9DAFFF', ; '#29D0D0', '#FF9233', '#FFEE33', '#E9DEBB', '#FFCDF3', ; '#F2F3F4'; ]); ```. `palette=sc.pl.palettes.zeileis_28` works because `sc.pl.palettes.zeileis_28` is already a list of color. This also works for the `palete` argument in`sc.pl.umap()`, but it changes the `adata.uns['louvain_colors']` column values and will change other plots when using this column for plotting. Hope this helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1438#issuecomment-1641632885:324,message,message,324,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1438#issuecomment-1641632885,1,['message'],['message']
Integrability,"I don't think we should bother with numba, since it'll likely be a pretty core requirement once we can start transitioning to `pydata/sparse`. For `pyplot`, does `matplotlib` also take a while to import? Management of environment variables is a good reason not to defer that import. If we're already using `h5py`, could we drop `tables` as a requirement?. I think bad import times are only really noticeable for interactive use, since any script using scanpy will likely take longer to run. Do import times change depending on interactive environment? I wouldn't be surprised if different code ran when importing something like matplotlib in a notebook vs in a script.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/756#issuecomment-516404460:514,depend,depending,514,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/756#issuecomment-516404460,1,['depend'],['depending']
Integrability,I don't think we should have to worry about dealing with CI for R in this project. This only becomes harder if the intent is to have many other dependencies. I think doing anything like this makes much more sense in seperate dedicated package.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1271#issuecomment-666969144:144,depend,dependencies,144,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1271#issuecomment-666969144,1,['depend'],['dependencies']
Integrability,"I don't think we're going to get this implemented for sparse dataset per-se, but we have implemented this for dask arrays wrapping the sparse dataset in. * https://github.com/scverse/scanpy/pull/2856",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2764#issuecomment-2012543319:122,wrap,wrapping,122,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2764#issuecomment-2012543319,1,['wrap'],['wrapping']
Integrability,"I downloaded the github source archive at the 1.8.2 tag. The build process applies a few patches viewable [here](https://salsa.debian.org/med-team/python-scanpy/-/tree/master/debian/patches). One is a small change to some R code, and the other is I marked several more tests as needs internet because the Debian builds in an environment without network access and those ultimately tried to download something. (And it's really unclear if we can legally redistributed the 10x pbmc3k dataset.). The Debian build file is (here)[https://salsa.debian.org/med-team/python-scanpy/-/blob/master/debian/rules] though mostly it lets you see what tests I was skipping because of missing dependencies. Also if I set a color like in_tissue, or array_row the data shows up. I can paste the full build log if you'd like but this is the dependencies installed and the environment variables. . ```; Build-Origin: Debian; Build-Architecture: amd64; Build-Date: Sun, 14 Nov 2021 20:11:26 +0000; Build-Path: /<<PKGBUILDDIR>>; Installed-Build-Depends:; adduser (= 3.118),; adwaita-icon-theme (= 41.0-1),; autoconf (= 2.71-2),; automake (= 1:1.16.5-1),; autopoint (= 0.21-4),; autotools-dev (= 20180224.1+nmu1),; base-files (= 12),; base-passwd (= 3.5.52),; bash (= 5.1-3.1),; binutils (= 2.37-8),; binutils-common (= 2.37-8),; binutils-x86-64-linux-gnu (= 2.37-8),; blt (= 2.5.3+dfsg-4.1),; bsdextrautils (= 2.37.2-4),; bsdutils (= 1:2.37.2-4),; build-essential (= 12.9),; bzip2 (= 1.0.8-4),; ca-certificates (= 20211016),; coreutils (= 8.32-4.1),; cpp (= 4:11.2.0-2),; cpp-11 (= 11.2.0-10),; dash (= 0.5.11+git20210903+057cd650a4ed-3),; dbus (= 1.12.20-3),; dbus-bin (= 1.12.20-3),; dbus-daemon (= 1.12.20-3),; dbus-session-bus-common (= 1.12.20-3),; dbus-system-bus-common (= 1.12.20-3),; dbus-user-session (= 1.12.20-3),; dconf-gsettings-backend (= 0.40.0-2),; dconf-service (= 0.40.0-2),; debconf (= 1.5.79),; debhelper (= 13.5.2),; debianutils (= 5.5-1),; dh-autoreconf (= 20),; dh-python (= 5.20211105),; dh-strip-no",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616:676,depend,dependencies,676,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616,5,"['Depend', 'depend']","['Depends', 'dependencies']"
Integrability,"I feel like the `np.min(adata)` is more emblematic of the issue at hand here, which is how hard we should work to integrate with the rest of the python ML/data science ecosystem, e.g. `matplotlib.pyplot.scatter`. My personal view is nothing that works with a pandas DataFrame shouldn't work with an `AnnData` object; if you make it harder for people to work with AnnData than the most obvious competing data structure, they will simply use that other object.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-584219033:114,integrat,integrate,114,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-584219033,2,['integrat'],['integrate']
Integrability,"I figured the one-item-thing out: The emitted code is:. ```rst; :param copy: If an :class:`~anndata.AnnData` is passed, determines whether a copy; is returned.; :type copy: `bool`, optional (default: `False`). :returns: AnnData, None; Depending on `copy` returns or updates `adata` with the corrected data matrix.; ```. And since `:returns:` is part of a field list, and field lists are defined by the indentation of the *block starting in the second line*, the additional indentation of the second line is ignored. So yes, only numpydoc-style sections with one item are affected.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/610#issuecomment-484050694:235,Depend,Depending,235,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610#issuecomment-484050694,1,['Depend'],['Depending']
Integrability,I found that running the function 'tl.rank_genes_groups' gives the error the following error message:; UnboundLocalError: local variable 'adata_comp' referenced before assignment. ![scanpy api tl rank_genes_groups_error](https://user-images.githubusercontent.com/35155633/34642043-0191dce0-f305-11e7-847f-37b1ff34a77d.png),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/63:93,message,message,93,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/63,1,['message'],['message']
Integrability,"I get quite a strange scanpy error, which appears a bit stochastic... This is has happened for the first time in version 1.1. I am trying to get a scatter plot of a subsetted anndata object like this:; `p4 = sc.pl.scatter(adata[adata.obs['n_counts']<10000 ,:], 'n_counts', 'n_genes', color='mt_frac')`. When I do this the first time round, I get this error message about categorical variables from sanitize_anndata (none of which are actually used in the call). ```---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-66-fc1479c238f7> in <module>(); 9 plt.show(); 10 ; ---> 11 p4 = sc.pl.scatter(adata[adata.obs['n_counts']<10000 ,:], 'n_counts', 'n_genes', color='mt_frac'); 12 p5 = sc.pl.scatter(adata, 'n_counts', 'n_genes', color='mt_frac'); 13 . ~/scanpy/scanpy/plotting/anndata.py in scatter(adata, x, y, color, use_raw, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, right_margin, left_margin, size, title, show, save, ax); 162 show=show,; 163 save=save,; --> 164 ax=ax); 165 ; 166 elif x in adata.var_keys() and y in adata.var_keys() and color not in adata.obs_keys():. ~/scanpy/scanpy/plotting/anndata.py in _scatter_obs(adata, x, y, color, use_raw, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, right_margin, left_margin, size, title, show, save, ax); 281 ax=None):; 282 """"""See docstring of scatter.""""""; --> 283 sanitize_anndata(adata); 284 if legend_loc not in VALID_LEGENDLOCS:; 285 raise ValueError(. ~/scanpy/scanpy/utils.py in sanitize_anndata(adata); 481 # backwards compat... remove this in the future; 482 def sanitize_anndata(adata):; --> 483 adata._sanitize(); 484 ; 485 . ~/anndata/anndata/base.py in _sanitize(self); 1284 if len(c.categories) < len(c):; 1285 df[key] = c; -> 1286 df[key].cat.categories = df[key].cat.categories.ast",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/166:357,message,message,357,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/166,1,['message'],['message']
Integrability,"I got similar error when I was trying to use .h5 file from cellbender output. I have multiome data. . ```pytb; `>>> adata = scanpy.read_10x_h5(""/sc/arion/projects/hmDNAmap/snHeroin/analysis/ARC_TD005235-354/outs/cellbender/cb_feature_bc_matrix_filtered.h5"", gex_only=False)`; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/scanpy/readwrite.py"", line 183, in read_10x_h5; adata = _read_v3_10x_h5(filename, start=start); File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/scanpy/readwrite.py"", line 268, in _read_v3_10x_h5; _collect_datasets(dsets, f[""matrix""]); File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/scanpy/readwrite.py"", line 256, in _collect_datasets; dsets[k] = v[:]; File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper; File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper; File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/h5py/_hl/dataset.py"", line 738, in __getitem__; selection = sel2.select_read(fspace, args); File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/h5py/_hl/selections2.py"", line 101, in select_read; return ScalarReadSelection(fspace, args); File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/h5py/_hl/selections2.py"", line 86, in __init__; raise ValueError(""Illegal slicing argument for scalar dataspace""). > **ValueError: Illegal slicing argument for scalar dataspace**; ```. `>>> scanpy.logging.print_versions()`. anndata 0.8.0; scanpy 1.9.1. PIL 8.4.0; beta_ufunc NA; binom_ufunc NA; bottleneck 1.3.2; cffi 1.14.6; cloudpickle 2.0.0; colorama 0.4.4; concurrent NA; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2021.10.0; dateutil 2.8.2; defusedxml 0.7.1; encodings NA; fsspec 2021.08.1; genericpath NA; h5py 3.3.0; igraph 0.9.6; jinja2 2.11.3; joblib 1.1.0; kiwisolver 1.3.1; leidena",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2203#issuecomment-1129213572:900,wrap,wrapper,900,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2203#issuecomment-1129213572,1,['wrap'],['wrapper']
Integrability,I guess it has to do with the dependency on `scipy`. Downgrading to a previous setup of packages did the trick for me.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1407#issuecomment-690967045:30,depend,dependency,30,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407#issuecomment-690967045,1,['depend'],['dependency']
Integrability,"I had the exact same issue and error message at that step in the tutorial. I installed scanpy using pip, because installing with conda was not working.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1010#issuecomment-578570558:37,message,message,37,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1010#issuecomment-578570558,1,['message'],['message']
Integrability,"I have a similar issue to [this comment](https://github.com/theislab/scanpy/issues/1916#issuecomment-927497782). `Carraro=sc.read_10x_mtx('/mnt/Carraro',var_names='gene_ids')`. Switching to `gene_symbols` didn't work. Error messages:; ```; --> This might be very slow. Consider passing `cache=True`, which enables much faster reading from a cache file. ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); ~/miniconda3/envs/flng/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance); 3360 try:; -> 3361 return self._engine.get_loc(casted_key); 3362 except KeyError as err:. ~/miniconda3/envs/flng/lib/python3.8/site-packages/pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). ~/miniconda3/envs/flng/lib/python3.8/site-packages/pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.Int64HashTable.get_item(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.Int64HashTable.get_item(). KeyError: 1. The above exception was the direct cause of the following exception:. KeyError Traceback (most recent call last); /tmp/ipykernel_29519/245170133.py in <module>; ----> 1 Carraro=sc.read_10x_mtx('/mnt/Carraro',var_names='gene_ids'). ~/miniconda3/envs/flng/lib/python3.8/site-packages/scanpy/readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only); 452 genefile_exists = (path / 'genes.tsv').is_file(); 453 read = _read_legacy_10x_mtx if genefile_exists else _read_v3_10x_mtx; --> 454 adata = read(; 455 str(path),; 456 var_names=var_names,. ~/miniconda3/envs/flng/lib/python3.8/site-packages/scanpy/readwrite.py in _read_legacy_10x_mtx(path, var_names, make_unique, cache, cache_compression); 491 elif var_names == 'gene_ids':; 492 adata.var_names = genes[0].values; --> 493 adata.var['gene_symbols'] = genes[1].values; 494 else:; 495 raise V",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2053:224,message,messages,224,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2053,1,['message'],['messages']
Integrability,"I have an issue similar to this https://github.com/lmcinnes/pynndescent/issues/133. Code and Error message::; ```; sc.tl.ingest(bdata,; lungreference,obs='new_celltype'; ); ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); ~/miniconda3/envs/flng/lib/python3.8/site-packages/numba/core/caching.py in save(self, key, data); 486 # If key already exists, we will overwrite the file; --> 487 data_name = overloads[key]; 488 except KeyError:. KeyError: ((array(int32, 1d, C), array(int32, 1d, C), array(float32, 1d, C), array(float32, 2d, C), type(CPUDispatcher(<function squared_euclidean at 0x7fbafd5a19d0>)), array(int64, 1d, C), float64), ('x86_64-unknown-linux-gnu', 'skylake-avx512', '+64bit,+adx,+aes,+avx,+avx2,-avx512bf16,-avx512bitalg,+avx512bw,+avx512cd,+avx512dq,-avx512er,+avx512f,-avx512ifma,-avx512pf,-avx512vbmi,-avx512vbmi2,+avx512vl,-avx512vnni,-avx512vpopcntdq,+bmi,+bmi2,-cldemote,+clflushopt,+clwb,-clzero,+cmov,+cx16,+cx8,-enqcmd,+f16c,+fma,-fma4,+fsgsbase,+fxsr,-gfni,+invpcid,-lwp,+lzcnt,+mmx,+movbe,-movdir64b,-movdiri,-mwaitx,+pclmul,-pconfig,+pku,+popcnt,-prefetchwt1,+prfchw,-ptwrite,-rdpid,+rdrnd,+rdseed,+rtm,+sahf,-sgx,-sha,-shstk,+sse,+sse2,+sse3,+sse4.1,+sse4.2,-sse4a,+ssse3,-tbm,-vaes,-vpclmulqdq,-waitpkg,-wbnoinvd,-xop,+xsave,+xsavec,+xsaveopt,+xsaves'), ('447c56dc5e270e4f82ab71861b297ed6de3def7f442a5fd25f557203e9177f64', 'e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855')). During handling of the above exception, another exception occurred:. TypeError Traceback (most recent call last); /tmp/ipykernel_875/1088574315.py in <module>; 2 print(transgene); 3 bdata=adata[adata.obs.treatment==transgene]; ----> 4 sc.tl.ingest(bdata,; 5 lungreference,obs='new_celltype'; 6 ). ~/miniconda3/envs/flng/lib/python3.8/site-packages/scanpy/tools/_ingest.py in ingest(adata, adata_ref, obs, embedding_method, labeling_method, neighbors_key, inplace, **kwargs); 128 ; 129 for method in embedd",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2406:99,message,message,99,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2406,1,['message'],['message']
Integrability,"I have been trying but I am not sure how to do it in an appropriate way. Can you teach me how to do it? I am not super familiar with git stuff since; I am not really using it.; ᐧ. On Mon, Mar 13, 2023 at 5:10 AM Lukas Heumos ***@***.***>; wrote:. > A pull request that fixes this :); >; > See https://scanpy.readthedocs.io/en/stable/dev/index.html; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/scverse/scanpy/issues/2436#issuecomment-1465764371>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AFOKIW6MR6MSUCKJ2YTVG53W33P75ANCNFSM6AAAAAAVQOJ5FE>; > .; > You are receiving this because you authored the thread.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2436#issuecomment-1479764509:670,Message,Message,670,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2436#issuecomment-1479764509,1,['Message'],['Message']
Integrability,"I have several plotting functions that allow to compare any two categorical columns in `.obs` to achieve similar output but never found the time to integrate them into scanpy. Is really quite some effort to add proper tests, documentation and code standards. I will be happy to share the code if other people is willing to help. . One problem with the stacked bar plot is that with lot of samples it is difficult to compare the fractions. To solve this I had used the dot plot with good results, see for example a comparison of the `louvain` clusters and the `bulk labels` annotation from `sc.datasets.pbmc68k_reduced()`:. ![image](https://user-images.githubusercontent.com/4964309/104466204-3e718280-55b5-11eb-9b87-ac3860af7979.png). and . ![image](https://user-images.githubusercontent.com/4964309/104466234-49c4ae00-55b5-11eb-92c8-45140de9e107.png). The dot plot also computes enrichment with respect to random expectations and sorts the rows and columns.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1573#issuecomment-759496093:148,integrat,integrate,148,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573#issuecomment-759496093,1,['integrat'],['integrate']
Integrability,"I have the same issue than @brianpenghe .; I am trying to plot a heatmap using:; `sc.pl.heatmap(adata_10x_day13_MPCs_early, output_great_1090, groupby='condition', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0, dendrogram=True)`. n_cells = 1071; n_genes = 1121. I got this message as well:. WARNING: Gene labels are not shown when more than 50 genes are visualized. To show gene labels set `show_gene_labels=True`. /home/grupos_srs_ap/Programs/anaconda3/lib/python3.7/site-packages/scipy/interpolate/fitpack2.py:227: UserWarning: ; The maximal number of iterations maxit (set to 20 by the program); allowed for finding a smoothing spline with fp=s has been reached: s; too small.; There is an approximation returned but the corresponding weighted sum; of squared residuals does not satisfy the condition abs(fp-s)/s < tol.; warnings.warn(message). The heatmap was plotted after few minutes, though:; ![image](https://user-images.githubusercontent.com/63011975/78168487-675dc800-7426-11ea-9374-a6a4aab24506.png). I think the plot is not correct. I tried modifying the scanpy/plotting/_anndata.py, removing the smoothing function as described above, but got the same results. . Can you help me?; Thanks a lot in advance",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/633#issuecomment-607391574:302,message,message,302,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633#issuecomment-607391574,2,['message'],['message']
Integrability,"I have two suggestions/questions about dot/matrix plots:. 1- If `standard_scale='var'` is given, we can write `Mean expression\nin group\n(min-max scaled)` on the color legend to be more accurate about what is being displayed. 2- People/journals usually expect gene names to be written with italicized characters (don't ask why, see https://en.wikipedia.org/wiki/Gene_nomenclature). So I was wondering if we can simply do that in the plots. One important thing to consider is whether this is organism-dependent. I guess it's not but would be cool to discuss.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1913:501,depend,dependent,501,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1913,1,['depend'],['dependent']
Integrability,I included the notebook with the residuals above. I'll reattach it to this message:. [benchmarks_PR1066_residuals.ipynb.zip](https://github.com/theislab/scanpy/files/4242812/benchmarks_PR1066_residuals.ipynb.zip),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1066#issuecomment-590137515:75,message,message,75,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-590137515,1,['message'],['message']
Integrability,"I input pip show scipy I get:. Name: scipy; Version: 1.4.1; Summary: SciPy: Scientific Library for Python; Home-page: https://www.scipy.org; Author: None; Author-email: None; License: BSD; Location: /home/ubuntu/.local/lib/python3.6/site-packages; Requires: numpy; Required-by: umap-learn, statsmodels, scikit-learn, scanpy, xgboost, seaborn, mnnpy, loompy, Keras, Keras-Preprocessing, ggplot, gensim, anndata; You are using pip version 18.0, however version 20.2b1 is available.; You should consider upgrading via the 'pip install --upgrade pip' command. Typing in pip show scanpy returns:; Name: scanpy; Version: 1.5.1; Summary: Single-Cell Analysis in Python.; Home-page: http://github.com/theislab/scanpy; Author: Alex Wolf, Philipp Angerer, Fidel Ramirez, Isaac Virshup, Sergei Rybakov, Gokcen Eraslan, Tom White, Malte Luecken, Davide Cittaro, Tobias Callies, Marius Lange, Andrés R. Muñoz-Rojas; Author-email: f.alex.wolf@gmx.de, philipp.angerer@helmholtz-muenchen.de; License: BSD; Location: /home/ubuntu/.local/lib/python3.6/site-packages; Requires: packaging, h5py, joblib, legacy-api-wrap, tqdm, seaborn, setuptools-scm, statsmodels, numba, matplotlib, scipy, patsy, networkx, tables, natsort, pandas, umap-learn, scikit-learn, importlib-metadata, anndata; Required-by: ; You are using pip version 18.0, however version 20.2b1 is available.; You should consider upgrading via the 'pip install --upgrade pip' command. I have to use !pip install scanpy --user; when starting my session to have it work properly so I thought maybe it was an issue of being in a different directory but based on the location of each package when I look them up that doesn't appear to be the case? I tried using !pip install scipy -U --user but it tells me that the updated version is already present. sc.logging.print_versions() still shows scipy 1.0.1 as the version so I'm a bit confused. Is scanpy somehow defaulting to a different version for some reason? Is there a way to make it use the correct version?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1252#issuecomment-635681942:1100,wrap,wrap,1100,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1252#issuecomment-635681942,2,['wrap'],['wrap']
Integrability,"I just found a small mistake in the documentation of `scanorama_integrate`:; **kwargs are passed to assemble, not integrate. <!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2647:114,integrat,integrate,114,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2647,1,['integrat'],['integrate']
Integrability,I just noticed that matplotlib 3.1.1 is installed when using conda to install scanpy 1.4.4.; In your requirements you state that this breaks the scatter plot.; In the repodata from bioconda however it seems like there are very old dependencies that were not changed when new scanpy versions were included. So still the dependency there ist matplotlib>=2.2.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/876:231,depend,dependencies,231,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876,2,['depend'],"['dependencies', 'dependency']"
Integrability,"I just stumbled upon a similar bug using SciKit Learn. It's not ScanPy, but this issue is the only result Google returned when I looked up my error. Here's my crash log:. ```; Crashed Thread: 0 Dispatch queue: com.apple.main-thread. Exception Type: EXC_BAD_ACCESS (SIGSEGV); Exception Codes: KERN_INVALID_ADDRESS at 0x0000000000000110; Exception Note: EXC_CORPSE_NOTIFY. Termination Signal: Segmentation fault: 11; Termination Reason: Namespace SIGNAL, Code 0xb; Terminating Process: exc handler [0]. VM Regions Near 0x110:; --> ; __TEXT 000000010ddfb000-000000010ddfd000 [ 8K] r-x/rwx SM=COW /usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python. Application Specific Information:; crashed on child side of fork pre-exec. Thread 0 Crashed:: Dispatch queue: com.apple.main-thread; 0 libdispatch.dylib 	0x00007fff4fb578e1 _dispatch_root_queue_push + 108; 1 libBLAS.dylib 	0x00007fff24844c9a rowMajorTranspose + 546; 2 libBLAS.dylib 	0x00007fff24844a65 cblas_dgemv + 757; 3 multiarray.cpython-36m-darwin.so	0x00000001104e3f86 gemv + 182; 4 multiarray.cpython-36m-darwin.so	0x00000001104e3527 cblas_matrixproduct + 2807; 5 multiarray.cpython-36m-darwin.so	0x00000001104a9b27 PyArray_MatrixProduct2 + 215; 6 multiarray.cpython-36m-darwin.so	0x00000001104aeabf array_matrixproduct + 191; 7 org.python.python 	0x000000010de4712e _PyCFunction_FastCallDict + 463; 8 org.python.python 	0x000000010dead0e6 call_function + 491; 9 org.python.python 	0x000000010dea5621 _PyEval_EvalFrameDefault + 1659; 10 org.python.python 	0x000000010dead866 _PyEval_EvalCodeWithName + 1747; ```. It's not very useful as it's the same as the OP's, but it might help shifting the blame to a common dependency of SciKit Learn and ScanPy (like BLAS having an issue with macOS' Grand Central Dispatch).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/182#issuecomment-408848214:1732,depend,dependency,1732,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182#issuecomment-408848214,1,['depend'],['dependency']
Integrability,"I just tried; ```python; import scanpy.api as sc; sc.queries.mitochondrial_genes('www.ensembl.org', 'strange_organism'); ```; I would expect scanpy complains that it does not know `'strange_organism'`, but I get the error ; ```python; ---------------------------------------------------------------------------; NameError Traceback (most recent call last); <ipython-input-13-6a41b361ab41> in <module>(); 1 import scanpy.api as sc; ----> 2 sc.queries.mitochondrial_genes('www.ensembl.org', 'drerio'). ~/software/scanpy/scanpy/queries/__init__.py in mitochondrial_genes(host, org); 34 s.add_attribute_to_xml('mgi_symbol'); 35 else:; ---> 36 logg.msg('organism ', str(org), ' is unavailable', v=4, no_indent=True); 37 return None; 38 s.add_attribute_to_xml('chromosome_name'). NameError: name 'logg' is not defined; ```; It seems to me like `queries/__init__.py` misses an `from .. import logging as logg` statement. Would maybe also make sense to show the the message that an organism is not available at verbosity level 1 instead of 4?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/258:958,message,message,958,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/258,1,['message'],['message']
Integrability,"I just wanted to update that this issue does not depend on scvelo at all, but I can recreate it by just using scanpy. I suspect it is an issue with running umap. I'm using version '0.4.6'. Any help would be much appreciated:. ### Minimal code sample (that we can copy&paste without having any data). ```python; import os; import scanpy as sc; import numpy as np; import pandas as pd; import copy; import anndata; import matplotlib.pyplot as plt; adata_pbmc3k = sc.datasets.pbmc3k_processed(); #del adata_pbmc3k.obsm['X_pca']; #del adata_pbmc3k.obsm['X_umap']; del adata_pbmc3k.obsp['distances']; del adata_pbmc3k.obsp['connectivities']; #sc.pp.pca(adata_pbmc3k, n_comps=50); sc.pp.neighbors(adata_pbmc3k); ```. ```pytb; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); TypeError: expected dtype object, got 'numpy.dtype[float32]'. The above exception was the direct cause of the following exception:. SystemError Traceback (most recent call last); /hps/scratch/lsf_tmpdir/hl-codon-13-02/ipykernel_2124423/1009160698.py in <module>; ----> 1 sc.pp.neighbors(adata_pbmc3k). /hps/software/users/marioni/Leah/miniconda3/envs/scvelo/lib/python3.8/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy); 137 adata._init_as_actual(adata.copy()); 138 neighbors = Neighbors(adata); --> 139 neighbors.compute_neighbors(; 140 n_neighbors=n_neighbors,; 141 knn=knn,. /hps/software/users/marioni/Leah/miniconda3/envs/scvelo/lib/python3.8/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds); 806 # we need self._distances also for method == 'gauss' if we didn't; 807 # use dense distances; --> 808 self._distances, self._connectivities = _compute_connectivities_umap(; 809 knn_indices,; 810 knn_distances,. /hps/software/users/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1983#issuecomment-903666863:49,depend,depend,49,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1983#issuecomment-903666863,1,['depend'],['depend']
Integrability,"I kicked out `save_knn` from BBKNN as it doesn't really accomplish anything, and that ended up breaking the scanpy wrapper for it. Took it out, and took the opportunity to update the docstring.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/636:115,wrap,wrapper,115,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/636,1,['wrap'],['wrapper']
Integrability,"I know this (quite ancient) pull request has been open (#403), but I wasn't sure on its status. I think the consensus was to wait for sklearn to integrate the necessary changes? If that's still the case, then please feel free to remove this PR. Here I make use of scipy's extremely nifty [LinearOperator](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.LinearOperator.html) class to customize the dot product functions for an input sparse matrix. In this case, the 'custom' dot product performs implicit mean centering. In my benchmarks, performing implicit mean centering in this way does not affect the runtime whatsoever. However, this approach has to use [svds](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.svds.html), for which randomized SVD is not implemented. So we have to use 'arpack', which can be significantly slower (but not intractably so.... in my hands, I could still do PCA on datasets of 200k+ cells in minutes, and it sure beats densifying the data, if you want more thorough benchmarks I am happy to generate them!). The way I incorporated this functionality into scanpy/preprocessing/_simple.py might be questionable, and would love any suggestions or advice on how to better integrate this if there is interest in pushing this PR through. Let me know!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1066:145,integrat,integrate,145,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066,2,['integrat'],['integrate']
Integrability,"I like @flying-sheep's very last solution. To enable this for truly large-scale data and AnnData's that are backed on disk we need a much more efficient transposition implementation, which will probably need to return a view. That's problematic as it will break backwards compat (`.T` returns a copy these days). But it's good as it will allow adding fields to `.var`. @LuckyMD: At the time, when you mentioned that you wanted to plot over genes in scatter, I was fine with with having the scatter wrapper and assuming no ambiguity in obs and var keys. Now, I'd advocate for @flying-sheep's solution. Of course, we'll maintain the feature in `pl.scatter` when refactoring its code (a lot of it became redundant after fidel introduced the completely rewritten scatter plots).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/375#issuecomment-441473742:498,wrap,wrapper,498,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375#issuecomment-441473742,1,['wrap'],['wrapper']
Integrability,"I like Seurat's CCA. A pull request using `rpy2` similar to the R wrapper of Haghverdi et al.'s version of [MNN](https://github.com/theislab/scanpy/blob/master/scanpy/rtools/mnn_correct.py) would be welcome. Regarding ""plugins"": I guess a lot of Scanpy's functionality already consists in ""plugins"":; - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html; - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.dca.html; - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.magic.html; - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.phate.html; - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html. and a lot more are on the way, as far as I know. I guess the strategy of having an optional dependency of the respective and a small wrapper in Scanpy is a scalable strategy. Do you think we need to do more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/265#issuecomment-423784343:66,wrap,wrapper,66,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-423784343,3,"['depend', 'wrap']","['dependency', 'wrapper']"
Integrability,"I like the `calculateQCMetrics` function from [`scater`](https://bioconductor.org/packages/release/bioc/html/scater.html), and have had a half finished python version drifting between my notebooks for a while. Is there is interest in adding this to scanpy?. I'm aiming to mostly copy the interface of `calculateQCMetrics` while being memory efficient, since this is likely run before filtering. # Todo. - [x] Figure out how I want to deal with more types of sparse matrix; - [x] Add `feature_control` argument, possibly `variable_control`; - [x] Clean up and expand tests; - [x] Expand documentation. # Questions. * What's up with sparse matrix choice in scanpy? Is `adata.X` expected to be any of the matrix types, or is it more limited? Just trying to figure out how much effort I should put into that.; * Are there any additional metrics that could be useful?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/316:288,interface,interface,288,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316,1,['interface'],['interface']
Integrability,"I like the idea! Better error messages, and getting our modalities a bit under control is a great goal as well!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3048#issuecomment-2106927009:30,message,messages,30,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3048#issuecomment-2106927009,1,['message'],['messages']
Integrability,"I like this idea, but think it could be expanded on a bit. I think there are benefits to approaching this through logging. Some advantages of doing this through logging:. * Global record. If a copy is made or an AnnData split, you could figure out which object came from where.; * Control over level of detail. What kind of information is recorded can be customized. Maybe the user wants provenance, but maybe they want performance information. What if tracking was done through logging? Here's a couple quick examples of what I mean:. <details>. <summary>Simple example. Logs `anndata` used, function called, time elapsed </summary>. ```python; from anndata import AnnData; from datetime import datetime; from functools import wraps; from structlog import get_logger; from time import sleep; import uuid. logger = get_logger(). def logged(func):; @wraps(func); def func_wrapper(*args, **kwargs):; call_id = uuid.uuid4() # So we can always match call start with call end; call_start_record = dict(call_id=call_id, called_func=func.__name__); if type(args[0]) is AnnData:; call_start_record[""adata_id""] = id(args[0]); logger.msg(""call"", **call_start_record). t0 = datetime.now(); output = func(*args, **kwargs); dt = datetime.now() - t0. call_finish_record = dict(called_func=func.__name__, elapsed=dt); if type(output) is AnnData:; call_finish_record[""returned_adata_id""] = id(output); logger.msg(""call_finish"", **call_finish_record, call_id=call_id); return output; return func_wrapper. # Usage. @logged; def foo(adata, x, copy=False):; sleep(0.5); if copy: return adata.copy(). import scanpy as sc; pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1); # 2019-02-13 19:27.58 call adata_id=4937049368 call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo; # 2019-02-13 19:27.58 call_finish call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo elapsed=datetime.timedelta(microseconds=500777); foo(pbmcs, 1, copy=True);; # 2019-02-13 19:28.02 call adata_id=4937049368 cal",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/472#issuecomment-463117273:728,wrap,wraps,728,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472#issuecomment-463117273,2,['wrap'],['wraps']
Integrability,"I looked at it a while ago (for one test dataset, probably), and got the impression that `louvain` was faster. That said, they're both very fast. I would note that solutions from either can be pretty unstable, frequently depending on size of the community. @LuckyMD When you say heavy tailed, are you thinking of the unweighted KNN graph case or both?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/586#issuecomment-483207692:221,depend,depending,221,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586#issuecomment-483207692,1,['depend'],['depending']
Integrability,"I mean any smFISH or highly-multiplexed protein technology. The plot I have in mind is this:; This visualisation is implemented in our package (in active development - we haven't released yet): https://cell2location.readthedocs.io/en/latest/cell2location.plt.html#cell2location.plt.mapping_video.plot_spatial; ![download-20](https://user-images.githubusercontent.com/22567383/95405951-0ea94380-0911-11eb-84bf-6f712da7875c.png). I agree that the original images can be quite large so it is probably better to not load them by default. However, it is useful to have an option to load. For the Visium data, the utility of using fullres depends on image quality and the goals. Generally, cell diameter in highres images is just 1-4 pixels meaning that a cropped image with, say 10*10 spots will look pixelated and may not be enough to recognise small structures like a gland or a blood vessel, not mentioning cell morphologies or staining (e.g. eosinophils containing red granules).; For single-cell resolution data, it is often useful to zoom in to see if only cells of specific morphology express the gene, like Agt below.; ![download-19](https://user-images.githubusercontent.com/22567383/95405958-12d56100-0911-11eb-9a9b-3a2faa3fa660.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1436#issuecomment-705283276:633,depend,depends,633,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1436#issuecomment-705283276,1,['depend'],['depends']
Integrability,"I mean, @vtraag is is the person I’d believe when asked which algorithm is superior, so we could. 1. add `sc.tl.leiden` as an alternative that doesn’t have a flavour argument.; 2. make `leidenalg` a dependency and `louvain-igraph` an optional one.; 3. when calling `sc.tl.louvain` (no matter the flavor used), emit a ``DeprecationWarning('We recommend to use `sc.tool.leiden` instead. Refer to its documentation for details')``. This meets the following goals:. - education: people will learn why we recommend the new function; - ease of use: no weird errors pop up suddenly; - reproducibility: If `louvain-igraph` is installed, the code works exactly as before (with an added warning), else it crashes. we could do the following within `sc.tl.louvain` to help users:. ```py; try:; import louvain; except ImportError:; raise ImportError(; 'The package “louvain-igraph“ is not installed. '; 'Try using `sc.tl.leiden` in case you do not need '; 'to reproduce results produced using `sc.tl.louvain`'; ); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/350#issuecomment-437039831:199,depend,dependency,199,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350#issuecomment-437039831,2,['depend'],['dependency']
Integrability,"I never used spatial data (so far), are they organized as separate `AnnData` objects? If everything that could be integrated is a single `AnnData` then the function would be easy, like. ```python; def leiden_multiplex(adata: Sequence[AnnData], use_computed: bool = False, weights: None):. adj_list = [x.uns['neighbors']['connectivities'] for x in adata]; G_list = [sc._utils.get_igraph_from_adjacency(x) for x in adj_list] #also add the `restrict_to` step. if use_computed:; part_list = [get_partitions_from_adata.obs] or [recalculate_partitions_with_neighbors_params]; # then run the optimizer; else:; membership, improv = la.find_partitions_multiplex(**params). for a in adata:; a.obs['multiplex'] = pd.Categorical(membership). ```; where `adata` is a list of `AnnData` objects, `use_computed` switches between recalculate partitions (`False`) or optimize partitions already calculated (`True`). Weights can be specified to give more or less importance to a specific view. Note that, by default, if set to `None` it is set to a list of ones by `leidenalg`.; Other options, in addition to the usual `copy = False` should be the `leidenalg` type of partitioning (`CPMVertexPartition`, `RBConfigurationVertexPartition`...)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1107#issuecomment-600076328:114,integrat,integrated,114,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1107#issuecomment-600076328,1,['integrat'],['integrated']
Integrability,"I only can advice you on your second part of questions there is no rule of thumb for that. I also don't know what do you exactly mean by best suggestion resolution and how did you assess that. This is a general problem for many supervised clustering methods such as k-mean that user has to provide number of clusters or in this case the resolution which determines the number of clusters. Although there are some indirect ways to assess the clustering quality for example silhouette coefficient which gives you a score between -1 to 1 that tell you how similar your point in each clusters are. The other possibility is that you already expect the number of clusters so you can optimize the resolution based on your previous knowledge. ; @falexwolf Out of curiosity, can we integrate such methods like silhouette coefficient inside scanpy? that would be cool!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/670#issuecomment-498046271:773,integrat,integrate,773,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670#issuecomment-498046271,1,['integrat'],['integrate']
Integrability,"I only just now got the distinction between types and classes in python. So when they talk about “types”, they mean stuff in the typing module, got it. So. - Types (`typing.*`), ABCs, and regular classes can be used for type annotation; - ABCs and regular classes can be used for `isinstance` and `issubclass` checking; - ABCs and mixins can be mixed in to enhance a class you defined. where a mixin is simply a regular class that happens to rely on some properties of the class it can be mixed with, and a regular class being any class that’s not a type or an ABC. - `collections.abc.Mapping` is an ABC and can be mixed in to enhance your basic mapping class with some convenience methods, or used to check if something has the basic mapping protocol (no matter if it was mixed in or not). What’s the basic protocol and what will be mixed in is [nicely documented](https://docs.python.org/3/library/collections.abc.html).; - `typing.Mapping` is a generic type, to be used in annotations only. There’s a few projects implementing type checking using them, e.g. mypy or typecheck-decorator. Check out the [docs for abstract base classes](https://docs.python.org/3/library/abc.html), they explain how ABCs work. (namely by `register`ing virtual subclasses and/or implementing `__subclasshook__`). Mixin example:. ```py; class EnumerableMixin:; """"""silly mixin class for iterables""""""; def enumerate(self, start=0):; yield from enumerate(self, start). class EnumerableList(list, EnumerableMixin):; pass. for i, e in EnumerableList.enumerate(): print(i, e); ```. ABC example:. ```py; class PositiveNumbers(collections.abc.Set):; def __contains__(self, i):; return isinstance(i, int) and i >= 0; def __iter__(self): return itertools.count(); def __len__(self): return float('inf'). # __lt__ is mixed in!; print({0, 1, 10_000} < PositiveNumbers()). # `set` doesn’t inherit from collections.abc.Set, the __subclasshook__ does its magic here; isinstance({}, collections.abc.Set); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-445181839:743,protocol,protocol,743,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-445181839,4,['protocol'],['protocol']
Integrability,"I run into this bug too.; when i run:; ```; import scanpy as sc; import anndata as ad; adata = sc.read(filepath); ```; it turns out:; ```; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); Cell In[6], [line 8](vscode-notebook-cell:?execution_count=6&line=8); [6](vscode-notebook-cell:?execution_count=6&line=6) if filename.endswith('.h5ad'):; [7](vscode-notebook-cell:?execution_count=6&line=7) filepath = os.path.join(directory, filename); ----> [8](vscode-notebook-cell:?execution_count=6&line=8) adata = sc.read(filepath); [10](vscode-notebook-cell:?execution_count=6&line=10) # Rename columns with periods in `.obs` attribute; [11](vscode-notebook-cell:?execution_count=6&line=11) for col in adata.obs.columns:. File /data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); [77](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:77) @wraps(fn); [78](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:78) def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; [79](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:79) if len(args_all) <= n_positional:; ---> [80](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80) return fn(*args_all, **kw); [82](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args; [83](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/323#issuecomment-2041512845:893,wrap,wrapper,893,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323#issuecomment-2041512845,1,['wrap'],['wrapper']
Integrability,I second the suggestion by @falexwolf to rename the function to something simpler but also to keep the previous functionality with a Deprecate message as suggested by @LuckyMD. @Koncopd The changes also requires adapting the corresponding `sc.pl.rank_genes_groups*` functions. I can take over that once the PR is ready.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1156#issuecomment-627433020:143,message,message,143,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156#issuecomment-627433020,2,['message'],['message']
Integrability,"I second this initiative. I had used the code from Brent and works quite; well. Naturally, having it integrated into Scanpy would be great. On Mon, Dec 17, 2018 at 2:18 PM Marius Lange <notifications@github.com>; wrote:. > *@Marius1311* commented on this pull request.; > ------------------------------; >; > In scanpy/preprocessing/combat.py; > <https://github.com/theislab/scanpy/pull/398#discussion_r242142511>:; >; > > @@ -0,0 +1,161 @@; > +import numpy as np; > +from scipy.sparse import issparse; > +import pandas as pd; > +import sys; > +from numpy import linalg as la; > +import patsy; > +; > +def design_mat(mod, batch_levels):; > + # require levels to make sure they are in the same order as we use in the; > + # rest of the script.; > + design = patsy.dmatrix(""~ 0 + C(batch, levels=%s)"" % str(batch_levels),; >; > thanks, did that!; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/pull/398#discussion_r242142511>, or mute; > the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1fZSO-j8m0NwemluQp-0wNEGDHJ9ks5u55mlgaJpZM4ZTmeq>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/398#issuecomment-447896676:101,integrat,integrated,101,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398#issuecomment-447896676,1,['integrat'],['integrated']
Integrability,I skimmed through the code only briefly yesterday but I think the problem might be that both `x` and `y` need to be specified: The error message seems to indicate so and the two lines seemingly responsible for the problem are the only ones where `x` and `y` are **not** both specified. I'll have a closer look at it in a bit.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1420#issuecomment-694079984:137,message,message,137,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1420#issuecomment-694079984,1,['message'],['message']
Integrability,"I started doing feature (gene) selection on scSeq data as well. I also observe dependency between Moran's I and gene expression sparsity. I was thinking that maybe I could regress out the expression effect or select genes per bin.; ![image](https://user-images.githubusercontent.com/47607471/112271293-0c4a6400-8c7b-11eb-953e-a7fcec362401.png); And this was the problem with p-values in squdpy-s Moran's I (plot for first 100 genes in my adata, using 100 permutations - but this should not really lead to low pvalues as I think that the reported pvalues are estimated based on null distn shape - not 100% sure though).; ![image](https://user-images.githubusercontent.com/47607471/112271483-461b6a80-8c7b-11eb-826d-0e93d708088b.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1698#issuecomment-805574439:79,depend,dependency,79,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698#issuecomment-805574439,1,['depend'],['dependency']
Integrability,"I still don't see 100% why the submodule `rtools` would be so much worse than `scanpy-contrib`. The submodule would be separate from the rest of the package and we could write something on top of its API overview page like: interfaces for R tools, address the maintainers of these tools for help... We need to keep some structure so that things remain clean, but yeah, additions will always be somewhat arbitrary. If someone suggests a ""meaningful addition"", we will accept it, if someone suggests something that does not seem to be of good quality, we will reject it. But this is not a problem only for `rtools` but for wrappers of python packages as well... see, for instance, https://github.com/theislab/scanpy/pull/126. So, I would set up the `rtools` submodule to save us the work of maintaining a different repo and the user the work of installing a `scanpy-contrib` package and figuring out which namespaces to use so that notebooks don't get completely messed up. So, if you don't mind, I'd set up the ""rtools"" submodule...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/125#issuecomment-382344299:224,interface,interfaces,224,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-382344299,2,"['interface', 'wrap']","['interfaces', 'wrappers']"
Integrability,"I sympathize with the problem which I tend to solve by plotting individual scatterplots, each one with its specific vmax. But I will be happy to have a better output. I like the idea of using quantile but I would avoid an increasing list of parameters. Thus @ivirshup suggestion to use `functools.partial` seems better. I like the flexibility it provides and I think we should implement it, but I don't know if this would be difficult to document and explain to the user that just would like to compute the quantile. An idea would be to allow some encoding for vmax as for example `vmax='q99'` which would be interpreted as np.quantile. My suggestion is to . - add vectorized vmax and vmin; - each entry of vmax or vmin would be interpreted depending on the data type. Besides a number, if it is a string then it is interpreted as for example quantile if it starts with 'q' or as a function if the type is `partial`. The following options would then be valid:. ```PYTHON; sc.pl.{scatterfunc}(adata, color=[""gene1"", ""gene2""], vmax=[4., 3.]); sc.pl.{scatterfunc}(adata, color=[""gene1"", ""gene2""], vmax=['q80', 'q90']). from functools import partial; sc.pl.{scatterfunc}(adata, color=[""gene1"", ""gene2""], vmax=[partial(np.mean), partial(np.median)]). # combination; sc.pl.{scatterfunc}(adata, color=[""gene1"", ""gene2"", ""gene3""], ; vmax=[4., 'q85', partial(np.percentile, q=90]); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/775#issuecomment-521550044:741,depend,depending,741,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/775#issuecomment-521550044,1,['depend'],['depending']
Integrability,I think I would rather just bump the h5py dependency since it's been over a year.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2064#issuecomment-1013003401:42,depend,dependency,42,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2064#issuecomment-1013003401,1,['depend'],['dependency']
Integrability,"I think `obs_values` is fine. But maybe, `aggregate_obs` is even better, as this describes what it does (aggregating annotations of observations with partial (projections of) observations). It's no problem at all to make the next Scanpy release depend on the current AnnData release, both in the requirements and the minimal version check upon importing Scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/619#issuecomment-487916208:245,depend,depend,245,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/619#issuecomment-487916208,1,['depend'],['depend']
Integrability,"I think if you add `show=False` to the `sc.pl.umap` commands it might work? . Depending on the type grid and subplots, it could be easier to plot things manually with matplotlib (to have more control over things like legend placement etc)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1802#issuecomment-824423022:78,Depend,Depending,78,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1802#issuecomment-824423022,1,['Depend'],['Depending']
Integrability,"I think it shows us some strategy, yes. I can't claim to understand what each line you did there does but I have a notebook open now and am trying to step through it. I think part of the issue here is we're wanting to store aggregate values like means within adata so that they don't have to be recomputed every time they are needed. Web interfaces are being driven off these files, so rapid access is preferred over storage concerns. . If, in the example of datasets with technical replicates, we almost always are only interested in the mean values across replicates. So if our input is like this:. ![screenshot from 2018-04-11 15-11-07](https://user-images.githubusercontent.com/330899/38640690-9b7c41a8-3d9a-11e8-9b40-b9763a3df422.png). Simple demo with 4 genes, 2 conditions, and 3 replicates of each condition. Does one make this exact matrix adata.X? . ![screenshot from 2018-04-11 15-16-11](https://user-images.githubusercontent.com/330899/38640934-4a25562c-3d9b-11e8-82eb-eb7811463fa4.png). Or should the means be calculated and stored as adata.X with individual replicates stored as separate matrices of the same size, like adata.uns['rep1'], adata.uns['rep2'], ... etc. This is specifically the part I was asking about regarding conventions, as it seems there must already be a convention for storing technical replicates and their aggregate values (mean, stddev, etc.). I think the examples given with values like 0 and 1 are a bit confusing because I can't tell if you're using that as a proxy for column names or are they index positions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/106#issuecomment-380582298:338,interface,interfaces,338,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/106#issuecomment-380582298,1,['interface'],['interfaces']
Integrability,I think it’s again some flakiness of some dependency (or our interaction with it),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/661#issuecomment-495239336:42,depend,dependency,42,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661#issuecomment-495239336,1,['depend'],['dependency']
Integrability,"I think it’s pretty impossible to know if they’ll render – Depends on the fonts available on the system and the way the font rendering stack falls back to other fonts. My approach would be to check which systems have the problem, and if it’s only some Linux server or some obsolete stuff like e.g. Windows versions up to Vista, ignore it. If it’s a commonly used and still supported desktop OS / Linux distribution, we have to deal with it. The reason I excluded Linux servers is that server admins often set up things minimalistically, excluding “GUI stuff” so trying to support those highly heterogenous systems will only bring pain. When people want better fonts, then fontconfig is happy to provide them with the means to do so.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/805#issuecomment-528244541:59,Depend,Depends,59,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/805#issuecomment-528244541,1,['Depend'],['Depends']
Integrability,"I think the `<` should work, it's part of [PEP-508](https://www.python.org/dev/peps/pep-0508/#motivation). At least that's what [this blog post](https://hynek.me/articles/conditional-python-dependencies/) says. Didn't those builds just fail before they got to that requirement?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/704#issuecomment-505432318:190,depend,dependencies,190,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/704#issuecomment-505432318,1,['depend'],['dependencies']
Integrability,"I think the current approach - a very simple interface as in `scanpy/tools/phate.py` and a bunch of others is the easiest way to go for the developer. So, I'd say we make a submodule `.ext` with the `.tools`, `.plotting`, `.preprocessing` substructure in it. We move things like `phate.py` into `scanpy/ext/tools`. We maintain backwards compat by still reexporting it in `scanpy.api`. The canonical way of calling these extension will be by importing `import scanpy.ext as sce` and people can use that extension namespace and call everything in the same way that they are used to. Users can look up extension tools on docs site like [this](https://scanpy.readthedocs.io/en/latest/api/index.html). It will also be clear to users that these extensions will require installing additional packages, which don't come with the default scanpy. Of course, all of this needs none of the ""extension mechanisms"" mentioned above. But people really don't want to write actual ""scanpy extensions""; they want to write their own packages and have them interface with scanpy so that convenient calls are enabled without the need to adapt to new conventions. For the scanpy users, the cool things is that a large number of tools can be quickly tested out. If you don't mind, @fidelram and @flying-sheep, @Koncopd would go along and make this modest change.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/271#issuecomment-431634492:45,interface,interface,45,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271#issuecomment-431634492,4,['interface'],['interface']
Integrability,"I think the issue here is that BBKNN only generates an integrated graph, while the tsne computation creates a new graph from some matrix representation of the data. There has been the suggestion of allowing a tsne layout (#1233) to be generated from a precomputed connectivity matrix, but that hasn't been implemented here yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1370#issuecomment-678131279:55,integrat,integrated,55,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1370#issuecomment-678131279,1,['integrat'],['integrated']
Integrability,"I think the level of threading can be dependent on BLAS/ LAPACK etc. However, it should generally be multithreaded. Does the issue persist in a conda environment?. If you still run into issues I'd be interested in seeing some details (e.g. what size and kind of matrix, cpu usage during computation). A counter example of a faster way to compute it could be useful to see too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1865#issuecomment-861467621:38,depend,dependent,38,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1865#issuecomment-861467621,1,['depend'],['dependent']
Integrability,"I think this has to do with us relying on UMAP. You can check this yourself in UMAP, but you'll actually end up with n-1 neighbors per node. I believe this has to do with each point being it's own nearest neighbor, but I forget if that's important for nearest neighbor descent algorithm (prevent node from adding itself by already having it in the heap) or UMAP (simplexes??). If I can find a link to where I read this, I'll share it here. Two considerations:. * This is the behaviour of UMAP, which we are fairly integrated with; * This has always been the behavior. I was definitely surprised when I read about this recently, and would be open to changing the behavior. It would effect reproducibility for everyone though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1706#issuecomment-788812203:514,integrat,integrated,514,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1706#issuecomment-788812203,2,['integrat'],['integrated']
Integrability,"I think this may be already implemented in https://scanpy.readthedocs.io/en/stable/generated/scanpy.tl.ingest.html, however, this function contains extra integration and label transfer steps that are not needed for all applications. It would be great if this could be disentangled to make the umap transform available as a separate function on scanpy umaps. Also, it seems that this function does not use scanpy umap to calculate umap so changes may be needed in how scanpy umap is currently calculated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2259#issuecomment-1237340704:154,integrat,integration,154,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2259#issuecomment-1237340704,1,['integrat'],['integration']
Integrability,"I think this more of an enhancement than a bug, though an error message saying we don't have a way to color by boolean values would be more clear. What would you expect this to look like? Which styling options apply here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1646#issuecomment-777891088:64,message,message,64,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646#issuecomment-777891088,2,['message'],['message']
Integrability,"I think to preserve the signature like that we could easily do a subset of what `update_wrapper` (and therefore `wraps`) does:. ```py; def wraps_plot_scatter(wrapper):; wrapper.__annotations__ = {; k: v for k, v in plot_scatter.__annotations__.items(); if k != 'basis'; }; wrapper.__wrapped__ = plot_scatter; return wrapper. @wraps_plot_scatter; def umap(adata, **kwargs):; """"""...""""""; return plot_scatter(adata, basis='umap', **kwargs); ```. but first: what did you try and why did it fail?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/535#issuecomment-473907861:113,wrap,wraps,113,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/535#issuecomment-473907861,5,['wrap'],"['wrapper', 'wraps']"
Integrability,"I think we sorta already did this:. ```python; sc.tl.leiden(adata); /mnt/workspace/repos/scanpy/scanpy/tools/_leiden.py:144: FutureWarning: Use of leidenalg is discouraged and will be deprecated in the future. Please use `flavor=""igraph""` `n_iterations=2` to achieve similar results. `directed` must also be `False` to work with `igraph`'s implementation.; warnings.warn(msg, FutureWarning); ```. However, I do think the warning could be reworked. - [ ] Stack level should be set so the right line is shown; - [ ] Message should be more like:. > FutureWarning: In the future, the default backend for `leiden` will be `igraph` instead of `leidenalg`. To achieve the future defaults please pass: `flavor=""igraph""` and `n_iterations=2`. `directed` must also be `False` to work with `igraph`'s implementation.; >; > Note that results will be slightly different with the changed backend. - [ ] We should only warn once, instead of every time leiden is called. Here's a util we defined in anndata for this (I think stolen from pandas?). ```python; def warn_once(msg: str, category: type[Warning], stacklevel: int = 1):; warnings.warn(msg, category, stacklevel=stacklevel); # Prevent from showing up every time an awkward array is used; # You'd think `'once'` works, but it doesn't at the repl and in notebooks; warnings.filterwarnings(""ignore"", category=category, message=re.escape(msg)); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2935#issuecomment-2015559682:514,Message,Message,514,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2935#issuecomment-2015559682,2,"['Message', 'message']","['Message', 'message']"
Integrability,"I tried to collect in one file the code used for plotting functions that use matplotlib scatter like `sc.pl.tsne`, `sc.pl.pca` and `sc.pl.umap` and others. Also, I tried to annotate the code and improve the readability. . Currently, the code is on a separate file called `scatter.py` and not integrated into the API as this facilitates comparison with previous code. . Besides readability the proposed code can:; * Plot a large number of plots in multiple columms (instead of a long row of plots); * Pass arguments directly to `matplotlib.pyplot.scatter` like vmax and vmin to adjust the color scale. When plotting multiple plots, this is useful to have a consistent range of values). See cells 15 and 15 in this example: https://gist.github.com/fidelram/8b43f786e7519bcfb7ffc0d5ccdbb0fe ; If the admins would like to merge these changes I can replaced the previous functions. An example on how to use the code:. ```python; import scanpy.plotting.tools.scatter as spl; spl.tsne(adata, color='louvain'); ```. ![image](https://user-images.githubusercontent.com/4964309/44652273-c908b580-a9eb-11e8-86fa-aa1b55fa9b0a.png). Further examples [here](https://gist.github.com/fidelram/8b43f786e7519bcfb7ffc0d5ccdbb0fe)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/244:292,integrat,integrated,292,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244,1,['integrat'],['integrated']
Integrability,I tried to install louvain through conda; `conda install -c vtraag louvain`; but got error message:; Solving environment: failed. PackagesNotFoundError: The following packages are not available from current channels:. - louvain; - python-igraph[version='>=0.7.1.0']. However I could install it by; `conda install -c conda-forge louvain`. Can you please update it on the webpage. Thanks!; [https://scanpy.readthedocs.io/en/latest/installation.html](url),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/143:91,message,message,91,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/143,1,['message'],['message']
Integrability,"I tried to set `var_names` from gene_symbols, and I get a warning message:; `Variable names are not unique. To make them unique, call `.var_names_make_unique`.`. In calling `adata.var_names_make_unique()` I get the error:; `TypeError: unsupported operand type(s) for +: 'float' and 'str'`. I can ignore this and take it through most of the analysis and am able to make the plots and rank the genes by name, however, I am unable to save. Calling `adata.write('./write/adata.h5ad')` gives the following error:. ```; File ""pandas/_libs/src/inference.pyx"", line 1472, in pandas._libs.lib.map_infer. TypeError: object of type 'float' has no len(); ```. Also, the clustering is slightly different, I'm guessing from not having unique gene names. I've looked through the documentation for `sc.pl.rank_genes_groups_*` and cannot figure out how to keep the index as the Ensembl gene ID and just use gene_symbols to call the plots (`sc.pl.violin`, etc.) and use the `sc.tl.rank_genes_groups`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/455#issuecomment-473778184:66,message,message,66,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455#issuecomment-473778184,1,['message'],['message']
Integrability,I uninstalled umap and made sure umap-learn was installed but it did not change anything. . I would guess that the problem comes from modules dependency as I managed to make it work on pycharm.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1978#issuecomment-898539219:142,depend,dependency,142,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1978#issuecomment-898539219,2,['depend'],['dependency']
Integrability,"I used the following code once in a time . however , no visible different could be found in the result (`sc.pl.umap(adata, color='batch')`). ```; sc.pp.combat(adata). sce.pp.bbknn(adata, batch_key='batch'). from itertools import cycle; sce.pp.mnn_correct(adata, var_index=None, var_subset=None, batch_key='batch', index_unique='-', batch_categories=None, k=20, sigma=1.0, cos_norm_in=True, cos_norm_out=True, svd_dim=None, var_adj=True, compute_angle=False, mnn_order=None, svd_mode='rsvd', do_concatenate=True, save_raw=False, n_jobs=None); #OR; import mnnpy; mnnpy.mnn_correct(adata). sce.pp.magic(adata). sce.tl.phate(adata); ```. Here，I attach the Integrate result from seurat based on same data. we can that most of the sample 001, 002，and 009 were grouped together on left , most part of sample 003 was on the right. ; ![20191016EVE_UMAP_Integrate](https://user-images.githubusercontent.com/49429496/66911577-5f11fc00-f043-11e9-8be2-742a4ffaa7a3.png). or this way (split by batch); ![20191016EVE_UMAP_Integrate_SplitByPatient](https://user-images.githubusercontent.com/49429496/66911623-79e47080-f043-11e9-8d5d-7769437831da.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/873#issuecomment-542637756:652,Integrat,Integrate,652,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873#issuecomment-542637756,1,['Integrat'],['Integrate']
Integrability,"I want to split AnnData after tl.diffmap according to each cell's library. But it appears that row-slicing AnnData after diffmap, dpt, or louvain gives the error message `AttributeError: 'AnnData' object has no attribute '_n_obs'`. But AnnData.X and AnnData.obs can be sliced. Could you please give me advice?. ```py; >>> adata = sc.read_10x_h5('filtered_gene_bc_matrices_h5.h5', 'mm10'); >>> scanpy.api.tl.diffmap(adata); >>> adata_diffmap[:, 0]; View of AnnData object with n_obs × n_vars = 5000 × 1; >>> adata_diffmap[0, :] ; AttributeError: 'AnnData' object has no attribute '_n_obs'; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/62:162,message,message,162,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/62,1,['message'],['message']
Integrability,"I want to start off by mentioning how much I love scanpy. I recommend this package to everyone I know who is doing single cell sequencing. I love how PAGA and UMAP can be integrated together. PAGA makes appreciation of data topology so simple. In addition, it's so easy to do velocity analysis with the scvelo integration. Installing scanpy as well as hdf5/loom compatibility is remarkably easier on python than in R, which gives scanpy users an obvious advantage. . I've learned so much using this package and it has allowed me to display my data in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to access the current version of this software? . I t",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/510:171,integrat,integrated,171,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510,2,['integrat'],"['integrated', 'integration']"
Integrability,"I was also investigating how `leiden` got `use_weights=True` by default, and noticed the lack of discussion. Seems like it just sorta happened when `leiden` got added #361?. I think it'd be pretty different from clustering on the embedding, because the embedding has constraints based on things like minimum distance two points can be from each other, and the number of dimensions it's embedded in. On the binarized KNN-graph, I think we've actually talked about this before (#240). I personally think using a weighted graph makes more sense. For example, say you have a cell type of which occurs 15 times in your dataset, but you've set k to 30. With a binarized graph there will be a less clear signal that this is a distinct cell-type. From a slightly more empirical/ anecdotal perspective, on a couple datasets I tested, total degree of the generated graph was sub-linear (looked log-ish) w.r.t. `k` for the weighted umap graph. Here's using one of the bone marrow donors from the hca immune census (y-axis is log scaled so you can still see the total weighted degree increase):. ![image](https://user-images.githubusercontent.com/8238804/56469005-400d2580-6477-11e9-98f1-b9dfe70bd1d7.png). To me, this suggested a stable representation of the dataset was being found. As a connected point, in my experience clustering results seems fairly robust to `k` for weighted graphs above a low threshold (I think dataset dependent, but 30-60 range). Using an unweighted graph, there is a much stronger dependence on `k` and some smaller clusters seem less stable (show up in a smaller proportion of clustering solutions from a parameter space).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/586#issuecomment-485242638:1417,depend,dependent,1417,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586#issuecomment-485242638,4,['depend'],"['dependence', 'dependent']"
Integrability,"I was just wondering if you guys have any plans for scATAC data analysis pipelines and especially integration with scRNA-seq? . cheers,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/725:98,integrat,integration,98,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/725,1,['integrat'],['integration']
Integrability,"I was running this code `sc.pl.umap(adata, color = ['KIR3DL1'], frameon = False, layer = 'scvi_normalized')` to create umap but it gave me an error message with empty heatmap color bar legend. ![스크린샷 2022-09-02 오전 9 34 00](https://user-images.githubusercontent.com/64761042/188034720-20eacca2-efa0-4d6c-9e7f-0543f85d1cd7.png). ![스크린샷 2022-09-01 오후 6 58 54](https://user-images.githubusercontent.com/64761042/188034773-c32610cc-60a3-4c17-a000-8296f013b3e7.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2318:148,message,message,148,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2318,1,['message'],['message']
Integrability,"I was thinking we could go a bit further. We could add `sinfo` as a dependency and make `print_versions` just call: `sinfo.sinfo(dependencies=True)` which will always be comprehensive. <details>; <summary> Example output: </summary>. ```; -----; IPython 7.16.1; scanpy 1.5.2.dev38+g6728bdab; sinfo 0.3.1; -----; IPython 7.16.1; PIL 7.2.0; anndata 0.7.5.dev0+g58886f0.d20200729; asciitree NA; backcall 0.2.0; cffi 1.14.0; cloudpickle 1.5.0; colorama 0.4.3; cycler 0.10.0; cython_runtime NA; dask 2.21.0; dateutil 2.8.1; decorator 4.4.2; fasteners NA; get_version 2.1; google NA; h5py 2.10.0; igraph 0.8.2; ipython_genutils 0.2.0; jedi 0.17.2; joblib 0.16.0; kiwisolver 1.2.0; legacy_api_wrap 1.2; leidenalg 0.8.1; llvmlite 0.33.0; louvain 0.7.0; matplotlib 3.3.0; monotonic NA; mpl_toolkits NA; msgpack 1.0.0; natsort 7.0.1; numba 0.50.1; numcodecs 0.6.4; numexpr 2.7.1; numpy 1.19.0; packaging 20.4; pandas 1.0.5; parso 0.7.0; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.5; psutil 5.7.0; ptyprocess 0.6.0; pygments 2.6.1; pyparsing 2.4.7; pytz 2020.1; scanpy 1.5.2.dev38+g6728bdab; scipy 1.5.1; sinfo 0.3.1; sitecustomize NA; six 1.15.0; sklearn 0.23.1; sphinxcontrib NA; storemagic NA; tables 3.6.1; tblib 1.6.0; texttable 1.6.2; tlz 0.10.0; toolz 0.10.0; traitlets 4.3.3; typing_extensions NA; wcwidth 0.2.5; yaml 5.3.1; zarr 2.4.0; -----; Python 3.8.5 (default, Jul 23 2020, 15:50:11) [Clang 11.0.3 (clang-1103.0.32.62)]; macOS-10.15.6-x86_64-i386-64bit; 16 logical CPU cores, i386; -----; Session information updated at 2020-07-30 19:28; ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1343#issuecomment-666257831:68,depend,dependency,68,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1343#issuecomment-666257831,2,['depend'],"['dependencies', 'dependency']"
Integrability,"I was trying some integration methods between the two pbmc datasets. Maybe, could you add a sc.datasets.pbmc68k_full() where the whole transcriptome is included?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1762#issuecomment-808189078:18,integrat,integration,18,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1762#issuecomment-808189078,1,['integrat'],['integration']
Integrability,"I was trying to plot a heatmap using this command:; `ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident',; use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,; var_group_rotation=0, dendrogram=True, save='ClusterMap.png')`. And it didn't finish running after an overnight, with the following warning message:; WARNING: Gene labels are not shown when more than 50 genes are visualized. To show gene labels set `show_gene_labels=True`; /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227: UserWarning: ; The maximal number of iterations maxit (set to 20 by the program); allowed for finding a smoothing spline with fp=s has been reached: s; too small.; There is an approximation returned but the corresponding weighted sum; of squared residuals does not satisfy the condition abs(fp-s)/s < tol.; warnings.warn(message). I don't understand why this is taking this long because seaborn was able to finish plotting within 30 minutes. Do you know why?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/633:319,message,message,319,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633,2,['message'],['message']
Integrability,"I was trying to reproduce the results in Example 1 on notebook; https://github.com/theislab/scanpy_usage/tree/master/170505_seurat. I'm getting two problems in the filtering steps in cell 9:; 1) although genes seem to be filtered (there are 1838 genes left versus 13714 before), the plot does not show a different colour for 'highly variable' and 'other' genes. Both appear black (see attached figure). I've both tried it in a jupyter notebook and ipython. I'm running python in a conda environment with matplotlib 4.3.2.25.py35_0 and seaborn 0.8_py35. 2) There's also the following warning message, that seems to complain of a divide by zero on the mean:; /anaconda/lib/python3.5/site-packages/scanpy/preprocessing/simple.py:193: RuntimeWarning: invalid value encountered in true_divide; dispersion = var / mean; Is ; ![figure_10](https://user-images.githubusercontent.com/10065683/30990958-f0e3dec6-a457-11e7-9921-f1b6b9f72861.png). Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/39:591,message,message,591,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/39,1,['message'],['message']
Integrability,"I was wondering if we could deprecate the scvi external wrapper as we now have `scvi-tools`. I could also update the wrapper to have minimal functionality, but I think it would be better for people to use our API now that it's tightly integrated with scanpy anyway.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1443:56,wrap,wrapper,56,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1443,3,"['integrat', 'wrap']","['integrated', 'wrapper']"
Integrability,"I was wrong, it worked well... ```python; %matplotlib inline. import scanpy as sc; import matplotlib as mpl. # 2 lines below solved the facecolor problem.; mpl.rcParams['figure.facecolor'] = 'white'; sc.settings.set_figure_params(dpi=80, color_map='viridis', transparent=False). adata = sc.datasets.paul15(); sc.pp.recipe_zheng17(adata); sc.tl.pca(adata, svd_solver='arpack'); sc.pl.pca(adata, color='paul15_clusters', legend_loc='on data'); ```; ![screenshot from 2019-02-13 12-42-06](https://user-images.githubusercontent.com/19543497/52685380-d9f2b680-2f8c-11e9-8ca2-692b083116ee.png). Anyway, . > So we should add a `facecolor` parameter and deprecate `transparent` (with a nice message to point people to `set_figure_params(facecolor=(0, 0, 0, 0))`). sounds a good solution. Expliciting that 'white', 'w' or (1,1,1) are also applicable may be kind.; Of course, you can add theme like Seurat's one though I'm not sure how many people are requiring it. - https://satijalab.org/seurat/mca.html; - https://matplotlib.org/gallery/style_sheets/style_sheets_reference.html#sphx-glr-gallery-style-sheets-style-sheets-reference-py",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/473#issuecomment-463049965:683,message,message,683,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/473#issuecomment-463049965,1,['message'],['message']
Integrability,"I wasn't aware that you could run `sc.pp.scale` without obtaining mean 0 at the end. Would that just scale the variance per gene then?. As for your question on HVG selection after `sc.pp.regress_out` vs in batches... I think that's an interesting question, but I reckon the two scenarios are actually not that related. I normally wouldn't use `sc.pp.regress_out` to remove batch effects, but rather to regress out continuous covariates like cell cycle scores. Batch effect removal is probably best done with methods that account for the variance contribution of the batch effect as well, such as Combat... or more complex data integration methods (Seuart, MNN, scanorama). Either way, it would be an interesting comparison... just with a caveat ^^.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/722#issuecomment-509687449:627,integrat,integration,627,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/722#issuecomment-509687449,1,['integrat'],['integration']
Integrability,"I welcome @VolkerBergen ideas about plot scatter. I have used the scvelo version of scatter and works quite well and always thought that we could integrate this. Our comprehensive collection of tests related to embeddings should facilitate the recreation of the current functionality using a scatter module. As @flying-sheep points out we have a mess with respect to `pl.scatter` and `pl.embeddings` and would be great to unify the code. Currently, `pl.scatter` is used to plot two genes or any two variables like in `sc.pl.highly_variable_genes`. `pl.embedding` takes x,y (and z if 3D) from `.obsm` while adjusting color and size depending on given parameters. When I started working on the plotting functions I didn't touch `pl.scatter` which remains quite convoluted and hard to follow.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/617#issuecomment-554257192:146,integrat,integrate,146,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617#issuecomment-554257192,2,"['depend', 'integrat']","['depending', 'integrate']"
Integrability,"I went over all the places where we use the `array_type` fixture and thought about your idea to use `@pytest.mark.parametrize` and I came around to it for this case:. For **unfinished** features, it’s great. Everwhere we can’t say “we fully support this” and gradually build in support, we should use it. It has its disadvantages:. - `@pytest.mark.parametrize(""array_type"", ARRAY_TYPES)` is so long that in practice, it’s hard to see the difference to something like this: `@pytest.mark.parametrize(""array_type"", ARRAY_TYPES_XYZ)`. 	E.g. I don’t like seeing; 	; 	```py; 	@pytest.mark.parametrize(""array_type"", ARRAY_TYPES); 	@pytest.mark.parametrize(""dtype"", [""float32"", ""int64""]); 	```. 	4 times in `test_normalize_total`. If the 3rd test had a different list of values in one of the params, it would be near impossible to see. - Fixtures can depend on other fixtures, but can’t easily have a parameter matrix without that. (`pytest.fixture(params=...)` only accepts a single list of parameters, we’d have to manually use `product` in there for a matrix). That’s why I didn’t go away from a fixture in `test_pca.py`. I therefore propose that we use `@pytest.mark.parametrize` for. - things that aren’t heavily reused; - things we don’t fully support. and fixtures for everything where there’s ~3 or more test functions using the same list of parameter values.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2696#issuecomment-1781361678:844,depend,depend,844,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2696#issuecomment-1781361678,1,['depend'],['depend']
Integrability,"I will close this issue because there is an external solution available. We may think about integrating this specific plot into Scanpy at some point, but I don't see it happening anytime soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1824#issuecomment-953646449:92,integrat,integrating,92,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1824#issuecomment-953646449,1,['integrat'],['integrating']
Integrability,"I will take a look later to see how we can integrate better the visualizations, the tests and the documentation. I will put back `kwds` also. . Have you consider adding another dataset to the repository? This will be good for showing examples.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/207#issuecomment-405505301:43,integrat,integrate,43,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207#issuecomment-405505301,1,['integrat'],['integrate']
Integrability,I wonder why this wasn't done in the first place. Is scipy not already a dependency of scanpy? Or is this slower than the initial implementation?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/621#issuecomment-486991947:73,depend,dependency,73,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/621#issuecomment-486991947,1,['depend'],['dependency']
Integrability,"I would definitely recommend using the `sc.logging.print_versions` function for a more complete listing of dependencies, which does include `pynndescent`. That said, I'm not against adding it to the more compact version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1613#issuecomment-768656224:107,depend,dependencies,107,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1613#issuecomment-768656224,1,['depend'],['dependencies']
Integrability,"I would like this be to somewhere where it'd also work for CPU. I think we can implement a `__dataframe__` interface that passes either GPU or CPU memory to data shader, then let data shader handle the rest.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2656#issuecomment-1711727606:107,interface,interface,107,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2656#issuecomment-1711727606,1,['interface'],['interface']
Integrability,"I would like to color the umap representation using gene expression values. For ease of use I'd like to display the Gene name instead of gene_id which are the adata.var_names in my case. Setting `gene_symbols = 'Symbol'` doesn't seem to work for me or I am using it the wrong way. When running `sc.pl.umap(adata, gene_symbols = 'Symbol', color = ['Tnnt2'])`. I get the follwoing error message:. ```; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-116-e09d49f2528c> in <module>; ----> 1 sc.pl.umap(adata, gene_symbols = 'Symbol', color = ['Tnnt2']). /anaconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/plotting/tools/scatterplots.py in umap(adata, **kwargs); 27 If `show==False` a `matplotlib.Axis` or a list of it.; 28 """"""; ---> 29 return plot_scatter(adata, basis='umap', **kwargs); 30 ; 31 . /anaconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/plotting/tools/scatterplots.py in plot_scatter(adata, color, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, basis, groups, components, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs); 275 color_vector, categorical = _get_color_values(adata, value_to_plot,; 276 groups=groups, palette=palette,; --> 277 use_raw=use_raw); 278 ; 279 # check if higher value points should be plot on top. /anaconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/plotting/tools/scatterplots.py in _get_color_values(adata, value_to_plot, groups, palette, use_raw); 665 raise ValueError(""The passed `color` {} is not a valid observation annotation ""; 666 ""or variable name. Valid observation annotation keys are: {}""; --> 667 .format(value_to_plot, adata.obs.columns)); 668 ; 669 return color_vector, categorical. ValueError: The passed `color` Tnnt2 is not a valid observation annotation or variable name. Valid observation annotat",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/455:385,message,message,385,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455,1,['message'],['message']
Integrability,"I would like to get an opinion from @gokceneraslan, @adamgayoso (who implemented this originally), or someone else more familiar in hvg selection here. I feel like we should probably have multiple methods for this, which we expose via a kwarg. I'm not confident median rank makes sense as a value here. I think there are assumptions about what kinds of values a rank can have that get broken by taking the median. Is the method for aggregating hvgs from multiple batches here reasonable? For reference, a similar code block can be found in `_highly_variable_genes_seurat_v3`, which says this is the method used in `SelectIntegrationFeatures` from Seurat. _Originally posted by @ivirshup in https://github.com/theislab/scanpy/pull/1715#discussion_r736690390_. @jlause commented:; I agree that exposing multiple options could make sense here. Any opinion @gokceneraslan and @adamgayoso which ones we should implement / which one should be the default?. I just copied this part from the seurat_v3 batch integration, so I don't have an expert opinion - but if I remember correctly, the other flavors use dispersions_norm to break ties instead of using the median rank. If I read the code correctly, this is the within-batch dispersion averaged over batches. We could also do something similar here: Compute residual variance within each batch, take the average across batches, and use that to break ties instead of using median rank.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2151:1000,integrat,integration,1000,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2151,1,['integrat'],['integration']
Integrability,"I would like to save my figured to a defined directory. It doesn't seem I can do that without first changing the current working directory outside the line of code. What is the best way to save my plot to a specific directory without having to change it each time using os.chdir? . I have seen this [issue](https://github.com/scverse/scanpy/issues/1508#issue-750736685) from 2 years ago but wondered if any changes have been made since. ### Minimal code sample. ```; output_dir_fig = ""chosen/path/to/directory""; sc.pl.highest_expr_genes(adata, n_top=10, save= f""{output_dir_fig}/highest_expr_genes.png""). ```. ### Error output. ---------------------------------------------------------------------------; FileNotFoundError Traceback (most recent call last); Cell In[85], line 1; ----> 1 sc.pl.highest_expr_genes(adata, n_top=10, save= f""{output_dir_fig}/highest_expr_genes.png""). File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/scanpy/plotting/_qc.py:105, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds); 103 ax.set_xscale(""log""); 104 show = settings.autoshow if show is None else show; --> 105 _utils.savefig_or_show(""highest_expr_genes"", show=show, save=save); 106 if show:; 107 return None. File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/scanpy/plotting/_utils.py:339, in savefig_or_show(writekey, show, dpi, ext, save); 337 show = settings.autoshow if show is None else show; 338 if save:; --> 339 savefig(writekey, dpi=dpi, ext=ext); 340 if show:; ...; -> 2456 fp = builtins.open(filename, ""w+b""); 2458 try:; 2459 save_handl",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3276:1355,wrap,wrapper,1355,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3276,1,['wrap'],['wrapper']
Integrability,"I would say this is not a scanpy question.; It is not clear what do you mean by correlation of a categorical variable with multiple categories and a continuous variable. ; If you have a binary categorical variable, you can calculate Point Biserial Correlation, but for a multicategorical variable you would have to discretize your continuous variable and calculate Chi-squared test. You can also try ANOVA. If you think you know what variables are dependent and independent you can use logistic regression and look at its coefficients or try ANCOVA.; some additional information with examples; https://datascience.stackexchange.com/questions/893/how-to-get-correlation-between-two-categorical-variable-and-a-categorical-variab",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1845#issuecomment-848101984:448,depend,dependent,448,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1845#issuecomment-848101984,2,['depend'],['dependent']
Integrability,"I'd add the line as a `.. note` to `neighbors`, but I'd also be fine with adding pynndescent as a dependency. We'd just need to get rid of the code that works with non-pynndescent search.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1675#issuecomment-783836263:98,depend,dependency,98,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1675#issuecomment-783836263,1,['depend'],['dependency']
Integrability,I'd agree with your statement that engineering the representation is more important than the analysis. I view my goal here as allowing more representations as input. Would you mind saying more about why you thing using different metics is less clean (simple?)? I would think that would depend on what representation you're calculating the distances on.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/240#issuecomment-416821973:286,depend,depend,286,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240#issuecomment-416821973,2,['depend'],['depend']
Integrability,"I'd argue a janky dependency is an issue with scanpy's code, as there are interfaces to BioMart which are better behaved. Here's a proof of concept:. ```python; def mitochondrial_genes(org, attrname=""external_gene_name"", host=""www.ensembl.org""):; """"""Mitochondrial gene symbols for specific organism through BioMart. Parameters; ----------; org : {{""hsapiens"", ""mmusculus"", ""drerio""}}; Organism to query. Must be an organism in ensembl biomart.; fieldname : `str`, optional (default: ""external_gene_name""); Biomart attribute field to return. Possible values include ; ""external_gene_name"", ""ensembl_gene_id"", ""hgnc_symbol"", ""mgi_symbol"",; and ""zfin_id_symbol"".; host : {{""www.ensembl.org"", ...}}; A valid BioMart host URL. Returns; -------; An `np.array` containing identifiers for mitochondrial genes.; """"""; try:; from pybiomart import Server; except ImportError:; raise ImportError(; ""You need to install the `pybiomart` module.""); server = Server(host); dataset = (server.marts[""ENSEMBL_MART_ENSEMBL""]; .datasets[""{}_gene_ensembl"".format(org)]); res = dataset.query(; attributes=[attrname], ; filters={""chromosome_name"": [""MT""]},; use_attr_names=True; ); return res[attrname].values; ```. Running it:. ```python; >>> mitochondrial_genes(""hsapiens""); array(['MT-TF', 'MT-RNR1', 'MT-TV', 'MT-RNR2', 'MT-TL1', 'MT-ND1',; 'MT-TI', 'MT-TQ', 'MT-TM', 'MT-ND2', 'MT-TW', 'MT-TA', 'MT-TN',; 'MT-TC', 'MT-TY', 'MT-CO1', 'MT-TS1', 'MT-TD', 'MT-CO2', 'MT-TK',; 'MT-ATP8', 'MT-ATP6', 'MT-CO3', 'MT-TG', 'MT-ND3', 'MT-TR',; 'MT-ND4L', 'MT-ND4', 'MT-TH', 'MT-TS2', 'MT-TL2', 'MT-ND5',; 'MT-ND6', 'MT-TE', 'MT-CYB', 'MT-TT', 'MT-TP'], dtype=object); >>> mitochondrial_genes(""hsapiens"", host=""asia.ensembl.org""); array(['MT-TF', 'MT-RNR1', 'MT-TV', 'MT-RNR2', 'MT-TL1', 'MT-ND1',; 'MT-TI', 'MT-TQ', 'MT-TM', 'MT-ND2', 'MT-TW', 'MT-TA', 'MT-TN',; 'MT-TC', 'MT-TY', 'MT-CO1', 'MT-TS1', 'MT-TD', 'MT-CO2', 'MT-TK',; 'MT-ATP8', 'MT-ATP6', 'MT-CO3', 'MT-TG', 'MT-ND3', 'MT-TR',; 'MT-ND4L', 'MT-ND4', 'MT-TH', 'MT-TS2', '",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/242#issuecomment-457039514:18,depend,dependency,18,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242#issuecomment-457039514,2,"['depend', 'interface']","['dependency', 'interfaces']"
Integrability,"I'd like to +1 @ivirshup 's prototype there -- perhaps `scale` is not the best example of a function that people would use on data other than anndata, but this would be a good pattern to follow throughout `scanpy`. Not infrequently I run the following workflow:; ```; adata = scanpy.AnnData(data); dpt = scanpy.tl.dpt(adata); del adata; ```; and it would be awfully nice if `dpt` just accepted my data matrix without me having to wrap it in an AnnData object. This could be true for any scanpy function that doesn't require row/column data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1135#issuecomment-609057722:430,wrap,wrap,430,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135#issuecomment-609057722,1,['wrap'],['wrap']
Integrability,"I'd like to integrate DCA with scanpy and I need to ask some questions regarding that. . There are two use cases for DCA so far. One is denoising as a preprocessing step and using denoised matrix in downstream analyses, especially in the ones that require original feature space such as differential expression. This seems to be a better fit for the `preprocessing` package i.e. `sc.pp.dca(adata)` which overwrites `adata.X` with the denoised matrix. Users can use `copy=True` to keep adata.X of original matrix unchanged. The other use case is the low dimensional representations of cells produced by the encoder. This can also be used for the downstream analyses that do not require original feature space e.g. clustering, and visualization. This is more like `sc.tl.dca(adata)` which stores the new representation into `adata.obsm['X_dca']`. Optionally it can store dropout probabilities (pi matrix) and the dispersion, as well. However, having both sc.tl.dca and sc.pp.dca might be confusing for the users. Any suggestions?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/142:12,integrat,integrate,12,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/142,1,['integrat'],['integrate']
Integrability,"I'd like to start using [pre-commit](https://pre-commit.com) with scanpy and anndata. Pre-commit is essentially a tool that manages scripts we'd like to run before each commit, e.g. linting and formatting, so it becomes essentially impossible to forget these. I think this can allow PRs to progress faster since it gives us a way to codify formatting requirements – so we don't have to remember them – and have these checks happen locally – so we don't have to wait on CI. Of course, having these checks run depends on developers installing pre-commit, so we can also run these checks on CI ([example ci script](https://github.com/pandas-dev/pandas/blob/master/.github/workflows/pre-commit.yml), [example run](https://github.com/pandas-dev/pandas/pull/38745/checks?check_run_id=1624558250)). There is a question of what things we'd like to add here. For sure: `black`. I think import checks (e.g. no unused imports) and `flake8` would be good too. We can also add custom checks for things like slow imports. I think this would be a good time to run `black` over the whole codebase so we don't have exempted files any more. My questions for the dev team:. * Does this sound good?; * Do you have more ideas for checks/ tools?. @michalk8, I saw you added this to [`squidpy`](https://github.com/theislab/squidpy/pull/203). How's the experience been there – that is, any major foot guns we should look out for? Also, are there any tools you're using (beyond the basic `black`, `isort`, `flake8`) you'd especially recommend?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1563:508,depend,depends,508,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563,1,['depend'],['depends']
Integrability,I'd love to have scVI integration! :),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/520#issuecomment-470977867:22,integrat,integration,22,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/520#issuecomment-470977867,1,['integrat'],['integration']
Integrability,"I'll take a look at this to make sure I've not messed up in writing this wrapper (I'm actually doing some more testing myself for production use right now). . But you should know that what we've done here is mirror some of the internals of scrublet, but using Scanpy functions. Scrublet should be supplied with raw counts, but does do its own normalisations internally before doing the actual doublet prediction, which is what we're doing here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1957#issuecomment-889126422:73,wrap,wrapper,73,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1957#issuecomment-889126422,1,['wrap'],['wrapper']
Integrability,"I'm a little late to the party, but here's my 0.02$. > What is the scope of this PR? Will this just be single dataset TSNE calculation, with integration/ ingest functionality happening separately, or would you like to do it all at once?. I think we can split it into two PRs, since they're going to touch different parts of the code base, and it should be easier to review them individually. > How different are the arguments to the various affinity methods?. So, if we use the KNNG provided by `sc.pp.neighbors`, these parameters become unnecessary. Both `perplexity` and `k` specify the number of k-nearest neighbors when constructing the KNNG. Here, we assume that the KNNG exists from before, so there is no need for this parameter. > Do you need to know what the affinity method was if you're just calculating an embeddings? Or does that only become important when you want to add new data?. Yes, the affinity model will have to be somehow kept, since when we call `transform`, we need to find the nearest neighbors in the index. I haven't checked how your UMAP functionality does this, but I'm guessing it's similar. Regarding the whole API, I have a few comments. I very much dislike the API `sc.pp.neighbors_tsne(adata)`. scanpy is nice because it's easy to use and the API is dead simple. I can just call `sc.pp.neighbors` followed by clustering, visualization, and whatever else I want using simple function calls. If we went this route, this would mean changing `sc.pp.neighbors` to `sc.pp.umap_neighbors`, and then splitting of yet another `sc.pp.gauss_neighbors`. This would not only make things confusing, it would mean re-calculating the KNNG at each call, which we would inevitably have to do if we wanted different visualizations. It then also becomes quite unclear what to do when I want to do Louvain clustering. Should there be a `sc.pp.louvain_neighbors` as well? Which neighbors should I use there? (As an aside, I don't understand why using UMAP connectivites is the default for",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-759374009:141,integrat,integration,141,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-759374009,2,['integrat'],['integration']
Integrability,"I'm finally merging this as we know have the `scanpy.external` submodule, where this can be properly linked - along with PHATE and all other wrappers to external single-cell packages. Sorry for that this took so long, I only could do this during the past calm couple of days. Thank you for the PR!. A very happy new year to you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/292#issuecomment-450770914:141,wrap,wrappers,141,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/292#issuecomment-450770914,1,['wrap'],['wrappers']
Integrability,"I'm having some trouble debugging whatever is going wrong with the notebook tests here. I get the same results if I run `pytest` on my machine, but don't get a failure if I run the code manually. Additionally, I don't get an error (the `abort`) if I *only* run the notebook tests (`pytest -k ""test_pbmc3k""`). Pretty sure the error is happening on the call to louvain in the notebook tests – an `assert False` fails the tests, one after gives current result – but I can't reproduce the abort interactively. Any idea what's going on/ how I can get a more helpful error message here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/248#issuecomment-419695136:567,message,message,567,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248#issuecomment-419695136,1,['message'],['message']
Integrability,"I'm not able to reproduce this. Here's what I tried. * Made a conda environment with `conda create -yn torch-scanpy ""python=3.8""`, and activated it `conda activate torch-scanpy`; * Installed: `pip install scanpy torch`; * Imported: `python3 -c ""import torch; import scanpy""`. IIRC, there has been an issue with the order of importing numba and pytorch due to how they require their LLVM dependency. I would make sure your version of pytorch and numba are up to date (I believe your pytorch is a few versions old) and trying again. If the issue persists, could you check if you run into problems with this?. ```python; import torch; import numba; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1286#issuecomment-646456990:387,depend,dependency,387,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1286#issuecomment-646456990,1,['depend'],['dependency']
Integrability,"I'm not sure what t-SNE implementation is currently used in scanpy, but would it make sense to switch it to openTSNE? It's a Cython re-implementation of FIt-SNE, it's available on conda and should be very easy to depend on. As far as I understand the scanpy architecture, it builds a kNN graph and then runs downstream analysis (like UMAP or Louvain) on this kNN graph. Is that right? I suppose this is currently not implemented for t-SNE? With openTSNE it'd be easy to use the pre-built kNN graph and run t-SNE directly on that. Also, the default parameters of t-SNE in scanpy could IMHO be improved, see https://www.nature.com/articles/s41467-019-13056-x. Some of these recommendations (learning rate, initialization) are now default in openTSNE. There are some open issues at scanpy related to t-SNE such as https://github.com/theislab/scanpy/issues/1150 and https://github.com/theislab/scanpy/issues/996 but I think this suggestion would supersede them. We had a brief discussion of this at openTSNE here https://github.com/pavlin-policar/openTSNE/issues/102. I can see four somewhat separate suggestions:. 1. switch scanpy to using openTSNE for tSNE, using already constructed kNN graph; 2. add tSNE support for `ingest` using openTSNE functionality.; 3. change default tSNE parameters (n_iter, learning rate, initialization) following openTSNE defaults.; 4. add some tSNE ""recipes"" based on https://www.nature.com/articles/s41467-019-13056-x. What of this, if any, makes sense from the scanpy point of view?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1233:213,depend,depend,213,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1233,1,['depend'],['depend']
Integrability,I'm pretty sure it's the pandas 0.23 issue... same error message as the one encountered here: https://github.com/theislab/scanpy/issues/158,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/162#issuecomment-391991529:57,message,message,57,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/162#issuecomment-391991529,1,['message'],['message']
Integrability,I'm pretty sure none of you are having the same issue as the original one reported here. Compare @abuchin 's error message of `KeyError: 'dict'` to the original poster's error of `OSError: Can't read data`. The thing you're seeing is a new one stemming from an update to anndata. You're trying to read in a `h5ad` file created with a newer version of the package with your older one. I think the cutoff point is 0.8.0 but I could be mistaken. Upgrade your anndata and you should be ok.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1351#issuecomment-1198015945:115,message,message,115,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351#issuecomment-1198015945,1,['message'],['message']
Integrability,"I'm still dubious of the value, especially when we provide different ways of ways of optimizing the score. What would you think of instead having `sc.metrics.modularity` where you match a clustering and a graph returning a modularity score?. It would basically wrap:. ```python; (; igraph.Graph.Weighted_Adjacency(adata.obsp[""connectivities""]); .modularity(adata.obs[""louvain""].cat.codes); ); ```. But you could also generate modularity scores for other labelings.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2908#issuecomment-1997873869:261,wrap,wrap,261,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2908#issuecomment-1997873869,1,['wrap'],['wrap']
Integrability,"I'm thinking. My favorite command line interfaces have the ability to query options and set options globally by writing to a config file (jupyter, npm, git, …). Maybe we should give scanpy that ability. People could use that if they use scanpy mainly through scripts. ```console; $ scanpy settings; Config file: ~/.config/scanpy/scanpy.toml; cachedir='~/.cache/scanpy' (default); ...; $ scanpy settings cachedir '/my/path'; Set cachedir to '/my/path' in ~/.config/scanpy/scanpy.toml; $ scanpy settings cachedir; /my/path; ```. And of course we also have a python API for this. People who use scanpy mainly interactively can use that one.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-477113150:39,interface,interfaces,39,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-477113150,1,['interface'],['interfaces']
Integrability,"I'm trying to import some data I downloaded from GEO using the read_10x_mtx() function. Since this data was generated with an older version of Cellranger, there is no features.tsv.gz file. I renamed the genes.tsv.gz file to features.tsv.gz but that still doesn't fix my problem. I am pasting the error message below: . ```pytb; --> This might be very slow. Consider passing `cache=True`, which enables much faster reading from a cache file.; ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); /Applications/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance); 3077 try:; -> 3078 return self._engine.get_loc(key); 3079 except KeyError:. pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.Int64HashTable.get_item(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.Int64HashTable.get_item(). KeyError: 2. During handling of the above exception, another exception occurred:. KeyError Traceback (most recent call last); <ipython-input-13-884b80f3079d> in <module>; ----> 1 adata = sc.read_10x_mtx('/Users/kulkarnia2/Box/scRNASeq/HNSCC/Combined_TC_CK_scRNAseq/all_samples/HD_1_PBL'). ~/.local/lib/python3.7/site-packages/scanpy/readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only); 302 make_unique=make_unique,; 303 cache=cache,; --> 304 cache_compression=cache_compression,; 305 ); 306 if genefile_exists or not gex_only:. ~/.local/lib/python3.7/site-packages/scanpy/readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache, cache_compression); 371 else:; 372 raise ValueError(""`var_names` needs to be 'gene_symbols' or 'gene_ids'""); --> 373 adata.var['feature_types'] = genes[2].values; 374 adata.obs_names = pd.read_csv(path / 'barcodes.tsv.gz', header=Non",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1408:302,message,message,302,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1408,1,['message'],['message']
Integrability,"I'm using Scanpy with the following software versions:. python==3.7; scanpy==1.4.4; numpy==1.17.2; anndata==0.6.22.post1. on Ubuntu 18.04. I am able to save my AnnData object just fine with . ```py; sc.write(results_file, adata); ```; and to load it again with . ```py; adata = sc.read(results_file); ```. however if I save it after I run the command . ```py; sc.tl.rank_genes_groups(adata, 'louvain12_lab', method='wilcoxon'); ```. the AnnData object will save but when I try to reload it, I get an error message:. ```pytb; ValueError Traceback (most recent call last); <ipython-input-141-159082f1696f> in <module>; 1 results_file = os.path.join(adir, '{project}.count_{count}.gene_{gene}.mito_{mito}.HVGs_{nhvgs}.TPT.{log}.scale.TEST.h5ad'.format(project=project_name, count=count_thresh, gene=gene_thresh, mito=mitothresh, nhvgs=nhvgs, log=logstatus)); 2 print(results_file); ----> 3 adata = sc.read(results_file). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 95 filename, backed=backed, sheet=sheet, ext=ext,; 96 delimiter=delimiter, first_column_names=first_column_names,; ---> 97 backup_url=backup_url, cache=cache, **kwargs,; 98 ); 99 # generate filename and read to dict. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 497 if ext in {'h5', 'h5ad'}:; 498 if sheet is None:; --> 499 return read_h5ad(filename, backed=backed); 500 else:; 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size); 445 else:; 446 # load everything into memory; --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size); 448 X = constructor_args[0]; 449 dtype = None. /opt/min",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/832:506,message,message,506,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832,1,['message'],['message']
Integrability,"I'm using scanpy 1.8.2, anndata 0.8.0 and h5py 3.1.0. I got this error while reading an h5ad file:. ```; adata=sc.read_h5ad('XXXX.h5ad'); ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); /opt/conda/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, *args, **kwargs); 155 if zarr and isinstance(elem, (zarr.Group, zarr.Array)):; --> 156 parent = elem.store # Not sure how to always get a name out of this; 157 elif isinstance(elem, SparseDataset):. /opt/conda/lib/python3.8/site-packages/anndata/_io/h5ad.py in read_group(group). /opt/conda/lib/python3.8/enum.py in __getitem__(cls, name); 386 def __getitem__(cls, name):; --> 387 return cls._member_map_[name]; 388 . KeyError: 'dict'. During handling of the above exception, another exception occurred:. AnnDataReadError Traceback (most recent call last); <ipython-input-15-a2632df74a34> in <module>; ----> 1 adata=sc.read_h5ad('XXXX.h5ad'). /opt/conda/lib/python3.8/site-packages/anndata/_io/h5ad.py in read_h5ad(filename, backed, as_sparse, as_sparse_fmt, chunk_size). /opt/conda/lib/python3.8/functools.py in wrapper(*args, **kw); 873 '1 positional argument'); 874 ; --> 875 return dispatch(args[0].__class__)(*args, **kw); 876 ; 877 funcname = getattr(func, '__name__', 'singledispatch function'). /opt/conda/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, *args, **kwargs); 160 parent = elem.file.name; 161 return parent; --> 162 ; 163 ; 164 def report_read_key_on_error(func):. AnnDataReadError: Above error raised while reading key '/layers' of type <class 'h5py._hl.group.Group'> from /.; ```. Any ideas?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2297:1154,wrap,wrapper,1154,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2297,1,['wrap'],['wrapper']
Integrability,"I'm wondering if we can come to some agreement on a slight modification to this proposal. > > How does this impact users vs. developers?; >; > user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. This seems good. > > Who manages the sub-packages?; >; > the IO subpackage? everyone 😅. 😅 indeed. > For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know. I feel like complicated dependency management was what we were trying to avoid here. Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. --------------. ## An alternative: project specific IO. `squidpy_io`, `muon_io`. Packages which read in package specific formats with a minimal set of dependencies. We can keep `muon.read_10x_atac`, so nothing changes for users. We skip out on complicated ownership and complicated dependencies. This should be very low overhead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059537650:692,depend,dependency,692,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059537650,4,['depend'],"['dependencies', 'dependency']"
Integrability,"I've come across a strange behavior related with this issue. Depending on whether or not I save the object I get the same warning as OP. This works as it should:; ```; import scanpy as sc. adata=sc.read_h5ad(data_dir+'scanpy_QC_sexchrom.h5ad'); adata.raw=adata.copy() #data to save; sc.pp.log1p(adata) # logaritmize. ### Test 1, no saving, works as it should; adata=adata.raw.to_adata(); sc.pp.log1p(adata); ##>>> no warning; ```. Saving mid-way does not allow to avoid the warning, even restarting the kernel before reading the data:; ```; import scanpy as sc. ## same as above; adata=sc.read_h5ad(data_dir+'scanpy_QC_sexchrom.h5ad'); adata.raw=adata.copy() #data to save; sc.pp.log1p(adata) # logaritmize. ### Test 2, saving and re-assigning from raw; ### saving object, reading, testing again; ### Doesnt work; adata.write_h5ad(tmp+'scanpy_test.h5ad'); adata=sc.read_h5ad(tmp+'scanpy_test.h5ad'); adata=adata.raw.to_adata(); sc.pp.log1p(adata); ###>>>WARNING: adata.X seems to be already log-transformed.; ```. I'm on scanpy 1.9.1 if it matters",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1333#issuecomment-1209486748:61,Depend,Depending,61,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1333#issuecomment-1209486748,1,['Depend'],['Depending']
Integrability,"I've got two main reasons for thinking they should be more visible:. 1. If I'm trying to find what tools are available through scanpy for a certain task, it should be very obvious where that might be available. For example, if I want to know what's available for batch correction, I (the user) am probably not too fussed about whether it's in the scanpy codebase or not.; 2. As a method developer, it'd encourage me to integrate my method if I saw it'd be highly visible and that other people were doing it. Right now there are links, but users still have to go to see the notes with those links, go to a separate page, and scroll for a bit to see any particular method. . Another strategy could be a top level `External API` heading underneath the `API` heading? Then there could be an expandable table of contents (how I typically navigate the site) to get an idea of what's there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/588#issuecomment-479739101:419,integrat,integrate,419,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/588#issuecomment-479739101,1,['integrat'],['integrate']
Integrability,"I've gotten complaints from Matplotlib about calling `mpl.use`, to set the backend after importing `pyplot` ([relevant matplotlib docs](https://matplotlib.org/tutorials/introductory/usage.html#what-is-a-backend)). I think it would be unintuitive if packages behaved differently depending on what functions had been called. In general, matplotlib has a lot of state and messing with it has only brought me pain.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/756#issuecomment-522822622:278,depend,depending,278,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/756#issuecomment-522822622,1,['depend'],['depending']
Integrability,I've had better luck integrating data from multiple experiments using [Harmony](https://portals.broadinstitute.org/harmony/) than the current integration methods in scanpy.external.pp. This PR adds a wrapper for Harmony to the external API.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1306:21,integrat,integrating,21,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1306,3,"['integrat', 'wrap']","['integrating', 'integration', 'wrapper']"
Integrability,"I've played around with ABCs for defining interfaces, but had thought the `typing` module was more closely tied with them. Do you understand the rationale for `typing` classes being separate when ABCs and regular classes can be used for annotation?. As far I can tell, it's for subscripted (parametric?) annotations. For many python classes those would have to be a runtime check – e.g. `List[int]` – which `ABC`s don't do. Not that `typing` does these checks, but it allows a way to express those constraints.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-445609843:42,interface,interfaces,42,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-445609843,1,['interface'],['interfaces']
Integrability,"I've pretty much just gotta write the docs and rebase on my other PR, and this should be good to go. For adding `top_segment_proportions` and `top_proportions` to preprocessing, should they get a wrapper to work on AnnData objects? Also, I'm not super happy with the name `top_segment_proportions`, and am open to suggestions for a better name.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/316#issuecomment-433284121:196,wrap,wrapper,196,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-433284121,1,['wrap'],['wrapper']
Integrability,"I've updated the code to store the image path in `adata.uns['spatial'][sample_id]['tif_image_path']`. The test now also checks whether the image file exists. . I can also test whether the image file is a valid tiff image, but for this we'd need to add a tiff reading library like `pillow` as a test dependency to scanpy. If that is ok, I'll update the PR with an additional test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1506#issuecomment-733605592:299,depend,dependency,299,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1506#issuecomment-733605592,1,['depend'],['dependency']
Integrability,"I've used it a bit, and have gotten nice results. I think I've mentioned it before (#938), but that was on an unrelated issue so it's good to have. The results are nice:. <details>; <summary> Example usage </summary>. ```python; from adjustText import adjust_text. def gen_mpl_labels(; adata, groupby, exclude=(), ax=None, adjust_kwargs=None, text_kwargs=None; ):; if adjust_kwargs is None:; adjust_kwargs = {""text_from_points"": False}; if text_kwargs is None:; text_kwargs = {}. medians = {}. for g, g_idx in adata.obs.groupby(groupby).groups.items():; if g in exclude:; continue; medians[g] = np.median(adata[g_idx].obsm[""X_umap""], axis=0). if ax is None:; texts = [; plt.text(x=x, y=y, s=k, **text_kwargs) for k, (x, y) in medians.items(); ]; else:; texts = [ax.text(x=x, y=y, s=k, **text_kwargs) for k, (x, y) in medians.items()]. adjust_text(texts, **adjust_kwargs). with plt.rc_context({""figure.figsize"": (8, 8), ""figure.dpi"": 300, ""figure.frameon"": False}):; ax = sc.pl.umap(pbmc, color=""Low-level celltypes"", show=False, legend_loc=None, frameon=False); gen_mpl_labels(; pbmc,; ""Low-level celltypes"",; exclude=(""None"",), # This was before we had the `nan` behaviour; ax=ax,; adjust_kwargs=dict(arrowprops=dict(arrowstyle='-', color='black')),; text_kwargs=dict(fontsize=14),; ); fig = ax.get_figure(); fig.tight_layout(); plt.show(); ```. </details>. ![image](https://user-images.githubusercontent.com/8238804/100496350-81af9780-31a7-11eb-8b38-2eb7f914c1a1.png). I believe you're also supposed to be able to make the text repel from points, so they don't sit on top of your data, but I had some trouble getting that working at the time. I'm a bit antsy about having this as a required dependency since maintenance [doesn't seem too active](https://pypi.org/project/adjustText/#history). Could be an optional dependency, used with `legend_loc=""adjust_text""`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1513#issuecomment-735051689:1693,depend,dependency,1693,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1513#issuecomment-735051689,2,['depend'],['dependency']
Integrability,"IPython/core/formatters.py in __call__(self, obj); ... FileNotFoundError: [Errno 2] No such file or directory: '/Users/name/.matplotlib/fontlist-v310.json.matplotlib-lock'. ```. #### Versions. <details>. ```; -----; anndata 0.7.6; scanpy 1.7.2; sinfo 0.3.4; -----; 0294638c8bf50491b025b096f3dba0a1 NA; absl NA; anyio NA; appnope 0.1.0; astunparse 1.6.3; attr 19.3.0; babel 2.9.0; backcall 0.2.0; brotli 1.0.9; certifi 2020.06.20; cffi 1.14.5; chardet 3.0.4; cycler 0.10.0; cython_runtime NA; dateutil 2.8.1; decorator 4.4.2; gast NA; get_version 2.2; google NA; h5py 2.10.0; idna 2.10; igraph 0.8.3; ipykernel 5.3.3; ipython_genutils 0.2.0; ipywidgets 7.5.1; jedi 0.17.2; jinja2 2.11.2; joblib 0.16.0; json5 NA; jsonschema 3.2.0; jupyter_server 1.2.2; jupyterlab_server 2.1.2; keras_preprocessing 1.1.2; kiwisolver 1.2.0; legacy_api_wrap 1.2; llvmlite 0.36.0; markupsafe 1.1.1; matplotlib 3.2.1; mpl_toolkits NA; natsort 7.1.1; nbclassic NA; nbformat 5.0.7; numba 0.53.1; numexpr 2.7.3; numpy 1.19.0; opt_einsum v3.3.0; packaging 20.4; pandas 1.2.4; parso 0.7.0; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prometheus_client NA; prompt_toolkit 3.0.5; psutil 5.8.0; ptyprocess 0.6.0; pycparser 2.20; pygments 2.6.1; pynndescent 0.5.2; pyparsing 2.4.7; pyrsistent NA; pytz 2019.3; requests 2.24.0; scipy 1.4.1; seaborn 0.10.0; send2trash NA; six 1.14.0; sklearn 0.23.1; sniffio 1.2.0; statsmodels 0.12.2; storemagic NA; swig_runtime_data4 NA; tables 3.6.1; tensorboard 2.2.2; tensorflow 2.2.0; termcolor 1.1.0; texttable 1.6.3; tornado 6.1; traitlets 4.3.3; typing_extensions NA; umap 0.5.1; urllib3 1.25.10; wcwidth 0.2.5; wrapt 1.12.1; yaml 5.3.1; zipp NA; zmq 19.0.1; -----; IPython 7.16.1; jupyter_client 6.1.6; jupyter_core 4.6.3; jupyterlab 3.0.5; notebook 6.0.3; -----; Python 3.7.7 (v3.7.7:d7c567b08f, Mar 10 2020, 02:56:16) [Clang 6.0 (clang-600.0.57)]; Darwin-20.2.0-x86_64-i386-64bit; 4 logical CPU cores, i386; -----; Session information updated at 2021-05-26 22:36. ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1857:2387,wrap,wrapt,2387,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1857,1,['wrap'],['wrapt']
Integrability,Identify optional dependencies,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/59:18,depend,dependencies,18,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/59,1,['depend'],['dependencies']
Integrability,"If I remember correctly, the SCTransform `vst` method uses `sqrt(n)` by default but the `SCTransform` wrapper in Seurat uses `sqrt(n/30)`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1643#issuecomment-791998671:102,wrap,wrapper,102,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643#issuecomment-791998671,1,['wrap'],['wrapper']
Integrability,In case you're still looking to use MAST or integrate other `R` tools into a scanpy pipeline. That works quite well via [anndata2ri](www.github.com/flying-sheep/anndata2ri). An example of how you can do this can be found in the case study notebook [here](https://github.com/theislab/single-cell-tutorial/blob/master/latest_notebook/Case-study_Mouse-intestinal-epithelium_1904.ipynb).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/625#issuecomment-487553407:44,integrat,integrate,44,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/625#issuecomment-487553407,1,['integrat'],['integrate']
Integrability,"In different time course, the batch effect and true biological variation will be entangled. . Batch effects, which occur because measurements are affected by laboratory conditions,reagent lots and personnel differences. This becomes a major problem when batch effects are correlated with an outcome of interest and lead to incorrect conclusions. However, in single cell RNAseq, different datasets should be integrated with suitable algorithm (such as mnn, CCA, bbknn, harmony, scvi et al.), even no batch effect exists.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/265#issuecomment-471808354:407,integrat,integrated,407,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-471808354,1,['integrat'],['integrated']
Integrability,"In the ""requirement already satisfied"" it looks like ""scikit-misc"" is installed in a different location and not within the `site-packages` folder of the anaconda env listed on the line below for `numpy`. From within the `py38` env you could try to reinstall it with `pip install --user scikit-misc --force` and also delete the other one or remove it from your `$PYTHONPATH`? Installing things in the jupyter notebook might be using a different version of pip than the one in the environment (depending on how your kernels are set up) so I think it's sometimes safer to do these things from the command line.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2073#issuecomment-989974912:492,depend,depending,492,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-989974912,1,['depend'],['depending']
Integrability,"In the described case, it is expected that `adata.raw` is set. Thus, by default `sc.pl.dotplot` will use the raw data instead of the scaled `adata.X`. In the case in which `adata.raw` is not set, then dotplot will wrongly compute the percentage. The suggestion to uncouple the dot size and the dot color is very good and after the 1.6 relase of scanpy is now possible to do this using the `dot_color_df` (see https://scanpy.readthedocs.io/en/stable/api/scanpy.pl.dotplot.html#scanpy.pl.dotplot). However, this requires that the user prepares a pandas dataframe for the color while the dot size (% of cells) can be computed based on raw, or a layer (depending on the settings for `use_raw` and `layer`). I am open to further suggestions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1361#issuecomment-674912356:649,depend,depending,649,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1361#issuecomment-674912356,1,['depend'],['depending']
Integrability,"In theory I think we can do most of that. In practice, I got some errors. I think it would be worth formalizing what the supported interface for doing multimodal analysis is. I'd really like it to be uniform. I could see it being based on keys in `.var`:. ```python; adata.var[""gex""] = adata.var[""expression_type""] == ""Gene Expression""; sc.pl.pca(adata, var_key=""gex""); sc.pl.pca(adata, color=[""Protein1"", ""Protein2""]); # This also has the nice feature that it could abstract out the current `use_highly_variable` argument; ```. View based:. ```python; gex_view = adata[:, adata.var[""expression_type""] == ""Gene Expression""]; sc.pp.pca(gex_view) # Calculate pca on gene expression; sc.pl.pca(adata, color=[""Protein1"", ""Protein2""]); ```. Different expression types could be put under `.obsm` (probably the closest ""analogy"" to `SingleCellExperiment`'s `assays()`). But this raises questions of what counts as a variable, and I think would take more work to implement. Of course, there are many other ways this could be done as well. As it could impact APIs throughout `scanpy`, I think input from @falexwolf and @flying-sheep is important here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/479#issuecomment-464417618:131,interface,interface,131,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/479#issuecomment-464417618,1,['interface'],['interface']
Integrability,Ingest won't integrate datasets of different lengths,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2085:13,integrat,integrate,13,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2085,1,['integrat'],['integrate']
Integrability,"Install it, it’s an optional dependency.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1369#issuecomment-673987048:29,depend,dependency,29,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1369#issuecomment-673987048,1,['depend'],['dependency']
Integrability,"Instead of having `self.d` as the central attribute, why not having a central property `self.stats` and that's a DataFrame with a multi-index for the columns. The outer index is the group that you're comparing against a reference, the inner index loops over `gene`, `score`, `pval`, `pval_adj`, etc. Meaning, there is a class for this besides the convenience function:; ```; rg = sc.RankGenes(); ```. And if you just want a dataframe and not store something in `adata`, you can do; ```; rg.compute_stats(...); ```. After this `rg.stats` contains your results. Within the convenience wrapper `sc.tl.rank_genes`, this `stats` attribute will be written to `adata`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1156#issuecomment-616490080:583,wrap,wrapper,583,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156#issuecomment-616490080,1,['wrap'],['wrapper']
Integrability,Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/859:0,Integrat,Integrate,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859,1,['Integrat'],['Integrate']
Integrability,"Integrated [Shannon Component Analysis](https://www.biorxiv.org/content/10.1101/2021.01.19.427303v2.full) into the external API, with full documentation and references. SCA operates like PCA, storing a lower-dimensional representation of `adata.X` (or the chosen `layer`) in `adata.obsm[key_added]`. . Like PCA and ICA, SCA is linear; however, we have found SCA representations better than PCA or ICA at separating true cell types, yielding better clusters downstream. The source repository can be found [here](https://github.com/bendemeo/shannonca) and installed using `pip`. If you decide you'd like to integrate this into the main API (i.e. as `sc.tl.sca`), I would be happy to assist.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1780:0,Integrat,Integrated,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780,2,"['Integrat', 'integrat']","['Integrated', 'integrate']"
Integrability,Integration across SmartSeq2 and 10X Datasets,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2662:0,Integrat,Integration,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2662,1,['Integrat'],['Integration']
Integrability,Integration of Marsilea to create Heatmap from AnnData,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2512:0,Integrat,Integration,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2512,1,['Integrat'],['Integration']
Integrability,Integration of dorothea and progeny,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1724:0,Integrat,Integration,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1724,1,['Integrat'],['Integration']
Integrability,"Interesting! I was coming across this error in #2816 (where there is a fix), but only with older versions of dependencies. Probably worth back porting that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2830#issuecomment-1910515749:109,depend,dependencies,109,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2830#issuecomment-1910515749,1,['depend'],['dependencies']
Integrability,"Is it fine or need to do anything else before make a pull request ?. Thanks. On Tue, May 14, 2019 at 4:05 PM Philipp A. <notifications@github.com> wrote:. > yes!; >; > 1. make sure you have all your modified copy somewhere outside of the; > cloned directory! we’re going to destroy all changes inside of that; > directory!; > 2. Destroy all changes and go back to 1.4: git reset --hard 1.4 (if; > it’s really 1.4 and not e.g. 1.4.1); > 3. Make a new branch based on 1.4: git checkout -b weighted-clustering; > 4. Copy your changes over again.; > 5. Add all files you changed individually (not git add . or git add -A,; > but git add scanpy/file1.py scanpy/file2.py ...); > 6. git commit -m 'your commit message'; >; > Now you can make a new PR with just your changes in it:; > master...Khalid-Usman:weighted-clustering; > <https://github.com/theislab/scanpy/compare/master...Khalid-Usman:weighted-clustering>; >; > Make sure that you only see your changes on the lower part of that page; > before hitting the “Create Pull Request” button; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/pull/630?email_source=notifications&email_token=ABREGOA2G7HECFLNHTNNZ33PVJXFZA5CNFSM4HKUCBXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVKTS6Y#issuecomment-492124539>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABREGOHOOABYXSQL6HLZUN3PVJXFZANCNFSM4HKUCBXA>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/630#issuecomment-492175356:703,message,message,703,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630#issuecomment-492175356,1,['message'],['message']
Integrability,"Is it possible to have one figure pf spatial gene expression stack over (superimpose) another? The following function will give me two subplots instead of an integrated one ; `sc.pl.spatial(ada, img_key=""hires"", color=[""Gene1"", ""Gene2""]); `",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2284:158,integrat,integrated,158,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2284,1,['integrat'],['integrated']
Integrability,Is there a way to trace back the version of Scanpy used from an h5ad file ? Our collaborator has shared some h5ad files generated over a year ago and I wanted to figure out the exact versions Scanpy and other dependencies used. Thank you.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1192:209,depend,dependencies,209,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1192,1,['depend'],['dependencies']
Integrability,Is there anything like [clustree](https://github.com/lazappi/clustree) in python that integrates nicely with scanpy?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/670#issuecomment-776593368:86,integrat,integrates,86,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670#issuecomment-776593368,1,['integrat'],['integrates']
Integrability,It appears this was an issue related to anndata2ri- scipy 1.0.1 was being installed when installing anndata2ri. Installing scanpy first prevented this issue. ; I use pip install --user for scanpy because otherwise I receive an error message: ; Cannot uninstall 'llvmlite'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.; My workaround has been to use --user as a directory and add a path to import scanpy.; I'm sorry for the trouble thank you for the help.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1252#issuecomment-636118302:233,message,message,233,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1252#issuecomment-636118302,1,['message'],['message']
Integrability,It causes exactly the same issue when I run:. ```; import numpy as np; import umap; ```; There is no segfault message. The jupyter kernel failure message is: . _Kernel restarting. The kernel appears to have died. It will restart automatically._,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1567#issuecomment-754566053:110,message,message,110,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567#issuecomment-754566053,2,['message'],['message']
Integrability,"It doesn’t. We could also. 1. wait to merge this until `skmisc` has a new release or; 2. throw a special error when people try to use seurat v3 with numpy 2. Sadly(?) Python doesn’t allow packages to add constraint to other packages’ dependencies, else we could tell the resolver that all currently release skmisc versions are incompatible with numpy 2",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3115#issuecomment-2182602501:234,depend,dependencies,234,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3115#issuecomment-2182602501,1,['depend'],['dependencies']
Integrability,"It is difficult to find a one-size-fits-all solution. Depending on the number of legends, the length of the labels and the DPI used the space required will vary. Thus, I think is is better for the user to adjust the plots.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1312#issuecomment-662452670:54,Depend,Depending,54,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1312#issuecomment-662452670,1,['Depend'],['Depending']
Integrability,"It is said that ""Be reminded that it is not advised to use the corrected data matrices for differential expression testing."" in scanpy document (http://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html) when execute MNN correction. However, Haghverdi Laleh (the one who presents MNN correction strategy, https://www.nature.com/articles/nbt.4091) says ""MNN correction improves differential expression analyses, After batch correction is performed, the corrected expression values can be used in routine downstream analyses such as clustering prior to differential gene expression identification"" in his Nature Biotech paper. So, I am a little confused. We have compared some corrections methods, such as regress_out, combat, MNN and MultiCCA (used by seurat), the results show that MNN and CCA have a better effect than regress_out and combat.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/168#issuecomment-395615173:511,rout,routine,511,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168#issuecomment-395615173,1,['rout'],['routine']
Integrability,"It looks like those warning are being raised from `scipy.stats.distributions.t.sf`. . This was also happening in the tests, but there's already a bunch of warnings in the tests so we didn't see it. ~~I believe we didn't get this warning from the older code because of these lines:~~. ```python; dof[np.isnan(dof)] = 0		; pvals = stats.t.sf(abs(scores), dof)*2 # *2 because of two-tailed t-test; ```. I don't think it's the above lines anymore, since the replacing the `ttest_ind_from_stats` call with the following still throws the warning:. ```python; df, denom = stats.stats._unequal_var_ttest_denom(; v1=var_group, n1=ns_group, v2=var_rest, n2=ns_rest; ); df[np.isnan(df)] = 0; scores, pvals = stats.stats._ttest_ind_from_stats(; mean_group, mean_rest, denom, df; ); ```. Other than that, potential solutions include:. * Mask out genes which aren't expressed in the compared groups (since there's not too much point in getting and correcting a pvalue for them); * Revert change (would bring back issue of genes with variance of 0); * Wrap the t-test with something like `np.errstate` to hide the warning",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/629#issuecomment-488907170:1037,Wrap,Wrap,1037,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/629#issuecomment-488907170,1,['Wrap'],['Wrap']
Integrability,"It looks to me like the sklearn dependency was update more due to bugs in earlier 0.21.* releases series, see 7716bfdec3cb9bd19923a91180dabc35ffd7709a. We don't promise compatibility with older versions of sklearn, so downgrading is not a good long-term solution. @Koncopd might also be able to give some advice on this, as I believe he has been using pytorch with scanpy, though I'm not sure if this is via conda environments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1121#issuecomment-604799158:32,depend,dependency,32,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1121#issuecomment-604799158,1,['depend'],['dependency']
Integrability,"It seems you do not always end up with n-1 neighbors, because for n=3, you suddenly get differing number of neighbors:; ```python; import scanpy as sc; adata = sc.datasets.blobs(n_observations=5). for n_neighbors in [1, 2, 3]:; sc.pp.neighbors(adata, n_neighbors=n_neighbors); print(f'n_neighbors = {n_neighbors}:\n', adata.uns['neighbors']['connectivities'].A); ```; Output:; ```; n_neighbors = 1:; [[0. 0. 0. 0. 0.]; [0. 0. 0. 0. 0.]; [0. 0. 0. 0. 0.]; [0. 0. 0. 0. 0.]; [0. 0. 0. 0. 0.]]; n_neighbors = 2:; [[0. 0. 0. 1. 0.]; [0. 0. 1. 0. 0.]; [0. 1. 0. 0. 0.]; [1. 0. 0. 0. 1.]; [0. 0. 0. 1. 0.]]; n_neighbors = 3:; [[0. 0.5849553 0. 1. 0.5849636 ]; [0.5849553 0. 1. 0.5849678 0. ]; [0. 1. 0. 0.58496827 0. ]; [1. 0.5849678 0.58496827 0. 1. ]; [0.5849636 0. 0. 1. 0. ]]; ```; It is been while that I read about UMAP and can't get my head around why this happens right now. Relying on UMAP seems a good idea to me, maybe the corner case `n_neighbors=1` should just be catched with a more meaningful error message?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1706#issuecomment-788885174:1008,message,message,1008,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1706#issuecomment-788885174,1,['message'],['message']
Integrability,"It works fine when I set the color to 'seurat_clusters'. But I want to show the pie chart for each node so I changed the color parameter according to the format in the documentation. This caused a ""unhashable type: 'dict'"" error. I looked into the paga.py code but couldn't figure out where the problem is. . The 'seurat_clusters' contains 8 clusters:. ```; adata.obs['seurat_clusters'].cat.categories; ```; `Index(['0', '1', '2', '3', '4', '5', '6', '7'], dtype='object')`. My command-line to run pl.paga with pie nodes:; ```; sc.pl.paga(adata,color={'0':{'red':0.2,'blue':0.8},'1':{'red':0.2,'blue':0.8},'2':{'red':0.2,'blue':0.8},'3':{'red':0.2,'blue':0.8},'4':{'red':0.2,'blue':0.8},'5':{'red':0.2,'blue':0.8},'6':{'red':0.2,'blue':0.8},'7':{'red':0.2,'blue':0.8}}, root=3, layout='eq_tree', node_size_scale=2, save='PAGA_tree.pdf'); ```. Error message:; ```; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-327-1f686f2dc40b> in <module>(); ----> 1 sc.pl.paga(adata,color=colors). /nfs/med-bfx-activeprojects/weishwu/common/Anaconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/plotting/_tools/paga.py in paga(adata, threshold, color, layout, layout_kwds, init_pos, root, labels, single_component, solid_edges, dashed_edges, transitions, fontsize, fontweight, fontoutline, text_kwds, node_size_scale, node_size_power, edge_width_scale, min_edge_width, max_edge_width, arrowsize, title, left_margin, random_state, pos, normalize_to_color, cmap, cax, colorbar, cb_kwds, frameon, add_pos, export_to_gexf, use_raw, colors, groups, plot, show, save, ax); 474 or (c in var_names); 475 ); --> 476 for c in colors; 477 ]; 478 else:. /nfs/med-bfx-activeprojects/weishwu/common/Anaconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/plotting/_tools/paga.py in <listcomp>(.0); 474 or (c in var_names); 475 ); --> 476 for c in colors; 477 ]; 478 else:. /nfs/med-bfx-activeprojects/weishwu/common/Anaconda3/envs/scan",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1497:849,message,message,849,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1497,1,['message'],['message']
Integrability,"It would be nice if we had a better way of handling upstream releases. E.g. when pandas makes a release it would be good that we had tested against their release candidates, or if we had a good process for dealing with bugs if they do occur. One think we could do, is defensively pin dependencies to below their current release series. I don't like doing this since I think it's pretty restrictive when most of the time we don't have issues. Maybe we could do this for breaking releases, but that wouldn't have prevented issues like #1917. It would be nice to automate the process of testing against upstream release candidates. Basically, when something comes out, we build against it so we can report issues early and don't have to deal with it in live releases. I'm not sure how to do this with `pip search` not working anymore.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1919:284,depend,dependencies,284,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1919,1,['depend'],['dependencies']
Integrability,"It would depend on what data was in the `Batch` column. HDF5 store are typed, so we can't store columns with mixed kinds of values. If the column's dtype is `object`, we check to see if it's string values, otherwise we say we don't know how to write it, since it could be any mix of things. What kinds of values did you have in `Batch`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1866#issuecomment-862113537:9,depend,depend,9,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1866#issuecomment-862113537,1,['depend'],['depend']
Integrability,"It'd be nice if some level of tab completion of argument names could be maintained for functions like `sc.pl.umap`. This came up recently in #455, and I find myself frequently misspelling (mostly mis-pluralizing) argument names like `color/ colors` and `gene_name/ gene_names`. I've given this a shot using `functools.wraps`, but no luck yet. Any ideas on if we could do this @flying-sheep?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/535:318,wrap,wraps,318,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/535,1,['wrap'],['wraps']
Integrability,"It's numpy.ndarray:; ```type(adata.X)```; ``` numpy.ndarray```; I guess it should be matrix? It's loaded once like this ; ```; path = '../count-genes/datafiles/all_counts.csv'; adata = sc.read(path, cache=True); ```; and then always manipulated with anndata interface. Maybe it should be transformed right after loading?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/220#issuecomment-408263645:258,interface,interface,258,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/220#issuecomment-408263645,1,['interface'],['interface']
Integrability,"It’s more code we have to maintain. If I had to decide between adding a common dependency, feature regression, or complex code, I’d go with the first one.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1344#issuecomment-666510858:79,depend,dependency,79,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1344#issuecomment-666510858,1,['depend'],['dependency']
Integrability,"I’m not categorically against it, but could you describe what issues you encounter trying to use scanpy on the data you want to use it on? E.g. naively, I’d think you’d just wrap your matrix in an AnnData object, then run `diffmap`:. ```pycon; >>> import scanpy as sc; >>> adata = sc.AnnData(my_matrix) # shape: (n_observations, n_variables); >>> sc.tl.diffmap(adata); ValueError: You need to run `pp.neighbors` first to compute a neighborhood graph.; ```. Then you just follow that advice and `repr` the object after to see what’s in there:. ```pycon; >>> sc.pp.neighbors(adata); >>> sc.tl.diffmap(adata); >>> adata; AnnData object ...; uns: diffmap_evals; obsm: X_diffmap; ```. Alternatively you read the docs: The [`diffmap` docs](https://scanpy.readthedocs.io/en/stable/generated/scanpy.tl.diffmap.html) point out both how to use `neighbors` …. > The width (“sigma”) of the connectivity kernel is implicitly determined by the number of neighbors used to compute the single-cell graph in [`neighbors()`](https://scanpy.readthedocs.io/en/stable/api/generated/scanpy.pp.neighbors.html#scanpy.pp.neighbors). To reproduce the original implementation using a Gaussian kernel, use `method=='gauss'` in [`neighbors()`](https://scanpy.readthedocs.io/en/stable/api/generated/scanpy.pp.neighbors.html#scanpy.pp.neighbors). To use an exponential kernel, use the default `method=='umap'`. Differences between these options shouldn’t usually be dramatic. … and where the results are pushed:. > … Sets the following fields:; > ; > `adata.obsm['X_diffmap']` : [`numpy.ndarray`](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray) (dtype `float`); > ; > > Diffusion map representation of data, which is the right eigen basis of the transition matrix with eigenvectors as columns.; >; > `adata.uns['diffmap_evals']` : [`numpy.ndarray`](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray) (dtype `float`); > ; > > Array of size (number of eigen vectors).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3054#issuecomment-2114964051:174,wrap,wrap,174,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3054#issuecomment-2114964051,1,['wrap'],['wrap']
Integrability,"I’m quite sure it has a resolution parameter, but at this point I’m also quite sure I’m messing up with modules and dependencies, both in this thread and on my local installation... about conda, I guess I’m one of the last around who hasn’t adopted it yet",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/97#issuecomment-440443151:116,depend,dependencies,116,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97#issuecomment-440443151,1,['depend'],['dependencies']
Integrability,"Jumping in on this conversation to ask a related question - I'm using Scanorama to integrate some datasets and generate an aligned low-dimensional embedding. I then subset the data to only look at specific clusters and want to re-make the UMAP/t-SNE plot. Do you usually re-do the integration to generate a new low-dimensional embedding matrix with Scanorama for the subsetted data? I know you can technically subset the original low-dimensional embedding matrix, but I thought it's preferable to re-do the embedding when you have a different subset of cells to capture more of the variance between those cells (re-select HVGs, etc). Any advice would be welcome - thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2162#issuecomment-1059551919:83,integrat,integrate,83,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2162#issuecomment-1059551919,2,['integrat'],"['integrate', 'integration']"
Integrability,"Just came across this - is this still relevant?; Scanpy as is does not feature a neat solution integrating with interactive interfaces which would allow to manually tag/select individual points from a plot - for such tasks, [holoviz](https://holoviz.org/) tools might be considered.; As sidenote, a heads-up about considering 2D representations with caution e.g. [here](https://www.sciencedirect.com/science/article/pii/S2405471223002090?via%3Dihub) - considering metrics instead of visual low-dimensional representations to detect or remove outliers might be considered as a viable alternative here in many cases :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1992#issuecomment-1798949475:95,integrat,integrating,95,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1992#issuecomment-1798949475,2,"['integrat', 'interface']","['integrating', 'interfaces']"
Integrability,"Just checked using this dockerfile, works flawlessly:. ```dockerfile; FROM continuumio/miniconda. RUN conda install python=3.8; RUN pip install flit>=3.1; RUN git clone https://github.com/theislab/scanpy.git; WORKDIR /scanpy; # Go to the mainline-pip branch if it hasn’t been merged into master yet; RUN git checkout mainline-pip || true; RUN FLIT_ROOT_INSTALL=1 flit install -s --dep=develop # Make development install of scanpy; # Make sure the dist-info folder has a plus in its name; RUN SCANPY_VERSION=$(python -c 'from importlib.metadata import version; print(version(""scanpy""))') && \; echo $SCANPY_VERSION | grep '+' &&; test -d /opt/conda/lib/python3.8/site-packages/scanpy-$SCANPY_VERSION.dist-info; # Install project that depends on scanpy; RUN pip install scvelo; # Make sure it’s still a dev install; RUN test -L /opt/conda/lib/python3.8/site-packages/scanpy; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1702#issuecomment-788200617:733,depend,depends,733,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1702#issuecomment-788200617,1,['depend'],['depends']
Integrability,"Just curious about the case of 'batch effect', it looks like to me library construction protocol/chemicals is main source of batch effects. However, if I use same protocols, sequencing platform and slight difference of sequencing depth for some sample, but in different time course, would you call it batch effect?. A more specific case is, if I have time-course data1 which has not geneX, however, I time-course data2 will have geneX till days later. In mnn correction, a prerequisite is same genes, will it filter out some genes meaningful?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/265#issuecomment-471439971:88,protocol,protocol,88,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-471439971,2,['protocol'],"['protocol', 'protocols']"
Integrability,"Just for reasons of practicality. I figured it would create a mess to visualize more than 10 plots with different groups. Also, you should have sufficient cells per group to make the kde calculation meaningful. That will depend on the number of cells as well though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/719#issuecomment-507260999:221,depend,depend,221,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/719#issuecomment-507260999,1,['depend'],['depend']
Integrability,"Just in general: the main route to ""improving the results"" is to make sure that your preprocessing yields a meaningful tSNE for the standard parameters. Then also DPT will perfectly do its job.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/25#issuecomment-310275631:26,rout,route,26,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/25#issuecomment-310275631,1,['rout'],['route']
Integrability,"Just my 2cents: ; I made really good experiences with Github actions.; * I find them easy to set-up and they run many (20-40?) jobs in parallel. ; * Really good integration with Github (e.g. upload to PyPI on release) ; * windows testing works well, but it is a pain to setup pycairo, see [here](https://github.com/icbi-lab/scirpy/blob/725664a22e6265643633d89a7f38ea3383ccab48/.github/workflows/test.yml#L34) and [here](https://github.com/pygobject/pycairo/issues/19#issuecomment-638716293). . Here's the [github actions script for scirpy](https://github.com/icbi-lab/scirpy/blob/master/.github/workflows/test.yml).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1358#issuecomment-674834154:161,integrat,integration,161,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1358#issuecomment-674834154,1,['integrat'],['integration']
Integrability,"Just pushing the updates now @LuckyMD 😄. One issue with the enrichment as is, is that `gprofiler-official` import name conflicts with the previous unofficial wrapper. I'm worried that this will break peoples environments if they're not aware of this. @liiskolb, do you have any thoughts on this?. Otherwise, I think this should be alright. I'd like to know if there'd be any interest in moving the utility function `rank_genes_groups_df` (added here) into a more central place. I personally use it anytime I use scanpys differential expression.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/467#issuecomment-483199474:158,wrap,wrapper,158,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467#issuecomment-483199474,1,['wrap'],['wrapper']
Integrability,"Just to add to @ivirshup's points. There are several examples of lower PCs containing batch effects rather than higher PCs. I've seen this many times, but this has also been report for e.g., ATAC data in the [SCALE paper](https://www.nature.com/articles/s41467-019-12630-7). > Is it correct to say that the each embedded PC is given equal weight in the neighbourhood graph?. I'm not entirely sure, but I don't think you can say this... higher PCs that explain less variance will contribute less to the total variance if you use them as an input to e.g., UMAP, t-SNE, or a kNN graph building algorithm. This is because the variance of the loadings is proportional to the total variance explained (unless a rescaling is used in scanpy by default?). Thus, the contribution of higher PCs to the distance calculations will be less discriminative between points. Putting these two aspects together, you can see exactly why you need batch integration methods. These effects affect leading PCs, and therefore contribute a lot to any distance calculation based on an embedding. You can't just remove the effects by filtering for only leading PCs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/872#issuecomment-822621611:932,integrat,integration,932,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872#issuecomment-822621611,1,['integrat'],['integration']
Integrability,"Just to add to this, PCA plots look fine with the newer `scikit-learn` I believe. Maybe it's the umap neighbourhood graph function depending on sklearn for something?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/654#issuecomment-494386051:131,depend,depending,131,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/654#issuecomment-494386051,2,['depend'],['depending']
Integrability,"Let's say I start a new session and generate a plot, then save it. All is fine. When I plot anything after the first save, a ""Do not localize"" message pops up and so does the previous ""save as"" window. The do not localize message window cannot be exited out of, and so I drag them to the upper right corner of the screen so they're out of the way. Then I exit out of the previous save as window, which by the way if you try to use it to save the current figure, it won't work. So I exit out of that window. But you can click the save button on the figure itself, then you can save. As I continue to plot and save figures, the do not localize windows pile up, and the chain of previous save as windows continues to grow. If you can follow my explanation, you can probably get the sense of how bothersome this can be. I have to go through the process of dragging the accumulating do not localize windows to the corner, and exiting out of the accumulating save as windows as I continue to generate and save figures. Has there been any similar experiences and if so how do I get rid of this situation?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/202:143,message,message,143,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/202,2,['message'],['message']
Integrability,"Like you say, the difference between this and `ingest` is joint PCA calculation vs asymmetric batch integration. This function is the first step in the `fastMNN` function, which I have found in some cases yields very sensible batch correction results. It would be awesome to see `multiBatchPCA` +/- `fastMNN` available in scanpy. I am aware of the python implementation of `mnncorrect`, but I think this still operates on expression values rather than a PCA representation (correct me if I am wrong..). Without going all the way the batch correction, `multiBatchPCA` is useful where different experiments have very different numbers of cells.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1289#issuecomment-671228353:100,integrat,integration,100,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1289#issuecomment-671228353,1,['integrat'],['integration']
Integrability,"Looking at the commits page I can see that the tests has been failing for some days already. . First was a problem with a notebook test (test_pbmc3k) that seems innocuous but should be addressed. This is related to release 1.4.3 (https://github.com/theislab/scanpy/commit/85acb6c8949d43d08a26437dceab4fa5db79e246). The commits are unrelated to the failing test so I assume that some dependency was updated . However, after this commit https://github.com/theislab/scanpy/commit/115d635bf950354509053d976b90c1db518bcffe more errors are found. But again, I don't see any relevant changes that will cause the problems. One of the errors is that statsmodels is using a deprecated module from scipy.misc:. ```; > from scipy.misc import factorial; E ImportError: cannot import name 'factorial'; ../../../virtualenv/python3.6.7/lib/python3.6/site-packages/statsmodels/distributions/edgeworth.py:7: ImportError; ```. This was introduced after scipy 1.3 was recently updated (https://github.com/statsmodels/statsmodels/issues/5759). It seems that currently, the only solution is to install statsmodels directly from the master branch. Or downgrade scipy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/661#issuecomment-495552166:383,depend,dependency,383,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661#issuecomment-495552166,1,['depend'],['dependency']
Integrability,"Looking at this again, now that I have gone through everything, I think we actually need to check types directly and shouldn't rely on `isbacked` because it is possible to do something like `adata.layers['foo'] = sparse_dataset(g_layer)` and this should also error our with a helpful message.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3048#issuecomment-2107583455:284,message,message,284,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3048#issuecomment-2107583455,1,['message'],['message']
Integrability,Looks super cool... but also like a heavy dependency. Do you think it would be worth using datashader when we are just looking for a simple additional function?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/575#issuecomment-479455800:42,depend,dependency,42,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575#issuecomment-479455800,2,['depend'],['dependency']
Integrability,"Louvain is being difficult to build since a new setuptools release dropped any python2 compatibility https://github.com/vtraag/louvain-igraph/issues/57. We've largely worked around this in #2063, by making louvain dependent tests optional. However, the paul15 PAGA test is difficult to extract louvian from. It checks hardcoded values based on the results of a louvain clustering. To adapt this test to use leiden, we would have to redo the tutorial and create new results. Or louvain building could be fixed, but the package is deprecated anyways.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2065:214,depend,dependent,214,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2065,1,['depend'],['dependent']
Integrability,"M / similar amount of free RAM. This happens both when using jupyter-notebook and python without jn. Error:; ```pytb; ---------------------------------------------------------------------------; OSError Traceback (most recent call last); ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, *args, **kwargs); 155 try:; --> 156 return func(elem, *args, **kwargs); 157 except Exception as e:. ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/anndata/_io/h5ad.py in read_group(group); 505 if ""h5sparse_format"" in group.attrs: # Backwards compat; --> 506 return SparseDataset(group).to_memory(); 507 . ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/anndata/_core/sparse_dataset.py in to_memory(self); 370 mtx = format_class(self.shape, dtype=self.dtype); --> 371 mtx.data = self.group[""data""][...]; 372 mtx.indices = self.group[""indices""][...]. h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/h5py/_hl/dataset.py in __getitem__(self, args); 572 fspace = selection.id; --> 573 self.id.read(mspace, fspace, arr, mtype, dxpl=self._dxpl); 574 . h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/h5d.pyx in h5py.h5d.DatasetID.read(). h5py/_proxy.pyx in h5py._proxy.dset_rw(). h5py/_proxy.pyx in h5py._proxy.H5PY_H5Dread(). OSError: Can't read data (file read failed: time = Sat Aug 1 13:27:54 2020; , filename = '/path.../filtered_gene_bc_matrices.h5ad', file descriptor = 47, errno = 5, error message = 'Input/output error', buf = 0x55ec782e9031, total read size = 7011, bytes this sub-read = 7011, bytes actually read = 18446744073709551615, offset = 0). During handling of the above exception, another exception occurred:. AnnDataReadError Traceback (most recent call last); <ipython-input-14-faac769583f8> in <module>; 17 #while True:; 18 #try:; ---> 19 adatas.append(",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1351:1368,wrap,wrapper,1368,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351,1,['wrap'],['wrapper']
Integrability,"MNN does take fairly long. There is a faster version of it, which runs on PCA I think though, but it's not in scanpy external. Before you didn't integrated anything, as the function thought you just have 1 batch the way you ran it. I would report your issue with bbknn on the BBKNN github repo directly. You may get better suggestions there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/873#issuecomment-543582157:145,integrat,integrated,145,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873#issuecomment-543582157,1,['integrat'],['integrated']
Integrability,Make sure dependencies are up to date in travis builds,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1320:10,depend,dependencies,10,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1320,1,['depend'],['dependencies']
Integrability,"Matplotlib 3.4 has dropped 3.6 support. Since matplotlib is our most painful dependency (reliably causes test failures when it updates), it's a great time to drop 3.6.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1697#issuecomment-809011473:77,depend,dependency,77,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1697#issuecomment-809011473,1,['depend'],['dependency']
Integrability,Maybe I could throw in another ID mapping tool. [BED](https://f1000research.com/articles/7-195/v1) is pretty good. More comprehensive than Biomart and quicker too. It is however a local implementation that runs in a docker container. The image is updated every month or so. At the moment I run a container internally here... but maybe we could make a webserver out of this which can be directly integrated with scanpy?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/242#issuecomment-458065068:395,integrat,integrated,395,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242#issuecomment-458065068,1,['integrat'],['integrated']
Integrability,Maybe it would be a good idea to have a separate repo of `rpy2` and `anndata2ri` wrappers for `R` methods that we want to run in scanpy workflows. Would you be interested in sth like that? I could create a separate repo in theislab github? Something like `www.github.com/theislab/Rforscanpy`?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1068#issuecomment-590116116:81,wrap,wrappers,81,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-590116116,1,['wrap'],['wrappers']
Integrability,Message from highly_variable_genes,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/411:0,Message,Message,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/411,1,['Message'],['Message']
Integrability,Minimum dependency test job,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816:8,depend,dependency,8,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816,1,['depend'],['dependency']
Integrability,"Much of the spatial data is stored in `uns`, which does not get combined by default. There is an example of concatenating visium datasets [in the tutorial](https://scanpy-tutorials.readthedocs.io/en/latest/spatial/integration-scanorama.html#Data-integration) and more information on concatenating `.uns` [in the latest anndata docs](https://anndata.readthedocs.io/en/latest/concatenation.html#merging-uns).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1254#issuecomment-635107317:214,integrat,integration-scanorama,214,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1254#issuecomment-635107317,2,['integrat'],"['integration', 'integration-scanorama']"
Integrability,"My intuition would be neighbor finding would take more time as dataset size increases. What exact fraction of the time will depend a lot on number of samples, number of features, and possibly distance metric. If you're investigating yourself, I think trying `line_profiler`'s `%lprun` on `umap.UMAP.fit` would be a good bet. I'd also bet that they'd have a better idea over at `UMAP` or Pynndescent.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/810#issuecomment-528383146:124,depend,depend,124,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/810#issuecomment-528383146,2,['depend'],['depend']
Integrability,"My main reasoning was that:. * Use dask array/ delayed is more simple; * One can go from dask array/ dask delayed -> `Future` with `client.compute`, but the other way around isn't really possible since `Future`s kick off computation immediately. So I'd like to use the higher level interface as much as possible, and only use lower level when necessary.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2856#issuecomment-1980790332:282,interface,interface,282,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2856#issuecomment-1980790332,2,['interface'],['interface']
Integrability,"My priority are intuitive semantics so people can add or bump dependencies without 100% understanding the algorithm of the minimum dependency script. So I can think of options:. 1. Each version must be fully specified (`>=1.2.0`, not `>=1.2`). The script installs exactly the specified minimum version. Implementation: Would be quickly done now, just check the job run and change `matplotlib>=3.6` to `matplotlib>=3.6.3` and so on. Effect: whenever we bump something, we probably need to bump more things, which might sometimes be painful. The minimum versions will be more accurate, as we know that the exact versions specified successfully run out test suite. 4. We maintain a list of all dependencies we have together with data about which version segment denotes the patch version (i.e. for semver it’s the third, for calendar ver, it’s nothing), then modify versions based on that knowledge (e.g. semver `>=1.2.3` → `>=1.2.3, <1.3`). Implementation: Each newly added dependency needs to be added to that list. Effect: This would be basically a more powerful (able to specify minimum patch) and obvious version of what you’re doing now (explicit data instead of the presence of a patch version indicating if something is semver or not). In both versions, there’s no hidden semantics in `>=1.2` that would distinguish it from `>=1.2.0`, which is what I’m after. What does your experience while implementing this so far say to these? Any other ideas?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1943497240:62,depend,dependencies,62,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1943497240,8,['depend'],"['dependencies', 'dependency']"
Integrability,"My version of scanpy:; scanpy 1.8.1 pyhd8ed1ab_0 conda-forge; I'm working on a linux system based server, and uses miniconda3 for environment management.; After some changes in my environment, I tried to run the routine process of my analysis.; But when running sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40), I encountered the following error: . > Traceback (most recent call last):; File ""/data1/exhaustT/process.py"", line 118, in <module>; sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40); File ""/data1/exhaustT/miniconda3/lib/python3.9/site-packages/scanpy/neighbors/ __init__.py"", line 139, in neighbors; neighbors.compute_neighbors(; File ""/data1/exhaustT/miniconda3/lib/python3.9/site-packages/scanpy/neighbors/ __init__.py"", line 808, in compute_neighbors; self._distances, self._connectivities = _compute_connectivities_umap(; File ""/data1/exhaustT/miniconda3/lib/python3.9/site-packages/scanpy/neighbors/ __init__.py"", line 387, in _compute_connectivities_umap; from umap.umap_ import fuzzy_simplicial_set; File ""/data1/exhaustT/umap.py"", line 48, in <module>; sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40); File ""/data1/exhaustT/miniconda3/lib/python3.9/site-packages/scanpy/neighbors/ __init__.py"", line 139, in neighbors; neighbors.compute_neighbors(; File ""/data1/exhaustT/miniconda3/lib/python3.9/site-packages/scanpy/neighbors/ __init__.py"", line 808, in compute_neighbors; self._distances, self._connectivities = _compute_connectivities_umap(; File ""/data1/exhaustT/miniconda3/lib/python3.9/site-packages/scanpy/neighbors/ __init__.py"", line 387, in _compute_connectivities_umap; from umap.umap_ import fuzzy_simplicial_set; ModuleNotFoundError: No module named 'umap.umap_'; 'umap' is not a package. I've tried to re-install umap-learn from conda-forge, and/or simply pip install umap, neither worked for me. #update, the problem found to be that I named my own script umap.py, even though it's in a different direction, it still caused trouble.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1987:212,rout,routine,212,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1987,1,['rout'],['routine']
Integrability,"Need a file `normalization.py` in `scanpy.preprocessing`. In that, there should be cleaned-up versions of `normalize_per_cell` and `normalize_per_cell_weinreib17` without any recursive calls and no calls to `filter_cells`. The new names of these function should be `normalize_total` and `normalize_quantile`. There should be a backup version of `normalize_per_cell` that behaves exactly as it currently does for backwards compatibility, which wraps `filter_cells` and `normalize_per_cell`. In the ideal case, `normalize_total` would wrap `normalize_quantile(quantile=1)`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/301:443,wrap,wraps,443,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/301,2,['wrap'],"['wrap', 'wraps']"
Integrability,"NeighborsTransformer; - [x] unify selection: algorithm+backend, metric, connectivity (maybe separate out connectivity); - [x] figure out how to do connectivities: can mode be changed after fit? *no, we just use umap connectivities as before*; - [x] check out where we have coverage; - is there paga specific stuff? *not in the parts I changed*; - gauss: dense matrix when knn=True (“build a symmetric mask”, …) *not covered, but also the logic shouldn’t have changed*; - pre-computed in umap transformer; handling of disconnected components in umap (indices == -1) *replaced with our own implementation, see below*; - add tests; - [x] pyknndescent (we already depend on it through umap). Maybe in another PR?. - [ ] maybe store index in unified way?; - [ ] for umap: use `UMAP(precomputed_knn=...)` instead of `compute_connectivities`?; - [ ] unifiy transformer args, e.g. verbose. ## Implementation. ### The way it used to be. Our default use case was basically a thin wrapper around umap’s `nearest_neighbors` function, which in turn is a thin wrapper around PyNNDescent. We did some special casing around euclidean distance and small data sizes. That special casing reduced our test coverage: . - we don’t actually test umap’s pynndescent codepath at all (just the fast `precomputed` path for small data); - umap’s `precomputed` code does some weird things to its knn `indices` array, which we don’t test for: ; ![grafik](https://github.com/scverse/scanpy/assets/291575/7f36cafe-98fb-48cc-9e35-3972fad65a3e). The logic was:. - if small data (<8000) and euclidean metric or knn==False, calculate `pairwise_distances`; - if knn=True, then sparsify that matrix by pulling out KNN using `_get_indices_distances_from_dense_matrix` and converting that into a sparse one; - if not, run `umap.nearest_neighbors`.; - if even smaller data (<4000), calculate `pairwise_distances` like above and run `umap.nearest_neighbors` with `metric='precomputed'`. its internal logic then does the same as `_get_indices_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2536:2459,wrap,wrapper,2459,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536,2,['wrap'],['wrapper']
Integrability,"Nice! Tests should also be run by Travis, shouldn't they? Or have we missed out on demanding dependencies and your tests won't run through for that reason? If so, please point me to it and I'll make sure that Travis actually runs the tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/439#issuecomment-456635443:93,depend,dependencies,93,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/439#issuecomment-456635443,1,['depend'],['dependencies']
Integrability,"Nice! Thanks!. On Mon, 8 Jun 2020, 10:46 giovp, <notifications@github.com> wrote:. > @vitkl <https://github.com/vitkl> now multiple samples are supported, see; > here; > <https://scanpy-tutorials.readthedocs.io/en/latest/spatial/integration-scanorama.html>; > for description on how to use the new concat strategy; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/1158#issuecomment-640496084>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AFMFTVZVVWII7Z7Q34ZPTQ3RVSXOVANCNFSM4MEXUAPQ>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1158#issuecomment-640632513:229,integrat,integration-scanorama,229,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1158#issuecomment-640632513,1,['integrat'],['integration-scanorama']
Integrability,"No problem, I'll change it to your preferred style. I don't think it's a problem to add the chunking but I'll need to test it for sparse matrices. Just to clarify, what I meant by ""more functional style"" is something like this:. ```; processed_data = raw_data.log1p().normalize(options...).some_other_method(options...); ```. That is, it allows a [functional programming](https://en.wikipedia.org/wiki/Functional_programming) style. Similar to libraries like `scikit-learn` (e.g. `fit()` returns `self` so you can immediately call another method) or `keras` (see the [functional API guide](https://keras.io/getting-started/functional-api-guide/). But as you say, that might be a dramatic change in coding style for your library. I find it can lead to simpler code but that's a personal preference. The above examples are notable because they allow both functional and declarative styles of coding, depending on the user.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/191#issuecomment-403242179:898,depend,depending,898,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191#issuecomment-403242179,2,['depend'],['depending']
Integrability,"No recent version of legacy-api-wrap has been uploaded to conda. So, we can't make a conda release of scanpy 1.10. * https://github.com/conda-forge/scanpy-feedstock/pull/15. Since it's a single file with a single function, I'm very up for vendor-ing it:. * https://github.com/scverse/anndata/issues/1301. cc: @flying-sheep",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2966:32,wrap,wrap,32,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2966,1,['wrap'],['wrap']
Integrability,"No worries, feel free to rename :) Monday, 20 June 2022, 09:11PM +02:00 from Isaac Virshup ***@***.*** :. ***@***.*** , would it be fair to retitle this something like ""Generate BioContainer images"", or should that be a separate issue?; >—; >Reply to this email directly, view it on GitHub , or unsubscribe .; >You are receiving this because you were mentioned. Message ID: @ github . com>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2281#issuecomment-1160765884:362,Message,Message,362,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2281#issuecomment-1160765884,1,['Message'],['Message']
Integrability,"No worries. Arguably scanpy should check for common compression formats with V2 (and no compression for V3) while reading the input files, or at least provide a more informative traceback, especially if data on GEO routinely includes these formats",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1408#issuecomment-689676689:215,rout,routinely,215,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1408#issuecomment-689676689,1,['rout'],['routinely']
Integrability,"No worries. It can be something like:. > Subset of groups, e.g. ['g1', 'g2', 'g3'], for which the list of DE genes should be computed. Each group of cells is always compared to the remaining cells, even if they don't belong to the subset of groups. However, it would be probably more useful to change the API and to implement subsetting of the groups through the 'groups' parameter. I think this would be more intuitive, also together with the use of the 'reference' parameter.; In particular, because retrieving of DE genes for specific groups can be more easily done with the [get](https://scanpy.readthedocs.io/en/stable/api/scanpy.get.rank_genes_groups_df.html#scanpy.get.rank_genes_groups_df) interface.; But there may be other use cases I haven't considered in which the actual implementation may be useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/842#issuecomment-531838315:698,interface,interface,698,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/842#issuecomment-531838315,2,['interface'],['interface']
Integrability,"No, not at all. It depends how you calculate highly variable genes. If you don't use the `batch` parameter, then it always works fine. If you use the `batch` parameter, it outputs `adata.var['highly_variable_genes_intersection']` and `adata.var['highly_variable_genes_nbatches']` which is information on how many batches a particular HVG is shared by. In the intersection field the genes are labelled as `True` that are shared by all batches. If no HVG is shared by all batches, this will be `False` for all genes. In that case you can define `adata.var['highly_variable']` as you like... usually you would do that depending on the nbatches output.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/935#issuecomment-559558621:19,depend,depends,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/935#issuecomment-559558621,2,['depend'],"['depending', 'depends']"
Integrability,"No, the matplotlib error message was really confusing... the 'on data' and 'right margin' locations are scanpy features and should be in the error message... wanted to do this anyways. :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/88#issuecomment-366300686:25,message,message,25,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/88#issuecomment-366300686,2,['message'],['message']
Integrability,"No, those “invalid instruction” errors pop up sometimes. I think they’re caused by some dependency being compiled for an instruction set that not all GitHub runners support. Ways to deal with it:. 1. just restart until it works (annoying, but not much work); 2. figure out broken dependency, then; 1. if the wheel on PyPI is broken, raise an issue upstream; 2. if we compile it in the runner ourselves, set a compile flag to make it only use instructions that are compatible with all runners (i.e. not `-m arch=native` but select [an older architecture](https://en.wikipedia.org/wiki/X86-64#Microarchitecture_levels))",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2742#issuecomment-1814222794:88,depend,dependency,88,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2742#issuecomment-1814222794,2,['depend'],['dependency']
Integrability,"None, there is no problem on our side right? @fidelram said in https://github.com/theislab/scanpy/pull/661#issuecomment-496144015 to wait for matplotlib/matplotlib#14298 to be fixed. It seems to be in matplotlib’s 3.1.2 milestone, so we can maybe just set the dependency to “matplotlib == 3.0.0 or matplotlib >= 3.1.2”",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/787#issuecomment-531732473:260,depend,dependency,260,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/787#issuecomment-531732473,1,['depend'],['dependency']
Integrability,"Not completely sure if this is doing what I intended. I added the `-U` so dependencies would be upgraded, but numpy still isn't being upgraded as shown by these warnings:. ```; umap-learn 0.4.6 has requirement numpy>=1.17, but you'll have numpy 1.15.4 which is incompatible.; scvi 0.6.6 has requirement numpy>=1.16.2, but you'll have numpy 1.15.4 which is incompatible.; ```. Not sure why this is happening. I'd prefer if we didn't have to manually specify the dependencies of our dependencies. Any ideas @flying-sheep?. ------------------. Updating pip doesn't seem to do anything (maybe it has to do with ""editable mode""?). --------------------. An easy fix is just to add a version requirement on `numpy`, but I really feel like dependency resolution should be dealing with that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1320#issuecomment-659867855:74,depend,dependencies,74,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1320#issuecomment-659867855,8,['depend'],"['dependencies', 'dependency']"
Integrability,"Note also, when I follow the same protocol for a similar dataset (different timepoint for sequencing), regressing out does not cause this problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/230#issuecomment-411603192:34,protocol,protocol,34,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230#issuecomment-411603192,1,['protocol'],['protocol']
Integrability,"Numba can’t correctly detect when a threading backend is available. This was fine in the past, since `pynndescent`/ `umap` were forcing a `workqueue` backend which is always available. `pynndescent` 0.5.3 recently came out, and started favoring the `tbb` backend. This is a problem since numba thinks this backend is available on our CI (and on my HPC), but it’s actually not. I also can’t get tbb to even install in a way numba sees in on these systems. If tbb isn’t actually available, but numba detects it we get errors with horrible tracebacks anytime parallelized numba code is used https://github.com/lmcinnes/pynndescent/issues/129. These tracebacks are so horrible they break our CI further https://github.com/pytest-dev/pytest-nunit/issues/47. So what do we do?. ## Possible solutions:. * Pin pynndescent below 0.5.3. This makes pynndescent a required dependency. We’ve previously avoided this since it would change results for people using `umap<0.4` (e.g. anyone with `scvelo` installed) who did not explicitly install pynndescent. However, given the lack of complaints around umap results changing as dependencies have increased, this may not be so bad. It would be great if we could constrain the version without having it be a dependency. This would be similar to what's possible with `pip` and [constraints files](https://pip.pypa.io/en/stable/user_guide/#constraints-files), I don't see how one would be able to specify this for a package. I don't think it's possible, but maybe I'm missing something about the version string syntax. * Make sure the numba threading layer is `“workqueue”` after pynndescent is imported. This is tricky. pynndescent<0.5.3 takes a long time to import, so we don’t want to do this at the top level. So we would need to add a check after everytiem pynndescent could possibly be imported to check that it didn’t set the threading backend to anything else. My understanding of the numba threading system is that once you’ve called for parallel compilation, y",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1931:861,depend,dependency,861,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1931,1,['depend'],['dependency']
Integrability,"OK, as I wrote here: https://github.com/scverse/scanpy/issues/2359#issuecomment-1909651108. > We have no native code in Scanpy, so we don’t cause segfaults. If there’s anything we can mitigate, we will, if someone demonstrates a reproducible problem with up-to-date dependencies. Reinstalling your environment often helps. If not, please give us a way to completely reproduce this just from copyable code, i.e. . 1. a lockfile (environment.yaml or requirements.txt) containing the exact versions of everything in your environment; 2. a block of code that throws the error when run in that environment (code should download the data necessary to reproduce the issue). Then we can help",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2361#issuecomment-1909658183:266,depend,dependencies,266,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2361#issuecomment-1909658183,1,['depend'],['dependencies']
Integrability,"OK, issues like this are almost always either memory or dependency problems: Something’s miscompiled or compiled for the wrong architecture (e.g. a newer CPU than you have) or simply buggy. We have no native code in Scanpy, so we don’t cause segfaults. If there’s anything we can mitigate, we will, if someone demonstrates a reproducible problem with up-to-date dependencies",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2359#issuecomment-1909651108:56,depend,dependency,56,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2359#issuecomment-1909651108,4,['depend'],"['dependencies', 'dependency']"
Integrability,"OK, so now the question is: should this become part of legacy-api-wrap?. I’d rather have the API fixed once than using multiple decorators. I think It’s clearer to see what the new API is like if you don’t have to think about the order of multiple decorators being applied. Also, I think. ```py; @renamed_args(new=""old""); ```. feels more natural than. ```py; @deprecated_arg_names({""old"": ""new""}); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/474#issuecomment-471489422:66,wrap,wrap,66,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474#issuecomment-471489422,2,['wrap'],['wrap']
Integrability,"OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2404:31,rout,routine,31,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2404,1,['rout'],['routine']
Integrability,"Oh damn, I was meant to send a PR from the web interface but didn't notice the default is to commit directly. Sorry about that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/740#issuecomment-519249007:47,interface,interface,47,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/740#issuecomment-519249007,1,['interface'],['interface']
Integrability,"Oh, by version numbers I meant not just `anndata` but the dependencies as well. I'm wondering in particular about the version of `h5py`. The output of something like [`sinfo(dependencies=True)`](https://pypi.org/project/sinfo/) would be great. If you try writing to a different path, are you able too? It kind of looks like you're writing to a file that already exists, though that should just overwrite the file.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1275#issuecomment-647275662:58,depend,dependencies,58,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1275#issuecomment-647275662,2,['depend'],['dependencies']
Integrability,Okay all done. `flit install -s` was getting too messy as some dependency installs scanpy and then things can’t be symlinked …. better leave the `pip install -r` in temporarily,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1527#issuecomment-778328479:63,depend,dependency,63,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-778328479,1,['depend'],['dependency']
Integrability,"Okay, this is weird. My current theory is that it's a bug in pip. It looks like if multiple packages depend on a single dependency, pip doesn't necessarily check if all requirements are satisfied. It just checks if one packages version requirements are. It seems non-deterministic which package is used to check. --------------------------------------. I was working on an example, but then I just found this: https://github.com/pypa/pip/issues/8218. There's an unstable feature for this (`--unstable-feature=resolver`), which does work, but I think adding a requirement for numpy is a bit more justifiable.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1320#issuecomment-659921938:101,depend,depend,101,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1320#issuecomment-659921938,2,['depend'],"['depend', 'dependency']"
Integrability,"One important thing: pip supports self-depending. I’ve written dep lists like. ```toml; [project]; name = 'myproj'. [project.optional-dependencies]; # myproj’s exported testing tools depend on those:; testing = ['pytest-postgresql']; # to run our package’s tests, we need:; test = ['pytest', 'myproj[testing]']; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2211#issuecomment-1088715295:39,depend,depending,39,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2211#issuecomment-1088715295,3,['depend'],"['depend', 'dependencies', 'depending']"
Integrability,"One of the aims of scanpy is to be self-contained and easy-to-install for users and also to be easy to maintain by the developers. Heavy dependencies like louvain and python-igraph are already troublesome, expecting users to have rpy2 + proper R installation + Bioconductor + scran would risk smooth user experience and easy maintainability. I was wondering whether it makes sense to have a community-maintained `scanpy-contrib` or `scanpy-extensions` repository (and python package) similar to https://github.com/keras-team/keras-contrib ? There are also couple of things I have in mind like `sc.pl.netsne(adata, anotheradata)` for embedding unseen samples via parametric tSNE, or `sc.tl.simlr` and `sc.pl.simlr` for [SIMLR](https://github.com/BatzoglouLabSU/SIMLR) via RPy2 bridge... . These are popular requests for Scanpy and people expect the same convenient API and an easy integration with AnnData objects. However, they will probably not be included in the mainstream Scanpy because of the reasons I mentioned above. What do you think @falexwolf and @flying-sheep ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/125#issuecomment-381980880:137,depend,dependencies,137,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-381980880,6,"['bridg', 'depend', 'integrat']","['bridge', 'dependencies', 'integration']"
Integrability,"One other thing. We'd like to have two new convenience functions:. ```; def neighbors_update(adata, adata_new); def umap_update(adata, adata_new); ```. The first maps the new data into the existing neighbor graph based on the chosen latent representation. The second maps the new data into the existing UMAP embedding. For the second function, one just needs to find a good way of wrapping; ```; model = umap.UMAP(seed=1234); model.fit(X); model.transform(new_X); ```; For the first, I'm not quite sure how easy it is easy. I'm using `pynndescent` for it, which will become UMAP's dependency at some point, but isn't yet. Maybe what UMAP does internally is already sufficient, but I don't know. Can you investigate and if it's easy cover in this PR? If it's tricky, let's wait for another PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/576#issuecomment-479424924:381,wrap,wrapping,381,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576#issuecomment-479424924,2,"['depend', 'wrap']","['dependency', 'wrapping']"
Integrability,"One potential solution is to convert the integrated connectivity matrix, C, into a pseudo-distance matrix (1-C) (this probably won't work for datasets much larger than 10k cells due to memory limitations) and run t-SNE with the 'precomputed' metric on that fake distance matrix. If scanpy's t-SNE wrapper does not allow passing a precomputed distance matrix, I would recommend using the sklearn implementation directly:. https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1370#issuecomment-689005446:41,integrat,integrated,41,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1370#issuecomment-689005446,4,"['integrat', 'wrap']","['integrated', 'wrapper']"
Integrability,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; $ git checkout 1.7.x; $ git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; $ git cherry-pick -m1 5fc12f4a918e21f0c57937b787d52040db046f01; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; $ git commit -am 'Backport PR #1587: Attach failing plots to CI results'; ```. 4. Push to a named branch :. ```; git push YOURFORK 1.7.x:auto-backport-of-pr-1587-on-1.7.x; ```. 5. Create a PR against branch 1.7.x, I would have named this PR:. > ""Backport PR #1587 on branch 1.7.x"". And apply the correct labels and milestones. Congratulation you did some good work ! Hopefully your backport PR will be tested by the continuous integration and merged soon!. If these instruction are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1587#issuecomment-787808128:885,integrat,integration,885,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1587#issuecomment-787808128,1,['integrat'],['integration']
Integrability,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; $ git checkout 1.7.x; $ git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; $ git cherry-pick -m1 ce508c4084e8df272163f4e17136386cfaec2605; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; $ git commit -am 'Backport PR #1768: Fix correlation plot test for new version of matplotlib'; ```. 4. Push to a named branch :. ```; git push YOURFORK 1.7.x:auto-backport-of-pr-1768-on-1.7.x; ```. 5. Create a PR against branch 1.7.x, I would have named this PR:. > ""Backport PR #1768 on branch 1.7.x"". And apply the correct labels and milestones. Congratulation you did some good work ! Hopefully your backport PR will be tested by the continuous integration and merged soon!. If these instruction are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1768#issuecomment-809014499:906,integrat,integration,906,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1768#issuecomment-809014499,1,['integrat'],['integration']
Integrability,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; $ git checkout 1.7.x; $ git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; $ git cherry-pick -m1 f7279f6342f1e4a340bae2a8d345c1c43b2097bb; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; $ git commit -am 'Backport PR #1679: enables highly_variable_genes_seurat_v3 to accept pseudocounts'; ```. 4. Push to a named branch :. ```; git push YOURFORK 1.7.x:auto-backport-of-pr-1679-on-1.7.x; ```. 5. Create a PR against branch 1.7.x, I would have named this PR:. > ""Backport PR #1679 on branch 1.7.x"". And apply the correct labels and milestones. Congratulation you did some good work ! Hopefully your backport PR will be tested by the continuous integration and merged soon!. If these instruction are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1679#issuecomment-814587648:913,integrat,integration,913,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1679#issuecomment-814587648,1,['integrat'],['integration']
Integrability,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; git checkout 1.10.x; git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; git cherry-pick -x -m1 5c0e89e99dc2461c654c549435a73f547f3573ce; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; git commit -am 'Backport PR #3339: Add PYI lints'; ```. 4. Push to a named branch:. ```; git push YOURFORK 1.10.x:auto-backport-of-pr-3339-on-1.10.x; ```. 5. Create a PR against branch 1.10.x, I would have named this PR:. > ""Backport PR #3339 on branch 1.10.x (Add PYI lints)"". And apply the correct labels and milestones. Congratulations — you did some good work! Hopefully your backport PR will be tested by the continuous integration and merged soon!. Remember to remove the `Still Needs Manual Backport` label once the PR gets merged. If these instructions are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3339#issuecomment-2457653625:881,integrat,integration,881,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3339#issuecomment-2457653625,1,['integrat'],['integration']
Integrability,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; git checkout 1.10.x; git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; git cherry-pick -x -m1 5d5d873b1fb0353089569f85580b43437df9c6cd; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; git commit -am 'Backport PR #3104: [pre-commit.ci] pre-commit autoupdate'; ```. 4. Push to a named branch:. ```; git push YOURFORK 1.10.x:auto-backport-of-pr-3104-on-1.10.x; ```. 5. Create a PR against branch 1.10.x, I would have named this PR:. > ""Backport PR #3104 on branch 1.10.x ([pre-commit.ci] pre-commit autoupdate)"". And apply the correct labels and milestones. Congratulations — you did some good work! Hopefully your backport PR will be tested by the continuous integration and merged soon!. Remember to remove the `Still Needs Manual Backport` label once the PR gets merged. If these instructions are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3104#issuecomment-2160085624:929,integrat,integration,929,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3104#issuecomment-2160085624,1,['integrat'],['integration']
Integrability,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; git checkout 1.10.x; git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; git cherry-pick -x -m1 8d046ff37e024ae88eadfb22ea8fd142a6b95aa1; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; git commit -am 'Backport PR #3093: [pre-commit.ci] pre-commit autoupdate'; ```. 4. Push to a named branch:. ```; git push YOURFORK 1.10.x:auto-backport-of-pr-3093-on-1.10.x; ```. 5. Create a PR against branch 1.10.x, I would have named this PR:. > ""Backport PR #3093 on branch 1.10.x ([pre-commit.ci] pre-commit autoupdate)"". And apply the correct labels and milestones. Congratulations — you did some good work! Hopefully your backport PR will be tested by the continuous integration and merged soon!. Remember to remove the `Still Needs Manual Backport` label once the PR gets merged. If these instructions are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3093#issuecomment-2146729991:929,integrat,integration,929,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3093#issuecomment-2146729991,1,['integrat'],['integration']
Integrability,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; git checkout 1.9.x; git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; git cherry-pick -x -m1 05dcf68f32ce255447ea804de55babefb3c47c92; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; git commit -am 'Backport PR #2753: [pre-commit.ci] pre-commit autoupdate'; ```. 4. Push to a named branch:. ```; git push YOURFORK 1.9.x:auto-backport-of-pr-2753-on-1.9.x; ```. 5. Create a PR against branch 1.9.x, I would have named this PR:. > ""Backport PR #2753 on branch 1.9.x ([pre-commit.ci] pre-commit autoupdate)"". And apply the correct labels and milestones. Congratulations — you did some good work! Hopefully your backport PR will be tested by the continuous integration and merged soon!. Remember to remove the `Still Needs Manual Backport` label once the PR gets merged. If these instructions are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2753#issuecomment-1809942763:924,integrat,integration,924,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2753#issuecomment-1809942763,1,['integrat'],['integration']
Integrability,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; git checkout 1.9.x; git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; git cherry-pick -x -m1 330a099ffe76286f0f047387701af7e9fd58831a; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; git commit -am 'Backport PR #2838: Fix pytest 8 compat'; ```. 4. Push to a named branch:. ```; git push YOURFORK 1.9.x:auto-backport-of-pr-2838-on-1.9.x; ```. 5. Create a PR against branch 1.9.x, I would have named this PR:. > ""Backport PR #2838 on branch 1.9.x (Fix pytest 8 compat)"". And apply the correct labels and milestones. Congratulations — you did some good work! Hopefully your backport PR will be tested by the continuous integration and merged soon!. Remember to remove the `Still Needs Manual Backport` label once the PR gets merged. If these instructions are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2838#issuecomment-1923260036:888,integrat,integration,888,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2838#issuecomment-1923260036,2,['integrat'],['integration']
Integrability,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; git checkout 1.9.x; git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; git cherry-pick -x -m1 47664d83a7bc47756356b907e5719076ab187361; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; git commit -am 'Backport PR #2784: [pre-commit.ci] pre-commit autoupdate'; ```. 4. Push to a named branch:. ```; git push YOURFORK 1.9.x:auto-backport-of-pr-2784-on-1.9.x; ```. 5. Create a PR against branch 1.9.x, I would have named this PR:. > ""Backport PR #2784 on branch 1.9.x ([pre-commit.ci] pre-commit autoupdate)"". And apply the correct labels and milestones. Congratulations — you did some good work! Hopefully your backport PR will be tested by the continuous integration and merged soon!. Remember to remove the `Still Needs Manual Backport` label once the PR gets merged. If these instructions are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2784#issuecomment-1862463379:924,integrat,integration,924,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2784#issuecomment-1862463379,1,['integrat'],['integration']
Integrability,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; git checkout 1.9.x; git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; git cherry-pick -x -m1 4f4b1c3a655546d981360bcce625d354a4291385; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; git commit -am 'Backport PR #2811: [pre-commit.ci] pre-commit autoupdate'; ```. 4. Push to a named branch:. ```; git push YOURFORK 1.9.x:auto-backport-of-pr-2811-on-1.9.x; ```. 5. Create a PR against branch 1.9.x, I would have named this PR:. > ""Backport PR #2811 on branch 1.9.x ([pre-commit.ci] pre-commit autoupdate)"". And apply the correct labels and milestones. Congratulations — you did some good work! Hopefully your backport PR will be tested by the continuous integration and merged soon!. Remember to remove the `Still Needs Manual Backport` label once the PR gets merged. If these instructions are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2811#issuecomment-1893536608:924,integrat,integration,924,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2811#issuecomment-1893536608,1,['integrat'],['integration']
Integrability,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; git checkout 1.9.x; git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; git cherry-pick -x -m1 585f58c9e4dd82dd7809a831538c4e230b008818; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; git commit -am 'Backport PR #2841: [pre-commit.ci] pre-commit autoupdate'; ```. 4. Push to a named branch:. ```; git push YOURFORK 1.9.x:auto-backport-of-pr-2841-on-1.9.x; ```. 5. Create a PR against branch 1.9.x, I would have named this PR:. > ""Backport PR #2841 on branch 1.9.x ([pre-commit.ci] pre-commit autoupdate)"". And apply the correct labels and milestones. Congratulations — you did some good work! Hopefully your backport PR will be tested by the continuous integration and merged soon!. Remember to remove the `Still Needs Manual Backport` label once the PR gets merged. If these instructions are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2841#issuecomment-1929072209:924,integrat,integration,924,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2841#issuecomment-1929072209,1,['integrat'],['integration']
Integrability,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; git checkout 1.9.x; git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; git cherry-pick -x -m1 5ccce795b19a5aa59a6b1f1c3552884ed6fc94d1; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; git commit -am 'Backport PR #2544: [pre-commit.ci] pre-commit autoupdate'; ```. 4. Push to a named branch:. ```; git push YOURFORK 1.9.x:auto-backport-of-pr-2544-on-1.9.x; ```. 5. Create a PR against branch 1.9.x, I would have named this PR:. > ""Backport PR #2544 on branch 1.9.x ([pre-commit.ci] pre-commit autoupdate)"". And apply the correct labels and milestones. Congratulations — you did some good work! Hopefully your backport PR will be tested by the continuous integration and merged soon!. Remember to remove the `Still Needs Manual Backport` label once the PR gets merged. If these instructions are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2544#issuecomment-1619899808:924,integrat,integration,924,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2544#issuecomment-1619899808,1,['integrat'],['integration']
Integrability,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; git checkout 1.9.x; git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; git cherry-pick -x -m1 86dc4d5d96eb7547833e7805ea2f7d603bd3ba2d; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; git commit -am 'Backport PR #2779: Fix anndata warnings'; ```. 4. Push to a named branch:. ```; git push YOURFORK 1.9.x:auto-backport-of-pr-2779-on-1.9.x; ```. 5. Create a PR against branch 1.9.x, I would have named this PR:. > ""Backport PR #2779 on branch 1.9.x (Fix anndata warnings)"". And apply the correct labels and milestones. Congratulations — you did some good work! Hopefully your backport PR will be tested by the continuous integration and merged soon!. Remember to remove the `Still Needs Manual Backport` label once the PR gets merged. If these instructions are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2779#issuecomment-1858121974:890,integrat,integration,890,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2779#issuecomment-1858121974,1,['integrat'],['integration']
Integrability,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; git checkout 1.9.x; git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; git cherry-pick -x -m1 95206dc54c8bb0d9d478f09f47dff9477a5c58c4; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; git commit -am 'Backport PR #2704: [pre-commit.ci] pre-commit autoupdate'; ```. 4. Push to a named branch:. ```; git push YOURFORK 1.9.x:auto-backport-of-pr-2704-on-1.9.x; ```. 5. Create a PR against branch 1.9.x, I would have named this PR:. > ""Backport PR #2704 on branch 1.9.x ([pre-commit.ci] pre-commit autoupdate)"". And apply the correct labels and milestones. Congratulations — you did some good work! Hopefully your backport PR will be tested by the continuous integration and merged soon!. Remember to remove the `Still Needs Manual Backport` label once the PR gets merged. If these instructions are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2704#issuecomment-1776676386:924,integrat,integration,924,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2704#issuecomment-1776676386,1,['integrat'],['integration']
Integrability,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; git checkout 1.9.x; git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; git cherry-pick -x -m1 b23229f9bfc95ff90a5d6393b4d53d062190d5bb; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; git commit -am 'Backport PR #2732: [pre-commit.ci] pre-commit autoupdate'; ```. 4. Push to a named branch:. ```; git push YOURFORK 1.9.x:auto-backport-of-pr-2732-on-1.9.x; ```. 5. Create a PR against branch 1.9.x, I would have named this PR:. > ""Backport PR #2732 on branch 1.9.x ([pre-commit.ci] pre-commit autoupdate)"". And apply the correct labels and milestones. Congratulations — you did some good work! Hopefully your backport PR will be tested by the continuous integration and merged soon!. Remember to remove the `Still Needs Manual Backport` label once the PR gets merged. If these instructions are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2732#issuecomment-1795950835:924,integrat,integration,924,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2732#issuecomment-1795950835,1,['integrat'],['integration']
Integrability,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; git checkout 1.9.x; git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; git cherry-pick -x -m1 bf5f27aa9e968de6e73fc7abb46a89084ddf6880; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; git commit -am 'Backport PR #2831: Prepare 1.9.8, stop ignoring citation errors'; ```. 4. Push to a named branch:. ```; git push YOURFORK 1.9.x:auto-backport-of-pr-2831-on-1.9.x; ```. 5. Create a PR against branch 1.9.x, I would have named this PR:. > ""Backport PR #2831 on branch 1.9.x (Prepare 1.9.8, stop ignoring citation errors)"". And apply the correct labels and milestones. Congratulations — you did some good work! Hopefully your backport PR will be tested by the continuous integration and merged soon!. Remember to remove the `Still Needs Manual Backport` label once the PR gets merged. If these instructions are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2831#issuecomment-1911960423:938,integrat,integration,938,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2831#issuecomment-1911960423,1,['integrat'],['integration']
Integrability,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; git checkout 1.9.x; git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; git cherry-pick -x -m1 c2f706b35d52a5e21ccf84f1cd299b0dadf49668; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; git commit -am 'Backport PR #2716: Add missing link targets'; ```. 4. Push to a named branch:. ```; git push YOURFORK 1.9.x:auto-backport-of-pr-2716-on-1.9.x; ```. 5. Create a PR against branch 1.9.x, I would have named this PR:. > ""Backport PR #2716 on branch 1.9.x (Add missing link targets)"". And apply the correct labels and milestones. Congratulations — you did some good work! Hopefully your backport PR will be tested by the continuous integration and merged soon!. Remember to remove the `Still Needs Manual Backport` label once the PR gets merged. If these instructions are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2716#issuecomment-1780921886:898,integrat,integration,898,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2716#issuecomment-1780921886,1,['integrat'],['integration']
Integrability,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; git checkout 1.9.x; git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; git cherry-pick -x -m1 c410cd123f5487f25c08b421c8d06da50551ff73; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; git commit -am 'Backport PR #2799: [pre-commit.ci] pre-commit autoupdate'; ```. 4. Push to a named branch:. ```; git push YOURFORK 1.9.x:auto-backport-of-pr-2799-on-1.9.x; ```. 5. Create a PR against branch 1.9.x, I would have named this PR:. > ""Backport PR #2799 on branch 1.9.x ([pre-commit.ci] pre-commit autoupdate)"". And apply the correct labels and milestones. Congratulations — you did some good work! Hopefully your backport PR will be tested by the continuous integration and merged soon!. Remember to remove the `Still Needs Manual Backport` label once the PR gets merged. If these instructions are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2799#issuecomment-1882962300:924,integrat,integration,924,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2799#issuecomment-1882962300,1,['integrat'],['integration']
Integrability,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; git checkout 1.9.x; git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; git cherry-pick -x -m1 e5d41d4aa58a925f0fa5cfcf580cb975167a71c9; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; git commit -am 'Backport PR #2235: Separate test utils from tests'; ```. 4. Push to a named branch:. ```; git push YOURFORK 1.9.x:auto-backport-of-pr-2235-on-1.9.x; ```. 5. Create a PR against branch 1.9.x, I would have named this PR:. > ""Backport PR #2235 on branch 1.9.x (Separate test utils from tests)"". And apply the correct labels and milestones. Congratulations — you did some good work! Hopefully your backport PR will be tested by the continuous integration and merged soon!. Remember to remove the `Still Needs Manual Backport` label once the PR gets merged. If these instructions are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2235#issuecomment-1604242870:910,integrat,integration,910,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2235#issuecomment-1604242870,1,['integrat'],['integration']
Integrability,"PS: You don't need a test for this... it would require installing phate on travis and this would take time... Also, the interface is trivial. You should simply link to your package within the docs to redirect people for bugs and more info.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/136#issuecomment-385960220:120,interface,interface,120,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/136#issuecomment-385960220,2,['interface'],['interface']
Integrability,"PYTHON_VERSION variable is empty, so we actually pass `python=` in `conda create` so Travis always tests scanpy with latest Python in Conda distribution. Therefore Python 3.5 is actually never tested. Furthermore, conda switched to python 3.7, so now all test are run on Python 3.7. This is also the reason of weird HDF error message we get in tests.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/201:326,message,message,326,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/201,1,['message'],['message']
Integrability,"Partially reverts #2220 to restore CI to a functional state. I will skip the review since this is a revert of a breakage. - broken links are banned again; - the tutorial links are just links again, but using intersphinx and therefore still not broken. A follow up PR can make the tutorials a submodule again if we go that route, but we won’t disable link checking again. Not for a PR like this, and not for anything else. PS: one of the `{tutorial}` links wasn’t converted to a ``{doc}`/tutorials/…` `` link in #2220. Fixed that too.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2614:322,rout,route,322,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2614,1,['rout'],['route']
Integrability,"People are discussing intersections [here](https://github.com/python/typing/issues/213). And I agree with you @ivirshup: That’s a great example where an intersection would be needed. Unions are only useful if you accept several things and somewhere switch behavior based on what you got like `if isinstance(...):`. I think that Alex just means that something like that isn’t needed anywhere in scanpy. An aside about switching behavior based on types: Too bad Python hasn’t been designed with destructuring `match`/`switch`. Rust is beautiful because of it. However, Python would probably need to use something like [scala’s unappy](https://docs.scala-lang.org/tour/extractor-objects.html) which I could never wrap my brain around.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-443655464:710,wrap,wrap,710,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-443655464,1,['wrap'],['wrap']
Integrability,Phneograph was recently updated and also new wrappers are available in external thanks to @awnimo @Koncopd .; Does this work for you @asmariyaz23 ? I will close this but feel free to reopen,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1407#issuecomment-706139788:45,wrap,wrappers,45,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407#issuecomment-706139788,1,['wrap'],['wrappers']
Integrability,Please ask usage questions here: https://discourse.scverse.org/. You should not integrate normalized and unnormalized counts. Consider getting the raw counts or integrating on the normalized counts,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2662#issuecomment-1723238652:80,integrat,integrate,80,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2662#issuecomment-1723238652,2,['integrat'],"['integrate', 'integrating']"
Integrability,"Please provide more details. What is `folder` in your case and what is the error message? Follow the issue template, please.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1795#issuecomment-817677727:81,message,message,81,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1795#issuecomment-817677727,1,['message'],['message']
Integrability,"Possible TODO:. - normalize_pearson_residuals_pca. @ivirshup I reverted the change in a6290ee9e0d1baf0e3483118aa552b6f6dcf02c0 where you changed. ```diff; -X_pca = np.zeros((X.shape[0], n_comps), X.dtype); +X_pca = np.zeros((adata_comp.shape[0], n_comps), adata.X.dtype); ```. the commit message is “Fix up pca tests”, but that change doesn’t seem to impact tests and it takes properties from several different object without reasoning.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2272#issuecomment-1807755523:288,message,message,288,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2272#issuecomment-1807755523,1,['message'],['message']
Integrability,"Potentially fixes #1355. * Would still need tests/ further consideration.; * Need to fix missing values not being plotted below present ones. Using this branch:. ```python; import scanpy as sc; import numpy as np; import matplotlib as mpl. pbmc = sc.datasets.pbmc3k_processed(); pbmc.obs[""louvain""].iloc[::2] = np.nan; with mpl.rc_context({""figure.dpi"": 150}):; sc.pl.umap(pbmc, color=""louvain""); ```. ![image](https://user-images.githubusercontent.com/8238804/89258138-e54b0d80-d66a-11ea-8d13-e7bf975c3203.png). ```python; with mpl.rc_context({""figure.dpi"": 150}):; sc.pl.umap(pbmc, color=""louvain"", groups=list(pbmc.obs[""louvain""].cat.categories[:3])); ```. ![image](https://user-images.githubusercontent.com/8238804/89258165-f6941a00-d66a-11ea-9eaf-3a51a5a49a38.png). ## Update:. This PR expanded in scope quite a bit, so I'd like to wrap it up. Most things are implemented and seem to work. Regression tests need to be added, for these cases. - [x] Tests for all fixed cases (probably write these down as well); - [x] Decide on adding arguments, and default values; - [x] Decide on whether continuous legend update happens in this PR",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1356:837,wrap,wrap,837,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1356,1,['wrap'],['wrap']
Integrability,"Project specific IO is interesting but IMO makes it even more complicated in some ways. The current biggest problem we face is that no one knows where to go to read certain formats.... scanpy? muon? squidpy? Scanpy has read visium but squidpy is the spatial package? I can analyze atac data in scanpy but need to use muon to read the file?. Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages?. Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places?. > How does this impact users vs. developers?. Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059551352:796,depend,dependency,796,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059551352,4,['depend'],['dependency']
Integrability,"Pynndescent 0.3.0 was released yesterday with support for multi-threading. This change allows scanpy to take advantage of multi-threading for computing nearest neighbors. To use it, wrap the call to scanpy in a `joblib.parallel_backend` context manager:. ```python; from joblib import parallel_backend; with parallel_backend('threading', n_jobs=16):; sc.pp.neighbors(adata); ```. Running on the 130K dataset on a 16 core machine before the change:. ```; computing neighbors; using 'X_pca' with n_pcs = 50; finished (0:01:31.54); ```; and with the change:. ```; computing neighbors; using 'X_pca' with n_pcs = 50; finished (0:00:32.02); ```. A threefold speedup. (Note that there is a small [bug](https://github.com/lmcinnes/pynndescent/pull/58) in pynndescent 0.3.0, which means that `n_jobs` needs to be set explicitly. When that's fixed you'll be able to leave it out to use all cores on a machine.)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/659:182,wrap,wrap,182,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659,1,['wrap'],['wrap']
Integrability,"Pytables is in requirements.txt (the PyPI package is called “tables”), how did y’all get Scanpy installed without all its dependencies?. https://github.com/theislab/scanpy/blob/f252d3a84200cc76060a786ef0589405fc5c9c12/requirements.txt#L7",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/454#issuecomment-462133198:122,depend,dependencies,122,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454#issuecomment-462133198,1,['depend'],['dependencies']
Integrability,Question: plans for scATAC integration with scRNA?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/725:27,integrat,integration,27,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/725,1,['integrat'],['integration']
Integrability,"Ran into this today as a coworker wanted to stick a UMAP legend into `lower left` and couldn't. I whipped up a hotfix, which turned out to be exactly the same syntax as you've got going on here, and was on my way to mention it when I found this. Would be nice to see this integrated as the docs imply this is possible, and it seems useful to have on occasion?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2277#issuecomment-1976241426:272,integrat,integrated,272,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2277#issuecomment-1976241426,1,['integrat'],['integrated']
Integrability,"Re: testing externals, I've tried my best to just test the way it interfaces with scanpy. i.e., if MAGIC silently fails to return the correct output, scanpy tests would pass so long as the output is the right type / shape. If MAGIC throws an error when run from scanpy, this might be something you would like to address (i.e. by contacting the relevant external developer) regardless.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/988#issuecomment-573589189:66,interface,interfaces,66,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/988#issuecomment-573589189,1,['interface'],['interfaces']
Integrability,"Ref https://github.com/theislab/scanpy/issues/1698#issuecomment-824634639, cut down in https://github.com/theislab/scanpy/issues/1698#issuecomment-826712541. Basically, `gearys_c`, `morans_i` can run into silent numba parallelization errors when all values for a variable are the same. We should specifically address these cases. As suggested by @Hrovatin, `nan` and a warning is probably sufficient for this case. That said, it may take some work to consistently throw a warning. * Depending on the threading backend, this may error (solving the problem of incorrect values); * We may be able to control this, but how the parallel backend interacts with the `error_model` seems unclear; * I don't think we can throw a warning from numba code, let alone parallel numba code; * It would be nice if we could guarantee a `tbb` or `omp` threading backend, since they seem more consistent, but I personally have not been able to `pip install` these for a bit over a year now. This may be macOS specific.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1806:483,Depend,Depending,483,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1806,1,['Depend'],['Depending']
Integrability,"Regarding the other packages: of course, we will also interface those as optional dependencies... But I'd do it from the original Scanpy repo. To me, the whole problem is simply about keeping a clean structure and throwing clear error messages if optional dependencies are not installed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/125#issuecomment-382344862:54,interface,interface,54,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-382344862,8,"['depend', 'interface', 'message']","['dependencies', 'interface', 'messages']"
Integrability,"Regarding this pull request: I'd merge it, put the `rpy2` dependency in the interface and remove it from the requirements. and put the interface into `rtools`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/125#issuecomment-382345473:58,depend,dependency,58,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-382345473,3,"['depend', 'interface']","['dependency', 'interface']"
Integrability,Remaining Qs:. * Does this need support for nans?; * This code is based off `sklearn.utils.sparsefuncs.mean_variance_axis`. Do we need a copy of the sklearn license here? Do we already include that since it's a dependency?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/857#issuecomment-537310661:211,depend,dependency,211,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/857#issuecomment-537310661,1,['depend'],['dependency']
Integrability,"Remove batch effect (""Integrate"" in Seurat"")",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/873:22,Integrat,Integrate,22,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873,1,['Integrat'],['Integrate']
Integrability,Remove dependency on scvelo for doc builds,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1608:7,depend,dependency,7,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1608,1,['depend'],['dependency']
Integrability,Remove legacy-api-wrap dependency,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1926:18,wrap,wrap,18,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1926,2,"['depend', 'wrap']","['dependency', 'wrap']"
Integrability,Remove pytables dependency,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2064:16,depend,dependency,16,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2064,1,['depend'],['dependency']
Integrability,"Removes the need for a pytables dependency. * pytables has been a source of installation heisenbugs, particularly on windows (#1468, #1284, #454); * why use two hdf5 libraries; * Makes it easier to move reading 10x files into anndata or elsewhere #1387. I've edited the code as lightly as possible, since these readers were originally contributed by someone at 10X, so I assume they had better knowledge of possible edge cases. - [ ] ~~Test with h5py 2~~; - [ ] Add release note",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2064:32,depend,dependency,32,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2064,1,['depend'],['dependency']
Integrability,"Right now we get different PCs when using sparse data (applying TruncatedSVD) compared to using dense data (applying PCA). . `TruncatedSVD(X-X.mean(0))` would be equivalent to `PCA(X)`. `X-X.mean(0)` would obviously not be sparse anymore, which is why it is currently implemented as `TruncatedSVD(X)`. The first PC will be mainly representing the vector of means, thus be very different from zero-centered PCA. The following components would approximately resemble PCA. However, since all subsequent PCs are orthogonal to the first PC, we will never get to the exact solution. Hence, the PCs are questionable, in particular when the very first ones are quite misleading. That's not desirable. I think we should obtain the same PCA representation regardless of the data type. Don't we have to densify `X` at some point anyways, as we would have to compute `X.dot(X.T)`. Thus it might be worth thinking of some EM approach?. Whatsoever, I think as long the data is manageable and fits into the RAM, we should just use the densified `X`. . Line 486 in preprocessing/simple I don't quite understand:; ```; if zero_center is not None:; zero_center = not issparse(adata_comp.X); ```; It doesn't depend on the actual value of the attribute `zero_center` anymore. Is that a bug, or what is the rationale behind this?. For now, we can change that into something like; ```; zero_center = zero_center if zero_center is not None else False if issparse(adata.X) and adata.X.shape[0] > 1e4 else True; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/393:1189,depend,depend,1189,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393,1,['depend'],['depend']
Integrability,"Running `0.10.3` here. I had a similar error message, originating in a **mismatch of the indices of the obs-like dataframes** of the `AnnData` object. Although it was not exactly the same (sorry i don't have it here), maybe this can be a hint. Here is how i solved the issue. ```python; ### Have a look at the indices; for key, obs_matrix in adata.obsm.items():; if hasattr(obs_matrix, ""index""):; print(key); print(obs_matrix.index); print(adata.obs_names); print(adata.obs.index). ### If there's a mismatch, you can fix by running something like:; for key, obs_matrix in adata.obsm.items():; if hasattr(obs_matrix, ""index""):; obs_matrix.index = adata.obs_names; ```. I also had a similar error message when there was a mismatch in the _name_ of the index, or if the name of the index was also the _name of a column_. Note that if there is a mismatch, the `adata.write_h5ad` function _does not_ crash. While reading the saved file with `ad.read_h5ad` _will crash_. Suggestion for developers: When the `AnnData.write_h5ad` method is called, check the homogeneity of the indices, and raise an exception if there is a mismatch. Suggestion for users: make sure all the index are the same, have the same name, and that no column has the same name.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2297#issuecomment-2162803279:45,message,message,45,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2297#issuecomment-2162803279,2,['message'],['message']
Integrability,"Same error occurred when using the default leiden with weights as well; downgrading python-igraph to 0.9.11 fixed the issue.; leidenalg is dependent on python-igraph (0.10.0 for my conda) and igraph (0.9.10), and I suppose the version discrepancy caused the problem. Or you can replace tl.leiden with leiden algorithm in python-igraph:. adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None); g = sc._utils.get_igraph_from_adjacency(adjacency); clustering = g.community_leiden(objective_function='modularity', weights='weight', resolution_parameter=0.5); adata.obs['leiden_igraph_weight'] = pd.Series(clustering.membership, dtype='category', index=adata.obs.index). sc.pl.umap(adata, color='leiden_igraph_weight')",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2339#issuecomment-1262134575:139,depend,dependent,139,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2339#issuecomment-1262134575,1,['depend'],['dependent']
Integrability,"Same thing: If the line is under-indented, the first line summary can’t be properly extracted. I’ll make the message more clear.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1492#issuecomment-726053728:109,message,message,109,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1492#issuecomment-726053728,2,['message'],['message']
Integrability,ScanPy's BSD license and GPLed dependencies,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3272:31,depend,dependencies,31,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3272,1,['depend'],['dependencies']
Integrability,"Scanpy does have logging implemented (examples: [neighbors](https://github.com/theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/neighbors/__init__.py#L84), [highly variable genes](https://github.com/theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/preprocessing/_highly_variable_genes.py#L81)), but it's not that widely used. I think this is because it has to be implemented manually in the code (not sure if this is what you mean by ""intrinsic""?), which makes it take some effort to implement and not all contributors are aware of. I think using a decorator would be nice for abstracting out the process. This would have benefits of consistency of usage by making it easy, consistency of logged messages, and separation of concerns between computation and tracking. I also think you'd be able to know the exact set of operations from this approach. Assuming all top level functions have been wrapped with a decorator like the one I presented above, this code:. ```python; adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""); sc.pp.normalize_per_cell(adata, 1000); sc.pp.log1p(adata); sc.pp.pca(adata); adata.write(""./cache/01_simple_process.h5ad""); ```. Should result in a set of (psuedo-)records like:. ```; # Where id(1) is a stand in for value like `id(adata)`; {""call"": ""read_10x_h5"", ""args"": {""filename"": ""./10x_run/outs/filtered_gene_matrix.h5""}, ""returned_adata"": id(1)}; {""call"": ""normalize_per_cell"", ""args"": {""counts_per_cell_after"": 1000}, ""adata_id"": id(1)}; {""call"": ""log1p"", ""adata_id"": id(1)}; {""call"": ""pca"", ""adata_id"": id(1)}; {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}; ```. It's pretty trivial to go through these logs and figure out what happened to the AnnData, and made accessible through helper functions. Maybe they'd look like `sc.logging.get_operations(adata_id=id(adata))` or `sc.logging.get_operations(written_to=""./cache/01_simple_process.h5ad"")`. There could also b",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/472#issuecomment-464575063:736,message,messages,736,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472#issuecomment-464575063,2,"['message', 'wrap']","['messages', 'wrapped']"
Integrability,"Scanpy vs. 1.3.6; installed using pip3; OSX 10.10.5; Jupyter lab. code:; `list_of_list_of_marker_genes = [mg1, mg2, mg3]; for mg in list_of_list_of_marker_genes:; sc.pl.stacked_violin(adata, mg, groupby = 'louvain’, rotation=90); print(mg)`. This works for some mg's but not for all; sc.pl.matrixplot works for all of them; the same for sc.pl.violin; it works also if I would combine all marker genes into one list and then run; sc.pl.stacked_violin. When I say it does not work: it typically generates one stacked plot and then; 'hangs up' with an error message for subsequent plots:. Error message:. IndexError: list index out of range. When I run sc.pl.stacked_violin for the individual mg’s, some work others don’t. It is a reproducible results. I explicitly tested whether the genes in the respective marker gene lists are present in ’n_cells’. If not, I update the lists. What am I missing here?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/405:555,message,message,555,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/405,2,['message'],['message']
Integrability,Shouldn’t we just depend on `requests` if it’s so complicated and we have to resort to code copying?. Basically every Python user should have it installed anyway.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1344#issuecomment-666331642:18,depend,depend,18,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1344#issuecomment-666331642,1,['depend'],['depend']
Integrability,"Since `umap-learn` updated to version `0.5.0` from `0.4.6`, the interface may have changed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1579#issuecomment-758543701:64,interface,interface,64,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579#issuecomment-758543701,2,['interface'],['interface']
Integrability,"Since the API being used here is deprecated, as is the wrapper, I'm inclined leave this as is.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1781#issuecomment-814593014:55,wrap,wrapper,55,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1781#issuecomment-814593014,1,['wrap'],['wrapper']
Integrability,"Since the `obs_values_df ` will now depend on an updated version of AnnData, I'm thinking I'll move this version of `rank_genes_groups_df` over to #467 so that can get merged. Edit: Actually, this isn't the case since we'll need backwards compatibility anyways, nvm",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/619#issuecomment-487811865:36,depend,depend,36,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/619#issuecomment-487811865,1,['depend'],['depend']
Integrability,"Since the bug happens in mnnpy and isn’t caused by the scanpy wrapper, this is not a scanpy bug: chriscainx/mnnpy#30",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/974#issuecomment-573589167:62,wrap,wrapper,62,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/974#issuecomment-573589167,1,['wrap'],['wrapper']
Integrability,Since this is an overflow any data set with 1000's of cells I can use for this? I think it is Windows specific crash and how python implements sqrt() on windows which probably is a wrapper of the native math library in C. I may be wrong. So will the regression test work in this case?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1061#issuecomment-588274013:181,wrap,wrapper,181,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1061#issuecomment-588274013,1,['wrap'],['wrapper']
Integrability,Skip louvain-dependent tests,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1524:13,depend,dependent,13,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1524,1,['depend'],['dependent']
Integrability,"So it will only work on non-negative expression values without any pre-process?; I guess that make sense, thank you for the reply. The version of the package:. scanpy==1.4.6 anndata==0.7.1 umap==0.4.0 numpy==1.18.1 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0. The AnnData objects were all read through same commands without any modification. sc.read_10x_h5(filepath, gex_only=False). the dataset I used to test them are:. https://support.10xgenomics.com/single-cell-vdj/datasets/2.2.0/vdj_v1_hs_nsclc_5gex; https://support.10xgenomics.com/single-cell-gene-expression/datasets/3.0.0/pbmc_10k_protein_v3; https://support.10xgenomics.com/single-cell-gene-expression/datasets/3.0.0/malt_10k_protein_v3. It appears to me that it only works on the v2 nsclc h5 data. I was trying to merge the three data sets and run through SAM to compare with the result of BBKNN, didn't work. So I tried to run each of them individually in the loop. I guess it won't work on CITESeq data without other processing?. I tried removed all the antibody read counts from adata.X and ran it once, still got same error message.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1157#issuecomment-614976989:1141,message,message,1141,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1157#issuecomment-614976989,2,['message'],['message']
Integrability,"So this would be a reproducible example:. ```py; import gzip; import shutil; from urllib.request import urlopen; from pathlib import Path. from tqdm.notebook import tqdm; import scanpy as sc. url = ""https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSE194122&format=file&file=GSE194122%5Fopenproblems%5Fneurips2021%5Fmultiome%5FBMMC%5Fprocessed%2Eh5ad%2Egz"". path = Path(""data/GSE194122_openproblems_neurips2021_multiome_BMMC_processed.h5ad""); if not path.is_file():; with (; urlopen(url) as raw,; tqdm.wrapattr(raw, ""read"", total=int(raw.headers[""Content-Length""])) as wrapped,; gzip.open(wrapped, 'rb') as f_in,; path.open('wb') as f_out,; ):; shutil.copyfileobj(f_in, f_out). adata_atac = sc.read(path); adata_atac.X = (adata_atac.X > 0)*1; sc.pp.highly_variable_genes(adata_atac, n_top_genes=13634); adata_atac = adata_atac[:,adata_atac.var['highly_variable']]; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2819#issuecomment-1910369113:500,wrap,wrapattr,500,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2819#issuecomment-1910369113,3,['wrap'],"['wrapattr', 'wrapped']"
Integrability,"So, I'm not too surprised to see this, since I don't think much of the distributed stuff has good testing, and I'm not too familiar with it. I believe the `AnnData` constructor is converting the array. You can get around this by assigning X to be a dask array, e.g.:. ```python; a = ad.AnnData(np.ones((1000, 100))); a.X = da.from_array(a.X); type(a.X); # dask.array.core.Array; ```. Better support for dask arrays would be a great feature request and series of additions to anndata. I think this is the endemic numeric python problem of ""these things are all like arrays, so can kinda use the same API, but in practice every type needs to be special cased"". > but there's a lot of other stuff happening before & afterwards in normalize_total() which I haven't looked at much. Yeah, I think this function has built up some cruft. I've opened a PR to streamline this #1667, but will need to check with people more familiar with the code. The private method should handle all of the computation, while the outer wrapper will do more argument handling/ getting data out of the `AnnData`/ assigning it back. > What combinations of inputs to _normalize_data() need to be supported. I believe `counts` should always be generated from `X`, so we don't need to worry about the combinations of types.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1663#issuecomment-782803190:1010,wrap,wrapper,1010,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1663#issuecomment-782803190,1,['wrap'],['wrapper']
Integrability,"So, question from a user stand point:. Is it worth it for us to include the really really easy to implement metrics? The ones where we'd basically just be wrapping scikit-learn? I think this fits with the idea of `scanpy`'s contents being curatorial to some extent. > Though I do understand the citation issue. It's definitely good to have a citation in the docstring for each function. For the docs of the metrics module, I think there would be a subsection for ""Integration metrics"" which could definitely point to `scIB` as a more comprehensive package for evaluating integration. > Maybe it's time for a global citation table and each function can add to the table if there is an appropriate citation?! . Are you suggesting that the table would be added to at runtime (when a function is called)? I think this may be better addressed by a broader solution to ""what has been done to this dataset?"". I'm not sure how this could be done without buy in from third party libraries. Also has been discussed a bit previously: #472.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/915#issuecomment-764392892:155,wrap,wrapping,155,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-764392892,5,"['Integrat', 'integrat', 'wrap']","['Integration', 'integration', 'wrapping']"
Integrability,"So, we could also not early load `scanpy.testing._pytest` or load `pytest-cov` first?. I would like to keep the `xdist` support and use a similar interface.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2874#issuecomment-1956920175:146,interface,interface,146,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2874#issuecomment-1956920175,1,['interface'],['interface']
Integrability,"Some notes/observations from my side towards choosing the proper resolution: . - the Leiden algorithm depends on a random seed. With a different random seed, you might get a different number of clusters with the same resolution; - a sensible resolution depends on the input data: when clustering on data processed with `sc.tl.diffmap` a much lower resolution will give the same number of clusters than without. ; - I performed a hyperparameter search for the resolution (steps of 0.005) on a large dataset of CD8+ T cells. I observed that at certain resolution ranges, the number of clusters is stable. In my case, I was looking for subtypes of CD8+ T cells and hypothesized that at ~0.1 and ~0.3 I would find something biologically meaningful. Would be interesting to re-do that on the PBMC dataset. I would expect a plateau at a resolution that recovers the well-known cell types CD8+, CD4+, etc. . ![2019-06-03_09:53:34_911x604](https://user-images.githubusercontent.com/7051479/58785259-7ea10e80-85e5-11e9-8e0b-789e2e74754a.png); **Fig:** hyperparameter search for resolution in steps of 0.005. The graph shows the resolution vs. detected number of Leiden-clusters.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/670#issuecomment-498153336:102,depend,depends,102,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670#issuecomment-498153336,2,['depend'],['depends']
Integrability,"Some of the arguments for bbknn have changed, so the wrapper is broken at the moment. https://github.com/Teichlab/bbknn/issues/10",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/635:53,wrap,wrapper,53,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/635,1,['wrap'],['wrapper']
Integrability,"Sorry about the late reply to this!. > and it seems odd that the existence of the wrapper (which just runs reduce and adds the result to the input AnnData) should disqualify it. I guess I wouldn't think of it as disqualification. If a wrapper is added to external, it adds maintanence burden to both of us by giving you multiple sets of documentation and code to keep in sync, and us for issue management and CI. Plus all the documentation you can provide through external is a docstring, while you can offer much more on your own repo. To us it just seems easier on both of us, especially since you've already implemented the interface with anndata on your side. We're aiming to make the ecosystem documentation much more visible for the next release as well (and are open to input of improving this further), in case that was your concern. So yes, I would still prefer to have your tool added to the ecosystem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1780#issuecomment-848587577:82,wrap,wrapper,82,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780#issuecomment-848587577,3,"['interface', 'wrap']","['interface', 'wrapper']"
Integrability,"Sorry about the wait, had to focus on getting the last release out. Now we can do new features!. > But the warning IMHO should not convey the message ""Do not do this!"". In my mind, it should convey the message ""What you are computing is not exactly t-SNE, but it is close enough to t-SNE that you can ignore this message. That sounds appropriate. > But we will have to control them anyway... Your suggested solution also controls them: namely, symmetrizes and normalizes. I think normalization is a ""lighter touch"" than binarization. To me, the alternative would be to error for non-normalized data since the optimization won't converge properly. Not knowing too much about the internals of tsne, is a symmetric graph necessary? If it's not, then I'd be fine with not doing that. Exactly how the option to do this is provided to users could take some consideration. I think it would be clean and composable to have graph weighting options separate from embedding layout options, but considering `tsne` has restrictions on graph weights there may have to be some exception here. Perhaps there needs to be a `weights` option on `tsne` which allows normalization, binarization, or just erroring if the passed graph doesn't have correct weighting. -------------------. From my perspective, what we have to gain here is:. * More efficient TSNE by default; * Consolidate implementation to a single well maintained library; * More flexibility in how tsne is computed. > Scanpy is in a unique position to offer people t-SNE with k=15 binary affinities as a convenient, faster, UMAP-independent, and nearly equivalent replacement for k=90, perplexity=30 affinities. I'm happy to have this be an option. I'm less comfortable with something like this being the ""recommended path"", since not using perplexity weights seems non-standard. -------------------. In general, are we agreed on these points?. * `tsne` should allow weights to be passed through (whether perplexity based, or not); * There should be a warn",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-773051636:142,message,message,142,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-773051636,3,['message'],['message']
Integrability,"Sorry about this; `sc.tl.sim` used to be a separate tool in the beginning and integration into Scanpy was erroneous. For the past months I've only used to produce the two reference datasets linked below. All of the problems you mentioned are fixed in Scanpy 0.3.2. Take a look at:; https://github.com/theislab/scanpy_usage/tree/master/170430_krumsiek11. Cheers,; Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/52#issuecomment-348018857:78,integrat,integration,78,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/52#issuecomment-348018857,1,['integrat'],['integration']
Integrability,"Sorry for opening this thread again, but I think I've run into the same problem. Here's my code and error:; ```; mat_all = sc.read_loom(filename=""RSV.loom""); sc.pp.pca(mat_all); sc.pp.neighbors(mat_all); sc.tl.umap(mat_all); ```; The error message:; ```; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); /tmp/31048.tmpdir/ipykernel_3245/2128514342.py in <module>; 3 sc.pp.pca(mat_all); 4 sc.pp.neighbors(mat_all); ----> 5 sc.tl.umap(mat_all); 6 sc.pl.tsne(mat_all, color=""cluster"",legend_loc=""on data"",; 7 size=20, save=True). /storage1/fs1/leyao.wang/Active/conda/envs/velocyto3.9/lib/python3.9/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key); 192 default_epochs = 500 if neighbors['connectivities'].shape[0] <= 10000 else 200; 193 n_epochs = default_epochs if maxiter is None else maxiter; --> 194 X_umap = simplicial_set_embedding(; 195 X,; 196 neighbors['connectivities'].tocoo(),. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'; ```. And the versions I've been running:; anndata 0.7.8; asttokens 2.0.5; bcrypt 3.2.0; Bottleneck 1.3.2; brotlipy 0.7.0; cached-property 1.5.2; certifi 2021.10.8; cffi 1.15.0; charset-normalizer 2.0.12; chart-studio 1.1.0; click 8.0.4; cmake 3.22.2; colorama 0.4.4; conda 4.11.0; conda-package-handling 1.7.3; cryptography 36.0.1; cycler 0.11.0; Cython 0.29.20; devtools 0.8.0; dunamai 1.9.0; executing 0.8.2; fa2 0.3.5; Fabric 1.6.1; fonttools 4.29.1; get_version 3.5.4; h5py 3.6.0; idna 3.3; igraph 0.9.9; install 1.3.5; joblib 1.1.0; kiwisolver 1.3.2; legacy-api-wrap 1.2; llvmlite 0.38.0; loom 0.0.18; loompy 3.0.6; mamba 0.15.3; matplotlib 3.5.1; mkl-fft 1.3.1; mkl-random 1.2.2; mkl-service 2.4.0; MulticoreTSNE 0.1; natsort 8.1.0; networkx 2.6.3; numba 0.55.1; numexpr 2",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1579#issuecomment-1062410460:240,message,message,240,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579#issuecomment-1062410460,2,['message'],['message']
Integrability,"Sorry for the late response, I was on holidays. I'm happy to merge a pull request for this, if the package appears solid. Would you want to add a file `scanpy/preprocessing/doubletdetection`? We should probably just ask @JonathanShor whether he's interested in an interface for easily accessing his package. If he is, he should also make the pull request, I'd say. :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/173#issuecomment-398678802:264,interface,interface,264,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-398678802,1,['interface'],['interface']
Integrability,Sorry for the slow response! Congrats on the very nice preprint and thanks for thanking me! :wink:. I'll upload the file from dropbox to `scanpy_usage`!. Regarding the result: the high PCs can change drastically depending on the platform and the random seed. I've seen clustering results changing completely after I became aware of it. I'm very sure that non of the preprocessing and other code changed at any point. What do you think?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/325#issuecomment-435735671:212,depend,depending,212,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325#issuecomment-435735671,1,['depend'],['depending']
Integrability,"Sorry for the super-late response! I just worked through almost 60 issues starting with the most recent, this is the last one... Sorry about that. `paga_path` requires computing a pseudotime before-hand as one needs to order cells at single-cell resolution along the path. I added a more meaningful error message stating that. PS: Now, there is also a test for PAGA [here](https://github.com/theislab/scanpy/blob/master/scanpy/tests/notebooks/test_paga_paul15_subsampled.py), making sure that the canonical use ([here](https://nbviewer.jupyter.org/github/theislab/paga/blob/master/blood/paul15/paul15.ipynb)) remains unchanged.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/328#issuecomment-435736335:305,message,message,305,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/328#issuecomment-435736335,1,['message'],['message']
Integrability,"Sorry yeah, the branch name might not be accurate when we’re done. What dependency causes those mismatches? Locally the image comparisons succeed unless I create a venv …. > master_clustermap.png; > ; > I believe the difference is just the margin, so we should be good to just change the test image. nah, it also has an ugly white gap between tree and heatmap, probably caused by the same reason as the other one …. /edit: the space seems exactly enough to hold the color bar’s x tick labels, that might be the cause. /edit2: also appears without scanpy’s style or anything: mwaskom/seaborn#1953",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1015#issuecomment-581052169:72,depend,dependency,72,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1015#issuecomment-581052169,1,['depend'],['dependency']
Integrability,"Sounds like a great idea! Currently, `pip install scanpy` avoids installing the C++ dependencies, which give some users trouble: `louvain` and `python-igraph`. I think that these should still remain optional dependencies: users should be able to do some basic analysis with plotting without having to install C++ dependencies. They can then continue to install optional dependencies, if they like.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/59#issuecomment-354904560:84,depend,dependencies,84,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/59#issuecomment-354904560,4,['depend'],['dependencies']
Integrability,"Sounds like a great idea. generally the order should be the same as in the signature, but I don’t see a problem in reshuffling the lovain args to match the leiden ones. We have to be careful with details though: e.g. `partition_type` needs to be slightly different for both:. ```rst; Type of partition to use. Defaults to :class:`~louvain.RBConfigurationVertexPartition`.; For the available options, consult the documentation for :func:`~louvain.find_partition`.; ```; ```rst; Type of partition to use. Defaults to :class:`~leidenalg.RBConfigurationVertexPartition`.; For the available options, consult the documentation for :func:`~leidenalg.find_partition`.; ```. @falexwolf do you think we should go ahead with https://pypi.org/project/legacy-api-wrap (and introduce `*` in `louvain`’s signature` or do you think we can slightly reshuffle the last few arguments of `louvain` without considering it a backwards compat break?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/570#issuecomment-477971741:750,wrap,wrap,750,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/570#issuecomment-477971741,1,['wrap'],['wrap']
Integrability,"Sounds reasonable, would you want to add anything more to Ecosystem about this? Also, would you like to make a PR with the deprecation message pointing to `scvi-tools`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1443#issuecomment-703461716:135,message,message,135,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1443#issuecomment-703461716,1,['message'],['message']
Integrability,"Starting from the end: I think if you could upload the clustering results from the Scanpy paper / PAGA preprint to the scanpy github repo, it would be great. I still have the dropbox link of course, but I guess in the long run it's better if that file was located here and linked from the https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells page. The issue with 1 cell missing was because I did not specify `header=None` when loading it with Pandas :) So my error, not yours. The file is correct as is. That said, I am worried about the influence the random seed in randomized PCA seems to give in this case. Let me show you how it looks:. ![mln-tsne-clustering-comparison](https://user-images.githubusercontent.com/8970231/47555195-71af9480-d90b-11e8-85fb-a3e8dcb7a66f.png). I would be fine with some cells getting into other clusters depending on the random seed, and it would even be okay if small clusters changed their identities, but what we see here is a very drastic change of the cluster structure. Are you sure that the only difference is the randomized PCA outcome? Can it be that some of the default parameters in `sc.pp.recipe_zheng17`, `sc.pp.neighbors`, or `sc.tl.louvain` changed since when you ran the clustering? The scanpy code I posted above is the full code I used, and I ran it yesterday after updating scanpy via pip. BTW, the visualization above is taken from https://www.biorxiv.org/content/early/2018/10/25/453449 which we posted yesterday. Any comments very welcome! I hope you don't mind being thanked in the acknowledgements!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/325#issuecomment-433334926:874,depend,depending,874,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325#issuecomment-433334926,1,['depend'],['depending']
Integrability,"Still the error message could be a lot better. I’ve made the same mistake,; it’s easy to forget to log the data. On Fri 2 Aug 2019 at 23:36, Stephen Fleming <notifications@github.com>; wrote:. > Closed #763 <https://github.com/theislab/scanpy/issues/763>.; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/763?email_source=notifications&email_token=AACL4TL6QHUQMHIBKEQT5GLQCSSFFA5CNFSM4IJBAFAKYY3PNVWWK3TUL52HS4DFWZEXG43VMVCXMZLOORHG65DJMZUWGYLUNFXW5KTDN5WW2ZLOORPWSZGOS3M3XBA#event-2530851716>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AACL4TM5VZDC544TAQPK7NDQCSSFFANCNFSM4IJBAFAA>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/763#issuecomment-517929825:16,message,message,16,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/763#issuecomment-517929825,1,['message'],['message']
Integrability,"Sure, I agree with you generally. I’m just saying that an easy, userfriendly way should make it easy to get beautiful colors, not ugly ones. So in the PR tuning that interface to be nicer, we should do something about that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/596#issuecomment-480746287:166,interface,interface,166,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/596#issuecomment-480746287,1,['interface'],['interface']
Integrability,Switching README.rst to markdown would also look nice on new pypi interface e.g. https://pypi.org/project/markdown-description-example/,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/116#issuecomment-378952904:66,interface,interface,66,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/116#issuecomment-378952904,1,['interface'],['interface']
Integrability,"TODOs:. 1. Figure out why some tests are passing when they shouldn't (hence why I pushed the branch, curious about CI). UPDATE: `tol` for `matplotlib.testing.compare.compare_images` is too high for a sparse-ish plot like `rank_genes_groups`. This is somewhat worrying so will need to be amended. Other than that, changed plotting outputs make sense so this should be resolved.; 2. Check with scanpy tutorials to see what needs to be changed there as well, if anything (if needed, the two PRs should be merged in tandem). The following use leiden in some capacity:; a. https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html; b. https://scanpy-tutorials.readthedocs.io/en/latest/plotting/core.html; c. https://scanpy-tutorials.readthedocs.io/en/latest/spatial/basic-analysis.html; d. https://scanpy-tutorials.readthedocs.io/en/latest/spatial/integration-scanorama.html; 3. Do a large dataset test - check NMI for accuracy of the new default against the old one, check speed to confirm what we're doing makes sense (although this was covered, it seems, in #1053), and scalability",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2815#issuecomment-1894255210:847,integrat,integration-scanorama,847,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2815#issuecomment-1894255210,1,['integrat'],['integration-scanorama']
Integrability,Tansfering data integration fom scanorama to a new dataset,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2162:16,integrat,integration,16,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2162,1,['integrat'],['integration']
Integrability,Test against pre-release dependencies,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2478:25,depend,dependencies,25,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2478,1,['depend'],['dependencies']
Integrability,"Test collection was taking a while, this should cut that time by about 90%. For some reason, pytest test collection seems dependent on the number of files the `python_files` parameter matches. Having a more specific path under `testpaths` doesn't – which would be a simpler change. Should close #326.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/327:122,depend,dependent,122,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/327,1,['depend'],['dependent']
Integrability,"Thank you for creating `scanpy`! It's such a useful tool!. ## Overview of request; Is there any chance that you can start tagging versions of your documentation on readthedocs? Currently, the only available versions are **stable** and **latest**. That makes it easy to view the current documentation but difficult to view docs from old releases of `scanpy`. ## Desired behavior; I would like to be able to...; 1. Click on `v: stable` in the flyout menu at the bottom-left corner of the page; ![image](https://user-images.githubusercontent.com/23412689/220255767-3fbf84e9-ccf5-420b-b067-7a4054aed047.png); 2. Click on the version of the documentation tagged for the older release of `scanpy` that I'm using. ## Resources; - [How to version RTD documentation](https://docs.readthedocs.io/en/stable/versions.html); - [How to automatically maintain versions upon creating new git tags](https://docs.readthedocs.io/en/stable/automation-rules.html#activate-all-new-tags); - [Automated versioning](https://docs.readthedocs.io/en/stable/integrations.html#automated-versioning) as part of continuous documentation deployment. ## An example; As an example of versioned documentation, take a look at [Snakemake's documentation](https://snakemake.readthedocs.io/). ![image](https://user-images.githubusercontent.com/23412689/220255676-39e9eee9-e658-4866-b8b9-63a6174577ba.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2425:1029,integrat,integrations,1029,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2425,1,['integrat'],['integrations']
Integrability,"Thank you for this, @awnimo! I added a few small comments. Could you move the whole code into `scanpy/external/_tools`, please? We'll transition to all wrapper code for external code to be in that directory. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/493#issuecomment-471330146:152,wrap,wrapper,152,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/493#issuecomment-471330146,1,['wrap'],['wrapper']
Integrability,"Thank you! As it stands, this destroys backwards compatibility. But fear not:. 1. Add [`legacy-api-wrap`](https://pypi.org/project/legacy-api-wrap/) to requirements.txt and docs/requirements.txt; 2. Use it to change the API to 1-2 positional parameters (i.e. either `def magic(adata: ..., *, name_list: ... = None, ...)` or with the `*` one parameter to the right); 3. While you’re at it, change `name_list`’s type to `Union[Literal['all_genes', 'pca_only'], Sequence[str], None]`. You can do `from .._compat import Literal` or so (maybe a dot less or more)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/896#issuecomment-547361635:99,wrap,wrap,99,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/896#issuecomment-547361635,2,['wrap'],['wrap']
Integrability,Thank you! If you add a few more details we can fix this quickly: Which call will update the groups but not the color and which call will error out with which stack trace? Please add the the traceback to your comment this:. ````md; ```python; sc.tl.something(adata); ```. ```pytb; XError Traceback (most recent call last); ....; XError: some message.; ```; ````,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/833#issuecomment-531482480:342,message,message,342,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/833#issuecomment-531482480,1,['message'],['message']
Integrability,"Thanks @falexwolf !! I will test if the feature works. Of course I've tested seurat cca, but it seems to work better when integrating data from different sequencing platforms. As for my own data, several batches generated by 10x, output if the MNN method looks more pleasing...; ![unknown](https://user-images.githubusercontent.com/8361080/39244909-b8d3cb38-48c4-11e8-9cdc-82c78703ceee.png). Plus, I haven't looked into the maths of CCA, but I have for MNN and feel more comfortable using it. Actually, @gokceneraslan 's comments do make sense to me, and I've spent quite some time working on a native implementation of MNN correct on python. Now it's nearly complete and features more complete multicore support than the scran implementation.; ![screen shot 2018-04-25 at 20 25 17](https://user-images.githubusercontent.com/8361080/39245687-0a17319a-48c7-11e8-934b-904ee6d75978.png); I built it to be fully compatible with anndata and scanpy. Now it already runs much faster than the scran version, and I'm planning to add more speedups, eg Cython and CUDA. I'm thinking of creating a full toolbox for scanpy, like scran for scater/sce, in python. Perhaps we could work together? 😄. I'm currently writing docstrings and will pack and upload the code to a repository shortly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/125#issuecomment-384268335:122,integrat,integrating,122,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-384268335,1,['integrat'],['integrating']
Integrability,"Thanks @fidelram, that will run the whole Scrublet workflow so will certainly do the trick. But I'd prefer a more Scanpy-integrated approach, which I think I can see how to do from @swolock's fork.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/173#issuecomment-545324439:121,integrat,integrated,121,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-545324439,1,['integrat'],['integrated']
Integrability,Thanks @stkmrc!. Could you merge master into this PR? It looks like there was a dependency issue that should be fixed now.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2175#issuecomment-1068381662:80,depend,dependency,80,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2175#issuecomment-1068381662,1,['depend'],['dependency']
Integrability,"Thanks a lot @a-munoz-rojas!. For the DE approach, I would go with the same thing you suggest. I would definitely not do DE testing on the corrected data (violation of distributional assumptions, potential overcorrection of background variation leading to false significant results). . Regarding altering the number of HVGs or latent dimensions... this is difficult to say in general. I would normally err on the higher side of the number of HVGs, but the latent dimensions will depend heavily on the complexity of the dataset i would imagine. I don't think it's possible to give a general recommendation there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2162#issuecomment-1061642536:479,depend,depend,479,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2162#issuecomment-1061642536,1,['depend'],['depend']
Integrability,"Thanks a lot. ---Original---; From: ""James ***@***.***&gt;; Date: Thu, Sep 29, 2022 00:06 AM; To: ***@***.***&gt;;; Cc: ""Sijian ***@***.******@***.***&gt;;; Subject: Re: [scverse/scanpy] sc.tl.leiden(adata,use_weights=False) (Issue#2339). ; I also saw this with python-igraph version 0.10. Downgrading to 0.9.9 fixed the issue.; ; —; Reply to this email directly, view it on GitHub, or unsubscribe.; You are receiving this because you authored the thread.Message ID: ***@***.***&gt;",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2339#issuecomment-1261162964:455,Message,Message,455,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2339#issuecomment-1261162964,1,['Message'],['Message']
Integrability,"Thanks everyone! I wonder how this affects one-pipeline-for-everything; portals, like the EBI single cell expression atlas... and standarized; pipelines like cellranger. On Mon, Jul 1, 2019 at 3:29 PM MalteDLuecken <notifications@github.com>; wrote:. > Based on my experience setting a single cutoff for all datasets will not; > work, as I've used a lot of different cutoffs depending on the; > distributions. I would echo @ivirshup <https://github.com/ivirshup>'s; > suggestion of looking at distributions. Joint distributions being a lot; > more important than individual histograms. There's a small discussion about; > it in our best practices paper; > <https://www.embopress.org/lookup/doi/10.15252/msb.20188746>; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/718?email_source=notifications&email_token=AACL4TMTNHMCCFM7MGMIZ73P5IBDPA5CNFSM4H4DUZEKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODY6D6LQ#issuecomment-507264814>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AACL4TKKTTZ4IHBJJDFAPKLP5IBDPANCNFSM4H4DUZEA>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/718#issuecomment-507267593:375,depend,depending,375,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/718#issuecomment-507267593,1,['depend'],['depending']
Integrability,"Thanks for bearing with me Isaac 😅 🙏 took some of your suggestions and here's current status:. - reverted back to look for `library_id` in spatial, but still added the exception that `adata.uns[""spatial""]` does not exist. This is in order to use `sc.pl.spatial` with non-visium data.; - if that's the case, then spatial should simply wrap embedding. This also refers to your point.; 	> I'm not totally sure what this means. The coordinates have been z-score transform across each axis? How is this useful? In particular, how is it useful to completely replace the original coordinates with this?. 	this is very likely to happen for anything that it's not visium. In that case, users will share already processed data that contains coordinates in some type of system, and this is the case for whatever processing they had to undertake (would suggest you to have a look at https://github.com/spacetx/starfish for examples of those processing steps.). Anyway, in short, it's much easier for us to just wrap embedding in that case, and I also think it's more correct cause then is the user to choose whatever heuristics they want for point sizes. - fixed a problem in #1534 , that is that the coordinate systems in non-visium has bottom left origin (whereas in visium is top-left, which makes sense because it's in image pixel coordiantes). For this reason, I added the y coordinate inversion in `sc.pl.spatial`, and only in the case where visium is selected, but with img_key = None. Note that this happens because if an img is plotted (before the spots with `circle`), then the origin automatically swap. But if `img_key` is None, then it reverts to default (bottom left). This made it easier as I could remove it from `def _get_data_points` and from `utils._get_edges`. Also added couple of tests for this case. This should be ready for another review, let me know if logic is clearer or I could add more comments in code. re; > Can the spatial neighbours be based off multiple library ids? If so, coul",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1512#issuecomment-739863306:334,wrap,wrap,334,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1512#issuecomment-739863306,2,['wrap'],['wrap']
Integrability,"Thanks for opening a new issue for this, and the info. Could you let me know a bit more about how you've installed scanpy? E.g. what OS, did you use conda or pip, etc. My guess would be that this is numba related (which, from reporting the cpu flags, I'm guessing you suspect too). Are you able to import `numba`? If so, what about `pynndescent` and `umap`? I'm trying to figure out if some code in scanpy is triggering the error, or if it's one of our dependencies. ---------------. Initially mentioned in https://github.com/theislab/scanpy/issues/1823#issuecomment-983551937",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2062#issuecomment-983814860:453,depend,dependencies,453,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2062#issuecomment-983814860,1,['depend'],['dependencies']
Integrability,"Thanks for opening the issue. It looks like a problem with pytables, which we are removing as a dependency since it's starting to have problems like this. Are you able to update the installation of pytables? Otherwise, you could try a dev version of scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2138#issuecomment-1040349484:96,depend,dependency,96,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2138#issuecomment-1040349484,1,['depend'],['dependency']
Integrability,"Thanks for opening this @ivirshup. The principle should be that Anndata doesn't change array types to numpy arrays. It may not have done this in the past (and this is one example), but we should fix that. Hopefully with improvements in numpy itself (like https://www.numpy.org/neps/nep-0018-array-function-protocol.html, and `__array_ufunc__`) it should be achievable. (I don't know how much work is needed in Anndata to do that though.). > Also, are `ZappyArray`s always read-only like the `DirectZappyArrays` are?. Yes, the idea is that they are immutable, and they are changed by transformation (e.g. applying a ufunc) to create a new array.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/733#issuecomment-512213981:306,protocol,protocol,306,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/733#issuecomment-512213981,1,['protocol'],['protocol']
Integrability,"Thanks for reporting @dawe and thanks for updating @WeilerP .; I ran into the same problem with the pip version.; When using **python 3.9** in a fresh virtual enviroment, there's an error related to llvmlite:; <details>; <summary>; error message; </summary>. ```; Building wheel for llvmlite (setup.py) ... error; ERROR: Command errored out with exit status 1:; command: /home/mischko/test/python_virtual/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao; cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/; Complete output (15 lines):; running bdist_wheel; /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py; LLVM version... 11.1.0; ; Traceback (most recent call last):; File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>; main(); File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main; main_posix('linux', '.so'); File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix; raise RuntimeError(msg); RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path.; Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite.; ; error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1; ; ERR",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1799#issuecomment-830137752:238,message,message,238,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799#issuecomment-830137752,1,['message'],['message']
Integrability,"Thanks for tackling this one @falexwolf. I didn't realise until recently that umap has a copy of pynndescent too. However, I think it would be possible for scanpy to depend on both umap and pynndescent packages, and use the latter for generating the knn graph directly. This would mean new features in pynndescent (like the threading support) could be used from scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/522#issuecomment-472497425:166,depend,depend,166,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/522#issuecomment-472497425,1,['depend'],['depend']
Integrability,"Thanks for taking a look at this @giovp!. `@cache` is new in 3.8, but the implementation is:. ```; def cache(user_function, /):; 'Simple lightweight unbounded cache. Sometimes called ""memoize"".'; return lru_cache(maxsize=None)(user_function); ```. Also I don't think it returns a copy, so you would need to handle that. I've got a branch which implements cached datasets for testing as:. ```python; from functools import wraps; import scanpy as sc. def cached_dataset(func):; store = []; @wraps(func); def wrapper():; if len(store) < 1:; store.append(func()); return store[0].copy(); return wrapper. pbmc3k = cached_dataset(sc.datasets.pbmc3k); pbmc68k_reduced = cached_dataset(sc.datasets.pbmc68k_reduced); pbmc3k_processed = cached_dataset(sc.datasets.pbmc3k_processed); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-1050030241:421,wrap,wraps,421,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1050030241,4,['wrap'],"['wrapper', 'wraps']"
Integrability,"Thanks for the PR! We've been thinking about refactoring this part of the package, and this looks like an interesting way to do it. However, we're not accepting any additions to the `external` module anymore. Instead we are pointing people to the broader [scverse ecosystem](https://scverse.org/packages/#ecosystem). We may be interested in using this as a direct dependency but may need to do some research into this first + request/ add a few features in `Marsilea` such as dot plots. cc @grst",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2512#issuecomment-1597429208:364,depend,dependency,364,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2512#issuecomment-1597429208,1,['depend'],['dependency']
Integrability,"Thanks for the clarification. I don't have a good idea for a middle ground integration now. If possible, I would like to hear what @flying-sheep suggests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2444#issuecomment-2379271488:75,integrat,integration,75,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444#issuecomment-2379271488,1,['integrat'],['integration']
Integrability,"Thanks for the fix!. For the `p` vs `q` thing, I was just thinking if the user passes a string that doesn't start with `p`, the error message could be something like `""ValueError: Couldn't understand string value '{passed_val}' for vmax. Percentile cutoffs can be specified like 'p99' (99th percentile).""`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/800#issuecomment-526476496:134,message,message,134,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/800#issuecomment-526476496,1,['message'],['message']
Integrability,"Thanks for the information. Very interesting read. From the paper it seems that the Leiden method indeed is superior to the louvain method. However, instead of replacing the louvain function I think is better to explicitly create a leiden clustering function as you have done and you can add the n_iterations parameter. . I would suggest to add a PR with this function as part of tools, but because of the new dependency required I am not sure. @falexwolf any suggestions?. Meanwhile I will give it a try.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/350#issuecomment-437026529:410,depend,dependency,410,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350#issuecomment-437026529,1,['depend'],['dependency']
Integrability,"Thanks for the quick response, @flying-sheep!. I can confirm that updating _pandas-2.2.2_ does fix this. I totally missed this possibility; it's not clear to me why the dots would change ordering, but the totals wouldn't (maybe _scanpy_ relies on default _pandas_ behaviour that changed between 1.x and 2.x?). That said, _pandas-2.x_ unfortunately breaks some dependencies in our environment, so I'll either pin _scanpy_ or use your workaround. Regarding the ordering and issue title change. Maybe a nit, but it's my understanding that the default ordering is alphabetical (which makese perfect sense as a default!). If this is correct, then I'd suggest that the wrong ordering is not the totals, but the categories themselves. Given this, the workaround that gives me the expected behaviour would be `dp.categories_order = dp.dot_color_df.index.sort_values()`:; <img width=""439"" alt=""image"" src=""https://github.com/scverse/scanpy/assets/5192495/6f7622f5-14b5-4ea5-a44f-288c4507c4f0"">",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3062#issuecomment-2115841629:360,depend,dependencies,360,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3062#issuecomment-2115841629,2,['depend'],['dependencies']
Integrability,"Thanks for the quick responses @LuckyMD and @ivirshup.; If `obsm` entries were accessible for plotting functions that would be fantastic. It would really solve all our problems. Once this is implemented I would only need to write a wrapper to model differences of activities between groups and that's it.; Looking forward for this update, thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1724#issuecomment-795050056:232,wrap,wrapper,232,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1724#issuecomment-795050056,1,['wrap'],['wrapper']
Integrability,"Thanks for the response! The core `reduce` function of SCA is not scanpy-based, but I wrote a very simple wrapper called `reduce_scanpy` to make it easier for scanpy users while this pull request is being considered. It would be even easier for scanpy users to access this code natively in `sc.tl.external`, and it seems odd that the existence of the wrapper (which just runs `reduce` and adds the result to the input AnnData) should disqualify it. Although the current pull request implements `sc.tl.external.sca`as an additional wrapper to `reduce_scanpy`, I could easily write it as a wrapper to `reduce`, which would remove the redundancy of having separate scanpy interfaces in the base package and in sc.tl.external. I would then mark `reduce_scanpy` as deprecated in further releases of SCA, and direct the user instead to `sc.tl.external.sca`. Does this seem reasonable? Of course, I'd be happy to be part of `ecosystem` if that's still where you think it belongs!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1780#issuecomment-825877662:106,wrap,wrapper,106,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780#issuecomment-825877662,10,"['interface', 'wrap']","['interfaces', 'wrapper']"
Integrability,"Thanks for the responses!. > Sounds reasonable, would you want to add anything more to Ecosystem about this? Also, would you like to make a PR with the deprecation message pointing to scvi-tools?. I already made a PR to update the docs (https://scanpy.readthedocs.io/en/latest/ecosystem.html). I can also do the latter. > With the scanpy ecosystem growing also with packages outside of scanpy external, it would probably be good to have a page with all packages that are part of this ecosystem. Ideally this would be on the Scanpy page I guess as it would be a first port-of-call for users.; >; > Such a page could have brief explanations of the capabilities of each of these tools, and link to tutorials from all the independent packages that show how these tools are integrated into a standard Scanpy workflow. What do you guys think?. Is this different than the ecosystem page? Sounds reasonable though",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1443#issuecomment-703693133:164,message,message,164,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1443#issuecomment-703693133,2,"['integrat', 'message']","['integrated', 'message']"
Integrability,Thanks for this PR @sjfleming . If you don't mind I will integrate this functionality into a new PR that also covers #512 and #525,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/524#issuecomment-471454944:57,integrat,integrate,57,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/524#issuecomment-471454944,1,['integrat'],['integrate']
Integrability,"Thanks for your answer！; I will spend time finding where the problem is.; ^-^. ---Original---; From: ""Philipp ***@***.***&gt;; Date: Thu, Nov 16, 2023 16:49 PM; To: ***@***.***&gt;;; Cc: ***@***.******@***.***&gt;;; Subject: Re: [scverse/scanpy] sc.pp.filter_genes runs out of memory (Issue#2754). ; Seems like you ran out of memory. Maybe you only have enough memory to run store your data once, but for calculating this, a second copy has to be made.; ; If you think this is a bug and scanpy uses much more memory than it should here, please tell us and we will reopen this issue.; ; —; Reply to this email directly, view it on GitHub, or unsubscribe.; You are receiving this because you authored the thread.Message ID: ***@***.***&gt;",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2754#issuecomment-1817861190:710,Message,Message,710,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2754#issuecomment-1817861190,1,['Message'],['Message']
Integrability,"Thanks for your comments! This is an interesting discussion. I agree that the validity of a p-value depends on the user's data. From the standpoint of an analysis tool, I think it's better to report a p-value (and a t-statistic, which was already reported in the original function) and let the user decide what to do with that information.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/270#issuecomment-425729339:100,depend,depends,100,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270#issuecomment-425729339,1,['depend'],['depends']
Integrability,"Thanks for your comments, I understand the struggle of implementing CI for GPU code!. @Zethson here are my answers to your questions:; 1. Instead of checking if a gpu is available, I would suggest to rather check if the related library is installed (depending on the method, it could be cugraph, cupy or cuml) since each of these libraries always require a GPU at installation and usage, I think using these as check would suffice.; 2. I agree with moving to the usage of 'device' as much as possible. It should be easily possible to rename ""method""/""flavor"" to ""device"" for `tl.draw_graph`, `tl.leiden` and `tl.louvain`, and use only ""cpu""/""gpu"" as choices as theses parameters would have only two choices anyway. In most case this would indeed remove the name of the python backend used, but one could instead mention it in the api/doc. ; `pp.neighbors` is a bit more tricky to handle, running it in gpu mode lead to a combination of distances/neighbors calculation with gpu/cuml backend and then connectivities calculations on cpu/umap backend, this could be solved if maintainers of cuml decide to allow the latter to be computed with cuml: https://github.com/rapidsai/cuml/issues/3123. Since it will take time before CI can be implemented, I can just add the easy small changes proposed on 2. and let the PR open so you decide what to do later!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1533#issuecomment-816665412:250,depend,depending,250,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1533#issuecomment-816665412,1,['depend'],['depending']
Integrability,"Thanks for your reply! (And no worries - mine is even later). . I see your point about ecosystem vs. external. My main qualm about ecosystem (at least in its current form) is that it's just links to external projects that happen to use scanpy, and the burden of downloading these projects, learning their unique syntax, and seeing how they apply to the scanpy project at hand is off-loaded to the user. The main reason I have pushed for inclusion in external is the convenience of being able to call the function with a single scanpy command, in a format the user is already very familiar with. On the other hand, I do see your point about code maintenance and syncing between my project and scanpy. Changes in my shannonca project might necessitate changes in the wrapper function. That said, since my wrapper is very agnostic to the underlying methods used, I would hope this wouldn't have to happen very often (basically, it just controls where the inputs are found and where the outputs are deposited. This wouldn't change unless scanpy's architecture did). However, as currently written, the documentation may have to change more frequently since it refers to specific function arguments used in my package. For now, I am willing to open a new pull request into ecosystem (if that is the correct workflow) and you can feel free to close this issue. For future releases, if you want to combine the convenience of external with the low maintenance burden of ecosystem, you might consider allowing external modules to ""outsource"" their documentation. So in scanpy's documentation, a function F under external would simply have the format sc.external.tl.F(adata, **kwargs), where **kwargs is passed directly to a method maintained by the tool developer, with a link to a docstring in the external repository. I would happily make this for shannonca as a proof of concept, if you think it's worth trying.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1780#issuecomment-911791808:765,wrap,wrapper,765,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780#issuecomment-911791808,4,['wrap'],['wrapper']
Integrability,Thanks for your thoughts on this!. 1. That seems like a good approach - the size of the grid cells would be adjusted for each plot depending on cell number? ; 3. This probably depends a lot on the kind of dataset and comparison one wants to make: Does one want to know if there are *any* differences at all or also *where* on the grid these difference are? Up to now my approach was to use clusters as 'grid' and calculate the differences in proportion of cells per cluster across two or more conditions. The reason that I like your approach is that it is a very good (qualitative) visualization and much more subtle then just binning the data based on clusters. It seems hard to capture this visual aspect with a statistic.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/575#issuecomment-478830365:131,depend,depending,131,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575#issuecomment-478830365,2,['depend'],"['depending', 'depends']"
Integrability,"Thanks so much for your contribution @dwnorton ! Looks good. Two questions: 1) is there any way to do without densifying the matrix? it's not a big deal though since the matrix is small. 2) Is the UMAP API for the `umap.distances.pairwise_special_metric` function version-dependent somehow in other words do we need to constrain our umap dependency to a specific version, or is the API stable across all commonly used UMAP versions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1413#issuecomment-697932813:272,depend,dependent,272,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413#issuecomment-697932813,2,['depend'],"['dependency', 'dependent']"
Integrability,"Thanks! Can I ask for two clarifications before replying:. > Right now, we tend to use a connectivity graph built by UMAP ... UMAP uses Pynndescent to construct the kNN graph. So does it mean that you depend on Pynndescent to construct the kNN graph, and then if the user calls UMAP, it's run on this previously constructred kNN graph?. By the way, openTSNE uses Annoy instead of Pynndescent for non-sparse input data and simple metrics that are supported by Annoy (like Euclidean or cosine). It seems to work quite a bit faster. For sparse input data and/or fancy metrics, it uses Pynndescent. > ... but are working on making this more generic. We're thinking about allowing the UMAP embedding to be generated on graphs we provide as well. What are the use cases here that you thinking of?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1233#issuecomment-633311550:201,depend,depend,201,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1233#issuecomment-633311550,2,['depend'],['depend']
Integrability,"Thanks!. On Wed, Jun 7, 2023 at 9:34 AM Philipp A. ***@***.***> wrote:. > I had the same thought and opened #2505; > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2505&g=NmVkM2RiMWY2M2U4YzZhYw==&h=YTlmZWU5MDlhNTJlOWJjMTkxZDczZTg2MGE2ODdiNzU2NmIwYjE2OTMzZTczY2M1ZjNlNzEyM2Q0Mjc1OWM5Yg==&p=YzJlOmltbXVuYWk6YzpnOjBhNjA3ZDgxZmY2OGQ1YTVjYWY3YWUzM2MzZGM0MDU3OnYxOmg6VA==>; > to track that!; >; > —; > Reply to this email directly, view it on GitHub; > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500%23issuecomment-1580835422&g=YWNlMjU3YjI5ODM4NTJkYQ==&h=ZTJiNzVlYzQ0NzM5YmY0ZTdiMWEzMDQ2MmQ0MGMwOWZmZTVlOGRhN2JmYjZiYTcxYjg1Nzg3OTRjMzEwZDY3OA==&p=YzJlOmltbXVuYWk6YzpnOjBhNjA3ZDgxZmY2OGQ1YTVjYWY3YWUzM2MzZGM0MDU3OnYxOmg6VA==>,; > or unsubscribe; > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/notifications/unsubscribe-auth/AUHCMAR6XJXLYMBK224NMETXKB7MDANCNFSM6AAAAAAY3HAO3E&g=OGRhMDE2YzcyZWIwNGMxNg==&h=M2I1NTIwM2JlNTIwNjA4MGViYjE3YTRmYjQ0MWM3NzNhYzNkNjBlNzVjYjg1NDUwMGVkMjJhNWFkYmZlZTIxYQ==&p=YzJlOmltbXVuYWk6YzpnOjBhNjA3ZDgxZmY2OGQ1YTVjYWY3YWUzM2MzZGM0MDU3OnYxOmg6VA==>; > .; > You are receiving this because you authored the thread.Message ID:; > ***@***.***>; >. -- ; PLEASE NOTE: The information contained in this message is privileged and ; confidential, and is intended only for the use of the individual to whom it ; is addressed and others who have been specifically authorized to receive ; it. If you are not the intended recipient, you are hereby notified that any ; dissemination, distribution, or copying of this communication is strictly ; prohibited. If you have received this communication in error, or if any ; problems occur with the transmission, please contact the sender.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2500#issuecomment-1580840698:1245,Message,Message,1245,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2500#issuecomment-1580840698,2,"['Message', 'message']","['Message', 'message']"
Integrability,"Thanks, @LisaSikkema; I figured it out shortly after I sent my message. For now, I only have these solutions, but I don't like any of them:; - Filtering this warning. This is not very clean: I would prefer to really solve the issue. Also, it makes a copy, which is memory expensive, as I sometimes have about `10^7` observations.; - Creating a copy so that I don't use a view. Same issue about the memory.; - Plotting this UMAP on the complete anndata object so that it stores the colors, and then plotting with the view I'm creating. The problem is that, for my use case, I don't want to plot it with the entire dataset, so it makes a useless plot. I think I will go for the first solution, even though I'm not really satisfied.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2315#issuecomment-1257146488:63,message,message,63,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2315#issuecomment-1257146488,1,['message'],['message']
Integrability,"Thanks, and you're welcome! I figure if a scanpy user wants to take the plunge and use the OO interface, they shouldn't have to change the way they interact with their AnnData. Thanks again!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/139#issuecomment-386322714:94,interface,interface,94,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/139#issuecomment-386322714,1,['interface'],['interface']
Integrability,"Thanks, that's all very helpful. I'll work on getting these recommendations integrated. One quick question, is this still the most efficient way to get the data for one gene for the violin plot?. selected = adata[:, adata.var_names.isin([gene.gene_symbol,])]. And before all the data was just in relational tables but, of course, the scale was a lot less. EDIT: I just re-read and saw that it seems you can pass the gene name to the violin() call itself. Beautiful magic.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/85#issuecomment-370200511:76,integrat,integrated,76,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/85#issuecomment-370200511,1,['integrat'],['integrated']
Integrability,"That being said, maybe this better be reflected in the conda dependencies, so pandas 1.0 is not pulled with scanpy?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1028#issuecomment-583439056:61,depend,dependencies,61,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1028#issuecomment-583439056,1,['depend'],['dependencies']
Integrability,"That problem occurs within h5py (we just wrap the underlying OSError) and isn’t a consequence of how scanpy uses h5py. The relevant part of the traceback is:. ```pytb; OSError: Can't read data (file read failed:; time = Sat Aug 1 13:27:54 2020,; filename = '/path.../filtered_gene_bc_matrices.h5ad',; file descriptor = 47,; errno = 5,; error message = 'Input/output error',; buf = 0x55ec782e9031,; total read size = 7011,; bytes this sub-read = 7011,; bytes actually read = 18446744073709551615,; offset = 0); ```. The reported filename looks weird: `'/path.../filtered_gene_bc_matrices.h5ad'`. Is that file on some network share or colab or so? Because that’d explain wonky I/O. 18 exabytes (18 quintillion bytes!) read seems really off too!. I assume `self.group[""data""][...]` tries to read all the data for `.X` and some bug or connection problem tells h5py that there’s 18 exabytes. h5py then asks the OS to give them those 18 exabytes which the OS politely denies. See also:. - https://github.com/googlecolab/colabtools/issues/559; - https://forum.hdfgroup.org/t/errors-accessing-hdf5-over-cifs-and-or-nfs/6341",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1351#issuecomment-667531196:41,wrap,wrap,41,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351#issuecomment-667531196,2,"['message', 'wrap']","['message', 'wrap']"
Integrability,"That's a good point, and it is not:; ```python; reducer = umap.UMAP(min_dist=0.5); embedding = reducer.fit_transform(adata.obsm[""X_scVI""]); adata.obsm[""X_umap""] = embedding; ```; again produces stable results on only 3/4 CPUs. . Ok, let's forget about UMAP. It's only a nice figure to get an overview of the data and I don't use it for downstream stuff. Irreproducible clustering, on the other hand, is quite a deal-breaker, as for instance cell-type annotations depend on it. I mean, why would I even bother releasing the source code of an analysis alongside the paper if it is not reproducible anyway? . I found out a few more things: ; - the leiden algorithm itself seems deterministic on all 4 nodes, when started from a pre-computed `adata.obsp[""connectivities""]`. ; - when running `pp.neighbors` with `NUMBA_DISABLE_JIT=1`, the clustering is stable on all four nodes (but terribly slow, ofc); - when rounding the connectivities to 3-4 digits, the clustering is also stable (plus the total runtime is reduced from 2:30 to 1:50min). ```python; adata.obsp[""connectivities""] = np.round(adata.obsp[""connectivities""], decimals=3); adata.obsp[""distances""] = np.round(adata.obsp[""distances""], decimals=3); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2014#issuecomment-946539365:463,depend,depend,463,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2014#issuecomment-946539365,1,['depend'],['depend']
Integrability,"That's great, I'll add Isaac to that project so he can see code (repo is private). Let's discuss whether to integrate in scanpy at next meeting! Thank you Sergei !",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1383#issuecomment-693400228:108,integrat,integrate,108,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383#issuecomment-693400228,1,['integrat'],['integrate']
Integrability,"That's not a bug, that's a feature ;). You can only compute as many PCs as the minimum number of dimensions in n_samples and n_features. Do you feel as though the error message is unclear on this? I feel as though changing the default to match the setting can be dangerous as may not recall how many PCs were used then.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1051#issuecomment-586474464:169,message,message,169,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1051#issuecomment-586474464,1,['message'],['message']
Integrability,"That's weird. Why would cuda be a dependency?. I'm not sure who is maintaining the bioconda recipe, so that might just be wrong. Do the new installation instructions work?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1142#issuecomment-608191630:34,depend,dependency,34,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1142#issuecomment-608191630,1,['depend'],['dependency']
Integrability,"That’s weird, but that might be another issue, please check out #1378. /edit: seems to be a conda bug that only occurs on windows due to flit ([legally](https://www.python.org/dev/peps/pep-0376/#record)) writing windows newlines into the RECORD file, and conda reading them as two newlines each and then crashing. ---. This PR adds instructions on how to integrate with conda, which I screenshotted. It fails for me with this error:. ```; Collecting package metadata (repodata.json): done; Solving environment: failed. ResolvePackageNotFound: ; - loompy[version='>=3.0.5']; ```. But since loompy 3.x isn’t on conda-forge, that’s correct. Seems that resolving anndata’s dependencies on conda is currently not possible and you need to wait until loompy gets upgraded on conda-forge. Or until Quansight-Labs/beni#3 is resolved and you can specify that you don’t want all deps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1377#issuecomment-675423209:355,integrat,integrate,355,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1377#issuecomment-675423209,2,"['depend', 'integrat']","['dependencies', 'integrate']"
Integrability,"The Leiden algorithm is now [included](https://igraph.org/python/doc/igraph.Graph-class.html#community_leiden) in the latest release of `python-igraph`, version 0.8.0. I believe this alleviates the need to depend on the `leidenalg` packages. The Leiden algorithm provided in `python-igraph` is substantially faster than the `leidenalg` package. It is simpler though, providing fewer options, but I believe the more extensive options of the `leidenalg` package are not necessarily needed for the purposes of `scanpy`. We provide binary wheels on PyPI and binaries for conda are available from the conda-forge channel, also for Windows.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1053:206,depend,depend,206,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053,1,['depend'],['depend']
Integrability,"The UMAP (actually neighbours in the current implementation) is already now derived from any embedding the user wants, including integration embedding, so this is not an issue in itself.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2259#issuecomment-1133920985:129,integrat,integration,129,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2259#issuecomment-1133920985,1,['integrat'],['integration']
Integrability,"The `scipy.sparse` wrapper is actually interesting. I think it's tricky to add directly to SciPy, but it could be split out as a separate package that users could use and we could link to in the `scipy.sparse` docs. With some context about things like `@` vs `*` users can then make an informed decision to use it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/921#issuecomment-557319885:19,wrap,wrapper,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-557319885,1,['wrap'],['wrapper']
Integrability,"The additional cases that would need to be handled (and what I believe the current internal solutions are):. * What if colors haven't been saved to uns yet; * If it's an view, we just return what the colors would be by default; * If it's an ""actual"" we return the colors, but also assign them to the object; * What if there's a different number of colors and categories; * We warn and reassign the colors. If it's a view, we don't modify the object, but probably still warn. Most of the logic for handling this internally is wrapped in `_get_palette`: https://github.com/theislab/scanpy/blob/ed364a887db2eb604d0a09cd72325cb5e1f4e27e/scanpy/plotting/_tools/scatterplots.py#L1192-L1204",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1881#issuecomment-863776992:525,wrap,wrapped,525,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1881#issuecomment-863776992,1,['wrap'],['wrapped']
Integrability,"The broader issue here is what we do and don't support with `backed` mode. As our dask support grows, that will likely be the recommended route going forward. Thus, it would make sense to come up with concrete recommendations for that so we have ""officially supported"" out of core functionality.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2894#issuecomment-2012531056:138,rout,route,138,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2894#issuecomment-2012531056,1,['rout'],['route']
Integrability,"The code in UMAP and pynndescent is very close now, but I don't know when UMAP is going to use the pyyndescent dependency. So I thought it would be useful to have this PR in the meantime.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/659#issuecomment-495163867:111,depend,dependency,111,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659#issuecomment-495163867,1,['depend'],['dependency']
Integrability,"The dependencies of scanpy state that anndata>=0.7 are required, please update:. https://github.com/theislab/scanpy/blob/c255fa10fb75f607780ed7d9afc6683cbcecc38e/requirements.txt#L1. Pip would have fullfilled that requirement if you installed the development version using it, but I assume you cloned and installed it manually?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1125#issuecomment-602818921:4,depend,dependencies,4,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1125#issuecomment-602818921,1,['depend'],['dependencies']
Integrability,"The function documentation for [`filter_cells`](https://scanpy.readthedocs.io/en/stable/generated/scanpy.pp.filter_cells.html#scanpy.pp.filter_cells) has the following under ""Returns"":. > **number_per_cell** : [ndarray](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray); > Depending on what was thresholded (counts or genes), the array stores `n_counts` or `n_cells` per gene. I have a feeling this is copied from [filter_genes](https://scanpy.readthedocs.io/en/stable/generated/scanpy.pp.filter_genes.html#scanpy.pp.filter_genes) and should read . > the array stores `n_counts` or `n_cells` per **_cell_**. If not, perhaps more explanation is needed as to why this output is identical to the [filter_genes](https://scanpy.readthedocs.io/en/stable/generated/scanpy.pp.filter_genes.html#scanpy.pp.filter_genes) function.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3332:306,Depend,Depending,306,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3332,1,['Depend'],['Depending']
Integrability,"The initial problem is due to the fact that the new 'highly_variable_genes' function does not take numpy arrays anymore: https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/highly_variable_genes.py. It's also mentioned in the docs, but we should, of course, have thrown a clear error message. Now it does: https://github.com/theislab/scanpy/commit/a578ced0b2e44b26998fb9e08c5bb0ffb82a7a4b. To return the annotation, one can set `inplace=False`. But the updated plotting function also takes the full `AnnData` object.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/391#issuecomment-445515304:300,message,message,300,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391#issuecomment-445515304,2,['message'],['message']
Integrability,"The issue is that there was a blank line starting with three spaces, and that triggered an error whose message wasn't particularly related to that. I think this test could probably get rewritten. https://github.com/theislab/scanpy/blob/a8ee1e01e6cea1d3b9f5474997508c99497d4fb4/scanpy/tests/test_docs.py#L18-L38. The error came from the `if any(broken)` block. Basically it's checking for lines which aren't 1) empty, 2) start with a four space indent. The error message is specific to the first line. The PR had a three line indent in between paragraphs triggering the failure. @flying-sheep, thoughts on removing this part of the test? Should we have a separate rst linting check?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1484#issuecomment-725894623:103,message,message,103,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1484#issuecomment-725894623,2,['message'],['message']
Integrability,"The issue that you mention has been reported to matplotlib 3.1 and the; solution is to downgrade to 3.0*. I just updated the dependencies of; scanpy to be matplotlib 3.0. As soon as this is solved we will update the; dependencies. On Mon, May 27, 2019 at 3:33 PM bioguy2018 <notifications@github.com> wrote:. > Dear all; > I would like to project my umap from scanpy in 3d but I have faced the; > following problem:; >; > ValueError: operands could not be broadcast together with remapped shapes; > [original->remapped]: (0,4) and requested shape (816,4); >; > It's very strange because before I update some of my packages, I could run; > it it with no problem with the following packages:; >; > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4; > scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760; > louvain==0.6.1; >; > but after updating some of my packages it was not possible due to that; > error!; >; > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1; > pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0; > python-igraph==0.7.1+4.bed07760 louvain==0.6.1; >; > Should I roll back to the previous version of annadata or scanpy? has; > anyone ran this feature with my package version with no problems?; >; > Thanks a lot; >; > Here are the packages I use; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/663?email_source=notifications&email_token=ABF37VPMR3WSZT3FIGCFNJ3PXPPJ3A5CNFSM4HP4ASU2YY3PNVWWK3TUL52HS4DFUVEXG43VMWVGG33NNVSW45C7NFSM4GWBCDVA>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABF37VJLTRD6ZHIBLZIRHYLPXPPJ3ANCNFSM4HP4ASUQ>; > .; >. -- . Fidel Ramirez",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/663#issuecomment-496226076:125,depend,dependencies,125,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663#issuecomment-496226076,4,['depend'],['dependencies']
Integrability,"The packages dorothea-py and progeny-py are now deprecated, instead one should use decoupler (https://github.com/saezlab/decoupler-py). It contains both prior knowledge resources plus many more since it integrates the meta-resource OmniPath, and it also contains many footprint enrichment methods instead of a fixed one.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2186:203,integrat,integrates,203,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2186,1,['integrat'],['integrates']
Integrability,"The problem is not the collapsible thing. I still don't see the images here but only links instead, and when I click on a link e.g. https://user-images.githubusercontent.com/5758119/115158730-f0768a00-a08f-11eb-939a-1b9c35373fae.png I get an error message that the image cannot be displayed because it contains errors. Odd. Maybe it's something weird on my side.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-822059389:248,message,message,248,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-822059389,1,['message'],['message']
Integrability,"The problem referenced above is that we want to skip some doctests when using pandas<2 since outputs changed slightly. doctestplus doesn't currently solve that problem, since we use `doctest-requires` to skip a block when that block exists in a docstring. `doctest-requires` blocks only work in `.rst` files. For code files there's only the option to skip the whole file via `__doctest_requires__`. https://github.com/scientific-python/pytest-doctestplus?tab=readme-ov-file#doctest-dependencies. I think we can request this feature, but it doesn't exist now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2729#issuecomment-1934516029:482,depend,dependencies,482,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2729#issuecomment-1934516029,1,['depend'],['dependencies']
Integrability,"The reason I actually noticed it in the first place was a difference in t-SNE and UMAP... the difference wasn't huge, but noticeable. Here's the issue where you can see the UMAP plots (#324). The clustering differences were actually rather small... the more worrying thing was the UMAP at the time as it appeared to show a bridge between two clusters that I didn't really expect to be there and didn't see in the float64 data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/325#issuecomment-436028541:323,bridg,bridge,323,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325#issuecomment-436028541,1,['bridg'],['bridge']
Integrability,The remaining failed test is related to matplotlib 3.1.0 and 3d scatter plots. There is a report of a similar error (https://github.com/matplotlib/matplotlib/issues/14298). My suggestion is to wait for those issues to be solved and then upgrade the dependencies.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/661#issuecomment-496144015:249,depend,dependencies,249,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661#issuecomment-496144015,1,['depend'],['dependencies']
Integrability,"The reorganization of using the ""external API"" (shallow interfaces) via an `import scanpy.external as sce` and the ""internal API"" as accessible via `import scanpy as sc`, sort of, provided a solution to what bothered people the most: expecting the ""internal API"" to run through at a single install, be properly maintained etc. and the interfaces to external packages be clearly marked. I think this is a sustainable, long-term solution, which scales and is convenient for contributors. @flying-sheep agreed as I understood it. Do you think we need more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/457#issuecomment-460063977:56,interface,interfaces,56,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/457#issuecomment-460063977,4,['interface'],['interfaces']
Integrability,"The same error in `sc.pp.highly_variable_genes` can pop up also if you forget to `sc.pp.filter_genes(adata, min_cells=0)` before running normalization and logging. Some informative error messages could for sure save some time here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/763#issuecomment-1137813331:187,message,messages,187,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/763#issuecomment-1137813331,1,['message'],['messages']
Integrability,"The short answer is that `flit_core`, which provides the PEP 517 hooks, makes a minimal sdist which should always have the files you need to install the module, but may leave out e.g. tests and docs. The Flit CLI tries to make a 'publication quality' sdist. It's kind of an ugly compromise, because how I approached sdists (before PEP 517) wasn't a good fit for the PEP 517 `build_sdist` hook. I view sdists on PyPI as like a snapshot of the development process, so it should (by default) include everything that you'd get if you checked out the corresponding tag from git (except the git history). But using git assumes that it's something the maintainer makes once and publishes on PyPI. PEP 517 defined a `build_sdist` hook which user tools (like pip) can call. I didn't want this to depend on git, so I gave it a way to make working but minimal sdists. Specifying includes & excludes under `[tool.flit.sdist]` should affect both the Flit CLI and the PEP 517 hooks. So if you want to make the sdists to publish with `python -m build` or similar, you'll need to use those to determine what goes in.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1909#issuecomment-874715324:787,depend,depend,787,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1909#issuecomment-874715324,1,['depend'],['depend']
Integrability,"The tests started failing with 4499d9460b8fe6e08fffb3c4cae0948849d40586, which obviously changed nothing that could make tests fail. So I’m sure some dependency updating is responsible (Travis started installing the newwer versions and the tests fail). We need to investigate why that is and how to fix it.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/162:150,depend,dependency,150,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/162,1,['depend'],['dependency']
Integrability,"The variable folder has one file in .h5ad format as input or raw data. No, I execute the code correctly because every time I run this command or move forward with other commands, the number on the kernel increases without any error message. But in a folder, no object is generated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1795#issuecomment-817693943:232,message,message,232,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1795#issuecomment-817693943,1,['message'],['message']
Integrability,There is [something](https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.coloring.greedy_color.html#networkx.algorithms.coloring.greedy_color) if we want to make Networkx a dependency. Igraph only seems to have graph colouring [in their c library](https://igraph.org/c/doc/igraph-Coloring.html).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2313#issuecomment-1239413303:214,depend,dependency,214,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2313#issuecomment-1239413303,1,['depend'],['dependency']
Integrability,"There now is a much more powerful differential testing package `diffxpy`, @davidsebfischer, which easily integrates into Scanpy. @a-munoz-rojas Would you consider making a pull request that adds log-fold changes for t-test etc. in `rank_genes_groups`? My bandwidth is limited these days, I will certainly do it at some point, but it's faster if you do it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/159#issuecomment-420332760:105,integrat,integrates,105,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/159#issuecomment-420332760,1,['integrat'],['integrates']
Integrability,"There's a possibility of negative values depending on how careful you are with compensation and whether or not you clip values, but at least in my case the counts matrix was always non-negative. **Edit:** But that shouldn't matter because NNDescent is routinely called on PCA-embedded data which is zero centered, right? . If I can find a small subset of the matrix that produces this error reliably, I will share that with the `pynndescent` repo and link back here. Currently that's challenging given the original size of the matrix (a few million observations).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1696#issuecomment-802982688:41,depend,depending,41,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696#issuecomment-802982688,2,"['depend', 'rout']","['depending', 'routinely']"
Integrability,"There’s a few uses:. 1. Humans. Once you understand the syntax ([very easy](https://docs.python.org/3/library/typing.html), i just get `Generator` wrong all the time) it improves your understanding what a function really accepts and returns; 2. IDEs. They’ll get better when inferring the types of variables and will show you more actual problems in the code and less false positives; 3. Testing. Some projects use mypy to check if all code in your repo typechecks properly, which can be integrated into a test suite; 4. Runtime type checking. Has a performance hit (as said) but given proper type hints, it makes your code safer and the error messages better (“Function blah excepted a parameter foo of type Bar, but you passed a foo of type Baz”). i’m not planning to do 3 and 4 (yet, and probably never)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-441256142:488,integrat,integrated,488,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441256142,2,"['integrat', 'message']","['integrated', 'messages']"
Integrability,These changes enable Scanpy's pre-processing functions to run on distributed engines including Dask and Spark. The Spark integration itself relies on [Zap](https://github.com/lasersonlab/zap) for a distributed version of NumPy. . The main change is the `materialize_as_ndarray` function that is used at certain points of the computation to materialize intermediate results (not the full matrix). This is a no-op in the non-distributed case.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/283:121,integrat,integration,121,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/283,1,['integrat'],['integration']
Integrability,"They are capitalized, which means they are treated as constants and shouldn’t be modified. They *are* the original default value you want to refer to, so if you change them, you can’t do that anymore. So yes, `style` or the parameters in `def dotplot` are the correct way to do this. If you want a customized version, just do:. ```py; my_style = dict(...) # resusable customizations. sc.pl.dotplot(..., **my_style); ```. or. ```py; from functools import wraps. @wraps(sc.pl.dotplot); def my_dotpot(*args, return_fig: bool = False, **kw):; dp = sc.pl.dotplot(*args, **kw, return_fig=True); dp.style(...) # customize here; if return_fig:; return dp; dp.show(); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2708#issuecomment-1803768928:454,wrap,wraps,454,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2708#issuecomment-1803768928,2,['wrap'],['wraps']
Integrability,Think the Travis thing failing is because of the new dependencies,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/361#issuecomment-438281756:53,depend,dependencies,53,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361#issuecomment-438281756,1,['depend'],['dependencies']
Integrability,"This PR addresses #646 by adding the option to pass a dict to the plotting functions heatmap, dotplot, matrixplot, tracksplot and stacked_violin. . Now, when `var_names` is a dictionary the `var_group_labels` and `var_group_positions` are set such that the dictionary key is a label and the group is the dict values. In the following example the 'brackets' plot on top of the image are prepared based on the markers dictionary:. ```PYTHON; marker_genes_dict = {'B-cell': ['CD79A', 'MS4A1'], ; 'T-cell': 'CD3D',; 'T-cell CD8+': ['CD8A', 'CD8B'],; 'NK': ['GNLY', 'NKG7'],; 'Myeloid': ['CST3', 'LYZ'],; 'Monocytes': ['FCGR3A'],; 'Dendritic': ['FCER1A']}; # use marker genes as dict to group them; ax = sc.pl.dotplot(pbmc, marker_genes_dict, groupby='bulk_labels'); ```; ![image](https://user-images.githubusercontent.com/4964309/58255475-5dcaf480-7d6d-11e9-83f6-bb4ebc8e33a7.png). This PR also introduces a small change in `sc.pl.stacked_violin` by setting `cut=0` as default parameter for `seaborn.violin`. This produces in my opinion better plots by removing the extension of the violin past extreme points. This is specially useful to avoid the violin plot to extend below zero expression values. . **Update**: I set the dependencies to `matplotlib==3.0.*` and `scipy==1.2` to solve failing tests. More details in the conversation",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/661:1221,depend,dependencies,1221,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661,1,['depend'],['dependencies']
Integrability,"This PR addresses https://github.com/theislab/scanpy/issues/1645, which was caused by my injection of normalised matrices from Scanpy workflows, thereby bypassing a sparseness check Scrublet does with the raw matrix. The fix contained here is apply the Scrublet sparseness check, before calling Scrublet functions with a dependency on sparseness.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1707:89,inject,injection,89,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1707,2,"['depend', 'inject']","['dependency', 'injection']"
Integrability,"This PR addresses https://scanpy.discourse.group/t/workflow-for-selecting-number-of-marker-genes-in-sc-queries-enrich/286. I wanted to have a simple interface to get the top n marker genes. Right now, `rank_genes_groups_df` only allows to threshold on logfc and pval, but especially for marker genes pval computation might not be statistically meaningful. It adds the following kind of functionality:; ```python; import scanpy as sc. adata = sc.datasets.pbmc68k_reduced(); sc.tl.rank_genes_groups(adata, 'louvain'). print(sc.get.rank_genes_groups_df(adata, ""1"", n_top_genes=2)); ```; output is just the top 2 genes of the list.; ```; names scores logfoldchanges pvals pvals_adj; 0 FCGR3A 47.682064 5.891937 3.275554e-141 3.579712e-139; 1 FTL 45.653259 2.497682 9.003150e-208 6.887410e-205; ```. it also works for multiple groups:. ```python; print(sc.get.rank_genes_groups_df(adata, None, n_top_genes=2)); ```; ```; group names scores logfoldchanges pvals pvals_adj; 0 0 CD3D 26.250046 3.859759 4.379061e-75 2.233321e-73; 1 0 LDHB 21.207499 2.134979 1.488480e-67 5.993089e-66; 2 1 FCGR3A 47.682064 5.891937 3.275554e-141 3.579712e-139; 3 1 FTL 45.653259 2.497682 9.003150e-208 6.887410e-205; 4 2 LYZ 38.981312 5.096991 1.697105e-172 1.298285e-169; 5 2 CST3 34.241749 4.388617 1.448193e-149 5.539337e-147; 6 3 NKG7 34.214161 6.089183 2.356710e-55 2.575547e-53; 7 3 CTSW 24.584066 5.091688 2.026294e-39 9.118324e-38; 8 4 CD79A 52.583344 6.626956 4.032974e-84 7.713062e-82; 9 4 CD79B 32.102913 4.990217 1.958507e-51 1.872822e-49; 10 5 FTL 26.084383 1.844273 1.236398e-74 2.364611e-72; 11 5 LST1 25.554073 3.170759 5.653851e-81 4.325196e-78; 12 6 LYZ 31.497107 4.328516 9.041131e-106 6.916466e-103; 13 6 CST3 23.850258 3.281016 2.491629e-83 9.530482e-81; 14 7 CST3 33.024582 4.195395 5.768439e-136 4.412856e-133; 15 7 LYZ 31.264187 4.267053 9.712334e-101 1.485987e-98; 16 8 PPIB 39.260998 3.990153 7.159966e-47 3.651583e-45; 17 8 MZB1 33.305500 8.979518 7.611322e-26 1.878278e-24; 18 9 STMN1 27.133045 5.9",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2145:149,interface,interface,149,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2145,1,['interface'],['interface']
Integrability,"This PR addresses issues from #979 and #1103 related to `sc.pl.dotplot` and replaces PR #1127. Furthermore, the current PR attempts to unify common code between `dotplot`, `matrixplot` and `stacked_violin` plots while at the same time adding more flexibility to the plots. . This PR also makes possible to plot fold changes, log fold changes and p-values from `sc.tl.rank_genes_groups` as suggested in #562. To achieve this the `sc.pl.dotplot`, `sc.pl.matrixplot` and `sc.pl.stacked_violin` method had been transformed into wrappers for the new `DotPlot`, `MatrixPlot` and `StackedVioling` classes. Accessing the new classes directly allows further fine tuning of the plots. The use of the `sc.pl.dotplot`, `sc.pl.matrixplot` and `sc.pl.stacked_violin` didn't change, only the new parameter `return_fig` was added. For clarity, the relevant code has been moved to `scanpy.plotting._groupby_plots.py`. . The new plot classes are all descendants of a `BasePlot` class that captures most of the common code shared between the plots. The design of the classes follows the method chaining (as found in Pandas or Altair). This allows the addition of independent features (via well documented methods) to the plot without increasing the number parameters of a single function. This was first suggested here #956. . For example, for dotplot is now is possible to `add category totals`, define the titles for the colorbar and size legend and modify the values for edge color and edge line with for dot plot and the size exponent used (among may other options). For example:. ```PYTHON; adata = sc.datasets.pbmc68k_reduced(); markers = {'T-cell': ['CD3D', 'CD3E', 'IL32'], 'B-cell': ['CD79A', 'CD79B', 'MS4A1'], ; 'myeloid': ['CST3', 'LYZ']}; dp = sc.pl.dotplot(adata, markers, groupby='bulk_labels', return_fig=True); dp.add_totals(size=1.2)\; .legend(color_title='log(UMI count+1)', width=1.6, show_size_legend=True)\; .style(cmap='Blues', dot_edge_color='black', dot_edge_lw=1, size_exponent=1.5)\; .show(); ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1210:524,wrap,wrappers,524,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1210,1,['wrap'],['wrappers']
Integrability,"This PR adds a module `sc.metrics` for functions which wouldn't modify an anndata object, but are useful calculations. I'm basing this on `sklearn.metrics`, namely, how `sklearn` has separated transformers (`sc.tl`) from measurements. I've started it with two functions, `confusion_matrix` and `gearys_c` but think there are more use cases (e.g. `modularity`). I'm open to this not being a module, but I think these methods should be available and I'm not sure where they'd fit within the current api. My vision for this module is to make it easier to calculate values based on values you'd get using the scanpy ecosystem. Methods that would be included would be either *a)* not available in other libraries (`gearys_c`) or *b)* are available, but have difficult interfaces (`confusion_matrix`). ## `sc.metrics.confusion_matrix`. Creates a confusion matrix for comparing categorical labels. This is based on `sklearn.metrics.confusion_matrix` but is easier to use, and returns a object with labels. I think this is mostly done, though I'm considering changing the calling convention. Here's an example of usage:. ```python; import scanpy as sc; import seaborn as sns. pbmc = sc.datasets.pbmc68k_reduced(); sc.tl.leiden(pbmc); sns.heatmap(sc.metrics.confusion_matrix(""bulk_labels"", ""leiden"", pbmc.obs)); ```. ![image](https://user-images.githubusercontent.com/8238804/68737959-1a28b780-0639-11ea-8576-4cf1907066d9.png). I've copied `seaborn`s calling convention here, but I think that could change. Right now the above call is equivalent to:. ```python; sc.metrics.confusion_matrix(pbmc.obs[""bulk_labels""], pbmc.obs[""louvain""]); ```. But I wonder if it would make more sense to have the DataFrame go first if it's provided. I've also based the API around my usage of confusion matrices, so I'm very open to more general feedback on this. My reason for including it here was the amount of code it took wrapping `sklearn.metrics.confusion_matrix` to get useful output. ## `sc.metrics.gearys_c` ([Wiki pag",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/915:763,interface,interfaces,763,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915,1,['interface'],['interfaces']
Integrability,This PR adds the `--doctest-modules` flag to pytest and makes changes necessary to allow the doctests to run. Those changes include:. - Fixing verbosity; - using [pytest-doctestplus](https://github.com/scientific-python/pytest-doctestplus) to skip doctests entirely or when their dependencies are absent; - fixing broken doctests (the bulk of the changes),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2605:280,depend,dependencies,280,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2605,1,['depend'],['dependencies']
Integrability,"This PR aims to add more GPU functionalities and to integrate more an exisiting one:; * `tl.draw_graph` and `tl.leiden` can now both be GPU accelerated using rapids framework.; * on `pp.neighbors`, now 'rapids' method allows for more metrics. Calculated distance does not need to be root squared anymore (_See https://github.com/rapidsai/cuml/issues/1078#issuecomment-551284134_). I also have slightly rearranged the code to integrate more 'rapids' into the general processing of neighbors, to ensure that distances and connectivities results between 'rapids' and 'umap' are the same.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1533:52,integrat,integrate,52,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1533,2,['integrat'],['integrate']
Integrability,"This PR extends the original PR #512 by @gokceneraslan which adds the `standard_scaling` parameter to matrixplot. . I added the same functionality to dotplot, heatmap and stacked_violin. Also, I integrated PR #524 by @sjfleming which adds a `smallest_dot` option to dotplot.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/528:195,integrat,integrated,195,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/528,1,['integrat'],['integrated']
Integrability,"This PR introduces a family of plots to validate gene markers obtained using `scanpy.api.tl.rank_genes_groups`. The plots work similarly as `scanpy.api.pl.rank_genes_groups`:; ; ![image](https://user-images.githubusercontent.com/4964309/43768141-efa3633a-9a36-11e8-856f-3970352c33a8.png). ![image](https://user-images.githubusercontent.com/4964309/43768035-b8ba9082-9a36-11e8-91e6-8c828f456981.png). ![image](https://user-images.githubusercontent.com/4964309/43768064-c78dc64c-9a36-11e8-95ed-351ef6fff99e.png). ![image](https://user-images.githubusercontent.com/4964309/43768099-d776a510-9a36-11e8-9708-959d5d0afe92.png). Those functions are wrappers around `scanpy.api.pl.heatmap`, `scanpy.api.pl.stacked_violin`, `scanpy.api.pl.dotplot` and `scanpy.api.pl.matrixplot`. `heatmap` and `dotplot` were modified to allow the 'brackets' on top of the images. This functionality can be used directly from those plots. E.g.:. ![image](https://user-images.githubusercontent.com/4964309/43771788-83bbb604-9a40-11e8-90d6-51f084343a98.png). The `pl.stacked_violin` plot was before part of `pl.violin` but I thought that the code is cleaner by separating the two plots. Also, this PR adds the new plot `scanpy.api.pl.matrixplot` that plots the average gene expression per category. E.g.:. ![image](https://user-images.githubusercontent.com/4964309/43771966-07c9bd92-9a41-11e8-818e-263dfae69b7f.png). This PR also updates several test for the plotting options and adds new ones. . I tried to update the documentation to reflect the changes. Also a new dataset called `pbmc68k_reduced` was added. This dataset is used for tests and for the example notebook [here](https://gist.github.com/fidelram/2289b7a8d6da055fb058ac9a79ed485c). This dataset contains around 700 cells and 200 genes from the original 68k 10x genomics dataset and is saved as a small anndata object. It contains cell type annotations, UMAP coordinates and rank_gene_groups. The dataset can be accessed as; ```python; adata = sc.datasets.pbmc68k_r",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/228:642,wrap,wrappers,642,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/228,1,['wrap'],['wrappers']
Integrability,"This PR overhauls the `_compat` submodule. 1. it fixes the typing exactly like scverse/anndata#1692; 2. it switches all functions and methods to a `legacy_api` wrapper that raises a `FutureWarning`, and paves the way for a potential making-optional of `legacy-api-wrap`",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3264:160,wrap,wrapper,160,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3264,2,['wrap'],"['wrap', 'wrapper']"
Integrability,"This PR stores modularity metric into adata.uns. I also added `key_added` as a suffix to uns key ""louvain"", so that we can run louvain with different parameters and store their quality metrics separately. ```python; import scanpy as sc. sc.settings.verbosity = 3; adata = sc.datasets.pbmc3k(); sc.pp.pca(adata, svd_solver='arpack'); sc.pp.neighbors(adata); sc.tl.louvain(adata, resolution=2.0, key_added='newkey'); adata.uns; ```. ![image](https://user-images.githubusercontent.com/1140359/64479798-2a7c6c00-d18a-11e9-9c83-179ddfc54a8a.png). I'm really bad at writing warning messages and naming things, so feel free to edit these :)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/819:576,message,messages,576,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/819,1,['message'],['messages']
Integrability,"This also fixes a few problems namely that tests are no modules, so you should use fixtures instead of importing. If we want test tools that dependent packages can use we should create a submodule like `scanpy.test_utils` or so. I forgot to add the new locations to `tool.black.exclude`, so the files got reformatted. I hope that’s OK?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1528:141,depend,dependent,141,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1528,1,['depend'],['dependent']
Integrability,"This error message was hard to debug! Indeed it is due to the behavior of `sc.pp.neighbors`. Cells are sometimes given different numbers of neighbors. Sometimes that the errant cells have a number of neighbors greater than zero (unlike as seen in #2244, where the # of neighbors of some cells was 0). My fix builds on the one above and was:; ```; b = np.array(list(map(len, adata_ref.obsp['distances'].tolil().rows))) # number of neighbors of each cell; adata_ref_subset = adata_ref[np.where(b == DEFINED_NEIGHB_NUM-1)[0]] # select only those with the right number; sc.pp.neighbors(adata_ref_subset, DEFINED_NEIGHB_NUM) # rebuild the neighbor graph; ```; ___; @ViriatoII Your comment helped me fix things – but your fix itself didn't work for me. First, when I use `adata_ref = adata_ref[b]`, with `b` defined as above, it interprets `b` not as a boolean mask but as an index, returning a single cell duplicated over and over. I'm not sure what the intended behavior is here. My solution is to use `adata_ref[np.where(b == n_neigh-1)[0]]`. However, this subselection changes the number of neighbors that other cells have. For example, for my data,. ```; b = np.array(list(map(len,adata_ref.obsp['distances'].tolil().rows))); print(""Before subselecting"", np.unique(b, return_counts = True)). adata_ref_subset = adata_ref[np.where(b == DEFINED_NEIGHB_NUM-1)[0]]; c = np.array(list(map(len,adata_ref_subset.obsp['distances'].tolil().rows))); print(""After subselecting"", np.unique(c, return_counts = True)); ```; yields; ```; Before subselecting, (array([13, 14]), array([ 28, 1161359])); After subselecting, (array([10, 11, 12, 13, 14]),; array([ 15, 1, 633, 46, 1160664]))); ```. To solve both problems I needed to rebuild the neighbor graph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2085#issuecomment-1382325586:11,message,message,11,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2085#issuecomment-1382325586,1,['message'],['message']
Integrability,"This fixes the small progress when downloading the data as `sc.datasets.paul_15()`, etc..; Also fixes removing the file when `KeyboardInterrupt` occurs (previously, it was just on exceptions and at least for me I sometimes stop a download by interupting the jupyter kernel - now it also correctly removes the partially downloaded file). I've also opted for `requests` library, since it's much cleaner than urllib3 and as a dependency.; However, this is completely unrelated to the 2 things I've mentioned above and can be quickly reverted. Relevant PRs I could find:; - tqdm version (I opted for bumping the version [search through open issues, lowest one had 4.32]): https://github.com/theislab/scanpy/issues/1244. Not relevant:; My [reaction](https://www.youtube.com/watch?v=nixR6wVa4HY&t=72) when using the progress bar (slightly NSFW [foul language]).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1507:423,depend,dependency,423,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1507,1,['depend'],['dependency']
Integrability,"This has been discussed previously: https://github.com/theislab/scanpy/issues/1451, https://github.com/mwaskom/seaborn/issues/1423. I don't think that this sort of normalization is necessarily invalid or wrong, just situational. I also think it makes sense to mimic prior art, and this is how the argument works in seaborn. I do agree that just `x / max(abs(x))` is useful, and more often what people want to use here (if scaling at all). I like suggestion 2. more for this. I would suggest the following api:. ```python; normalization: Optional[Union[str, Callable[np.ndarray, np.ndarray]] (default: None); Normalization to apply to values. Can be selected from ""z-score"", ""minxmax_scale"", etc. or a Callable.; normalization_axis: Literal[""var"", ""group""] (default: ""var""); If a `normalization` is passed, which dimension of the data to normalize along.; ```. It would be nice if the normalization method was mentioned by default in the legend, but that can be difficult with how matplotlib doesn't really do text wrapping with it's notebook backend. Arguably, for `dotplot` `normalization` should be available for both size and color. What to you think @gokceneraslan @fidelram?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1757#issuecomment-804509620:1014,wrap,wrapping,1014,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1757#issuecomment-804509620,1,['wrap'],['wrapping']
Integrability,"This is a PR attempting to address https://github.com/theislab/scanpy/issues/173, and building on the work of @swolock in his fork (see https://github.com/theislab/scanpy/compare/master...swolock:master). . I've taken a different approach to that fork:. - Various component functions wrapped individually (core Scrublet functionality, doublet simulation), to allow for custom workflows with different Scanpy preprocessing where required.; - ... but a scrub_doublets() function that does pre-processing analogously with Scrublet (but using Scanpy calls) is what imagine most people will use. ; - I've moved what was sensible to use Scanpy functions. ; - ... but some parts are left alone. In particular the PCA seems like it's used a bit differently in Scrublet, fitting the model to the observed transcriptomes (.fit()) and then applying dimensionality reductions to both observed and simulated matrices (.transform()). That's complex to implement with Scanpy functions (which just uses fit_transform()), so thought I'd leave that in Scrublet (which calls the same libraries anyway). Similarly the neighbour graph generation done in the classifier is quite wired in, so I haven't tried to pull it into Scanpy. I hope that this is a low-maintenance solution, allowing use of Scrublet with Scanpy functions, without a large amount of re-implementation which might cause drift wrt Scrublet itself. . It does seem to work, but happy to make any changes you or @swolock want- would just be really useful for us to have this in Scanpy :-).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1476:284,wrap,wrapped,284,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1476,1,['wrap'],['wrapped']
Integrability,"This is a bit of a catch all to improve cite-seq support. Currently a dependency of https://github.com/theislab/scanpy-tutorials/pull/14. I'll write a bit more about this after some sleep. The main goals here are:. * Implement analysis functions for cite-seq data (like joint clustering, geometric mean normalization, etc.); * Improve generality of existing APIs. Basically, if the antibody counts are in obsm, we should be able to apply most functions to that. This is not currently the case.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1117:70,depend,dependency,70,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117,1,['depend'],['dependency']
Integrability,"This is a bit of a grab bag, but is mostly `io` related. This started out as me trying to learn some vscode git integration, but turns out it's not great at figuring out what lines changed. Apologies for any weird stuff in the commits. Main changes:. * I've made the tests for `sc.datasets` more thorough. Now they actually check the data looks kinda okay, rather than whether they throw an error.; * I've removed cache-ing in a few places; * The `read_10x_*` tests, where that definitely shouldn't have been happening; * In a couple of the `sc.datasets`. I'd be willing to go back on this, but we shouldn't let them use the cache during testing.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/592:112,integrat,integration,112,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/592,1,['integrat'],['integration']
Integrability,This is a pull request to integrate the Self-Assembling Manifold (SAM) algorithm into scanpy. A brief summary of the method:; SAM iteratively rescales the expressions of genes based on their spatial variability along the intrinsic manifold of the data. Extensive benchmarking has shown this approach to improve dimensionality reduction and feature selection for both 'easy' and 'challenging' datasets. SAM is especially powerful when analyzing datasets with only subtle differences in gene expression between cell types. More information can be found in the eLife publication: https://elifesciences.org/articles/48994. I still need to write the test script as well as ensure that the added code follows the BLACK coding style. Please let me know if there are any other issues I should fix prior to merging.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/903:26,integrat,integrate,26,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/903,1,['integrat'],['integrate']
Integrability,"This is an implementation for the support of anndatas with multiple tissues (slides). It depends on the anndata PR theislab/anndata#329; I made a short notebook to explain how it should work https://github.com/giovp/spatial-scripts/blob/master/multiple_slices_functionality.ipynb. There is a lot of room for improvement in terms of code, but in terms of functionality it should cover most of the use cases",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1073:89,depend,depends,89,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1073,1,['depend'],['depends']
Integrability,"This is basically the minimum amount of changes to separate things out and fix some problems with the test setup, plus unification of how we handle optional dependencies in tests. Fixes #2225",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2235:157,depend,dependencies,157,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2235,1,['depend'],['dependencies']
Integrability,"This is caused by an adjustment that tries to keep the legends closer to; the figure compared to the default placing. Clearly, there is some; dependency with the font size that I was not aware of. I will prepare a fix; soon. On Wed, Nov 21, 2018 at 3:23 PM Artemis-R <notifications@github.com> wrote:. > Here is a minimal example to recreate the issue I am describing:; >; > import scanpy.api as sc; > adata = sc.datasets.pbmc68k_reduced(); > sc.tl.pca(adata); > sc.pp.neighbors(adata); > sc.tl.umap(adata); >; > when you plot the umap of the bulk labels contained in adata.obs without; > specifying any further settings (i.e. sc.pl.umap(adata, color =; > ['bulk_labels']) ) everything looks fine.; >; > [image: image]; > <https://user-images.githubusercontent.com/15019107/48847064-efe34780-eda0-11e8-8d51-b503d7912d1e.png>; >; > But as soon as you try and adjust the legend font size (sc.pl.umap(adata,; > color = ['bulk_labels'], legend_fontsize = 4)) to a value that is smaller; > than the default font size it selects for your legend, the legend overlaps; > with the right edge of the plot.; >; > [image: image]; > <https://user-images.githubusercontent.com/15019107/48847096-04274480-eda1-11e8-9bac-dceb31aba155.png>; >; > For me this sometimes leads to issues that I can no longer export figures; > with my desired fontsize for presentations, etc. without it overlapping the; > plot in an ugly way.; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/374>, or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1VijUK8tuCXIowEtUo1I45iOhNd-ks5uxWHfgaJpZM4YtOB0>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/374#issuecomment-440727446:142,depend,dependency,142,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/374#issuecomment-440727446,1,['depend'],['dependency']
Integrability,"This is in an Ubuntu 16..04 Docker container:; ```; docker run --rm -it ubuntu:16.04; ```; Then I ran:; ```; apt-get update && apt-get install -y \; python3-pip \; python3-setuptools; python3-wheel. pip3 install scanpy; ```. I get the following output (after all of the dependencies are downloaded):. ```; warning: manifest_maker: standard file '-c' not found; ; reading manifest file 'scanpy.egg-info/SOURCES.txt'; reading manifest template 'MANIFEST.in'; writing manifest file 'scanpy.egg-info/SOURCES.txt'; Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""/tmp/pip-build-33o4crd7/scanpy/setup.py"", line 51, in <module>; 'Topic :: Scientific/Engineering :: Visualization',; File ""/usr/lib/python3.5/distutils/core.py"", line 148, in setup; dist.run_commands(); File ""/usr/lib/python3.5/distutils/dist.py"", line 955, in run_commands; self.run_command(cmd); File ""/usr/lib/python3.5/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/usr/lib/python3/dist-packages/wheel/bdist_wheel.py"", line 179, in run; self.run_command('build'); File ""/usr/lib/python3.5/distutils/cmd.py"", line 313, in run_command; self.distribution.run_command(command); File ""/usr/lib/python3.5/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/usr/lib/python3.5/distutils/command/build.py"", line 135, in run; self.run_command(cmd_name); File ""/usr/lib/python3.5/distutils/cmd.py"", line 313, in run_command; self.distribution.run_command(command); File ""/usr/lib/python3.5/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/tmp/pip-build-33o4crd7/scanpy/versioneer.py"", line 1559, in run; _build_py.run(self); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 52, in run; self.build_package_data(); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 107, in build_package_data; for package, src_dir, build_dir, filenames in self.data_files:; File ""/usr/lib/python3/dist-packages/setuptools/command/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/355:270,depend,dependencies,270,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/355,1,['depend'],['dependencies']
Integrability,"This is inspired by the Spring exporter. It also fills a directory with output files, but in addition to the matrix, adds cluster markers, meta data and embeddings. I keep the conversion code in the cellbrowser package, so I can change it easily in the future. We could also copy the whole function over, but I don't know if this makes a lot of sense, as people will need the cellbrowser package anyways and currently it doesn't have any dependencies, so should be easy to install. This function can optionally convert to html files, start a webserver and serve the result.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/382:438,depend,dependencies,438,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/382,1,['depend'],['dependencies']
Integrability,"This is likely because you have [`scvi-tools`](https://scvi-tools.org/) and this wrapper supports our now deprecated `scvi` package. This wrapper will be removed in the next release, so I recommend using scvi-tools directly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1781#issuecomment-814561000:81,wrap,wrapper,81,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1781#issuecomment-814561000,2,['wrap'],['wrapper']
Integrability,"This is really cool! How expansive should this be? Scanpy core + scvelo? Or also other scanpy-based things like single-cell-tutorial, scGen, scvi-tools or diffxpy?. In our data integration benchmarking we find that 3 of the top 4 tools are in the scanpy ecosystem now: scanorama, scGen, and scANVI.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1571#issuecomment-754652730:177,integrat,integration,177,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1571#issuecomment-754652730,1,['integrat'],['integration']
Integrability,"This is something I'd very much be interested in. A few questions. * I'd really like to have scanpy and anndata work better with dask, but am wary of a high code overhead. Could you provide examples of where you were running into issues with arrays being materialized? I think this can be worked around in AnnData side in many cases.; * Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. And a few questions about sparse matrices on the GPU:. * How difficult do you think these methods would be to implement? It looks like there is functionality for taking the intersection of sparsity patterns in [`cusparseConstrainedGeMM`](https://docs.nvidia.com/cuda/cusparse/index.html#cusparse-generic-function-cgemm) which could help.; * Have you looked into other backends for sparse matrices on the GPU? `suitesparse`/ `GraphBLAS` or `taco` may cover these use cases, though would need wrapping. > So I wrote a wrapper around scipy.sparse to implement NumPy's __array_function__ protocol. This allows sparse arrays to be chunks in a Dask array. 👍. ~~Any chance you've taken a look at implementing gufuncs?~~ Oops, missed the `__array_ufunc__` definition.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/921#issuecomment-554871161:948,wrap,wrapping,948,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-554871161,3,"['protocol', 'wrap']","['protocol', 'wrapper', 'wrapping']"
Integrability,"This is the only way the README renders on PyPI. depends on github/markup#1222, which in turn depends on jch/html-pipeline#302",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/234:49,depend,depends,49,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/234,2,['depend'],['depends']
Integrability,"This is very strange... Do you have this issue also in the [standard tutorial](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb)? Of course, the louvain function produces string-named categories, see [here](https://github.com/theislab/scanpy/blob/5299c6caaec6402513f1e0442186350787177d2c/scanpy/tools/louvain.py#L135) and has always done so. I'm puzzled that the `.astype('U')`, where the `'U'` stands for unicode-string, seems to have no effect in your version of Scanpy. Do you use the most recent version (0.4.4) and recent dependencies? If not, run `pip install --upgrade scanpy`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/94#issuecomment-370140266:575,depend,dependencies,575,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/94#issuecomment-370140266,1,['depend'],['dependencies']
Integrability,"This looks good! :smile:. Storing the forest in the AnnData is good! It should also be compatible with the updates the @tomwhite plans on UMAP and pynndescent (UMAP will depend on pynndescent) as that should be the most basic object to store when to enable queries later on... But I would not store the ""forest"" in a default neighbors call. Or do you have any estimate on how large it is?. Great work!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/576#issuecomment-487035737:170,depend,depend,170,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576#issuecomment-487035737,1,['depend'],['depend']
Integrability,This pr moves the (in)famous `_prepare_dataframe` function from scanpy.plotting._anndata to sc.get as `_indexed_expression_df` (bcs why would it be in plotting anyway) and implements a simple public interface called `sc.get.summarized_expresion_df` which simply provides nonzero mean/var and fraction using `_indexed_expression_df` function. . As discussed here (https://github.com/theislab/scanpy/pull/1388#issuecomment-678739734) we can use this in rank_genes_groups_df.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1390:199,interface,interface,199,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1390,1,['interface'],['interface']
Integrability,"This replaces the random choice with iterating over all combinations. That way, if you want to debug a certain combination, you can just do so instead of rerunning the test and hoping it gets picked. Sadly AFAIK it’s not possible to have a fixture that depends on other fixture values and generates a variable amount of values depending on their arguments: either you have `fixture(params=some_list)` which creates `len(some_list)` values or not, then it creates one. Therefore I had to get rid of the fixtures and use a static list instead. It’s not that much slower to run them all:. before: 24 passed, 6 xfailed in 1.81s ; after: 56 passed, 14 xfailed in 3.41s",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3294:253,depend,depends,253,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3294,2,['depend'],"['depending', 'depends']"
Integrability,"This seems reasonable to me @flying-sheep . Does using the patched version change results over the unpatched @ashish615 i.e., for a given random seed, unpatched and patched are the same? If the two are the same for a given seed/state, then I think what @flying-sheep is proposing could be done separately (even if we make the dependency optional IMO). However, if the new version does change results, we will need the handling that @flying-sheep describes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3061#issuecomment-2114818805:326,depend,dependency,326,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3061#issuecomment-2114818805,1,['depend'],['dependency']
Integrability,"This still works in `1.4.4.post1`. It's very likely caused by changes to `setup.py`. I experienced similar problems before and fixed them via `package_data`. But this got removed. It's probably only a problem for the source-based installs. https://github.com/theislab/scanpy/commit/881f0bef31cdfe0df7333641dc847a60894b5c41#diff-2eeaed663bd0d25b7e608891384b7298. ```; >>> import scanpy; >>> scanpy.__version__; <Version('1.4.5.post2')>; >>> scanpy.datasets.pbmc68k_reduced(); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/Users/alexwolf/miniconda3/lib/python3.6/site-packages/scanpy/datasets/__init__.py"", line 239, in pbmc68k_reduced; return read(filename); File ""/Users/alexwolf/miniconda3/lib/python3.6/site-packages/scanpy/readwrite.py"", line 114, in read; **kwargs,; File ""/Users/alexwolf/miniconda3/lib/python3.6/site-packages/scanpy/readwrite.py"", line 524, in _read; return read_h5ad(filename, backed=backed); File ""/Users/alexwolf/miniconda3/lib/python3.6/site-packages/anndata/readwrite/read.py"", line 447, in read_h5ad; constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size); File ""/Users/alexwolf/miniconda3/lib/python3.6/site-packages/anndata/readwrite/read.py"", line 481, in _read_args_from_h5ad; f = h5py.File(filename, 'r'); File ""/Users/alexwolf/miniconda3/lib/python3.6/site-packages/anndata/h5py/h5sparse.py"", line 162, in __init__; **kwds,; File ""/Users/alexwolf/miniconda3/lib/python3.6/site-packages/h5py/_hl/files.py"", line 312, in __init__; fid = make_fid(name, mode, userblock_size, fapl, swmr=swmr); File ""/Users/alexwolf/miniconda3/lib/python3.6/site-packages/h5py/_hl/files.py"", line 142, in make_fid; fid = h5f.open(name, flags, fapl=fapl); File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper; File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper; File ""h5py/h5f.pyx"", line 78, in h5py.h5f.open; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/995:1792,wrap,wrapper,1792,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/995,2,['wrap'],['wrapper']
Integrability,"This uses the `__array__` method on ndarray-like classes to convert from; a distributed array to a regular NumPy ndarray when `materialize_as_ndarray` is called. This was prompted by an [improvement in Zappy](https://github.com/lasersonlab/zappy/pull/7) (released in 0.2.0) that makes Zappy arrays implement the `__array__` method. As another benefit, this change should make it easier to use other implementations of the ndarray interface in Scanpy in the future. Note that there is still a Dask-specific call for the case of a sequence of arrays, since Dask can materialize these in one call to `compute`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/439:430,interface,interface,430,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/439,1,['interface'],['interface']
Integrability,This. - moves all the external plotting routines to `scanpy/external/pl.py`; - adds one for harmony; - Fixes a plotting bug this triggered; - Makes harmony the first tool using obsp/varp. @awnimo can you please test this? Does the plot look like you want it to?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1004:40,rout,routines,40,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1004,1,['rout'],['routines']
Integrability,"Those packages are optional dependencies, and also aren't installed with `pip install scanpy`. You'll need to specify those separately if you'd like to use the features that require them.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2000#issuecomment-953226267:28,depend,dependencies,28,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2000#issuecomment-953226267,1,['depend'],['dependencies']
Integrability,"To help debug it, can one of you do what I suggested? I don’t have the problem locally and if one of you does see it, it would be much easier to find out which dependency started to import that module.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/862#issuecomment-562545044:160,depend,dependency,160,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862#issuecomment-562545044,1,['depend'],['dependency']
Integrability,"True,; ---> 20 use_raw=False); 21 data.to_csv(""C:/Users/Lin/write/paga_path_{}.csv"".format(descr)); 22 pl.savefig(""C:/Users/Lin/figures/paga_path_KTC.pdf""). ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax); 1038 old_len_x = len(x); 1039 print(x); -> 1040 x = moving_average(x); 1041 if ikey == 0:; 1042 for key in annotations:. ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in moving_average(a); 980 ; 981 def moving_average(a):; --> 982 return _sc_utils.moving_average(a, n_avg); 983 ; 984 ax = pl.gca() if ax is None else ax. ~\Miniconda3\envs\project\lib\site-packages\scanpy\_utils.py in moving_average(a, n); 374 An array view storing the moving average.; 375 """"""; --> 376 ret = np.cumsum(a, dtype=float); 377 ret[n:] = ret[n:] - ret[:-n]; 378 return ret[n - 1:] / n. <__array_function__ internals> in cumsum(*args, **kwargs). ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in cumsum(a, axis, dtype, out); 2421 ; 2422 """"""; -> 2423 return _wrapfunc(a, 'cumsum', axis=axis, dtype=dtype, out=out); 2424 ; 2425 . ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in _wrapfunc(obj, method, *args, **kwds); 56 bound = getattr(obj, method, None); 57 if bound is None:; ---> 58 return _wrapit(obj, method, *args, **kwds); 59 ; 60 try:. ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in _wrapit(obj, method, *args, **kwds); 45 except AttributeError:; 46 wrap = None; ---> 47 result = getattr(asarray(obj), method)(*args, **kwds); 48 if wrap:; 49 if not isinstance(result, mu.ndarray):. ValueError: setting an array element with a sequence. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1168#issuecomment-619031543:80925,wrap,wrap,80925,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168#issuecomment-619031543,2,['wrap'],['wrap']
Integrability,"Trying out the tutorials these days and it seems this issue still persists. ---; Here is what I got from running the tutorial `pbmc3k.ipynb`:; Before writing the `AnnData` object to a `.h5ad` file (after the PCA step; before computing the neighborhood graph); - Inside `adata.uns`:; ```; OverloadedDict, wrapping:; 	OrderedDict([('log1p', {'base': None}), ('hvg', {'flavor': 'seurat'}), ('pca', {'params': {'zero_center': True, 'use_highly_variable': True}, 'variance': array([ (not showing the numbers for simplicity here) ],; dtype=float32), 'variance_ratio': array([ (not showing the numbers for simplicity here) ],; dtype=float32)})]); With overloaded keys:; 	['neighbors'].; ```. ---; After loading the matrix from the `.h5ad` file:; - Inside `adata.uns`, the `log1p` key became an empty dictionary:; ```; OverloadedDict, wrapping:; 	{'hvg': {'flavor': 'seurat'}, 'log1p': {}, 'pca': {'params': {'use_highly_variable': True, 'zero_center': True}, 'variance': array([ (not showing the numbers for simplicity here) ],; dtype=float32), 'variance_ratio': array([ (not showing the numbers for simplicity here) ],; dtype=float32)}}; With overloaded keys:; 	['neighbors'].; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2239#issuecomment-1319791016:304,wrap,wrapping,304,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2239#issuecomment-1319791016,4,['wrap'],['wrapping']
Integrability,Two options:; - bbknn < 1.5.0; - use `bbknn.bbknn()` instead of the scanpy wrapper,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1873#issuecomment-872833609:75,wrap,wrapper,75,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1873#issuecomment-872833609,1,['wrap'],['wrapper']
Integrability,"UMAP won't do any correction of batch effects for you, like CCA (it looks at the basis that leads to the greatest overlap between the batches, assuming that this captures the common biological variation and projects out everything else, assuming it's nuisance/technical batch effects). Similar for all other ""alignment tools"": you throw away some information in order to align. When you map a new dataset into an existing dataset using UMAP, this will do an _exact_ mapping. If you have pronounced batch effects, the second dataset will cluster as a whole far away from the first. So, I don't think that there will be much to gain. Why not give BBKNN (https://github.com/Teichlab/bbknn) a try? It integrates nicely with Scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/265#issuecomment-424704691:697,integrat,integrates,697,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-424704691,1,['integrat'],['integrates']
Integrability,"Ultimately I don't think these things should be blockers. Lazy dataframes are now en-route with the xarray PR accepted in principle, and the subsetting operation is what it is. I think it's ok to iterate on these things",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2856#issuecomment-1983064398:85,rout,route,85,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2856#issuecomment-1983064398,1,['rout'],['route']
Integrability,"Unfortunately, I run into. ```; __________________________________________________________________________________ test_scale[use_fastpp] ___________________________________________________________________________________. flavor = 'use_fastpp'. @pytest.mark.parametrize(""flavor"", [""default"", ""use_fastpp""]); def test_scale(flavor):; adata = pbmc68k_reduced(); adata.X = adata.raw.X; v = adata[:, 0 : adata.shape[1] // 2]; # Should turn view to copy https://github.com/scverse/anndata/issues/171#issuecomment-508689965; assert v.is_view; with pytest.warns(Warning, match=""view""):; > sc.pp.scale(v, flavor=flavor). scanpy/tests/test_preprocessing.py:127: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _; ../../miniconda3/envs/scanpy/lib/python3.9/functools.py:888: in wrapper; return dispatch(args[0].__class__)(*args, **kw); scanpy/preprocessing/_simple.py:888: in scale_anndata; X, adata.var[""mean""], adata.var[""std""] = do_scale(; ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args; error_rewrite(e, 'typing'); _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'); issue_type = 'typing'. def error_rewrite(e, issue_type):; """"""; Rewrite and raise Exception `e` with help supplied based on the; specified issue_type.; """"""; if config.SHOW_HELP:; help_msg = errors.error_extras[issue_type]; e.patch_message('\n'.join((str(e).rstrip(), help_msg))); if config.FULL_TRACEBACKS:; raise e; else:; > raise e.with_traceback(Non",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2457#issuecomment-1533308183:911,wrap,wrapper,911,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457#issuecomment-1533308183,1,['wrap'],['wrapper']
Integrability,Unlock h5py dependency,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1006:12,depend,dependency,12,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1006,1,['depend'],['dependency']
Integrability,Update doc dependencies,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2775:11,depend,dependencies,11,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2775,1,['depend'],['dependencies']
Integrability,Update setup.py to update dependent packages,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/518:26,depend,dependent,26,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/518,1,['depend'],['dependent']
Integrability,"Update to `downsample_counts` to allow downsampling total counts, similar to normalization by `cellranger aggr` (I'm pretty sure on this, there's a lot going on in their code). Additionally, enabled caching for the `numba`'d function, which cuts down on test time. As adding this feature meant renaming `target_counts` to `counts_per_cell`, this becomes a breaking change. Since it's breaking, I've also gone ahead and set `replace=False` by default as mentioned before (#340). Definitely willing to make changes. I've implemented this since I'm doing some integration work and figured it'd be nice to be able to try the basic `cellranger` strategy. Edit: The failing PAGA test occurs locally on master as well, but I don't think I broke that.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/474:557,integrat,integration,557,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474,1,['integrat'],['integration']
Integrability,Use pynndescent dependency to support threaded nearest neighbors,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/659:16,depend,dependency,16,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659,1,['depend'],['dependency']
Integrability,"Using the progress bar from tqdm.auto causes a `ImportError` when `ipywidgets` is not installed. ; The progressbar from the top level `tqdm` module does not have this dependency. . Repex: . ```python; import scanpy as sc; sc.datasets.moignard15(); ```. Output: ; ```; ---------------------------------------------------------------------------; NameError Traceback (most recent call last); ~/anaconda3/envs/test_scanpy/lib/python3.8/site-packages/tqdm/notebook.py in status_printer(_, total, desc, ncols); 97 else: # No total? Show info style bar with no progress tqdm status; ---> 98 pbar = IProgress(min=0, max=1); 99 pbar.value = 1. NameError: name 'IProgress' is not defined. During handling of the above exception, another exception occurred:. ImportError Traceback (most recent call last); <ipython-input-5-ec5b1e8cd660> in <module>; ----> 1 sc.datasets.moignard15(). ~/anaconda3/envs/test_scanpy/lib/python3.8/site-packages/scanpy/datasets/__init__.py in moignard15(); 108 filename = settings.datasetdir / 'moignard15/nbt.3154-S3.xlsx'; 109 backup_url = 'http://www.nature.com/nbt/journal/v33/n3/extref/nbt.3154-S3.xlsx'; --> 110 adata = sc.read(filename, sheet='dCt_values.txt', backup_url=backup_url); 111 # filter out 4 genes as in Haghverdi et al. (2016); 112 gene_subset = ~np.in1d(adata.var_names, ['Eif2b1', 'Mrpl19', 'Polr2a', 'Ubc']). ~/anaconda3/envs/test_scanpy/lib/python3.8/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 92 filename = Path(filename) # allow passing strings; 93 if is_valid_filename(filename):; ---> 94 return _read(; 95 filename, backed=backed, sheet=sheet, ext=ext,; 96 delimiter=delimiter, first_column_names=first_column_names,. ~/anaconda3/envs/test_scanpy/lib/python3.8/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 489 else:; 490 ext = is_valid_filename(filena",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1130:167,depend,dependency,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1130,1,['depend'],['dependency']
Integrability,"Very good catch! It does indeed look like in the function itself it should be. ```; df.sort_values( ; ['highly_variable_nbatches', 'highly_variable_rank'], ; ascending=[False, True], ; na_position='last', ; inplace=True, ; ) ; ```. However, as the test sorting order was correct (though not testing the code the right way), it would still be great to figure out why there is a discrepancy at all. . For reference, here's the seurat code:. https://github.com/satijalab/seurat/blob/4e868fcde49dc0a3df47f94f5fb54a421bfdf7bc/R/integration.R#L2244-L2308",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1733#issuecomment-802145791:523,integrat,integration,523,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1733#issuecomment-802145791,1,['integrat'],['integration']
Integrability,"We are currently using `sklearn.utils.check_random_state` to validate the argument for `random_state` in most places. Sometime's (especially in external) we pass the argument directly to the wrapped tool. In sklearn `0.24.1`, this looks like `np.random.RandomState` if you pass an integer.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1626#issuecomment-773090710:191,wrap,wrapped,191,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1626#issuecomment-773090710,1,['wrap'],['wrapped']
Integrability,We have SCVI working on anndata objects for data integration in our benchmarking study.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/520#issuecomment-553385617:49,integrat,integration,49,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/520#issuecomment-553385617,1,['integrat'],['integration']
Integrability,"We have a backwards compatibility wrapper, I have no idea how this error can be possible:. https://github.com/theislab/anndata/blob/41eadb2a76d91ae455faf01afd2382143b9af4b2/anndata/_core/anndata.py#L2137-L2140",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1027#issuecomment-587178083:34,wrap,wrapper,34,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1027#issuecomment-587178083,1,['wrap'],['wrapper']
Integrability,"We have our [CLI layer for Scanpy](https://github.com/ebi-gene-expression-group/scanpy-scripts), and I could put this integration there, but it'd be a shame to silo code that might be useful to other Scanpy users, so happy to contribute to something in the external API if you guys are willing.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1955#issuecomment-885007122:118,integrat,integration,118,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1955#issuecomment-885007122,1,['integrat'],['integration']
Integrability,"We have some failing tests due to a couple bugs introduced in pandas 1.3.0:. * https://github.com/pandas-dev/pandas/issues/42380. I think this one is small in scope. Has problems when `df.agg` is called, when all columns are categorical and index is non-unique. Definitely a bug in pandas, and I don't think we do this much. Switching to `df.apply` works around the problem. * https://github.com/pandas-dev/pandas/issues/42376. Assignment of single columns `np.matrix` to dataframe columns no longer works as if the matrix were a 1d array. I think this is a bug since it's an undocumented behaviour change. Fixes are pretty easy, since we can just wrap occurrences with `np.ravel`, however I wouldn't be surprised if there were many places in the codebase that this happened. I would be good if these didn't happen.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1917:648,wrap,wrap,648,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1917,1,['wrap'],['wrap']
Integrability,We never wanted APIs that can be used with more that ~2 positional parameters. We should go to keyword-only-parameters. This can be done via [legacy-api-wrap](https://github.com/flying-sheep/legacy-api-wrap),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/464:153,wrap,wrap,153,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/464,2,['wrap'],['wrap']
Integrability,We should replace the whole logging module with. ```py; import logging. root_logger = logging.getLogger('scanpy'); root_logger.setLevel('INFO'); root_logger.propagate = False # Don’t pass log messages on to the root logger and its handler. handler = logging.StreamHandler(sys.stderr) # Why did we use stdout?; handler.setFormatter(logging.Formatter('%(message)s')); handler.setLevel('INFO'); root_logger.addHandler(handler). def get_logger(name):; return root_logger.getChild(name); ```. and in all submodules just do. ```py; from .logging import get_logger. logger = get_logger(__name__); ```,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/256:192,message,messages,192,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/256,2,['message'],"['message', 'messages']"
Integrability,"We want notebooks that automatically run through on readthedocs: https://nbsphinx.readthedocs.io/en/0.3.5. So that the basic tutorial (https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb) can be integrated into the scanpy repository in `docs/tutorials` as a notebook with all output cleared (no images etc.). It is run on the readthedocs server and will produce exactly the same output, as is guaranteed by this test: https://github.com/theislab/scanpy/blob/master/scanpy/tests/notebooks/pbmc3k.py",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/302:240,integrat,integrated,240,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/302,1,['integrat'],['integrated']
Integrability,"We were getting a lot of errors from dask tests because they were relying on test helpers from anndata 0.10. It's a small number of functions, but it depends on the types in the compat module so is difficult to copy out. To work around this I've temporarily bumped the minimum required version of anndata up to 0.10, but we definitely shouldn't actually do that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895765916:150,depend,depends,150,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895765916,1,['depend'],['depends']
Integrability,"We're having trouble installing louvain on CI due to a recent setuptools release (would have been nice if setuptools had more vocal warnings about this ahead of time, but alas). See: vtraag/louvain-igraph/issues/57. This PR makes louvain optional. This was done by:. ### Skip louvain dependent tests. While these largely were tests checking that louvain works, some of these are testing other things. The biggest example here is `test_paga_paul15_subsampled.py`, which is really a test of PAGA. This should be corrected. ### Remove louvain dependency from tests. Some tests, like those for `rank_genes_groups_logreg` used louvain, but really didn't have to. `test_pbmc3k` could just have `louvain` calls replaced with `leiden` with only one plot triggering an error. ### `louvain` is no longer installed on CI. This should get around the build issue.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2063:284,depend,dependent,284,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2063,2,['depend'],"['dependency', 'dependent']"
Integrability,"We've been dealing with long queue times for CI builds. This is at least partially because for each PR four jobs start, each of which takes at least 12 minutes. Since travis gives us at most five concurrent jobs, only one PR can be built at a time. This becomes worse if a PR is based on a branch on the main repo, since CI runs on those too. Azure offers 10 free concurrent jobs. Seems like an easy win. * 10 free concurrent jobs; * Easier to do multiple checks per build (i.e. linting and testing can happen in the same build, but be independent checks); * Output looks easy to navigate, has good integration with github; * We could test on windows (depending on how hard this is to set up); * (possible) Some projects seem to use multiple cores for testing. Cons:. * New system, will take some time to learn; * Maybe microsoft will start being evil again",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1358:599,integrat,integration,599,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1358,2,"['depend', 'integrat']","['depending', 'integration']"
Integrability,"Well, “implemented”… I’d say “wrapped”.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/350#issuecomment-441213626:30,wrap,wrapped,30,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350#issuecomment-441213626,1,['wrap'],['wrapped']
Integrability,"We’re thinking about making the backend configurable through something like https://github.com/frankier/sklearn-ann (that specific one doesn’t seem maintained though). A recipe for this is found here: https://scikit-learn.org/stable/auto_examples/neighbors/approximate_nearest_neighbors.html#sphx-glr-auto-examples-neighbors-approximate-nearest-neighbors-py. Faiss does seem nice as an option, but a hard dependency on something that isn’t on PyPI is out of the question.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2519#issuecomment-1603957399:405,depend,dependency,405,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2519#issuecomment-1603957399,2,['depend'],['dependency']
Integrability,"What dependency problems do you have? If you installed everything through conda, you should just be able to update it with conda…",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/871#issuecomment-545908616:5,depend,dependency,5,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/871#issuecomment-545908616,1,['depend'],['dependency']
Integrability,"What do you mean? In which way is it incompatible?. In matplotlib/matplotlib#9698 it’s said that. > The goal is to ultimately replace setting `savefig.transparent` by; `figure.facecolor = (0, 0, 0, 0)`. So we should add a `facecolor` parameter and deprecate `transparent` (with a nice message to point people to `set_figure_params(facecolor=(0, 0, 0, 0))`)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/473#issuecomment-462694429:285,message,message,285,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/473#issuecomment-462694429,1,['message'],['message']
Integrability,"What do you mean? Scanpy’s metadata only specifies a lower Python version bound:. https://github.com/scverse/scanpy/blob/d7e13025b931ad4afd03b4344ef5ff4a46f78b2b/pyproject.toml#L13. Which of the following is it?. - Does it refuse to install because some dependency is not Python 3.11 compatible?; - Does it crash when run there?; - Something else?. I assume it’s just that numba is incompatible still (which will always be a recurring problem as long as we depend on it), but please let me know. With installation issues, you can always run `pip -vv install scanpy` to get much more output that could be helpful. Please [use code blocks](https://docs.github.com/en/get-started/writing-on-github/working-with-advanced-formatting/creating-and-highlighting-code-blocks) when including it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369#issuecomment-1331976462:254,depend,dependency,254,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369#issuecomment-1331976462,2,['depend'],"['depend', 'dependency']"
Integrability,What if we had copy-on-write behavior for AnnData? Then we could never modify AnnData inplace but always return a view of an AnnData with references to the objects that were unchanged and only the new data added. . Pandas seems to be going that route: https://github.com/pandas-dev/pandas/blob/57390ada100466dac777e5b66d5a4f2a72700c38/web/pandas/pdeps/0008-inplace-methods-in-pandas.md (HT @bernheder),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2583#issuecomment-1674322622:245,rout,route,245,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2583#issuecomment-1674322622,1,['rout'],['route']
Integrability,What is the content of the variable `folder`? There must be an error message or else you are not executing the code.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1795#issuecomment-817683273:69,message,message,69,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1795#issuecomment-817683273,1,['message'],['message']
Integrability,What we could do instead is to add a [extras_require section](http://setuptools.readthedocs.io/en/latest/setuptools.html#declaring-extras-optional-features-with-their-own-dependencies) to the `setup()` call in setup.py. then people could do `pip install scanpy[louvain]` or `pip install scanpy[all]` to get the whole thing.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/176#issuecomment-398656375:171,depend,dependencies,171,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/176#issuecomment-398656375,1,['depend'],['dependencies']
Integrability,"What you describe doesn‘t need to happen, and you can fix this!. 1. go to https://github.com/conda-forge/conda-forge-repodata-patches-feedstock/; 2. make a PR that patches conda’s dependency data to include this constraint; 3. the problem is gone",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3029#issuecomment-2363200158:180,depend,dependency,180,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3029#issuecomment-2363200158,1,['depend'],['dependency']
Integrability,"When I did `pip install --user scikit-misc` in my shell and then in python tried the line that errored for you `from skmisc.loess import loess`, everything worked fine for me. Also, depending on how conda is setup `pip install --user` might install it in your home directory, rather than the conda env. So you could also try activating the conda env and then running `pip install scikit-misc --force`. . Can you print out the full traceback of what happens when you run `from skmisc.loess import loess`? If that was causing the `ImportError` it might be easier to see outside of the try/except block. You can also try `import skmisc; print(skmisc.__file__)` to see what that returns. I also see some related issues (https://github.com/has2k1/scikit-misc/issues/12), which could indicate that it did not install correctly because it did not install the cython scripts properly on windows. The solution (install the numpy+mkl .whl first) in https://github.com/has2k1/scikit-misc/issues/4 might work?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2073#issuecomment-996270340:182,depend,depending,182,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-996270340,1,['depend'],['depending']
Integrability,"When I tried to import scanpy into python 3.5.2, I got the following error message,. ```; >>> import scanpy as sc; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/usr/local/lib/python3.5/dist-packages/scanpy/__init__.py"", line 3, in <module>; from .utils import check_versions, annotate_doc_types; File ""/usr/local/lib/python3.5/dist-packages/scanpy/utils.py"", line 18, in <module>; from ._settings import settings; File ""/usr/local/lib/python3.5/dist-packages/scanpy/_settings.py"", line 351; f'{k} = {v!r}'; ^; SyntaxError: invalid syntax; ```; My OS platform is `Ubuntu 16.04` and I installed `scanpy` by `pip install scanpy`. How could I resolve this issue? Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/855:75,message,message,75,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/855,1,['message'],['message']
Integrability,"When the C core `igraph` version 0.10 is released, including a new release of the Python interface building on this new version, i.e. including 64-bit support, I will also update the `leidenalg` implementation to follow suit. In principle, `leidenalg` is already working with 64-bit integers, but since it builds on `igraph`, it is limited by that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1053#issuecomment-1103987652:89,interface,interface,89,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053#issuecomment-1103987652,1,['interface'],['interface']
Integrability,Which package has the `scipy<1.3.0` dependency?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1273#issuecomment-661095384:36,depend,dependency,36,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1273#issuecomment-661095384,1,['depend'],['dependency']
Integrability,"Will do once there are things that are big enough... you set the bar quite high with these headlines ;). Maybe things like single-cell-tutorial as F1000 recommended paper, or the news about top performing data integration methods, once the paper is out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1571#issuecomment-754704191:210,integrat,integration,210,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1571#issuecomment-754704191,1,['integrat'],['integration']
Integrability,Windows compatibility issues with dependencies,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/454:34,depend,dependencies,34,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454,1,['depend'],['dependencies']
Integrability,Wishbone integration,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1063:9,integrat,integration,9,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1063,1,['integrat'],['integration']
Integrability,"With the scanpy ecosystem growing also with packages outside of scanpy external, it would probably be good to have a page with all packages that are part of this ecosystem. Ideally this would be on the Scanpy page I guess as it would be a first port-of-call for users. . Such a page could have brief explanations of the capabilities of each of these tools, and link to tutorials from all the independent packages that show how these tools are integrated into a standard Scanpy workflow. What do you guys think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1443#issuecomment-703481988:443,integrat,integrated,443,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1443#issuecomment-703481988,1,['integrat'],['integrated']
Integrability,"Within that loop, I believe large amounts of memory can be allocated. If it's ""group vs rest"", at least one `X` worth of memory is allocated per loop from matrix subsetting – since there's an `X_group = X[mask]; X_rest = X[~mask]`. If you parallelize over groups, now the max memory usage can be `~ min(n_procs, n_groups) * X` as opposed to `~2X`. For large `X` (probably where you want to see the speed up most), this can make you run out of memory. https://github.com/scverse/scanpy/blob/d26be443373549f26226de367f0213f153556915/scanpy/tools/_rank_genes_groups.py#L164-L178. Another memory related concern comes from `multiprocessing` (mentioned in your email). I think there's recently been some improvement here, but my impression was it's difficult/ impossible to share memory with `multiprocessing` – so everything that goes into or out of a subprocess has to be copied. So while I think we can absolutely make use of more processing power here, I think we need to consider the approach. * The link I mentioned above should reduce copies, and could potentially use a parallelized BLAS for compute; * Much of the loops over ""all of the genes between compared samples"" are already in compiled code, but could be parallelized. > In terms of our use case, an interactive way to run DE via the client is too slow. What is the interface here? Scanpy computes results for all groups at once, but in most interfaces I've used you can only really ""ask"" for one comparison at a time. This could also be much faster, if you can just reduce total computation. ---------. > What @ivirshup was referring to though, is that rank_genes_groups on single cells in general isn't seen anymore as best practice for DE analysis because it doesn't account for pseudoreplication bias. Partially, I'm not sure what comparisons are actually being run. I was also wondering if you'd benefit from something fancy like a covariate. > Diffxpy is currently being reimplemented. . As a heads up, I'm unaware of a timeline here",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2390#issuecomment-1397484486:1720,interface,interface,1720,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390#issuecomment-1397484486,2,['interface'],"['interface', 'interfaces']"
Integrability,"Would you say that there is an optimal range to set n_neighbors usually? And maybe a max value that rarely should be exceeded?. I'm trying to optimize louvain clustering for several datasets, and I'm aiming to automate at least a portion of the process, by going through a range of neighbor values (tl.neighbors) and resolution values (for tl.louvain), while keeping n_pcs constant, and most of my highest scoring clustering arrangements (measured by the silhouette index) uses neighbor parameters ~ 22 - 30. I know that these parameters will depend on the dataset, but I'm wondering if I should set a lower upper limit (For now it's 30), then go in and try to optimize the clustering of specific clusters using the restrict_to parameter for the louvain function. The clustering arrangements I have don't seem to be adequate based on certain markers that I'm plotting across the cells. . Hope this makes sense. Best",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/223:543,depend,depend,543,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223,1,['depend'],['depend']
Integrability,Wrap legacy APIs,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2702:0,Wrap,Wrap,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2702,1,['Wrap'],['Wrap']
Integrability,Wrong dependencies on bioconda for 1.4.4,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/876:6,depend,dependencies,6,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876,1,['depend'],['dependencies']
Integrability,"Yeah, I can't reproduce it with a canned dataset either --- I'm doing something a bit weird and transforming imaging mass cytometry data into AnnData objects (hence the `imctools` dependency). I have an object that looks like:; ```{python}; AnnData object with n_obs × n_vars = 68865 × 29; obs: 'nuclei_counts', 'n_antibodies_by_intensity', 'log1p_n_antibodies_by_intensity', 'total_intensity', 'log1p_total_intensity', 'n_counts'; var: 'ab_mass', 'ab_name', 'n_cells_by_intensity', 'mean_intensity', 'log1p_mean_intensity', 'pct_dropout_by_intensity', 'total_intensity', 'log1p_total_intensity', 'highly_variable'; uns: 'spatial', 'log1p', 'pca',; obsm: 'X_spatial', 'X_spatial_lowres', 'X_pca'; varm: 'PCs'; layers: 'cleaned', 'normed', 'lognormed'; ```. I will probably raise this with `pynndescent` then because; ```; sc.pp.neighbors(imc42, n_pcs=10, metric=""euclidean"", n_neighbors=15) # <-- works; sc.pp.neighbors(imc42, n_pcs=10, metric=""correlation"", n_neighbors=15) # <-- works; sc.pp.neighbors(imc42, n_pcs=10, metric=""euclidean"", n_neighbors=11) # <-- crashes; sc.pp.neighbors(imc42, n_pcs=10, metric=""correlation"", n_neighbors=11) # <-- crashes; sc.pp.neighbors(imc42, n_pcs=10, metric=""euclidean"", n_neighbors=5) # <-- crashes; sc.pp.neighbors(imc42, n_pcs=10, metric=""correlation"", n_neighbors=5) # <-- crashes; ```. Sorry for hijacking this issue @giovp and @TiongSun .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1696#issuecomment-797647223:180,depend,dependency,180,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696#issuecomment-797647223,1,['depend'],['dependency']
Integrability,"Yeah, it's not working . Here https://scanpy.readthedocs.io/en/stable/generated/scanpy.pl.violin.html they say; > Wraps [seaborn.violinplot()](https://seaborn.pydata.org/generated/seaborn.violinplot.html#seaborn.violinplot) for [AnnData](https://anndata.readthedocs.io/en/stable/generated/anndata.AnnData.html#anndata.AnnData). but when you add `orient='h'` or `orient='v'` to the `sc.pl.violin` run, it fails wit this error:; ```; TypeError: seaborn.categorical.violinplot() got multiple values for keyword argument 'orient'; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2092#issuecomment-1513930164:114,Wrap,Wraps,114,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2092#issuecomment-1513930164,1,['Wrap'],['Wraps']
Integrability,"Yeah, someone creates a package and whenever a new release appears on PyPI, the bot makes a PR that increases the version number in the build recipe. A human then checks if everything works and merges. In this case that human didn’t check the dependencies changing (very understandable, it’s draining to search where they’re defined and compare manually multiple times per day). You could simply do a quick PR that updates dependencies and build number and I’m sure they’ll quickly merge it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/876#issuecomment-545971170:243,depend,dependencies,243,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876#issuecomment-545971170,4,['depend'],['dependencies']
Integrability,"Yeah, that’s true. I think depending on what’s happening, it’s actually the spacer we add, not the area for the dendrogram, but sometimes we also need that spacer …. Our plotting code is complicated and needs to be overhauled. If anyone feels like diving int",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3320#issuecomment-2437408562:27,depend,depending,27,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3320#issuecomment-2437408562,1,['depend'],['depending']
Integrability,"Yeah, the UMAP plots work alright, and I can recognize many of the genes I get in the message are indeed genes that are expressed in the data, for I can visualize them on my UMAP plots. It is kind of weird, I am wondering if somewhere I messed up the format of those names, but then why would they work on UMAP?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/438#issuecomment-456760052:86,message,message,86,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438#issuecomment-456760052,1,['message'],['message']
Integrability,"Yep, but still you need data passed to not only have the same dimensionality, you need dimensions to have the same meaning any time you want to project new data. If you have integrated embeddings (such ash X_pca_harmony) those will change every time you add new data. Using genes to fit a initial UMAP will ensure that you can transform new data, provided you have the same genes",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2259#issuecomment-1133929205:174,integrat,integrated,174,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2259#issuecomment-1133929205,1,['integrat'],['integrated']
Integrability,"Yes, I agree. My proposal would be to integrate this PR as it currently stands (after the testing system is working), as it fixes #1097 (namely, you can't currently set x, y, or color to something that is in `adata.raw.var.index` but not `adata.var.index`, even if `use_raw=True`) and leaves the logic flow for the transposition case as is. It also adds some test coverage to use cases of `sc.pl.scatter()` that were not covered before. Then, I can start working on coming up with a strategy for solving the second problem (`use_raw=True` with transposition) separately, as it's mostly unrelated, and seems to be a bit more complicated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2027#issuecomment-964309514:38,integrat,integrate,38,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2027#issuecomment-964309514,1,['integrat'],['integrate']
Integrability,"Yes, I think that would be the best solution for the time during which we rely on packages which do not ship proper wheels... . I agree that in the future, `scanpy` could become the full installation. Why not `scanpy-core`, `scanpy`, `scanpy-full`? I don't think it will bother anyone if we stop supporting `scanpy-full` at some point and only use `scanpy`. Given how Scanpy is set up and used, I could also imagine that, upon growing, it will become in some parts even more a thin wrapper for packages that should be optionally installed (it is already a thin wrapper for `igraph`, `louvain` and `MulticoreTSNE`, where Scanpy simply makes the usage more convenient by unifying visualization etc. and efficient by reusing input parameters that have previously been computed and used in other parts of Scanpy - right now, essentially all the preprocessing, the neighborhood relations and graph stuff). . What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/59#issuecomment-355144559:482,wrap,wrapper,482,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/59#issuecomment-355144559,4,['wrap'],['wrapper']
Integrability,"Yes, I totally agree that creating a fast implementation is probably not straightforward. The major bottleneck IMHO is computing this many neighbors to maximize the rejection rate, especially with 900k cells. In the original paper, we tried to find a range of neighborhood sizes K that return a maximal rejection rate, which is roughly between K = 50 to 0.5 * N where N denotes the number of cells, but there might be also a dependence on the number of batches, which we did not fully explore.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/364#issuecomment-1372444139:425,depend,dependence,425,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/364#issuecomment-1372444139,1,['depend'],['dependence']
Integrability,"Yes, for sure, one of the two commits I just pushed adds a few tests for various cases of `sc.pl.scatter()` with `basis=None`. You're totally right that plotting the transposed matrix (i.e., cells as points in the scatterplot, instead of genes) does not work. I think this is a separate problem from the one I'm fixing here, in response to the issue #1097. It seems to me there are two problems going on:; 1. The wrapper function `sc.pl.scatter()` mistakenly raises a ValueError if x, y, or color are var_names that exist only in raw but not in the base layer, even if `use_raw=True`. The underlying `_scatter_obs()` has no problem dealing with this situation, so to solve this, `sc.pl.scatter()` just needs to call `_scatter_obs()` in this case instead of raising a ValueError. This PR fixes that.; 2. When x, y, and/or color are variables found in `obs.index` or `var.keys()`, `sc.pl.scatter()` makes a transposed version of `adata`, but as you said, `adata.raw` does not get transposed. This leads to an `AttributeError` on this line of `_scatter_obs()`:. https://github.com/theislab/scanpy/blob/cab9f781f9fdee2eeebf05a84c2ce5f717afa514/scanpy/plotting/_anndata.py#L250; `AttributeError: 'NoneType' object has no attribute 'obs_vector'`. I'm not sure what the correct way to handle this is, so for now, I'm; * adding a commit so that this PR does not modify anything in the part of `sc.pl.scatter()` dealing with transposition; * adding a parametrized test to `test_plotting.py` that tests all use-cases of `sc.pl.scatter()` where `basis=None`, _except_ for the one where `use_raw=True` and x/y/color are per-obs variables, as that didn't work before this commit or after it. It's still in the parameter list but commented out for now. I'd be happy to help fix it since I've familiarized myself with this code pretty well, but we might need to discuss how this case should be handled first, and my suggestion would be to deal with this in a separate PR/bug fix/commit. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2027#issuecomment-964253242:413,wrap,wrapper,413,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2027#issuecomment-964253242,1,['wrap'],['wrapper']
Integrability,"Yes, graph_tool is nice and I'm also using it; but yes, it's installation is even worse than igraph... hence, no option for a dependency...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/97#issuecomment-370355231:126,depend,dependency,126,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97#issuecomment-370355231,1,['depend'],['dependency']
Integrability,"Yes, it looks like we didn't update our dependency requirements correctly. It looks like the `rmatmat` argument for `LinearOperators` was only added as of `1.4`. I believe using `scanpy 1.5.1` with `scipy>1.4` should fix this. Could you let me know if that solves your problem?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1246#issuecomment-633451019:40,depend,dependency,40,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1246#issuecomment-633451019,1,['depend'],['dependency']
Integrability,"Yes, it works great for me. I can compare scores obtained on individual samples and on integrated data to show that genes that are spatially variable in samples remain such on integrated data. However, with default n_iters most pvalues were 0 (for my known marker set), but calculating more iters would take too long, so I might just use the I-score where possible. ; I would like to add Moran's I as an bio conservation integration metric to scIB - this is for me the only metric that does not require cell subtype annotation (which is cumbersome and unreliable procedure) and it performs similar to current scIB metrics. However, scIB has as dependency only scanpy, not squidpy. It seems a bit of an overkill to add package dependency to scIB for a single function. . Semitones (https://www.biorxiv.org/content/10.1101/2020.11.17.386664v1) is a package for finding genes linearly variable across embedding and I think Moran's I would also give me similar genes (must try it out) - Moran's I might be even better for the task and quicker + less complicated. This is another reason why it would be neat to have Moran's I directly in scanpy. You may not have spatial data, so not really needing squidpy. But finding gene patterns may be useful when you have continuous effects but no trajectories - this is what my main beta cell subtype analysis is currently based on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1698#issuecomment-787504982:87,integrat,integrated,87,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698#issuecomment-787504982,5,"['depend', 'integrat']","['dependency', 'integrated', 'integration']"
Integrability,"Yes, thank you. That would be very welcome! . @rfechtner: could it be that compat with your interface got messed up? It would be nice if you'd maintained the interface when you do so drastic changes to the underlying package. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/310#issuecomment-430990692:92,interface,interface,92,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/310#issuecomment-430990692,2,['interface'],['interface']
Integrability,"Yes, the image does not synchronize with flipped spatial dots. ; I find a way to flip the image by changing the image coords:; hires_coord = slide.uns['spatial']['sample1']['images'][""hires""]; slide.uns['spatial']['sample1']['images'][""hires""] = hires_coord[:,:,::-1]",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2520#issuecomment-1605898378:24,synchroniz,synchronize,24,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2520#issuecomment-1605898378,1,['synchroniz'],['synchronize']
Integrability,"Yes, this makes a lot of sense. This is also what we found in our review of data integration methods and pre-processing decisions [here](https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2). I'm not sure I agree with ""only a small fraction of genes are expected to be informative though"". There is definitely a variable signal-to-noise ratio though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1578#issuecomment-764850023:81,integrat,integration,81,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578#issuecomment-764850023,1,['integrat'],['integration']
Integrability,"Yes, this was my impression too. However there is a documented option; ""Switch to Windows containers"" which is available if you right click on the; Docker icon in the taskbar and this allows one to run vms using a Windows; kernel. On Fri, Sep 6, 2024, 3:36 AM Philipp A. ***@***.***> wrote:. > If you want to try it out, I give instructions for how to reproduce the; > error with a Docker container for Windows in the cross-referenced issue; >; > Yes please. I’m confused how Windows comes into play though since I thougt; > that Docker always runs on a Linux kernel – natively on Linux and in a VM; > on macOS and Windows.; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/scverse/scanpy/issues/2969#issuecomment-2333436219>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AH2OS47KNFAVTYUHGAMORILZVFLRXAVCNFSM6AAAAABFM3NQROVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDGMZTGQZTMMRRHE>; > .; > You are receiving this because you were mentioned.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2969#issuecomment-2334006260:997,Message,Message,997,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2969#issuecomment-2334006260,1,['Message'],['Message']
Integrability,"Yes. I'm interested in many of the things here. Thank you for pinging me. I'm happy to engage going forward in a variety of ways. Let's start with a few responses. > I tried looking at pydata sparse with Dask, but it ran a lot slower than regular scipy.sparse (which is what Scanpy uses). It would be great to get a slimmed down version of the operations that you're running with pydata/sparse and submit those to the issue tracker there. @hameerabbasi is usually pretty responsive, and I know that he appreciates learning about new use cases of pydata/sparse. > So I wrote a wrapper around scipy.sparse to implement NumPy's __array_function__ protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular scipy.sparse. Thoughts on adding this to scipy.sparse itself so that we can avoid the wrapper? cc @rgommers. > It turned out that by using Anndata arrays, Dask has to materialize intermediate data more than is necessary in order to populate the Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a f",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/921#issuecomment-557191880:576,wrap,wrapper,576,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-557191880,6,"['protocol', 'wrap']","['protocol', 'wrapper']"
Integrability,"You can easily access the silhouette coefficient via scikit-learn. . I would be hesitant to base optimal numbers of clusters on the silhouette coefficient though. The number of clusters is typically dependent on the biological question of interest. There's not really a scale at which all biological questions can be answered. Therefore you have a resolution parameter to check multiple resolutions. For example, T cells could be taken as one cluster or sub-clustered into CD4+ and CD8+ (which is typically done). Here a problem with the silhouette coefficient also shows: often you have one big cluster of T-cells which reluctantly cluster into the CD4+ and CD8+ subtypes (early 10X datasets show this nicely). This will have a lower silhouette coefficient, but it is probably more informative for many people.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/670#issuecomment-498066846:199,depend,dependent,199,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670#issuecomment-498066846,2,['depend'],['dependent']
Integrability,You can store different forms of the matrix in `layers` and often choose which one to use with the `layers` argument. It really depends on the function whether it expects normalized or count data. Most functions should mention it in the documentation if the expect count data.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1875#issuecomment-867418801:128,depend,depends,128,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1875#issuecomment-867418801,1,['depend'],['depends']
Integrability,"You could use conda ([relevant docs](https://scanpy.readthedocs.io/en/stable/installation.html#bioconda)). Not having a GUI shouldn't matter, but I'm not sure if Tkinter is an installation dependency for `matplotlib`. If you're getting an error related to an interactive backend when you try to plot, you can switch the [matplotlib backend](https://matplotlib.org/faq/usage_faq.html#what-is-a-backend).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/595#issuecomment-480657396:189,depend,dependency,189,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/595#issuecomment-480657396,1,['depend'],['dependency']
Integrability,You have already replied [here](https://github.com/theislab/single-cell-tutorial/issues/103#issuecomment-1313299316). Many apologies for missing this message.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1599#issuecomment-1467415727:150,message,message,150,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599#issuecomment-1467415727,1,['message'],['message']
Integrability,"Your way sounds sure better, many things into the scrublet algorithm are in; redundancy with components of scanpy. It will sure look great :); Just one thing: in the scrublet paper they suggest always to just run the; simulation of doublets and look at the expected vs estimated fraction of; doublets before removing doublets. If those two values do not match, they; say one should rerun scrublet and tune the expected fraction.; Does your script only run simulation of doublets and output the doublets; score, or does it also remove doublets at once? If you do the latter, then; one is not able to simulate doublets more than once to adjust the expected; doublet fraction.; Cheers. Den tor. 16. maj 2019 kl. 05.15 skrev Sam Wolock <notifications@github.com>:. > @cartal <https://github.com/cartal> @SamueleSoraggi; > <https://github.com/SamueleSoraggi>; > For some reason I decided to integrate Scrublet using Scanpy's functions; > where possible, rather than making a simple wrapper. The core functionality; > is up and running in this fork <https://github.com/swolock/scanpy>, and; > now I just need to add documentation, make some of the code more; > Scanpythonic(?), and add an example.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/173?email_source=notifications&email_token=ACC66UNQC744WOUTLRZ2CN3PVTGWTA5CNFSM4FE4LIF2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVQRA2I#issuecomment-492900457>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACC66UI4FF4LES7GRVKHZZDPVTGWTANCNFSM4FE4LIFQ>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/173#issuecomment-492936700:886,integrat,integrate,886,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-492936700,4,"['integrat', 'wrap']","['integrate', 'wrapper']"
Integrability,[Proposal] Integrate Marsilea to visualize AnnData,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2444:11,Integrat,Integrate,11,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444,1,['Integrat'],['Integrate']
Integrability,"[`sinfo` has been replaced](https://pypi.org/project/sinfo/) with [`session_info`](https://gitlab.com/joelostblom/session_info), which is definitely a better name. We should switch over to using this. I think we'll be calling it like: `import session_info; session_info.show(dependencies=True, html=False, **extra_kwargs)`",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1852:275,depend,dependencies,275,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1852,1,['depend'],['dependencies']
Integrability,"],; [-1.145477, 10.185449, 4.414117, ..., -0.087394, -1.327791,...; ```. The second test:. ```; sc.pp.neighbors(adata1, n_pcs=30, use_rep='X_pca_harmony'); sc.pp.neighbors(adata2, n_pcs=30, use_rep='X_pca_harmony'); np.testing.assert_array_equal(adata1.obsp[""connectivities""].data, adata2.obsp[""connectivities""].data); ```. It raised the Error:. ```; AssertionError: ; Arrays are not equal. Mismatched elements: 268636 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 434492 (61.8%); Max absolute difference: 0.99820393; Max relative difference: 810.4644; x: array([0.158963, 0.206843, 0.234457, ..., 0.095996, 0.179325, 1. ],; dtype=float32); y: array([0.158963, 0.206843, 0.234457, ..., 0.095996, 0.179324, 1. ],; dtype=float32); ```. This is my session_info:. ```; Click to view session information; -----; anndata 0.9.2; loguru 0.7.2; matplotlib 3.8.0; numpy 1.26.0; pandas 1.4.3; scanpy 1.9.6; seaborn 0.12.2; session_info 1.0.0; -----; Click to view modules imported as dependencies; PIL 9.4.0; argcomplete NA; asttokens NA; attr 23.1.0; awkward 2.4.2; awkward_cpp NA; backcall 0.2.0; cffi 1.15.1; comm 0.1.4; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; debugpy 1.8.0; decorator 5.1.1; dot_parser NA; etils 1.4.1; exceptiongroup 1.1.3; executing 1.2.0; get_annotations NA; gmpy2 2.1.2; h5py 3.9.0; harmonypy NA; igraph 0.10.8; importlib_metadata NA; importlib_resources NA; ipykernel 6.25.2; ipywidgets 8.1.1; jax 0.4.20; jaxlib 0.4.20; jedi 0.19.0; joblib 1.2.0; kiwisolver 1.4.4; leidenalg 0.10.1; llvmlite 0.41.1; ml_dtypes 0.2.0; mpl_toolkits NA; mpmath 1.3.0; natsort 8.4.0; numba 0.58.1; nvfuser NA; opt_einsum v3.0.0; packaging 23.1; parso 0.8.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; platformdirs 3.10.0; prompt_toolkit 3.0.39; psutil 5.9.5; ptyprocess 0.7.0; pure_eval 0.2.2; pyarrow 13.0.0; pycparser 2.21; pydev_ipython NA; pydevconsole NA; pydevd 2.9.5; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pydot 1.4.2;",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2655#issuecomment-1823084227:3154,depend,dependencies,3154,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655#issuecomment-1823084227,1,['depend'],['dependencies']
Integrability,"_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy); 137 adata._init_as_actual(adata.copy()); 138 neighbors = Neighbors(adata); --> 139 neighbors.compute_neighbors(; 140 n_neighbors=n_neighbors,; 141 knn=knn,. /storage1/fs1/leyao.wang/Active/conda/envs/velocyto3.9/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds); 806 # we need self._distances also for method == 'gauss' if we didn't; 807 # use dense distances; --> 808 self._distances, self._connectivities = _compute_connectivities_umap(; 809 knn_indices,; 810 knn_distances,. /storage1/fs1/leyao.wang/Active/conda/envs/velocyto3.9/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity); 385 # umap 0.5.0; 386 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""); --> 387 from umap.umap_ import fuzzy_simplicial_set; 388 ; 389 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). /storage1/fs1/leyao.wang/Active/conda/envs/velocyto3.9/lib/python3.9/site-packages/umap/__init__.py in <module>; 1 from warnings import warn, catch_warnings, simplefilter; ----> 2 from .umap_ import UMAP; 3 ; 4 try:; 5 with catch_warnings():. /storage1/fs1/leyao.wang/Active/conda/envs/velocyto3.9/lib/python3.9/site-packages/umap/umap_.py in <module>; 45 ); 46 ; ---> 47 from pynndescent import NNDescent; 48 from pynndescent.distances import named_distances as pynn_named_distances; 49 from pynndescent.sparse import sparse_named_distances as pynn_sparse_named_distances. /storage1/fs1/leyao.wang/Active/conda/envs/velocyto3.9/lib/python3.9/site-packages/pynndescent/__init__.py in <module>; 13 numba.config.THREADING_LAYER = ""workqueue""; 14 ; ---> 15 __version__ = pkg_resources.get_distribution(""pynndescent"").version. /opt/conda/lib/python3.9/site-packages/pkg",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2169:1921,message,message,1921,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2169,1,['message'],['message']
Integrability,"_path)); RuntimeError: cannot cache function '__shear_dense': no locator available for file '/opt/conda/lib/python3.7/site-packages/librosa/util/utils.py'; 1.10.1+cu102; Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""/opt/conda/lib/python3.7/site-packages/scanpy/__init__.py"", line 14, in <module>; from . import tools as tl; File ""/opt/conda/lib/python3.7/site-packages/scanpy/tools/__init__.py"", line 1, in <module>; from ..preprocessing import pca; File ""/opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/__init__.py"", line 1, in <module>; from ._recipes import recipe_zheng17, recipe_weinreb17, recipe_seurat; File ""/opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/_recipes.py"", line 7, in <module>; from ._deprecated.highly_variable_genes import (; File ""/opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/_deprecated/highly_variable_genes.py"", line 11, in <module>; from .._utils import _get_mean_var; File ""/opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/_utils.py"", line 45, in <module>; @numba.njit(cache=True); File ""/opt/conda/lib/python3.7/site-packages/numba/core/decorators.py"", line 214, in wrapper; disp.enable_caching(); File ""/opt/conda/lib/python3.7/site-packages/numba/core/dispatcher.py"", line 812, in enable_caching; self._cache = FunctionCache(self.py_func); File ""/opt/conda/lib/python3.7/site-packages/numba/core/caching.py"", line 610, in __init__; self._impl = self._impl_class(py_func); File ""/opt/conda/lib/python3.7/site-packages/numba/core/caching.py"", line 348, in __init__; ""for file %r"" % (qualname, source_path)); RuntimeError: cannot cache function 'sparse_mean_var_minor_axis': no locator available for file '/opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/_utils.py'. ```. I would highly appreciate if you could please point out how to fix this issue. . Thank you in advance!. Best wishes,; Abdelrahman . ```. #### Versions. <details>. numba==0.53.1; scanpy==1.8.1. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2113:3450,wrap,wrapper,3450,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2113,1,['wrap'],['wrapper']
Integrability,"_phil.wrapper(). h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/h5d.pyx in h5py.h5d.DatasetID.read(). h5py/_proxy.pyx in h5py._proxy.dset_rw(). h5py/_proxy.pyx in h5py._proxy.H5PY_H5Dread(). OSError: Can't read data (file read failed: time = Sat Aug 1 13:27:54 2020; , filename = '/path.../filtered_gene_bc_matrices.h5ad', file descriptor = 47, errno = 5, error message = 'Input/output error', buf = 0x55ec782e9031, total read size = 7011, bytes this sub-read = 7011, bytes actually read = 18446744073709551615, offset = 0). During handling of the above exception, another exception occurred:. AnnDataReadError Traceback (most recent call last); <ipython-input-14-faac769583f8> in <module>; 17 #while True:; 18 #try:; ---> 19 adatas.append(sc.read_h5ad(file)); 20 file_diffs.append('_'.join([file.split('/')[i] for i in diff_path_idx])); 21 #break. ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/anndata/_io/h5ad.py in read_h5ad(filename, backed, as_sparse, as_sparse_fmt, chunk_size); 411 d[k] = read_dataframe(f[k]); 412 else: # Base case; --> 413 d[k] = read_attribute(f[k]); 414 ; 415 d[""raw""] = _read_raw(f, as_sparse, rdasp). ~/miniconda3/envs/rpy2_3/lib/python3.8/functools.py in wrapper(*args, **kw); 873 '1 positional argument'); 874 ; --> 875 return dispatch(args[0].__class__)(*args, **kw); 876 ; 877 funcname = getattr(func, '__name__', 'singledispatch function'). ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, *args, **kwargs); 160 else:; 161 parent = _get_parent(elem); --> 162 raise AnnDataReadError(; 163 f""Above error raised while reading key {elem.name!r} of ""; 164 f""type {type(elem)} from {parent}."". AnnDataReadError: Above error raised while reading key '/X' of type <class 'h5py._hl.group.Group'> from /.; ```. #### Versions:; ```; scanpy==1.5.1 anndata==0.7.4 umap==0.4.6 numpy==1.18.5 scipy==1.4.1 pandas==1.0.5 scikit-learn==0.23.1 statsmodels==0.11.1 python-igraph==0.8.2 louvain==0.6.1 leidenalg==0.8.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1351:2826,wrap,wrapper,2826,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351,1,['wrap'],['wrapper']
Integrability,"```; try:; from bbknn import bbknn; except ImportError:; def bbknn(*args, **kwargs):; raise ImportError('Please install BBKNN: `pip3 install bbknn`'); ```. > I went that way since I didn’t want to make it look like we coded it (with the docs hosted on our page and so on). Do you think that’s a good solution or would you like it to be done differently?. This is great!. https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.bbknn.html#scanpy.api.pp.bbknn. But I actually don't see any docs there, I don't know why it doesn't find the original docstring... We'd like to have the reference to @ktpolanski preprint in the docstring in the first line together with a short summary of what it does and how it does it, just as for any other function. As this directly uses the implementation from @ktpolanski, we'd also want an explicit statement about that. > pp.bbknn is just an alias for bbknn.bbknn(). Refer to it for the documentation. ... can be removed. That should be evident... In the case of the `tl.leiden` wrapper, I'd also add an explicit statement that this wraps the leiden package of Traag (2018). Otherwise, all of this is great!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/361#issuecomment-439841050:1017,wrap,wrapper,1017,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361#issuecomment-439841050,2,['wrap'],"['wrapper', 'wraps']"
Integrability,"```pytb; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <timed exec> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_pca.py in pca(data, n_comps, zero_center, svd_solver, random_state, return_info, use_highly_variable, dtype, copy, chunked, chunk_size); 201 ); 202 ; --> 203 output = _pca_with_sparse(X, n_comps, solver=svd_solver); 204 # this is just a wrapper for the results; 205 X_pca = output['X_pca']. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_pca.py in _pca_with_sparse(X, npcs, solver, mu, random_state); 293 return XHmat(x) - mhmat(ones(x)); 294 ; --> 295 XL = LinearOperator(; 296 matvec=matvec,; 297 dtype=X.dtype,. TypeError: __init__() got an unexpected keyword argument 'rmatmat'; ```. I got this error once with the new spare PCA. @atarashansky do we need to write an explicit scipy version as dependency? It might be something weird with my setup too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1066#issuecomment-636055632:453,wrap,wrapper,453,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-636055632,2,"['depend', 'wrap']","['dependency', 'wrapper']"
Integrability,"```python; import scanpy as sc; adata = sc.datasets.pbmc68k_reduced(); sc.get.aggregate(adata, by=""louvain"", func=""mean""); ```. ```; AnnData object with n_obs × n_vars = 11 × 765; obs: 'louvain'; var: 'n_counts', 'means', 'dispersions', 'dispersions_norm', 'highly_variable'; layers: 'mean'; ```. ```python; sc.get.aggregate(adata.obsm[""X_umap""], by=adata.obs[""louvain""].array, func=""mean""); ```. ```; {'mean': array([[ -6.18019123, -6.12846152],; [ -3.10995685, 8.4991954 ],; [ 6.30307056, -2.15245383],; [ -4.72268065, -3.24033642],; [-11.94002487, -5.39480163],; [ -1.39242794, 6.6239316 ],; [ 4.3991326 , -0.16749119],; [ 4.847834 , -9.30549509],; [-10.41891144, -1.15700949],; [ -7.91249486, -4.06782072],; [ 1.12418592, -6.94506866]])}; ```. So it returns an `AnnData` when an `AnnData` is passed, but a dict when a less structured object is passed. This is probably because it's `singledispatched` under the hood, but IDK that this behaviour is great. I think it could make more sense for this to either:. * Always return an `AnnData`; * Throw an error if something other than an AnnData is passed in. A third option is that we document this behaviour, but I generally don't love it. There are other places that we do something like this, i.e. return a different type depending on the input. However, I feel like there's more of a loss of information here and less of an obvious return type. Maybe in future this could get a `return_type: type[AnnData] | type[Dict] | type[xr.Dataset] = AnnData` argument that controls what is returned?. WDYT @ilan-gold @Intron7?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2930:1275,depend,depending,1275,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2930,1,['depend'],['depending']
Integrability,"`conda install` (not `pip`). Perhaps, that is due to pytables' conda dependencies (such as hdf).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/454#issuecomment-462140438:69,depend,dependencies,69,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454#issuecomment-462140438,1,['depend'],['dependencies']
Integrability,"`conda install` meant to be related to `pytables`, not `scanpy`. `scanpy` runs easily via `pip`, only the tables dependency complains..",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/454#issuecomment-462261457:113,depend,dependency,113,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454#issuecomment-462261457,1,['depend'],['dependency']
Integrability,`gprofiler` functionality is being added to scanpy? I have a small wrapper for that as well... the main components being a try-catch wrapper around it as it can give an error when there are no results.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/467#issuecomment-463965367:67,wrap,wrapper,67,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467#issuecomment-463965367,2,['wrap'],['wrapper']
Integrability,"`log_transformed` disappeared in the last commit here... Better: `pp.log1p` should write an attribute to `.uns`, say simply `.uns['log1p'] = True`. Depending on that attribute, log2fc is computed by rexponaniating or not. Also: If trying to call a t-test with non-logarithmized data, a warning should be written. The overflow and 0 warnings: are you sure you used logarithmized data, Gökcen?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/519#issuecomment-477907809:148,Depend,Depending,148,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/519#issuecomment-477907809,1,['Depend'],['Depending']
Integrability,"`numba` really is the main blocker, see also: . * Tracking issue: https://github.com/pyodide/pyodide/issues/621; * Potential PR: https://github.com/emscripten-forge/recipes/pull/168; * Unfortunately, the author recently founded prefix.dev so may not have time to complete this 😢. `pynndescent` also depends on numba. I am hopeful that numba's new AOT backend may make this easier in the future. Unclear how painful it would be to distribute binaries capable of multithreading though. I think `h5py` would also be pretty reasonable to make optional if we could otherwise run in pyodide, since we wouldn't have a filesystem anyways. Though I think pytables runs in pyodide, so it's probably reasonable to get `h5py` there too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2667#issuecomment-1804146936:299,depend,depends,299,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2667#issuecomment-1804146936,1,['depend'],['depends']
Integrability,"`pytables` in pip is named `tables`, and scanpy `import tables` accordingly, so you have to separately install it using also pip, not conda. I suggest editing the [installation guide](https://scanpy.readthedocs.io/en/stable/installation.html). Besides, my installing using `conda install -c bioconda scanpy` would always give conflicts with nvidia cuda versions, but changing the version makes nothing change except the conflict message. Is it a Windows feature?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1468#issuecomment-747217584:429,message,message,429,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1468#issuecomment-747217584,2,['message'],['message']
Integrability,a 2.19.1; fastrlock 0.8.2; fcsparser 0.2.8; filelock 3.13.1; fiona 1.9.5; folium 0.16.0; fonttools 4.49.0; fqdn 1.5.1; frozenlist 1.4.1; fsspec 2024.2.0; GDAL 3.8.1; gdown 5.1.0; geopandas 0.14.3; h11 0.14.0; h2 4.1.0; h5py 3.10.0; harmonypy 0.0.9; holoviews 1.18.3; hpack 4.0.0; httpcore 1.0.4; httpx 0.27.0; hyperframe 6.0.1; idna 3.6; igraph 0.11.4; imagecodecs 2024.1.1; imageio 2.34.0; importlib_metadata 7.0.2; importlib_resources 6.1.3; inflect 7.0.0; ipykernel 6.29.3; ipylab 1.0.0; ipython 8.22.2; ipywidgets 8.1.2; isoduration 20.11.0; jedi 0.19.1; Jinja2 3.1.3; joblib 1.3.2; json5 0.9.22; jsonpointer 2.4; jsonschema 4.21.1; jsonschema-specifications 2023.12.1; jupyter_client 8.6.0; jupyter_core 5.7.1; jupyter-events 0.9.0; jupyter-lsp 2.2.4; jupyter_server 2.13.0; jupyter_server_proxy 4.1.0; jupyter_server_terminals 0.5.2; jupyterlab 4.1.4; jupyterlab_pygments 0.3.0; jupyterlab_server 2.25.3; jupyterlab_widgets 3.0.10; kiwisolver 1.4.5; lamin_utils 0.13.0; lazy_loader 0.3; legacy-api-wrap 1.4; leidenalg 0.10.2; linkify-it-py 2.0.3; llvmlite 0.42.0; locket 1.0.0; louvain 0.8.1; lz4 4.3.3; mapclassify 2.6.1; Markdown 3.5.2; markdown-it-py 3.0.0; MarkupSafe 2.1.5; matplotlib 3.8.3; matplotlib-inline 0.1.6; mdit-py-plugins 0.4.0; mdurl 0.1.2; mistune 3.0.2; msgpack 1.0.7; mudata 0.2.3; multidict 6.0.5; multipledispatch 0.6.0; munkres 1.1.4; muon 0.1.5; natsort 8.4.0; nbclient 0.8.0; nbconvert 7.16.2; nbformat 5.9.2; nbproject 0.10.1; nest_asyncio 1.6.0; networkx 3.2.1; notebook 7.1.1; notebook_shim 0.2.4; numba 0.59.0; numcodecs 0.12.1; numpy 1.24.4; nvtx 0.2.10; omnipath 1.0.8; openpyxl 3.1.2; orjson 3.9.15; overrides 7.7.0; packaging 24.0; pandas 1.5.3; pandocfilters 1.5.0; panel 1.3.8; param 2.0.2; parso 0.8.3; partd 1.4.1; patsy 0.5.6; pexpect 4.9.0; pickleshare 0.7.5; pillow 10.2.0; pip 24.0; pkgutil_resolve_name 1.3.10; platformdirs 4.2.0; pooch 1.8.1; prometheus_client 0.20.0; prompt-toolkit 3.0.42; protobuf 4.25.3; psutil 5.9.8; ptxcompiler 0.8.1; ptyprocess,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2964:3851,wrap,wrap,3851,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2964,1,['wrap'],['wrap']
Integrability,"a in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to access the current version of this software? . I think that major partitions identified with CytOF or scRNA-seq can be linked together providing a coarse-grained mechanism to demonstrate how heterogeneity identified with each technique relates to each other based on a given experimental time point. . (3) Histogram integration in the plotting api for QC metrics would be helpful. While scatter plots and violin plots are effective, I find myself wanting to make cuts (bounds on percent mitochondrial or nGenes) in my data based off histograms. Again, thank you so much for the amazing software!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/510:1723,integrat,integrate,1723,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510,2,['integrat'],"['integrate', 'integration']"
Integrability,"a.caching'; 0.53.1; 0.7.8; 1.10.1+cu102; Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""/opt/conda/lib/python3.7/site-packages/librosa/__init__.py"", line 211, in <module>; from . import core; File ""/opt/conda/lib/python3.7/site-packages/librosa/core/__init__.py"", line 5, in <module>; from .convert import * # pylint: disable=wildcard-import; File ""/opt/conda/lib/python3.7/site-packages/librosa/core/convert.py"", line 7, in <module>; from . import notation; File ""/opt/conda/lib/python3.7/site-packages/librosa/core/notation.py"", line 8, in <module>; from ..util.exceptions import ParameterError; File ""/opt/conda/lib/python3.7/site-packages/librosa/util/__init__.py"", line 83, in <module>; from .utils import * # pylint: disable=wildcard-import; File ""/opt/conda/lib/python3.7/site-packages/librosa/util/utils.py"", line 1848, in <module>; def __shear_dense(X, factor=+1, axis=-1):; File ""/opt/conda/lib/python3.7/site-packages/numba/core/decorators.py"", line 214, in wrapper; disp.enable_caching(); File ""/opt/conda/lib/python3.7/site-packages/numba/core/dispatcher.py"", line 812, in enable_caching; self._cache = FunctionCache(self.py_func); File ""/opt/conda/lib/python3.7/site-packages/numba/core/caching.py"", line 610, in __init__; self._impl = self._impl_class(py_func); File ""/opt/conda/lib/python3.7/site-packages/numba/core/caching.py"", line 348, in __init__; ""for file %r"" % (qualname, source_path)); RuntimeError: cannot cache function '__shear_dense': no locator available for file '/opt/conda/lib/python3.7/site-packages/librosa/util/utils.py'; 1.10.1+cu102; Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""/opt/conda/lib/python3.7/site-packages/scanpy/__init__.py"", line 14, in <module>; from . import tools as tl; File ""/opt/conda/lib/python3.7/site-packages/scanpy/tools/__init__.py"", line 1, in <module>; from ..preprocessing import pca; File ""/opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/__init__.py"", ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2113:1836,wrap,wrapper,1836,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2113,1,['wrap'],['wrapper']
Integrability,"a.uns['iroot']=0; sc.tl.dpt(adata). import matplotlib.pyplot as plt; fig,ax=plt.subplots(1,1,figsize=(7,1)); path_data = sc.pl.paga_path(; adata,; [4, 5],; [""Elane""],; ax=ax,; show_node_names=False,; ytick_fontsize=12,; return_data=True,; #n_avg=1,; color_map=""Greys"",; groups_key=""leiden"",; color_maps_annotations={""dpt_pseudotime"": ""viridis""}; ); ```. ### Error output. ```pytb; TypeError Traceback (most recent call last); Cell In[1], line 15; 13 import matplotlib.pyplot as plt; 14 fig,ax=plt.subplots(1,1,figsize=(7,1)); ---> 15 path_data = sc.pl.paga_path(; 16 adata,; 17 [4, 5],; 18 [""Elane""],; 19 ax=ax,; 20 show_node_names=False,; 21 ytick_fontsize=12,; 22 return_data=True,; 23 #n_avg=1,; 24 color_map=""Greys"",; 25 groups_key=""leiden"",; 26 color_maps_annotations={""dpt_pseudotime"": ""viridis""}; 27 ). File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\scanpy\plotting\_tools\paga.py:1255, in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax); 1253 print(X.shape); 1254 if as_heatmap:; -> 1255 img = ax.imshow(X, aspect=""auto"", interpolation=""nearest"", cmap=color_map); 1256 if show_yticks:; 1257 ax.set_yticks(range(len(X))). File ~\AppData\Local\Packag",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3025:2084,wrap,wrapper,2084,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3025,1,['wrap'],['wrapper']
Integrability,"a\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy); 137 adata._init_as_actual(adata.copy()); 138 neighbors = Neighbors(adata); --> 139 neighbors.compute_neighbors(; 140 n_neighbors=n_neighbors,; 141 knn=knn,. C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds); 809 # we need self._distances also for method == 'gauss' if we didn't; 810 # use dense distances; --> 811 self._distances, self._connectivities = _compute_connectivities_umap(; 812 knn_indices,; 813 knn_distances,. C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity); 390 # umap 0.5.0; 391 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""); --> 392 from umap.umap_ import fuzzy_simplicial_set; 393 ; 394 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). C:\ProgramData\Anaconda3\lib\site-packages\umap\__init__.py in <module>; ----> 1 from .umap_ import UMAP; 2 ; 3 # Workaround: https://github.com/numba/numba/issues/3341; 4 import numba; 5 . C:\ProgramData\Anaconda3\lib\site-packages\umap\umap_.py in <module>; 52 from umap.spectral import spectral_layout; 53 from umap.utils import deheap_sort, submatrix; ---> 54 from umap.layouts import (; 55 optimize_layout_euclidean,; 56 optimize_layout_generic,. C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py in <module>; 37 },; 38 ); ---> 39 def rdist(x, y):; 40 """"""Reduced Euclidean distance.; 41 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\decorators.py in wrapper(func); 217 with typeinfer.register_dispatcher(disp):; 218 for sig in sigs:; --> 219 disp.compile(sig); 220 disp.disable_compile(); 221 return disp. C:\ProgramData\Anaconda3\lib\site-p",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-1319286325:4847,message,message,4847,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-1319286325,1,['message'],['message']
Integrability,"ache, suppress_cache_warning, **kwargs); 458 'Provide `sheet` parameter when reading \'.xlsx\' files.'); 459 else:; --> 460 adata = read_excel(filename, sheet); 461 elif ext in {'mtx', 'mtx.gz'}:; 462 adata = read_mtx(filename). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/anndata/readwrite/read.py in read_excel(filename, sheet, dtype); 59 # rely on pandas for reading an excel file; 60 from pandas import read_excel; ---> 61 df = read_excel(fspath(filename), sheet, dtype=dtype); 62 X = df.values[:, 1:]; 63 row = {'row_names': df.iloc[:, 0].values.astype(str)}. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs); 186 else:; 187 kwargs[new_arg_name] = new_arg_value; --> 188 return func(*args, **kwargs); 189 return wrapper; 190 return _deprecate_kwarg. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs); 186 else:; 187 kwargs[new_arg_name] = new_arg_value; --> 188 return func(*args, **kwargs); 189 return wrapper; 190 return _deprecate_kwarg. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/excel.py in read_excel(io, sheet_name, header, names, index_col, parse_cols, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, verbose, parse_dates, date_parser, thousands, comment, skip_footer, skipfooter, convert_float, mangle_dupe_cols, **kwds); 373 convert_float=convert_float,; 374 mangle_dupe_cols=mangle_dupe_cols,; --> 375 **kwds); 376 ; 377 . ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/excel.py in parse(self, sheet_name, header, names, index_col, usecols, squeeze, converters, true_values, false_values, skiprows, nrows, na_values, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, **kwds); 716 convert_float=convert_float,; 717 mangle_dupe_cols=mangle_dupe_cols,; --> 718 **kwds); 719 ; 720 @property. ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/547:2952,wrap,wrapper,2952,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/547,2,['wrap'],['wrapper']
Integrability,"adata.var[""highly_variable""] and adata.var[""highly_variable_intersection""] have very different meanings and it's good to have them separate, I think. Considering that PCA looks for the genes marked True in adata.var[""highly_variable""] (regardless of the value of the batch_key option), using adata.var[""highly_variable_intersection""] for filtering is not a good idea. If there is confusion between adata.var[""highly_variable""] and adata.var[""highly_variable_intersection""]:. If the user specifies n_top_genes, adata.var[""highly_variable""] contains top variable genes in the list of genes sorted by number of batches they are detected as variable (ties broken using dispersion). If mean/dispersion filters are provided, we apply these cutoffs to mean mean/dispersion across batches to construct a unified adata.var[""highly_variable""]. adata.var[""highly_variable_intersection""] is a very strict definition that I personally avoid using at all, but it also depends on the experimental setting and batch_key itself. Therefore, there is a mistake in the following code:. ```python; sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""); adata_hvg = adata[:, adata.var.highly_variable_intersection].copy(); sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will error; see below; ```. This possibly removes many genes that are identified as highly variable in adata.var.highly_variable because adata_hvg = adata[:, adata.var.highly_variable_intersection] keeps only a subset of highly variable genes (see the definitions above). If one wants to use the strict definition, correct usage would be:. ```python; sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""); adata.var.highly_variable = adata.var.highly_variable_intersection; sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will e",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1032#issuecomment-616740607:954,depend,depends,954,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032#issuecomment-616740607,1,['depend'],['depends']
Integrability,added integration tutorial spatial,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1229:6,integrat,integration,6,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1229,1,['integrat'],['integration']
Integrability,"also as an aside, would it be appropriate to include some of @LuckyMD scIB integration metrics here? It would give people easier access and probably expand general use.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/915#issuecomment-763812897:75,integrat,integration,75,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-763812897,1,['integrat'],['integration']
Integrability,"anaconda3/envs/ml/lib/python3.9/site-packages/IPython/core/pylabtools.py:151, in print_figure(fig, fmt, bbox_inches, base64, **kwargs); 148 from matplotlib.backend_bases import FigureCanvasBase; 149 FigureCanvasBase(fig); --> 151 fig.canvas.print_figure(bytes_io, **kw); 152 data = bytes_io.getvalue(); 153 if fmt == 'svg':. File ~/anaconda3/envs/ml/lib/python3.9/site-packages/matplotlib/backend_bases.py:2230, in FigureCanvasBase.print_figure(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs); 2226 ctx = (renderer._draw_disabled(); 2227 if hasattr(renderer, '_draw_disabled'); 2228 else suppress()); 2229 with ctx:; -> 2230 self.figure.draw(renderer); 2232 if bbox_inches:; 2233 if bbox_inches == ""tight"":. File ~/anaconda3/envs/ml/lib/python3.9/site-packages/matplotlib/artist.py:74, in _finalize_rasterization.<locals>.draw_wrapper(artist, renderer, *args, **kwargs); 72 @wraps(draw); 73 def draw_wrapper(artist, renderer, *args, **kwargs):; ---> 74 result = draw(artist, renderer, *args, **kwargs); 75 if renderer._rasterizing:; 76 renderer.stop_rasterizing(). File ~/anaconda3/envs/ml/lib/python3.9/site-packages/matplotlib/artist.py:51, in allow_rasterization.<locals>.draw_wrapper(artist, renderer, *args, **kwargs); 48 if artist.get_agg_filter() is not None:; 49 renderer.start_filter(); ---> 51 return draw(artist, renderer, *args, **kwargs); 52 finally:; 53 if artist.get_agg_filter() is not None:. File ~/anaconda3/envs/ml/lib/python3.9/site-packages/matplotlib/figure.py:2790, in Figure.draw(self, renderer); 2787 # ValueError can occur when resizing a window.; 2789 self.patch.draw(renderer); -> 2790 mimage._draw_list_compositing_images(; 2791 renderer, self, artists, self.suppressComposite); 2793 for sfig in self.subfigs:; 2794 sfig.draw(renderer). File ~/anaconda3/envs/ml/lib/python3.9/site-packages/matplotlib/image.py:132, in _draw_list_compositing_images(renderer, parent, artists, suppress_composi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2285:6387,wrap,wraps,6387,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2285,1,['wrap'],['wraps']
Integrability,"ar-type:none;; 	display:none;}; -->; </style>; </head>. <body link=""#0563C1"" vlink=""#954F72"">. Package | Version; -- | --; Anaconda | 2.1.0; Python | 3.6.13; anndata | 0.7.6; anyio | 2.2.0; argon2-cffi | 20.1.0; async-generator | 1.1; attrs | 21.2.0; Babel | 2.9.1; backcall | 0.2.0; bleach | 4.0.0; brotlipy | 0.7.0; cached-property | 1.5.2; certifi | 2021.5.30; cffi | 1.14.6; charset-normalizer | 2.0.4; colorama | 0.4.4; contextvars | 2.4; **cryptography | 35.0.0**; cycler | 0.11.0; dataclasses | 0.8; decorator | 4.4.2; defusedxml | 0.7.1; entrypoints | 0.3; get-version | 2.1; h5py | 3.1.0; idna | 3.2; igraph | 0.9.8; immutables | 0.16; importlib-metadata | 4.8.1; ipykernel | 5.3.4; ipython | 7.16.1; ipython-genutils | 0.2.0; jedi | 0.17.0; **Jinja2 | 3.0.2**; joblib | 1.1.0; json5 | 0.9.6; jsonschema | 3.2.0; jupyter-client | 7.0.1; jupyter-core | 4.8.1; jupyter-server | 1.4.1; **jupyterlab | 3.2.1**; jupyterlab-pygments | 0.1.2; jupyterlab-server | 2.8.2; kiwisolver | 1.3.1; legacy-api-wrap | 1.2; leidenalg | 0.8.8; llvmlite | 0.36.0; MarkupSafe | 2.0.1; matplotlib | 3.3.4; mistune | 0.8.4; **natsort | 8.0.0**; nbclassic | 0.2.6; nbclient | 0.5.3; nbconvert | 6.0.7; nbformat | 5.1.3; nest-asyncio | 1.5.1; networkx | 2.5.1; notebook | 6.4.3; numba | 0.53.1; numexpr | 2.7.3; numpy | 1.19.5; packaging | 21; pandas | 1.1.5; pandocfilters | 1.4.3; parso | 0.8.2; patsy | 0.5.2; pickleshare | 0.7.5; Pillow | 8.4.0; pip | 21.2.2; prometheus-client | 0.11.0; prompt-toolkit | 3.0.20; pycparser | 2.2; Pygments | 2.10.0; pynndescent | 0.5.5; pyOpenSSL | 21.0.0; **pyparsing | 3.0.4**; pyrsistent | 0.17.3; PySocks | 1.7.1; python-dateutil | 2.8.2; python-igraph | 0.9.8; pytz | 2021.3; pywin32 | 228; pywinpty | 0.5.7; pyzmq | 22.2.1; requests | 2.26.0; scanpy | 1.7.2; scikit-learn | 0.24.2; scipy | 1.5.4; seaborn | 0.11.2; Send2Trash | 1.8.0; setuptools | 58.0.4; sinfo | 0.3.4; six | 1.16.0; sniffio | 1.2.0; statsmodels | 0.12.2; stdlib-list | 0.8.0; tables | 3.6.1; terminado | 0",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2046#issuecomment-963453699:2717,wrap,wrap,2717,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2046#issuecomment-963453699,2,['wrap'],['wrap']
Integrability,"args; 4346 ); 4347 . /usr/local/lib/python3.7/site-packages/pandas/core/generic.py in fillna(self, value, method, axis, inplace, limit, downcast); 6256 ; 6257 new_data = self._data.fillna(; -> 6258 value=value, limit=limit, inplace=inplace, downcast=downcast; 6259 ); 6260 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in fillna(self, **kwargs); 573 ; 574 def fillna(self, **kwargs):; --> 575 return self.apply(""fillna"", **kwargs); 576 ; 577 def downcast(self, **kwargs):. /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in apply(self, f, axes, filter, do_integrity_check, consolidate, **kwargs); 436 kwargs[k] = obj.reindex(b_items, axis=axis, copy=align_copy); 437 ; --> 438 applied = getattr(b, f)(**kwargs); 439 result_blocks = _extend_blocks(applied, result_blocks); 440 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/blocks.py in fillna(self, value, limit, inplace, downcast); 1934 def fillna(self, value, limit=None, inplace=False, downcast=None):; 1935 values = self.values if inplace else self.values.copy(); -> 1936 values = values.fillna(value=value, limit=limit); 1937 return [; 1938 self.make_block_same_class(. /usr/local/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs); 206 else:; 207 kwargs[new_arg_name] = new_arg_value; --> 208 return func(*args, **kwargs); 209 ; 210 return wrapper. /usr/local/lib/python3.7/site-packages/pandas/core/arrays/categorical.py in fillna(self, value, method, limit); 1871 elif is_hashable(value):; 1872 if not isna(value) and value not in self.categories:; -> 1873 raise ValueError(""fill value must be in categories""); 1874 ; 1875 mask = codes == -1. ValueError: fill value must be in categories; ```. That's because `colors = colors.fillna('white')` line in the seaborn code is trying to add a new category to a categorical variable, which is not allowed in pandas. I simply converted the color categorical variable to numpy array and added tests.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/809:3233,wrap,wrapper,3233,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809,2,['wrap'],['wrapper']
Integrability,"aslan. > I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". As a point about this, I don't think `raw` completley solves this problem. There's two reasons for this:. ### Only a different set of variables. Raw only differs from the main object by variables. But we just as often want to remove observations (doublet detection for example). To account for this, I think it makes sense to just have two different anndata objects. ### absolutely everything. I don't think we really can expect to have everything. There are always going to be analyses that require going back to the BAM. If ""single file"" is the issue, we could definitely allow something like:. ```python; with h5py.File(""analysis.h5"") as f:; processed = ad.read_h5ad(f[""processed""]); raw = ad.read_h5ad(f[""raw""]); ```. -----------------------------. @LuckyMD . > Integration works better with HVGs typically. I'm thinking of the case where I have a few datasets saved as `h5ad` that I want to integrate. What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset?. I feel like it's helpful to have the all the measured genes present, so that when you do gather your datasets together you can select features from the full set. > > This does run into memory usage problems if want do a densifying transform on the data; > Don't understand this entirely... I was thinking about what happens if you do something like `sc.pp.scale`, where you don't have any 0s in your expression matrix anymore, so it has to be stored as a dense matrix. I believe this is why `raw` was even introduced originally, since the normalization workflow then was feature selection -> scale. It was wasteful to store the entire set of var",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472:1047,integrat,integrate,1047,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472,2,['integrat'],['integrate']
Integrability,"ass_inst = _pass_registry.get(pss).pass_inst; [340](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler_machinery.py?line=339) if isinstance(pass_inst, CompilerPass):; --> [341](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler_machinery.py?line=340) self._runPass(idx, pass_inst, state); [342](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler_machinery.py?line=341) else:; [343](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler_machinery.py?line=342) raise BaseException(""Legacy pass in use""). File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\compiler_lock.py:35, in _CompilerLock.__call__.<locals>._acquire_compile_lock(*args, **kwargs); [32](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler_lock.py?line=31) @functools.wraps(func); [33](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler_lock.py?line=32) def _acquire_compile_lock(*args, **kwargs):; [34](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler_lock.py?line=33) with self:; ---> [35](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler_lock.py?line=34) return func(*args, **kwargs). File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\compiler_machinery.py:296, in PassManager._runPass(self, index, pss, internal_state); [294](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler_machinery.py?line=293) mutated |= check(pss.run_initialization, internal_state); [295](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler_machinery.py?line=294) with SimpleTimer() as pass_time:; --> [296](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler_machinery.py?line=295) mutated",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659:24165,wrap,wraps,24165,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659,2,['wrap'],['wraps']
Integrability,"ast NA; genericpath NA; google NA; gprofiler 1.0.0; h5py 3.3.0; idna 3.1; igraph 0.9.6; imagecodecs 2021.6.8; imageio 2.9.0; ipykernel 6.0.3; ipython_genutils 0.2.0; jedi 0.17.2; joblib 1.0.1; keras_preprocessing 1.1.2; kiwisolver 1.3.1; leidenalg 0.8.7; llvmlite 0.36.0; louvain 0.7.0; matplotlib 3.4.2; matplotlib_inline NA; mpl_toolkits NA; natsort 7.1.1; nbinom_ufunc NA; networkx 2.5; ntpath NA; numba 0.53.1; numexpr 2.7.3; numpy 1.21.1; opcode NA; openpyxl 3.0.7; opt_einsum v3.3.0; packaging 21.0; pandas 1.3.0; parso 0.7.0; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; pooch v1.4.0; posixpath NA; prompt_toolkit 3.0.19; psutil 5.8.0; ptyprocess 0.7.0; pycparser 2.20; pydoc_data NA; pyexpat NA; pygments 2.9.0; pynndescent 0.5.4; pyparsing 2.4.7; pytz 2021.1; requests 2.26.0; scanpy 1.8.1; scipy 1.7.0; seaborn 0.11.1; sinfo 0.3.1; sip NA; six 1.16.0; skimage 0.18.2; sklearn 0.24.2; socks 1.7.1; soupsieve 2.0.1; sphinxcontrib NA; spyder 5.0.5; spyder_kernels 2.0.5; spydercustomize NA; sre_compile NA; sre_constants NA; sre_parse NA; statsmodels 0.12.2; storemagic NA; tables 3.6.1; tensorboard 2.5.0; tensorflow 2.5.0; termcolor 1.1.0; texttable 1.6.4; tifffile 2021.7.2; tlz 0.11.0; toolz 0.11.1; tornado 6.1; tqdm 4.61.2; traitlets 5.0.5; typing_extensions NA; umap 0.5.1; urllib3 1.26.6; wcwidth 0.2.5; wrapt 1.12.1; wurlitzer 2.1.0; xlsxwriter 1.4.4; yaml 5.4.1; zmq 22.1.0; -----; IPython 7.25.0; jupyter_client 6.1.12; jupyter_core 4.7.1; -----; Python 3.9.6 | packaged by conda-forge | (default, Jul 11 2021, 03:39:48) [GCC 9.3.0]; Linux-5.4.0-72-generic-x86_64-with-glibc2.27; 40 logical CPU cores, x86_64; -----; Session information updated at 2021-07-29 21:02; </details>. PS - As a side note, another issue that pandas 1.3 introduced is that adata.write() no longer works unless I manually convert every column in adata.obs back to its original data type (e.g. adata.obs['leiden']=adata.obs['leiden'].astype('int')), because I get a TypeError from h5py until I do that.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1971:5685,wrap,wrapt,5685,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1971,1,['wrap'],['wrapt']
Integrability,"ate the AnnData object; adata = ad.AnnData(X=data, obs=obs, var=var). # Test layer call function; adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test; sc.pl.highest_expr_genes(adata, layer='normalised'); ```. ### Error output. ```pytb; Output exceeds the size limit. Open the full output data in a text editor; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); Cell In[32], line 17; 15 # Test layer call function; 16 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test; ---> 17 sc.pl.highest_expr_genes(adata, layer='normalised'); 19 # Test layer call function; 20 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\scanpy\plotting\_qc.py:100, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds); 98 height = (n_top * 0.2) + 1.5; 99 fig, ax = plt.subplots(figsize=(5, height)); --> 100 sns.boxplot(data=counts_top_genes, orient=""h"", ax=ax, fliersize=1, **kwds); 101 ax.set_xlabel(""% of total counts""); 102 if log:. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\seaborn\categorical.py:1634, in boxplot(data, x, y, hue, order, hue_order, orient, color, palette, saturation, fill, dodge, width, gap, whis, linecolor, linewidth, fliersize, hue_norm, native_scale, log_scale, formatter, legend, ax, **kwargs); ...; --> 700 artists = ax.bxp(**boxplot_k",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3318:1815,wrap,wraps,1815,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3318,1,['wrap'],['wraps']
Integrability,"ated with `'mean'` and `'std'` in `adata.var`.; """"""; return scale_array(X, *args, **kwargs). @scale.register(np.ndarray); def scale_array(; X,; zero_center: bool = True,; max_value: Optional[float] = None,; copy: bool = False,; return_mean_var=False,; ):; if copy:; X = X.copy(); if not zero_center and max_value is not None:; logg.info( # Be careful of what? This should be more specific; '... be careful when using `max_value` '; 'without `zero_center`.'; ); if max_value is not None:; logg.debug(f'... clipping at max_value {max_value}'); mean, std = _scale(X, zero_center) # the code from here could probably just be ; # do the clipping; if max_value is not None:; X[X > max_value] = max_value; if return_mean_var:; return X, mean, var; else:; return X. @scale.register(AnnData); def scale_anndata(; adata: AnnData,; *,; zero_center: bool = True,; max_value: Optional[float] = None,; copy: bool = False,; ) -> Optional[AnnData]:; adata = adata.copy() if copy else adata; view_to_actual(adata); adata.X, adata.var[""mean""], adata.var[""std""] = scale(; X, ; zero_center=zero_center, ; max_value=max_value, ; copy=False, # because a copy has already been made, if it were to be made; return_mean_var=True; ); if copy:; return adata. @scale.register(sparse.spmatrix); def scale_sparse(; X, ; *, ; zero_center: bool = True,; copy=False,; **kwargs; ):; # need to add the following here to make inplace logic work; if zero_center:; logg.info(; '... as `zero_center=True`, sparse input is '; 'densified and may lead to large memory consumption'; ); X = X.toarray(); copy = False # Since the data has been copied; return scale_array(X, zero_center=zero_center, copy=copy, **kwargs); ```. </details>. I actually really like this pattern of having an underlying function which has all the logic, but then dispatching through wrappers for the argument handling. It splits out the cases quite nicely, and makes the code flexible. This pattern is very common in Julia, and fairly common in Bioconductor packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1135#issuecomment-608200735:3462,wrap,wrappers,3462,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135#issuecomment-608200735,1,['wrap'],['wrappers']
Integrability,"atest/notebooks/scverse_data_backed.html. ![grafik](https://github.com/user-attachments/assets/0317c570-0af8-4c5e-8f3f-7831335763af). That was probably a mistake and the data just got loaded to memory, but since `dendrogram` can be reimplemented using `.get.aggregate`, we should do that!. ### Minimal code sample. ```python; import scanpy as sc. adata = sc.datasets.pbmc3k(); adata.filename = ""test.h5ad""; sc.pl.dotplot(adata, [""FCN1""], groupby=""index"", dendrogram=True); ```. ### Error output. ```pytb; ---------------------------------------------------------------------------; NotImplementedError Traceback (most recent call last); Cell In[44], line 1; ----> 1 sc.pl.dotplot(mdata[""rna""], var_names=[""CD2""], groupby=""leiden"", figsize=(10, 3), dendrogram=True, swap_axes=True). File ~/.local/share/hatch/env/virtual/scverse-tutorials/_YRPCeuX/basic-scrna/lib/python3.12/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File ~/.local/share/hatch/env/virtual/scverse-tutorials/_YRPCeuX/basic-scrna/lib/python3.12/site-packages/scanpy/plotting/_dotplot.py:1046, in dotplot(adata, var_names, groupby, use_raw, log, num_categories, expression_cutoff, mean_only_expressed, cmap, dot_max, dot_min, standard_scale, smallest_dot, title, colorbar_title, size_title, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, dot_color_df, show, save, ax, return_fig, vmin, vmax, vcenter, norm, **kwds); 1019 dp = DotPlot(; 1020 adata,; 1021 var_names,; (...); 1042 **kwds,; 1043 ); 1045 if dendrogram:; -> 1046 dp.add_dendrogram(dendrogram_key=dendrogram); 1047 if swap_axes:; 1048 dp.swap_axes(). File ~/.local/share/hatch/env/virtual/s",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3199:1471,wrap,wraps,1471,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3199,1,['wrap'],['wraps']
Integrability,"ath(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax); 1057 if as_heatmap:; 1058 img = ax.imshow(; -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map; 1060 ); 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs); 1599 def inner(ax, *args, data=None, **kwargs):; 1600 if data is None:; -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs); 1602 ; 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs); 367 f""%(removal)s. If any parameter follows {name!r}, they ""; 368 f""should be pass as keyword, not positionally.""); --> 369 return func(*args, **kwargs); 370 ; 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs); 367 f""%(removal)s. If any parameter follows {name!r}, they ""; 368 f""should be pass as keyword, not positionally.""); --> 369 return func(*args, **kwargs); 370 ; 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs); 5669 resample=resample, **kwargs); 5670 ; -> 5671 im.set_data(X); 5672 im.set_alpha(alpha); 5673 if im.get_clip_path() is None:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/image.py in set_data(self, A); 683 not np.can_cast(self._A.dtype, float, ""same_kind"")):; 684 raise TypeError(""Image data of dtype {} cannot be converted to ""; --> 685 ""float"".format(self._A.dtype)); 686 ; 687 if not (self._A.ndim == 2. TypeError: Image data of dtype object cannot be conv",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/953:4468,wrap,wrapper,4468,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953,1,['wrap'],['wrapper']
Integrability,"ath(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax); 1057 if as_heatmap:; 1058 img = ax.imshow(; -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map; 1060 ); 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs); 1599 def inner(ax, *args, data=None, **kwargs):; 1600 if data is None:; -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs); 1602 ; 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs); 367 f""%(removal)s. If any parameter follows {name!r}, they ""; 368 f""should be pass as keyword, not positionally.""); --> 369 return func(*args, **kwargs); 370 ; 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs); 367 f""%(removal)s. If any parameter follows {name!r}, they ""; 368 f""should be pass as keyword, not positionally.""); --> 369 return func(*args, **kwargs); 370 ; 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs); 5669 resample=resample, **kwargs); 5670 ; -> 5671 im.set_data(X); 5672 im.set_alpha(alpha); 5673 if im.get_clip_path() is None:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/image.py in set_data(self, A); 688 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):; 689 raise TypeError(""Invalid shape {} for image data""; --> 690 .format(self._A.shape)); 691 ; 692 if self._A.ndim == 3:. TypeError: Invalid shape (3, 43, 1) for image data; ```; If I convert the `a",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/953:1953,wrap,wrapper,1953,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953,1,['wrap'],['wrapper']
Integrability,"atterplots.umap(all_data_flt_clst, color=markers, cmap=""Blues"", ncols=5); 27 ; 28 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/scatterplots.py in umap(adata, **kwargs); 27 If `show==False` a `matplotlib.Axis` or a list of it.; 28 """"""; ---> 29 return plot_scatter(adata, basis='umap', **kwargs); 30 ; 31 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/scatterplots.py in plot_scatter(adata, color, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, basis, groups, components, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs); 280 if sort_order is True and value_to_plot is not None and categorical is False:; 281 order = np.argsort(color_vector); --> 282 color_vector = color_vector[order]; 283 _data_points = data_points[component_idx][order, :]; 284 . h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/h5py/_hl/dataset.py in __getitem__(self, args); 474 ; 475 # Perform the dataspace selection.; --> 476 selection = sel.select(self.shape, args, dsid=self.id); 477 ; 478 if selection.nselect == 0:. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/h5py/_hl/selections.py in select(shape, args, dsid); 70 elif isinstance(arg, np.ndarray):; 71 sel = PointSelection(shape); ---> 72 sel[arg]; 73 return sel; 74 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/h5py/_hl/selections.py in __getitem__(self, arg); 210 """""" Perform point-wise selection from a NumPy boolean array """"""; 211 if not (isinstance(arg, np.ndarray) and arg.dtype.kind == 'b'):; --> 212 raise TypeError(""PointSelection __getitem__ only works with bool arrays""); 213 if not arg.shape == self.shape:; 214 raise TypeError(""Boolean indexing array has incompatible shape""). TypeError: PointSelection __getitem__ only work",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/440:1772,wrap,wrapper,1772,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/440,1,['wrap'],['wrapper']
Integrability,"ave multiple weighting options that depend on the `openTSNE` package. If it turns out these methods don't have much in the way of parameters, then it might be reasonable for this to be a part of `sc.pp.neighbors`. How about this, the implementation here should be well factored out into:. 1. Getting nearest neighbors; 2. Weighting the graph; 3. Computing the layout. Once the available parameters are clear I think it'll be easier to make an informed decision about whether neighbor weighting for tsne should occur through `sc.pp.neighbors`. Additionally, I think it'll be easier to integrate cleanly separated code than to separate integrated code. > The weights constructed by UMAP in neighbors are not normalized. So if you run neighbors() and then tsne() then t-SNE should do something in order to be able to use this graph. For passing the umap connectivity matrix to tsne layout, I think I would expect the weights to be used. Something like this should accomplish that:. ```python; class WrappedAffinities(openTSNE.affinity.Affinities):; def __init__(self, neighbors, symmetrize=True, verbose=False):; self.verbose = verbose; P = neighbors; if symmetrize:; P = (P + P.T) / 2; total = P.sum(); if not np.isclose(total, 1.):; P = P / total; self.P = P; ```. That said, I'm not too familiar with the assumptions of tsne, or if this would be appropriate. I think binarizing the edge weights is a bit of a strong assumption unless specifically requested though. With `umap`, we throw a warning if it looks like the passed graph didn't come from `umap`. You could do the same here?. > From an implementation standpoint, the sc.pp.tsne_negihbors will inevitably have to call the UMAP KNNG construction, since I can see that it's not split out in the code-base. I would like nearest neighbor calculation and graph weighting to be split out eventually. Since it's already done this way in `openTSNE`, I think this could help with that goal. > From what I can tell, the standard way of weighing the KNNG",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-761950200:1125,Wrap,WrappedAffinities,1125,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-761950200,1,['Wrap'],['WrappedAffinities']
Integrability,"b/python3.6/site-packages/scanpy/preprocessing/_qc.py"", line 399:; def top_segment_proportions_sparse_csr(data, indptr, ns):; <source elided>; start, end = indptr[i], indptr[i + 1]; sums[i] = np.sum(data[start:end]); ^. [1] During: lowering ""id=13[LoopNest(index_variable = parfor_index.271, range = (0, $100.6, 1))]{386: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (403)>, 388: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (404)>, 264: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (401)>, 306: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (402)>, 118: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (397)>}Var(parfor_index.271, _qc.py:397)"" at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (397). This is not usually a problem with Numba itself but instead often caused by; the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:; http://numba.pydata.org/numba-doc/latest/reference/pysupported.html; and; http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:; http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message; and traceback, along with a minimal reproducer at:; https://github.com/numba/numba/issues/new. ```; numba is 0.47.0 but 0.43.1 gave the same error.; It seems that ```top_segment_proportions_sparse_csr``` is new for scanpy 1.4.5. Please help. Thank you very much.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/978:5115,message,message,5115,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978,1,['message'],['message']
Integrability,"b4dc6fd3d98411eb3ba53823c4f42 NA; absl NA; astor 0.8.1; astunparse 1.6.3; bottleneck 1.3.4; cached_property 1.5.2; certifi 2021.10.08; cffi 1.15.0; chardet 3.0.4; cloudpickle 1.3.0; cvxopt 1.2.7; cycler 0.10.0; cython_runtime NA; dask 2.12.0; dateutil 2.8.2; debugpy 1.0.0; decorator 4.4.2; dill 0.3.4; flatbuffers 2.0; gast 0.5.3; google NA; google_auth_httplib2 NA; googleapiclient NA; h5py 3.1.0; httplib2 0.17.4; idna 2.10; igraph 0.9.9; ipykernel 4.10.1; ipython_genutils 0.2.0; ipywidgets 7.7.0; jax 0.3.4; jaxlib 0.3.2; joblib 1.1.0; keras 2.8.0; keras_preprocessing 1.1.2; kiwisolver 1.4.0; leidenalg 0.8.9; llvmlite 0.34.0; matplotlib 3.2.2; mpl_toolkits NA; natsort 5.5.0; numba 0.51.2; numexpr 2.8.1; numpy 1.21.5; oauth2client 4.1.3; opt_einsum v3.3.0; packaging 21.3; pandas 1.3.5; patsy 0.5.2; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; portpicker NA; prompt_toolkit 1.0.18; psutil 5.4.8; ptyprocess 0.7.0; pyarrow 6.0.1; pyasn1 0.4.8; pyasn1_modules 0.2.8; pycparser 2.21; pydev_ipython NA; pydevconsole NA; pydevd 2.0.0; pydevd_concurrency_analyser NA; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pydot_ng 2.0.0; pygments 2.6.1; pynndescent 0.5.6; pyparsing 3.0.7; pytz 2018.9; requests 2.23.0; rsa 4.8; scipy 1.4.1; seaborn 0.11.2; session_info 1.0.0; simplegeneric NA; sitecustomize NA; six 1.15.0; sklearn 1.0.2; socks 1.7.1; sphinxcontrib NA; statsmodels 0.10.2; storemagic NA; tblib 1.7.0; tensorboard 2.8.0; tensorflow 2.8.0; tensorflow_probability 0.16.0; termcolor 1.1.0; texttable 1.6.4; threadpoolctl 3.1.0; toolz 0.11.2; tornado 5.1.1; tqdm 4.63.0; traitlets 5.1.1; tree 0.1.6; typing_extensions NA; umap 0.5.2; uritemplate 3.0.1; urllib3 1.24.3; wcwidth 0.2.5; wrapt 1.14.0; yaml 3.13; zipp NA; zmq 22.3.0. IPython 5.5.0; jupyter_client 5.3.5; jupyter_core 4.9.2; notebook 5.3.1. Python 3.7.13 (default, Mar 16 2022, 17:37:17) [GCC 7.5.0]; Linux-5.4.144+-x86_64-with-Ubuntu-18.04-bionic. Session information updated at 2022-04-04 17:56. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2208:5369,wrap,wrapt,5369,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2208,1,['wrap'],['wrapt']
Integrability,"batch_queries, verbose); 802 self._angular_trees,; 803 ); --> 804 leaf_array = rptree_leaf_array(self._rp_forest); 805 else:; 806 self._rp_forest = None. ~/.local/lib/python3.8/site-packages/pynndescent/rp_trees.py in rptree_leaf_array(rp_forest); 1095 def rptree_leaf_array(rp_forest):; 1096 if len(rp_forest) > 0:; -> 1097 return np.vstack(rptree_leaf_array_parallel(rp_forest)); 1098 else:; 1099 return np.array([[-1]]). ~/.local/lib/python3.8/site-packages/pynndescent/rp_trees.py in rptree_leaf_array_parallel(rp_forest); 1087 ; 1088 def rptree_leaf_array_parallel(rp_forest):; -> 1089 result = joblib.Parallel(n_jobs=-1, require=""sharedmem"")(; 1090 joblib.delayed(get_leaves_from_tree)(rp_tree) for rp_tree in rp_forest; 1091 ). /omics/groups/OE0540/internal/B260/users/olga/.conda/envs/10x_analysis/lib/python3.8/site-packages/joblib/parallel.py in __call__(self, iterable); 1054 ; 1055 with self._backend.retrieval_context():; -> 1056 self.retrieve(); 1057 # Make sure that we get a last message telling us we are done; 1058 elapsed_time = time.time() - self._start_time. /omics/groups/OE0540/internal/B260/users/olga/.conda/envs/10x_analysis/lib/python3.8/site-packages/joblib/parallel.py in retrieve(self); 933 try:; 934 if getattr(self._backend, 'supports_timeout', False):; --> 935 self._output.extend(job.get(timeout=self.timeout)); 936 else:; 937 self._output.extend(job.get()). /omics/groups/OE0540/internal/B260/users/olga/.conda/envs/10x_analysis/lib/python3.8/multiprocessing/pool.py in get(self, timeout); 769 return self._value; 770 else:; --> 771 raise self._value; 772 ; 773 def _set(self, i, obj):. /omics/groups/OE0540/internal/B260/users/olga/.conda/envs/10x_analysis/lib/python3.8/multiprocessing/pool.py in worker(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception); 123 job, i, func, args, kwds = task; 124 try:; --> 125 result = (True, func(*args, **kwds)); 126 except Exception as e:; 127 if wrap_exception and func is not _helper_reraises_exception:. /om",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2472:3905,message,message,3905,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2472,1,['message'],['message']
Integrability,bbknn integrates multiple variables,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2004:6,integrat,integrates,6,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2004,1,['integrat'],['integrates']
Integrability,bbknn wrapper needs an update,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/635:6,wrap,wrapper,6,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/635,1,['wrap'],['wrapper']
Integrability,"been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python; import scanpy; scanpy.logging.print_versions(); ```. ### Error output. ```pytb; ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); Cell In[44], line 1; ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file); 178 print_versions(); 179 else:; --> 180 session_info.show(; 181 dependencies=True,; 182 html=False,; 183 excludes=[; 184 'builtins',; 185 'stdlib_list',; 186 'importlib_metadata',; 187 # Special module present if test coverage being calculated; 188 # https://gitlab.com/joelostblom/session_info/-/issues/10; 189 ""$coverage"",; 190 ],; 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes); 207 for mod_name in clean_modules:; 208 mod_names.append(mod_name); --> 209 mod = sys.modules[mod_name]; 210 # Since modules use different attribute names to store version info,; 211 # try the most common ones.; 212 try:. KeyError: 'numcodecs'; ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes several versions in the output. ```; scanpy==1.10.0.dev88+gedd61302 anndata==0.9.2 umap==0.5.3 numpy==1.24.4 scipy==1.11.1 pandas==2.0.3 scikit-learn==1.3.0 ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2580:1005,depend,dependencies,1005,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580,1,['depend'],['dependencies']
Integrability,"brief recap: https://github.com/theislab/scanpy/pull/130 was the initial work on integrating RNA velocity into scanpy, which was a slimmed version of velocyto; yet not working well due to its simplification and several missing required processing steps. Consequently, and with the additional objective of extending velocyto, we outsourced that to scvelo. For directed paga this is already adjusted. I think we missed https://github.com/theislab/scanpy/blob/740c4a510ec598ab03ff3de1d9b1c091f0aac292/scanpy/plotting/_utils.py#L334; the convention became `'velocity_' + basis ` (instead of `'Delta_' + basis `). This is used only for scatter plots, if I get it correctly. The velocity plotting modules within scvelo have been extensively optimized, thus questionable whether still needed within scanpy. Anything else I am missing?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/792#issuecomment-523824420:81,integrat,integrating,81,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/792#issuecomment-523824420,2,['integrat'],['integrating']
Integrability,"bxau=1.0.11; - xorg-libxdmcp=1.1.3; - xz=5.2.6; - yaml=0.2.5; - zeromq=4.3.4; - zipp=3.16.2; - zlib=1.2.13; - zlib-ng=2.0.7; - zstd=1.5.2; - pip:; - absl-py==1.4.0; - astunparse==1.6.3; - bcbio-gff==0.7.0; - biopython==1.81; - cachetools==5.3.1; - click==8.1.7; - flatbuffers==23.5.26; - gast==0.4.0; - geoparse==2.0.3; - gffpandas==1.2.0; - google-auth==2.22.0; - google-auth-oauthlib==1.0.0; - google-pasta==0.2.0; - grpcio==1.57.0; - imageio==2.34.1; - keras==2.13.1; - lazy-loader==0.4; - libclang==16.0.6; - louvain==0.8.2; - markdown==3.4.4; - numpy==1.24.3; - oauthlib==3.2.2; - opt-einsum==3.3.0; - protobuf==4.24.1; - pyasn1==0.5.0; - pyasn1-modules==0.3.0; - requests-oauthlib==1.3.1; - rsa==4.9; - scikit-image==0.24.0; - tensorboard==2.13.0; - tensorboard-data-server==0.7.1; - tensorflow==2.13.0; - tensorflow-estimator==2.13.0; - tensorflow-macos==2.13.0; - termcolor==2.3.0; - tifffile==2024.6.18; - tqdm==4.66.1; - typing-extensions==4.5.0; - urllib3==1.26.16; - werkzeug==2.3.7; - wrapt==1.15.0; ```. ### Minimal code sample. ```python; sc.pp.scrublet(adata); ```. ### Error output. _No response_. ### Versions. <details>. ```; # Successful case; -----; anndata 0.10.5.post1; scanpy 1.10.1; -----; PIL 9.4.0; astunparse 1.6.3; cffi 1.15.1; colorama 0.4.6; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; defusedxml 0.7.1; dill 0.3.7; gmpy2 2.1.2; google NA; h5py 3.9.0; igraph 0.11.3; joblib 1.3.2; kiwisolver 1.4.4; legacy_api_wrap NA; leidenalg 0.10.2; llvmlite 0.40.1; louvain 0.8.2; matplotlib 3.7.2; mpl_toolkits NA; mpmath 1.3.0; natsort 8.4.0; numba 0.57.1; numexpr 2.8.4; numpy 1.24.4; opt_einsum v3.3.0; packaging 23.1; pandas 2.0.3; pkg_resources NA; plotly 5.16.1; psutil 5.9.5; pyparsing 3.0.9; pytz 2023.3; scipy 1.11.2; session_info 1.0.0; six 1.16.0; sklearn 1.3.0; sympy 1.12; texttable 1.7.0; threadpoolctl 3.2.0; torch 2.0.1; tqdm 4.66.2; typing_extensions NA; wcwidth 0.2.6; yaml 6.0.1; -----; Python 3.11.4 | packaged by conda-forge | (main, Jun 10 2023, 18:08:",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3116:15465,wrap,wrapt,15465,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3116,1,['wrap'],['wrapt']
Integrability,"c(elem, key, val, *args, **kwargs); 210 except Exception as e:. ~/anaconda3/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_series(group, key, series, dataset_kwargs); 269 if series.dtype == object: # Assuming it’s string; --> 270 group.create_dataset(; 271 key,. ~/anaconda3/lib/python3.8/site-packages/h5py/_hl/group.py in create_dataset(self, name, shape, dtype, data, **kwds); 147 ; --> 148 dsid = dataset.make_new_dset(group, shape, dtype, data, name, **kwds); 149 dset = dataset.Dataset(dsid). ~/anaconda3/lib/python3.8/site-packages/h5py/_hl/dataset.py in make_new_dset(parent, shape, dtype, data, name, chunks, compression, shuffle, fletcher32, maxshape, compression_opts, fillvalue, scaleoffset, track_times, external, track_order, dcpl, allow_unknown_filter); 139 if (data is not None) and (not isinstance(data, Empty)):; --> 140 dset_id.write(h5s.ALL, h5s.ALL, data); 141 . h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/h5d.pyx in h5py.h5d.DatasetID.write(). h5py/_proxy.pyx in h5py._proxy.dset_rw(). h5py/_conv.pyx in h5py._conv.str2vlen(). h5py/_conv.pyx in h5py._conv.generic_converter(). h5py/_conv.pyx in h5py._conv.conv_str2vlen(). TypeError: Can't implicitly convert non-string objects to strings. The above exception was the direct cause of the following exception:. TypeError Traceback (most recent call last); ~/anaconda3/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, key, val, *args, **kwargs); 208 try:; --> 209 return func(elem, key, val, *args, **kwargs); 210 except Exception as e:. ~/anaconda3/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_dataframe(f, key, df, dataset_kwargs); 262 for col_name, (_, series) in zip(col_names, df.items()):; --> 263 write_series(group, col_name, series, dataset_kwargs=dataset_kwargs); 264 . ~/anaconda3/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, key, val, *args, **kwargs); 211 parent = _get_par",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1866:2725,wrap,wrapper,2725,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1866,1,['wrap'],['wrapper']
Integrability,c3986-validator 0.1.1; rich 13.6.0; rope 1.10.0; rpds-py 0.10.4; Rtree 1.0.1; ruamel.yaml 0.17.35; ruamel.yaml.clib 0.2.7; ruamel-yaml-conda 0.15.80; s3fs 0.5.1; sacremoses 0.0.53; safetensors 0.3.3; scanpy 1.9.5; scikit-image 0.21.0; scikit-learn 1.3.1; scikit-learn-intelex 20230725.122106; scipy 1.11.3; Scrapy 2.11.0; scrublet 0.2.3; scTE 1.0; scTE 1.0; seaborn 0.13.0; SecretStorage 3.3.3; semver 3.0.1; Send2Trash 1.8.2; service-identity 18.1.0; session-info 1.0.0; setuptools 68.0.0; sip 6.6.2; six 1.16.0; smart-open 6.4.0; smmap 5.0.0; snakemake 7.32.3; sniffio 1.3.0; snowballstemmer 2.2.0; sortedcontainers 2.4.0; soupsieve 2.5; Sphinx 7.2.6; sphinxcontrib-applehelp 1.0.7; sphinxcontrib-devhelp 1.0.5; sphinxcontrib-htmlhelp 2.0.4; sphinxcontrib-jsmath 1.0.1; sphinxcontrib-qthelp 1.0.6; sphinxcontrib-serializinghtml 1.1.9; spyder 5.4.3; spyder-kernels 2.4.4; SQLAlchemy 2.0.21; stack-data 0.6.2; statsmodels 0.14.0; stdlib-list 0.8.0; stopit 1.1.2; sympy 1.12; tables 3.9.1; tabulate 0.9.0; TBB 0.2; tblib 2.0.0; tenacity 8.2.3; terminado 0.17.1; text-unidecode 1.3; textdistance 4.5.0; texttable 1.7.0; threadpoolctl 3.2.0; three-merge 0.1.1; throttler 1.2.2; tifffile 2023.4.12; tinycss2 1.2.1; tldextract 3.6.0; tokenizers 0.14.0; toml 0.10.2; tomli 2.0.1; tomlkit 0.12.1; toolz 0.12.0; toposort 1.10; tornado 6.3.3; tqdm 4.66.1; traitlets 5.11.2; transformers 4.34.0; truststore 0.8.0; Twisted 22.10.0; types-python-dateutil 2.8.19.14; typing_extensions 4.8.0; typing-utils 0.1.0; tzdata 2023.3; uc-micro-py 1.0.1; ujson 5.8.0; umap-learn 0.5.4; uri-template 1.3.0; urllib3 1.26.15; virtualenv 20.24.5; w3lib 2.1.2; watchdog 3.0.0; wcwidth 0.2.8; webcolors 1.13; webencodings 0.5.1; websocket-client 1.6.4; Werkzeug 3.0.0; whatthepatch 1.0.5; wheel 0.38.4; widgetsnbextension 4.0.9; wrapt 1.15.0; wurlitzer 3.0.3; xarray 2023.9.0; xxhash 3.4.1; xyzservices 2023.10.0; yapf 0.24.0; yarl 1.9.2; yte 1.5.1; zict 3.0.0; zipp 3.17.0; zope.interface 6.1; zstandard 0.21.0. ```. </details>,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2680:10579,wrap,wrapt,10579,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2680,2,"['interface', 'wrap']","['interface', 'wrapt']"
Integrability,"canpy 1.7.0; sinfo 0.3.1; -----; PIL 7.2.0; anndata 0.7.5; appdirs 1.4.4; attr 20.1.0; autoreload NA; backcall 0.2.0; bioservices 1.7.8; bs4 4.9.1; cairo 1.19.1; certifi 2020.12.05; cffi 1.14.4; chardet 3.0.4; cloudpickle 1.6.0; colorama 0.4.3; colorlog NA; cupy 7.8.0; cupyx NA; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2021.03.1; dateutil 2.8.1; decorator 4.4.2; deprecated 1.2.10; easydev 0.9.38; fa2 NA; fastrlock 0.5; fsspec 0.8.7; future 0.18.2; future_fstrings NA; get_version 2.1; graphtools 1.5.2; gseapy 0.10.1; h5py 2.10.0; idna 2.10; igraph 0.8.2; iniconfig NA; ipykernel 5.3.4; ipython_genutils 0.2.0; ipywidgets 7.5.1; jedi 0.17.2; joblib 0.16.0; kiwisolver 1.2.0; legacy_api_wrap 1.2; leidenalg 0.8.1; llvmlite 0.34.0; louvain 0.7.0; lxml 4.5.2; magic 2.0.3; matplotlib 3.3.1; mkl 2.3.0; mpl_toolkits NA; natsort 7.1.1; numba 0.51.2; numexpr 2.7.1; numpy 1.19.1; packaging 20.8; pandas 1.2.1; parso 0.7.1; pexpect 4.8.0; phenograph 1.5.7; pickleshare 0.7.5; pkg_resources NA; pluggy 0.13.1; prompt_toolkit 3.0.6; psutil 5.7.2; ptyprocess 0.6.0; py 1.9.0; pyarrow 0.16.0; pycparser 2.20; pygments 2.6.1; pygsp 0.5.1; pylab NA; pyparsing 2.4.7; pytest 6.1.2; pytz 2020.1; requests 2.24.0; requests_cache 0.5.2; sca NA; scanpy 1.7.0; scipy 1.6.1; scprep 1.0.5.post2; seaborn 0.11.1; setuptools_scm NA; sinfo 0.3.1; six 1.15.0; sklearn 0.23.2; skmisc 0.1.3; soupsieve 2.0.1; statsmodels 0.11.1; storemagic NA; tables 3.6.1; tasklogger 1.0.0; tblib 1.7.0; texttable 1.6.2; threadpoolctl 2.1.0; tlz 0.11.0; toolz 0.11.1; tornado 6.0.4; tqdm 4.48.2; traitlets 4.3.3; typing_extensions NA; umap 0.4.6; urllib3 1.25.10; wcwidth 0.2.5; wrapt 1.12.1; yaml 5.4.1; zmq 19.0.2; zope NA; -----; IPython 7.17.0; jupyter_client 6.1.6; jupyter_core 4.6.3; notebook 6.1.3; -----; Python 3.8.2 (default, May 7 2020, 20:00:49) [GCC 7.3.0]; Linux-3.10.0-957.12.2.el7.x86_64-x86_64-with-glibc2.10; 64 logical CPU cores, x86_64; -----; Session information updated at 2021-05-03 16:30; </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1827:2835,wrap,wrapt,2835,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1827,1,['wrap'],['wrapt']
Integrability,"closed with #1472 , next time @danielStrobl you could just do `git commit -m ""<commit-message> #1471""` and it would be automatically linked (and then closed if PR is merged)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1471#issuecomment-776889361:86,message,message,86,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1471#issuecomment-776889361,1,['message'],['message']
Integrability,"confirmed this bug exists on the main branch of scanpy. ### What happened?. `sc.tl.dpt` was successfully done. But when I want to plot the result of dpt,the error comes out. ### Minimal code sample. ```python; sc.tl.dpt(a1,n_branchings=2); sc.pl.dpt_groups_pseudotime(a1); sc.pl.dpt_timeseries(a1); ```. ### Error output. Error in dpt_timeseries:. ```pytb; WARNING: Plotting more than 100 genes might take some while, consider selecting only highly variable genes, for example.; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); TypeError: float() argument must be a string or a real number, not 'csr_matrix'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last); Cell In[85], line 1; ----> 1 sc.pl.dpt_timeseries(a1). File D:\anaconda\Lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File D:\anaconda\Lib\site-packages\scanpy\plotting\_tools\__init__.py:245, in dpt_timeseries(adata, color_map, show, save, as_heatmap, marker); 242 # only if number of genes is not too high; 243 if as_heatmap:; 244 # plot time series as heatmap, as in Haghverdi et al. (2016), Fig. 1d; --> 245 timeseries_as_heatmap(; 246 adata.X[adata.obs[""dpt_order_indices""].values],; 247 var_names=adata.var_names,; 248 highlights_x=adata.uns[""dpt_changepoints""],; 249 color_map=color_map,; 250 ); 251 else:; 252 # plot time series as gene expression vs time; 253 timeseries(; 254 adata.X[adata.obs[""dpt_order_indices""].values],; 255 var_names=adata.var_names,; (...); 258 marker=marker,; 259 ). File D:\anaconda\Lib\site-packages\scanpy\plotting\_utils.py:227, in timeserie",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3086:1209,wrap,wraps,1209,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3086,1,['wrap'],['wraps']
Integrability,"cs.com/samples/cell-exp/6.0.0/SC3_v3_NextGem_DI_Nuclei_5K_Multiplex/SC3_v3_NextGem_DI_Nuclei_5K_Multiplex_count_raw_molecule_info.h5. https://www.10xgenomics.com/resources/datasets/5-k-mouse-e-18-combined-cortex-hippocampus-and-subventricular-zone-nuclei-3-1-standard-6-0-0 . Using this function, . ```; scanpy.read_10x_h5(filename, genome=None, gex_only=True, backup_url=None); ```. I get the following error, ; ```; ValueError: 'SC3_v3_NextGem_DI_Nuclei_5K_Multiplex_count_raw_molecule_info.h5' contains more than one genome. For legacy 10x h5 files you must specify the genome if more than one is present. Available genomes are: ['barcode_idx', 'barcode_info', 'barcodes', 'count', 'feature_idx', 'features', 'gem_group', 'library_idx', 'library_info', 'metrics_json', 'umi', 'umi_type']; ```; and I don't know how to fix this. The dataset is only from mouse. This is not a legacy h5 file. The data are output from CR v6.0.0. Can you please assist? . The ""available genomes"" suggested in the error message are not genome, but columns in the h5. Ideally, I want to import all of these. Are we really only allowed to select 1? Even selecting one, does not seem fix the problem. . ```; h5_info = scanpy.read_10x_h5(molecule_info_file,genome='umi_type'); ```; ```; TypeError: node ``/umi_type`` is not a group; ```. - [X ] I have checked that this issue has not already been reported.; - [X ] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python; h5_info = scanpy.read_10x_h5(molecule_info_file, backup_url=""https://cf.10xgenomics.com/samples/cell-exp/6.0.0/SC3_v3_NextGem_DI_Nuclei_5K_Multiplex/SC3_v3_NextGem_DI_Nuclei_5K_Multiplex_count_raw_molecule_info.h5""); ```. ```pytb; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2149:1138,message,message,1138,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2149,1,['message'],['message']
Integrability,"ct, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` that returns a `RankGenesGroups` object, which then has `.to_df()` function (e.g. the function `rank_genes_groups` from (https://github.com/theislab/scanpy/pull/619) could immediately go into that namespace. Maybe we can even borrow a `diffxpy` object for that. The good thing is, we can keep the current rec arrays as they are very efficient and basic data types, which will work with hdf5 and zarr and xarray and everything else that might come in the future. And: Fidel wrote a ton of plotting functions around them already, which we don't want to simply rewrite... We don't have to as users won't see the recarrays anymore... Other possible names for the API would be `sc.cast` or `sc.object` (`sc.ob`), less conflicting with `sc.external`. I think `sc.ob` makes sense as it really makes clear that Scanpy's main API is for writing convenient scripts for compute-heavy stuff in a functional way. If one wants to transition to more light-weight ""post-analysis"", one can transition to objects that are designed for specific tasks. PS: I'd love to move away from the name `rank_genes_groups` at some point, and simply have something like `difftest` or `DiffTest`... I always thought that we might have differential expression tests for longitudinal data at some point (like Monocle), otherwise the function would be `rank_genes` but I don't think this is gonna happen soon, and if, it will be in the `external` API... A minimal difftest API should though continue be in the core of Scanpy, with at its heart, a scalable Wilcoxon rank (much more scalable than scipy's or diffxpy's), the t test and the scikit learn logreg approach. `diffxpy` with it's tensorflow dependency can then handle very complex cases...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/562#issuecomment-487409358:4969,depend,dependency,4969,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562#issuecomment-487409358,2,['depend'],['dependency']
Integrability,"d mode: https://scverse-tutorials.readthedocs.io/en/latest/notebooks/scverse_data_backed.html. ![grafik](https://github.com/user-attachments/assets/0317c570-0af8-4c5e-8f3f-7831335763af). That was probably a mistake and the data just got loaded to memory, but since `dendrogram` can be reimplemented using `.get.aggregate`, we should do that!. ### Minimal code sample. ```python; import scanpy as sc. adata = sc.datasets.pbmc3k(); adata.filename = ""test.h5ad""; sc.pl.dotplot(adata, [""FCN1""], groupby=""index"", dendrogram=True); ```. ### Error output. ```pytb; ---------------------------------------------------------------------------; NotImplementedError Traceback (most recent call last); Cell In[44], line 1; ----> 1 sc.pl.dotplot(mdata[""rna""], var_names=[""CD2""], groupby=""leiden"", figsize=(10, 3), dendrogram=True, swap_axes=True). File ~/.local/share/hatch/env/virtual/scverse-tutorials/_YRPCeuX/basic-scrna/lib/python3.12/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File ~/.local/share/hatch/env/virtual/scverse-tutorials/_YRPCeuX/basic-scrna/lib/python3.12/site-packages/scanpy/plotting/_dotplot.py:1046, in dotplot(adata, var_names, groupby, use_raw, log, num_categories, expression_cutoff, mean_only_expressed, cmap, dot_max, dot_min, standard_scale, smallest_dot, title, colorbar_title, size_title, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, dot_color_df, show, save, ax, return_fig, vmin, vmax, vcenter, norm, **kwds); 1019 dp = DotPlot(; 1020 adata,; 1021 var_names,; (...); 1042 **kwds,; 1043 ); 1045 if dendrogram:; -> 1046 dp.add_dendrogram(dendrogram_key=dendrogram); 1047 if swap_axes:; 1048 d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3199:1418,wrap,wrapper,1418,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3199,1,['wrap'],['wrapper']
Integrability,diffxpy integration,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1955:8,integrat,integration,8,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1955,1,['integrat'],['integration']
Integrability,"ding to a regressed gene column). ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\scanpy\preprocessing\simple.py in _regress_out_chunk(data); 798 ; 799 responses_chunk_list = []; --> 800 import statsmodels.api as sm; 801 from statsmodels.tools.sm_exceptions import PerfectSeparationError; 802 . ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\statsmodels\api.py in <module>(); 3 from . import tools; 4 from .tools.tools import add_constant, categorical; ----> 5 from . import regression; 6 from .regression.linear_model import OLS, GLS, WLS, GLSAR; 7 from .regression.recursive_ls import RecursiveLS. ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\statsmodels\regression\__init__.py in <module>(); ----> 1 from .linear_model import yule_walker; 2 ; 3 from statsmodels import PytestTester; 4 test = PytestTester(); 5 . ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\statsmodels\regression\linear_model.py in <module>(); 46 cache_readonly,; 47 cache_writable); ---> 48 import statsmodels.base.model as base; 49 import statsmodels.base.wrapper as wrap; 50 from statsmodels.emplike.elregress import _ELRegOpts. ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\statsmodels\base\model.py in <module>(); 13 from statsmodels.tools.sm_exceptions import ValueWarning, \; 14 HessianInversionWarning; ---> 15 from statsmodels.formula import handle_formula_data; 16 from statsmodels.compat.numpy import np_matrix_rank; 17 from statsmodels.base.optimizer import Optimizer. ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\statsmodels\formula\__init__.py in <module>(); 3 ; 4 ; ----> 5 from .formulatools import handle_formula_data. ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\statsmodels\formula\formulatools.py in <module>(); 1 from statsmodels.compat.python import iterkeys; 2 import statsmodels.tools.data as data_util; ----> 3 from patsy import dmatrices, NAAction; 4 import numpy as np; 5 . ModuleNotFoundError: No module named 'patsy'",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/212:3208,wrap,wrapper,3208,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/212,2,['wrap'],"['wrap', 'wrapper']"
Integrability,"ds. /usr/local/lib/python3.8/site-packages/pandas/core/internals/construction.py in init_dict(data, index, columns, dtype); 237 else:; 238 nan_dtype = dtype; --> 239 val = construct_1d_arraylike_from_scalar(np.nan, len(index), nan_dtype); 240 arrays.loc[missing] = [val] * missing.sum(); 241 . /usr/local/lib/python3.8/site-packages/pandas/core/dtypes/cast.py in construct_1d_arraylike_from_scalar(value, length, dtype); 1438 else:; 1439 if not isinstance(dtype, (np.dtype, type(np.dtype))):; -> 1440 dtype = dtype.dtype; 1441 ; 1442 if length and is_integer_dtype(dtype) and isna(value):. AttributeError: type object 'object' has no attribute 'dtype'; ```. #### Versions. <details>. WARNING: If you miss a compact list, please try `print_header`!; The `sinfo` package has changed name and is now called `session_info` to become more discoverable and self-explanatory. The `sinfo` PyPI package will be kept around to avoid breaking old installs and you can downgrade to 0.3.2 if you want to use it without seeing this message. For the latest features and bug fixes, please install `session_info` instead. The usage and defaults also changed slightly, so please review the latest README at https://gitlab.com/joelostblom/session_info.; -----; anndata 0.7.4; scanpy 1.6.0; sinfo 0.3.4; -----; MulticoreTSNE NA; PIL 8.0.1; appnope 0.1.2; attr 20.3.0; backcall 0.2.0; cffi 1.14.4; cloudpickle 2.0.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.2; dask 2022.01.0; dateutil 2.8.1; decorator 4.4.2; dunamai 1.7.0; fsspec 2022.01.0; get_version 3.5.3; google NA; h5py 2.10.0; idna 2.10; igraph 0.9.6; ipykernel 5.3.4; ipython_genutils 0.2.0; ipywidgets 7.6.5; jedi 0.17.2; jinja2 2.11.2; joblib 1.0.1; jsonschema 3.2.0; jupyter_server 1.13.3; kiwisolver 1.3.1; legacy_api_wrap 1.2; leidenalg 0.8.0; llvmlite 0.38.0; loompy 3.0.6; louvain 0.7.0; markupsafe 1.1.1; matplotlib 3.3.3; mpl_toolkits NA; natsort 7.1.1; nbformat 5.0.8; numba 0.55.0; numexpr 2.7.3; numpy 1.21.5; numpy_groupies 0.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2121:4383,message,message,4383,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2121,1,['message'],['message']
Integrability,"dtoolcache/Python/3.9.18/x64/bin/pytest"", line 8 in <module>; /home/vsts/work/_temp/1dc6f140-196e-4393-a84a-ebdaa5dcda61.sh: line 1: 1811 Illegal instruction (core dumped) pytest. ##[error]Bash exited with code '132'.; ##[section]Finishing: PyTest; ```. ### Versions. <details>. ```; anndata 0.10.5.post1; annoy 1.17.3; array_api_compat 1.4.1; asciitree 0.3.3; attrs 23.2.0; cfgv 3.4.0; click 8.1.7; cloudpickle 3.0.0; contourpy 1.2.0; coverage 7.4.1; cycler 0.12.1; dask 2024.2.0; dask-glm 0.3.2; dask-ml 2023.3.24; decorator 5.1.1; Deprecated 1.2.14; distlib 0.3.8; distributed 2024.2.0; exceptiongroup 1.2.0; fasteners 0.19; fbpca 1.0; filelock 3.13.1; fonttools 4.49.0; fsspec 2024.2.0; future 0.18.3; geosketch 1.2; get-annotations 0.1.2; graphtools 1.5.3; h5py 3.10.0; harmonypy 0.0.9; identify 2.5.35; igraph 0.11.4; imageio 2.34.0; importlib-metadata 7.0.1; importlib-resources 6.1.1; iniconfig 2.0.0; intervaltree 3.1.0; Jinja2 3.1.3; joblib 1.3.2; kiwisolver 1.4.5; lazy_loader 0.3; legacy-api-wrap 1.4; leidenalg 0.10.2; llvmlite 0.42.0; locket 1.0.0; magic-impute 3.0.0; MarkupSafe 2.1.5; matplotlib 3.8.3; msgpack 1.0.7; multipledispatch 1.0.0; natsort 8.4.0; networkx 3.2.1; nodeenv 1.8.0; numba 0.59.0; numcodecs 0.12.1; numpy 1.26.4; packaging 23.2; pandas 2.0.3; partd 1.4.1; patsy 0.5.6; pbr 6.0.0; pillow 10.2.0; pip 24.0; platformdirs 4.2.0; pluggy 1.4.0; pre-commit 3.6.2; profimp 0.1.0; psutil 5.9.8; PyGSP 0.5.1; pynndescent 0.5.11; pyparsing 3.1.1; pytest 8.0.1; pytest-mock 3.12.0; pytest-nunit 1.0.6; python-dateutil 2.8.2; pytz 2024.1; PyYAML 6.0.1; scanorama 1.7.4; scanpy 1.10.0.dev220+g534145f6; scikit-image 0.22.0; scikit-learn 1.4.1.post1; scikit-misc 0.3.1; scipy 1.12.0; scprep 1.2.3; seaborn 0.13.2; session-info 1.0.0; setuptools 58.1.0; setuptools-scm 8.0.4; six 1.16.0; sortedcontainers 2.4.0; sparse 0.15.1; statsmodels 0.14.1; stdlib-list 0.10.0; tasklogger 1.2.0; tblib 3.0.0; texttable 1.7.0; threadpoolctl 3.3.0; tifffile 2024.2.12; tomli 2.0.1; toolz 0.12.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2866:7658,wrap,wrap,7658,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2866,1,['wrap'],['wrap']
Integrability,"e ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_io/specs/registry.py"", line 171, in write_elem; _REGISTRY.get_writer(dest_type, (t, elem.dtype.kind), modifiers)(; File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_io/specs/registry.py"", line 24, in wrapper; result = func(g, k, *args, **kwargs); File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_io/specs/methods.py"", line 346, in write_vlen_string_array; f.create_dataset(k, data=elem.astype(str_dtype), dtype=str_dtype, **dataset_kwargs); File ""/home/joyzheng/.local/lib/python3.8/site-packages/h5py/_hl/group.py"", line 183, in create_dataset; dsid = dataset.make_new_dset(group, shape, dtype, data, name, **kwds); File ""/home/joyzheng/.local/lib/python3.8/site-packages/h5py/_hl/dataset.py"", line 168, in make_new_dset; dset_id.write(h5s.ALL, h5s.ALL, data); File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper; File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper; File ""h5py/h5d.pyx"", line 280, in h5py.h5d.DatasetID.write; File ""h5py/_proxy.pyx"", line 145, in h5py._proxy.dset_rw; File ""h5py/_conv.pyx"", line 444, in h5py._conv.str2vlen; File ""h5py/_conv.pyx"", line 95, in h5py._conv.generic_converter; File ""h5py/_conv.pyx"", line 249, in h5py._conv.conv_str2vlen; TypeError: Can't implicitly convert non-string objects to strings. The above exception was the direct cause of the following exception:; Traceback (most recent call last):; File ""integration.py"", line 66, in <module>; adata.write_h5ad('Integrated.h5ad'); File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_core/anndata.py"", line 1918, in write_h5ad; _write_h5ad(; File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_io/h5ad.py"", line 98, in write_h5ad; write_elem(f, ""obs"", adata.obs, dataset_kwargs=dataset_kwargs); File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_io/utils.py"", line 214, in func_wrapper; return func(elem, key, val, *args, **kwargs); File ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2432:2501,wrap,wrapper,2501,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2432,1,['wrap'],['wrapper']
Integrability,"e error when I try to do; adata.write('trial.hdf5') ; #or; sc.pl.violin(adata, 'volume'); ```. ### Error output. ```pytb; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); Cell In[8], line 1; ----> 1 sc.pl.violin(adata, 'volume'). File /home/denizparmaksiz/anaconda3/envs/scanpy/lib/python3.9/site-packages/scanpy/plotting/_anndata.py:749, in violin(adata, keys, groupby, log, use_raw, stripplot, jitter, size, layer, scale, order, multi_panel, xlabel, ylabel, rotation, show, save, ax, **kwds); 645 """"""\; 646 Violin plot.; 647 ; (...); 745 pl.stacked_violin; 746 """"""; 747 import seaborn as sns # Slow import, only import if called; --> 749 sanitize_anndata(adata); 750 use_raw = _check_use_raw(adata, use_raw); 751 if isinstance(keys, str):. File /home/denizparmaksiz/anaconda3/envs/scanpy/lib/python3.9/site-packages/scanpy/_utils/__init__.py:406, in sanitize_anndata(adata); 404 def sanitize_anndata(adata):; 405 """"""Transform string annotations to categoricals.""""""; --> 406 adata._sanitize(). File ~/.local/lib/python3.9/site-packages/anndata/_core/anndata.py:1228, in AnnData.strings_to_categoricals(self, df); 1226 if len(c.categories) >= len(c):; 1227 continue; ...; 1232 ""AnnData, not on this view. You might encounter this""; 1233 ""error message while copying or writing to disk.""; 1234 ). TypeError: reorder_categories() got an unexpected keyword argument 'inplace'; ```. ### Versions. <details>. ```; anndata 0.7.8; scanpy 1.9.3; -----; PIL 10.0.0; asttokens NA; backcall 0.2.0; clustergrammer2 0.18.0; comm 0.1.4; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; debugpy 1.6.7.post1; decorator 5.1.1; executing 1.2.0; google NA; h5py 3.9.0; igraph 0.10.6; importlib_resources NA; ipykernel 6.25.1; ipywidgets 8.1.0; jedi 0.19.0; joblib 1.3.2; kiwisolver 1.4.4; leidenalg 0.9.0; ...; Python 3.9.12 (main, Jun 1 2022, 11:38:51) [GCC 7.5.0]; Linux-5.10.16.3-microsoft-standard-WSL2-x86_64-with-glibc2.31; ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2645:2931,message,message,2931,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2645,1,['message'],['message']
Integrability,"e latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened?. `sc.tl.dpt` was successfully done. But when I want to plot the result of dpt,the error comes out. ### Minimal code sample. ```python; sc.tl.dpt(a1,n_branchings=2); sc.pl.dpt_groups_pseudotime(a1); sc.pl.dpt_timeseries(a1); ```. ### Error output. Error in dpt_timeseries:. ```pytb; WARNING: Plotting more than 100 genes might take some while, consider selecting only highly variable genes, for example.; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); TypeError: float() argument must be a string or a real number, not 'csr_matrix'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last); Cell In[85], line 1; ----> 1 sc.pl.dpt_timeseries(a1). File D:\anaconda\Lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File D:\anaconda\Lib\site-packages\scanpy\plotting\_tools\__init__.py:245, in dpt_timeseries(adata, color_map, show, save, as_heatmap, marker); 242 # only if number of genes is not too high; 243 if as_heatmap:; 244 # plot time series as heatmap, as in Haghverdi et al. (2016), Fig. 1d; --> 245 timeseries_as_heatmap(; 246 adata.X[adata.obs[""dpt_order_indices""].values],; 247 var_names=adata.var_names,; 248 highlights_x=adata.uns[""dpt_changepoints""],; 249 color_map=color_map,; 250 ); 251 else:; 252 # plot time series as gene expression vs time; 253 timeseries(; 254 adata.X[adata.obs[""dpt_order_indices""].values],; 255 var_names=adata.var_names,; (...); 258 marker=marker,; 259 ). File D:\anaconda\Lib\sit",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3086:1156,wrap,wrapper,1156,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3086,1,['wrap'],['wrapper']
Integrability,"e not already familiar with https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview. ; Also, ok for having `uns` changes in another PR, I can work on that as soon as this is merged.; > Update: heard back, the `library_id` should be fine, at least for this version.; > . good !. > > support for multiple slices should be first; > ; > I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it.; > ; > I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle.; > . support for multiple slices and concatenation of anndata objects is by far the priority to me. It's a really useful functionality since:; * most people don't work with one slide; * having the same anndata object containing scRNA-seq as well as matched visium tissue would allow for a very straightforward approach to integration and label propagation (with ingest/bbknn). This would also be extremely useful for the tutorial (which I can't update until anndata supports multiple tissues). I am very interested to see the applications of spatial connectivities you think can be useful. I see the potential but I don't think it's straightforward to make use of that info (especially because in essence the spatial graph derived from visium is completely homogeneous, hence lack of structure).; ; > Also, I think spatial plotting code should get moved out of `sc.pl.embedding` before we allow plotting multiple slides at a time. Why is that? `sc.pl.spatial` is essentially a scatterplot that calls `sc.pl.embedding` yet using another method (circles instead of scatter, but inherits all the arguments)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1088#issuecomment-596965855:2121,integrat,integration,2121,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088#issuecomment-596965855,1,['integrat'],['integration']
Integrability,"e of your suggestions and here's current status:. - reverted back to look for `library_id` in spatial, but still added the exception that `adata.uns[""spatial""]` does not exist. This is in order to use `sc.pl.spatial` with non-visium data.; - if that's the case, then spatial should simply wrap embedding. This also refers to your point.; 	> I'm not totally sure what this means. The coordinates have been z-score transform across each axis? How is this useful? In particular, how is it useful to completely replace the original coordinates with this?. 	this is very likely to happen for anything that it's not visium. In that case, users will share already processed data that contains coordinates in some type of system, and this is the case for whatever processing they had to undertake (would suggest you to have a look at https://github.com/spacetx/starfish for examples of those processing steps.). Anyway, in short, it's much easier for us to just wrap embedding in that case, and I also think it's more correct cause then is the user to choose whatever heuristics they want for point sizes. - fixed a problem in #1534 , that is that the coordinate systems in non-visium has bottom left origin (whereas in visium is top-left, which makes sense because it's in image pixel coordiantes). For this reason, I added the y coordinate inversion in `sc.pl.spatial`, and only in the case where visium is selected, but with img_key = None. Note that this happens because if an img is plotted (before the spots with `circle`), then the origin automatically swap. But if `img_key` is None, then it reverts to default (bottom left). This made it easier as I could remove it from `def _get_data_points` and from `utils._get_edges`. Also added couple of tests for this case. This should be ready for another review, let me know if logic is clearer or I could add more comments in code. re; > Can the spatial neighbours be based off multiple library ids? If so, could you have:; ```python; uns = {; ""spatial"": ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1512#issuecomment-739863306:999,wrap,wrap,999,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1512#issuecomment-739863306,2,['wrap'],['wrap']
Integrability,"e output for PAGA looks weird. . Please see below. Is this something to do with figure margins/axes?. Thanks in advance. ![Screenshot 2023-09-13 at 20 48 43](https://github.com/scverse/scanpy/assets/40166451/c7313231-a2dc-49cb-a3df-21e97d86d278). ### Minimal code sample. ```python; adata2 = sc.datasets.pbmc3k_processed(); sc.tl.paga(adata2, groups='louvain'); sc.pl.paga(adata2); ```. ### Error output. _No response_. ### Versions. <details>. ```; -----; anndata 0.9.1; scanpy 1.9.5; -----; PIL 8.0.1; backcall 0.2.0; bottleneck 1.3.7; cellrank 1.5.1; cffi 1.15.1; cloudpickle 2.2.1; colorama 0.4.6; cycler 0.10.0; cython_runtime NA; cytoolz 0.12.1; dask 2023.5.0; dateutil 2.8.2; decorator 5.1.1; docrep 0.3.2; google NA; h5py 3.8.0; igraph 0.10.4; importlib_resources NA; ipykernel 5.3.4; ipython_genutils 0.2.0; ipywidgets 7.5.1; jedi 0.17.2; jinja2 3.0.3; joblib 1.2.0; kiwisolver 1.3.0; leidenalg 0.9.1; llvmlite 0.34.0; lz4 3.1.10; markupsafe 2.1.3; matplotlib 3.7.1; mpl_toolkits NA; natsort 8.3.1; numba 0.51.2; numexpr 2.8.5; numpy 1.23.5; packaging 23.1; pandas 1.5.3; parso 0.7.0; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; progressbar 4.2.0; prompt_toolkit 3.0.8; psutil 5.7.2; ptyprocess 0.6.0; pygam 0.8.0; pygments 2.7.2; pygpcca 1.0.4; pyparsing 2.4.7; python_utils NA; pytz 2020.1; ruamel NA; scipy 1.10.1; scvelo 0.2.5; seaborn 0.11.0; session_info 1.0.0; six 1.15.0; sklearn 1.2.2; sphinxcontrib NA; statsmodels 0.12.0; storemagic NA; tblib 1.7.0; texttable 1.6.7; threadpoolctl 2.1.0; tlz 0.12.1; toolz 0.11.1; tornado 6.1; tqdm 4.50.2; traitlets 5.0.5; typing_extensions NA; wcwidth 0.2.5; wrapt 1.15.0; yaml 5.3.1; zipp NA; zmq 19.0.2; zope NA; -----; IPython 7.25.0; jupyter_client 6.1.12; jupyter_core 4.7.1; notebook 6.4.0; -----; Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]; Linux-5.4.0-137-generic-x86_64-with-glibc2.10; -----; Session information updated at 2023-09-15 09:42; Running scvelo 0.2.5 (python 3.8.5) on 2023-09-15 09:42. ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2665:2035,wrap,wrapt,2035,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2665,1,['wrap'],['wrapt']
Integrability,"e, *args, **kwargs); 128 if key in f:; 129 del f[key]; --> 130 _write_method(type(value))(f, key, value, *args, **kwargs); 131 ; 132 . ~/anaconda3/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, key, val, *args, **kwargs); 210 except Exception as e:; 211 parent = _get_parent(elem); --> 212 raise type(e)(; 213 f""{e}\n\n""; 214 f""Above error raised while writing key {key!r} of {type(elem)}"". TypeError: Can't implicitly convert non-string objects to strings. Above error raised while writing key 'Batch' of <class 'h5py._hl.group.Group'> from /. Above error raised while writing key 'obs' of <class 'h5py._hl.files.File'> from /. ```. #### Versions. <details>. WARNING: If you miss a compact list, please try `print_header`!; The `sinfo` package has changed name and is now called `session_info` to become more discoverable and self-explanatory. The `sinfo` PyPI package will be kept around to avoid breaking old installs and you can downgrade to 0.3.2 if you want to use it without seeing this message. For the latest features and bug fixes, please install `session_info` instead. The usage and defaults also changed slightly, so please review the latest README at https://gitlab.com/joelostblom/session_info.; -----; anndata 0.7.6; scanpy 1.7.2; sinfo 0.3.4; -----; PIL 8.2.0; anyio NA; appnope 0.1.2; attr 20.3.0; babel 2.9.0; backcall 0.2.0; bottleneck 1.3.2; brotli NA; cairo 1.20.0; certifi 2020.12.05; cffi 1.14.5; chardet 4.0.0; cloudpickle 1.6.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2021.04.0; dateutil 2.8.1; decorator 5.0.6; fsspec 2021.05.0; get_version 2.2; google NA; h5py 3.2.1; idna 2.10; igraph 0.7.1; ipykernel 5.3.4; ipython_genutils 0.2.0; jedi 0.17.2; jinja2 2.11.3; joblib 1.0.1; json5 NA; jsonschema 3.2.0; jupyter_server 1.4.1; jupyterlab_server 2.4.0; kiwisolver 1.3.1; legacy_api_wrap 1.2; leidenalg 0.7.0; llvmlite 0.36.0; loompy 3.0.6; markupsafe 1.1.1; matplotlib 3.3.4; mpl_toolkits NA; natsort 7.1.1; nbclassic ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1866:6256,message,message,6256,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1866,1,['message'],['message']
Integrability,"e, ir.Expr):; --> 626 return self.lower_expr(ty, value); 627 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_expr(self, resty, expr); 1161 elif expr.op == 'call':; -> 1162 res = self.lower_call(resty, expr); 1163 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_call(self, resty, expr); 890 else:; --> 891 res = self._lower_call_normal(fnty, expr, signature); 892 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in _lower_call_normal(self, fnty, expr, signature); 1132 ; -> 1133 res = impl(self.builder, argvals, self.loc); 1134 return res; C:\ProgramData\Anaconda3\lib\site-packages\numba\core\base.py in __call__(self, builder, args, loc); 1189 def __call__(self, builder, args, loc=None):; -> 1190 res = self._imp(self._context, builder, self._sig, args, loc=loc); 1191 self._context.add_linking_libs(getattr(self, 'libs', ())). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\base.py in wrapper(*args, **kwargs); 1219 kwargs.pop('loc') # drop unused loc; -> 1220 return fn(*args, **kwargs); 1221 . C:\ProgramData\Anaconda3\lib\site-packages\numba\cpython\rangeobj.py in range1_impl(context, builder, sig, args); 37 state.start = context.get_constant(int_type, 0); ---> 38 state.stop = stop; 39 state.step = context.get_constant(int_type, 1). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setattr__(self, field, value); 163 return super(_StructProxy, self).__setattr__(field, value); --> 164 self[self._datamodel.get_field_position(field)] = value; 165 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setitem__(self, index, value); 187 else:; --> 188 raise TypeError(""Invalid store of {value.type} to ""; 189 ""{ptr.type.pointee} in "". TypeError: Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x000001FF1D57B970> (trying to write member #1). During handling of the above exception, another exception occurred:. LoweringErro",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-1319286325:2675,wrap,wrapper,2675,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-1319286325,1,['wrap'],['wrapper']
Integrability,"e:///C:/Program%20Files/Python312/Lib/gzip.py:193) if filename is None:; [194](file:///C:/Program%20Files/Python312/Lib/gzip.py:194) filename = getattr(fileobj, 'name', ''). FileNotFoundError: [Errno 2] No such file or directory: 'GSE212966\\GSM6567159_PDAC2_features.tsv.gz'; ```. I have tried with other datasets which are originally named ad matrix, features and barcodes, and those are working properly. Any idea?. ### Minimal code sample. ```python; data1 = sc.read_10x_mtx(""GSE212966"", prefix=""GSM6567159_PDAC2_"", var_names='gene_symbols', cache=True); ```. ### Error output. ```pytb; ---------------------------------------------------------------------------; FileNotFoundError Traceback (most recent call last); Cell In[62], line 1; ----> 1 data1 = sc.read_10x_mtx(""GSE212966"", prefix=""GSM6567159_PDAC2_"", var_names='gene_symbols', cache=True); 2 data1.var_names_make_unique(). File ~\AppData\Roaming\Python\Python312\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File ~\AppData\Roaming\Python\Python312\site-packages\scanpy\readwrite.py:560, in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only, prefix); 558 prefix = """" if prefix is None else prefix; 559 is_legacy = (path / f""{prefix}genes.tsv"").is_file(); --> 560 adata = _read_10x_mtx(; 561 path,; 562 var_names=var_names,; 563 make_unique=make_unique,; 564 cache=cache,; 565 cache_compression=cache_compression,; 566 prefix=prefix,; 567 is_legacy=is_legacy,; 568 ); 569 if is_legacy or not gex_only:; 570 return adata. File ~\AppData\Roaming\Python\Python312\site-packages\scanpy\readwrite.py:594, in _read_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix, is_legac",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:20400,wrap,wrapper,20400,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['wrap'],['wrapper']
Integrability,"e_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity); 390 # umap 0.5.0; 391 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""); --> 392 from umap.umap_ import fuzzy_simplicial_set; 393 ; 394 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). C:\ProgramData\Anaconda3\lib\site-packages\umap\__init__.py in <module>; ----> 1 from .umap_ import UMAP; 2 ; 3 # Workaround: https://github.com/numba/numba/issues/3341; 4 import numba; 5 . C:\ProgramData\Anaconda3\lib\site-packages\umap\umap_.py in <module>; 52 from umap.spectral import spectral_layout; 53 from umap.utils import deheap_sort, submatrix; ---> 54 from umap.layouts import (; 55 optimize_layout_euclidean,; 56 optimize_layout_generic,. C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py in <module>; 37 },; 38 ); ---> 39 def rdist(x, y):; 40 """"""Reduced Euclidean distance.; 41 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\decorators.py in wrapper(func); 217 with typeinfer.register_dispatcher(disp):; 218 for sig in sigs:; --> 219 disp.compile(sig); 220 disp.disable_compile(); 221 return disp. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, sig); 963 with ev.trigger_event(""numba:compile"", data=ev_details):; 964 try:; --> 965 cres = self._compiler.compile(args, return_type); 966 except errors.ForceLiteralArg as e:; 967 def folded(args, kws):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, args, return_type); 123 ; 124 def compile(self, args, return_type):; --> 125 status, retval = self._compile_cached(args, return_type); 126 if status:; 127 return retval. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in _compile_cached(self, args, return_type); 137 ; 138 try:; --> 139 retval = self._compile_core(args, return_type); 140 except errors.TypingError as e:; 141 self._failed_cache[key] = e. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\disp",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-1319286325:5672,wrap,wrapper,5672,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-1319286325,1,['wrap'],['wrapper']
Integrability,"eapdict NA; idna 2.10; igraph 0.9.1; ipykernel 5.4.3; ipython_genutils 0.2.0; ipywidgets 7.6.3; jedi 0.18.0; jinja2 2.11.2; joblib 1.0.1; json5 NA; jsonschema 3.2.0; jupyter_server 1.2.2; jupyterlab_server 2.1.2; keras_preprocessing 1.1.2; kiwisolver 1.3.1; legacy_api_wrap 0.0.0; leidenalg 0.8.4; llvmlite 0.36.0; louvain 0.7.0; markupsafe 1.1.1; matplotlib 3.4.2; mpl_toolkits NA; msgpack 1.0.2; natsort 7.1.1; nbclassic NA; nbformat 5.1.2; netifaces 0.10.9; networkx 2.5.1; numba 0.53.1; numexpr 2.7.3; numpy 1.19.5; nvtx NA; opt_einsum v3.3.0; packaging 20.8; pandas 1.2.4; parso 0.8.1; petsc4py 3.14.1; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; progressbar 3.53.1; prometheus_client NA; prompt_toolkit 3.0.10; psutil 5.8.0; ptyprocess 0.7.0; pvectorc NA; pyarrow 1.0.1; pycparser 2.20; pygam 0.8.0; pygments 2.7.4; pygpcca 1.0.2; pynndescent 0.5.2; pynvml 8.0.4; pyparsing 2.4.7; pyrsistent NA; python_utils NA; pytz 2021.1; requests 2.25.1; rmm 0.20.0a+28.g7768d4d; scanpy 1.7.2; scanpy_gpu_funcs NA; scipy 1.6.3; scvelo 0.2.3; seaborn 0.11.1; send2trash NA; setuptools_scm NA; sinfo 0.3.1; six 1.15.0; sklearn 0.24.2; slepc4py 3.14.0; sniffio 1.2.0; socks 1.7.1; sortedcontainers 2.3.0; sphinxcontrib NA; statsmodels 0.12.2; storemagic NA; tables 3.6.1; tblib 1.7.0; tensorboard 2.6.0a20210510; tensorflow 2.6.0-dev20210510; termcolor 1.1.0; texttable 1.6.3; threadpoolctl 2.1.0; tlz 0.11.0; toolz 0.11.1; tornado 6.1; traitlets 5.0.5; treelite 1.1.0; treelite_runtime 1.1.0; typing_extensions NA; ucp 0.20.0a+30.g2aa87da; umap 0.5.1; urllib3 1.26.4; virtualenvwrapper NA; wcwidth 0.2.5; wrapt 1.12.1; yaml 5.4.1; zict 2.0.0; zipp NA; zmq 21.0.1; -----; IPython 7.19.0; jupyter_client 6.1.11; jupyter_core 4.7.1; jupyterlab 3.0.5; notebook 6.2.0; -----; Python 3.8.8 | packaged by conda-forge | (default, Feb 20 2021, 16:22:27) [GCC 9.3.0]; Linux-5.8.0-50-generic-x86_64-with-glibc2.10; 32 logical CPU cores, x86_64; -----; Session information updated at 2021-05-12 13:23. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1837:4363,wrap,wrapt,4363,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1837,1,['wrap'],['wrapt']
Integrability,"eature would you like to request? -->; - [ ] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [x] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python; import matplotlib.pyplot as plt; import scanpy as sc; import numpy as np. # given integrated object adata, clustered via the leiden algorithm and; # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID; count_series = adata.obs.groupby(['leiden', 'batch']).size(); new_df = count_series.to_frame(name = 'size').reset_index(); # convert from multi index to pivot; constitution = new_df.pivot(index='leiden', columns='batch')['size']; # convert to %batch (but could be modified to show different things instead; perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))); # keep track of the batch, cluster IDs so we can use them for plotting; clusters = adata.obs.leiden.cat.categories; batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots; # replace styling with scanpy defaults probably?; fig, ax = plt.subplots(); ax.grid(False); ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]); bottom = np.zeros(clusters.shape); for i, b in enumerate(batches):; ax.bar(clusters, perc_clust[i], 0.6, bo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1573:954,integrat,integrated,954,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573,1,['integrat'],['integrated']
Integrability,"ecifically what you want (e.g. `typing.Sequence[T]` if it only needs to be iterable and indexable, but not necessarily a `list`). About your examples, generally I always have to dig into the code to figure such things out. Annoying, but it means that people after me can just use the type annotations instead of wasting their time doing the same. - Currently we only accept `str` for obs_names, so depending on the actual usage in the function, `Iterable[str]` or `Sequence[str]` would be it I guess.; - Usually single integers being passed around should be python `int`s, because they can be any size. But if you need to accept `int32` and `int64`, you can do `Union[int, np.integer]`. > Is there a way to say: ""should behave right if I call np.array on it"". In truth, `array` eats just about anything (with weird results), and for us this is probably a good idea:. ```py; Number = Union[float, int, np.integer, np.floating]; Num1DArrayLike = Sequence[Number]; Num2DArrayLike = Sequence[Num1DArrayLike]; Num3DArrayLike = Sequence[Num2DArrayLike]; NumNDArrayLike = Union[Num1DArrayLike, Num2DArrayLike, Num3DArrayLike]; ```. But if we want to be exact about `array_like`s, we’d need this ABC:. ```py; class ArrayLike(ABC):; """"""An array,; any object exposing the array interface,; an object whose __array__ method returns an array,; or any (nested) sequence.; """"""; @classmethod; def __subclasshook__(cls, C):; if issubclass(C, np.ndarray):; return True; if any('__array_interface__' in B.__dict__ for B in C.__mro__):; return True; if any('__array__' in B.__dict__ for B in C.__mro__):; return True; return Sequence.__subclasshook__(cls, C); ```. ----. Two thoughts here:. 1. It’s fine if you don’t know exactly. Just use your best guess. The worst case is that someone wastes a second of runtime converting their argument to the type you thought was needed, while the original type would have been OK.; 2. It’s good if someone thinks about all that because that means things don’t break unexpectedly!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-441207438:1385,interface,interface,1385,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441207438,1,['interface'],['interface']
Integrability,"ed globally and shared by all users. I am looking to change that. # The issue. When calculating the `sc.tl.marker_gene_overlap` I get the expected and reasonable results on the agando environment, but completely rubbish results when running the same code with a fresh Conda environment and the latest dependencies installed. ![image](https://user-images.githubusercontent.com/21954664/106739402-659dfb80-6619-11eb-84f1-e75abfa6167d.png). Top = new, trash results; Bottom = old=agando expected results. The old environment has:. ```; scanpy==1.6.1.dev110+gb4234d81 anndata==0.7.4 umap==0.4.6 numpy==1.19.0 scipy==1.5.1 pandas==1.1.5 scikit-learn==0.23.1 statsmodels==0.12.1 python-igraph==0.8.0 louvain==0.6.1 leidenalg==0.8.3; ```. The new environment has ; ```; scanpy==1.6.1 anndata==0.7.5 umap==0.4.6 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.1 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3; ```; Full new conda environment:; ```; name: single_cell_analysis; channels:; - defaults; dependencies:; - _libgcc_mutex=0.1=main; - argon2-cffi=20.1.0=py37h7b6447c_1; - async_generator=1.10=py37h28b3542_0; - attrs=20.3.0=pyhd3eb1b0_0; - backcall=0.2.0=pyhd3eb1b0_0; - bleach=3.3.0=pyhd3eb1b0_0; - ca-certificates=2021.1.19=h06a4308_0; - certifi=2020.12.5=py37h06a4308_0; - cffi=1.14.4=py37h261ae71_0; - dbus=1.13.18=hb2f20db_0; - decorator=4.4.2=pyhd3eb1b0_0; - defusedxml=0.6.0=py_0; - entrypoints=0.3=py37_0; - expat=2.2.10=he6710b0_2; - fontconfig=2.13.0=h9420a91_0; - freetype=2.10.4=h5ab3b9f_0; - glib=2.66.1=h92f7085_0; - gst-plugins-base=1.14.0=h8213a91_2; - gstreamer=1.14.0=h28cd5cc_2; - icu=58.2=he6710b0_3; - importlib_metadata=2.0.0=1; - ipykernel=5.3.4=py37h5ca1d4c_0; - ipython=7.20.0=py37hb070fc8_1; - ipython_genutils=0.2.0=pyhd3eb1b0_1; - ipywidgets=7.6.3=pyhd3eb1b0_1; - jedi=0.17.0=py37_0; - jinja2=2.11.3=pyhd3eb1b0_0; - jpeg=9b=h024ee3a_2; - jsonschema=3.2.0=py_2; - jupyter=1.0.0=py37_7; - jupyter_client=6.1.7=py_0; - jupyter_console=6.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1625:1451,depend,dependencies,1451,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625,1,['depend'],['dependencies']
Integrability,"ed: six>=1.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from python-dateutil>=2.1->matplotlib>=3.1.2->scanpy[leiden]) (1.16.0); Collecting threadpoolctl>=2.0.0; Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB); Collecting pynndescent>=0.5; Using cached pynndescent-0.5.5-py3-none-any.whl; Collecting get-version>=2.0.4; Using cached get_version-2.1-py3-none-any.whl (43 kB); Collecting igraph==0.9.8; Using cached igraph-0.9.8-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting texttable>=1.6.2; Using cached texttable-1.6.4-py2.py3-none-any.whl (10 kB); Collecting stdlib-list; Using cached stdlib_list-0.8.0-py3-none-any.whl (63 kB); Collecting numexpr>=2.6.2; Using cached numexpr-2.7.3-cp36-cp36m-win_amd64.whl (93 kB); Requirement already satisfied: colorama in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from tqdm->scanpy[leiden]) (0.4.4); Installing collected packages: numpy, threadpoolctl, scipy, llvmlite, joblib, texttable, scikit-learn, pillow, numba, kiwisolver, cycler, cached-property, xlrd, tqdm, stdlib-list, pynndescent, patsy, pandas, numexpr, natsort, matplotlib, igraph, h5py, get-version, decorator, umap-learn, tables, statsmodels, sinfo, seaborn, python-igraph, networkx, legacy-api-wrap, anndata, scanpy, leidenalg; Attempting uninstall: decorator; Found existing installation: decorator 5.1.0; Uninstalling decorator-5.1.0:; Successfully uninstalled decorator-5.1.0; Successfully installed anndata-0.7.6 cached-property-1.5.2 cycler-0.11.0 decorator-4.4.2 get-version-2.1 h5py-3.1.0 igraph-0.9.8 joblib-1.1.0 kiwisolver-1.3.1 legacy-api-wrap-1.2 leidenalg-0.8.8 llvmlite-0.36.0 matplotlib-3.3.4 natsort-8.0.0 networkx-2.5.1 numba-0.53.1 numexpr-2.7.3 numpy-1.19.5 pandas-1.1.5 patsy-0.5.2 pillow-8.4.0 pynndescent-0.5.5 python-igraph-0.9.8 scanpy-1.7.2 scikit-learn-0.24.2 scipy-1.5.4 seaborn-0.11.2 sinfo-0.3.4 statsmodels-0.12.2 stdlib-list-0.8.0 tables-3.6.1 texttable-1.6.4 threadpoolctl-3.0.0 tqdm-4.62.3 umap-learn-0.5.2 xlrd-1.2.0",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:5088,wrap,wrap,5088,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,4,['wrap'],"['wrap', 'wrap-']"
Integrability,"ed:; 114 adata.file.open(filepath, ""r+""). /usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/functools.py in wrapper(*args, **kw); 838 '1 positional argument'); 839 ; --> 840 return dispatch(args[0].__class__)(*args, **kw); 841 ; 842 funcname = getattr(func, '__name__', 'singledispatch function'). ~/github/anndata/anndata/_io/h5ad.py in write_attribute_h5ad(f, key, value, *args, **kwargs); 124 if key in f:; 125 del f[key]; --> 126 _write_method(type(value))(f, key, value, *args, **kwargs); 127 ; 128 . ~/github/anndata/anndata/_io/h5ad.py in write_mapping(f, key, value, dataset_kwargs); 284 def write_mapping(f, key, value, dataset_kwargs=MappingProxyType({})):; 285 for sub_key, sub_value in value.items():; --> 286 write_attribute(f, f""{key}/{sub_key}"", sub_value, dataset_kwargs=dataset_kwargs); 287 ; 288 . /usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/functools.py in wrapper(*args, **kw); 838 '1 positional argument'); 839 ; --> 840 return dispatch(args[0].__class__)(*args, **kw); 841 ; 842 funcname = getattr(func, '__name__', 'singledispatch function'). ~/github/anndata/anndata/_io/h5ad.py in write_attribute_h5ad(f, key, value, *args, **kwargs); 124 if key in f:; 125 del f[key]; --> 126 _write_method(type(value))(f, key, value, *args, **kwargs); 127 ; 128 . ~/github/anndata/anndata/_io/h5ad.py in write_mapping(f, key, value, dataset_kwargs); 284 def write_mapping(f, key, value, dataset_kwargs=MappingProxyType({})):; 285 for sub_key, sub_value in value.items():; --> 286 write_attribute(f, f""{key}/{sub_key}"", sub_value, dataset_kwargs=dataset_kwargs); 287 ; 288 . /usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/functools.py in wrapper(*args, **kw); 838 '1 positional argument'); 839 ; --> 840 return dispatch(args[0].__class__)(*args, **kw); 841 ; 842 funcname = getattr(func, '__name__', 'singledispatch function'). ~/github/anndata/anndata/_io/h5ad.py in ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1131:3220,wrap,wrapper,3220,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1131,1,['wrap'],['wrapper']
Integrability,"elf.sca(current_ax). c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\matplotlib\colorbar.py in colorbar_factory(cax, mappable, **kwargs); 1732 cb = ColorbarPatch(cax, mappable, **kwargs); 1733 else:; -> 1734 cb = Colorbar(cax, mappable, **kwargs); 1735 ; 1736 cid = mappable.callbacksSM.connect('changed', cb.update_normal). c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\matplotlib\colorbar.py in __init__(self, ax, mappable, **kwargs); 1226 if isinstance(mappable, martist.Artist):; 1227 _add_disjoint_kwargs(kwargs, alpha=mappable.get_alpha()); -> 1228 ColorbarBase.__init__(self, ax, **kwargs); 1229 ; 1230 @cbook.deprecated(""3.3"", alternative=""update_normal""). c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\matplotlib\cbook\deprecation.py in wrapper(*args, **kwargs); 449 ""parameter will become keyword-only %(removal)s."",; 450 name=name, obj_type=f""parameter of {func.__name__}()""); --> 451 return func(*args, **kwargs); 452 ; 453 return wrapper. c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\matplotlib\colorbar.py in __init__(self, ax, cmap, norm, alpha, values, boundaries, orientation, ticklocation, extend, spacing, ticks, format, drawedges, filled, extendfrac, extendrect, label); 489 else:; 490 self.formatter = format # Assume it is a Formatter or None; --> 491 self.draw_all(); 492 ; 493 def _extend_lower(self):. c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\matplotlib\colorbar.py in draw_all(self); 506 # sets self._boundaries and self._values in real data units.; 507 # takes into account extend values:; --> 508 self._process_values(); 509 # sets self.vmin and vmax in data units, but just for the part of the; 510 # colorbar that is not part of the extend patch:. c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\matplotlib\colorbar.py in _process_values(self, b); 961 expander=0.1); 962 ; --> 963 b = sel",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2003:2806,wrap,wrapper,2806,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2003,1,['wrap'],['wrapper']
Integrability,"ellRanger that is changing the output format. This pull request makes Scanpy forward compatible with the new version. In particular, the following changes are made:. Updated `read_10x_h5`:; - Renamed the original `read_10x_h5` as `_read_legacy_10x_h5`;; - Added `_read_v3_10x_h5` to read the new Cell Ranger output format;; - The new `read_10x_h5` determines the version of HDF5 input by the presence of the matrix key, and wraps the above two functions. In addition, it takes a `gex_only` argument which filters out feature barcoding counts from the outcome object when it is True (default). Otherwise, the full matrix will be retained.; - For CR-v3, `feature_types` and `genome` were added into the outcome object as new attributes. Updated `read_10x_mtx`:; - Renamed the original `read_10x_mtx` as `_read_legacy_10x_mtx`;; - Added `_read_v3_10x_mtx` to read the new Cell Ranger output format;; - The new `read_10x_mtx` determines the version of matrix input by the presence of the `genes.tsv` file under the input directory, and wraps the above two functions. In addition, it takes a `gex_only` argument which filters out feature barcoding counts from the outcome object when it is `True` (default). Otherwise, the full matrix will be retained.; - For CR-v3, `feature_types` was added into the outcome object as a new attribute. Added small test datasets and code for the revised functions to verify the expected behavior. Note for the `genome` argument:; - There is a genome argument in Scanpy's `read_10x_h5` function but not in `read_10x_mtx` as the genome was already specified by the path of input directory. The outcome object of the two functions should be the same which always take one genome at a time.; - In this PR, when there are multiple genomes (e.g. Barnyard), `read_10x_mtx` always read them all, whereas `read_10x_h5` always need to specify one of them (mm10 by default). However, when `gex_only == False`, the `genome` argument will be ignored and the whole matrix will be read.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/334:1083,wrap,wraps,1083,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/334,1,['wrap'],['wraps']
Integrability,"elta(microseconds=500777); foo(pbmcs, 1, copy=True);; # 2019-02-13 19:28.02 call adata_id=4937049368 call_id=UUID('986f57e4-656a-41b1-9c7c-a7c5ad5b01fc') called_func=foo; # 2019-02-13 19:28.03 call_finish call_id=UUID('986f57e4-656a-41b1-9c7c-a7c5ad5b01fc') called_func=foo elapsed=datetime.timedelta(microseconds=505970) returned_adata_id=4940502352; ```. </details>. <details>; <summary>More complicated example with argument value logging</summary>. ```python; from anndata import AnnData; from copy import copy; from datetime import datetime; from functools import wraps; import inspect; from itertools import chain; from structlog import get_logger; from time import sleep; import uuid. logger = get_logger(). def logged(logged_args=None):; """"""; Params; ------; logged_args : list[str], optional (default: `None`); Names of arguments to log.; """"""; if logged_args is None:; logged_args = []; def logged_decorator(func):; argnames = inspect.getfullargspec(func).args; @wraps(func); def func_wrapper(*args, **kwargs):; call_id = uuid.uuid4() # So we can always match call start with call end; logged_params = {}. for param, val in chain(zip(argnames, args), kwargs.items()):; if type(val) is AnnData:; logged_params[param] = id(val); elif param in logged_args:; logged_params[param] = copy(val) # Probably need to consider how these values get logged. logger.msg(""call"", called_func=func.__name__,; logged_args=logged_params, call_id=call_id). t0 = datetime.now(); output = func(*args, **kwargs); dt = datetime.now() - t0. call_finish_record = dict(; called_func=func.__name__, elapsed=dt,; ); if type(output) is AnnData:; call_finish_record[""returned_adata_id""] = id(output); logger.msg(""call_finish"", **call_finish_record, call_id=call_id); return output; return func_wrapper; return logged_decorator. # Usage. @logged(logged_args=[""x""]); def foo(adata, x, copy=True):; sleep(0.5); if copy: return adata.copy(). import scanpy as sc; pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1, copy=True);",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/472#issuecomment-463117273:2869,wrap,wraps,2869,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472#issuecomment-463117273,1,['wrap'],['wraps']
Integrability,"en/latest/pbmc3k.html). The first steps worked quit well, but when I want to run PCA the following error occurs (see below). <!-- To reproduce -->; Follow the tutorial ([pbmc3k.ipynb](https://github.com/scverse/scanpy-tutorials/blob/75c5ebb5b63769aee65f38842a34b7f7e1bbd476/pbmc3k.ipynb)) until the following line:. ```python; sc.tl.pca(adata, svd_solver='arpack'); ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb; Traceback (most recent call last):; File ""/Users/pbinder/Nextcloud/projects/memory-tcell-scRNA-seq/tutorials/scanpy/scanpy-tutorials-master/test.py"", line 51, in <module>; sc.tl.pca(adata, svd_solver='arpack'); File ""/Users/pbinder/opt/anaconda3/envs/scRNA/lib/python3.9/site-packages/scanpy/preprocessing/_pca.py"", line 188, in pca; X_pca = pca_.fit_transform(X); File ""/Users/pbinder/opt/anaconda3/envs/scRNA/lib/python3.9/site-packages/sklearn/utils/_set_output.py"", line 140, in wrapped; data_to_wrap = f(self, X, *args, **kwargs); File ""/Users/pbinder/opt/anaconda3/envs/scRNA/lib/python3.9/site-packages/sklearn/decomposition/_pca.py"", line 462, in fit_transform; U, S, Vt = self._fit(X); File ""/Users/pbinder/opt/anaconda3/envs/scRNA/lib/python3.9/site-packages/sklearn/decomposition/_pca.py"", line 514, in _fit; return self._fit_truncated(X, n_components, self._fit_svd_solver); File ""/Users/pbinder/opt/anaconda3/envs/scRNA/lib/python3.9/site-packages/sklearn/decomposition/_pca.py"", line 609, in _fit_truncated; U, S, Vt = svds(X, k=n_components, tol=self.tol, v0=v0); File ""/Users/pbinder/opt/anaconda3/envs/scRNA/lib/python3.9/site-packages/scipy/sparse/linalg/_eigen/_svds.py"", line 532, in svds; _, eigvec = eigsh(XH_X, k=k, tol=tol ** 2, maxiter=maxiter,; File ""/Users/pbinder/opt/anaconda3/envs/scRNA/lib/python3.9/site-packages/scipy/sparse/linalg/_eigen/arpack/arpack.py"", line 1697, in eigsh; params.iterate(); File ""/Users/pbinder/opt/anaconda3/envs/scRNA/lib/python3.9/site-packages/scipy/sparse/linalg/_eigen",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2473:1088,wrap,wrapped,1088,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2473,1,['wrap'],['wrapped']
Integrability,"ently came out, and started favoring the `tbb` backend. This is a problem since numba thinks this backend is available on our CI (and on my HPC), but it’s actually not. I also can’t get tbb to even install in a way numba sees in on these systems. If tbb isn’t actually available, but numba detects it we get errors with horrible tracebacks anytime parallelized numba code is used https://github.com/lmcinnes/pynndescent/issues/129. These tracebacks are so horrible they break our CI further https://github.com/pytest-dev/pytest-nunit/issues/47. So what do we do?. ## Possible solutions:. * Pin pynndescent below 0.5.3. This makes pynndescent a required dependency. We’ve previously avoided this since it would change results for people using `umap<0.4` (e.g. anyone with `scvelo` installed) who did not explicitly install pynndescent. However, given the lack of complaints around umap results changing as dependencies have increased, this may not be so bad. It would be great if we could constrain the version without having it be a dependency. This would be similar to what's possible with `pip` and [constraints files](https://pip.pypa.io/en/stable/user_guide/#constraints-files), I don't see how one would be able to specify this for a package. I don't think it's possible, but maybe I'm missing something about the version string syntax. * Make sure the numba threading layer is `“workqueue”` after pynndescent is imported. This is tricky. pynndescent<0.5.3 takes a long time to import, so we don’t want to do this at the top level. So we would need to add a check after everytiem pynndescent could possibly be imported to check that it didn’t set the threading backend to anything else. My understanding of the numba threading system is that once you’ve called for parallel compilation, you’re locked into the threading backend for that session. * Make sure the threading layer is workqueue after pynndescent is imported, but make pynndescent import fast. A kinda simpler solution would be to req",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1931:1241,depend,dependency,1241,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1931,1,['depend'],['dependency']
Integrability,"erwise 'X_pca' is used.; If 'X_pca' is not present, it's computed with default parameters. **knn** : bool, optional (default: True). If `True`, use a hard threshold to restrict the number of neighbors to; `n_neighbors`, that is, consider a knn graph. Otherwise, use a Gaussian; Kernel to assign low weights to neighbors more distant than the; `n_neighbors` nearest neighbor. **random_state** : typing.Union[int, mtrand.RandomState, NoneType]. A numpy random seed. **method** : {'umap', 'gauss', `None`} (default: `'umap'`). Use 'umap' [McInnes18]_ or 'gauss' (Gauss kernel following [Coifman05]_; with adaptive width [Haghverdi16]_) for computing connectivities. **metric** : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], float]], optional (default: 'euclidean'). A known metric’s name or a callable that returns a distance. **metric_kwds** : Mapping. Options for the metric. **copy** : bool. Return a copy instead of writing to adata. :Returns:. Depending on `copy`, updates or returns `adata` with the following:. . **connectivities** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Weighted adjacency matrix of the neighborhood graph of data; points. Weights should be interpreted as connectivities. **distances** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Instead of decaying weights, this stores distances for each pair of; neighbors.; File: ~/_hholtz/01_projects/1512_scanpy/scanpy/scanpy/neighbors/__init__.py; Type: function; ```. PS: ; - Already the [docs](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.Neighbors.compute_neighbors.html) show that `Neighbors.compute_neighbors` has invalid numpydoc... this was the case in several instances and I'm slowly fixing all of them... It's just a matter of adding `\` at the line breaks.; - I completely agree that the redundency between signature and docstring information lead to a a very small number of errors in the docstrings. However, in several instances, I'm setting the default value in the ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:6750,Depend,Depending,6750,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999,1,['Depend'],['Depending']
Integrability,"es())))); tarashansky_palette = {k: to_hex(colors[v]) for k, v in color_map.items()}. tf.shade(pts, color_key=tarashansky_palette); ```. </details>. ![image](https://user-images.githubusercontent.com/8238804/105131791-c3244980-5b3d-11eb-83cc-2691b392b1c1.png). I think it's good, but there's room for improvement. I was initially worried about this example since the cluster connectivity graph is highly connected, but this seems to have worked out alright. Overplotting (points sitting on top of eachother) is still a problem, but I think that's a separate problem from choosing colors. * Even if they do not touch, can we make it so clusters close to each-other are less likely to get similar colors? This would start becoming more of an optimization problem, and more complicated.; * Picking a color palette where all colors are very visible is important. (maybe adding a light border when there's nothing in the background? maybe something similar to ""player model contrast boost"" shaders?); * I think there are some ""spurious"" connections in the graph. Many clusters have >20 neighbors. I think this has to do with outlier points and dispersed points. Not completely sure how to deal with this. Maybe less of an issue with smaller datasets/ leiden clustering?; * When categories are disconnected, how do we indicate they're the same category if unique color is no longer an option? Do we require disconnected categories be uniquely assigned a color? Is this a case for interactivity?. I've been thinking that it might be worth starting a package for dealing with common issues in plotting single cell data. Largely involving color assignment and overplotting. I think this should be, or at least start out as, separate from scanpy since there are a number of dependencies I think are useful here, which aren't required for scanpy. Plus being able to iterate quickly would be nice. I've started collecting some notebooks on this [here](https://github.com/ivirshup/notebooks/tree/master/plotting).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1366#issuecomment-763341345:5506,depend,dependencies,5506,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366#issuecomment-763341345,1,['depend'],['dependencies']
Integrability,"eturn func_wrapper. # Usage. @logged; def foo(adata, x, copy=False):; sleep(0.5); if copy: return adata.copy(). import scanpy as sc; pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1); # 2019-02-13 19:27.58 call adata_id=4937049368 call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo; # 2019-02-13 19:27.58 call_finish call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo elapsed=datetime.timedelta(microseconds=500777); foo(pbmcs, 1, copy=True);; # 2019-02-13 19:28.02 call adata_id=4937049368 call_id=UUID('986f57e4-656a-41b1-9c7c-a7c5ad5b01fc') called_func=foo; # 2019-02-13 19:28.03 call_finish call_id=UUID('986f57e4-656a-41b1-9c7c-a7c5ad5b01fc') called_func=foo elapsed=datetime.timedelta(microseconds=505970) returned_adata_id=4940502352; ```. </details>. <details>; <summary>More complicated example with argument value logging</summary>. ```python; from anndata import AnnData; from copy import copy; from datetime import datetime; from functools import wraps; import inspect; from itertools import chain; from structlog import get_logger; from time import sleep; import uuid. logger = get_logger(). def logged(logged_args=None):; """"""; Params; ------; logged_args : list[str], optional (default: `None`); Names of arguments to log.; """"""; if logged_args is None:; logged_args = []; def logged_decorator(func):; argnames = inspect.getfullargspec(func).args; @wraps(func); def func_wrapper(*args, **kwargs):; call_id = uuid.uuid4() # So we can always match call start with call end; logged_params = {}. for param, val in chain(zip(argnames, args), kwargs.items()):; if type(val) is AnnData:; logged_params[param] = id(val); elif param in logged_args:; logged_params[param] = copy(val) # Probably need to consider how these values get logged. logger.msg(""call"", called_func=func.__name__,; logged_args=logged_params, call_id=call_id). t0 = datetime.now(); output = func(*args, **kwargs); dt = datetime.now() - t0. call_finish_record = dict(; called_func=func.__",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/472#issuecomment-463117273:2466,wrap,wraps,2466,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472#issuecomment-463117273,1,['wrap'],['wraps']
Integrability,"everyone agrees it’s objectively bad 😉. therefore the characters “jet” should only appear in the commit message “jettisoned bad colormap defaults”. * https://jakevdp.github.io/blog/2014/10/16/how-bad-is-your-colormap/; * http://cresspahl.blogspot.de/2012/03/expanded-control-of-octaves-colormap.html; * http://stats.stackexchange.com/questions/223315/why-use-colormap-viridis-over-jet; * https://eagereyes.org/basics/rainbow-color-map; * https://courses.washington.edu/engageuw/why-you-should-dump-the-rainbow/; * http://researchweb.watson.ibm.com/people/l/lloydt/color/color.HTM. i’ll do it if you want, and i’ll make sure contrast and distinction is preserved in all figures",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3:104,message,message,104,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3,1,['message'],['message']
Integrability,"exception:. /omics/groups/OE0540/internal/B260/users/olga/.conda/envs/10x_analysis/lib/python3.8/site-packages/joblib/_parallel_backends.py in __call__(self, *args, **kwargs); 593 def __call__(self, *args, **kwargs):; 594 try:; --> 595 return self.func(*args, **kwargs); 596 except KeyboardInterrupt as e:; 597 # We capture the KeyboardInterrupt and reraise it as. /omics/groups/OE0540/internal/B260/users/olga/.conda/envs/10x_analysis/lib/python3.8/site-packages/joblib/parallel.py in __call__(self); 260 # change the default number of processes to -1; 261 with parallel_backend(self._backend, n_jobs=self._n_jobs):; --> 262 return [func(*args, **kwargs); 263 for func, args, kwargs in self.items]; 264 . /omics/groups/OE0540/internal/B260/users/olga/.conda/envs/10x_analysis/lib/python3.8/site-packages/joblib/parallel.py in <listcomp>(.0); 260 # change the default number of processes to -1; 261 with parallel_backend(self._backend, n_jobs=self._n_jobs):; --> 262 return [func(*args, **kwargs); 263 for func, args, kwargs in self.items]; 264 . ValueError: cannot assign slice from input of different size```. #### Versions; ```. <details>. here is the error from ` sc.logging.print_versions()` ; ```pytb; ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); <ipython-input-81-c71c26e11b3b> in <module>; ----> 1 sc.logging.print_versions(). ~/.local/lib/python3.8/site-packages/scanpy/logging.py in print_versions(file); 178 print_versions(); 179 else:; --> 180 session_info.show(; 181 dependencies=True,; 182 html=False,. ~/.local/lib/python3.8/site-packages/session_info/main.py in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes); 207 for mod_name in clean_modules:; 208 mod_names.append(mod_name); --> 209 mod = sys.modules[mod_name]; 210 # Since modules use different attribute names to store version info,; 211 # try the most common ones. KeyError: 'dask'. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2472:6451,depend,dependencies,6451,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2472,2,['depend'],['dependencies']
Integrability,"exists on the main branch of scanpy. ### What happened?. When I try to plot a scatter plot color coded by categorical data, it still output the image, but the legend does not include colors. Additionally, if i use continuous data as the key for plotting, the code executes as expected. Thanks!. ![image](https://github.com/scverse/scanpy/assets/43973217/8c1b0a03-3c0b-4452-b759-9cf588b45c53). ### Minimal code sample. ```python; sc.pl.scatter(adata, x='total_counts', y='n_genes_by_counts', color='reference', show=True); ```. ### Error output. ```pytb; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); Cell In[37], line 1; ----> 1 sc.pl.scatter(adata, x='total_counts', y='n_genes_by_counts', color='reference', show=True). File /project/hipaa_ycheng11lab/atlas/CAMR2024/py311env/lib/python3.11/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /project/hipaa_ycheng11lab/atlas/CAMR2024/py311env/lib/python3.11/site-packages/scanpy/plotting/_anndata.py:166, in scatter(adata, x, y, color, use_raw, layers, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, legend_fontoutline, color_map, palette, frameon, right_margin, left_margin, size, marker, title, show, save, ax); 160 raise ValueError(""Either provide a `basis` or `x` and `y`.""); 161 if (; 162 (x in adata.obs.keys() or x in var_index); 163 and (y in adata.obs.keys() or y in var_index); 164 and (color is None or color in adata.obs.keys() or color in var_index); 165 ):; --> 166 return _scatter_obs(**args); 167 if (; 168 (x in adata.var.keys() or x in adata.obs.index); 169 and (y in ada",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3102:1228,wrap,wraps,1228,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3102,1,['wrap'],['wraps']
Integrability,"from ._settings import settings, Verbosity; ---> 14 from . import tools as tl; 15 from . import preprocessing as pp; 16 from . import plotting as pl. ~/miniconda3/envs/flng/lib/python3.8/site-packages/scanpy/tools/__init__.py in <module>; 15 from ._leiden import leiden; 16 from ._louvain import louvain; ---> 17 from ._sim import sim; 18 from ._score_genes import score_genes, score_genes_cell_cycle; 19 from ._dendrogram import dendrogram. ~/miniconda3/envs/flng/lib/python3.8/site-packages/scanpy/tools/_sim.py in <module>; 21 from anndata import AnnData; 22 ; ---> 23 from .. import _utils, readwrite, logging as logg; 24 from .._settings import settings; 25 from .._compat import Literal. ~/miniconda3/envs/flng/lib/python3.8/site-packages/scanpy/readwrite.py in <module>; 8 import pandas as pd; 9 from matplotlib.image import imread; ---> 10 import tables; 11 import anndata; 12 from anndata import (. ModuleNotFoundError: No module named 'tables'. ```. The messages when updating anndata:; ```; The following packages will be REMOVED:. pytables-3.6.1-py38h9f153d1_1. The following packages will be UPDATED:. anndata 0.7.6-py38h578d9bd_0 --> 0.8.0-py38h578d9bd_0; ca-certificates pkgs/main::ca-certificates-2022.4.26-~ --> conda-forge::ca-certificates-2022.5.18.1-ha878542_0; h5py 2.10.0-nompi_py38h513d04c_102 --> 3.6.0-nompi_py38hfbb2109_100; hdf5 1.10.5-nompi_h5b725eb_1114 --> 1.12.1-nompi_h2750804_100. The following packages will be SUPERSEDED by a higher-priority channel:. certifi pkgs/main::certifi-2022.5.18.1-py38h0~ --> conda-forge::certifi-2022.5.18.1-py38h578d9bd_0; openssl pkgs/main::openssl-1.1.1o-h7f8727e_0 --> conda-forge::openssl-1.1.1o-h166bdaf_0. Proceed ([y]/n)? y. Downloading and Extracting Packages; keyutils-1.6.1 | 115 KB | ############################################################################# | 100% ; h5py-3.6.0 | 1.4 MB | ############################################################################# | 100% ; cached_property-1.5. | 11 KB | ###############",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2265:1400,message,messages,1400,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2265,1,['message'],['messages']
Integrability,"fueled by reading passively in the bioconda channel for years now and memorizing this rule of thumb regarding where to put recipes:. Anything bio-specific --> bioconda; Anything else --> conda-forge . If this does not hold true (anymore?), @bgruening , I believe that one could still stay with conda-forge and instead try to maintain own biocontainers (need to check with the folks there if uploading would be fine for them etc pp). . >The documentation for bioconda has been incomplete and out of date for years. It could be better, but most of the points are still valid and with some help from the community recipes are still created fine ;-) . >conda-forge autoupdates recipes. When we make a pip release, a conda-forge release is automatically generated. Bioconda-bot does the same for you ;-) . >bioconda packages can depend on conda-forge packages, but not the other way around (last I checked at least). If we go on >bioconda all our dependents do too – this could make it extremely painful to do a migration to bioconda. Thats not the case: E.g. when you move `scanpy` over, the libraries that are not bio related, can stay on conda-forge. That way, resolving will work. I am really not sure if the resolving will not take other channels into account, unless there is different versions of packages on various channels, e.g. a library both on conda-forge and bioconda which would then be handled by channel priorities. >All of our dependencies are on conda-forge. Thats the case for the majority of bio tools - most rely on general purpose tools ;-) . >Fewer channels to search means easier, faster environment solving. `mamba` can help you here, at least for most of the conda recipes I have used (some have hundreds of dependencies in total, especially in multi-tool environments), I didn't notice that much of a difference between using 1 - 2 channels ❓ . And thanks all for the ongoing discussion, still learning things here and also getting new perspectives on the general topic here 👍🏻",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2281#issuecomment-1161394817:1640,depend,dependencies,1640,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2281#issuecomment-1161394817,4,['depend'],['dependencies']
Integrability,"g codes:. ```; sc.pp.normalize_per_cell(adata, counts_per_cell_after=1e4); sc.pp.log1p(adata); adata.raw = adata. sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=4, min_disp=0.5); sc.pl.highly_variable_genes(adata). adata = adata[:, adata.var['highly_variable']]; sc.pp.regress_out(adata, ['n_counts', 'percent_mito']); sc.pp.scale(adata, max_value=10). sc.tl.pca(adata, svd_solver='arpack'); sc.pl.pca(adata, color='cell_types'). sc.pp.neighbors(adata, n_neighbors=20, n_pcs=20); sc.tl.louvain(adata). sc.tl.umap(adata); sc.tl.diffmap(adata, n_comps=15). adata.write(""Scanpy.h5ad""); ```. Then reload into another session and tring to plot:; ```; adata2 = anndata.read_h5ad(""Scanpy.h5ad"", backed = 'r'); sc.pl.umap(adata, color=['louvain', ""MMP3""]); ```. However the bug come out as:; ```; sc.pl.umap(adata, color=['louvain', ""MMP3""]); File ""/Users/temp/anaconda/envs/python36/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 29, in umap; return plot_scatter(adata, basis='umap', **kwargs); File ""/Users/temp/anaconda/envs/python36/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 280, in plot_scatter; color_vector = color_vector[order]; File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper; File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper; File ""/Users/temp/anaconda/envs/python36/lib/python3.6/site-packages/h5py/_hl/dataset.py"", line 476, in __getitem__; selection = sel.select(self.shape, args, dsid=self.id); File ""/Users/temp/anaconda/envs/python36/lib/python3.6/site-packages/h5py/_hl/selections.py"", line 72, in select; sel[arg]; File ""/Users/temp/anaconda/envs/python36/lib/python3.6/site-packages/h5py/_hl/selections.py"", line 212, in __getitem__; raise TypeError(""PointSelection __getitem__ only works with bool arrays""); TypeError: PointSelection __getitem__ only works with bool arrays; ```. The version of packages:; ```; 1.4 for scanpy; 0.6.22 for anndata; ```. Thanks for the help.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/777:1317,wrap,wrapper,1317,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/777,2,['wrap'],['wrapper']
Integrability,"g.print_versions(); sc.settings.set_figure_params(dpi=80, frameon=False, figsize=(3, 3), facecolor=""white""). adata_ref = sc.datasets.pbmc3k_processed(); adata = sc.datasets.pbmc68k_reduced(). var_names = adata_ref.var_names.intersection(adata.var_names); adata_ref = adata_ref[:, var_names]; adata = adata[:, var_names]. # start from scratch; del adata.obs[""louvain""]; adata.uns = {}; adata_ref.uns = {}. # example code for ingest function:; sc.pp.neighbors(adata_ref); sc.tl.umap(adata_ref); sc.tl.ingest(adata, adata_ref, obs=""louvain""); ```. ### Error output. ```pytb; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); Cell In[11], line 23; 21 sc.pp.neighbors(adata_ref); 22 sc.tl.umap(adata_ref); ---> 23 sc.tl.ingest(adata, adata_ref, obs=""louvain""). File ~/miniconda3/envs/sc/lib/python3.12/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File ~/miniconda3/envs/sc/lib/python3.12/site-packages/scanpy/tools/_ingest.py:141, in ingest(adata, adata_ref, obs, embedding_method, labeling_method, neighbors_key, inplace, **kwargs); 138 labeling_method = labeling_method * len(obs); 140 ing = Ingest(adata_ref, neighbors_key); --> 141 ing.fit(adata); 143 for method in embedding_method:; 144 ing.map_embedding(method). File ~/miniconda3/envs/sc/lib/python3.12/site-packages/scanpy/tools/_ingest.py:404, in Ingest.fit(self, adata_new); 401 self._obsm = _DimDict(adata_new.n_obs, axis=0); 403 self._adata_new = adata_new; --> 404 self._obsm[""rep""] = self._same_rep(). File ~/miniconda3/envs/sc/lib/python3.12/site-packages/scanpy/tools/_ingest.py:371, in Ingest._same_rep(self); 369 adata = self._adata_new; 3",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3074:1764,wrap,wraps,1764,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3074,1,['wrap'],['wraps']
Integrability,h code '132'.; ##[section]Finishing: PyTest; ```. ### Versions. <details>. ```; anndata 0.10.5.post1; annoy 1.17.3; array_api_compat 1.4.1; asciitree 0.3.3; attrs 23.2.0; cfgv 3.4.0; click 8.1.7; cloudpickle 3.0.0; contourpy 1.2.0; coverage 7.4.1; cycler 0.12.1; dask 2024.2.0; dask-glm 0.3.2; dask-ml 2023.3.24; decorator 5.1.1; Deprecated 1.2.14; distlib 0.3.8; distributed 2024.2.0; exceptiongroup 1.2.0; fasteners 0.19; fbpca 1.0; filelock 3.13.1; fonttools 4.49.0; fsspec 2024.2.0; future 0.18.3; geosketch 1.2; get-annotations 0.1.2; graphtools 1.5.3; h5py 3.10.0; harmonypy 0.0.9; identify 2.5.35; igraph 0.11.4; imageio 2.34.0; importlib-metadata 7.0.1; importlib-resources 6.1.1; iniconfig 2.0.0; intervaltree 3.1.0; Jinja2 3.1.3; joblib 1.3.2; kiwisolver 1.4.5; lazy_loader 0.3; legacy-api-wrap 1.4; leidenalg 0.10.2; llvmlite 0.42.0; locket 1.0.0; magic-impute 3.0.0; MarkupSafe 2.1.5; matplotlib 3.8.3; msgpack 1.0.7; multipledispatch 1.0.0; natsort 8.4.0; networkx 3.2.1; nodeenv 1.8.0; numba 0.59.0; numcodecs 0.12.1; numpy 1.26.4; packaging 23.2; pandas 2.0.3; partd 1.4.1; patsy 0.5.6; pbr 6.0.0; pillow 10.2.0; pip 24.0; platformdirs 4.2.0; pluggy 1.4.0; pre-commit 3.6.2; profimp 0.1.0; psutil 5.9.8; PyGSP 0.5.1; pynndescent 0.5.11; pyparsing 3.1.1; pytest 8.0.1; pytest-mock 3.12.0; pytest-nunit 1.0.6; python-dateutil 2.8.2; pytz 2024.1; PyYAML 6.0.1; scanorama 1.7.4; scanpy 1.10.0.dev220+g534145f6; scikit-image 0.22.0; scikit-learn 1.4.1.post1; scikit-misc 0.3.1; scipy 1.12.0; scprep 1.2.3; seaborn 0.13.2; session-info 1.0.0; setuptools 58.1.0; setuptools-scm 8.0.4; six 1.16.0; sortedcontainers 2.4.0; sparse 0.15.1; statsmodels 0.14.1; stdlib-list 0.10.0; tasklogger 1.2.0; tblib 3.0.0; texttable 1.7.0; threadpoolctl 3.3.0; tifffile 2024.2.12; tomli 2.0.1; toolz 0.12.1; tornado 6.4; tqdm 4.66.2; typing_extensions 4.9.0; tzdata 2024.1; umap-learn 0.5.5; urllib3 2.2.1; virtualenv 20.25.0; wheel 0.42.0; wrapt 1.16.0; zarr 2.17.0; zict 3.0.0; zipp 3.17.0; ```. </details>,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2866:8791,wrap,wrapt,8791,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2866,1,['wrap'],['wrapt']
Integrability,"he functionalities and setup and it does look very nice!. - BCR makes sense to add, there seems to be generally less happening in this space in single-cell though right now, compared to TCR. Would be good to have somebody on board who actually works on this data.; - [tcellmatch](https://github.com/theislab/tcellmatch)'s primary purpose is specificity prediction, this could be easily added ontop of this, I will look into your data structure and will think about the necessary changes. I am in the process of making this code public anyway, hopefully next week or so.; - You mentioned distance metrics, this is definitely an interesting and relevant area, in [tcellmatch](https://github.com/theislab/tcellmatch), we implicitly use 1. manhatten distances, 2. euclidian distances in BLOSUM embedding and 3. learned embedding distances, 2. and maybe 3. could be potentially integrated, would be worth discussing in any case.; - Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? These anticipated use cases would determine how and whether this makes sense i think.; - Potentially additionally relevant: An integration with dextramer counts to ""stain"" TCR specificity? There is the purely numeric, standard multi-modal single-cell, nature to this data that can be covered by standard scanpy work flows. This data is especially useful in the context of clonotypes etc which then would require additional functionalities, which could be built on what you have here. I have been looking into this type of analysis a lot in context of tcellmatch. Would be to contribute but also happy to see what other people do here, too!. Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though!. Best,; David",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1163#issuecomment-613297254:966,Integrat,Integration,966,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163#issuecomment-613297254,5,"['Integrat', 'integrat']","['Integration', 'integrate', 'integration']"
Integrability,"heatmap, return_data, show, save, ax); 1057 if as_heatmap:; 1058 img = ax.imshow(; -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map; 1060 ); 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs); 1599 def inner(ax, *args, data=None, **kwargs):; 1600 if data is None:; -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs); 1602 ; 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs); 367 f""%(removal)s. If any parameter follows {name!r}, they ""; 368 f""should be pass as keyword, not positionally.""); --> 369 return func(*args, **kwargs); 370 ; 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs); 367 f""%(removal)s. If any parameter follows {name!r}, they ""; 368 f""should be pass as keyword, not positionally.""); --> 369 return func(*args, **kwargs); 370 ; 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs); 5669 resample=resample, **kwargs); 5670 ; -> 5671 im.set_data(X); 5672 im.set_alpha(alpha); 5673 if im.get_clip_path() is None:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/image.py in set_data(self, A); 683 not np.can_cast(self._A.dtype, float, ""same_kind"")):; 684 raise TypeError(""Image data of dtype {} cannot be converted to ""; --> 685 ""float"".format(self._A.dtype)); 686 ; 687 if not (self._A.ndim == 2. TypeError: Image data of dtype object cannot be converted to float; ```; Plotting a heatmap with `sc.pl.heatmap` works. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; ```; scanpy==1.4.5.dev137+ge46f89b anndata==0.6.22.post2.dev73+g00b4b91 umap==0.3.8 numpy==1.17.3 scipy==1.2.1 pandas==0.25.2 scikit-learn==0.20.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/953:4749,wrap,wrapper,4749,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953,1,['wrap'],['wrapper']
Integrability,"heatmap, return_data, show, save, ax); 1057 if as_heatmap:; 1058 img = ax.imshow(; -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map; 1060 ); 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs); 1599 def inner(ax, *args, data=None, **kwargs):; 1600 if data is None:; -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs); 1602 ; 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs); 367 f""%(removal)s. If any parameter follows {name!r}, they ""; 368 f""should be pass as keyword, not positionally.""); --> 369 return func(*args, **kwargs); 370 ; 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs); 367 f""%(removal)s. If any parameter follows {name!r}, they ""; 368 f""should be pass as keyword, not positionally.""); --> 369 return func(*args, **kwargs); 370 ; 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs); 5669 resample=resample, **kwargs); 5670 ; -> 5671 im.set_data(X); 5672 im.set_alpha(alpha); 5673 if im.get_clip_path() is None:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/image.py in set_data(self, A); 688 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):; 689 raise TypeError(""Invalid shape {} for image data""; --> 690 .format(self._A.shape)); 691 ; 692 if self._A.ndim == 3:. TypeError: Invalid shape (3, 43, 1) for image data; ```; If I convert the `adata.X` to sparse matrix format, I have the following error:; ```python; adata.X = sci.sparse.csr_matrix(adata.X); sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']); ```. ```pytb; ---------------------------------------------------------------------------; Typ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/953:2234,wrap,wrapper,2234,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953,1,['wrap'],['wrapper']
Integrability,"hey all, thanks for feedback. @LuckyMD I totally see the point but disagree; > i guess one of the difficult things to actually using this is tuning the inter layer weight. . exactly and this will be different (I think?) across different multi modal tech integration (e.g. cite-seq, or spatial etc.) and e.g. for spatial it will potentially different across tissues (some tissues have more structure spatial/image features graphs than others). . Nervetheless, I think it would be very empowering to users to be able to play around with this. It is ""just"" another knob to tune that would nonetheless enrich the analysis experience imho",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1818#issuecomment-830652212:254,integrat,integration,254,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818#issuecomment-830652212,2,['integrat'],['integration']
Integrability,"hmm, I’m not sure if it’s possible to require versions depending on the platform. where’s the h5py issue about this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/454#issuecomment-459034769:55,depend,depending,55,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454#issuecomment-459034769,1,['depend'],['depending']
Integrability,hon 8.22.2 pypi_0 pypi; ipywidgets 8.1.2 pypi_0 pypi; isoduration 20.11.0 pypi_0 pypi; jedi 0.19.1 pypi_0 pypi; jinja2 3.1.3 py311haa95532_0 ; joblib 1.3.2 pypi_0 pypi; json5 0.9.22 pypi_0 pypi; jsonpointer 2.4 pypi_0 pypi; jsonschema 4.21.1 pypi_0 pypi; jsonschema-specifications 2023.12.1 pypi_0 pypi; jupyter-client 8.6.1 pypi_0 pypi; jupyter-core 5.7.2 pypi_0 pypi; jupyter-events 0.9.1 pypi_0 pypi; jupyter-lsp 2.2.4 pypi_0 pypi; jupyter-server 2.13.0 pypi_0 pypi; jupyter-server-terminals 0.5.3 pypi_0 pypi; jupyter_client 8.6.0 py311haa95532_0 ; jupyter_core 5.5.0 py311haa95532_0 ; jupyter_events 0.8.0 py311haa95532_0 ; jupyter_server 2.10.0 py311haa95532_0 ; jupyter_server_terminals 0.4.4 py311haa95532_1 ; jupyterlab 4.1.5 pypi_0 pypi; jupyterlab-pygments 0.3.0 pypi_0 pypi; jupyterlab-server 2.25.4 pypi_0 pypi; jupyterlab-widgets 3.0.10 pypi_0 pypi; jupyterlab_pygments 0.1.2 py_0 ; jupyterlab_server 2.25.1 py311haa95532_0 ; kiwisolver 1.4.5 pypi_0 pypi; lazy-loader 0.3 pypi_0 pypi; legacy-api-wrap 1.4 pypi_0 pypi; leidenalg 0.10.2 pypi_0 pypi; libffi 3.4.4 hd77b12b_0 ; libsodium 1.0.18 h62dcd97_0 ; llvmlite 0.42.0 pypi_0 pypi; m2w64-bwidget 1.9.10 2 ; m2w64-bzip2 1.0.6 6 ; m2w64-expat 2.1.1 2 ; m2w64-fftw 3.3.4 6 ; m2w64-flac 1.3.1 3 ; m2w64-gcc-libgfortran 5.3.0 6 ; m2w64-gcc-libs 5.3.0 7 ; m2w64-gcc-libs-core 5.3.0 7 ; m2w64-gettext 0.19.7 2 ; m2w64-gmp 6.1.0 2 ; m2w64-gsl 2.1 2 ; m2w64-libiconv 1.14 6 ; m2w64-libjpeg-turbo 1.4.2 3 ; m2w64-libogg 1.3.2 3 ; m2w64-libpng 1.6.21 2 ; m2w64-libsndfile 1.0.26 2 ; m2w64-libsodium 1.0.10 2 ; m2w64-libtiff 4.0.6 2 ; m2w64-libvorbis 1.3.5 2 ; m2w64-libwinpthread-git 5.0.0.4634.697f757 2 ; m2w64-libxml2 2.9.3 4 ; m2w64-mpfr 3.1.4 4 ; m2w64-openblas 0.2.19 1 ; m2w64-pcre 8.38 2 ; m2w64-speex 1.2rc2 3 ; m2w64-speexdsp 1.2rc3 3 ; m2w64-tcl 8.6.5 3 ; m2w64-tk 8.6.5 3 ; m2w64-tktable 2.10 5 ; m2w64-wineditline 2.101 5 ; m2w64-xz 5.2.2 2 ; m2w64-zeromq 4.1.4 2 ; m2w64-zlib 1.2.8 10 ; markupsafe 2.1.5 pypi_0 pypi; matplotlib 3.8.3,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2969:6955,wrap,wrap,6955,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2969,1,['wrap'],['wrap']
Integrability,"hon3.7/site-packages (from scanpy) (0.3.10); Requirement already satisfied: numpy>=1.13.3 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from pandas>=0.21->scanpy) (1.17.4); Requirement already satisfied: pytz>=2017.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from pandas>=0.21->scanpy) (2019.3); Requirement already satisfied: python-dateutil>=2.6.1 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from pandas>=0.21->scanpy) (2.8.1); Requirement already satisfied: pyparsing>=2.0.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from packaging->scanpy) (2.4.5); Requirement already satisfied: six in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from packaging->scanpy) (1.13.0); Requirement already satisfied: setuptools in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from legacy-api-wrap->scanpy) (42.0.2.post20191203); Requirement already satisfied: get-version>=2.0.4 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from legacy-api-wrap->scanpy) (2.1); Requirement already satisfied: llvmlite>=0.30.0dev0 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from numba>=0.41.0->scanpy) (0.30.0); Requirement already satisfied: zipp>=0.5 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from importlib-metadata>=0.7; python_version < ""3.8""->scanpy) (0.6.0); Requirement already satisfied: numexpr>=2.6.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from tables->scanpy) (2.7.0); Requirement already satisfied: decorator>=4.3.0 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from networkx->scanpy) (4.4.1); Requirement already satisfied: kiwisolver>=1.0.1 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from matplotlib==3.0.*->scanpy) (1.1.0); Requirement already satisfied: cycler>=0.10 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from matplotlib==3.0.*->scanpy) (0.10.0); Requirement already satisfied: more-itertools in /home/tsundoku/anaconda3/lib/python3.7/site-pa",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:4912,wrap,wrap,4912,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452,2,['wrap'],['wrap']
Integrability,how would you avoid the context manager?. it’s either that or wrapping try/catch around every single use of the `writedir`.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/50#issuecomment-346763343:62,wrap,wrapping,62,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/50#issuecomment-346763343,1,['wrap'],['wrapping']
Integrability,"how?). ### Minimal code sample. Original error upon running highly variable genes; ```python; <details>. ---------------------------------------------------------------------------; ModuleNotFoundError Traceback (most recent call last); File /opt/miniconda3/envs/scanpyenvt/lib/python3.12/site-packages/scanpy/preprocessing/_highly_variable_genes.py:66, in _highly_variable_genes_seurat_v3(adata, flavor, layer, n_top_genes, batch_key, check_values, span, subset, inplace); 65 try:; ---> 66 from skmisc.loess import loess; 67 except ImportError:. ModuleNotFoundError: No module named 'skmisc'. During handling of the above exception, another exception occurred:. ImportError Traceback (most recent call last); Cell In[14], line 1; ----> 1 doublet_training_data = sc.pp.highly_variable_genes(adata, n_top_genes=6000, subset=True, flavor='seurat_v3'); 2 doublet_training_data. File /opt/miniconda3/envs/scanpyenvt/lib/python3.12/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /opt/miniconda3/envs/scanpyenvt/lib/python3.12/site-packages/scanpy/preprocessing/_highly_variable_genes.py:655, in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key, check_values); 653 sig = signature(_highly_variable_genes_seurat_v3); 654 n_top_genes = cast(int, sig.parameters[""n_top_genes""].default); --> 655 return _highly_variable_genes_seurat_v3(; 656 adata,; 657 flavor=flavor,; 658 layer=layer,; 659 n_top_genes=n_top_genes,; 660 batch_key=batch_key,; 661 check_values=check_values,; 662 span=span,; 663 subset=subset,; 664 inplace=inplace,; 665 ); 667 cutoff = _Cutoffs.validate(; 668 n_top_genes=n_top_g",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3144:2699,wrap,wrapper,2699,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3144,1,['wrap'],['wrapper']
Integrability,"https://github.com/theislab/scanpy/blob/63b42e4bff46a9e1386abfede4adeef3a8100c7b/pyproject.toml#L65. > The sinfo package has changed name and is now called session_info to become more discoverable and self-explanatory. The sinfo PyPI package will be kept around to avoid breaking old installs and you can downgrade to 0.3.2 if you want to use it without seeing this message. For the latest features and bug fixes, please install session_info instead. The usage and defaults also changed slightly, so please review the latest README at https://gitlab.com/joelostblom/session_info.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1996:366,message,message,366,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1996,1,['message'],['message']
Integrability,"https://scanpy-tutorials.readthedocs.io/en/latest/integrating-data-using-ingest.html; Hi, this should be relevant.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1847#issuecomment-845420561:50,integrat,integrating-data-using-ingest,50,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1847#issuecomment-845420561,1,['integrat'],['integrating-data-using-ingest']
Integrability,"i guess we could go this route:. ```py; mean_filter = 0.01; cv_filter = 2; nr_pcs = 50. # row normalize ; adata = adata.smp_norm(max_fraction=0.05, mult_with_mean=True); # filter out genes with mean expression < 0.1 and coefficient of variance < ; # cvFilter ; adata = adata.filter_var_cv(mean_filter, cv_filter); # compute zscore of filtered matrix ; Xz = zscore(adata.X); # PCA ; Xpca = pca(Xz, nr_comps=nr_pcs); # update dictionary; adata['Xpca'] = Xpca; sett.m(0, 'Xpca has shape', Xpca.shape[0], 'x', Xpca.shape[1]); print(adata.X); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/4#issuecomment-278579015:25,rout,route,25,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/4#issuecomment-278579015,1,['rout'],['route']
Integrability,"ib/python3.7/site-packages/scanpy/tools/_ingest.py in _init_pynndescent(self, distances); 284 ; 285 first_col = np.arange(distances.shape[0])[:, None]; --> 286 init_indices = np.hstack((first_col, np.stack(distances.tolil().rows))); 287 ; 288 self._nnd_idx = NNDescent(. <__array_function__ internals> in stack(*args, **kwargs). ~/.local/lib/python3.7/site-packages/numpy/core/shape_base.py in stack(arrays, axis, out); 425 shapes = {arr.shape for arr in arrays}; 426 if len(shapes) != 1:; --> 427 raise ValueError('all input arrays must have the same shape'); 428 ; 429 result_ndim = arrays[0].ndim + 1. ValueError: all input arrays must have the same shape; ```. ### Versions. ```; WARNING: If you miss a compact list, please try `print_header`!; The `sinfo` package has changed name and is now called `session_info` to become more discoverable and self-explanatory. The `sinfo` PyPI package will be kept around to avoid breaking old installs and you can downgrade to 0.3.2 if you want to use it without seeing this message. For the latest features and bug fixes, please install `session_info` instead. The usage and defaults also changed slightly, so please review the latest README at https://gitlab.com/joelostblom/session_info.; -----; anndata 0.8.0; scanpy 1.8.1; sinfo 0.3.4; -----; PIL 7.1.2; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; cffi 1.15.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; decorator 5.1.1; google NA; h5py 3.7.0; ipykernel 5.3.0; ipython_genutils 0.2.0; ipywidgets 7.5.1; jedi 0.17.0; joblib 1.1.0; kiwisolver 1.2.0; llvmlite 0.38.1; matplotlib 3.3.0; mpl_toolkits NA; natsort 8.1.0; nbinom_ufunc NA; netifaces 0.11.0; numba 0.55.2; numexpr 2.8.3; numpy 1.20.0; packaging 21.3; pandas 1.1.3; parso 0.7.0; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.5; psutil 5.9.0; ptyprocess 0.6.0; pygments 2.6.1; pyparsing 3.0.8; pytz 2022.1; ruamel NA; scipy 1.7.0; seaborn 0.11.2; setuptools 62.1.0; simplejson 3.17.6; six 1.16.0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2635:3590,message,message,3590,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2635,1,['message'],['message']
Integrability,"ign"", experiment_dir / ""experimental_design.tsv"",; 44 ); 45 _download(. ~/anaconda3/envs/sc-py/lib/python3.6/site-packages/scanpy/readwrite.py in _download(url, path); 877 ; 878 try:; --> 879 urlretrieve(url, str(path), reporthook=update_to); 880 except Exception:; 881 # Make sure file doesn’t exist half-downloaded. ~/anaconda3/envs/sc-py/lib/python3.6/urllib/request.py in urlretrieve(url, filename, reporthook, data); 246 url_type, path = splittype(url); 247 ; --> 248 with contextlib.closing(urlopen(url, data)) as fp:; 249 headers = fp.info(); 250 . ~/anaconda3/envs/sc-py/lib/python3.6/urllib/request.py in urlopen(url, data, timeout, cafile, capath, cadefault, context); 221 else:; 222 opener = _opener; --> 223 return opener.open(url, data, timeout); 224 ; 225 def install_opener(opener):. ~/anaconda3/envs/sc-py/lib/python3.6/urllib/request.py in open(self, fullurl, data, timeout); 530 for processor in self.process_response.get(protocol, []):; 531 meth = getattr(processor, meth_name); --> 532 response = meth(req, response); 533 ; 534 return response. ~/anaconda3/envs/sc-py/lib/python3.6/urllib/request.py in http_response(self, request, response); 640 if not (200 <= code < 300):; 641 response = self.parent.error(; --> 642 'http', request, response, code, msg, hdrs); 643 ; 644 return response. ~/anaconda3/envs/sc-py/lib/python3.6/urllib/request.py in error(self, proto, *args); 568 if http_err:; 569 args = (dict, 'default', 'http_error_default') + orig_args; --> 570 return self._call_chain(*args); 571 ; 572 # XXX probably also want an abstract factory that knows when it makes. ~/anaconda3/envs/sc-py/lib/python3.6/urllib/request.py in _call_chain(self, chain, kind, meth_name, *args); 502 for handler in handlers:; 503 func = getattr(handler, meth_name); --> 504 result = func(*args); 505 if result is not None:; 506 return result. ~/anaconda3/envs/sc-py/lib/python3.6/urllib/request.py in http_error_default(self, req, fp, code, msg, hdrs); 648 class HTTPDefaultErrorHandler(Ba",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1221:2159,protocol,protocol,2159,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1221,1,['protocol'],['protocol']
Integrability,"ile ""h5py/h5d.pyx"", line 280, in h5py.h5d.DatasetID.write; File ""h5py/_proxy.pyx"", line 145, in h5py._proxy.dset_rw; File ""h5py/_conv.pyx"", line 444, in h5py._conv.str2vlen; File ""h5py/_conv.pyx"", line 95, in h5py._conv.generic_converter; File ""h5py/_conv.pyx"", line 249, in h5py._conv.conv_str2vlen; TypeError: Can't implicitly convert non-string objects to strings. The above exception was the direct cause of the following exception:; Traceback (most recent call last):; File ""integration.py"", line 66, in <module>; adata.write_h5ad('Integrated.h5ad'); File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_core/anndata.py"", line 1918, in write_h5ad; _write_h5ad(; File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_io/h5ad.py"", line 98, in write_h5ad; write_elem(f, ""obs"", adata.obs, dataset_kwargs=dataset_kwargs); File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_io/utils.py"", line 214, in func_wrapper; return func(elem, key, val, *args, **kwargs); File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_io/specs/registry.py"", line 175, in write_elem; _REGISTRY.get_writer(dest_type, t, modifiers)(f, k, elem, *args, **kwargs); File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_io/specs/registry.py"", line 24, in wrapper; result = func(g, k, *args, **kwargs); File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_io/specs/methods.py"", line 514, in write_dataframe; write_elem(group, colname, series._values, dataset_kwargs=dataset_kwargs); File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_io/utils.py"", line 220, in func_wrapper; raise type(e)(; TypeError: Can't implicitly convert non-string objects to strings. Above error raised while writing key 'orig.ident' of <class 'h5py._hl.group.Group'> to /; /home/joyzheng/.conda/envs/cellrank/lib/python3.8/tempfile.py:818: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmpzjzxl3q5'>; _warnings.warn(warn_message, ResourceWarning)```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2432:3797,wrap,wrapper,3797,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2432,1,['wrap'],['wrapper']
Integrability,"ing, knn_dist_metric, normalize_variance, log_transform, mean_center, n_prin_comps, use_approx_neighbors, get_doublet_neighbor_parents, n_neighbors, threshold, verbose, copy, random_state); 279 adata.uns[""scrublet""][""batched_by""] = batch_key; 281 else:; --> 282 scrubbed = _run_scrublet(adata_obs, adata_sim); 284 # Copy outcomes to input object from our processed version; 286 adata.obs[""doublet_score""] = scrubbed[""obs""][""doublet_score""]. File C:\ProgramData\Anaconda3\envs\dl\lib\site-packages\scanpy\preprocessing\_scrublet\__init__.py:204, in scrublet.<locals>._run_scrublet(ad_obs, ad_sim); 201 # HVG process needs log'd data.; 203 logged = pp.log1p(ad_obs, copy=True); --> 204 pp.highly_variable_genes(logged); 205 ad_obs = ad_obs[:, logged.var[""highly_variable""]].copy(); 207 # Simulate the doublets based on the raw expressions from the normalised; 208 # and filtered object. File C:\ProgramData\Anaconda3\envs\dl\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File C:\ProgramData\Anaconda3\envs\dl\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py:648, in highly_variable_genes(***failed resolving arguments***); 645 del min_disp, max_disp, min_mean, max_mean, n_top_genes; 647 if batch_key is None:; --> 648 df = _highly_variable_genes_single_batch(; 649 adata, layer=layer, cutoff=cutoff, n_bins=n_bins, flavor=flavor; 650 ); 651 else:; 652 df = _highly_variable_genes_batched(; 653 adata, batch_key, layer=layer, cutoff=cutoff, n_bins=n_bins, flavor=flavor; 654 ). File C:\ProgramData\Anaconda3\envs\dl\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py:281, in _highly_variable_genes_single_batch(adata, layer, cutoff, n_bins, flavor)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3070:8660,wrap,wrapper,8660,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3070,1,['wrap'],['wrapper']
Integrability,"input-21-ded14f7730cd> in <module>; 8 zf_48.var.index = zf_48.var[""gene_name""]; 9 ; ---> 10 zf_48.write_h5ad(""/Users/julius/Desktop/zf_48.h5ad""). ~/opt/anaconda3/lib/python3.8/site-packages/anndata/_core/anndata.py in write_h5ad(self, filename, compression, compression_opts, force_dense, as_dense); 1903 filename = self.filename; 1904 ; -> 1905 _write_h5ad(; 1906 Path(filename),; 1907 self,. ~/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_h5ad(filepath, adata, force_dense, as_dense, dataset_kwargs, **kwargs); 110 write_attribute(f, ""raw"", adata.raw, dataset_kwargs=dataset_kwargs); 111 write_attribute(f, ""obs"", adata.obs, dataset_kwargs=dataset_kwargs); --> 112 write_attribute(f, ""var"", adata.var, dataset_kwargs=dataset_kwargs); 113 write_attribute(f, ""obsm"", adata.obsm, dataset_kwargs=dataset_kwargs); 114 write_attribute(f, ""varm"", adata.varm, dataset_kwargs=dataset_kwargs). ~/opt/anaconda3/lib/python3.8/functools.py in wrapper(*args, **kw); 873 '1 positional argument'); 874 ; --> 875 return dispatch(args[0].__class__)(*args, **kw); 876 ; 877 funcname = getattr(func, '__name__', 'singledispatch function'). ~/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_attribute_h5ad(f, key, value, *args, **kwargs); 128 if key in f:; 129 del f[key]; --> 130 _write_method(type(value))(f, key, value, *args, **kwargs); 131 ; 132 . ~/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, key, val, *args, **kwargs); 210 except Exception as e:; 211 parent = _get_parent(elem); --> 212 raise type(e)(; 213 f""{e}\n\n""; 214 f""Above error raised while writing key {key!r} of {type(elem)}"". RuntimeError: Unable to create link (name already exists). Above error raised while writing key 'gene_name' of <class 'h5py._hl.group.Group'> from /. Above error raised while writing key 'gene_name' of <class 'h5py._hl.group.Group'> from /. Above error raised while writing key 'var' of <class 'h5py._hl.files.File'> from /. ```. Am ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1982:5223,wrap,wrapper,5223,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1982,1,['wrap'],['wrapper']
Integrability,integrate with CCA and pyscenic,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/265:0,integrat,integrate,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265,1,['integrat'],['integrate']
Integrability,"inutes: could we not make a submodule rtools? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful.; > . That'd make things a lot easier for many people (including myself 😃), I agree. However. 1) There are (and will be) so many R packages about single cell, so once we open the door, there might be so many requests about these packages so that it'd be difficult to decide what to include and what not to include. The decision might be a bit arbitrary. This is why I suggested a contrib repo, which will have everything users request (as soon as there is someone who is willing to maintain it), in a `use at your own risk` way... 2) There might be several bug reports about rpy2 itself or thin wrappers or R installation or R packages themselves. I was wondering whether this might introduce more maintenance burden, although supported packages will be limited. > The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a scanpy-contrib: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: anndata is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions.; > ; > What do you think?. Alternatively, we can just prepare jupyter notebooks with some Python 3 and some R cells in it (which is super easy via rpy2 magics anyway) for some R packages/functions like mnn or SIMLR and put those in scanpy_usage as a reference for the community. For example:. ![image](https://user-images.githubusercontent.com/1140359",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/125#issuecomment-382002901:1108,wrap,wrappers,1108,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-382002901,1,['wrap'],['wrappers']
Integrability,ion=0.17.2; - tornado=6.4.1; - tqdm=4.66.4; - traitlets=5.14.3; - types-python-dateutil=2.9.0.20240316; - typing-extensions=4.12.2; - typing_extensions=4.12.2; - typing_utils=0.1.0; - tzdata=2024a; - umap-learn=0.5.5; - uri-template=1.3.0; - urllib3=2.2.1; - wcwidth=0.2.13; - webcolors=24.6.0; - webencodings=0.5.1; - websocket-client=1.8.0; - wheel=0.43.0; - xlrd=1.2.0; - xorg-libxau=1.0.11; - xorg-libxdmcp=1.1.3; - xz=5.2.6; - yaml=0.2.5; - zeromq=4.3.5; - zipp=3.19.2; - zlib-ng=2.0.7; - zstd=1.5.6; - pip:; - absl-py==2.1.0; - astunparse==1.6.3; - bcbio-gff==0.7.1; - flatbuffers==24.3.25; - gast==0.5.4; - google-pasta==0.2.0; - grpcio==1.64.1; - keras==3.3.3; - libclang==18.1.1; - markdown==3.6; - markdown-it-py==3.0.0; - mdurl==0.1.2; - ml-dtypes==0.3.2; - namex==0.0.8; - opt-einsum==3.3.0; - optree==0.11.0; - rich==13.7.1; - tensorboard==2.16.2; - tensorboard-data-server==0.7.2; - tensorflow==2.16.1; - tensorflow-io-gcs-filesystem==0.37.0; - termcolor==2.4.0; - werkzeug==3.0.3; - wrapt==1.16.0; ```. The virtual environment on my laptop (successful case):; ```; channels:; - pytorch; - bioconda; - conda-forge; dependencies:; - adjusttext=1.0.4; - anndata=0.10.5.post1; - anyio=3.7.1; - aom=3.5.0; - appnope=0.1.3; - argcomplete=3.3.0; - argh=0.31.2; - argon2-cffi=23.1.0; - argon2-cffi-bindings=21.2.0; - arpack=3.8.0; - array-api-compat=1.4.1; - arrow=1.2.3; - asttokens=2.2.1; - async-lru=2.0.4; - attrs=23.1.0; - babel=2.12.1; - backcall=0.2.0; - backports=1.0; - backports.functools_lru_cache=1.6.5; - beautifulsoup4=4.12.2; - bleach=6.0.0; - blosc=1.21.4; - brotli=1.0.9; - brotli-bin=1.0.9; - brotli-python=1.0.9; - bzip2=1.0.8; - c-ares=1.19.1; - c-blosc2=2.10.2; - ca-certificates=2024.6.2; - cached-property=1.5.2; - cached_property=1.5.2; - cairo=1.18.0; - certifi=2024.6.2; - cffi=1.15.1; - charset-normalizer=3.2.0; - colorama=0.4.6; - colorcet=3.0.1; - colorful=0.5.4; - comm=0.1.4; - contourpy=1.1.0; - cryptography=41.0.4; - cycler=0.11.0; - dav1d=1.2.1; - debugpy=1,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3116:8536,wrap,wrapt,8536,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3116,1,['wrap'],['wrapt']
Integrability,"ioned above, not pip.; hint: See above for details.; Exception information:; Traceback (most recent call last):; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/build/metadata_legacy.py"", line 64, in generate_metadata; call_subprocess(; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/utils/subprocess.py"", line 224, in call_subprocess; raise error; pip._internal.exceptions.InstallationSubprocessError: python setup.py egg_info exited with 1. The above exception was the direct cause of the following exception:. Traceback (most recent call last):; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/cli/base_command.py"", line 160, in exc_logging_wrapper; status = run_func(*args); ^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/cli/req_command.py"", line 247, in wrapper; return func(self, options, args); ^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/commands/install.py"", line 400, in run; requirement_set = resolver.resolve(; ^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/resolver.py"", line 92, in resolve; result = self._result = resolver.resolve(; ^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 481, in resolve; state = resolution.resolve(requirements, max_rounds=max_rounds); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 373, in resolve; failure_causes = self._attempt_to_pin_criterion(name); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", lin",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209:5744,wrap,wrapper,5744,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209,2,['wrap'],['wrapper']
Integrability,"ipp>=0.5 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from importlib-metadata>=0.7; python_version < ""3.8""->scanpy) (0.6.0); Requirement already satisfied: kiwisolver>=1.0.1 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from matplotlib==3.0.*->scanpy) (1.1.0); Requirement already satisfied: python-dateutil>=2.1 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from matplotlib==3.0.*->scanpy) (2.8.1); Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from matplotlib==3.0.*->scanpy) (2.4.5); Requirement already satisfied: cycler>=0.10 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from matplotlib==3.0.*->scanpy) (0.10.0); Requirement already satisfied: pytz>=2017.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from pandas>=0.21->scanpy) (2019.3); Requirement already satisfied: get-version>=2.0.4 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from legacy-api-wrap->scanpy) (2.1); Requirement already satisfied: setuptools in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from legacy-api-wrap->scanpy) (42.0.2.post20191203); Requirement already satisfied: numexpr>=2.6.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from tables->scanpy) (2.7.0); Requirement already satisfied: more-itertools in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata>=0.7; python_version < ""3.8""->scanpy) (7.2.0); ```. ```; conda install -c bioconda scanpy; ```. ```; Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: |; /; Found conflicts! Looking for ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:10280,wrap,wrap,10280,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452,2,['wrap'],['wrap']
Integrability,"is None:; --> 564 self.linkage = self.calculated_linkage; 565 else:; 566 self.linkage = linkage. /usr/local/lib/python3.6/site-packages/seaborn/matrix.py in calculated_linkage(self); 624 def calculated_linkage(self):; 625 try:; --> 626 return self._calculate_linkage_fastcluster(); 627 except ImportError:; 628 return self._calculate_linkage_scipy(). /usr/local/lib/python3.6/site-packages/seaborn/matrix.py in _calculate_linkage_fastcluster(self); 618 else:; 619 linkage = fastcluster.linkage(self.array, method=self.method,; --> 620 metric=self.metric); 621 return linkage; 622 . /usr/local/lib/python3.6/site-packages/fastcluster.py in linkage(X, method, metric, preserve_input); 241 assert X.ndim==2; 242 N = len(X); --> 243 X = pdist(X, metric); 244 X = array(X, dtype=double, copy=False, order='C', subok=True); 245 Z = empty((N-1,4)). /usr/local/lib/python3.6/site-packages/scipy/spatial/distance.py in pdist(X, metric, *args, **kwargs); 1932 if metric_name is not None:; 1933 X, typ, kwargs = _validate_pdist_input(X, m, n,; -> 1934 metric_name, **kwargs); 1935 ; 1936 # get pdist wrapper. /usr/local/lib/python3.6/site-packages/scipy/spatial/distance.py in _validate_pdist_input(X, m, n, metric_name, **kwargs); 287 typ = types[types.index(X.dtype)] if X.dtype in types else types[0]; 288 # validate data; --> 289 X = _convert_to_type(X, out_type=typ); 290 ; 291 # validate kwargs. /usr/local/lib/python3.6/site-packages/scipy/spatial/distance.py in _convert_to_type(X, out_type); 182 ; 183 def _convert_to_type(X, out_type):; --> 184 return np.ascontiguousarray(X, dtype=out_type); 185 ; 186 . /usr/local/lib/python3.6/site-packages/numpy/core/numeric.py in ascontiguousarray(a, dtype); 588 ; 589 """"""; --> 590 return array(a, dtype, copy=False, order='C', ndmin=1); 591 ; 592 . ValueError: setting an array element with a sequence.; ```. </details>. -------------------. I think this is happening because `pandas` doesn't like being passed a sparse array ([which can happen in `sc.pl.cluster",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/356:3601,wrap,wrapper,3601,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/356,1,['wrap'],['wrapper']
Integrability,"ite-packages (from scanpy) (3.3.3); Requirement already satisfied: importlib-metadata>=0.7; python_version < ""3.8"" in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy)(1.1.0); Requirement already satisfied: h5py!=2.10.0 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (2.9.0); Requirement already satisfied: seaborn in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.9.0); Requirement already satisfied: statsmodels>=0.10.0rc2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.10.1); Requirement already satisfied: anndata>=0.6.22.post1 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.6.22.post1); Requirement already satisfied: matplotlib==3.0.* in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (3.0.3); Requirement already satisfied: pandas>=0.21 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.25.3); Requirement already satisfied: legacy-api-wrap in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (1.2); Requirement already satisfied: natsort in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (7.0.0); Requirement already satisfied: tables in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (3.6.1); Requirement already satisfied: joblib in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.14.0); Requirement already satisfied: umap-learn>=0.3.10 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.3.10); Requirement already satisfied: tqdm in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (4.40.0); Requirement already satisfied: scipy>=1.3 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (1.3.2); Requirement already satisfied: packaging in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (19.2); Requirement already satisfied: scikit-learn>=0.21.2 in /home/tsundoku/anaconda3",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:7652,wrap,wrap,7652,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452,2,['wrap'],['wrap']
Integrability,"ium but squidpy is the spatial package? I can analyze atac data in scanpy but need to use muon to read the file?. Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages?. Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places?. > How does this impact users vs. developers?. Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use `scio` and then be on their way (a reality that many people do not feel the need to use scanpy/muon). If there are R converters, this would be a major use case. > What we read in, and how we represent it, is very tightly c",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059551352:1257,depend,dependencies,1257,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059551352,2,['depend'],['dependencies']
Integrability,"joelostblom/session_info.; -----; anndata 0.7.6; scanpy 1.8.1; sinfo 0.3.4; -----; PIL 8.3.1; anyio NA; appdirs 1.4.4; appnope 0.1.2; attr 21.2.0; babel 2.9.1; backcall 0.2.0; bioservices 1.7.12; bottleneck 1.3.2; brotli NA; bs4 4.9.3; certifi 2021.05.30; cffi 1.14.6; chardet 4.0.0; cloudpickle 1.6.0; colorama 0.4.4; colorlog NA; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2021.07.2; dateutil 2.8.2; decorator 5.0.9; defusedxml 0.7.1; docutils 0.17.1; easydev 0.11.1; fsspec 2021.07.0; gseapy 0.10.5; h5py 2.10.0; html5lib 1.1; idna 2.10; igraph 0.9.4; ipykernel 5.3.4; ipython_genutils 0.2.0; ipywidgets 7.6.3; jedi 0.17.2; jinja2 2.11.3; joblib 1.0.1; json5 NA; jsonschema 3.2.0; jupyter_server 1.4.1; jupyterlab_server 2.6.1; kiwisolver 1.3.1; leidenalg 0.8.4; llvmlite 0.36.0; louvain 0.7.0; lxml 4.6.3; markupsafe 2.0.1; matplotlib 3.4.2; mpl_toolkits NA; natsort 7.1.1; nbclassic NA; nbformat 5.1.3; numba 0.53.1; numexpr 2.7.3; numpy 1.18.5; packaging 21.0; pandas 1.1.5; parso 0.7.0; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prometheus_client NA; prompt_toolkit 3.0.17; psutil 5.8.0; ptyprocess 0.7.0; pvectorc NA; pycparser 2.20; pygments 2.9.0; pylab NA; pynndescent 0.5.2; pyparsing 2.4.7; pyrsistent NA; pytz 2021.1; requests 2.25.1; requests_cache 0.6.4; scipy 1.6.2; seaborn 0.11.1; send2trash NA; six 1.16.0; sklearn 0.24.2; sniffio 1.2.0; socks 1.7.1; soupsieve 2.2.1; sphinxcontrib NA; statsmodels 0.12.2; storemagic NA; tables 3.6.1; tblib 1.7.0; texttable 1.6.3; tlz 0.11.0; toolz 0.11.1; tornado 6.1; traitlets 5.0.5; typing_extensions NA; umap 0.5.1; url_normalize 1.4.3; urllib3 1.26.6; wcwidth 0.2.5; webencodings 0.5.1; wrapt 1.12.1; yaml 5.4.1; zmq 20.0.0; zope NA; -----; IPython 7.22.0; jupyter_client 6.1.12; jupyter_core 4.7.1; jupyterlab 3.0.14; notebook 6.4.0; -----; Python 3.8.11 (default, Aug 3 2021, 05:10:14) [Clang 10.0.0 ]; macOS-10.16-x86_64-i386-64bit; 12 logical CPU cores, i386; -----; Session information updated at 2021-08-16 12:24",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1981:5417,wrap,wrapt,5417,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1981,1,['wrap'],['wrapt']
Integrability,"just pin pytest for now. ### Minimal code sample. ```python; Examples; --------; >>> import scanpy as sc; >>> adata = sc.datasets.krumsiek11(); UserWarning: Observation names are not unique. To make them unique, call `.obs_names_make_unique`.; utils.warn_names_duplicates(""obs""); ```. ### Error output. ```pytb; ======================================================================================================================= FAILURES =======================================================================================================================; _________________________________________________________________________________________________ [doctest] scanpy.preprocessing._simple.filter_cells __________________________________________________________________________________________________; 081 Boolean index mask that does filtering. `True` means that the; 082 cell is kept. `False` means the cell is removed.; 083 number_per_cell; 084 Depending on what was thresholded (`counts` or `genes`),; 085 the array stores `n_counts` or `n_cells` per gene.; 086 ; 087 Examples; 088 --------; 089 >>> import scanpy as sc; 090 >>> adata = sc.datasets.krumsiek11(); UNEXPECTED EXCEPTION: UserWarning('Observation names are not unique. To make them unique, call `.obs_names_make_unique`.'); Traceback (most recent call last):; File ""/mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/doctest.py"", line 1353, in __run; exec(compile(example.source, filename, ""single"",; File ""<doctest scanpy.preprocessing._simple.filter_cells[1]>"", line 1, in <module>; File ""/mnt/workspace/repos/scanpy/scanpy/datasets/_datasets.py"", line 109, in krumsiek11; adata = read(filename, first_column_names=True); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/legacy_api_wrap/__init__.py"", line 80, in fn_compatible; return fn(*args_all, **kw); ^^^^^^^^^^^^^^^^^^^; File ""/mnt/workspace/repos/scanpy/scanpy/readwrite.py"", line 124, in r",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2836:1639,Depend,Depending,1639,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2836,1,['Depend'],['Depending']
Integrability,"kages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy); 137 adata._init_as_actual(adata.copy()); 138 neighbors = Neighbors(adata); --> 139 neighbors.compute_neighbors(; 140 n_neighbors=n_neighbors,; 141 knn=knn,; ; ~/miniforge3/envs/scVelo/lib/python3.8/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds); 806 # we need self._distances also for method == 'gauss' if we didn't; 807 # use dense distances; --> 808 self._distances, self._connectivities = _compute_connectivities_umap(; 809 knn_indices,; 810 knn_distances,; ; ~/miniforge3/envs/scVelo/lib/python3.8/site-packages/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity); 385 # umap 0.5.0; 386 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""); --> 387 from umap.umap_ import fuzzy_simplicial_set; 388 ; 389 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)); ; ~/miniforge3/envs/scVelo/lib/python3.8/site-packages/umap/__init__.py in <module>; ----> 1 from .umap_ import UMAP; 2 ; 3 # Workaround: https://github.com/numba/numba/issues/3341; 4 import numba; 5 ; ; ~/miniforge3/envs/scVelo/lib/python3.8/site-packages/umap/umap_.py in <module>; 52 from umap.spectral import spectral_layout; 53 from umap.utils import deheap_sort, submatrix; ---> 54 from umap.layouts import (; 55 optimize_layout_euclidean,; 56 optimize_layout_generic,; ; ~/miniforge3/envs/scVelo/lib/python3.8/site-packages/umap/layouts.py in <module>; 37 },; 38 ); ---> 39 def rdist(x, y):; 40 """"""Reduced Euclidean distance.; 41 ; ; ~/miniforge3/envs/scVelo/lib/python3.8/site-packages/numba/core/decorators.py in wrapper(func); 219 with typeinfer.register_dispatcher(disp):; 220 for sig in sigs:; --> 221 disp.compile(sig); 222 disp.disable_compile(); 223 ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1799:2578,message,message,2578,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799,1,['message'],['message']
Integrability,"key, val, *args, **kwargs); 208 try:; --> 209 return func(elem, key, val, *args, **kwargs); 210 except Exception as e:. ~/anaconda3/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_series(group, key, series, dataset_kwargs); 269 if series.dtype == object: # Assuming it’s string; --> 270 group.create_dataset(; 271 key,. ~/anaconda3/lib/python3.8/site-packages/h5py/_hl/group.py in create_dataset(self, name, shape, dtype, data, **kwds); 147 ; --> 148 dsid = dataset.make_new_dset(group, shape, dtype, data, name, **kwds); 149 dset = dataset.Dataset(dsid). ~/anaconda3/lib/python3.8/site-packages/h5py/_hl/dataset.py in make_new_dset(parent, shape, dtype, data, name, chunks, compression, shuffle, fletcher32, maxshape, compression_opts, fillvalue, scaleoffset, track_times, external, track_order, dcpl, allow_unknown_filter); 139 if (data is not None) and (not isinstance(data, Empty)):; --> 140 dset_id.write(h5s.ALL, h5s.ALL, data); 141 . h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/h5d.pyx in h5py.h5d.DatasetID.write(). h5py/_proxy.pyx in h5py._proxy.dset_rw(). h5py/_conv.pyx in h5py._conv.str2vlen(). h5py/_conv.pyx in h5py._conv.generic_converter(). h5py/_conv.pyx in h5py._conv.conv_str2vlen(). TypeError: Can't implicitly convert non-string objects to strings. The above exception was the direct cause of the following exception:. TypeError Traceback (most recent call last); ~/anaconda3/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, key, val, *args, **kwargs); 208 try:; --> 209 return func(elem, key, val, *args, **kwargs); 210 except Exception as e:. ~/anaconda3/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_dataframe(f, key, df, dataset_kwargs); 262 for col_name, (_, series) in zip(col_names, df.items()):; --> 263 write_series(group, col_name, series, dataset_kwargs=dataset_kwargs); 264 . ~/anaconda3/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1866:2669,wrap,wrapper,2669,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1866,1,['wrap'],['wrapper']
Integrability,"late the Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). To avoid this complication I rewrote the Zheng17 recipe to do all the NumPy array computations and then construct an Anndata representation at the end,; to take advantage of Dask's deferred processing of lazy values. (See https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/preprocessing/_dask_optimized.py#L115 for the code.). With this change, running on the 1M neurons dataset with 64 cores `scipy.sparse` takes 334s, while Dask with `scipy.sparse` takes 138s, a 2.4x speedup. That's a significant speedup, but I'm not sure that it justifies the code overhead. I'd be interested to hear what others think. . ### Other notes. #### Code; See this branch: https://github.com/theislab/scanpy/compare/master...tomwhite:sparse-dask. #### CuPy and GPUs; I also wrote a [wrapper](https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/sparsearray/_cupy_sparse.py) around the GPU equivalent of `scipy.sparse`, [`cupyx.scipy.sparse`](https://docs-cupy.chainer.org/en/stable/reference/sparse.html). Many operations work, however `cupyx.scipy.sparse` has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:; * `multiply` - not implemented by `cupyx.scipy.sparse.csr.csr_matrix`; * `mean` - no method on `cupyx.scipy.sparse.csr.csr_matrix` (note that it does have `sum`); * column subset not supported, e.g. `xs[:, 1:3]` (note that row subset is); * boolean indexing, i.e. `xs[:, subset]`, where `subset` is e.g. `np.array([True, False, True, False, True])`; note this fails for rows too. #### NumPy 1.16 vs NumPy 1.17; I used NumPy 1.16 for the above experiments. However, when I tried NumPy 1.17 the Dask implementation slowed down significantly. I haven't been able to pinpoint the",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/921:2355,wrap,wrapper,2355,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921,1,['wrap'],['wrapper']
Integrability,"left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax); 1057 if as_heatmap:; 1058 img = ax.imshow(; -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map; 1060 ); 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs); 1599 def inner(ax, *args, data=None, **kwargs):; 1600 if data is None:; -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs); 1602 ; 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs); 367 f""%(removal)s. If any parameter follows {name!r}, they ""; 368 f""should be pass as keyword, not positionally.""); --> 369 return func(*args, **kwargs); 370 ; 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs); 367 f""%(removal)s. If any parameter follows {name!r}, they ""; 368 f""should be pass as keyword, not positionally.""); --> 369 return func(*args, **kwargs); 370 ; 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs); 5669 resample=resample, **kwargs); 5670 ; -> 5671 im.set_data(X); 5672 im.set_alpha(alpha); 5673 if im.get_clip_path() is None:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/image.py in set_data(self, A); 683 not np.can_cast(self._A.dtype, float, ""same_kind"")):; 684 raise TypeError(""Image data of dtype {} cannot be converted to ""; --> 685 ""float"".format(self._A.dtype)); 686 ; 687 if not (self._A.ndim == 2. TypeError: Image data of dtype object cannot be converted to float; ```; Plotting a heatmap with `sc.pl.heatmap` works. #### Versions:; <!-- Output of scanpy.logging.print_versions",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/953:4552,wrap,wrapper,4552,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953,1,['wrap'],['wrapper']
Integrability,"left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax); 1057 if as_heatmap:; 1058 img = ax.imshow(; -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map; 1060 ); 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs); 1599 def inner(ax, *args, data=None, **kwargs):; 1600 if data is None:; -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs); 1602 ; 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs); 367 f""%(removal)s. If any parameter follows {name!r}, they ""; 368 f""should be pass as keyword, not positionally.""); --> 369 return func(*args, **kwargs); 370 ; 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs); 367 f""%(removal)s. If any parameter follows {name!r}, they ""; 368 f""should be pass as keyword, not positionally.""); --> 369 return func(*args, **kwargs); 370 ; 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs); 5669 resample=resample, **kwargs); 5670 ; -> 5671 im.set_data(X); 5672 im.set_alpha(alpha); 5673 if im.get_clip_path() is None:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/image.py in set_data(self, A); 688 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):; 689 raise TypeError(""Invalid shape {} for image data""; --> 690 .format(self._A.shape)); 691 ; 692 if self._A.ndim == 3:. TypeError: Invalid shape (3, 43, 1) for image data; ```; If I convert the `adata.X` to sparse matrix format, I have the following error:; ```python; adata.X = sci.sparse.csr_matrix(adata.X); sc.pl.paga_pa",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/953:2037,wrap,wrapper,2037,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953,1,['wrap'],['wrapper']
Integrability,"ler/issues/9). Do you know why the enrich function uses capitalization, and how I can get my gprofiler compatible with it? Versions listed below. Thanks. ```pytb; ImportError Traceback (most recent call last); /anaconda3/envs/mm_singlecell_v2/lib/python3.6/site-packages/scanpy/queries/_queries.py in enrich(container, org, gprofiler_kwargs); 264 try:; --> 265 from gprofiler import GProfiler; 266 except ImportError:. ImportError: cannot import name 'GProfiler'. During handling of the above exception, another exception occurred:. ImportError Traceback (most recent call last); <ipython-input-383-c1b09359d1a1> in <module>; 14 ; 15 #get gene set enrichment; ---> 16 print(sc.queries.enrich(this_adata, org='hsapiens', group='malignant', key='malignantvshealthy', pval_cutoff=0.01, log2fc_min=np.log2(1.5))); 17 ; 18 #plot volcano (makes a sep df along the way, should consolidate with above). /anaconda3/envs/mm_singlecell_v2/lib/python3.6/functools.py in wrapper(*args, **kw); 805 '1 positional argument'); 806 ; --> 807 return dispatch(args[0].__class__)(*args, **kw); 808 ; 809 funcname = getattr(func, '__name__', 'singledispatch function'). /anaconda3/envs/mm_singlecell_v2/lib/python3.6/site-packages/scanpy/queries/_queries.py in _enrich_anndata(adata, group, org, key, pval_cutoff, log2fc_min, log2fc_max, gene_symbols, gprofiler_kwargs); 305 else:; 306 gene_list = list(de[""names""].dropna()); --> 307 return enrich(gene_list, org=org, gprofiler_kwargs=gprofiler_kwargs). /anaconda3/envs/mm_singlecell_v2/lib/python3.6/functools.py in wrapper(*args, **kw); 805 '1 positional argument'); 806 ; --> 807 return dispatch(args[0].__class__)(*args, **kw); 808 ; 809 funcname = getattr(func, '__name__', 'singledispatch function'). /anaconda3/envs/mm_singlecell_v2/lib/python3.6/site-packages/scanpy/queries/_queries.py in enrich(container, org, gprofiler_kwargs); 266 except ImportError:; 267 raise ImportError(; --> 268 ""This method requires the `gprofiler-official` module to be installed.""; 26",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1896:1288,wrap,wrapper,1288,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1896,1,['wrap'],['wrapper']
Integrability,"line 1016, in _send_output; self.send(msg); File ""/anaconda3/envs/scIB-python/lib/python3.7/http/client.py"", line 956, in send; self.connect(); File ""/anaconda3/envs/scIB-python/lib/python3.7/http/client.py"", line 1392, in connect; server_hostname=server_hostname); File ""/anaconda3/envs/scIB-python/lib/python3.7/ssl.py"", line 412, in wrap_socket; session=session; File ""/anaconda3/envs/scIB-python/lib/python3.7/ssl.py"", line 853, in _create; self.do_handshake(); File ""/anaconda3/envs/scIB-python/lib/python3.7/ssl.py"", line 1117, in do_handshake; self._sslobj.do_handshake(); ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1056). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/anaconda3/envs/scIB-python/lib/python3.7/site-packages/scanpy/datasets/_utils.py"", line 10, in wrapper; return f(*args, **kwargs); File ""/anaconda3/envs/scIB-python/lib/python3.7/site-packages/scanpy/datasets/_datasets.py"", line 305, in pbmc3k_processed; backup_url='https://raw.githubusercontent.com/chanzuckerberg/cellxgene/main/example-dataset/pbmc3k.h5ad',; File ""/anaconda3/envs/scIB-python/lib/python3.7/site-packages/scanpy/readwrite.py"", line 122, in read; **kwargs,; File ""/anaconda3/envs/scIB-python/lib/python3.7/site-packages/scanpy/readwrite.py"", line 694, in _read; is_present = _check_datafile_present_and_download(filename, backup_url=backup_url,); File ""/anaconda3/envs/scIB-python/lib/python3.7/site-packages/scanpy/readwrite.py"", line 970, in _check_datafile_present_and_download; _download(backup_url, path); File ""/anaconda3/envs/scIB-python/lib/python3.7/site-packages/scanpy/readwrite.py"", line 936, in _download; urlopen(Request(url, headers={""User-agent"": ""scanpy-user""})) as resp:; File ""/anaconda3/envs/scIB-python/lib/python3.7/urllib/request.py"", line 222, in urlopen; return opener.open(url, data, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1472#issuecomment-721326665:1863,wrap,wrapper,1863,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1472#issuecomment-721326665,1,['wrap'],['wrapper']
Integrability,lite 0.34.0 ; markdown-it-py 0.5.6 ; MarkupSafe 1.1.1 ; matplotlib 3.3.3 ; mccabe 0.6.1 ; mistune 0.8.4 ; mypy-extensions 0.4.3 ; natsort 7.0.1 ; nbclient 0.5.1 ; nbconvert 6.0.7 ; nbdime 2.1.0 ; nbformat 5.0.8 ; nbresuse 0.3.6 ; nest-asyncio 1.4.3 ; networkx 2.5 ; notebook 6.1.5 ; numba 0.51.2 ; numexpr 2.7.1 ; numpy 1.19.4 ; packaging 20.4 ; pandas 1.1.4 ; pandocfilters 1.4.3 ; parso 0.7.1 ; path 15.0.0 ; pathspec 0.8.1 ; pathtools 0.1.2 ; patsy 0.5.1 ; peepdis 0.1.13 ; pexpect 4.8.0 ; pickleshare 0.7.5 ; Pillow 8.0.1 ; pip 20.0.2 ; plotly 4.12.0 ; pluggy 0.13.1 ; prometheus-client 0.8.0 ; prompt-toolkit 3.0.8 ; psutil 5.7.3 ; ptvsd 4.3.2 ; ptyprocess 0.6.0 ; py 1.9.0 ; pycodestyle 2.6.0 ; pycparser 2.20 ; pydocstyle 5.1.1 ; pyflakes 2.2.0 ; Pygments 2.7.2 ; PyGObject 3.36.0 ; pylint 2.6.0 ; pymongo 3.11.0 ; pyparsing 2.4.7 ; pyrsistent 0.17.3 ; pytest 6.1.2 ; python-apt 2.0.0+ubuntu0.20.4.1 ; python-dateutil 2.8.1 ; python-debian 0.1.36ubuntu1 ; python-jsonrpc-server 0.4.0 ; python-language-server 0.36.1 ; pytoml 0.1.21 ; pytz 2020.4 ; PyYAML 5.3.1 ; pyzmq 20.0.0 ; regex 2020.11.13 ; requests 2.22.0 ; requests-unixsocket 0.2.0 ; retrying 1.3.3 ; rich 9.2.0 ; rope 0.18.0 ; scikit-learn 0.23.2 ; scikit-misc 0.1.3 ; scipy 1.5.4 ; scriptedforms 0.10.1 ; scvi-tools 0.7.1 ; seaborn 0.11.0 ; Send2Trash 1.5.0 ; setuptools 50.3.2 ; setuptools-scm 4.1.2 ; sinfo 0.3.1 ; six 1.14.0 ; smmap 3.0.4 ; snowballstemmer 2.0.0 ; SQLAlchemy 1.3.20 ; statsmodels 0.12.1 ; stdlib-list 0.7.0 ; tables 3.6.1 ; termcolor 1.1.0 ; terminado 0.9.1 ; testpath 0.4.4 ; threadpoolctl 2.1.0 ; toml 0.10.2 ; torch 1.7.0 ; tornado 6.1 ; tqdm 4.51.0 ; traitlets 5.0.5 ; typed-ast 1.4.1 ; typeguard 2.10.0 ; typing-extensions 3.7.4.3 ; ujson 4.0.1 ; umap-learn 0.4.6 ; unattended-upgrades 0.1 ; urllib3 1.25.8 ; watchdog 0.10.3 ; wcwidth 0.2.5 ; webencodings 0.5.1 ; Werkzeug 1.0.1 ; wheel 0.35.1 ; widgetsnbextension 3.5.1 ; wrapt 1.12.1 ; xeus-python 0.8.3 ; xlrd 1.2.0 ; yapf 0.30.0 ; zipp 3.4.0. </details>,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1496:6913,wrap,wrapt,6913,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1496,1,['wrap'],['wrapt']
Integrability,"lling numba from a `ThreadPoolExecutor` isn’t supported at all, even if it comes from dask. ```console; $ hatch test tests/test_utils.py::test_is_constant_dask[csr_matrix-0] --capture=no; Numba function called from a non-threadsafe context. Try installing `tbb`.; Numba function called from a non-threadsafe context. Try installing `tbb`. Numba workqueue threading layer is terminating: Concurrent access has been detected. - The workqueue threading layer is not threadsafe and may not be accessed concurrently by multiple threads. Concurrent access typically occurs through a nested parallel region launch or by calling Numba parallel=True functions from multiple Python threads.; - Try using the TBB threading layer as an alternative, as it is, itself, threadsafe. Docs: https://numba.readthedocs.io/en/stable/user/threading-layer.html. Fatal Python error: Aborted. Thread 0x000000016fd2f000 (most recent call first):; File ""~/Dev/scanpy/src/scanpy/_compat.py"", line 133 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 109 in _; File ""<venv>/lib/python3.12/functools.py"", line 909 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 30 in func; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 157 in get; File ""<venv>/lib/python3.12/site-packages/dask/optimization.py"", line 1001 in __call__; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 225 in execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 239 in batch_execute_tasks; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 64 in run; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 92 in _worker; File ""<venv>/lib/python3.12/threading.py"", line 1010 in run; File ""<venv>/lib/python3.12/threading.py"", line 1073 in _bootstrap_inner; Fi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478:1206,wrap,wrapper,1206,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478,2,['wrap'],['wrapper']
Integrability,"llowing error which is fixed when rolling back to scanpy=1.8.2; ```pytb; ValueError Traceback (most recent call last); <ipython-input-3-8ddd0a13aab2> in <module>; 8 print(results_file); ----> 9 adata = sc.read_10x_h5(results_file); 10 adata.var_names_make_unique(); 11 adata.obs.index = meta.iloc[idx,2] + '-' + adata.obs.index. /opt/conda/lib/python3.8/site-packages/scanpy/readwrite.py in read_10x_h5(filename, genome, gex_only, backup_url); 181 v3 = '/matrix' in f; 182 if v3:; --> 183 adata = _read_v3_10x_h5(filename, start=start); 184 if genome:; 185 if genome not in adata.var['genome'].values:. /opt/conda/lib/python3.8/site-packages/scanpy/readwrite.py in _read_v3_10x_h5(filename, start); 266 try:; 267 dsets = {}; --> 268 _collect_datasets(dsets, f[""matrix""]); 269 ; 270 from scipy.sparse import csr_matrix. /opt/conda/lib/python3.8/site-packages/scanpy/readwrite.py in _collect_datasets(dsets, group); 254 for k, v in group.items():; 255 if isinstance(v, h5py.Dataset):; --> 256 dsets[k] = v[:]; 257 else:; 258 _collect_datasets(dsets, v). h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). /opt/conda/lib/python3.8/site-packages/h5py/_hl/dataset.py in __getitem__(self, args, new_dtype); 767 if self.shape == ():; 768 fspace = self.id.get_space(); --> 769 selection = sel2.select_read(fspace, args); 770 if selection.mshape is None:; 771 arr = numpy.ndarray((), dtype=new_dtype). /opt/conda/lib/python3.8/site-packages/h5py/_hl/selections2.py in select_read(fspace, args); 99 """"""; 100 if fspace.shape == ():; --> 101 return ScalarReadSelection(fspace, args); 102 ; 103 raise NotImplementedError(). /opt/conda/lib/python3.8/site-packages/h5py/_hl/selections2.py in __init__(self, fspace, args); 84 self.mshape = (); 85 else:; ---> 86 raise ValueError(""Illegal slicing argument for scalar dataspace""); 87 ; 88 self.mspace = h5s.create(h5s.SCALAR). ValueError: Illegal slicing argument for scalar dataspace; ```. Thanks!! . Nadav",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2203:1276,wrap,wrapper,1276,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2203,2,['wrap'],['wrapper']
Integrability,"lmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps?. > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs?. Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though!. There's already some information [at the beginning of the tutorial](https://icbi-lab.github.io/scirpy/tutorials/tutorial_3k_tcr.html#Analysis-of-3k-T-cells-from-cancer). But I agree that this deserves an own section in the docs (created https://github.com/icbi-lab/scirpy/issues/110). Currently, we simply add columns to `adata.obs` - but I'm still open to discussion. The data-structure needs slight modifications for BCR data anyway. See also: https://github.com/theislab/anndata/issues/115#issuecomment-579275853. . Cheers, ; Gregor",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1163#issuecomment-613394910:1716,integrat,integration,1716,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163#issuecomment-613394910,2,['integrat'],['integration']
Integrability,"lobs""] = pd.Categorical(blobs.obs[""blobs""]); blobs.obs[""cov""] = pd.Categorical(blobs.obs[""blobs""] == ""0""). sc.pp.combat(blobs, ""blobs"", covariates=[""cov""]); # LinAlgError: Singular matrix; ```. <details>; <summary> Full traceback </summary>. ```pytb; ---------------------------------------------------------------------------; LinAlgError Traceback (most recent call last); <ipython-input-13-5685c001369c> in <module>; ----> 1 sc.pp.combat(blobs, ""blobs"", covariates=[""cov""]). ~/github/scanpy/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace); 204 # standardize across genes using a pooled variance estimator; 205 logg.info(""Standardizing Data across genes.\n""); --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key); 207 ; 208 # fitting the parameters on the standardized data. ~/github/scanpy/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key); 102 ; 103 # compute pooled variance estimator; --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T); 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]); 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **kwargs). /usr/local/lib/python3.8/site-packages/numpy/linalg/linalg.py in inv(a); 544 signature = 'D->D' if isComplexType(t) else 'd->d'; 545 extobj = get_linalg_error_extobj(_raise_linalgerror_singular); --> 546 ainv = _umath_linalg.inv(a, signature=signature, extobj=extobj); 547 return wrap(ainv.astype(result_t, copy=False)); 548 . /usr/local/lib/python3.8/site-packages/numpy/linalg/linalg.py in _raise_linalgerror_singular(err, flag); 86 ; 87 def _raise_linalgerror_singular(err, flag):; ---> 88 raise LinAlgError(""Singular matrix""); 89 ; 90 def _raise_linalgerror_nonposdef(err, flag):. LinAlgError: Singular matrix; ```. </details>. Does this occur in your data? You can check with:. ```python; pd.crosstab(adata.obs[""384plate""], adata.obs[""age_group""]); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1606#issuecomment-766480303:1894,wrap,wrap,1894,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606#issuecomment-766480303,1,['wrap'],['wrap']
Integrability,"looks good, let's take one these. but before integrating it, the scatter plot of dpt should invoke `plotting.plot_tool` as well. then it's going to be just a one line change.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3#issuecomment-278376182:45,integrat,integrating,45,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3#issuecomment-278376182,1,['integrat'],['integrating']
Integrability,"ls>=4.22.0 in c:\users\charles\anaconda3\lib\site-packages (from matplotlib>=3.1.2->scanpy) (4.25.0); Requirement already satisfied: python-dateutil>=2.7 in c:\users\charles\anaconda3\lib\site-packages (from matplotlib>=3.1.2->scanpy) (2.8.2); Requirement already satisfied: llvmlite>=0.29.0 in c:\users\charles\anaconda3\lib\site-packages (from numba>=0.41.0->scanpy) (0.29.0); Requirement already satisfied: pytz>=2017.3 in c:\users\charles\anaconda3\lib\site-packages (from pandas>=0.21->scanpy) (2021.3); Requirement already satisfied: threadpoolctl>=2.0.0 in c:\users\charles\anaconda3\lib\site-packages (from scikit-learn>=0.21.2->scanpy) (2.2.0); Collecting numba>=0.41.0; Using cached numba-0.55.1-cp37-cp37m-win_amd64.whl (2.4 MB); Requirement already satisfied: pynndescent>=0.5 in c:\users\charles\anaconda3\lib\site-packages (from umap-learn>=0.3.10->scanpy) (0.5.2); Requirement already satisfied: setuptools in c:\users\charles\anaconda3\lib\site-packages (from numba>=0.41.0->scanpy) (58.0.4); Collecting llvmlite>=0.29.0; Using cached llvmlite-0.38.0-cp37-cp37m-win_amd64.whl (23.2 MB); Requirement already satisfied: get-version>=2.0.4 in c:\users\charles\anaconda3\lib\site-packages (from legacy-api-wrap->scanpy) (2.2); Requirement already satisfied: stdlib-list in c:\users\charles\anaconda3\lib\site-packages (from sinfo->scanpy) (0.8.0); Requirement already satisfied: numexpr>=2.6.2 in c:\users\charles\anaconda3\lib\site-packages (from tables->scanpy) (2.8.1); Requirement already satisfied: colorama in c:\users\charles\anaconda3\lib\site-packages (from tqdm->scanpy) (0.4.4); Installing collected packages: llvmlite, numba, xlrd; Attempting uninstall: llvmlite; Found existing installation: llvmlite 0.29.0; Note: you may need to restart the kernel to use updated packages.; ERROR: Cannot uninstall 'llvmlite'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.; ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2173#issuecomment-1063704626:4909,wrap,wrap,4909,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2173#issuecomment-1063704626,2,['wrap'],['wrap']
Integrability,"lue, pairwise); 817 # Annotation for other axis; 818 alt_annot = merge_dataframes(; --> 819 [getattr(a, alt_dim) for a in adatas], alt_indices, merge; 820 ); 821 . ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in merge_dataframes(dfs, new_index, merge_strategy); 529 dfs: Iterable[pd.DataFrame], new_index, merge_strategy=merge_unique; 530 ) -> pd.DataFrame:; --> 531 dfs = [df.reindex(index=new_index) for df in dfs]; 532 # New dataframe with all shared data; 533 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in <listcomp>(.0); 529 dfs: Iterable[pd.DataFrame], new_index, merge_strategy=merge_unique; 530 ) -> pd.DataFrame:; --> 531 dfs = [df.reindex(index=new_index) for df in dfs]; 532 # New dataframe with all shared data; 533 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs); 310 @wraps(func); 311 def wrapper(*args, **kwargs) -> Callable[..., Any]:; --> 312 return func(*args, **kwargs); 313 ; 314 kind = inspect.Parameter.POSITIONAL_OR_KEYWORD. ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in reindex(self, *args, **kwargs); 4174 kwargs.pop(""axis"", None); 4175 kwargs.pop(""labels"", None); -> 4176 return super().reindex(**kwargs); 4177 ; 4178 def drop(. ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs); 4810 # perform the reindex on the axes; 4811 return self._reindex_axes(; -> 4812 axes, level, limit, tolerance, method, fill_value, copy; 4813 ).__finalize__(self, method=""reindex""); 4814 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy); 4021 if index is not None:; 4022 frame = frame._reindex_index(; -> 4023 index, method, copy, level, fill_value, limit, tolerance; 4024 ); 4025 . ~/anaconda3/lib/python3.7/site-packages/panda",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/267#issuecomment-1018908683:2207,wrap,wrapper,2207,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267#issuecomment-1018908683,3,['wrap'],"['wrapper', 'wraps']"
Integrability,"m of count layer of MALAT1 in cell: (0, 0)	289.61862; .X value of MALAT1 in cell: (0, 0)	289.61862; ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]; -----; anndata 0.8.0; scanpy 1.9.1; -----; PIL 9.2.0; anndata2ri 1.1; annoy NA; backcall 0.2.0; backports NA; bbknn NA; beta_ufunc NA; binom_ufunc NA; cffi 1.15.1; cloudpickle 2.2.0; colorama 0.4.6; cycler 0.10.0; cython_runtime NA; cytoolz 0.12.0; dask 2022.02.0; dateutil 2.8.2; debugpy 1.6.3; decorator 5.1.1; defusedxml 0.7.1; deprecated 1.2.13; entrypoints 0.4; fsspec 2022.11.0; future_fstrings NA; google NA; h5py 3.7.0; igraph 0.9.1; ipykernel 6.14.0; ipython_genutils 0.2.0; ipywidgets 8.0.2; jedi 0.18.1; jinja2 3.1.2; joblib 1.2.0; kiwisolver 1.4.4; leidenalg 0.8.10; llvmlite 0.39.1; louvain 0.7.2; markupsafe 2.1.1; matplotlib 3.5.3; matplotlib_inline 0.1.6; mpl_toolkits NA; natsort 8.2.0; nbinom_ufunc NA; numba 0.56.3; numpy 1.21.6; packaging 21.3; pandas 1.3.5; parso 0.8.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.31; psutil 5.9.3; ptyprocess 0.7.0; pycparser 2.21; pydev_ipython NA; pydevconsole NA; pydevd 2.8.0; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.13.0; pynndescent 0.5.7; pyparsing 3.0.9; pytz 2022.5; pytz_deprecation_shim NA; rpy2 3.5.1; scib 1.0.4; scipy 1.7.3; seaborn 0.12.1; session_info 1.0.0; six 1.16.0; sklearn 1.0.2; statsmodels 0.13.2; storemagic NA; texttable 1.6.4; threadpoolctl 3.1.0; tlz 0.12.0; toolz 0.12.0; tornado 6.2; tqdm 4.64.1; traitlets 5.5.0; typing_extensions NA; tzlocal NA; umap 0.5.3; wcwidth 0.2.5; wrapt 1.14.1; yaml 6.0; zipp NA; zmq 24.0.1; zope NA; -----; IPython 7.33.0; jupyter_client 7.4.4; jupyter_core 4.11.1; notebook 6.5.1; -----; Python 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21) [GCC 9.4.0]; Linux-5.4.0-131-generic-x86_64-with-debian-buster-sid; -----; Session information updated at 2022-12-28 13:52; </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2389:5483,wrap,wrapt,5483,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2389,1,['wrap'],['wrapt']
Integrability,mambaforge/envs/scanpy-dev2:; #; # Name Version Build Channel; _libgcc_mutex 0.1 conda_forge conda-forge; _openmp_mutex 4.5 2_gnu conda-forge; anndata 0.10.7 pypi_0 pypi; array-api-compat 1.6 pypi_0 pypi; asciitree 0.3.3 pypi_0 pypi; attrs 23.2.0 pypi_0 pypi; bzip2 1.0.8 hd590300_5 conda-forge; ca-certificates 2024.2.2 hbcca054_0 conda-forge; cfgv 3.4.0 pypi_0 pypi; click 8.1.7 pypi_0 pypi; cloudpickle 3.0.0 pypi_0 pypi; contourpy 1.2.1 pypi_0 pypi; coverage 7.4.4 pypi_0 pypi; cycler 0.12.1 pypi_0 pypi; dask 2024.4.1 pypi_0 pypi; dask-expr 1.0.10 pypi_0 pypi; distlib 0.3.8 pypi_0 pypi; execnet 2.1.1 pypi_0 pypi; fasteners 0.19 pypi_0 pypi; filelock 3.13.3 pypi_0 pypi; fonttools 4.51.0 pypi_0 pypi; fsspec 2024.3.1 pypi_0 pypi; h5py 3.10.0 pypi_0 pypi; identify 2.5.35 pypi_0 pypi; igraph 0.11.4 pypi_0 pypi; imageio 2.34.0 pypi_0 pypi; iniconfig 2.0.0 pypi_0 pypi; joblib 1.4.0 pypi_0 pypi; kiwisolver 1.4.5 pypi_0 pypi; lazy-loader 0.4 pypi_0 pypi; ld_impl_linux-64 2.40 h41732ed_0 conda-forge; legacy-api-wrap 1.4 pypi_0 pypi; leidenalg 0.10.2 pypi_0 pypi; libexpat 2.6.2 h59595ed_0 conda-forge; libffi 3.4.2 h7f98852_5 conda-forge; libgcc-ng 13.2.0 h807b86a_5 conda-forge; libgomp 13.2.0 h807b86a_5 conda-forge; libnsl 2.0.1 hd590300_0 conda-forge; libsqlite 3.45.2 h2797004_0 conda-forge; libuuid 2.38.1 h0b41bf4_0 conda-forge; libxcrypt 4.4.36 hd590300_1 conda-forge; libzlib 1.2.13 hd590300_5 conda-forge; llvmlite 0.42.0 pypi_0 pypi; locket 1.0.0 pypi_0 pypi; matplotlib 3.8.4 pypi_0 pypi; natsort 8.4.0 pypi_0 pypi; ncurses 6.4.20240210 h59595ed_0 conda-forge; networkx 3.3 pypi_0 pypi; nodeenv 1.8.0 pypi_0 pypi; numba 0.59.1 pypi_0 pypi; numcodecs 0.12.1 pypi_0 pypi; numpy 1.26.4 pypi_0 pypi; openssl 3.2.1 hd590300_1 conda-forge; packaging 24.0 pypi_0 pypi; pandas 2.2.1 pypi_0 pypi; partd 1.4.1 pypi_0 pypi; patsy 0.5.6 pypi_0 pypi; pbr 6.0.0 pypi_0 pypi; pillow 10.3.0 pypi_0 pypi; pip 24.0 pyhd8ed1ab_0 conda-forge; platformdirs 4.2.0 pypi_0 pypi; pluggy 1.4.0 pypi_0 pypi; pre,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2993:26503,wrap,wrap,26503,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2993,2,['wrap'],['wrap']
Integrability,"matrix'> has not been implemented yet. The above exception was the direct cause of the following exception:. NotImplementedError Traceback (most recent call last); <ipython-input-102-4378df4ffefd> in <module>; ----> 1 adpt.write_h5ad('../data/ra19_10_liverprimary_yubin_latest.h5ad.gz'). ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_core/anndata.py in write_h5ad(self, filename, compression, compression_opts, force_dense, as_dense); 1844 filename = self.filename; 1845 ; -> 1846 _write_h5ad(; 1847 Path(filename),; 1848 self,. ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_h5ad(filepath, adata, force_dense, as_dense, dataset_kwargs, **kwargs); 90 elif not (adata.isbacked and Path(adata.filename) == Path(filepath)):; 91 # If adata.isbacked, X should already be up to date; ---> 92 write_attribute(f, ""X"", adata.X, dataset_kwargs=dataset_kwargs); 93 if ""raw/X"" in as_dense and isinstance(; 94 adata.raw.X, (sparse.spmatrix, SparseDataset). ~/miniconda3/envs/scrna/lib/python3.8/functools.py in wrapper(*args, **kw); 872 '1 positional argument'); 873 ; --> 874 return dispatch(args[0].__class__)(*args, **kw); 875 ; 876 funcname = getattr(func, '__name__', 'singledispatch function'). ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_attribute_h5ad(f, key, value, *args, **kwargs); 124 if key in f:; 125 del f[key]; --> 126 _write_method(type(value))(f, key, value, *args, **kwargs); 127 ; 128 . ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, key, val, *args, **kwargs); 189 except Exception as e:; 190 parent = _get_parent(elem); --> 191 raise type(e)(; 192 f""{e}\n\n""; 193 f""Above error raised while writing key {key!r} of {type(elem)}"". NotImplementedError: Failed to write value for X, since a writer for type <class 'scipy.sparse.csr.csr_matrix'> has not been implemented yet. Above error raised while writing key 'X' of <class 'h5py._hl.files.File'> from /.; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1670#issuecomment-783799732:1828,wrap,wrapper,1828,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670#issuecomment-783799732,1,['wrap'],['wrapper']
Integrability,"mba_c251d9588484449eb116f16ee1b89979/setup.py"", line 51, in <module>; _guard_py_ver(); File ""/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py"", line 48, in _guard_py_ver; raise RuntimeError(msg.format(cur_py, min_py, max_py)); RuntimeError: Cannot install on Python version 3.11.0; only versions >=3.7,<3.11 are supported.; error: subprocess-exited-with-error; ; × python setup.py egg_info did not run successfully.; │ exit code: 1; ╰─> See above for output.; ; note: This error originates from a subprocess, and is likely not a problem with pip.; full command: /Users/dang/opt/miniconda3/envs2/test/bin/python3.11 -c '; exec(compile('""'""''""'""''""'""'; # This is <pip-setuptools-caller> -- a caller that pip uses to run setup.py; #; # - It imports setuptools before invoking setup.py, to enable projects that directly; # import from `distutils.core` to work with newer packaging standards.; # - It provides a clear error message when setuptools is not installed.; # - It sets `sys.argv[0]` to the underlying `setup.py`, when invoking `setup.py` so; # setuptools doesn'""'""'t think the script is `-c`. This avoids the following warning:; # manifest_maker: standard file '""'""'-c'""'""' not found"".; # - It generates a shim setup.py, for handling setup.cfg-only projects.; import os, sys, tokenize; ; try:; import setuptools; except ImportError as error:; print(; ""ERROR: Can not execute `setup.py` since setuptools is not available in ""; ""the build environment."",; file=sys.stderr,; ); sys.exit(1); ; __file__ = %r; sys.argv[0] = __file__; ; if os.path.exists(__file__):; filename = __file__; with tokenize.open(__file__) as f:; setup_py_code = f.read(); else:; filename = ""<auto-generated setuptools caller>""; setup_py_code = ""from setuptools import setup; setup()""; ; exec(compile(setup_py_code, filename, ""exec"")); '""'""''""'""''""'""' % ('""'""'/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209:3281,message,message,3281,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209,2,['message'],['message']
Integrability,"me NA; cytoolz 0.11.0; dask 2021.04.0; dateutil 2.8.1; debugpy 1.5.1; decorator 5.0.6; dot_parser NA; dunamai 1.6.0; executing 0.8.2; fbpca NA; flatbuffers NA; fsspec 0.7.4; gast 0.5.3; get_version 3.5; google NA; gprofiler 1.0.0; h5py 3.7.0; idna 2.10; igraph 0.10.2; importlib_resources NA; intervaltree NA; ipykernel 6.8.0; ipython_genutils 0.2.0; ipywidgets 7.6.3; jedi 0.17.2; jinja2 3.0.2; jmespath 1.0.1; joblib 1.0.1; json5 NA; jsonschema 3.2.0; jupyter_server 1.4.1; jupyterlab_server 2.4.0; keras 2.8.0; keras_preprocessing 1.1.2; kiwisolver 1.3.1; leidenalg 0.7.0; llvmlite 0.36.0; louvain 0.7.0; markupsafe 2.0.1; matplotlib 3.7.1; matplotlib_inline NA; mpl_toolkits NA; natsort 7.1.1; nbclassic NA; nbformat 5.1.3; numba 0.53.1; numpy 1.22.0; opt_einsum v3.3.0; packaging 20.9; pandas 1.2.5; parso 0.7.0; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prometheus_client NA; prompt_toolkit 3.0.17; psutil 5.8.0; ptyprocess 0.7.0; pure_eval 0.2.2; pvectorc NA; pycparser 2.20; pydev_ipython NA; pydevconsole NA; pydevd 2.6.0; pydevd_concurrency_analyser NA; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pydot 1.4.2; pygments 2.8.1; pynndescent 0.5.4; pyparsing 2.4.7; pyrsistent NA; pytz 2021.3; requests 2.25.1; rpy2 3.4.2; ruamel NA; scanorama 1.7.1; scipy 1.6.2; scrublet NA; scvelo 0.2.5; seaborn 0.11.1; send2trash NA; session_info 1.0.0; six 1.15.0; sklearn 0.24.1; sniffio 1.2.0; socks 1.7.1; sortedcontainers 2.3.0; sparse 0.13.0; sphinxcontrib NA; stack_data 0.1.4; statsmodels 0.12.2; tblib 1.7.0; tensorboard 2.8.0; tensorflow 2.8.0; termcolor 1.1.0; texttable 1.6.4; tlz 0.11.0; toolz 0.11.1; tornado 6.1; traitlets 5.1.1; typing_extensions NA; tzlocal NA; umap 0.5.1; urllib3 1.26.4; wcwidth 0.2.5; wrapt 1.12.1; yaml 5.4.1; zipp NA; zmq 20.0.0; zope NA; -----; IPython 8.0.1; jupyter_client 6.1.12; jupyter_core 4.7.1; jupyterlab 3.0.14; notebook 6.3.0; -----; Python 3.8.8 (default, Apr 13 2021, 12:59:45) [Clang 10.0.0 ]; macOS-10.15.7-x86_64-i386-64bit",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2443:3213,wrap,wrapt,3213,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2443,1,['wrap'],['wrapt']
Integrability,"methods?. So, if we use the KNNG provided by `sc.pp.neighbors`, these parameters become unnecessary. Both `perplexity` and `k` specify the number of k-nearest neighbors when constructing the KNNG. Here, we assume that the KNNG exists from before, so there is no need for this parameter. > Do you need to know what the affinity method was if you're just calculating an embeddings? Or does that only become important when you want to add new data?. Yes, the affinity model will have to be somehow kept, since when we call `transform`, we need to find the nearest neighbors in the index. I haven't checked how your UMAP functionality does this, but I'm guessing it's similar. Regarding the whole API, I have a few comments. I very much dislike the API `sc.pp.neighbors_tsne(adata)`. scanpy is nice because it's easy to use and the API is dead simple. I can just call `sc.pp.neighbors` followed by clustering, visualization, and whatever else I want using simple function calls. If we went this route, this would mean changing `sc.pp.neighbors` to `sc.pp.umap_neighbors`, and then splitting of yet another `sc.pp.gauss_neighbors`. This would not only make things confusing, it would mean re-calculating the KNNG at each call, which we would inevitably have to do if we wanted different visualizations. It then also becomes quite unclear what to do when I want to do Louvain clustering. Should there be a `sc.pp.louvain_neighbors` as well? Which neighbors should I use there? (As an aside, I don't understand why using UMAP connectivites is the default for clustering at all. From what I can tell, the standard way of weighing the KNNG for graph-based clustering in single-cell is to use the Jaccard index of the mutual nearest neighbors to weigh the edges). From an implementation standpoint, the `sc.pp.tsne_negihbors` will inevitably have to call the UMAP KNNG construction, since I can see that it's not split out in the code-base. `sc.pp.neighbors` calls the UMAP implementation directly, and since th",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-759374009:1441,rout,route,1441,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-759374009,2,['rout'],['route']
Integrability,"mple_rate, init_pos, random_state, a, b, copy, method, neighbors_key); 192 default_epochs = 500 if neighbors['connectivities'].shape[0] <= 10000 else 200; 193 n_epochs = default_epochs if maxiter is None else maxiter; --> 194 X_umap = simplicial_set_embedding(; 195 X,; 196 neighbors['connectivities'].tocoo(),. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'; ```. And the versions I've been running:; anndata 0.7.8; asttokens 2.0.5; bcrypt 3.2.0; Bottleneck 1.3.2; brotlipy 0.7.0; cached-property 1.5.2; certifi 2021.10.8; cffi 1.15.0; charset-normalizer 2.0.12; chart-studio 1.1.0; click 8.0.4; cmake 3.22.2; colorama 0.4.4; conda 4.11.0; conda-package-handling 1.7.3; cryptography 36.0.1; cycler 0.11.0; Cython 0.29.20; devtools 0.8.0; dunamai 1.9.0; executing 0.8.2; fa2 0.3.5; Fabric 1.6.1; fonttools 4.29.1; get_version 3.5.4; h5py 3.6.0; idna 3.3; igraph 0.9.9; install 1.3.5; joblib 1.1.0; kiwisolver 1.3.2; legacy-api-wrap 1.2; llvmlite 0.38.0; loom 0.0.18; loompy 3.0.6; mamba 0.15.3; matplotlib 3.5.1; mkl-fft 1.3.1; mkl-random 1.2.2; mkl-service 2.4.0; MulticoreTSNE 0.1; natsort 8.1.0; networkx 2.6.3; numba 0.55.1; numexpr 2.8.1; numpy 1.21.2; numpy-groupies 0.9.14; opt-einsum 3.3.0; packaging 21.3; pandas 1.4.1; paramiko 2.9.2; patsy 0.5.2; Pillow 9.0.1; pip 21.2.4; plotly 5.6.0; pycosat 0.6.3; pycparser 2.21; PyNaCl 1.5.0; pynndescent 0.5.6; pyOpenSSL 22.0.0; pyparsing 3.0.7; PyQt5 5.12.3; PyQt5_sip 4.19.18; PyQtChart 5.12; PyQtWebEngine 5.12.1; pyro-api 0.1.2; pyro-ppl 1.8.0; pysam 0.18.0; PySocks 1.7.1; python-dateutil 2.8.2; pytz 2021.3; requests 2.27.1; retrying 1.3.3; ruamel-yaml-conda 0.15.80; scanpy 1.7.0rc1; scikit-learn 1.0.2; scipy 1.7.3; seaborn 0.11.2; setuptools 58.0.4; sinfo 0.3.4; six 1.16.0; statsmodels 0.13.2; stdlib-list 0.8.0; tables 3.7.0; tenacity 8.0.1; texttable 1.6.4; threadpoolctl 3.1.0; torch 1.10.2; tornado 6.1; tqdm 4.62.3; umap-learn 0.4.6; unicodedata2 14.0.0; url",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1579#issuecomment-1062410460:1790,wrap,wrap,1790,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579#issuecomment-1062410460,2,['wrap'],['wrap']
Integrability,"ms, dtype, name, reduced_meta); 362 try:; --> 363 meta = func(reduced_meta, computing_meta=True); 364 # no meta keyword argument exists for func, and it isn't required. File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/dask/array/reductions.py:685, in mean_combine(pairs, sum, numel, dtype, axis, computing_meta, **kwargs); 684 ns = deepmap(lambda pair: pair[""n""], pairs) if not computing_meta else pairs; --> 685 n = _concatenate2(ns, axes=axis).sum(axis=axis, **kwargs); 687 if computing_meta:. TypeError: _cs_matrix.sum() got an unexpected keyword argument 'keepdims'. During handling of the above exception, another exception occurred:. IndexError Traceback (most recent call last); Cell In[19], line 1; ----> 1 result = sc.pp.highly_variable_genes(adata, inplace=False); 2 result. File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /mnt/workspace/repos/scanpy/scanpy/preprocessing/_highly_variable_genes.py:702, in highly_variable_genes(***failed resolving arguments***); 699 del min_disp, max_disp, min_mean, max_mean, n_top_genes; 701 if batch_key is None:; --> 702 df = _highly_variable_genes_single_batch(; 703 adata, layer=layer, cutoff=cutoff, n_bins=n_bins, flavor=flavor; 704 ); 705 else:; 706 df = _highly_variable_genes_batched(; 707 adata, batch_key, layer=layer, cutoff=cutoff, n_bins=n_bins, flavor=flavor; 708 ). File /mnt/workspace/repos/scanpy/scanpy/preprocessing/_highly_variable_genes.py:274, in _highly_variable_genes_single_batch(adata, layer, cutoff, n_bins, flavor); 271 else:; 272 X = np.expm1(X); --> 274 mean, var = _get_mean_var(X); 275 # now actually compute t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2809#issuecomment-1906001725:1468,wrap,wraps,1468,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2809#issuecomment-1906001725,1,['wrap'],['wraps']
Integrability,"n func_wrapper(elem, *args, **kwargs); 155 try:; --> 156 return func(elem, *args, **kwargs); 157 except Exception as e:. ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/anndata/_io/h5ad.py in read_group(group); 505 if ""h5sparse_format"" in group.attrs: # Backwards compat; --> 506 return SparseDataset(group).to_memory(); 507 . ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/anndata/_core/sparse_dataset.py in to_memory(self); 370 mtx = format_class(self.shape, dtype=self.dtype); --> 371 mtx.data = self.group[""data""][...]; 372 mtx.indices = self.group[""indices""][...]. h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/h5py/_hl/dataset.py in __getitem__(self, args); 572 fspace = selection.id; --> 573 self.id.read(mspace, fspace, arr, mtype, dxpl=self._dxpl); 574 . h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/h5d.pyx in h5py.h5d.DatasetID.read(). h5py/_proxy.pyx in h5py._proxy.dset_rw(). h5py/_proxy.pyx in h5py._proxy.H5PY_H5Dread(). OSError: Can't read data (file read failed: time = Sat Aug 1 13:27:54 2020; , filename = '/path.../filtered_gene_bc_matrices.h5ad', file descriptor = 47, errno = 5, error message = 'Input/output error', buf = 0x55ec782e9031, total read size = 7011, bytes this sub-read = 7011, bytes actually read = 18446744073709551615, offset = 0). During handling of the above exception, another exception occurred:. AnnDataReadError Traceback (most recent call last); <ipython-input-14-faac769583f8> in <module>; 17 #while True:; 18 #try:; ---> 19 adatas.append(sc.read_h5ad(file)); 20 file_diffs.append('_'.join([file.split('/')[i] for i in diff_path_idx])); 21 #break. ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/anndata/_io/h5ad.py in read_h5ad(filename, backed, as_sparse, as_sparse_fmt, chunk_size); 411 d[k] = read_dataframe(f[k]); 412 else: # Base case; --> 4",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1351:1681,wrap,wrapper,1681,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351,1,['wrap'],['wrapper']
Integrability,"n reraise; raise value.with_traceback(tb); numba.errors.TypingError: Failed at nopython (nopython frontend); Unknown attribute 'partition' of type Module(<module 'numpy' from '/n/app/python/3.7.4-ext/lib/python3.7/site-packages/numpy/__init__.py'>). File ""../../../../../../../home/pjb40/jupytervenv/lib/python3.7/site-packages/scanpy/preprocessing/_qc.py"", line 399:; def _top_segment_proportions_sparse_csr(data, indptr, ns):; <source elided>; elif (end - start) > maxidx:; partitioned[i, :] = -(np.partition(-data[start:end], maxidx))[:maxidx]; ^. [1] During: typing of get attribute at /home/pjb40/jupytervenv/lib/python3.7/site-packages/scanpy/preprocessing/_qc.py (399). File ""../../../../../../../home/pjb40/jupytervenv/lib/python3.7/site-packages/scanpy/preprocessing/_qc.py"", line 399:; def _top_segment_proportions_sparse_csr(data, indptr, ns):; <source elided>; elif (end - start) > maxidx:; partitioned[i, :] = -(np.partition(-data[start:end], maxidx))[:maxidx]; ^. This is not usually a problem with Numba itself but instead often caused by; the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:; http://numba.pydata.org/numba-doc/dev/reference/pysupported.html; and; http://numba.pydata.org/numba-doc/dev/reference/numpysupported.html. For more information about typing errors and how to debug them visit:; http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message; and traceback, along with a minimal reproducer at:; https://github.com/numba/numba/issues/new. ...; ```. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; > ... [Version](url) of the packages in path : ; scanpy 1.4.4.post1; anndata 0.6.22.post1; anndata2ri 1.0.1; umap-learn 0.3.10; numpy 1.16.5; scipy 1.3.1; pandas 1.0.1; scikit-learn 0.21.3; statsmodels 0.10.1; python-igraph 0.7.1.post6; louvain 0.6.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1193:4328,message,message,4328,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1193,1,['message'],['message']
Integrability,"n't match with a 2D-gaussian; - skewing of the ""absence"" of a marker depending on presence of another marker; - a weird double-positive tail that extends along the diagonal. These types of effects are reminiscent of [flow cytometry artifacts](https://docs.flowjo.com/flowjo/graphs-and-gating/gw-transform-overview/gw-transform-digital/). However, without proving which one is ground truth, we don't know for sure which one is true. At least initially, I would think that the CLR plots look more plausible. ![image](https://user-images.githubusercontent.com/20694664/83360046-51985080-a34c-11ea-9ec0-2057301ae4fc.png). ![image](https://user-images.githubusercontent.com/20694664/83360065-74c30000-a34c-11ea-9e0b-d28cea53993e.png). ![image](https://user-images.githubusercontent.com/20694664/83360079-84dadf80-a34c-11ea-9026-4256d8a3199b.png). I used a neutral word earlier: that CLR ""injects"" additional changes, but now it seems that may be a positive thing because many of these empirical cases seem believable from a biological standpoint -- a more systematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in a relative ratio way, so doesn't preserve a 1-to-1 monotonic mapping as a rescaling function like log, asinh, biexponential/logicle/vlog would. But without having tested it in all cases, it's not clear that it will *always* be better with this kind of assumption for other types of markers that may have different fundamental characteristics. I would recommend that people plot both ways and decide on a case-by-case basis for each marker. . EDIT: I looked around a bit more in the literature and do think that the absolute count based transforms (i.e. all the ones not the CLRatio based), do seem to represent physical reality more: cell size (as one explana",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1117#issuecomment-636513215:1543,inject,injects,1543,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-636513215,2,['inject'],['injects']
Integrability,"n3.8/site-packages/docutils/readers/__init__.py"", line 77, in parse; self.parser.parse(self.input, document); File ""/usr/local/lib/python3.8/site-packages/sphinx/parsers.py"", line 100, in parse; self.statemachine.run(inputlines, document, inliner=self.inliner); File ""/usr/local/lib/python3.8/site-packages/docutils/parsers/rst/states.py"", line 170, in run; results = StateMachineWS.run(self, input_lines, input_offset,; File ""/usr/local/lib/python3.8/site-packages/docutils/statemachine.py"", line 241, in run; context, next_state, result = self.check_line(; File ""/usr/local/lib/python3.8/site-packages/docutils/statemachine.py"", line 459, in check_line; return method(match, context, next_state); File ""/usr/local/lib/python3.8/site-packages/docutils/parsers/rst/states.py"", line 2769, in underline; self.section(title, source, style, lineno - 1, messages); File ""/usr/local/lib/python3.8/site-packages/docutils/parsers/rst/states.py"", line 327, in section; self.new_subsection(title, lineno, messages); File ""/usr/local/lib/python3.8/site-packages/docutils/parsers/rst/states.py"", line 393, in new_subsection; newabsoffset = self.nested_parse(; File ""/usr/local/lib/python3.8/site-packages/docutils/parsers/rst/states.py"", line 281, in nested_parse; state_machine.run(block, input_offset, memo=self.memo,; File ""/usr/local/lib/python3.8/site-packages/docutils/parsers/rst/states.py"", line 196, in run; results = StateMachineWS.run(self, input_lines, input_offset); File ""/usr/local/lib/python3.8/site-packages/docutils/statemachine.py"", line 241, in run; context, next_state, result = self.check_line(; File ""/usr/local/lib/python3.8/site-packages/docutils/statemachine.py"", line 459, in check_line; return method(match, context, next_state); File ""/usr/local/lib/python3.8/site-packages/docutils/parsers/rst/states.py"", line 2344, in explicit_markup; self.explicit_list(blank_finish); File ""/usr/local/lib/python3.8/site-packages/docutils/parsers/rst/states.py"", line 2369, in explicit_list; newl",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1946#issuecomment-877995557:7043,message,messages,7043,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1946#issuecomment-877995557,1,['message'],['messages']
Integrability,"n39\lib\site-packages\numba\core\lowering.py"", line 624, in lower_assign; return self.lower_expr(ty, value); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1159, in lower_expr; res = self.lower_call(resty, expr); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 889, in lower_call; res = self._lower_call_normal(fnty, expr, signature); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1130, in _lower_call_normal; res = impl(self.builder, argvals, self.loc); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1201, in __call__; res = self._imp(self._context, builder, self._sig, args, loc=loc); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1231, in wrapper; return fn(*args, **kwargs); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\cpython\rangeobj.py"", line 40, in range1_impl; state.stop = stop; File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 164, in __setattr__; self[self._datamodel.get_field_position(field)] = value; File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 188, in __setitem__; raise TypeError(""Invalid store of {value.type} to ""; TypeError: Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x0000015592499520> (trying to write member #1). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""C:\Users\RUTBO\PycharmProjects\pvalue\single cell analysis.py"", line 44, in <module>; sc.pp.neighbors(tab, n_neighbors=10, n_pcs=40); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 139, in",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2160:2015,wrap,wrapper,2015,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2160,1,['wrap'],['wrapper']
Integrability,"n39\lib\site-packages\numba\core\lowering.py"", line 624, in lower_assign; return self.lower_expr(ty, value); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1159, in lower_expr; res = self.lower_call(resty, expr); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 889, in lower_call; res = self._lower_call_normal(fnty, expr, signature); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1130, in _lower_call_normal; res = impl(self.builder, argvals, self.loc); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1201, in __call__; res = self._imp(self._context, builder, self._sig, args, loc=loc); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1231, in wrapper; return fn(*args, **kwargs); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\cpython\rangeobj.py"", line 40, in range1_impl; state.stop = stop; File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 164, in __setattr__; self[self._datamodel.get_field_position(field)] = value; File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 188, in __setitem__; raise TypeError(""Invalid store of {value.type} to ""; TypeError: Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x0000015592499520> (trying to write member #1). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""C:\Users\RUTBO\PycharmProjects\pvalue\single cell analysis.py"", line 44, in <module>; sc.pp.neighbors(tab, n_neighbors=10, n_pcs=40); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 139, in",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1652#issuecomment-1054106418:2084,wrap,wrapper,2084,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652#issuecomment-1054106418,1,['wrap'],['wrapper']
Integrability,"n_info.; -----; anndata 0.7.8; scanpy 1.8.2; sinfo 0.3.4; -----; PIL 8.3.1; aa8f2297d25b4dc6fd3d98411eb3ba53823c4f42 NA; absl NA; anyio NA; astunparse 1.6.3; attr 21.2.0; babel 2.9.1; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; celltypist 0.2.0; certifi 2021.05.30; cffi 1.14.6; charset_normalizer 2.0.4; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; decorator 5.0.9; defusedxml 0.7.1; entrypoints 0.3; et_xmlfile 1.1.0; flatbuffers 2.0; gast 0.5.3; google NA; h5py 3.3.0; idna 3.2; igraph 0.9.9; ipykernel 6.2.0; ipython_genutils 0.2.0; jedi 0.18.0; jinja2 3.0.1; joblib 1.0.1; json5 NA; jsonschema 3.2.0; jupyter_server 1.10.2; jupyterlab_server 2.7.1; keras 2.8.0; keras_preprocessing 1.1.2; kiwisolver 1.3.1; leidenalg 0.8.9; llvmlite 0.38.0; louvain 0.7.1; markupsafe 2.0.1; matplotlib 3.4.3; matplotlib_inline NA; mpl_toolkits NA; natsort 8.1.0; nbclassic NA; nbformat 5.1.3; nbinom_ufunc NA; numba 0.55.1; numexpr 2.8.1; numpy 1.21.5; openpyxl 3.0.9; opt_einsum v3.3.0; packaging 21.3; pandas 1.3.2; parso 0.8.2; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prometheus_client NA; prompt_toolkit 3.0.19; ptyprocess 0.7.0; pvectorc NA; pycparser 2.20; pyexpat NA; pygments 2.10.0; pynndescent 0.5.6; pyparsing 2.4.7; pyrsistent NA; pytz 2021.1; requests 2.26.0; scipy 1.7.1; send2trash NA; six 1.16.0; sklearn 0.24.2; sniffio 1.2.0; statsmodels 0.13.2; storemagic NA; tables 3.7.0; tensorboard 2.8.0; tensorflow 2.8.0; termcolor 1.1.0; terminado 0.11.1; texttable 1.6.4; tornado 6.1; tqdm 4.63.0; traitlets 5.0.5; typing_extensions NA; umap 0.5.2; urllib3 1.26.6; wcwidth 0.2.5; websocket 1.2.1; wrapt 1.14.0; xlrd 1.2.0; zmq 22.2.1; -----; IPython 7.26.0; jupyter_client 6.1.12; jupyter_core 4.7.1; jupyterlab 3.1.7; notebook 6.4.3; -----; Python 3.9.6 (default, Aug 18 2021, 11:08:34) [GCC 4.8.5 20150623 (Red Hat 4.8.5-44)]; Linux-3.10.0-1160.41.1.el7.x86_64-x86_64-with-glibc2.17; 36 logical CPU cores, x86_64; -----; Session information updated at 2022-03-26 18:52. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2193:2938,wrap,wrapt,2938,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2193,1,['wrap'],['wrapt']
Integrability,"na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, colorbar_loc, vmax, vmin, vcenter, norm, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs); 397 rasterized=settings._vector_friendly,; 398 norm=normalize,; --> 399 **kwargs,; 400 ); 401 . ~/.conda/envs/python_spatial/lib/python3.7/site-packages/scanpy/plotting/_utils.py in circles(x, y, s, ax, marker, c, vmin, vmax, scale_factor, **kwargs); 1106 # while you can only set `facecolors` with a value for all.; 1107 if scale_factor != 1.0:; -> 1108 x = x * scale_factor; 1109 y = y * scale_factor; 1110 zipped = np.broadcast(x, y, s). TypeError: can't multiply sequence by non-int of type 'float'; ```; Bascially, the same type of error keeps showing up no matter what the `color = ` parameter is. I put a gene name there, and the same message showed up. Additional information about the error (the image that showed up together with the error message from the jupyter notebook cell): ![image](https://user-images.githubusercontent.com/78611089/209900312-f6acd040-93e5-43b8-858d-3d1ef0c91cd0.png). #### Versions. <details>. -----; anndata 0.8.0; scanpy 1.9.1; -----; PIL 9.2.0; annoy NA; asciitree NA; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; cffi 1.15.1; cloudpickle 2.2.0; cycler 0.10.0; cython_runtime NA; dask 2022.02.0; dateutil 2.8.2; debugpy 1.6.3; decorator 5.1.1; defusedxml 0.7.1; entrypoints 0.4; fasteners 0.18; fbpca NA; fsspec 2022.11.0; google NA; h5py 3.7.0; igraph 0.10.2; intervaltree NA; ipykernel 6.16.2; ipython_genutils 0.2.0; ipywidgets 8.0.4; jedi 0.18.2; jinja2 3.1.2; joblib 1.2.0; jupyter_server 1.23.4; kiwisolver 1.4.4; leidenalg 0.9.1; llvmlite 0.39.1; markupsafe 2.1.1; matplotlib 3.5.3; matplotlib_inline 0.1.6; mpl_toolkits NA; msgpack 1.0.4; natsort 8.2.0; nbinom_ufunc NA; numba 0.56.4; numcodecs 0.10.2; numpy 1.21.6; packaging 22.0; pandas 1.3.5; parso 0.8.3; pexpect 4.8.0; pickleshare 0.7.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2391:3127,message,message,3127,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2391,1,['message'],['message']
Integrability,"ng and then only plotting the fit. Can you help me solve the issue by sharing the data with me? Or, can you find an example that can reproduce the problem?; > […](#); > On Tue, May 7, 2019 at 10:19 AM brianpenghe ***@***.***> wrote: I was trying to plot a heatmap using this command: ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0, dendrogram=True, save='ClusterMap.png') And it didn't finish running after an overnight, with the following warning message: WARNING: Gene labels are not shown when more than 50 genes are visualized. To show gene labels set show_gene_labels=True /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227: UserWarning: The maximal number of iterations maxit (set to 20 by the program) allowed for finding a smoothing spline with fp=s has been reached: s too small. There is an approximation returned but the corresponding weighted sum of squared residuals does not satisfy the condition abs(fp-s)/s < tol. warnings.warn(message) I don't understand why this is taking this long because seaborn was able to finish plotting within 30 minutes. Do you know why? — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#633>, or mute the thread <https://github.com/notifications/unsubscribe-auth/ABF37VNDX37RZL256MWKDM3PUE3RFANCNFSM4HLGOYGA> .; > -- Fidel Ramirez. I was planning to plot a heatmap of 300 genes. However, I have 90k cells. I guess the time-consuming part is the PCA because that's what's required to do the clustering by groups. I thought a naive way to do the clustering is just to construct the ""pseudobulks"" for each group by calculating the average and then simply clustering the ""pseudobulks"", instead of trying to look at individual cells. Another advantage of checking the pseudobulk is that the size of each group won't affect the landscape of principle components in that way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/633#issuecomment-491103142:1560,message,message,1560,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633#issuecomment-491103142,2,['message'],['message']
Integrability,"niconda3/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, *args, **kwargs); 176 try:; --> 177 return func(elem, *args, **kwargs); 178 except Exception as e:. ~/miniconda3/lib/python3.8/site-packages/anndata/_io/h5ad.py in read_group(group); 526 if encoding_type:; --> 527 EncodingVersions[encoding_type].check(; 528 group.name, group.attrs[""encoding-version""]. ~/miniconda3/lib/python3.8/enum.py in __getitem__(cls, name); 343 def __getitem__(cls, name):; --> 344 return cls._member_map_[name]; 345 . KeyError: 'dict'. During handling of the above exception, another exception occurred:. AnnDataReadError Traceback (most recent call last); <ipython-input-17-97568eff5295> in <module>; ----> 1 adata=anndata.read_h5ad('./visium_merge_inter_upload.h5ad'). ~/miniconda3/lib/python3.8/site-packages/anndata/_io/h5ad.py in read_h5ad(filename, backed, as_sparse, as_sparse_fmt, chunk_size); 419 d[k] = read_dataframe(f[k]); 420 else: # Base case; --> 421 d[k] = read_attribute(f[k]); 422 ; 423 d[""raw""] = _read_raw(f, as_sparse, rdasp). ~/miniconda3/lib/python3.8/functools.py in wrapper(*args, **kw); 872 '1 positional argument'); 873 ; --> 874 return dispatch(args[0].__class__)(*args, **kw); 875 ; 876 funcname = getattr(func, '__name__', 'singledispatch function'). ~/miniconda3/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, *args, **kwargs); 181 else:; 182 parent = _get_parent(elem); --> 183 raise AnnDataReadError(; 184 f""Above error raised while reading key {elem.name!r} of ""; 185 f""type {type(elem)} from {parent}."". AnnDataReadError: Above error raised while reading key '/layers' of type <class 'h5py._hl.group.Group'> from /.; ```. ### Minimal code sample (that we can copy&paste without having any data). ```python; # Your code here; ```. ```pytb; [Paste the error output produced by the above code here]; ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2376:1299,wrap,wrapper,1299,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2376,1,['wrap'],['wrapper']
Integrability,"not yet, but we sent the message to them only recently.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/542#issuecomment-479554225:25,message,message,25,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/542#issuecomment-479554225,1,['message'],['message']
Integrability,"nt handling gets very confusing. However, I think we could do something more like this (note, it's not tested yet, and could be cleaner... it's my ten minute version):. <details>; <summary> Alternative implementation of scale </summary>. ```python; @singledispatch; def scale(X, *args, **kwargs):; """"""\; Scale data to unit variance and zero mean.; .. note::; Variables (genes) that do not display any variation (are constant across; all observations) are retained and set to 0 during this operation. In; the future, they might be set to NaNs.; Parameters; ----------; data; The (annotated) data matrix of shape `n_obs` × `n_vars`.; Rows correspond to cells and columns to genes.; zero_center; If `False`, omit zero-centering variables, which allows to handle sparse; input efficiently.; max_value; Clip (truncate) to this value after scaling. If `None`, do not clip.; copy; If an :class:`~anndata.AnnData` is passed,; determines whether a copy is returned.; Returns; -------; Depending on `copy` returns or updates `adata` with a scaled `adata.X`,; annotated with `'mean'` and `'std'` in `adata.var`.; """"""; return scale_array(X, *args, **kwargs). @scale.register(np.ndarray); def scale_array(; X,; zero_center: bool = True,; max_value: Optional[float] = None,; copy: bool = False,; return_mean_var=False,; ):; if copy:; X = X.copy(); if not zero_center and max_value is not None:; logg.info( # Be careful of what? This should be more specific; '... be careful when using `max_value` '; 'without `zero_center`.'; ); if max_value is not None:; logg.debug(f'... clipping at max_value {max_value}'); mean, std = _scale(X, zero_center) # the code from here could probably just be ; # do the clipping; if max_value is not None:; X[X > max_value] = max_value; if return_mean_var:; return X, mean, var; else:; return X. @scale.register(AnnData); def scale_anndata(; adata: AnnData,; *,; zero_center: bool = True,; max_value: Optional[float] = None,; copy: bool = False,; ) -> Optional[AnnData]:; adata = adat",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1135#issuecomment-608200735:1568,Depend,Depending,1568,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135#issuecomment-608200735,1,['Depend'],['Depending']
Integrability,"nt; 48 from pynndescent.distances import named_distances as pynn_named_distances; 49 from pynndescent.sparse import sparse_named_distances as pynn_sparse_named_distances. ~/.local/lib/python3.9/site-packages/pynndescent/__init__.py in <module>; 1 import pkg_resources; 2 import numba; ----> 3 from .pynndescent_ import NNDescent, PyNNDescentTransformer; 4 ; 5 # Workaround: https://github.com/numba/numba/issues/3341. ~/.local/lib/python3.9/site-packages/pynndescent/pynndescent_.py in <module>; 19 import heapq; 20 ; ---> 21 import pynndescent.sparse as sparse; 22 import pynndescent.sparse_nndescent as sparse_nnd; 23 import pynndescent.distances as pynnd_dist. ~/.local/lib/python3.9/site-packages/pynndescent/sparse.py in <module>; 341 },; 342 ); --> 343 def sparse_alternative_jaccard(ind1, data1, ind2, data2):; 344 num_non_zero = arr_union(ind1, ind2).shape[0]; 345 num_equal = arr_intersect(ind1, ind2).shape[0]. ~/.local/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func); 216 with typeinfer.register_dispatcher(disp):; 217 for sig in sigs:; --> 218 disp.compile(sig); 219 disp.disable_compile(); 220 return disp. ~/.local/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs); 30 def _acquire_compile_lock(*args, **kwargs):; 31 with self:; ---> 32 return func(*args, **kwargs); 33 return _acquire_compile_lock; 34 . ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig); 817 self._cache_misses[sig] += 1; 818 try:; --> 819 cres = self._compiler.compile(args, return_type); 820 except errors.ForceLiteralArg as e:; 821 def folded(args, kws):. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type); 80 return retval; 81 else:; ---> 82 raise retval; 83 ; 84 def _compile_cached(self, args, return_type):. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type); 90 ; 91 try:; ---> 92 retval = self._compile",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1652:3262,wrap,wrapper,3262,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652,1,['wrap'],['wrapper']
Integrability,"ntime NA; dateutil 2.8.2; debugpy 1.6.0; decorator 5.1.1; defusedxml 0.7.1; deprecate 0.3.2; docrep 0.3.2; entrypoints 0.4; etils 0.6.0; executing 0.8.3; flatbuffers 2.0; flax 0.5.2; fsspec 2022.5.0; google NA; h5py 3.7.0; hypergeom_ufunc NA; idna 3.3; igraph 0.9.11; iniconfig NA; ipykernel 6.15.1; ipython_genutils 0.2.0; ipywidgets 7.7.1; jax 0.3.14; jaxlib 0.3.14; jedi 0.18.1; joblib 1.1.0; kiwisolver 1.4.3; leidenalg 0.8.10; llvmlite 0.38.1; louvain 0.7.1; matplotlib 3.5.2; matplotlib_inline NA; mpl_toolkits NA; msgpack 1.0.4; mudata 0.2.0; multipledispatch 0.6.0; natsort 8.1.0; nbinom_ufunc NA; newick 1.0.0; numba 0.55.2; numpy 1.22.4; numpyro 0.10.0; opt_einsum v3.3.0; optax 0.1.3; packaging 21.3; pandas 1.4.3; parso 0.8.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; pluggy 1.0.0; prompt_toolkit 3.0.30; psutil 5.9.1; ptyprocess 0.7.0; pure_eval 0.2.2; py 1.11.0; pycparser 2.21; pydev_ipython NA; pydevconsole NA; pydevd 2.8.0; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.12.0; pyparsing 3.0.9; pyro 1.8.1; pytest 7.1.2; pytorch_lightning 1.6.5; pytz 2022.1; requests 2.28.1; rich NA; scHPL NA; scarches 0.5.3; scipy 1.8.1; scvi 0.17.1; seaborn 0.11.2; session_info 1.0.0; setuptools 63.1.0; six 1.16.0; sklearn 1.1.1; socks 1.7.1; stack_data 0.3.0; statsmodels 0.13.2; tensorboard 2.9.1; texttable 1.6.4; threadpoolctl 3.1.0; toolz 0.12.0; torch 1.12.0+cu102; torchmetrics 0.9.2; tornado 6.2; tqdm 4.64.0; traitlets 5.3.0; tree 0.1.7; typing_extensions NA; urllib3 1.26.10; wcwidth 0.2.5; wrapt 1.14.1; yaml 6.0; zmq 23.2.0; -----; IPython 8.4.0; jupyter_client 7.3.4; jupyter_core 4.10.0; notebook 6.4.12; -----; Python 3.10.5 | packaged by conda-forge | (main, Jun 14 2022, 07:04:59) [GCC 10.3.0]; Linux-3.10.0-1160.71.1.el7.x86_64-x86_64-with-glibc2.17; -----; Session information updated at 2022-09-09 14:21; combined_data.write(f""{workspace}Data/Models/Healthy/combined_hvgs_recalculated.h5ad""); combined_data.write(f""{workspace}Data/Models/H",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2321:3165,wrap,wrapt,3165,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2321,1,['wrap'],['wrapt']
Integrability,nx <2.3 depends on a since-removed matplotlib API,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1323:8,depend,depends,8,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1323,1,['depend'],['depends']
Integrability,"o do velocity analysis with the scvelo integration. Installing scanpy as well as hdf5/loom compatibility is remarkably easier on python than in R, which gives scanpy users an obvious advantage. . I've learned so much using this package and it has allowed me to display my data in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to access the current version of this software? . I think that major partitions identified with CytOF or scRNA-seq can be linked together providing a coarse-grained mechanism to demonstrate how heterogeneity identified with each technique relates to each other based on a given experimental time point. . (3) Histogram inte",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/510:1267,integrat,integrated,1267,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510,1,['integrat'],['integrated']
Integrability,"obs, n_neighbors, set_op_mix_ratio, local_connectivity); 385 # umap 0.5.0; 386 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""); --> 387 from umap.umap_ import fuzzy_simplicial_set; 388 ; 389 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)); ; ~/miniforge3/envs/scVelo/lib/python3.8/site-packages/umap/__init__.py in <module>; ----> 1 from .umap_ import UMAP; 2 ; 3 # Workaround: https://github.com/numba/numba/issues/3341; 4 import numba; 5 ; ; ~/miniforge3/envs/scVelo/lib/python3.8/site-packages/umap/umap_.py in <module>; 52 from umap.spectral import spectral_layout; 53 from umap.utils import deheap_sort, submatrix; ---> 54 from umap.layouts import (; 55 optimize_layout_euclidean,; 56 optimize_layout_generic,; ; ~/miniforge3/envs/scVelo/lib/python3.8/site-packages/umap/layouts.py in <module>; 37 },; 38 ); ---> 39 def rdist(x, y):; 40 """"""Reduced Euclidean distance.; 41 ; ; ~/miniforge3/envs/scVelo/lib/python3.8/site-packages/numba/core/decorators.py in wrapper(func); 219 with typeinfer.register_dispatcher(disp):; 220 for sig in sigs:; --> 221 disp.compile(sig); 222 disp.disable_compile(); 223 return disp; ; ~/miniforge3/envs/scVelo/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, sig); 907 with ev.trigger_event(""numba:compile"", data=ev_details):; 908 try:; --> 909 cres = self._compiler.compile(args, return_type); 910 except errors.ForceLiteralArg as e:; 911 def folded(args, kws):; ; ~/miniforge3/envs/scVelo/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, args, return_type); 77 ; 78 def compile(self, args, return_type):; ---> 79 status, retval = self._compile_cached(args, return_type); 80 if status:; 81 return retval; ; ~/miniforge3/envs/scVelo/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type); 91 ; 92 try:; ---> 93 retval = self._compile_core(args, return_type); 94 except errors.TypingError as e:; 95 self._failed_cache[key] = e; ; ~/miniforge3/envs/scVelo/lib",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1799:3451,wrap,wrapper,3451,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799,1,['wrap'],['wrapper']
Integrability,"ode 1]; ```. If I switch to the terminal and try `pip` or `conda` I get:. ```; pip install scanpy; ```. ```; Requirement already satisfied: scanpy in /home/tsundoku/anaconda3/lib/python3.7/site-packages (1.4.5.post2); Requirement already satisfied: setuptools-scm in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (3.3.3); Requirement already satisfied: scipy>=1.3 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (1.3.2); Requirement already satisfied: pandas>=0.21 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.25.3); Requirement already satisfied: packaging in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (19.2); Requirement already satisfied: natsort in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (7.0.0); Requirement already satisfied: statsmodels>=0.10.0rc2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.10.1); Requirement already satisfied: legacy-api-wrap in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (1.2); Requirement already satisfied: anndata>=0.6.22.post1 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.6.22.post1); Requirement already satisfied: tqdm in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (4.40.0); Requirement already satisfied: numba>=0.41.0 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.46.0); Requirement already satisfied: joblib in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.14.0); Requirement already satisfied: patsy in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.5.1); Requirement already satisfied: importlib-metadata>=0.7; python_version < ""3.8"" in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy)(1.1.0); Requirement already satisfied: tables in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (3.6.1); Requirement already satisfied:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:2231,wrap,wrap,2231,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452,2,['wrap'],['wrap']
Integrability,"of post doublets removal and QC plot; Running Scrublet; normalizing counts per cell. C:\ProgramData\Anaconda3\envs\dl\lib\site-packages\scanpy\preprocessing\_normalization.py:233: UserWarning: Some cells have zero counts; warn(UserWarning(""Some cells have zero counts"")). finished (0:00:00); WARNING: adata.X seems to be already log-transformed.; extracting highly variable genes. C:\ProgramData\Anaconda3\envs\dl\lib\site-packages\scanpy\preprocessing\_simple.py:377: RuntimeWarning: invalid value encountered in log1p; np.log1p(X, out=X). ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); Cell In[59], line 2; 1 print('Begin of post doublets removal and QC plot'); ----> 2 sc.pp.scrublet(alldata, n_neighbors=10); 3 alldata = alldata[alldata.obs['predicted_doublet']==False, :].copy(); 4 n1 = alldata.shape[0]. File C:\ProgramData\Anaconda3\envs\dl\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File C:\ProgramData\Anaconda3\envs\dl\lib\site-packages\scanpy\preprocessing\_scrublet\__init__.py:282, in scrublet(adata, adata_sim, batch_key, sim_doublet_ratio, expected_doublet_rate, stdev_doublet_rate, synthetic_doublet_umi_subsampling, knn_dist_metric, normalize_variance, log_transform, mean_center, n_prin_comps, use_approx_neighbors, get_doublet_neighbor_parents, n_neighbors, threshold, verbose, copy, random_state); 279 adata.uns[""scrublet""][""batched_by""] = batch_key; 281 else:; --> 282 scrubbed = _run_scrublet(adata_obs, adata_sim); 284 # Copy outcomes to input object from our processed version; 286 adata.obs[""doublet_score""] = scrubbed[""obs""][""doublet_score""]. File C:\ProgramData\Anaconda3",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3070:7130,wrap,wrapper,7130,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3070,1,['wrap'],['wrapper']
Integrability,"okexec = lambda hook, methods, kwargs: hook.multicall(; File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/pluggy/callers.py"", line 187, in _multicall; res = hook_impl.function(*args); File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/_pytest/python.py"", line 183, in pytest_pyfunc_call; result = testfunction(**testargs); File ""/home/vsts/work/1/s/scanpy/tests/notebooks/test_paga_paul15_subsampled.py"", line 39, in test_paga_paul15_subsampled; sc.tl.draw_graph(adata); File ""/home/vsts/work/1/s/scanpy/tools/_draw_graph.py"", line 181, in draw_graph; logg.info(; File ""/home/vsts/work/1/s/scanpy/logging.py"", line 244, in info; return settings._root_logger.info(msg, time=time, deep=deep, extra=extra); File ""/home/vsts/work/1/s/scanpy/logging.py"", line 56, in info; return self.log(INFO, msg, time=time, deep=deep, extra=extra); File ""/home/vsts/work/1/s/scanpy/logging.py"", line 43, in log; super().log(level, msg, extra=extra); Message: "" finished: added\n 'X_draw_graph_fr', graph_drawing coordinates (adata.obsm)""; Arguments: (); --- Logging error ---; Traceback (most recent call last):; File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/logging/__init__.py"", line 1084, in emit; stream.write(msg + self.terminator); ValueError: I/O operation on closed file.; Call stack:; File ""/opt/hostedtoolcache/Python/3.8.8/x64/bin/pytest"", line 8, in <module>; sys.exit(console_main()); File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/_pytest/config/__init__.py"", line 185, in console_main; code = main(); File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/_pytest/config/__init__.py"", line 162, in main; ret: Union[ExitCode, int] = config.hook.pytest_cmdline_main(; File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/pluggy/hooks.py"", line 286, in __call__; return self._hookexec(self, self.get_hookimpls(), kwargs); File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packag",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1736:2331,Message,Message,2331,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1736,1,['Message'],['Message']
Integrability,"ome/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.21.3); Requirement already satisfied: umap-learn>=0.3.10 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.3.10); Requirement already satisfied: numpy>=1.13.3 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from pandas>=0.21->scanpy) (1.17.4); Requirement already satisfied: pytz>=2017.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from pandas>=0.21->scanpy) (2019.3); Requirement already satisfied: python-dateutil>=2.6.1 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from pandas>=0.21->scanpy) (2.8.1); Requirement already satisfied: pyparsing>=2.0.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from packaging->scanpy) (2.4.5); Requirement already satisfied: six in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from packaging->scanpy) (1.13.0); Requirement already satisfied: setuptools in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from legacy-api-wrap->scanpy) (42.0.2.post20191203); Requirement already satisfied: get-version>=2.0.4 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from legacy-api-wrap->scanpy) (2.1); Requirement already satisfied: llvmlite>=0.30.0dev0 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from numba>=0.41.0->scanpy) (0.30.0); Requirement already satisfied: zipp>=0.5 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from importlib-metadata>=0.7; python_version < ""3.8""->scanpy) (0.6.0); Requirement already satisfied: numexpr>=2.6.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from tables->scanpy) (2.7.0); Requirement already satisfied: decorator>=4.3.0 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from networkx->scanpy) (4.4.1); Requirement already satisfied: kiwisolver>=1.0.1 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from matplotlib==3.0.*->scanpy) (1.1.0); Requirement already satisfied: cycler>=0.10 in /home/tsundoku/anaconda3/lib/pyt",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:4752,wrap,wrap,4752,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452,2,['wrap'],['wrap']
Integrability,"ompile(tuple(argtypes)); 368 except errors.ForceLiteralArg as e:; 369 # Received request for compiler re-entry with the list of arguments. ~/miniconda3/envs/flng/lib/python3.8/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs); 30 def _acquire_compile_lock(*args, **kwargs):; 31 with self:; ---> 32 return func(*args, **kwargs); 33 return _acquire_compile_lock; 34 . ~/miniconda3/envs/flng/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, sig); 823 raise e.bind_fold_arguments(folded); 824 self.add_overload(cres); --> 825 self._cache.save_overload(sig, cres); 826 return cres.entry_point; 827 . ~/miniconda3/envs/flng/lib/python3.8/site-packages/numba/core/caching.py in save_overload(self, sig, data); 669 """"""; 670 with self._guard_against_spurious_io_errors():; --> 671 self._save_overload(sig, data); 672 ; 673 def _save_overload(self, sig, data):. ~/miniconda3/envs/flng/lib/python3.8/site-packages/numba/core/caching.py in _save_overload(self, sig, data); 679 key = self._index_key(sig, _get_codegen(data)); 680 data = self._impl.reduce(data); --> 681 self._cache_file.save(key, data); 682 ; 683 @contextlib.contextmanager. ~/miniconda3/envs/flng/lib/python3.8/site-packages/numba/core/caching.py in save(self, key, data); 494 break; 495 overloads[key] = data_name; --> 496 self._save_index(overloads); 497 self._save_data(data_name, data); 498 . ~/miniconda3/envs/flng/lib/python3.8/site-packages/numba/core/caching.py in _save_index(self, overloads); 540 def _save_index(self, overloads):; 541 data = self._source_stamp, overloads; --> 542 data = self._dump(data); 543 with self._open_for_write(self._index_path) as f:; 544 pickle.dump(self._version, f, protocol=-1). ~/miniconda3/envs/flng/lib/python3.8/site-packages/numba/core/caching.py in _dump(self, obj); 568 ; 569 def _dump(self, obj):; --> 570 return pickle.dumps(obj, protocol=-1); 571 ; 572 @contextlib.contextmanager. TypeError: cannot pickle 'weakref' object; ```. Any ideas?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2406:5535,protocol,protocol,5535,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2406,2,['protocol'],['protocol']
Integrability,"onfirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. I am trying to concatenate multiple datasets. I have Anndata's of 4 runs that I'm trying to analyze together. I tried the plotting functions (e.g. sc.pl. violin) and saving to h5ad files on individual Anndata's and it works. When I merge them via ad.concat I keep getting a Type Error due to Anndata calling sanitize and strings_to_categoricals when I try to save or use plotting functions. I thought it might be because I'm adding new obs before merging but then I tried to just merge without any manipulation of individual Anndata's and it still gave the same error. Then I tried to manually merge the Anndata's by saving X as dataframe and obs as separate dataframes, merging them as dataframes and then creating a new Anndata object. I still keep getting these errors. The error message says that I'm trying to manipulate a view of the Anndata object although I'm not subsetting it and when I do adata.is_view it says False. . I'm not sure how to provide a code sample that can be replicated without data in this case. . ### Minimal code sample. ```python; samples= [ <list of 4 hdf5 files>]; all_adata = []; i = 0; for s in samples:; curr_adata = sc.read_h5ad(f""/mnt/d/Labmembers/Deniz/aging_data/{s}""); curr_adata.var_names_make_unique(); all_adata.append(curr_adata); adata= ad.concat(all_adata); #I get the same type error when I try to do; adata.write('trial.hdf5') ; #or; sc.pl.violin(adata, 'volume'); ```. ### Error output. ```pytb; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); Cell In[8], line 1; ----> 1 sc.pl.violin(adata, 'volume'). File /home/denizparmaksiz/anaconda3/envs/scanpy/lib/python3.9/site-packages/scanpy/plotting/_anndata.py:749, in violin(adata, keys, groupby, log, use_raw, stripplot, jitter, size, layer, scale, order, multi_panel,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2645:1074,message,message,1074,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2645,1,['message'],['message']
Integrability,"ors (0), warnings (1), info (2), hints (3); sc.logging.print_versions(); sc.settings.set_figure_params(dpi=80, frameon=False, figsize=(3, 3), facecolor=""white""). adata_ref = sc.datasets.pbmc3k_processed(); adata = sc.datasets.pbmc68k_reduced(). var_names = adata_ref.var_names.intersection(adata.var_names); adata_ref = adata_ref[:, var_names]; adata = adata[:, var_names]. # start from scratch; del adata.obs[""louvain""]; adata.uns = {}; adata_ref.uns = {}. # example code for ingest function:; sc.pp.neighbors(adata_ref); sc.tl.umap(adata_ref); sc.tl.ingest(adata, adata_ref, obs=""louvain""); ```. ### Error output. ```pytb; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); Cell In[11], line 23; 21 sc.pp.neighbors(adata_ref); 22 sc.tl.umap(adata_ref); ---> 23 sc.tl.ingest(adata, adata_ref, obs=""louvain""). File ~/miniconda3/envs/sc/lib/python3.12/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File ~/miniconda3/envs/sc/lib/python3.12/site-packages/scanpy/tools/_ingest.py:141, in ingest(adata, adata_ref, obs, embedding_method, labeling_method, neighbors_key, inplace, **kwargs); 138 labeling_method = labeling_method * len(obs); 140 ing = Ingest(adata_ref, neighbors_key); --> 141 ing.fit(adata); 143 for method in embedding_method:; 144 ing.map_embedding(method). File ~/miniconda3/envs/sc/lib/python3.12/site-packages/scanpy/tools/_ingest.py:404, in Ingest.fit(self, adata_new); 401 self._obsm = _DimDict(adata_new.n_obs, axis=0); 403 self._adata_new = adata_new; --> 404 self._obsm[""rep""] = self._same_rep(). File ~/miniconda3/envs/sc/lib/python3.12/site-packages/scanpy/tools/_ingest.py:371, in ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3074:1711,wrap,wrapper,1711,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3074,1,['wrap'],['wrapper']
Integrability,"ots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, vcenter, norm, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs); 383 rasterized=settings._vector_friendly,; 384 norm=normalize,; --> 385 **kwargs,; 386 ); 387 . TypeError: functools.partial object got multiple values for keyword argument 'marker'; ```. #### Versions. <details>. WARNING: If you miss a compact list, please try `print_header`!; The `sinfo` package has changed name and is now called `session_info` to become more discoverable and self-explanatory. The `sinfo` PyPI package will be kept around to avoid breaking old installs and you can downgrade to 0.3.2 if you want to use it without seeing this message. For the latest features and bug fixes, please install `session_info` instead. The usage and defaults also changed slightly, so please review the latest README at https://gitlab.com/joelostblom/session_info.; -----; anndata 0.7.6; scanpy 1.8.1; sinfo 0.3.4; -----; PIL 8.3.2; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; cached_property 1.5.2; cffi 1.14.6; cloudpickle 2.0.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; dask 2021.09.1; dateutil 2.8.2; debugpy 1.5.0; decorator 5.1.0; defusedxml 0.7.1; entrypoints 0.3; fsspec 2021.10.0; google NA; h5py 3.4.0; ipykernel 6.4.1; ipython_genutils 0.2.0; ipywidgets 7.6.5; jedi 0.18.0; jinja2 3.0.2; joblib 1.1.0; kiwisolver 1.3.2; llvmlite 0.37.0; markupsafe 2.0.1; matplotlib 3.4.3; matplotlib_inline NA; mpl_toolkits NA; mudata 0.1.0; muon 0.1.1; natsort 7.1.1; nbinom_ufunc NA; numba 0.54.1; numexpr 2.7.3; numpy 1.20.3; packaging 21.0; pandas 1.3.3; parso 0.8.2; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2122:2787,message,message,2787,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2122,1,['message'],['message']
Integrability,"ows where to go to read certain formats.... scanpy? muon? squidpy? Scanpy has read visium but squidpy is the spatial package? I can analyze atac data in scanpy but need to use muon to read the file?. Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages?. Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places?. > How does this impact users vs. developers?. Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use `scio` and then be on their way (a reality that many people do not feel the need to use scanpy/muon). If there are R converters, this wo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059551352:1139,synchroniz,synchronization,1139,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059551352,2,['synchroniz'],['synchronization']
Integrability,"ows, arrows_kwds, groups, components, dimensions, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, colorbar_loc, vmax, vmin, vcenter, norm, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs); 397 rasterized=settings._vector_friendly,; 398 norm=normalize,; --> 399 **kwargs,; 400 ); 401 . ~/.conda/envs/python_spatial/lib/python3.7/site-packages/scanpy/plotting/_utils.py in circles(x, y, s, ax, marker, c, vmin, vmax, scale_factor, **kwargs); 1106 # while you can only set `facecolors` with a value for all.; 1107 if scale_factor != 1.0:; -> 1108 x = x * scale_factor; 1109 y = y * scale_factor; 1110 zipped = np.broadcast(x, y, s). TypeError: can't multiply sequence by non-int of type 'float'; ```; Bascially, the same type of error keeps showing up no matter what the `color = ` parameter is. I put a gene name there, and the same message showed up. Additional information about the error (the image that showed up together with the error message from the jupyter notebook cell): ![image](https://user-images.githubusercontent.com/78611089/209900312-f6acd040-93e5-43b8-858d-3d1ef0c91cd0.png). #### Versions. <details>. -----; anndata 0.8.0; scanpy 1.9.1; -----; PIL 9.2.0; annoy NA; asciitree NA; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; cffi 1.15.1; cloudpickle 2.2.0; cycler 0.10.0; cython_runtime NA; dask 2022.02.0; dateutil 2.8.2; debugpy 1.6.3; decorator 5.1.1; defusedxml 0.7.1; entrypoints 0.4; fasteners 0.18; fbpca NA; fsspec 2022.11.0; google NA; h5py 3.7.0; igraph 0.10.2; intervaltree NA; ipykernel 6.16.2; ipython_genutils 0.2.0; ipywidgets 8.0.4; jedi 0.18.2; jinja2 3.1.2; joblib 1.2.0; jupyter_server 1.23.4; kiwisolver 1.4.4; leidenalg 0.9.1; llvmlite 0.39.1; markupsafe 2.1.1; matplotlib 3.5.3; matplotlib_inline 0.1.6; mpl_toolkits NA; msgpack 1.0.4; natsort 8.2.0; nbinom_ufunc NA; numba 0.56.4",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2391:3019,message,message,3019,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2391,1,['message'],['message']
Integrability,"packages\scanpy\neighbors\__init__.py"", line 139, in neighbors; neighbors.compute_neighbors(; File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 808, in compute_neighbors; self._distances, self._connectivities = _compute_connectivities_umap(; File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 387, in _compute_connectivities_umap; from umap.umap_ import fuzzy_simplicial_set; File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\__init__.py"", line 2, in <module>; from .umap_ import UMAP; File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\umap_.py"", line 41, in <module>; from umap.layouts import (; File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py"", line 39, in <module>; def rdist(x, y):; File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\decorators.py"", line 219, in wrapper; disp.compile(sig); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 965, in compile; cres = self._compiler.compile(args, return_type); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 125, in compile; status, retval = self._compile_cached(args, return_type); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 139, in _compile_cached; retval = self._compile_core(args, return_type); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 152, in _compile_core; cres = compiler.compile_extra(self.targetdescr.typing_context,; File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 693, in compile_extra; return pipeline.compile_extra(func); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2160:4025,wrap,wrapper,4025,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2160,1,['wrap'],['wrapper']
Integrability,"packages\scanpy\neighbors\__init__.py"", line 139, in neighbors; neighbors.compute_neighbors(; File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 808, in compute_neighbors; self._distances, self._connectivities = _compute_connectivities_umap(; File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 387, in _compute_connectivities_umap; from umap.umap_ import fuzzy_simplicial_set; File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\__init__.py"", line 2, in <module>; from .umap_ import UMAP; File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\umap_.py"", line 41, in <module>; from umap.layouts import (; File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py"", line 39, in <module>; def rdist(x, y):; File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\decorators.py"", line 219, in wrapper; disp.compile(sig); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 965, in compile; cres = self._compiler.compile(args, return_type); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 125, in compile; status, retval = self._compile_cached(args, return_type); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 139, in _compile_cached; retval = self._compile_core(args, return_type); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 152, in _compile_core; cres = compiler.compile_extra(self.targetdescr.typing_context,; File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 693, in compile_extra; return pipeline.compile_extra(func); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1652#issuecomment-1054106418:4094,wrap,wrapper,4094,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652#issuecomment-1054106418,1,['wrap'],['wrapper']
Integrability,"paga.py:911: in _paga_graph; sct = ax.scatter(; ../../../../anaconda3/envs/scanpy-dev/lib/python3.8/site-packages/matplotlib/__init__.py:1442: in inner; return func(ax, *map(sanitize_sequence, args), **kwargs); ../../../../anaconda3/envs/scanpy-dev/lib/python3.8/site-packages/matplotlib/axes/_axes.py:4602: in scatter; self._parse_scatter_color_args(; ../../../../anaconda3/envs/scanpy-dev/lib/python3.8/site-packages/matplotlib/axes/_axes.py:4400: in _parse_scatter_color_args; and isinstance(cbook._safe_first_finite(c), str))); _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. obj = [nan, nan, nan, nan, nan, nan, ...]. def _safe_first_finite(obj, *, skip_nonfinite=True):; """"""; Return the first non-None (and optionally finite) element in *obj*.; ; This is a method for internal use.; ; This is a type-independent way of obtaining the first non-None element,; supporting both index access and the iterator protocol.; The first non-None element will be obtained when skip_none is True.; """"""; def safe_isfinite(val):; if val is None:; return False; try:; return np.isfinite(val) if np.isscalar(val) else True; except TypeError:; # This is something that numpy can not make heads or tails; # of, assume ""finite""; return True; if skip_nonfinite is False:; if isinstance(obj, collections.abc.Iterator):; # needed to accept `array.flat` as input.; # np.flatiter reports as an instance of collections.Iterator; # but can still be indexed via [].; # This has the side effect of re-setting the iterator, but; # that is acceptable.; try:; return obj[0]; except TypeError:; pass; raise RuntimeError(""matplotlib does not support generators ""; ""as input""); return next(iter(obj)); elif isinstance(obj, np.flatiter):; # TODO do the finite filtering on this; return obj[0]; elif isinstance(obj, collections.abc.Iterator):; raise RuntimeError(""matplotlib does not """,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2459:4163,protocol,protocol,4163,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2459,1,['protocol'],['protocol']
Integrability,"pe({})):; 285 for sub_key, sub_value in value.items():; --> 286 write_attribute(f, f""{key}/{sub_key}"", sub_value, dataset_kwargs=dataset_kwargs); 287 ; 288 . /usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/functools.py in wrapper(*args, **kw); 838 '1 positional argument'); 839 ; --> 840 return dispatch(args[0].__class__)(*args, **kw); 841 ; 842 funcname = getattr(func, '__name__', 'singledispatch function'). ~/github/anndata/anndata/_io/h5ad.py in write_attribute_h5ad(f, key, value, *args, **kwargs); 124 if key in f:; 125 del f[key]; --> 126 _write_method(type(value))(f, key, value, *args, **kwargs); 127 ; 128 . ~/github/anndata/anndata/_io/h5ad.py in write_mapping(f, key, value, dataset_kwargs); 284 def write_mapping(f, key, value, dataset_kwargs=MappingProxyType({})):; 285 for sub_key, sub_value in value.items():; --> 286 write_attribute(f, f""{key}/{sub_key}"", sub_value, dataset_kwargs=dataset_kwargs); 287 ; 288 . /usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/functools.py in wrapper(*args, **kw); 838 '1 positional argument'); 839 ; --> 840 return dispatch(args[0].__class__)(*args, **kw); 841 ; 842 funcname = getattr(func, '__name__', 'singledispatch function'). ~/github/anndata/anndata/_io/h5ad.py in write_attribute_h5ad(f, key, value, *args, **kwargs); 124 if key in f:; 125 del f[key]; --> 126 _write_method(type(value))(f, key, value, *args, **kwargs); 127 ; 128 . ~/github/anndata/anndata/_io/utils.py in func_wrapper(elem, key, val, *args, **kwargs); 193 f""Above error raised while writing key {key!r} of {type(elem)}""; 194 f"" from {parent}.""; --> 195 ) from e; 196 ; 197 return func_wrapper. NotImplementedError: Failed to write value for uns/umap/params/random_state, since a writer for type <class 'numpy.random.mtrand.RandomState'> has not been implemented yet. Above error raised while writing key 'uns/umap/params/random_state' of <class 'h5py._hl.files.File'> from /.; ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1131:4844,wrap,wrapper,4844,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1131,1,['wrap'],['wrapper']
Integrability,pi; ipython 8.22.2 pypi_0 pypi; ipywidgets 8.1.2 pypi_0 pypi; isoduration 20.11.0 pypi_0 pypi; jedi 0.19.1 pypi_0 pypi; jinja2 3.1.3 py311haa95532_0; joblib 1.3.2 pypi_0 pypi; json5 0.9.22 pypi_0 pypi; jsonpointer 2.4 pypi_0 pypi; jsonschema 4.21.1 pypi_0 pypi; jsonschema-specifications 2023.12.1 pypi_0 pypi; jupyter-client 8.6.1 pypi_0 pypi; jupyter-core 5.7.2 pypi_0 pypi; jupyter-events 0.9.1 pypi_0 pypi; jupyter-lsp 2.2.4 pypi_0 pypi; jupyter-server 2.13.0 pypi_0 pypi; jupyter-server-terminals 0.5.3 pypi_0 pypi; jupyter_client 8.6.0 py311haa95532_0; jupyter_core 5.5.0 py311haa95532_0; jupyter_events 0.8.0 py311haa95532_0; jupyter_server 2.10.0 py311haa95532_0; jupyter_server_terminals 0.4.4 py311haa95532_1; jupyterlab 4.1.5 pypi_0 pypi; jupyterlab-pygments 0.3.0 pypi_0 pypi; jupyterlab-server 2.25.4 pypi_0 pypi; jupyterlab-widgets 3.0.10 pypi_0 pypi; jupyterlab_pygments 0.1.2 py_0; jupyterlab_server 2.25.1 py311haa95532_0; kiwisolver 1.4.5 pypi_0 pypi; lazy-loader 0.3 pypi_0 pypi; legacy-api-wrap 1.4 pypi_0 pypi; leidenalg 0.10.2 pypi_0 pypi; libffi 3.4.4 hd77b12b_0; libsodium 1.0.18 h62dcd97_0; llvmlite 0.42.0 pypi_0 pypi; m2w64-bwidget 1.9.10 2; m2w64-bzip2 1.0.6 6; m2w64-expat 2.1.1 2; m2w64-fftw 3.3.4 6; m2w64-flac 1.3.1 # Name Version Build Channel; _r-mutex 1.0.0 anacondar_1 ; anndata 0.10.6 pypi_0 pypi; anyio 4.3.0 pypi_0 pypi; argon2-cffi 23.1.0 pypi_0 pypi; argon2-cffi-bindings 21.2.0 py311h2bbff1b_0 ; array-api-compat 1.5.1 pypi_0 pypi; arrow 1.3.0 pypi_0 pypi; asttokens 2.4.1 pypi_0 pypi; async-lru 2.0.4 py311haa95532_0 ; attrs 23.2.0 pypi_0 pypi; babel 2.14.0 pypi_0 pypi; beautifulsoup4 4.12.3 pypi_0 pypi; bleach 6.1.0 pypi_0 pypi; brotli-python 1.0.9 py311hd77b12b_7 ; bzip2 1.0.8 h2bbff1b_5 ; ca-certificates 2023.12.12 haa95532_0 ; certifi 2024.2.2 py311haa95532_0 ; cffi 1.16.0 py311h2bbff1b_0 ; chardet 5.2.0 pypi_0 pypi; charset-normalizer 3.3.2 pypi_0 pypi; colorama 0.4.6 py311haa95532_0 ; comm 0.2.2 pypi_0 pypi; contourpy 1.2.0 pypi_0 pypi; cycler ,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2969:4512,wrap,wrap,4512,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2969,1,['wrap'],['wrap']
Integrability,"ple who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful.; > . That'd make things a lot easier for many people (including myself 😃), I agree. However. 1) There are (and will be) so many R packages about single cell, so once we open the door, there might be so many requests about these packages so that it'd be difficult to decide what to include and what not to include. The decision might be a bit arbitrary. This is why I suggested a contrib repo, which will have everything users request (as soon as there is someone who is willing to maintain it), in a `use at your own risk` way... 2) There might be several bug reports about rpy2 itself or thin wrappers or R installation or R packages themselves. I was wondering whether this might introduce more maintenance burden, although supported packages will be limited. > The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a scanpy-contrib: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: anndata is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions.; > ; > What do you think?. Alternatively, we can just prepare jupyter notebooks with some Python 3 and some R cells in it (which is super easy via rpy2 magics anyway) for some R packages/functions like mnn or SIMLR and put those in scanpy_usage as a reference for the community. For example:. ![image](https://user-images.githubusercontent.com/1140359/38873972-4953977a-4257-11e8-8675-a238738eb558.png). Another question is other single cell Python packages like magic, ZIFA or DCA, for example. There will hopefully be more in the future. A contrib repo might include these, as well i.e. `sc.tl.magic`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/125#issuecomment-382002901:1341,wrap,wrappers,1341,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-382002901,2,"['interface', 'wrap']","['interfaces', 'wrappers']"
Integrability,"portError:. ImportError: cannot import name 'GProfiler'. During handling of the above exception, another exception occurred:. ImportError Traceback (most recent call last); <ipython-input-383-c1b09359d1a1> in <module>; 14 ; 15 #get gene set enrichment; ---> 16 print(sc.queries.enrich(this_adata, org='hsapiens', group='malignant', key='malignantvshealthy', pval_cutoff=0.01, log2fc_min=np.log2(1.5))); 17 ; 18 #plot volcano (makes a sep df along the way, should consolidate with above). /anaconda3/envs/mm_singlecell_v2/lib/python3.6/functools.py in wrapper(*args, **kw); 805 '1 positional argument'); 806 ; --> 807 return dispatch(args[0].__class__)(*args, **kw); 808 ; 809 funcname = getattr(func, '__name__', 'singledispatch function'). /anaconda3/envs/mm_singlecell_v2/lib/python3.6/site-packages/scanpy/queries/_queries.py in _enrich_anndata(adata, group, org, key, pval_cutoff, log2fc_min, log2fc_max, gene_symbols, gprofiler_kwargs); 305 else:; 306 gene_list = list(de[""names""].dropna()); --> 307 return enrich(gene_list, org=org, gprofiler_kwargs=gprofiler_kwargs). /anaconda3/envs/mm_singlecell_v2/lib/python3.6/functools.py in wrapper(*args, **kw); 805 '1 positional argument'); 806 ; --> 807 return dispatch(args[0].__class__)(*args, **kw); 808 ; 809 funcname = getattr(func, '__name__', 'singledispatch function'). /anaconda3/envs/mm_singlecell_v2/lib/python3.6/site-packages/scanpy/queries/_queries.py in enrich(container, org, gprofiler_kwargs); 266 except ImportError:; 267 raise ImportError(; --> 268 ""This method requires the `gprofiler-official` module to be installed.""; 269 ); 270 gprofiler = GProfiler(user_agent=""scanpy"", return_dataframe=True). ImportError: This method requires the `gprofiler-official` module to be installed.; ```. #### Versions. gprofiler-official bioconda/noarch::gprofiler-official-1.0.0-py_0. scanpy==1.7.1 anndata==0.7.5 umap==0.5.1 numpy==1.19.5 scipy==1.5.3 pandas==1.1.5 scikit-learn==0.19.1 statsmodels==0.12.2 python-igraph==0.8.3 leidenalg==0.8.3",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1896:1875,wrap,wrapper,1875,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1896,1,['wrap'],['wrapper']
Integrability,"port_read_key_on_error.<locals>.func_wrapper(*args, **kwargs); 201 try:; --> 202 return func(*args, **kwargs); 203 except Exception as e:. File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\registry.py:235, in Reader.read_elem(self, elem, modifiers); 234 if self.callback is not None:; --> 235 return self.callback(read_func, elem.name, elem, iospec=get_spec(elem)); 236 else:. File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:241, in read_h5ad.<locals>.callback(func, elem_name, elem, iospec); 240 return read_dataframe(elem); --> 241 return func(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\methods.py:323, in read_array(elem, _reader); 319 @_REGISTRY.register_read(H5Array, IOSpec(""array"", ""0.2.0"")); 320 @_REGISTRY.register_read(ZarrArray, IOSpec(""array"", ""0.2.0"")); 321 @_REGISTRY.register_read(ZarrArray, IOSpec(""string-array"", ""0.2.0"")); 322 def read_array(elem, _reader):; --> 323 return elem[()]. File h5py\_objects.pyx:54, in h5py._objects.with_phil.wrapper(). File h5py\_objects.pyx:55, in h5py._objects.with_phil.wrapper(). File D:\Python3.10.9\lib\site-packages\h5py\_hl\dataset.py:768, in Dataset.__getitem__(self, args, new_dtype); 767 try:; --> 768 return self._fast_reader.read(args); 769 except TypeError:. File h5py\_selector.pyx:368, in h5py._selector.Reader.read(). File h5py\_selector.pyx:342, in h5py._selector.Reader.make_array(). MemoryError: Unable to allocate 9.90 GiB for an array with shape (310385, 8563) and data type float32. The above exception was the direct cause of the following exception:. AnnDataReadError Traceback (most recent call last); Cell In[2], line 4; 2 import pandas as pd; 3 import scanpy as sc; ----> 4 annData = sc.read_h5ad(""ReplogleWeissman2022_K562_essential.h5ad""). File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:243, in read_h5ad(filename, backed, as_sparse, as_sparse_fmt, chunk_size); 240 return read_dataframe(elem); 241 return func(elem); --> 243 adata = read_dispatched(f, callback=callback)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2551:2169,wrap,wrapper,2169,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551,1,['wrap'],['wrapper']
Integrability,"posed to reduce the plotting time. I would not wait for; more than 5 minutes to see a plot. How many genes were you planning to plot?. The background is that when plotting a heatmap, the matplotlib; visualization will randomly drop genes because the resolution of the; screens is not high enough. Thus, when the number of genes is large, I was; trying to find a compromise by fitting a line before the plotting and then; only plotting the fit. Can you help me solve the issue by sharing the data with me? Or, can you; find an example that can reproduce the problem?. On Tue, May 7, 2019 at 10:19 AM brianpenghe <notifications@github.com>; wrote:. > I was trying to plot a heatmap using this command:; > ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident',; > use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0,; > dendrogram=True, save='ClusterMap.png'); >; > And it didn't finish running after an overnight, with the following; > warning message:; > WARNING: Gene labels are not shown when more than 50 genes are visualized.; > To show gene labels set show_gene_labels=True; > /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227:; > UserWarning:; > The maximal number of iterations maxit (set to 20 by the program); > allowed for finding a smoothing spline with fp=s has been reached: s; > too small.; > There is an approximation returned but the corresponding weighted sum; > of squared residuals does not satisfy the condition abs(fp-s)/s < tol.; > warnings.warn(message); >; > I don't understand why this is taking this long because seaborn was able; > to finish plotting within 30 minutes. Do you know why?; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/633>, or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABF37VNDX37RZL256MWKDM3PUE3RFANCNFSM4HLGOYGA>; > .; >. -- . Fidel Ramirez",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/633#issuecomment-490792870:1620,message,message,1620,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633#issuecomment-490792870,1,['message'],['message']
Integrability,"pp.scale(adata_dask, max_value=10). File ~/miniconda3/envs/omics/lib/python3.10/functools.py:889, in singledispatch..wrapper(*args, **kw); 885 if not args:; 886 raise TypeError(f'{funcname} requires at least '; 887 '1 positional argument'); --> 889 return dispatch(args[0].__class__)(*args, **kw). File ~/miniconda3/envs/omics/lib/python3.10/site-packages/scanpy/preprocessing/_simple.py:844, in scale_anndata(adata, zero_center, max_value, copy, layer, obsm); 842 view_to_actual(adata); 843 X = _get_obs_rep(adata, layer=layer, obsm=obsm); --> 844 X, adata.var[""mean""], adata.var[""std""] = scale(; 845 X,; 846 zero_center=zero_center,; 847 max_value=max_value,; 848 copy=False, # because a copy has already been made, if it were to be made; 849 return_mean_std=True,; 850 ); 851 _set_obs_rep(adata, X, layer=layer, obsm=obsm); 852 if copy:. File ~/miniconda3/envs/omics/lib/python3.10/functools.py:889, in singledispatch..wrapper(*args, **kw); 885 if not args:; 886 raise TypeError(f'{funcname} requires at least '; 887 '1 positional argument'); --> 889 return dispatch(args[0].__class__)(*args, **kw). TypeError: scale() got an unexpected keyword argument 'return_mean_std'; ```. #### Versions. <details>. -----; anndata 0.9.1; scanpy 1.9.3; -----; PIL 9.5.0; appnope 0.1.3; asciitree NA; asttokens NA; attr 23.1.0; backcall 0.2.0; brotli NA; certifi 2023.05.07; cffi 1.15.1; charset_normalizer 3.1.0; click 8.1.3; cloudpickle 2.2.1; colorama 0.4.6; colorful 0.5.5; colorful_orig 0.5.5; comm 0.1.3; cycler 0.10.0; cython_runtime NA; cytoolz 0.12.0; dask 2023.5.0; dateutil 2.8.2; debugpy 1.6.7; decorator 5.1.1; defusedxml 0.7.1; distributed 2023.5.0; entrypoints 0.4; executing 1.2.0; fasteners 0.17.3; filelock 3.12.0; fsspec 2023.5.0; google NA; grpc 1.43.0; h5py 3.8.0; idna 3.4; igraph 0.10.4; ipykernel 6.23.1; jedi 0.18.2; jinja2 3.1.2; joblib 1.2.0; jsonschema 4.17.3; kiwisolver 1.4.4; leidenalg 0.9.1; llvmlite 0.40.0; locket NA; lz4 4.3.2; markupsafe 2.1.2; matplotlib 3.6.3; matplotlib_i",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2491:4242,wrap,wrapper,4242,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2491,1,['wrap'],['wrapper']
Integrability,"py.; - [ ✔] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. Hello Scanpy,; I installed Scanpy, scVelo, CellRank, bbknn 2 months ago and never upgrade the packages. They were running very smoothly until I reimage my PC and reinstall Scanpy in anaconda today (Anaconda3-2021.05-Windows-x86_64, python3.8.12).; I tired `pip install scanpy[leiden]`. Tried `conda install seaborn scikit-learn statsmodels numba pytables`, `conda install -c conda-forge python-igraph leidenalg`. Tried installing `Java` and `visual C++ 2012-2022 redistributable`. Also tried rebuilding a new environment and reinstalled everything. Whatever I try, this bug still exists when I import Scanpy.; I guess it may be the incompatibility issue of packages. Some dependency packages which were upgraded by the developer in these months caused this incompatibility issue. Could you please help me with this bug?; Thanks!; Best,; YJ. ### Minimal code sample (that we can copy&paste without having any data). ```python; import numpy as np; import pandas as pd; import scanpy as sc; import scanpy.external as sce; import scipy; sc.settings.verbosity = 3; sc.logging.print_header(); sc.set_figure_params(dpi=100, dpi_save=600); ; import matplotlib.pyplot as pl; from matplotlib import rcParams; ```. ```pytb; ImportError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_7844/2696797780.py in <module>; 1 import numpy as np; 2 import pandas as pd; ----> 3 import scanpy as sc; 4 import scanpy.external as sce; 5 import scipy. ~\.conda\envs\Python38\lib\site-packages\scanpy\__init__.py in <module>; 12 # (start with settings as several tools are using it); 13 from ._settings import settings, Verbosity; ---> 14 from . import tools as tl; 15 from . import preprocessing as pp; 16 from . import plotting ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2108:1088,depend,dependency,1088,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2108,1,['depend'],['dependency']
Integrability,"py2_3/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, *args, **kwargs); 155 try:; --> 156 return func(elem, *args, **kwargs); 157 except Exception as e:. ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/anndata/_io/h5ad.py in read_group(group); 505 if ""h5sparse_format"" in group.attrs: # Backwards compat; --> 506 return SparseDataset(group).to_memory(); 507 . ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/anndata/_core/sparse_dataset.py in to_memory(self); 370 mtx = format_class(self.shape, dtype=self.dtype); --> 371 mtx.data = self.group[""data""][...]; 372 mtx.indices = self.group[""indices""][...]. h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/h5py/_hl/dataset.py in __getitem__(self, args); 572 fspace = selection.id; --> 573 self.id.read(mspace, fspace, arr, mtype, dxpl=self._dxpl); 574 . h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/h5d.pyx in h5py.h5d.DatasetID.read(). h5py/_proxy.pyx in h5py._proxy.dset_rw(). h5py/_proxy.pyx in h5py._proxy.H5PY_H5Dread(). OSError: Can't read data (file read failed: time = Sat Aug 1 13:27:54 2020; , filename = '/path.../filtered_gene_bc_matrices.h5ad', file descriptor = 47, errno = 5, error message = 'Input/output error', buf = 0x55ec782e9031, total read size = 7011, bytes this sub-read = 7011, bytes actually read = 18446744073709551615, offset = 0). During handling of the above exception, another exception occurred:. AnnDataReadError Traceback (most recent call last); <ipython-input-14-faac769583f8> in <module>; 17 #while True:; 18 #try:; ---> 19 adatas.append(sc.read_h5ad(file)); 20 file_diffs.append('_'.join([file.split('/')[i] for i in diff_path_idx])); 21 #break. ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/anndata/_io/h5ad.py in read_h5ad(filename, backed, as_sparse, as_sparse_fmt, chunk_size); 411 d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1351:1625,wrap,wrapper,1625,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351,1,['wrap'],['wrapper']
Integrability,"py?line=1129) res = impl(self.builder, argvals, self.loc); [1131](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/lowering.py?line=1130) return res. File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\base.py:1201, in _wrap_impl.__call__(self, builder, args, loc); [1200](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/base.py?line=1199) def __call__(self, builder, args, loc=None):; -> [1201](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/base.py?line=1200) res = self._imp(self._context, builder, self._sig, args, loc=loc); [1202](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/base.py?line=1201) self._context.add_linking_libs(getattr(self, 'libs', ())). File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\base.py:1231, in _wrap_missing_loc.__call__.<locals>.wrapper(*args, **kwargs); [1230](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/base.py?line=1229) kwargs.pop('loc') # drop unused loc; -> [1231](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/base.py?line=1230) return fn(*args, **kwargs). File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\cpython\rangeobj.py:40, in make_range_impl.<locals>.range1_impl(context, builder, sig, args); [39](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/cpython/rangeobj.py?line=38) state.start = context.get_constant(int_type, 0); ---> [40](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/cpython/rangeobj.py?line=39) state.stop = stop; [41](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/cpython/rangeobj.py?line=40) state.step = context.get_constant(int_type, 1). File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\cgutils.py:164, in _StructProxy.__setattr__(self, field, value); [",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659:5259,wrap,wrapper,5259,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659,2,['wrap'],['wrapper']
Integrability,"python; _, axs = pl.subplots(ncols=3, figsize=(6, 2.5), gridspec_kw={'wspace': 0.05, 'left': 0.12}); pl.subplots_adjust(left=0.05, right=0.98, top=0.82, bottom=0.2); for ipath, (descr, path) in enumerate(paths):; _, data = sc.pl.paga_path(; adata, path, gene_names,; show_node_names=False,; ax=axs[ipath],; ytick_fontsize=8,; left_margin=0.15,; n_avg=50,; annotations=['distance'],; show_yticks=True if ipath==0 else False,; show_colorbar=False,; color_map='Greys',; groups_key='clusters',; color_maps_annotations={'distance': 'viridis'},; title='{} path'.format(descr),; return_data=True,; show=False); data.to_csv('./write/paga_path_{}.csv'.format(descr)); pl.savefig('./figures/paga_path_panglao.pdf'); pl.show(); ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```pytb; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); TypeError: float() argument must be a string or a number, not 'csr_matrix'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last); <ipython-input-8-86ecf06e6589> in <module>(); 18 title='{} path'.format(descr),; 19 return_data=True,; ---> 20 show=False); 21 data.to_csv('./write/paga_path_{}.csv'.format(descr)); 22 pl.savefig('./figures/paga_path_panglao.pdf'). 5 frames; <__array_function__ internals> in cumsum(*args, **kwargs). /usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py in _wrapit(obj, method, *args, **kwds); 45 except AttributeError:; 46 wrap = None; ---> 47 result = getattr(asarray(obj), method)(*args, **kwds); 48 if wrap:; 49 if not isinstance(result, mu.ndarray):. ValueError: setting an array element with a sequence.; ```. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; > scanpy==1.5.1 anndata==0.7.3 umap==0.4.4 numpy==1.18.5 scipy==1.4.1 pandas==1.0.5 scikit-learn==0.22.2.post1 statsmodels==0.10.2 python-igraph==0.8.2 leidenalg==0.8.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1295:2023,wrap,wrap,2023,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1295,2,['wrap'],['wrap']
Integrability,"python_genutils 0.2.0; ipywidgets 8.0.4; jedi 0.18.1; jinja2 3.1.2; joblib 1.2.0; json5 NA; jsonpointer 2.1; jsonschema 4.17.3; jupyter_server 1.23.4; jupyterlab_server 2.22.0; kiwisolver 1.4.4; legacy_api_wrap NA; leidenalg 0.10.2; lifelines 0.28.0; llvmlite 0.42.0; louvain 0.8.2; lz4 4.3.2; markupsafe 2.1.1; matplotlib 3.7.2; matplotlib_inline 0.1.6; mpl_toolkits NA; natsort 8.4.0; nbformat 5.9.2; nbinom_ufunc NA; ncf_ufunc NA; nct_ufunc NA; ncx2_ufunc NA; numba 0.59.1; numexpr 2.8.4; numpy 1.26.4; packaging 23.1; pandas 2.2.2; parso 0.8.3; patsy 0.5.3; pickleshare 0.7.5; pkg_resources NA; platformdirs 3.10.0; plotly 5.9.0; prometheus_client NA; prompt_toolkit 3.0.36; psutil 5.9.0; pure_eval 0.2.2; pvectorc NA; pyarrow 11.0.0; pycparser 2.21; pydev_ipython NA; pydevconsole NA; pydevd 2.9.5; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.15.1; pynndescent 0.5.11; pyparsing 3.0.9; pyrsistent NA; pythoncom NA; pytz 2023.3.post1; pywintypes NA; requests 2.31.0; rfc3339_validator 0.1.4; rfc3986_validator 0.1.1; ruamel NA; scipy 1.11.1; seaborn 0.13.2; send2trash NA; session_info 1.0.0; setuptools 69.5.1; six 1.16.0; skewnorm_ufunc NA; sklearn 1.3.0; sniffio 1.2.0; socks 1.7.1; sparse 0.15.1; sphinxcontrib NA; stack_data 0.2.0; statsmodels 0.14.0; tblib 1.7.0; terminado 0.17.1; texttable 1.7.0; threadpoolctl 2.2.0; tlz 0.12.0; toolz 0.12.0; torch 2.2.1+cpu; torchgen NA; tornado 6.3.2; tqdm 4.65.0; traitlets 5.7.1; typing_extensions NA; umap 0.5.5; urllib3 1.26.16; wcwidth 0.2.5; websocket 0.58.0; win32api NA; win32com NA; win32con NA; win32trace NA; winerror NA; winpty 2.0.10; wrapt 1.14.1; xxhash 2.0.2; yaml 6.0; zipp NA; zmq 23.2.0; zope NA; -----; IPython 8.15.0; jupyter_client 7.4.9; jupyter_core 5.3.0; jupyterlab 3.6.3; notebook 6.5.4; -----; Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]; Windows-10-10.0.22631-SP0; -----; Session information updated at 2024-06-02 17:20. ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3086:8617,wrap,wrapt,8617,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3086,1,['wrap'],['wrapt']
Integrability,"r=""seurat_v3"", batch_key=SOME_KEY)` potentially differs in the implementation of how HVGs are ranked from its Seurat counterpart:; - either by sorting by number-of-batches-in-which-genes-are-highly-variable and then breaking ties with median-rank-in-batches (this is described in [Stuart et al. 2019](https://www.cell.com/cell/pdf/S0092-8674(19)30559-8.pdf), and implemented in Seurat's [`SelectIntegrationFeatures`](https://satijalab.org/seurat/reference/selectintegrationfeatures)*).; - OR by sorting first by median-rank-in-batches and breaking ties with number-of-batches-in-which-genes-are-highly-variable (this is how `""seurat_v3""` in scanpy is currently implemented); ; causing quite some discrepancy in the results. *I am not an R expert, so this might not be correct: Digging into the code of `SelectIntegrationFeatures`, I suspect the genes _above_ a treshold level of batches in which they are HVGs are [ordered by their median rank](https://github.com/satijalab/seurat/blob/41d19a8a55350bff444340d6ae7d7e03417d4173/R/integration.R#L2988), in contrary to the textual description in Stuart et al.; and only the genes displaying this threshold of number of batches in which they are highly variable are ranked by their median rank - to decide which are kept as highly variable. This would have an effect on the ordering of the very top genes, but NOT on the actual genes which are selected by `SelectIntegrationFeatures`. **Note**; All of this does not affect the fairly good match, up to potentially numerics, between `sc.pp.highly_variable_genes(adata, flavor=""seurat_v3"", batch_key=None)` and Seurat's `FindVariableFeatures` with `selection.method = 'vst'` introduced in Stuart et al.; If it helps to avoid confusion between the two: `FindVariableFeatures` is called within `SelectIntegrationFeatures`, on each batch separately. **Technical additions here**; This PR suggests to solve this by introducing a new flavor. Either. -`seurat_v3_paper` This fixes to exactly what @jlause noticed ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2792:1737,integrat,integration,1737,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2792,1,['integrat'],['integration']
Integrability,"records_fixed_width(self._obs); 2186 var_rec, uns_var = df_to_records_fixed_width(self._var); 2187 layers = self.layers.as_dict(). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in df_to_records_fixed_width(df); 212 names.append(k); 213 if is_string_dtype(df[k]):; --> 214 max_len_index = df[k].map(len).max(); 215 arrays.append(df[k].values.astype('S{}'.format(max_len_index))); 216 elif is_categorical(df[k]):. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in stat_func(self, axis, skipna, level, numeric_only, **kwargs); 10954 skipna=skipna); 10955 return self._reduce(f, name, axis=axis, skipna=skipna,; > 10956 numeric_only=numeric_only); 10957 ; 10958 return set_function_name(stat_func, name, cls). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/series.py in _reduce(self, op, name, axis, skipna, numeric_only, filter_type, **kwds); 3613 # dispatch to ExtensionArray interface; 3614 if isinstance(delegate, ExtensionArray):; -> 3615 return delegate._reduce(name, skipna=skipna, **kwds); 3616 elif is_datetime64_dtype(delegate):; 3617 # use DatetimeIndex implementation to handle skipna correctly. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in _reduce(self, name, axis, skipna, **kwargs); 2179 msg = 'Categorical cannot perform the operation {op}'; 2180 raise TypeError(msg.format(op=name)); -> 2181 return func(**kwargs); 2182 ; 2183 def min(self, numeric_only=None, **kwargs):. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in max(self, numeric_only, **kwargs); 2222 max : the maximum of this `Categorical`; 2223 """"""; -> 2224 self.check_for_ordered('max'); 2225 if numeric_only:; 2226 good = self._codes != -1. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorica",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/515:3351,interface,interface,3351,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515,1,['interface'],['interface']
Integrability,"rgs); 5657 self.set_aspect(aspect); 5658 im = mimage.AxesImage(self, cmap=cmap, norm=norm,; 5659 interpolation=interpolation, origin=origin,; 5660 extent=extent, filternorm=filternorm,; 5661 filterrad=filterrad, resample=resample,; 5662 interpolation_stage=interpolation_stage,; 5663 **kwargs); -> 5665 im.set_data(X); 5666 im.set_alpha(alpha); 5667 if im.get_clip_path() is None:; 5668 # image does not already have clipping set, clip to axes patch. File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\matplotlib\image.py:710, in _ImageBase.set_data(self, A); 706 self._A = self._A[:, :, 0]; 708 if not (self._A.ndim == 2; 709 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):; --> 710 raise TypeError(""Invalid shape {} for image data""; 711 .format(self._A.shape)); 713 if self._A.ndim == 3:; 714 # If the input data has values outside the valid range (after; 715 # normalisation), we issue a warning and then clip X to the bounds; 716 # - otherwise casting wraps extreme values, hiding outliers and; 717 # making reliable interpretation impossible.; 718 high = 255 if np.issubdtype(self._A.dtype, np.integer) else 1. TypeError: Invalid shape (633,) for image data; ```. ### Versions. <details>. ```; -----; anndata 0.9.2; scanpy 1.10.1; -----; PIL 9.5.0; anyio NA; arrow 1.3.0; asttokens NA; astunparse 1.6.3; attr 23.1.0; attrs 23.1.0; babel 2.13.0; backcall 0.2.0; certifi 2023.07.22; cffi 1.16.0; charset_normalizer 3.3.0; cloudpickle 2.2.1; colorama 0.4.6; comm 0.1.4; cycler 0.10.0; cython_runtime NA; dask 2023.10.0; dateutil 2.8.2; debugpy 1.8.0; decorator 5.1.1; defusedxml 0.7.1; executing 2.0.0; fastjsonschema NA; fqdn NA; h5py 3.9.0; idna 3.4; igraph 0.10.8; ipykernel 6.25.2; ipywidgets 8.1.1; isoduration NA; jedi 0.19.1; jinja2 3.1.2; joblib 1.3.2; json5 NA; jsonpointer 2.4; jsonschema 4.19.1; jsonschema_specifications NA; jupyter_events 0.7.0; jupyter_server 2.7.3; jupyterlab_server 2.25.0; kiwi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3025:4955,wrap,wraps,4955,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3025,1,['wrap'],['wraps']
Integrability,"rical labels. This is based on `sklearn.metrics.confusion_matrix` but is easier to use, and returns a object with labels. I think this is mostly done, though I'm considering changing the calling convention. Here's an example of usage:. ```python; import scanpy as sc; import seaborn as sns. pbmc = sc.datasets.pbmc68k_reduced(); sc.tl.leiden(pbmc); sns.heatmap(sc.metrics.confusion_matrix(""bulk_labels"", ""leiden"", pbmc.obs)); ```. ![image](https://user-images.githubusercontent.com/8238804/68737959-1a28b780-0639-11ea-8576-4cf1907066d9.png). I've copied `seaborn`s calling convention here, but I think that could change. Right now the above call is equivalent to:. ```python; sc.metrics.confusion_matrix(pbmc.obs[""bulk_labels""], pbmc.obs[""louvain""]); ```. But I wonder if it would make more sense to have the DataFrame go first if it's provided. I've also based the API around my usage of confusion matrices, so I'm very open to more general feedback on this. My reason for including it here was the amount of code it took wrapping `sklearn.metrics.confusion_matrix` to get useful output. ## `sc.metrics.gearys_c` ([Wiki page](https://en.wikipedia.org/wiki/Geary%27s_C)). Calculates autocorrelation on a measure on a network. Used in [VISION](https://doi.org/10.1038/s41467-019-12235-0) for ranking gene sets. This is useful for finding out whether some per-cell measure is correlated with the structure of a connectivity graph. In practice, I've found it useful for identifying features that look good on a UMAP:. ```python; import numpy as np; pbmc.layers[""logcounts""] = pbmc.raw.X. %time gearys_c = sc.metrics.gearys_c(pbmc, layer=""logcounts""); # CPU times: user 496 ms, sys: 3.88 ms, total: 500 ms; # Wall time: 74.9 ms; to_plot = pbmc.var_names[np.argsort(gearys_c)[:4]]; sc.pl.umap(pbmc, color=to_plot, ncols=2); ```. ![image](https://user-images.githubusercontent.com/8238804/68736833-e304d700-0635-11ea-87f4-ac066f3e270c.png). It can also be useful to rank components of dimensionality reduct",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/915:1900,wrap,wrapping,1900,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915,1,['wrap'],['wrapping']
Integrability,"rids.; 482 zorder_offset = max(axis.get_zorder(); 483 for axis in self._get_axis_list()) + 1; 484 for i, col in enumerate(; --> 485 sorted(self.collections,; 486 key=do_3d_projection,; 487 reverse=True)):; 488 col.zorder = zorder_offset + i; 489 for i, patch in enumerate(; 490 sorted(self.patches,; 491 key=do_3d_projection,; 492 reverse=True)):. File ~/anaconda3/envs/ml/lib/python3.9/site-packages/mpl_toolkits/mplot3d/axes3d.py:471, in Axes3D.draw.<locals>.do_3d_projection(artist); 458 """"""; 459 Call `do_3d_projection` on an *artist*, and warn if passing; 460 *renderer*.; (...); 464 *renderer* and raise a warning.; 465 """"""; 467 if artist.__module__ == 'mpl_toolkits.mplot3d.art3d':; 468 # Our 3D Artists have deprecated the renderer parameter, so; 469 # avoid passing it to them; call this directly once the; 470 # deprecation has expired.; --> 471 return artist.do_3d_projection(); 473 _api.warn_deprecated(; 474 ""3.4"",; 475 message=""The 'renderer' parameter of ""; 476 ""do_3d_projection() was deprecated in Matplotlib ""; 477 ""%(since)s and will be removed %(removal)s.""); 478 return artist.do_3d_projection(renderer). File ~/anaconda3/envs/ml/lib/python3.9/site-packages/matplotlib/_api/deprecation.py:431, in delete_parameter.<locals>.wrapper(*inner_args, **inner_kwargs); 421 deprecation_addendum = (; 422 f""If any parameter follows {name!r}, they should be passed as ""; 423 f""keyword, not positionally.""); 424 warn_deprecated(; 425 since,; 426 name=repr(name),; (...); 429 else deprecation_addendum,; 430 **kwargs); --> 431 return func(*inner_args, **inner_kwargs). File ~/anaconda3/envs/ml/lib/python3.9/site-packages/mpl_toolkits/mplot3d/art3d.py:599, in Path3DCollection.do_3d_projection(self, renderer); 597 @_api.delete_parameter('3.4', 'renderer'); 598 def do_3d_projection(self, renderer=None):; --> 599 xs, ys, zs = self._offsets3d; 600 vxs, vys, vzs, vis = proj3d.proj_transform_clip(xs, ys, zs,; 601 self.axes.M); 602 # Sort the points based on z coordinates; 603 # Performance o",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2285:9118,message,message,9118,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2285,1,['message'],['message']
Integrability,"rm, mean_center, n_prin_comps, use_approx_neighbors, get_doublet_neighbor_parents, n_neighbors, threshold, verbose, copy, random_state); 279 adata.uns[""scrublet""][""batched_by""] = batch_key; 281 else:; --> 282 scrubbed = _run_scrublet(adata_obs, adata_sim); 284 # Copy outcomes to input object from our processed version; 286 adata.obs[""doublet_score""] = scrubbed[""obs""][""doublet_score""]. File C:\ProgramData\Anaconda3\envs\dl\lib\site-packages\scanpy\preprocessing\_scrublet\__init__.py:204, in scrublet.<locals>._run_scrublet(ad_obs, ad_sim); 201 # HVG process needs log'd data.; 203 logged = pp.log1p(ad_obs, copy=True); --> 204 pp.highly_variable_genes(logged); 205 ad_obs = ad_obs[:, logged.var[""highly_variable""]].copy(); 207 # Simulate the doublets based on the raw expressions from the normalised; 208 # and filtered object. File C:\ProgramData\Anaconda3\envs\dl\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File C:\ProgramData\Anaconda3\envs\dl\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py:648, in highly_variable_genes(***failed resolving arguments***); 645 del min_disp, max_disp, min_mean, max_mean, n_top_genes; 647 if batch_key is None:; --> 648 df = _highly_variable_genes_single_batch(; 649 adata, layer=layer, cutoff=cutoff, n_bins=n_bins, flavor=flavor; 650 ); 651 else:; 652 df = _highly_variable_genes_batched(; 653 adata, batch_key, layer=layer, cutoff=cutoff, n_bins=n_bins, flavor=flavor; 654 ). File C:\ProgramData\Anaconda3\envs\dl\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py:281, in _highly_variable_genes_single_batch(adata, layer, cutoff, n_bins, flavor); 279 # all of the following quantities are ""per-gene""",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3070:8713,wrap,wraps,8713,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3070,1,['wrap'],['wraps']
Integrability,"rmagic.py:943, in RMagics.R(self, line, cell, local_ns); 941 if not e.stdout.endswith(e.err):; 942 print(e.err); --> 943 raise e; 944 finally:; 945 if self.device in DEVICES_STATIC:. File /scratch/work/malonzm1/.conda_envs/R_for_scater/lib/python3.9/site-packages/rpy2/ipython/rmagic.py:923, in RMagics.R(self, line, cell, local_ns); 921 return_output = False; 922 else:; --> 923 text_result, result, visible = self.eval(code); 924 text_output += text_result; 925 if visible:. File /scratch/work/malonzm1/.conda_envs/R_for_scater/lib/python3.9/site-packages/rpy2/ipython/rmagic.py:389, in RMagics.eval(self, code); 386 except (ri.embedded.RRuntimeError, ValueError) as exception:; 387 # Otherwise next return seems to have copy of error.; 388 warning_or_other_msg = self.flush(); --> 389 raise RInterpreterError(code, str(exception),; 390 warning_or_other_msg); 391 text_output = self.flush(); 392 return text_output, value, visible[0]. RInterpreterError: Failed to parse and evaluate line '\n# specify row and column names of data\nrownames(data) = genes\ncolnames(data) = cells\n# ensure correct sparse format for table of counts and table of droplets\ndata <- as(data, ""sparseMatrix"")\ndata_tod <- as(data_tod, ""sparseMatrix"")\n\n# Generate SoupChannel Object for SoupX \nsc = SoupChannel(data_tod, data, calcSoupProfile = FALSE)\n\n# Add extra meta data to the SoupChannel object\nsoupProf = data.frame(row.names = rownames(data), est = rowSums(data)/sum(data), counts = rowSums(data))\nsc = setSoupProfile(sc, soupProf)\n# Set cluster information in SoupChannel\nsc = setClusters(sc, soupx_groups)\n\n# Estimate contamination fraction\nsc = autoEstCont(sc, doPlot=FALSE)\n# Infer corrected table of counts and rount to integer\nout = adjustCounts(sc, roundToInt = TRUE)\n'.; R error message: 'Error in data.frame(row.names = rownames(data), est = rowSums(data)/sum(data), : \n duplicate row.names: TBCE, LINC01238, CYB561D2, MATR3, LINC01505, HSPA14, GOLGA8M, GGT1, ARMCX5-GPRASP2, TMSB15B'; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2685#issuecomment-1763978277:5800,message,message,5800,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2685#issuecomment-1763978277,1,['message'],['message']
Integrability,"rol metrics. Calculates a number of qc metrics for an AnnData object, see section ; Returns for specifics. Largely based on `calculateQCMetrics` from scater; [McCarthy17]_. Currently is most efficient on a sparse CSR or dense matrix. Parameters; ----------; adata : :class:`~anndata.AnnData`; Annotated data matrix.; expr_type : `str`, optional (default: `""counts""`); Name of kind of values in X.; var_type : `str`, optional (default: `""genes""`); The kind of thing the variables are.; qc_vars : `Container`, optional (default: `()`); Keys for boolean columns of `.var` which identify variables you could ; want to control for (e.g. ""ERCC"" or ""mito"").; percent_top : `Container[int]`, optional (default: `(50, 100, 200, 500)`); Which proportions of top genes to cover. If empty or `None` don't; calculate.; inplace : bool, optional (default: `False`); Whether to place calculated metrics in `.obs` and `.var`. Returns; -------; Union[NoneType, Tuple[pd.DataFrame, pd.DataFrame]]; Depending on `inplace` returns calculated metrics (`pd.DataFrame`) or; updates `adata`'s `obs` and `var`. Observation level metrics include:. * `total_{var_type}_by_{expr_type}`; E.g. ""total_genes_by_counts"". Number of genes with positive counts ; in a cell.; * `total_{expr_type}`; E.g. ""total_counts"". Total number of counts for a cell.; * `pct_{expr_type}_in_top_{n}_{var_type}` - for `n` in `percent_top`; E.g. ""pct_counts_in_top_50_genes"". Cumulative percentage of counts ; for 50 most expressed genes in a cell.; * `total_{expr_type}_{qc_var}` - for `qc_var` in `qc_vars`; E.g. ""total_counts_mito"". Total number of counts for variabes in ; `qc_vars`.; * `pct_{expr_type}_{qc_var}` - for `qc_var` in `qc_vars`; E.g. ""pct_counts_mito"". Proportion of total counts for a cell which ; are mitochondrial. Variable level metrics include:. * `total_{expr_type}`; E.g. ""total_counts"". Sum of counts for a gene.; * `mean_{expr_type}`; E.g. ""mean counts"". Mean expression over all cells.; * `n_cells_by_{expr_type}`; E.g. ""n_c",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/424#issuecomment-454024688:1414,Depend,Depending,1414,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/424#issuecomment-454024688,1,['Depend'],['Depending']
Integrability,rotlipy 0.7.0; cached-property 1.5.2; cachetools 5.2.0; certifi 2020.12.5; cffi 1.14.5; chardet 4.0.0; charset-normalizer 2.0.12; chex 0.1.3; click 8.1.3; colormath 3.0.0; commonmark 0.9.1; conda 4.6.14; conda-package-handling 1.7.3; cryptography 3.4.7; cycler 0.10.0; Cython 0.29.30; decorator 5.0.7; defusedxml 0.7.1; dill 0.3.3; dm-tree 0.1.7; docrep 0.3.2; entrypoints 0.4; et-xmlfile 1.1.0; fa2 0.3.5; fastjsonschema 2.15.3; flatbuffers 2.0; flax 0.5.0; frozenlist 1.3.0; fsspec 2022.5.0; future 0.18.2; get-version 2.2; google-auth 2.6.6; google-auth-oauthlib 0.4.6; google-pasta 0.2.0; grpcio 1.46.3; h5py 3.2.1; idna 2.10; imageio 2.19.3; importlib-metadata 4.11.4; importlib-resources 5.7.1; ipykernel 5.5.4; ipython 7.23.1; ipython-genutils 0.2.0; ipywidgets 7.7.0; jax 0.3.13; jaxlib 0.3.10; jedi 0.18.0; Jinja2 3.1.2; jmespath 0.10.0; joblib 1.0.1; jsonschema 4.6.0; jupyter-client 6.1.12; jupyter-core 4.7.1; jupyterlab-pygments 0.2.2; jupyterlab-widgets 1.1.0; kiwisolver 1.3.1; legacy-api-wrap 1.2; leidenalg 0.8.4; llvmlite 0.35.0; loompy 3.0.7; louvain 0.7.0; Markdown 3.3.7; MarkupSafe 2.1.1; matplotlib 3.4.1; matplotlib-inline 0.1.2; mistune 0.8.4; msgpack 1.0.4; multidict 6.0.2; multipledispatch 0.6.0; multiprocess 0.70.11.1; natsort 7.1.1; nbclient 0.6.4; nbconvert 6.5.0; nbformat 5.4.0; nest-asyncio 1.5.5; networkx 2.5; notebook 6.4.11; numba 0.52.0; numexpr 2.7.3; numpy 1.19.5; numpy-groupies 0.9.17; numpyro 0.9.2; oauthlib 3.2.0; openpyxl 3.0.10; opt-einsum 3.3.0; optax 0.1.2; packaging 20.9; pandas 1.2.0; pandocfilters 1.5.0; parso 0.8.2; pathos 0.2.7; patsy 0.5.1; pexpect 4.8.0; pickleshare 0.7.5; Pillow 9.1.1; pip 21.1.1; pox 0.2.9; ppft 1.6.6.3; prometheus-client 0.14.1; prompt-toolkit 3.0.18; protobuf 3.19.0; protobuf3-to-dict 0.1.5; ptyprocess 0.7.0; pyasn1 0.4.8; pyasn1-modules 0.2.8; pycosat 0.6.3; pycparser 2.20; pyDeprecate 0.3.1; Pygments 2.9.0; pyOpenSSL 20.0.1; pyparsing 2.4.7; pyro-api 0.1.2; pyro-ppl 1.8.1; pyrsistent 0.18.1; PySocks 1.7.1; pyth,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1351#issuecomment-1146346336:3424,wrap,wrap,3424,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351#issuecomment-1146346336,2,['wrap'],['wrap']
Integrability,"round. > Who manages the sub-packages?. Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places?. > How does this impact users vs. developers?. Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use `scio` and then be on their way (a reality that many people do not feel the need to use scanpy/muon). If there are R converters, this would be a major use case. > What we read in, and how we represent it, is very tightly coupled to the methods we have. Up for discussion, but read the maximal amount of information by default. If necessary (don't see any particular cases at the moment), package devs use the underlying `scio` function and reorganize.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059551352:1820,depend,depend,1820,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059551352,2,['depend'],['depend']
Integrability,"rror: DLL load failed while importing _loess: The specified module could not be found.; ```; Step5: run `import skmisc; print(skmisc.__file__)`; ```python; import skmisc; print(skmisc.__file__); C:\Users\Park_Lab\AppData\Roaming\Python\Python38\site-packages\skmisc\__init__.py; ```; Step6: due to Step4, I follow the solution (https://github.com/has2k1/scikit-misc/issues/4) to install Numpy with mkl.; ```python; (base) C:\Users\Park_Lab>conda activate Python38; (Python38) C:\Users\Park_Lab>cd Downloads/; (Python38) C:\Users\Park_Lab\Downloads>pip install numpy-1.21.5+mkl-cp38-cp38-win_amd64.whl; Processing c:\users\park_lab\downloads\numpy-1.21.5+mkl-cp38-cp38-win_amd64.whl; Installing collected packages: numpy; Attempting uninstall: numpy; Found existing installation: numpy 1.21.5; Uninstalling numpy-1.21.5:; Successfully uninstalled numpy-1.21.5; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; numba 0.54.1 requires numpy<1.21,>=1.17, but you have numpy 1.21.5+mkl which is incompatible.; Successfully installed numpy-1.21.5+mkl; ```; Step7: check anaconda Python38 environment, numpy-1.21.5+mkl is successfully installed; ![image](https://user-images.githubusercontent.com/75048821/147306587-eb94c188-5c18-40f0-add0-3a899872d786.png). Step8: Scanpy import error. Numpy>v1.20 is conflicted with Scanpy; ```python; import numpy as np; import pandas as pd; import scanpy as sc; import scanpy.external as sce; import scipy; sc.settings.verbosity = 3; sc.logging.print_header(); sc.set_figure_params(dpi=100, dpi_save=600); ; import scvelo as scv; scv.settings.verbosity = 3; scv.settings.presenter_view = True; scv.logging.print_versions(). import cellrank as cr; cr.settings.verbosity = 3; cr.logging.print_versions(). import matplotlib.pyplot as pl; from matplotlib import rcParams; ImportError Traceback (most recent call last); ~\AppData\Local\Temp/ipykerne",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342:4861,depend,dependency,4861,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342,1,['depend'],['dependency']
Integrability,"ry familiar with Scanpy (which seems like a fantastic library!), so please bear with me if some of the things I mention are not relevant to Scanpy. PyMDE (documentation here: https://pymde.org/) has a few benefits:; - as @adamgayoso mentioned, PyMDE supports computing embeddings on GPU. This makes it possible to compute very large embeddings quickly (often 4-10x faster than CPU).; - PyMDE is a very general embedding library. It is based on a general framework for embedding, and this framework includes many well-known methods --- such as UMAP, PCA, Laplacian embedding, multi-dimensional scaling, and more --- as special cases. This makes it easy to compare different methods using a single framework.; - PyMDE also supports creating entirely new types of embeddings, as custom instances of our framework.; - PyMDE provides ways to reason about how much an embedding distorts the original neighborhood graph. There are some comparisons to UMAP & openTSNE in the third part of our manuscript, which has been published in Foundations & Trends in Machine Learning and is available here: https://web.stanford.edu/~boyd/papers/pdf/min_dist_emb.pdf; - on CPU, UMAP and PyMDE are comparable in speed, with UMAP often having a slight edge; on GPU PyMDE can be much faster; - unlike UMAP/openTSNE, PyMDE allows users to fit constrained embeddings. Right now the supported constraints are standardization (zero mean, unit covariance; this forces embeddings to spread out, but not too much, and as a result standardized embeddings are typically similarly scaled), centering, and anchoring (pre-specifying the coordinates of a subset of the items); - PyMDE allows for more types of embeddings, in addition to UMAP-style embeddings. On the other hand, PyMDE is young software. If you do depend on it, I would recommend including it as an optional dependency, not a required one. Happy to chat more, to answer any questions, and to help with integration, if that is something you are ultimately interested in.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2154#issuecomment-1051103627:1903,depend,depend,1903,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2154#issuecomment-1051103627,3,"['depend', 'integrat']","['depend', 'dependency', 'integration']"
Integrability,"s 5.0.5 pyhd3eb1b0_0 ; triku 1.3.1 pypi_0 pypi; tslearn 0.5.0.5 pypi_0 pypi; typed-ast 1.4.2 py38h27cfd23_1 ; typing 3.10.0.0 py38h06a4308_0 ; typing-extensions 3.10.0.0 pypi_0 pypi; typing_extensions 3.7.4.3 pyha847dfd_0 ; tzlocal 2.1 py38_0 ; ujson 4.0.2 py38h2531618_0 ; umap-learn 0.5.1 py38h578d9bd_0 conda-forge; unicodecsv 0.14.1 py38_0 ; unixodbc 2.3.9 h7b6447c_0 ; urllib3 1.26.4 pyhd3eb1b0_0 ; vendorize 0.2.1 pypi_0 pypi; watchdog 1.0.2 py38h06a4308_1 ; wcwidth 0.2.5 py_0 ; webencodings 0.5.1 py38_1 ; werkzeug 1.0.1 pyhd3eb1b0_0 ; wheel 0.36.2 pyhd3eb1b0_0 ; widgetsnbextension 3.5.1 py38_0 ; wrapt 1.12.1 py38h7b6447c_1 ; wurlitzer 2.1.0 py38h06a4308_0 ; xlrd 1.2.0 pypi_0 pypi; xlsxwriter 1.3.8 pyhd3eb1b0_0 ; xlwt 1.3.0 py38_0 ; xz 5.2.5 h7b6447c_0 ; yaml 0.2.5 h7b6447c_0 ; yapf 0.31.0 pyhd3eb1b0_0 ; yarl 1.6.3 pypi_0 pypi; zeromq 4.3.4 h2531618_0 ; zict 2.0.0 pyhd3eb1b0_0 ; zipp 3.4.1 pyhd3eb1b0_0 ; zlib 1.2.11 h7b6447c_3 ; zope 1.0 py38_1 ; zope.event 4.5.0 py38_0 ; zope.interface 5.3.0 py38h27cfd23_0 ; zstd 1.4.5 h9ceee32_0 ; ```. </Details>. <Details>; <Summary>ImportError traceback</Summary>. ```python; ---------------------------------------------------------------------------; ImportError Traceback (most recent call last); <ipython-input-2-0074c9bc0b31> in <module>; ----> 1 import scanpy as sc. ~/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/scanpy/__init__.py in <module>; 3 from ._metadata import __version__, __author__, __email__; 4 ; ----> 5 from ._utils import check_versions; 6 ; 7 check_versions(). ~/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/scanpy/_utils.py in <module>; 16 from numpy import random; 17 from scipy import sparse; ---> 18 from anndata import AnnData, __version__ as anndata_version; 19 from textwrap import dedent; 20 from packaging import version. ~/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/anndata/__init__.py in <module>; 5 if not within_flit():; 6 del within_flit; ----> 7 from ._core.anndata import AnnDat",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:18608,interface,interface,18608,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310,2,['interface'],['interface']
Integrability,"s of:; - preserving visual interpretation of absent/low/med/high (corresponding to expectations of cell subsets); - handling a variety of marker distribution shapes (unimodal/bimodal/trimodal, skewed shapes); - making it easier to spot nonspecific antibody staining / off-target effects; - not introducing more bias in downstream differential comparisons (fits with assumptions about variable distribution properties, based on the commonly used statistical testing methods). absent a convincing answer, it may be worth implementing multiple as options, leaving the choice to the user, and just documenting these use-cases through citations; eventually, someone can make a notebook that compares the behaviors, biological expectations, and/or impacts on statistical comparisons to inform which method should be the default. While the CITEseq paper applied CLR, it's not obvious that one is better than the ones used in more time-tested fields like mass cytometry and flow cytometry. ```python; def CLR_transform(df):; '''; implements the CLR transform used in CITEseq (need to confirm in Seurat's code); https://doi.org/10.1038/nmeth.4380; '''; logn1 = np.log(df + 1); T_clr = logn1.sub(logn1.mean(axis=1), axis=0); return T_clr. def asinh_transform(df, cofactor=5):; '''; implements the hyperbolic arcsin transform used in CyTOF/mass cytometry; https://doi.org/10.1038/nmeth.4380; '''; T_cytof = np.arcsinh(df / cofactor); return T_cytof. def geometric_transform(df):; '''; implements the scanpy transform originating from ivirshup:multimodal; '''; from scipy.stats.mstats import gmean; T_geometric = np.divide(df, gmean(df + 1, axis=0)); return T_geometric. #optionally, for each of these, similar to some cytof workflows, ; #anchor 1-99% quantiles to 0-1, to rescale distribution within a standardized range; #use quantiles as a simple heuristic, due to extreme signal outliers that throw off the scale; #can also floor/ceil, depending on whether values beyond 0-1 are compatible or meaningful; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1117#issuecomment-635963691:2533,depend,depending,2533,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-635963691,2,['depend'],['depending']
Integrability,"s the error:; ```pytb; RuntimeError Traceback (most recent call last); ~/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, key, val, *args, **kwargs); 208 try:; --> 209 return func(elem, key, val, *args, **kwargs); 210 except Exception as e:. ~/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_array(f, key, value, dataset_kwargs); 184 value = _to_hdf5_vlen_strings(value); --> 185 f.create_dataset(key, data=value, **dataset_kwargs); 186 . ~/opt/anaconda3/lib/python3.8/site-packages/h5py/_hl/group.py in create_dataset(self, name, shape, dtype, data, **kwds); 138 if name is not None:; --> 139 self[name] = dset; 140 return dset. ~/opt/anaconda3/lib/python3.8/site-packages/h5py/_hl/group.py in __setitem__(self, name, obj); 372 if isinstance(obj, HLObject):; --> 373 h5o.link(obj.id, self.id, name, lcpl=lcpl, lapl=self._lapl); 374 . h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/h5o.pyx in h5py.h5o.link(). RuntimeError: Unable to create link (name already exists). The above exception was the direct cause of the following exception:. RuntimeError Traceback (most recent call last); ~/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, key, val, *args, **kwargs); 208 try:; --> 209 return func(elem, key, val, *args, **kwargs); 210 except Exception as e:. ~/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_series(group, key, series, dataset_kwargs); 283 write_array(group, category_key, categories, dataset_kwargs=dataset_kwargs); --> 284 write_array(group, key, codes, dataset_kwargs=dataset_kwargs); 285 . ~/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, key, val, *args, **kwargs); 211 parent = _get_parent(elem); --> 212 raise type(e)(; 213 f""{e}\n\n"". RuntimeError: Unable to create link (name already exists). Above error raised while writing key 'gene_name' of <cla",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1982:2087,wrap,wrapper,2087,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1982,1,['wrap'],['wrapper']
Integrability,"s,; 1784 plot_kws=kwargs,; 1785 ); 1787 p._add_axis_labels(ax); 1788 p._adjust_cat_axis(ax, axis=p.orient). File ~/miniconda3/envs/scarf_env/lib/python3.12/site-packages/seaborn/categorical.py:1047, in _CategoricalPlotter.plot_violins(self, width, dodge, gap, split, color, fill, linecolor, linewidth, inner, density_norm, common_norm, kde_kws, inner_kws, plot_kws); 1045 # Plot the main violin body; 1046 plot_func = {""x"": ax.fill_betweenx, ""y"": ax.fill_between}[self.orient]; -> 1047 plot_func(; 1048 inv_val(data[value_var]),; 1049 inv_pos(data[self.orient] - offsets[0]),; 1050 inv_pos(data[self.orient] + offsets[1]),; 1051 **violin[""kwargs""]; 1052 ); 1054 # Adjust the observation data; 1055 obs = violin[""observations""]. File ~/miniconda3/envs/scarf_env/lib/python3.12/site-packages/matplotlib/__init__.py:1473, in _preprocess_data.<locals>.inner(ax, data, *args, **kwargs); 1470 @functools.wraps(func); 1471 def inner(ax, *args, data=None, **kwargs):; 1472 if data is None:; -> 1473 return func(; 1474 ax,; 1475 *map(sanitize_sequence, args),; 1476 **{k: sanitize_sequence(v) for k, v in kwargs.items()}); 1478 bound = new_sig.bind(ax, *args, **kwargs); 1479 auto_label = (bound.arguments.get(label_namer); 1480 or bound.kwargs.get(label_namer)). File ~/miniconda3/envs/scarf_env/lib/python3.12/site-packages/matplotlib/axes/_axes.py:5662, in Axes.fill_betweenx(self, y, x1, x2, where, step, interpolate, **kwargs); 5660 def fill_betweenx(self, y, x1, x2=0, where=None,; 5661 step=None, interpolate=False, **kwargs):; -> 5662 return self._fill_between_x_or_y(; 5663 ""y"", y, x1, x2,; 5664 where=where, interpolate=interpolate, step=step, **kwargs). File ~/miniconda3/envs/scarf_env/lib/python3.12/site-packages/matplotlib/axes/_axes.py:5629, in Axes._fill_between_x_or_y(self, ind_dir, ind, dep1, dep2, where, interpolate, step, **kwargs); 5625 pts = pts[:, ::-1]; 5627 polys.append(pts); -> 5629 collection = mcoll.PolyCollection(polys, **kwargs); 5631 # now update the datalim and autoscale; ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3140:5441,wrap,wraps,5441,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3140,1,['wrap'],['wraps']
Integrability,"s. Found 34 batches. Found 1 categorical variables:; 	age_group. Found 0 numerical variables:; 	. ---------------------------------------------------------------------------; LinAlgError Traceback (most recent call last); <timed eval> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace); 204 # standardize across genes using a pooled variance estimator; 205 logg.info(""Standardizing Data across genes.\n""); --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key); 207 ; 208 # fitting the parameters on the standardized data. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key); 102 ; 103 # compute pooled variance estimator; --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T); 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]); 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **kwargs). ~/.local/lib/python3.8/site-packages/numpy/linalg/linalg.py in inv(a); 544 signature = 'D->D' if isComplexType(t) else 'd->d'; 545 extobj = get_linalg_error_extobj(_raise_linalgerror_singular); --> 546 ainv = _umath_linalg.inv(a, signature=signature, extobj=extobj); 547 return wrap(ainv.astype(result_t, copy=False)); 548 . ~/.local/lib/python3.8/site-packages/numpy/linalg/linalg.py in _raise_linalgerror_singular(err, flag); 86 ; 87 def _raise_linalgerror_singular(err, flag):; ---> 88 raise LinAlgError(""Singular matrix""); 89 ; 90 def _raise_linalgerror_nonposdef(err, flag):. LinAlgError: Singular matrix; ```; #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>. scanpy==1.7.0rc2.dev7+g57ec8a7e anndata==0.7.3 umap==0.4.6 numpy==1.19.5 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.1 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1606:2677,wrap,wrap,2677,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606,1,['wrap'],['wrap']
Integrability,"savefig(writekey, dpi=dpi, ext=ext); 313 if show:; 314 pl.show(). ~/opt/anaconda3/lib/python3.8/site-packages/scanpy/plotting/_utils.py in savefig(writekey, dpi, ext); 280 else:; 281 dpi = rcParams['savefig.dpi']; --> 282 settings.figdir.mkdir(parents=True, exist_ok=True); 283 if ext is None:; 284 ext = settings.file_format_figs. AttributeError: 'str' object has no attribute 'mkdir'; ```. #### Versions. scanpy==1.8.1 anndata==0.7.6 umap==0.5.1 numpy==1.18.5 scipy==1.6.2 pandas==1.1.5 scikit-learn==0.24.2 statsmodels==0.12.2 python-igraph==0.9.4 louvain==0.7.0 pynndescent==0.5.2. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. WARNING: If you miss a compact list, please try `print_header`!; The `sinfo` package has changed name and is now called `session_info` to become more discoverable and self-explanatory. The `sinfo` PyPI package will be kept around to avoid breaking old installs and you can downgrade to 0.3.2 if you want to use it without seeing this message. For the latest features and bug fixes, please install `session_info` instead. The usage and defaults also changed slightly, so please review the latest README at https://gitlab.com/joelostblom/session_info.; -----; anndata 0.7.6; scanpy 1.8.1; sinfo 0.3.4; -----; PIL 8.3.1; anyio NA; appdirs 1.4.4; appnope 0.1.2; attr 21.2.0; babel 2.9.1; backcall 0.2.0; bioservices 1.7.12; bottleneck 1.3.2; brotli NA; bs4 4.9.3; certifi 2021.05.30; cffi 1.14.6; chardet 4.0.0; cloudpickle 1.6.0; colorama 0.4.4; colorlog NA; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2021.07.2; dateutil 2.8.2; decorator 5.0.9; defusedxml 0.7.1; docutils 0.17.1; easydev 0.11.1; fsspec 2021.07.0; gseapy 0.10.5; h5py 2.10.0; html5lib 1.1; idna 2.10; igraph 0.9.4; ipykernel 5.3.4; ipython_genutils 0.2.0; ipywidgets 7.6.3; jedi 0.17.2; jinja2 2.11.3; joblib 1.0.1; json5 NA; jsonschema 3.2.0; jupyter_server 1.4.1; jupyterlab_server 2.6.1; kiwisolver 1.3.1; leidenalg 0.8.4; llvmlite 0.36.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1981:3558,message,message,3558,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1981,1,['message'],['message']
Integrability,"sc.pl creates ""Do not localize"" message",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/202:32,message,message,32,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/202,1,['message'],['message']
Integrability,"sc.pl.scatter() is a wrapper for _scatter_obs(). It checks to make sure; the variable names the caller is requesting to plot exist in var and/or; obs, but does not take into account whether it should look in raw based; on the use_raw flag, as _scatter_obs() does. This leads to errors when a; user asks to plot variables that are in the raw but not the filtered; matrix of adata. This commit fixes that bug. <!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2027:21,wrap,wrapper,21,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2027,1,['wrap'],['wrapper']
Integrability,sc.pl.umap error message if sc.tl.umap has not been computed.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1460:17,message,message,17,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1460,1,['message'],['message']
Integrability,"scTransform is easily usable if you use rpy2 and anndata2ri. I use directly; the vst R function at this address to make it work; https://github.com/ChristophH/sctransform/blob/master/R/vst.R. Den søn. 23. feb. 2020 kl. 00.44 skrev MalteDLuecken <; notifications@github.com>:. > Hi, It's not available in scanpy at the moment, but I wrote a wrapper for; > it via rpy2 and anndata2ri which is available here:; >; > https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/1068?email_source=notifications&email_token=ACC66UMYH2ZHSMFFQS35FRLREG2ENA5CNFSM4KZJFJP2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEMVNJCY#issuecomment-590009483>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ACC66UJ2GVSPUTR4WLWM2V3REG2ENANCNFSM4KZJFJPQ>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1068#issuecomment-590049395:340,wrap,wrapper,340,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-590049395,2,['wrap'],['wrapper']
Integrability,"scvelo docs have been changed so the url for the sphinx inventory is different. We also probably don't want to depend on scvelo's documentation for our doc builds, especially since it's pre 1.0, and we weren't really doing much with it. Should fix current doc build problems.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1608:111,depend,depend,111,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1608,1,['depend'],['depend']
Integrability,"scvelo/preprocessing/neighbors.py in neighbors(adata, n_neighbors, n_pcs, use_rep, use_highly_variable, knn, random_state, method, metric, metric_kwds, num_threads, copy); 161 warnings.simplefilter(""ignore""); 162 neighbors = Neighbors(adata); --> 163 neighbors.compute_neighbors(; 164 n_neighbors=n_neighbors,; 165 knn=knn,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds); 748 # we need self._distances also for method == 'gauss' if we didn't; 749 # use dense distances; --> 750 self._distances, self._connectivities = _compute_connectivities_umap(; 751 knn_indices,; 752 knn_distances,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity); 353 # umap 0.5.0; 354 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""); --> 355 from umap.umap_ import fuzzy_simplicial_set; 356 ; 357 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/__init__.py in <module>; ----> 1 from .umap_ import UMAP; 2 ; 3 # Workaround: https://github.com/numba/numba/issues/3341; 4 import numba; 5 . ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/umap_.py in <module>; 52 from umap.spectral import spectral_layout; 53 from umap.utils import deheap_sort, submatrix; ---> 54 from umap.layouts import (; 55 optimize_layout_euclidean,; 56 optimize_layout_generic,. ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py in <module>; 37 },; 38 ); ---> 39 def rdist(x, y):; 40 """"""Reduced Euclidean distance.; 41 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func); 219 with typeinfer.register_dispatcher(disp):; 220 for sig in sigs:; --> 221 disp.compile(sig); 222 disp.disable_compile(); 223 return disp. ~/.conda/envs/rpy/lib/p",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796:2746,message,message,2746,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796,2,['message'],['message']
Integrability,scvi integration,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/520:5,integrat,integration,5,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/520,1,['integrat'],['integration']
Integrability,"sed that this step is taking too long as is was supposed to reduce the plotting time. I would not wait for more than 5 minutes to see a plot. How many genes were you planning to plot? The background is that when plotting a heatmap, the matplotlib visualization will randomly drop genes because the resolution of the screens is not high enough. Thus, when the number of genes is large, I was trying to find a compromise by fitting a line before the plotting and then only plotting the fit. Can you help me solve the issue by sharing the data with me? Or, can you find an example that can reproduce the problem?; > […](#); > On Tue, May 7, 2019 at 10:19 AM brianpenghe ***@***.***> wrote: I was trying to plot a heatmap using this command: ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0, dendrogram=True, save='ClusterMap.png') And it didn't finish running after an overnight, with the following warning message: WARNING: Gene labels are not shown when more than 50 genes are visualized. To show gene labels set show_gene_labels=True /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227: UserWarning: The maximal number of iterations maxit (set to 20 by the program) allowed for finding a smoothing spline with fp=s has been reached: s too small. There is an approximation returned but the corresponding weighted sum of squared residuals does not satisfy the condition abs(fp-s)/s < tol. warnings.warn(message) I don't understand why this is taking this long because seaborn was able to finish plotting within 30 minutes. Do you know why? — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#633>, or mute the thread <https://github.com/notifications/unsubscribe-auth/ABF37VNDX37RZL256MWKDM3PUE3RFANCNFSM4HLGOYGA> .; > -- Fidel Ramirez. I was planning to plot a heatmap of 300 genes. However, I have 90k cells. I guess t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/633#issuecomment-491103142:1042,message,message,1042,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633#issuecomment-491103142,2,['message'],['message']
Integrability,seems like the `--deps` flag can be used to select dependencies. Default is all dependencies. ```bash; $ beni --deps production pyproject.toml; channels:; - conda-forge; dependencies:; - pip:; - flit; - python>=3.7; - pip; - anndata>=0.7.4; - numpy>=1.17.0; - matplotlib-base>=3.1.2; - pandas>=0.21; - scipy>=1.4; - seaborn-split; - h5py>=2.10.0; - pytables; - tqdm; - scikit-learn>=0.22; - statsmodels>=0.10.0rc2; - patsy; - networkx>=2.3; - natsort; - joblib; - numba>=0.41.0; - umap-learn>=0.3.10; - packaging; - sinfo; name: scanpy; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2144#issuecomment-1055433811:51,depend,dependencies,51,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2144#issuecomment-1055433811,6,['depend'],['dependencies']
Integrability,"similar error when I was trying to use .h5 file from cellbender output. I have multiome data. . ```pytb; `>>> adata = scanpy.read_10x_h5(""/sc/arion/projects/hmDNAmap/snHeroin/analysis/ARC_TD005235-354/outs/cellbender/cb_feature_bc_matrix_filtered.h5"", gex_only=False)`; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/scanpy/readwrite.py"", line 183, in read_10x_h5; adata = _read_v3_10x_h5(filename, start=start); File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/scanpy/readwrite.py"", line 268, in _read_v3_10x_h5; _collect_datasets(dsets, f[""matrix""]); File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/scanpy/readwrite.py"", line 256, in _collect_datasets; dsets[k] = v[:]; File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper; File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper; File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/h5py/_hl/dataset.py"", line 738, in __getitem__; selection = sel2.select_read(fspace, args); File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/h5py/_hl/selections2.py"", line 101, in select_read; return ScalarReadSelection(fspace, args); File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/h5py/_hl/selections2.py"", line 86, in __init__; raise ValueError(""Illegal slicing argument for scalar dataspace""). > **ValueError: Illegal slicing argument for scalar dataspace**; ```. `>>> scanpy.logging.print_versions()`. anndata 0.8.0; scanpy 1.9.1. PIL 8.4.0; beta_ufunc NA; binom_ufunc NA; bottleneck 1.3.2; cffi 1.14.6; cloudpickle 2.0.0; colorama 0.4.4; concurrent NA; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2021.10.0; dateutil 2.8.2; defusedxml 0.7.1; encodings NA; fsspec 2021.08.1; genericpath NA; h5py 3.3.0; igraph 0.9.6; jinja2 2.11.3; joblib 1.1.0; kiwisolver 1.3.1; leidenalg 0.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2203#issuecomment-1129213572:971,wrap,wrapper,971,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2203#issuecomment-1129213572,1,['wrap'],['wrapper']
Integrability,"sing=2.4.7=pyhd3eb1b0_0; - pyqt=5.9.2=py37h05f1152_2; - pyrsistent=0.17.3=py37h7b6447c_0; - python=3.7.9=h7579374_0; - python-dateutil=2.8.1=pyhd3eb1b0_0; - pyzmq=20.0.0=py37h2531618_1; - qt=5.9.7=h5867ecd_1; - qtconsole=4.7.7=py_0; - qtpy=1.9.0=py_0; - readline=8.1=h27cfd23_0; - send2trash=1.5.0=pyhd3eb1b0_1; - setuptools=52.0.0=py37h06a4308_0; - sip=4.19.8=py37hf484d3e_0; - six=1.15.0=py37h06a4308_0; - sqlite=3.33.0=h62c20be_0; - terminado=0.9.2=py37h06a4308_0; - testpath=0.4.4=pyhd3eb1b0_0; - tk=8.6.10=hbc83047_0; - tornado=6.1=py37h27cfd23_0; - traitlets=5.0.5=pyhd3eb1b0_0; - wcwidth=0.2.5=py_0; - webencodings=0.5.1=py37_1; - wheel=0.36.2=pyhd3eb1b0_0; - widgetsnbextension=3.5.1=py37_0; - xz=5.2.5=h7b6447c_0; - zeromq=4.3.3=he6710b0_3; - zipp=3.4.0=pyhd3eb1b0_0; - zlib=1.2.11=h7b6447c_3; - pip:; - anndata==0.7.5; - cached-property==1.5.2; - click==7.1.2; - cycler==0.10.0; - get-version==2.1; - h5py==3.1.0; - importlib-metadata==3.4.0; - joblib==1.0.0; - kiwisolver==1.3.1; - legacy-api-wrap==1.2; - leidenalg==0.8.3; - llvmlite==0.35.0; - loompy==3.0.6; - louvain==0.7.0; - matplotlib==3.3.4; - natsort==7.1.1; - networkx==2.5; - numba==0.52.0; - numexpr==2.7.2; - numpy==1.20.0; - numpy-groupies==0.9.13; - pandas==1.2.1; - patsy==0.5.1; - pillow==8.1.0; - python-igraph==0.8.3; - pytz==2021.1; - scanpy==1.6.1; - scikit-learn==0.24.1; - scipy==1.6.0; - scvelo==0.2.2; - seaborn==0.11.1; - setuptools-scm==5.0.1; - sinfo==0.3.1; - statsmodels==0.12.1; - stdlib-list==0.8.0; - tables==3.6.1; - texttable==1.6.3; - threadpoolctl==2.1.0; - tqdm==4.56.0; - typing-extensions==3.7.4.3; - umap-learn==0.4.6; ```. I can reproduce the issue with a Docker container that only has the minimal conda environment above. Additionally, I already tried installing the exact same dependency versions in the new environment, but got the same results. ; If you need access to the data and the container please contact me and I will make it available to you.; The data is already at the ICB cluster. ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1625:4621,wrap,wrap,4621,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625,1,['wrap'],['wrap']
Integrability,"sion of scanpy.; - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python; import scanpy; scanpy.logging.print_versions(); ```. ### Error output. ```pytb; ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); Cell In[44], line 1; ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file); 178 print_versions(); 179 else:; --> 180 session_info.show(; 181 dependencies=True,; 182 html=False,; 183 excludes=[; 184 'builtins',; 185 'stdlib_list',; 186 'importlib_metadata',; 187 # Special module present if test coverage being calculated; 188 # https://gitlab.com/joelostblom/session_info/-/issues/10; 189 ""$coverage"",; 190 ],; 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes); 207 for mod_name in clean_modules:; 208 mod_names.append(mod_name); --> 209 mod = sys.modules[mod_name]; 210 # Since modules use different attribute names to store version info,; 211 # try the most common ones.; 212 try:. KeyError: 'numcodecs'; ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes several versions in the output. ```; scanpy==1.10.0.dev88+gedd61302 anndata==0.9.2 umap==0.5.3 numpy==1.24.4 scipy==1.11.1 pandas==2.0.3 scikit-learn==1.3.0 statsmodels==0.14.0 igraph==0.10.6 pynndescent==0.5.10; ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2580:1391,depend,dependencies,1391,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580,1,['depend'],['dependencies']
Integrability,"so you would want three installations, right?. - full (by manually installing all optional dependencies); - uncomplicated but limited; - super barebones. since `scanpy` is already the second version, we don’t lose anything this way. in the future maybe we can achieve that `scanpy` becomes the full installation (once the C++ dependencies start shipping wheels)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/59#issuecomment-355115416:91,depend,dependencies,91,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/59#issuecomment-355115416,2,['depend'],['dependencies']
Integrability,"sure!. as long as the last version of `scanpy-full` does nothing but depend on `scanpy`, nobody will suffer any consequences if it becomes obsolete one day.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/59#issuecomment-355146641:69,depend,depend,69,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/59#issuecomment-355146641,1,['depend'],['depend']
Integrability,"t aren't repeated. I think it's fine for this to work. I do think it should error if the key is one values that is duplicated in the index. ```python; adata = sc.AnnData(; X=np.ones((2, 3)),; obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),; var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),; ); sc.get.obs_df(adata, [""gene-1""]); ``````. ### This PR (errors). ```pytb; ---------------------------------------------------------------------------; InvalidIndexError Traceback (most recent call last); <ipython-input-62-405d671e2970> in <module>; ----> 1 sc.get.obs_df(adata, [""a"", ""gene-1""]). ~/github/scanpy/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw); 213 var_idx = adata.raw.var_names.get_indexer(var_names); 214 else:; --> 215 var_idx = adata.var_names.get_indexer(var_names); 216 ; 217 # for backed AnnData is important that the indices are ordered. /usr/local/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance); 3169 ; 3170 if not self.is_unique:; -> 3171 raise InvalidIndexError(; 3172 ""Reindexing only valid with uniquely valued Index objects""; 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects; ```. ### 1.6 (suceeds). ```python; gene-1; cell-0 1.0; cell-1 1.0; ```. 1.6 does error if I use `""gene-0""` as a key, but the error message could definitley be better. ## What should we do about this?. My current inclination is to revert most changes to `obs_df` and `var_df` from this PR and #1499. This should leave the use of indices as groupby untouched. Also, the loss of perfomance from reverting #1499 should be partially mitigated by improvements in pandas (see https://github.com/pandas-dev/pandas/issues/37954). We would keep all the user facing changes, and all the tests from both PRs. We can then make a release now, and can patch in performance boosts during the release cycle. Do you agree with this assessment? If not, could you propose an alternative?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1583#issuecomment-770167421:5526,message,message,5526,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583#issuecomment-770167421,1,['message'],['message']
Integrability,"t call last); Input In [3], in <cell line: 1>(); ----> 1 adatas = sc.read_10x_h5('GSE164690_RAW/GSM5017021_HN01_PBL/'). File ~/opt/anaconda3/lib/python3.9/site-packages/scanpy/readwrite.py:180, in read_10x_h5(filename, genome, gex_only, backup_url); 178 if not is_present:; 179 logg.debug(f'... did not find original file {filename}'); --> 180 with h5py.File(str(filename), 'r') as f:; 181 v3 = '/matrix' in f; 182 if v3:. File ~/opt/anaconda3/lib/python3.9/site-packages/h5py/_hl/files.py:507, in File.__init__(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, **kwds); 502 fapl = make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,; 503 locking, page_buf_size, min_meta_keep, min_raw_keep, **kwds); 504 fcpl = make_fcpl(track_order=track_order, fs_strategy=fs_strategy,; 505 fs_persist=fs_persist, fs_threshold=fs_threshold,; 506 fs_page_size=fs_page_size); --> 507 fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr); 509 if isinstance(libver, tuple):; 510 self._libver = libver. File ~/opt/anaconda3/lib/python3.9/site-packages/h5py/_hl/files.py:220, in make_fid(name, mode, userblock_size, fapl, fcpl, swmr); 218 if swmr and swmr_support:; 219 flags |= h5f.ACC_SWMR_READ; --> 220 fid = h5f.open(name, flags, fapl=fapl); 221 elif mode == 'r+':; 222 fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl). File h5py/_objects.pyx:54, in h5py._objects.with_phil.wrapper(). File h5py/_objects.pyx:55, in h5py._objects.with_phil.wrapper(). File h5py/h5f.pyx:106, in h5py.h5f.open(). IsADirectoryError: [Errno 21] Unable to open file (file read failed: time = Fri Sep 16 14:17:08 2022; , filename = 'GSE164690_RAW/GSM5017021_HN01_PBL/', file descriptor = 75, errno = 21, error message = 'Is a directory', buf = 0x3072d2728, total read size = 8, bytes this sub-read = 8, bytes actually read = 18446744073709551615, offset = 0); `",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2328:2004,wrap,wrapper,2004,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2328,3,"['message', 'wrap']","['message', 'wrapper']"
Integrability,"t fuzzy_simplicial_set; 388 ; 389 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/__init__.py in <module>; 1 from warnings import warn, catch_warnings, simplefilter; ----> 2 from .umap_ import UMAP; 3 ; 4 try:; 5 with catch_warnings():. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/umap_.py in <module>; 30 import umap.distances as dist; 31 ; ---> 32 import umap.sparse as sparse; 33 ; 34 from umap.utils import (. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/sparse.py in <module>; 10 import numpy as np; 11 ; ---> 12 from umap.utils import norm; 13 ; 14 locale.setlocale(locale.LC_NUMERIC, ""C""). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/utils.py in <module>; 38 ; 39 @numba.njit(""i4(i8[:])""); ---> 40 def tau_rand_int(state):; 41 """"""A fast (pseudo)-random number generator.; 42 . ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/decorators.py in wrapper(func); 219 with typeinfer.register_dispatcher(disp):; 220 for sig in sigs:; --> 221 disp.compile(sig); 222 disp.disable_compile(); 223 return disp. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, sig); 907 with ev.trigger_event(""numba:compile"", data=ev_details):; 908 try:; --> 909 cres = self._compiler.compile(args, return_type); 910 except errors.ForceLiteralArg as e:; 911 def folded(args, kws):. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, args, return_type); 77 ; 78 def compile(self, args, return_type):; ---> 79 status, retval = self._compile_cached(args, return_type); 80 if status:; 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type); 91 ; 92 try:; ---> 93 retval = self._compile_core(args, return_type); 94 except errors.TypingError as e:; 95 self._failed_cache[key] = e. ~/miniconda3/envs/scanpy",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-846931466:2392,wrap,wrapper,2392,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-846931466,2,['wrap'],['wrapper']
Integrability,"t.py"", line 974, in run_command; cmd_obj.run(); File ""/usr/lib/python3.5/distutils/command/build.py"", line 135, in run; self.run_command(cmd_name); File ""/usr/lib/python3.5/distutils/cmd.py"", line 313, in run_command; self.distribution.run_command(command); File ""/usr/lib/python3.5/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/tmp/pip-build-33o4crd7/scanpy/versioneer.py"", line 1559, in run; _build_py.run(self); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 52, in run; self.build_package_data(); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 107, in build_package_data; for package, src_dir, build_dir, filenames in self.data_files:; File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 65, in __getattr__; self.data_files = self._get_data_files(); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 79, in _get_data_files; return list(map(self._get_pkg_data_files, self.packages or ())); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 91, in _get_pkg_data_files; for file in self.find_data_files(package, src_dir); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 98, in find_data_files; + self.package_data.get(package, [])); TypeError: Can't convert 'list' object to str implicitly; ; ----------------------------------------; Command ""/usr/bin/python3 -u -c ""import setuptools, tokenize;__file__='/tmp/pip-build-33o4crd7/scanpy/setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\r\n', '\n'), __file__, 'exec'))"" install --record /tmp/pip-65l8zi0l-record/install-record.txt --single-version-externally-managed --compile"" failed with error code 1 in /tmp/pip-build-33o4crd7/scanpy/; You are using pip version 8.1.1, however version 18.1 is available.; You should consider upgrading via the 'pip install --upgrade pip' command.; ```. Same error message after upgrading pip. Thank you!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/355:11988,message,message,11988,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/355,1,['message'],['message']
Integrability,t: MacOS Ventura 13.4.1. Intel MacBook pro. <details>; <summary> Luke's failing env </summary>. ```; # packages in environment at /Users/luke.zappia/miniconda3/envs/scanpy-dev:; #; # Name Version Build Channel; anndata 0.10.6 pypi_0 pypi; array-api-compat 1.4.1 pypi_0 pypi; asciitree 0.3.3 pypi_0 pypi; attrs 23.2.0 pypi_0 pypi; bzip2 1.0.8 h10d778d_5 conda-forge; ca-certificates 2024.2.2 h8857fd0_0 conda-forge; cfgv 3.4.0 pypi_0 pypi; click 8.1.7 pypi_0 pypi; cloudpickle 3.0.0 pypi_0 pypi; contourpy 1.2.0 pypi_0 pypi; coverage 7.4.4 pypi_0 pypi; cycler 0.12.1 pypi_0 pypi; dask 2024.3.0 pypi_0 pypi; distlib 0.3.8 pypi_0 pypi; execnet 2.1.1 pypi_0 pypi; fasteners 0.19 pypi_0 pypi; filelock 3.13.3 pypi_0 pypi; fonttools 4.49.0 pypi_0 pypi; fsspec 2024.2.0 pypi_0 pypi; h5py 3.10.0 pypi_0 pypi; identify 2.5.35 pypi_0 pypi; igraph 0.11.4 pypi_0 pypi; imageio 2.34.0 pypi_0 pypi; iniconfig 2.0.0 pypi_0 pypi; joblib 1.3.2 pypi_0 pypi; kiwisolver 1.4.5 pypi_0 pypi; lazy-loader 0.3 pypi_0 pypi; legacy-api-wrap 1.4 pypi_0 pypi; leidenalg 0.10.2 pypi_0 pypi; libexpat 2.6.2 h73e2aa4_0 conda-forge; libffi 3.4.2 h0d85af4_5 conda-forge; libsqlite 3.45.2 h92b6c6a_0 conda-forge; libzlib 1.2.13 h8a1eda9_5 conda-forge; llvmlite 0.42.0 pypi_0 pypi; locket 1.0.0 pypi_0 pypi; matplotlib 3.8.3 pypi_0 pypi; natsort 8.4.0 pypi_0 pypi; ncurses 6.4 h93d8f39_2 conda-forge; networkx 3.2.1 pypi_0 pypi; nodeenv 1.8.0 pypi_0 pypi; numba 0.59.0 pypi_0 pypi; numcodecs 0.12.1 pypi_0 pypi; numpy 1.26.4 pypi_0 pypi; openssl 3.2.1 hd75f5a5_0 conda-forge; packaging 24.0 pypi_0 pypi; pandas 2.2.1 pypi_0 pypi; partd 1.4.1 pypi_0 pypi; patsy 0.5.6 pypi_0 pypi; pbr 6.0.0 pypi_0 pypi; pillow 10.2.0 pypi_0 pypi; pip 24.0 pyhd8ed1ab_0 conda-forge; platformdirs 4.2.0 pypi_0 pypi; pluggy 1.4.0 pypi_0 pypi; pre-commit 3.7.0 pypi_0 pypi; profimp 0.1.0 pypi_0 pypi; pynndescent 0.5.11 pypi_0 pypi; pyparsing 3.1.2 pypi_0 pypi; pytest 8.1.1 pypi_0 pypi; pytest-cov 4.1.0 pypi_0 pypi; pytest-mock 3.12.0 pypi_0 pypi; pytest-,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2993:33147,wrap,wrap,33147,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2993,1,['wrap'],['wrap']
Integrability,"t; normalizing counts per cell. C:\ProgramData\Anaconda3\envs\dl\lib\site-packages\scanpy\preprocessing\_normalization.py:233: UserWarning: Some cells have zero counts; warn(UserWarning(""Some cells have zero counts"")). finished (0:00:00); WARNING: adata.X seems to be already log-transformed.; extracting highly variable genes. C:\ProgramData\Anaconda3\envs\dl\lib\site-packages\scanpy\preprocessing\_simple.py:377: RuntimeWarning: invalid value encountered in log1p; np.log1p(X, out=X). ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); Cell In[59], line 2; 1 print('Begin of post doublets removal and QC plot'); ----> 2 sc.pp.scrublet(alldata, n_neighbors=10); 3 alldata = alldata[alldata.obs['predicted_doublet']==False, :].copy(); 4 n1 = alldata.shape[0]. File C:\ProgramData\Anaconda3\envs\dl\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File C:\ProgramData\Anaconda3\envs\dl\lib\site-packages\scanpy\preprocessing\_scrublet\__init__.py:282, in scrublet(adata, adata_sim, batch_key, sim_doublet_ratio, expected_doublet_rate, stdev_doublet_rate, synthetic_doublet_umi_subsampling, knn_dist_metric, normalize_variance, log_transform, mean_center, n_prin_comps, use_approx_neighbors, get_doublet_neighbor_parents, n_neighbors, threshold, verbose, copy, random_state); 279 adata.uns[""scrublet""][""batched_by""] = batch_key; 281 else:; --> 282 scrubbed = _run_scrublet(adata_obs, adata_sim); 284 # Copy outcomes to input object from our processed version; 286 adata.obs[""doublet_score""] = scrubbed[""obs""][""doublet_score""]. File C:\ProgramData\Anaconda3\envs\dl\lib\site-packages\scanpy\preprocessing\_scrub",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3070:7183,wrap,wraps,7183,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3070,1,['wrap'],['wraps']
Integrability,"t__.py:80: in fn_compatible; return fn(*args_all, **kw); scanpy/preprocessing/_highly_variable_genes.py:651: in highly_variable_genes; df = _highly_variable_genes_single_batch(; scanpy/preprocessing/_highly_variable_genes.py:288: in _highly_variable_genes_single_batch; df[""highly_variable""] = _subset_genes(; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . adata = AnnData object with n_obs × n_vars = 700 × 765; obs: 'bulk_labels', 'n_genes', 'percent_mito', 'n_counts', 'S_score...'pca', 'rank_genes_groups', 'log1p'; obsm: 'X_pca', 'X_umap'; varm: 'PCs'; obsp: 'distances', 'connectivities'. def _subset_genes(; adata: AnnData,; *,; mean: NDArray[np.float64] | DaskArray,; dispersion_norm: NDArray[np.float64] | DaskArray,; cutoff: _Cutoffs | int,; ) -> NDArray[np.bool_] | DaskArray:; """"""Get boolean mask of genes with normalized dispersion in bounds.""""""; if isinstance(cutoff, _Cutoffs):; > dispersion_norm[np.isnan(dispersion_norm)] = 0 # similar to Seurat; E ValueError: assignment destination is read-only. scanpy/preprocessing/_highly_variable_genes.py:365: ValueError; ```. </details>. Dependencies are different, looks like a dask update and a pyarrow added dep. I suspect this has to do with the new dask-expr. ----. I can replicate locally by install the new dask, dask-expr, and pyarrow. ----. Importing dask.dataframe changes the settings for pandas somehow:. ```python; In [1]: import pandas as pd. In [2]: pd.DataFrame({""a"": [1,2,3, None]})[""a""].to_numpy().flags; Out[2]: ; C_CONTIGUOUS : True; F_CONTIGUOUS : True; OWNDATA : False; WRITEABLE : True; ALIGNED : True; WRITEBACKIFCOPY : False. In [3]: import dask.dataframe as ddf. In [4]: pd.DataFrame({""a"": [1,2,3, None]})[""a""].to_numpy().flags; Out[4]: ; C_CONTIGUOUS : True; F_CONTIGUOUS : True; OWNDATA : False; WRITEABLE : False; ALIGNED : True; WRITEBACKIFCOPY : False; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2902:6192,Depend,Dependencies,6192,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2902,1,['Depend'],['Dependencies']
Integrability,"t_is_constant_dask[csr_matrix-0] --capture=no; Numba function called from a non-threadsafe context. Try installing `tbb`.; Numba function called from a non-threadsafe context. Try installing `tbb`. Numba workqueue threading layer is terminating: Concurrent access has been detected. - The workqueue threading layer is not threadsafe and may not be accessed concurrently by multiple threads. Concurrent access typically occurs through a nested parallel region launch or by calling Numba parallel=True functions from multiple Python threads.; - Try using the TBB threading layer as an alternative, as it is, itself, threadsafe. Docs: https://numba.readthedocs.io/en/stable/user/threading-layer.html. Fatal Python error: Aborted. Thread 0x000000016fd2f000 (most recent call first):; File ""~/Dev/scanpy/src/scanpy/_compat.py"", line 133 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 109 in _; File ""<venv>/lib/python3.12/functools.py"", line 909 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 30 in func; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 157 in get; File ""<venv>/lib/python3.12/site-packages/dask/optimization.py"", line 1001 in __call__; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 225 in execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 239 in batch_execute_tasks; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 64 in run; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 92 in _worker; File ""<venv>/lib/python3.12/threading.py"", line 1010 in run; File ""<venv>/lib/python3.12/threading.py"", line 1073 in _bootstrap_inner; File ""<venv>/lib/python3.12/threading.py"", line 1030 in _bootstrap. Thread 0x000000016ed23000 (most recent call first):; File ""<venv>/lib/pytho",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478:1347,wrap,wrapper,1347,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478,2,['wrap'],['wrapper']
Integrability,"ta_dist = AnnData object with n_obs × n_vars = 10000 × 1000; var: 'gene_ids'; uns: 'dist-mode'. def test_normalize_total(adata: AnnData, adata_dist: AnnData):; > normalize_total(adata_dist). scanpy/tests/test_preprocessing_distributed.py:93: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; /home/phil/.local/share/hatch/env/virtual/scanpy/q4In3tK-/hatch-test.stable/lib/python3.12/site-packages/legacy_api_wrap/__init__.py:80: in fn_compatible; return fn(*args_all, **kw); scanpy/preprocessing/_normalization.py:240: in normalize_total; adata, _normalize_data(X, counts_per_cell, target_sum), layer=layer; scanpy/preprocessing/_normalization.py:49: in _normalize_data; return axis_mul_or_truediv(; /usr/lib/python3.12/functools.py:909: in wrapper; return dispatch(args[0].__class__)(*args, **kw); _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . X = <zappy.direct.array.DirectZappyArray object at 0x7f06eb607a40>, scaling_array = array([0.9270073, 0.4379562, 1.5985402, ..., 0.8540146, 1.5109489,; 0.8978102], dtype=float32), axis = 0, op = <built-in function truediv>. @singledispatch; def axis_mul_or_truediv(; X: sparse.spmatrix,; scaling_array,; axis: Literal[0, 1],; op: Callable[[Any, Any], Any],; *,; allow_divide_by_zero: bool = True,; out: sparse.spmatrix | None = None,; ) -> sparse.spmatrix:; check_op(op); if out is not None:; if X.data is not out.data:; raise ValueError(; ""`out` argument provided but not equal to X. This behavior is not supported for sparse matrix scaling.""; ); if not al",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3087:1993,wrap,wrapper,1993,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3087,1,['wrap'],['wrapper']
Integrability,"tall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right?. The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for 1-3 built-in commands. Other people are interested in creating those scripts (and did so already, but for the time being just call `scanpy-mycommand` with a dash in there). > I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. […] Generally, I think there should be a longer planning discussion about how configuration works. Agreed, probably in an extra issue. > I'm wondering if we couldn't cut down on the need to explain by adopting a convention of referencing relevant settings in any function that access them? For example, the docs for expression_atlas would have a reference to dataset_dir?. sounds great!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-478230940:1753,interface,interface,1753,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-478230940,2,['interface'],['interface']
Integrability,"taset_kwargs=dataset_kwargs); 287 ; 288 . /usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/functools.py in wrapper(*args, **kw); 838 '1 positional argument'); 839 ; --> 840 return dispatch(args[0].__class__)(*args, **kw); 841 ; 842 funcname = getattr(func, '__name__', 'singledispatch function'). ~/github/anndata/anndata/_io/h5ad.py in write_attribute_h5ad(f, key, value, *args, **kwargs); 124 if key in f:; 125 del f[key]; --> 126 _write_method(type(value))(f, key, value, *args, **kwargs); 127 ; 128 . ~/github/anndata/anndata/_io/h5ad.py in write_mapping(f, key, value, dataset_kwargs); 284 def write_mapping(f, key, value, dataset_kwargs=MappingProxyType({})):; 285 for sub_key, sub_value in value.items():; --> 286 write_attribute(f, f""{key}/{sub_key}"", sub_value, dataset_kwargs=dataset_kwargs); 287 ; 288 . /usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/functools.py in wrapper(*args, **kw); 838 '1 positional argument'); 839 ; --> 840 return dispatch(args[0].__class__)(*args, **kw); 841 ; 842 funcname = getattr(func, '__name__', 'singledispatch function'). ~/github/anndata/anndata/_io/h5ad.py in write_attribute_h5ad(f, key, value, *args, **kwargs); 124 if key in f:; 125 del f[key]; --> 126 _write_method(type(value))(f, key, value, *args, **kwargs); 127 ; 128 . ~/github/anndata/anndata/_io/h5ad.py in write_mapping(f, key, value, dataset_kwargs); 284 def write_mapping(f, key, value, dataset_kwargs=MappingProxyType({})):; 285 for sub_key, sub_value in value.items():; --> 286 write_attribute(f, f""{key}/{sub_key}"", sub_value, dataset_kwargs=dataset_kwargs); 287 ; 288 . /usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/functools.py in wrapper(*args, **kw); 838 '1 positional argument'); 839 ; --> 840 return dispatch(args[0].__class__)(*args, **kw); 841 ; 842 funcname = getattr(func, '__name__', 'singledispatch function'). ~/github/anndata/anndata/_io/h5ad.py in ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1131:4032,wrap,wrapper,4032,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1131,1,['wrap'],['wrapper']
Integrability,"tblib 1.7.0 py_0 ; terminado 0.9.4 py38h06a4308_0 ; testpath 0.4.4 pyhd3eb1b0_0 ; textdistance 4.2.1 pyhd3eb1b0_0 ; threadpoolctl 2.1.0 pyh5ca1d4c_0 ; three-merge 0.1.1 pyhd3eb1b0_0 ; tk 8.6.10 hbc83047_0 ; tktable 2.10 h14c3975_0 ; tokenize-rt 4.1.0 pyhd8ed1ab_0 conda-forge; toml 0.10.2 pyhd3eb1b0_0 ; toolz 0.11.1 pyhd3eb1b0_0 ; tornado 6.1 py38h27cfd23_0 ; tqdm 4.59.0 pyhd3eb1b0_1 ; traitlets 5.0.5 pyhd3eb1b0_0 ; triku 1.3.1 pypi_0 pypi; tslearn 0.5.0.5 pypi_0 pypi; typed-ast 1.4.2 py38h27cfd23_1 ; typing 3.10.0.0 py38h06a4308_0 ; typing-extensions 3.10.0.0 pypi_0 pypi; typing_extensions 3.7.4.3 pyha847dfd_0 ; tzlocal 2.1 py38_0 ; ujson 4.0.2 py38h2531618_0 ; umap-learn 0.5.1 py38h578d9bd_0 conda-forge; unicodecsv 0.14.1 py38_0 ; unixodbc 2.3.9 h7b6447c_0 ; urllib3 1.26.4 pyhd3eb1b0_0 ; vendorize 0.2.1 pypi_0 pypi; watchdog 1.0.2 py38h06a4308_1 ; wcwidth 0.2.5 py_0 ; webencodings 0.5.1 py38_1 ; werkzeug 1.0.1 pyhd3eb1b0_0 ; wheel 0.36.2 pyhd3eb1b0_0 ; widgetsnbextension 3.5.1 py38_0 ; wrapt 1.12.1 py38h7b6447c_1 ; wurlitzer 2.1.0 py38h06a4308_0 ; xlrd 1.2.0 pypi_0 pypi; xlsxwriter 1.3.8 pyhd3eb1b0_0 ; xlwt 1.3.0 py38_0 ; xz 5.2.5 h7b6447c_0 ; yaml 0.2.5 h7b6447c_0 ; yapf 0.31.0 pyhd3eb1b0_0 ; yarl 1.6.3 pypi_0 pypi; zeromq 4.3.4 h2531618_0 ; zict 2.0.0 pyhd3eb1b0_0 ; zipp 3.4.1 pyhd3eb1b0_0 ; zlib 1.2.11 h7b6447c_3 ; zope 1.0 py38_1 ; zope.event 4.5.0 py38_0 ; zope.interface 5.3.0 py38h27cfd23_0 ; zstd 1.4.5 h9ceee32_0 ; ```. </Details>. <Details>; <Summary>ImportError traceback</Summary>. ```python; ---------------------------------------------------------------------------; ImportError Traceback (most recent call last); <ipython-input-2-0074c9bc0b31> in <module>; ----> 1 import scanpy as sc. ~/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/scanpy/__init__.py in <module>; 3 from ._metadata import __version__, __author__, __email__; 4 ; ----> 5 from ._utils import check_versions; 6 ; 7 check_versions(). ~/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:18220,wrap,wrapt,18220,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310,2,['wrap'],['wrapt']
Integrability,"te_h5ad(""/Users/julius/Desktop/zf_48.h5ad""); ```; Here is the error:; ```pytb; RuntimeError Traceback (most recent call last); ~/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, key, val, *args, **kwargs); 208 try:; --> 209 return func(elem, key, val, *args, **kwargs); 210 except Exception as e:. ~/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_array(f, key, value, dataset_kwargs); 184 value = _to_hdf5_vlen_strings(value); --> 185 f.create_dataset(key, data=value, **dataset_kwargs); 186 . ~/opt/anaconda3/lib/python3.8/site-packages/h5py/_hl/group.py in create_dataset(self, name, shape, dtype, data, **kwds); 138 if name is not None:; --> 139 self[name] = dset; 140 return dset. ~/opt/anaconda3/lib/python3.8/site-packages/h5py/_hl/group.py in __setitem__(self, name, obj); 372 if isinstance(obj, HLObject):; --> 373 h5o.link(obj.id, self.id, name, lcpl=lcpl, lapl=self._lapl); 374 . h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/h5o.pyx in h5py.h5o.link(). RuntimeError: Unable to create link (name already exists). The above exception was the direct cause of the following exception:. RuntimeError Traceback (most recent call last); ~/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, key, val, *args, **kwargs); 208 try:; --> 209 return func(elem, key, val, *args, **kwargs); 210 except Exception as e:. ~/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_series(group, key, series, dataset_kwargs); 283 write_array(group, category_key, categories, dataset_kwargs=dataset_kwargs); --> 284 write_array(group, key, codes, dataset_kwargs=dataset_kwargs); 285 . ~/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, key, val, *args, **kwargs); 211 parent = _get_parent(elem); --> 212 raise type(e)(; 213 f""{e}\n\n"". RuntimeError: Unable to create link (name already exists). ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1982:2031,wrap,wrapper,2031,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1982,1,['wrap'],['wrapper']
Integrability,"te_vlen_string_array; f.create_dataset(k, data=elem.astype(str_dtype), dtype=str_dtype, **dataset_kwargs); File ""/home/joyzheng/.local/lib/python3.8/site-packages/h5py/_hl/group.py"", line 183, in create_dataset; dsid = dataset.make_new_dset(group, shape, dtype, data, name, **kwds); File ""/home/joyzheng/.local/lib/python3.8/site-packages/h5py/_hl/dataset.py"", line 168, in make_new_dset; dset_id.write(h5s.ALL, h5s.ALL, data); File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper; File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper; File ""h5py/h5d.pyx"", line 280, in h5py.h5d.DatasetID.write; File ""h5py/_proxy.pyx"", line 145, in h5py._proxy.dset_rw; File ""h5py/_conv.pyx"", line 444, in h5py._conv.str2vlen; File ""h5py/_conv.pyx"", line 95, in h5py._conv.generic_converter; File ""h5py/_conv.pyx"", line 249, in h5py._conv.conv_str2vlen; TypeError: Can't implicitly convert non-string objects to strings. The above exception was the direct cause of the following exception:; Traceback (most recent call last):; File ""integration.py"", line 66, in <module>; adata.write_h5ad('Integrated.h5ad'); File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_core/anndata.py"", line 1918, in write_h5ad; _write_h5ad(; File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_io/h5ad.py"", line 98, in write_h5ad; write_elem(f, ""obs"", adata.obs, dataset_kwargs=dataset_kwargs); File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_io/utils.py"", line 214, in func_wrapper; return func(elem, key, val, *args, **kwargs); File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_io/specs/registry.py"", line 175, in write_elem; _REGISTRY.get_writer(dest_type, t, modifiers)(f, k, elem, *args, **kwargs); File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_io/specs/registry.py"", line 24, in wrapper; result = func(g, k, *args, **kwargs); File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_io/specs/methods.py"", line 514, ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2432:2991,integrat,integration,2991,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2432,1,['integrat'],['integration']
Integrability,"this PR fixes #2744 and. - moves `_pca_with_sparse` behind a check for scipy <1.4, which has @ivirshup’s port of that code https://github.com/scikit-learn/scikit-learn/pull/18689; - simplifies our logic around which parameters lead to which dispatch. this makes it useful to get this in before #3263. - throws a warning when people use the `lobpcg` solver, since the closure of https://github.com/scikit-learn/scikit-learn/issues/12794#issuecomment-2118064158 makes it unlikely that we can count on that getting in any time soon. I filed https://github.com/scikit-learn/scikit-learn/pull/30075. Depending on how that PR is received, we can update the warning here: either they like it, then we can remove the warning (we remove our legacy code once we depend on a scipy version that has lobpcg upstream), or they don’t, then we leave the warning for now and remove lobpcg support in the future.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3267:595,Depend,Depending,595,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3267,2,"['Depend', 'depend']","['Depending', 'depend']"
Integrability,"thon3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type); 315 raise e; 316 else:; --> 317 reraise(type(e), e, None); 318 ; 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb); 656 value = tp(); 657 if value.__traceback__ is not tb:; --> 658 raise value.with_traceback(tb); 659 raise value; 660 . TypingError: Failed in nopython mode pipeline (step: nopython frontend); Invalid use of type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)); Known signatures:; * (array(float32, 1d, A), array(float32, 1d, A)) -> float32; * parameterized; [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)); [2] During: typing of call at /opt/conda/lib/python3.7/site-packages/umap/umap_.py (776). File ""../../../opt/conda/lib/python3.7/site-packages/umap/umap_.py"", line 776:; def optimize_layout(; <source elided>. dist_squared = rdist(current, other); ^. This is not usually a problem with Numba itself but instead often caused by; the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:; http://numba.pydata.org/numba-doc/dev/reference/pysupported.html; and; http://numba.pydata.org/numba-doc/dev/reference/numpysupported.html. For more information about typing errors and how to debug them visit:; http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message; and traceback, along with a minimal reproducer at:; https://github.com/numba/numba/issues/new; ```. What I basically do from raw UMI counts:. 1. total counts normalization / logarithmization; 2. PCA, bbknn, louvain; 3. combat, HVG, PCA, UMAP (works well); 4. Paga (with louvain from 2., works well); 5. UMAP (with positions from 4., does not work). Any idea? Any further info needed?; Best,; Jens",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/666:3286,message,message,3286,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666,1,['message'],['message']
Integrability,"thub.com/user-attachments/assets/a3ee8f51-833d-4adb-ab9f-f6ff5b19387f). I have changed the *genes.tsv.gz file's name to *features.tsv.gz but still got the same error. Here is the full error log:; ```; ---------------------------------------------------------------------------; FileNotFoundError Traceback (most recent call last); Cell In[62], [line 1](vscode-notebook-cell:?execution_count=62&line=1); ----> [1](vscode-notebook-cell:?execution_count=62&line=1) data1 = sc.read_10x_mtx(""GSE212966"", prefix=""GSM6567159_PDAC2_"", var_names='gene_symbols', cache=True); [2](vscode-notebook-cell:?execution_count=62&line=2) data1.var_names_make_unique(). File ~\AppData\Roaming\Python\Python312\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); [77](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/legacy_api_wrap/__init__.py:77) @wraps(fn); [78](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/legacy_api_wrap/__init__.py:78) def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; [79](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/legacy_api_wrap/__init__.py:79) if len(args_all) <= n_positional:; ---> [80](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/legacy_api_wrap/__init__.py:80) return fn(*args_all, **kw); [82](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args; [83](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:1671,wrap,wraps,1671,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['wrap'],['wraps']
Integrability,"tically. However, I'm wary of abandoning a critical discussion of imputation methods in this space because other portions of the typical workflow have issues as well. Further, I think there are important distinctions to be made between different classes of methodology that are (mis)used in this problem space. I. Methods that are fundamentally flawed by their assumptions or algorithm. These should obviously be avoided.; II. Methods that are fundamentally sound but are not sufficiently validated, e.g. the validation doesn't exist in this problem space, isn't sufficiently comprehensive/relevant, performs poorly against other fundamentally sound methodologies, or has such restrictive assumptions it isn't broadly useful/applicable.; III. Methods that are fundamentally sound in assumption/algorithm and can be used by a competent practitioner but still have the potential to be abused through applying it to data that violate those assumptions. I'd consider t-SNE and a great deal of the clustering algorithms to be in class III for the reasons you said; they're valid, functional tools but can be applied in assumption-violating or quasi-valid ways. I'm pretty sure that scImpute, for example, belongs in class I because its description of dropout and simulated test cases are inappropriate. I'd put MAGIC and several other currently available imputation methods in class II as they've got strong foundations but currently insufficient validation IMO. I'm not trying to pick on MAGIC or any specific imputation method. Instead I'd like to have an open discussion about the benefits, limitations, and relative performance of the various imputation methods available with the goal leading to something like @gokceneraslan suggested. Well, and since you brought it up, batch correction and multimodal integration methods are in definite need of the same open discussion, which I'd be happy to have, and I think they should have the same disclaimer regarding their limitations in the documentation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/189#issuecomment-417692893:1980,integrat,integration,1980,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189#issuecomment-417692893,1,['integrat'],['integration']
Integrability,"ties_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity); 353 # umap 0.5.0; 354 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""); --> 355 from umap.umap_ import fuzzy_simplicial_set; 356 ; 357 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/__init__.py in <module>; ----> 1 from .umap_ import UMAP; 2 ; 3 # Workaround: https://github.com/numba/numba/issues/3341; 4 import numba; 5 . ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/umap_.py in <module>; 52 from umap.spectral import spectral_layout; 53 from umap.utils import deheap_sort, submatrix; ---> 54 from umap.layouts import (; 55 optimize_layout_euclidean,; 56 optimize_layout_generic,. ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py in <module>; 37 },; 38 ); ---> 39 def rdist(x, y):; 40 """"""Reduced Euclidean distance.; 41 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func); 219 with typeinfer.register_dispatcher(disp):; 220 for sig in sigs:; --> 221 disp.compile(sig); 222 disp.disable_compile(); 223 return disp. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig); 907 with ev.trigger_event(""numba:compile"", data=ev_details):; 908 try:; --> 909 cres = self._compiler.compile(args, return_type); 910 except errors.ForceLiteralArg as e:; 911 def folded(args, kws):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type); 77 ; 78 def compile(self, args, return_type):; ---> 79 status, retval = self._compile_cached(args, return_type); 80 if status:; 81 return retval. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type); 91 ; 92 try:; ---> 93 retval = self._compile_core(args, return_type); 94 except errors.TypingError as e:; 95 self._failed_cache[key] = e. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796:3583,wrap,wrapper,3583,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796,2,['wrap'],['wrapper']
Integrability,"too; long as is was supposed to reduce the plotting time. I would not wait for; more than 5 minutes to see a plot. How many genes were you planning to plot?. The background is that when plotting a heatmap, the matplotlib; visualization will randomly drop genes because the resolution of the; screens is not high enough. Thus, when the number of genes is large, I was; trying to find a compromise by fitting a line before the plotting and then; only plotting the fit. Can you help me solve the issue by sharing the data with me? Or, can you; find an example that can reproduce the problem?. On Tue, May 7, 2019 at 10:19 AM brianpenghe <notifications@github.com>; wrote:. > I was trying to plot a heatmap using this command:; > ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident',; > use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0,; > dendrogram=True, save='ClusterMap.png'); >; > And it didn't finish running after an overnight, with the following; > warning message:; > WARNING: Gene labels are not shown when more than 50 genes are visualized.; > To show gene labels set show_gene_labels=True; > /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227:; > UserWarning:; > The maximal number of iterations maxit (set to 20 by the program); > allowed for finding a smoothing spline with fp=s has been reached: s; > too small.; > There is an approximation returned but the corresponding weighted sum; > of squared residuals does not satisfy the condition abs(fp-s)/s < tol.; > warnings.warn(message); >; > I don't understand why this is taking this long because seaborn was able; > to finish plotting within 30 minutes. Do you know why?; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/633>, or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABF37VNDX37RZL256MWKDM3PUE3RFANCNFSM4HLGOYGA>; > .;",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/633#issuecomment-490792870:1072,message,message,1072,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633#issuecomment-490792870,1,['message'],['message']
Integrability,"try.py"", line 183, in read_elem; return _REGISTRY.get_reader(type(elem), get_spec(elem), frozenset(modifiers))(elem); File ""[python-path\site-packages\anndata\_io\specs\methods.py"", line 92, in read_basic; return {k: read_elem(v) for k, v in elem.items()}; File ""[python-path]\site-packages\anndata\_io\specs\methods.py"", line 92, in <dictcomp>; return {k: read_elem(v) for k, v in elem.items()}; File ""[python-path]\site-packages\anndata\_io\specs\registry.py"", line 183, in read_elem; return _REGISTRY.get_reader(type(elem), get_spec(elem), frozenset(modifiers))(elem); File ""[python-path]\site-packages\anndata\_io\specs\methods.py"", line 475, in read_sparse; return SparseDataset(elem).to_memory(); File ""[python-path]\site-packages\anndata\_core\sparse_dataset.py"", line 380, in to_memory; mtx.indices = self.group[""indices""][...]; File ""h5py\_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper; File ""h5py\_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper; File ""[python-path]\site-packages\h5py\_hl\dataset.py"", line 741, in __getitem__; return self._fast_reader.read(args); File ""h5py\_selector.pyx"", line 362, in h5py._selector.Reader.read; File ""h5py\_selector.pyx"", line 336, in h5py._selector.Reader.make_array; numpy.core._exceptions._ArrayMemoryError: Unable to allocate 56.8 GiB for an array with shape (7621750342,) and data type int64. Process finished with exit code 1. ```. #### Versions. <details>. -----; anndata 0.8.0; scanpy 1.9.1; -----; PIL 9.2.0; beta_ufunc NA; binom_ufunc NA; colorama 0.4.5; console_thrift NA; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; h5py 3.7.0; hypergeom_ufunc NA; joblib 1.2.0; kiwisolver 1.4.4; llvmlite 0.39.1; matplotlib 3.6.0; mpl_toolkits NA; natsort 8.2.0; nbinom_ufunc NA; nt NA; numba 0.56.2; numpy 1.22.3; packaging 21.3; pandas 1.4.1; pkg_resources NA; pydev_console NA; pydev_ipython NA; pydevconsole NA; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pyparsing 3.0.7; pytz 2022.1; scipy 1.8.0; session_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2365:3574,wrap,wrapper,3574,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2365,1,['wrap'],['wrapper']
Integrability,"types.append(self.typeof_pyval(a)); 366 try:; --> 367 return self.compile(tuple(argtypes)); 368 except errors.ForceLiteralArg as e:; 369 # Received request for compiler re-entry with the list of arguments. /usr/local/lib/python3.7/dist-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs); 30 def _acquire_compile_lock(*args, **kwargs):; 31 with self:; ---> 32 return func(*args, **kwargs); 33 return _acquire_compile_lock; 34 . /usr/local/lib/python3.7/dist-packages/numba/core/dispatcher.py in compile(self, sig); 823 raise e.bind_fold_arguments(folded); 824 self.add_overload(cres); --> 825 self._cache.save_overload(sig, cres); 826 return cres.entry_point; 827 . /usr/local/lib/python3.7/dist-packages/numba/core/caching.py in save_overload(self, sig, data); 669 """"""; 670 with self._guard_against_spurious_io_errors():; --> 671 self._save_overload(sig, data); 672 ; 673 def _save_overload(self, sig, data):. /usr/local/lib/python3.7/dist-packages/numba/core/caching.py in _save_overload(self, sig, data); 679 key = self._index_key(sig, _get_codegen(data)); 680 data = self._impl.reduce(data); --> 681 self._cache_file.save(key, data); 682 ; 683 @contextlib.contextmanager. /usr/local/lib/python3.7/dist-packages/numba/core/caching.py in save(self, key, data); 494 break; 495 overloads[key] = data_name; --> 496 self._save_index(overloads); 497 self._save_data(data_name, data); 498 . /usr/local/lib/python3.7/dist-packages/numba/core/caching.py in _save_index(self, overloads); 540 def _save_index(self, overloads):; 541 data = self._source_stamp, overloads; --> 542 data = self._dump(data); 543 with self._open_for_write(self._index_path) as f:; 544 pickle.dump(self._version, f, protocol=-1). /usr/local/lib/python3.7/dist-packages/numba/core/caching.py in _dump(self, obj); 568 ; 569 def _dump(self, obj):; --> 570 return pickle.dumps(obj, protocol=-1); 571 ; 572 @contextlib.contextmanager. TypeError: can't pickle weakref objects. How to solve this problem? Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1951:4772,protocol,protocol,4772,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1951,2,['protocol'],['protocol']
Integrability,"uild_update; len(to_build)); File ""/home/docs/checkouts/readthedocs.org/user_builds/icb-scanpy/envs/1204/lib/python3.6/site-packages/sphinx/builders/__init__.py"", line 311, in build; updated_docnames = set(self.read()); File ""/home/docs/.pyenv/versions/3.6.8/lib/python3.6/contextlib.py"", line 88, in __exit__; next(self.gen); File ""/home/docs/checkouts/readthedocs.org/user_builds/icb-scanpy/envs/1204/lib/python3.6/site-packages/sphinx/util/logging.py"", line 213, in pending_warnings; memhandler.flushTo(logger); File ""/home/docs/checkouts/readthedocs.org/user_builds/icb-scanpy/envs/1204/lib/python3.6/site-packages/sphinx/util/logging.py"", line 178, in flushTo; logger.handle(record); File ""/home/docs/.pyenv/versions/3.6.8/lib/python3.6/logging/__init__.py"", line 1454, in handle; self.callHandlers(record); File ""/home/docs/.pyenv/versions/3.6.8/lib/python3.6/logging/__init__.py"", line 1516, in callHandlers; hdlr.handle(record); File ""/home/docs/.pyenv/versions/3.6.8/lib/python3.6/logging/__init__.py"", line 861, in handle; rv = self.filter(record); File ""/home/docs/.pyenv/versions/3.6.8/lib/python3.6/logging/__init__.py"", line 720, in filter; result = f.filter(record); File ""/home/docs/checkouts/readthedocs.org/user_builds/icb-scanpy/envs/1204/lib/python3.6/site-packages/sphinx/util/logging.py"", line 415, in filter; raise SphinxWarning(location + "":"" + str(message)); sphinx.errors.SphinxWarning: /home/docs/checkouts/readthedocs.org/user_builds/icb-scanpy/checkouts/1204/scanpy/plotting/_dotplot.py:docstring of scanpy.pl.dotplot:122:Inline strong start-string without end-string. Warning, treated as error:; /home/docs/checkouts/readthedocs.org/user_builds/icb-scanpy/checkouts/1204/scanpy/plotting/_dotplot.py:docstring of scanpy.pl.dotplot:122:Inline strong start-string without end-string.; ```. </details>. I get the same error locally from just running `make html`, and was able to get rid of it by adding a page for the `DotPlot` class (though more errors came up after that).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1307:2947,message,message,2947,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1307,1,['message'],['message']
Integrability,"ule>; ----> 1 sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ~/Documents/Python/scanpy/scanpy/plotting/_tools/paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax); 1057 if as_heatmap:; 1058 img = ax.imshow(; -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map; 1060 ); 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs); 1599 def inner(ax, *args, data=None, **kwargs):; 1600 if data is None:; -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs); 1602 ; 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs); 367 f""%(removal)s. If any parameter follows {name!r}, they ""; 368 f""should be pass as keyword, not positionally.""); --> 369 return func(*args, **kwargs); 370 ; 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs); 367 f""%(removal)s. If any parameter follows {name!r}, they ""; 368 f""should be pass as keyword, not positionally.""); --> 369 return func(*args, **kwargs); 370 ; 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs); 5669 resample=resample, **kwargs); 5670 ; -> 5671 im.set_data(X); 5672 im.set_alpha(alpha); 5673 if im.get_clip_path() is None:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/image.py in set_data(self, A); 683 not np.can_cast(self._A.dtype, float, ""same_kind"")):; 684 raise TypeError(""Image data of dtype {} ca",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/953:4271,wrap,wrapper,4271,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953,1,['wrap'],['wrapper']
Integrability,"ule>; ----> 1 sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ~/Documents/Python/scanpy/scanpy/plotting/_tools/paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax); 1057 if as_heatmap:; 1058 img = ax.imshow(; -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map; 1060 ); 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs); 1599 def inner(ax, *args, data=None, **kwargs):; 1600 if data is None:; -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs); 1602 ; 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs); 367 f""%(removal)s. If any parameter follows {name!r}, they ""; 368 f""should be pass as keyword, not positionally.""); --> 369 return func(*args, **kwargs); 370 ; 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs); 367 f""%(removal)s. If any parameter follows {name!r}, they ""; 368 f""should be pass as keyword, not positionally.""); --> 369 return func(*args, **kwargs); 370 ; 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs); 5669 resample=resample, **kwargs); 5670 ; -> 5671 im.set_data(X); 5672 im.set_alpha(alpha); 5673 if im.get_clip_path() is None:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/image.py in set_data(self, A); 688 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):; 689 raise TypeError(""Invalid shape {} for im",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/953:1756,wrap,wrapper,1756,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953,1,['wrap'],['wrapper']
Integrability,"umap expects a list as group, so it will work if you do:. ```python; sc.pl.umap(adata, color='blobs', groups=['Zero']); ````. the improvement that I would consider is to automatically convert a string into a list to avoid the error message.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/231#issuecomment-414236960:232,message,message,232,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/231#issuecomment-414236960,1,['message'],['message']
Integrability,"unning highly variable genes; ```python; <details>. ---------------------------------------------------------------------------; ModuleNotFoundError Traceback (most recent call last); File /opt/miniconda3/envs/scanpyenvt/lib/python3.12/site-packages/scanpy/preprocessing/_highly_variable_genes.py:66, in _highly_variable_genes_seurat_v3(adata, flavor, layer, n_top_genes, batch_key, check_values, span, subset, inplace); 65 try:; ---> 66 from skmisc.loess import loess; 67 except ImportError:. ModuleNotFoundError: No module named 'skmisc'. During handling of the above exception, another exception occurred:. ImportError Traceback (most recent call last); Cell In[14], line 1; ----> 1 doublet_training_data = sc.pp.highly_variable_genes(adata, n_top_genes=6000, subset=True, flavor='seurat_v3'); 2 doublet_training_data. File /opt/miniconda3/envs/scanpyenvt/lib/python3.12/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /opt/miniconda3/envs/scanpyenvt/lib/python3.12/site-packages/scanpy/preprocessing/_highly_variable_genes.py:655, in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key, check_values); 653 sig = signature(_highly_variable_genes_seurat_v3); 654 n_top_genes = cast(int, sig.parameters[""n_top_genes""].default); --> 655 return _highly_variable_genes_seurat_v3(; 656 adata,; 657 flavor=flavor,; 658 layer=layer,; 659 n_top_genes=n_top_genes,; 660 batch_key=batch_key,; 661 check_values=check_values,; 662 span=span,; 663 subset=subset,; 664 inplace=inplace,; 665 ); 667 cutoff = _Cutoffs.validate(; 668 n_top_genes=n_top_genes,; 669 min_disp=min_disp,; (...); 672 max_mean=max",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3144:2752,wrap,wraps,2752,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3144,1,['wrap'],['wraps']
Integrability,"uns[""dpt_changepoints""],; 249 color_map=color_map,; 250 ); 251 else:; 252 # plot time series as gene expression vs time; 253 timeseries(; 254 adata.X[adata.obs[""dpt_order_indices""].values],; 255 var_names=adata.var_names,; (...); 258 marker=marker,; 259 ). File D:\anaconda\Lib\site-packages\scanpy\plotting\_utils.py:227, in timeseries_as_heatmap(X, var_names, highlights_x, color_map); 223 x_new[:, _hold:] = X[:, hold:]; 225 _, ax = plt.subplots(figsize=(1.5 * 4, 2 * 4)); 226 img = ax.imshow(; --> 227 np.array(X, dtype=np.float_),; 228 aspect=""auto"",; 229 interpolation=""nearest"",; 230 cmap=color_map,; 231 ); 232 plt.colorbar(img, shrink=0.5); 233 plt.yticks(range(X.shape[0]), var_names). ValueError: setting an array element with a sequence.; ```. Error in dpt_groups_pseudotime:. ```pytb; ValueError Traceback (most recent call last); Cell In[91], line 1; ----> 1 sc.pl.dpt_groups_pseudotime(a1). File D:\anaconda\Lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File D:\anaconda\Lib\site-packages\scanpy\plotting\_tools\__init__.py:276, in dpt_groups_pseudotime(adata, color_map, palette, show, save, marker); 274 """"""Plot groups and pseudotime.""""""; 275 _, (ax_grp, ax_ord) = plt.subplots(2, 1); --> 276 timeseries_subplot(; 277 adata.obs[""dpt_groups""].cat.codes,; 278 time=adata.obs[""dpt_order""].values,; 279 color=np.asarray(adata.obs[""dpt_groups""]),; 280 highlights_x=adata.uns[""dpt_changepoints""],; 281 ylabel=""dpt groups"",; 282 yticks=(; 283 np.arange(len(adata.obs[""dpt_groups""].cat.categories), dtype=int); 284 if len(adata.obs[""dpt_groups""].cat.categories) < 5; 285 else None; 286 ),; 287 palette=palette,; 288 ax=ax_grp,; 289 marker=marker,; 290 ); 291 timeseri",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3086:2875,wrap,wrapper,2875,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3086,1,['wrap'],['wrapper']
Integrability,"up and it does look very nice. Well, I learned a lot from `scanpy` here ;) . > tcellmatch's primary purpose is specificity prediction, this could be easily added ontop of this,. Scirpy currently supports the construction of clonotype similarity networks based on Levenshtein distance and BLOSUM62 pairwise sequence alignments. With these networks, we, indeed, had in mind, that clonotypes forming a connected subgraph should recognize the same antigen. Supporting `tcellmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps?. > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs?. Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though!. There's already some information [at the beginning of the tutorial]",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1163#issuecomment-613394910:996,Integrat,Integration,996,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163#issuecomment-613394910,3,"['Integrat', 'integrat']","['Integration', 'integrate']"
Integrability,"up(group); 526 if encoding_type:; --> 527 EncodingVersions[encoding_type].check(; 528 group.name, group.attrs[""encoding-version""]. /broad/software/free/Linux/redhat_7_x86_64/pkgs/anaconda3_2020.07/lib/python3.8/enum.py in __getitem__(cls, name); 343 def __getitem__(cls, name):; --> 344 return cls._member_map_[name]; 345 . KeyError: 'dict'. During handling of the above exception, another exception occurred:. AnnDataReadError Traceback (most recent call last); <ipython-input-2-2626ee07d023> in <module>; ----> 1 sc.read_h5ad('/xchip/beroukhimlab/michelle/jjeang_plgg/sc_labeled.h5ad'). ~/.local/lib/python3.8/site-packages/anndata/_io/h5ad.py in read_h5ad(filename, backed, as_sparse, as_sparse_fmt, chunk_size); 419 d[k] = read_dataframe(f[k]); 420 else: # Base case; --> 421 d[k] = read_attribute(f[k]); 422 ; 423 d[""raw""] = _read_raw(f, as_sparse, rdasp). /broad/software/free/Linux/redhat_7_x86_64/pkgs/anaconda3_2020.07/lib/python3.8/functools.py in wrapper(*args, **kw); 873 '1 positional argument'); 874 ; --> 875 return dispatch(args[0].__class__)(*args, **kw); 876 ; 877 funcname = getattr(func, '__name__', 'singledispatch function'). ~/.local/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, *args, **kwargs); 181 else:; 182 parent = _get_parent(elem); --> 183 raise AnnDataReadError(; 184 f""Above error raised while reading key {elem.name!r} of ""; 185 f""type {type(elem)} from {parent}."". AnnDataReadError: Above error raised while reading key '/layers' of type <class 'h5py._hl.group.Group'> from /.; ```. #### Versions. The following is my colleague's package version list (the one that isn't working). <details>. -----; anndata 0.7.8; scanpy 1.8.2; sinfo 0.3.4; -----; PIL 7.2.0; backcall 0.2.0; bottleneck 1.3.2; cffi 1.14.0; cloudpickle 1.5.0; colorama 0.4.3; cycler 0.10.0; cython_runtime NA; cytoolz 0.10.1; dask 2022.7.1; dateutil 2.8.1; decorator 4.4.2; fsspec 2022.01.0; google NA; h5py 3.6.0; igraph 0.9.9; ipykernel 5.3.2; ipython_genutils 0.2.0; jedi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2310:2269,wrap,wrapper,2269,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2310,1,['wrap'],['wrapper']
Integrability,"upby != dendro_info[""groupby""]:. File ~/.local/share/hatch/env/virtual/scverse-tutorials/_YRPCeuX/basic-scrna/lib/python3.12/site-packages/scanpy/plotting/_anndata.py:2384, in _get_dendrogram_key(adata, dendrogram_key, groupby); 2377 from ..tools._dendrogram import dendrogram; 2379 logg.warning(; 2380 f""dendrogram data not found (using key={dendrogram_key}). ""; 2381 ""Running `sc.tl.dendrogram` with default parameters. For fine ""; 2382 ""tuning it is recommended to run `sc.tl.dendrogram` independently.""; 2383 ); -> 2384 dendrogram(adata, groupby, key_added=dendrogram_key); 2386 if ""dendrogram_info"" not in adata.uns[dendrogram_key]:; 2387 raise ValueError(; 2388 f""The given dendrogram key ({dendrogram_key!r}) does not contain ""; 2389 ""valid dendrogram information.""; 2390 ). File ~/.local/share/hatch/env/virtual/scverse-tutorials/_YRPCeuX/basic-scrna/lib/python3.12/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File ~/.local/share/hatch/env/virtual/scverse-tutorials/_YRPCeuX/basic-scrna/lib/python3.12/site-packages/scanpy/tools/_dendrogram.py:121, in dendrogram(adata, groupby, n_pcs, use_rep, var_names, use_raw, cor_method, linkage_method, optimal_ordering, key_added, inplace); 25 @old_positionals(; 26 ""n_pcs"",; 27 ""use_rep"",; (...); 49 inplace: bool = True,; 50 ) -> dict[str, Any] | None:; 51 """"""\; 52 Computes a hierarchical clustering for the given `groupby` categories.; 53 ; (...); 118 >>> sc.pl.dotplot(adata, markers, groupby='bulk_labels', dendrogram=True); 119 """"""; --> 121 raise_not_implemented_error_if_backed_type(adata.X, ""dendrogram""); 122 if isinstance(groupby, str):; 123 # if not a list, turn into a list; 124 groupby = [groupby]. File ~/.local/sha",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3199:4375,wrap,wraps,4375,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3199,1,['wrap'],['wraps']
Integrability,update ValueError message in pca,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/858:18,message,message,18,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/858,1,['message'],['message']
Integrability,"url, cache=cache, **kwargs); 79 # generate filename and read to dict; 80 filekey = filename. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 458 'Provide `sheet` parameter when reading \'.xlsx\' files.'); 459 else:; --> 460 adata = read_excel(filename, sheet); 461 elif ext in {'mtx', 'mtx.gz'}:; 462 adata = read_mtx(filename). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/anndata/readwrite/read.py in read_excel(filename, sheet, dtype); 59 # rely on pandas for reading an excel file; 60 from pandas import read_excel; ---> 61 df = read_excel(fspath(filename), sheet, dtype=dtype); 62 X = df.values[:, 1:]; 63 row = {'row_names': df.iloc[:, 0].values.astype(str)}. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs); 186 else:; 187 kwargs[new_arg_name] = new_arg_value; --> 188 return func(*args, **kwargs); 189 return wrapper; 190 return _deprecate_kwarg. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs); 186 else:; 187 kwargs[new_arg_name] = new_arg_value; --> 188 return func(*args, **kwargs); 189 return wrapper; 190 return _deprecate_kwarg. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/excel.py in read_excel(io, sheet_name, header, names, index_col, parse_cols, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, verbose, parse_dates, date_parser, thousands, comment, skip_footer, skipfooter, convert_float, mangle_dupe_cols, **kwds); 373 convert_float=convert_float,; 374 mangle_dupe_cols=mangle_dupe_cols,; --> 375 **kwds); 376 ; 377 . ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/excel.py in parse(self, sheet_name, header, names, index_col, usecols, squeeze, converters, true_values, ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/547:2698,wrap,wrapper,2698,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/547,2,['wrap'],['wrapper']
Integrability,"use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy); 118 adata._init_as_actual(adata.copy()); 119 neighbors = Neighbors(adata); --> 120 neighbors.compute_neighbors(; 121 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,; 122 method=method, metric=metric, metric_kwds=metric_kwds,. ~/.local/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds); 732 X = pairwise_distances(X, metric=metric, **metric_kwds); 733 metric = 'precomputed'; --> 734 knn_indices, knn_distances, forest = compute_neighbors_umap(; 735 X, n_neighbors, random_state, metric=metric, metric_kwds=metric_kwds); 736 # very cautious here. ~/.local/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors_umap(X, n_neighbors, random_state, metric, metric_kwds, angular, verbose); 273 # umap 0.5.0; 274 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""); --> 275 from umap.umap_ import nearest_neighbors; 276 ; 277 random_state = check_random_state(random_state). ~/.local/lib/python3.9/site-packages/umap/umap_.py in <module>; 45 ); 46 ; ---> 47 from pynndescent import NNDescent; 48 from pynndescent.distances import named_distances as pynn_named_distances; 49 from pynndescent.sparse import sparse_named_distances as pynn_sparse_named_distances. ~/.local/lib/python3.9/site-packages/pynndescent/__init__.py in <module>; 1 import pkg_resources; 2 import numba; ----> 3 from .pynndescent_ import NNDescent, PyNNDescentTransformer; 4 ; 5 # Workaround: https://github.com/numba/numba/issues/3341. ~/.local/lib/python3.9/site-packages/pynndescent/pynndescent_.py in <module>; 19 import heapq; 20 ; ---> 21 import pynndescent.sparse as sparse; 22 import pynndescent.sparse_nndescent as sparse_nnd; 23 import pynndescent.distances as pynnd_dist. ~/.local/lib/python3.9/site-packages/pynndescent/sparse.py in <module>; 341 },; 342 ); --> ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1652:2015,message,message,2015,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652,1,['message'],['message']
Integrability,"ut your Error output in this code block (if applicable, else delete the block): -->; ```pytb; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/miniconda3/envs/path/lib/python3.7/site-packages/scanpy/__init__.py"", line 38, in <module>; from . import plotting as pl; File ""/miniconda3/envs/path/lib/python3.7/site-packages/scanpy/plotting/__init__.py"", line 1, in <module>; from ._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot, dendrogram, correlation_matrix; File ""/miniconda3/envs/path/lib/python3.7/site-packages/scanpy/plotting/_anndata.py"", line 16, in <module>; from matplotlib import pyplot as pl; File ""/miniconda3/envs/path/lib/python3.7/site-packages/matplotlib/pyplot.py"", line 2282, in <module>; switch_backend(rcParams[""backend""]); File ""/miniconda3/envs/path/lib/python3.7/site-packages/matplotlib/pyplot.py"", line 221, in switch_backend; backend_mod = importlib.import_module(backend_name); File ""/miniconda3/envs/path/lib/python3.7/importlib/__init__.py"", line 127, in import_module; return _bootstrap._gcd_import(name[level:], package, level); File ""/miniconda3/envs/path/lib/python3.7/site-packages/matplotlib/backends/backend_wxagg.py"", line 1, in <module>; import wx; ModuleNotFoundError: No module named 'wx'; ```. The solution is simple, install `wxPython` https://pypi.org/project/wxPython/. However, it would be nice if scanpy could handle this OS-specific dependancy. #### Versions:; The latest scanpy version (1.5.1) installed via conda- of course I cannot print the versions since the scanpy import fails, other details;. ```; >>> import sys; print(sys.version); 3.7.6 | packaged by conda-forge | (default, Jun 1 2020, 18:33:30) ; [Clang 9.0.1 ]; >>> import platform; print(platform.python_implementation()); print(platform.platform()); CPython; Darwin-17.7.0-x86_64-i386-64bit; ```; ...; ```; $ sw_vers; ProductName:	Mac OS X; ProductVersion:	10.13.6; BuildVersion:	17G11023; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1302:1783,depend,dependancy,1783,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1302,1,['depend'],['dependancy']
Integrability,"v for k, v in kw.items() if k not in NON_COLORBAR_KEYS}; -> 2343 cb = cbar.colorbar_factory(cax, mappable, **cb_kw); 2344 ; 2345 self.sca(current_ax). c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\matplotlib\colorbar.py in colorbar_factory(cax, mappable, **kwargs); 1732 cb = ColorbarPatch(cax, mappable, **kwargs); 1733 else:; -> 1734 cb = Colorbar(cax, mappable, **kwargs); 1735 ; 1736 cid = mappable.callbacksSM.connect('changed', cb.update_normal). c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\matplotlib\colorbar.py in __init__(self, ax, mappable, **kwargs); 1226 if isinstance(mappable, martist.Artist):; 1227 _add_disjoint_kwargs(kwargs, alpha=mappable.get_alpha()); -> 1228 ColorbarBase.__init__(self, ax, **kwargs); 1229 ; 1230 @cbook.deprecated(""3.3"", alternative=""update_normal""). c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\matplotlib\cbook\deprecation.py in wrapper(*args, **kwargs); 449 ""parameter will become keyword-only %(removal)s."",; 450 name=name, obj_type=f""parameter of {func.__name__}()""); --> 451 return func(*args, **kwargs); 452 ; 453 return wrapper. c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\matplotlib\colorbar.py in __init__(self, ax, cmap, norm, alpha, values, boundaries, orientation, ticklocation, extend, spacing, ticks, format, drawedges, filled, extendfrac, extendrect, label); 489 else:; 490 self.formatter = format # Assume it is a Formatter or None; --> 491 self.draw_all(); 492 ; 493 def _extend_lower(self):. c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\matplotlib\colorbar.py in draw_all(self); 506 # sets self._boundaries and self._values in real data units.; 507 # takes into account extend values:; --> 508 self._process_values(); 509 # sets self.vmin and vmax in data units, but just for the part of the; 510 # colorbar that is not part of the extend patch:. c:\users\pawandeep\appdata\local\progr",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2003:2609,wrap,wrapper,2609,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2003,1,['wrap'],['wrapper']
Integrability,"vals, we don't always have increasing score. I did check out the description on the main documentations page, and they say they're calculating the zscores underlying the distribution, however if that's the case shouldn't it always be higher with decreasing pval? Also, I went through the code: it looks like they're calculating the scores on the absolute values instead of the real values--why is this? Are the scores basically U1 values corresponding to the pvalues, in whcih case once again lower pvalues should always have higher scores right?; ares calculated from the p-values? What's the relation between the two. I have ran sc.rank_genes_groups() on my gene expression data, and I have generated the matrix for cluster-1 versus the rest, for reference. You can see that one, the pavlues don't increase as we go down the rows; and two, the scores seem kinda arbitrary to the p-values. What am I missing here? Thanks a lot, and sorry for the wordy question. [NOTE: This doesn't just depend on a specific type of clustering technique. Irrespective of whether I try the in-built Leiden clustering technique, or the Schit library, or even cluster the cells randomly, this issue keeps occuring--which makes me think it's ether a bug, or I don't quite understand how the score generation works, maybe both. I've provided a little code snipper below.]. [ClusterOneVsRest.csv](https://github.com/scverse/scanpy/files/12243486/ClusterOneVsRest.csv). ### Minimal code sample. ```python; import pickle; import numpy as np; import pandas as pd; from PIL import Image; import glob; import matplotlib.pyplot as plt; from skimage.morphology import convex_hull_image; from skimage import data, img_as_float; from skimage.util import invert; from scipy.spatial import ConvexHull, convex_hull_plot_2d; from multiprocessing import Pool; import time; import math; from collections import Counter; import scanpy as sc; import networkx as nx; import pandas as pd; import numpy as np; import itertools; import random;",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2586:1485,depend,depend,1485,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586,1,['depend'],['depend']
Integrability,"venv>/lib/python3.12/site-packages/dask/local.py"", line 225 in execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 239 in batch_execute_tasks; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 64 in run; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 92 in _worker; File ""<venv>/lib/python3.12/threading.py"", line 1010 in run; File ""<venv>/lib/python3.12/threading.py"", line 1073 in _bootstrap_inner; File ""<venv>/lib/python3.12/threading.py"", line 1030 in _bootstrap. Thread 0x000000016ed23000 (most recent call first):; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 89 in _worker; File ""<venv>/lib/python3.12/threading.py"", line 1010 in run; File ""<venv>/lib/python3.12/threading.py"", line 1073 in _bootstrap_inner; File ""<venv>/lib/python3.12/threading.py"", line 1030 in _bootstrap. Current thread 0x000000016dd17000 (most recent call first):; File ""~/Dev/scanpy/src/scanpy/_compat.py"", line 133 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 109 in _; File ""<venv>/lib/python3.12/functools.py"", line 909 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 30 in func; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 157 in get; File ""<venv>/lib/python3.12/site-packages/dask/optimization.py"", line 1001 in __call__; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 225 in execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 239 in batch_execute_tasks; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 58 in run; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 92 in _worker; File ""<venv>/lib/python3.12/threading.py"", line 1010 in run; File ""<venv>/lib/python3.12/threading.py"", line 1073 in _bootstrap_inner; Fi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478:2748,wrap,wrapper,2748,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478,2,['wrap'],['wrapper']
Integrability,"ward-array"",; ):; # Preventing recursing inside of these types; return ad.experimental.read_elem(elem); elif iospec.encoding_type == ""array"":; return da.from_zarr(elem); else:; return func(elem). adata = ad.experimental.read_dispatched(f, callback=callback). return adata; adata_dask = read_dask(f'./{rel_zarr_path}'). # perform preprocessing; sc.pp.normalize_total(adata, target_sum=1e4); sc.pp.log1p(adata); # sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5); sc.pp.scale(adata_dask, max_value=10); ```. ```pytb; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); Cell In[1], line 39; 37 sc.pp.log1p(adata); 38 # sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5); ---> 39 sc.pp.scale(adata_dask, max_value=10). File ~/miniconda3/envs/omics/lib/python3.10/functools.py:889, in singledispatch..wrapper(*args, **kw); 885 if not args:; 886 raise TypeError(f'{funcname} requires at least '; 887 '1 positional argument'); --> 889 return dispatch(args[0].__class__)(*args, **kw). File ~/miniconda3/envs/omics/lib/python3.10/site-packages/scanpy/preprocessing/_simple.py:844, in scale_anndata(adata, zero_center, max_value, copy, layer, obsm); 842 view_to_actual(adata); 843 X = _get_obs_rep(adata, layer=layer, obsm=obsm); --> 844 X, adata.var[""mean""], adata.var[""std""] = scale(; 845 X,; 846 zero_center=zero_center,; 847 max_value=max_value,; 848 copy=False, # because a copy has already been made, if it were to be made; 849 return_mean_std=True,; 850 ); 851 _set_obs_rep(adata, X, layer=layer, obsm=obsm); 852 if copy:. File ~/miniconda3/envs/omics/lib/python3.10/functools.py:889, in singledispatch..wrapper(*args, **kw); 885 if not args:; 886 raise TypeError(f'{funcname} requires at least '; 887 '1 positional argument'); --> 889 return dispatch(args[0].__class__)(*args, **kw). TypeError: scale() got an unexpected keyword argument 'return_mean_std'; ```. #### Versions.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2491:3437,wrap,wrapper,3437,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2491,1,['wrap'],['wrapper']
Integrability,"when I select a subset of cells using `ad_sub=ad[ad.obs['louvain']=='subcluster_of_interest',:]`, and then re-apply preprocessing routines, this will use only the genes of `ad.X` (variable over the entire dataset), but not those that are variable only within the subcluster and might be informative for its substructure even if the variance doesn't pass the cutoff when evaluated over the entire dataset. basically, the set of variable genes can only shrink by subsetting.. I'd propose to either use; ```; tmp=ad[ad.obs['louvain']=='subcluster_of_interest',:]; ad_sub=sc.AnnData(tmp.raw.X,obs=tmp.obs,var=tmp.raw.var); ```; to ""reset"" the `.X` matrix (maybe there's a better way?); or to make `sc.pp.highly_variable_genes` work on `ad.raw.X`. ```; scanpy==1.4.4 anndata==0.6.22.post1 umap==0.3.10 numpy==1.16.4 scipy==1.2.1 pandas==0.25.1 scikit-learn==0.20.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/826:130,rout,routines,130,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/826,1,['rout'],['routines']
Integrability,why umap showing bbknn perfectively integrated but tsne,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1370:36,integrat,integrated,36,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1370,1,['integrat'],['integrated']
Integrability,"workx 2.5; numba 0.51.2; numexpr 2.7.1; numpy 1.19.2; packaging 20.4; pandas 1.0.1; parso 0.5.2; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prometheus_client NA; prompt_toolkit 3.0.7; ptyprocess 0.6.0; pvectorc NA; pygments 2.7.0; pylab NA; pyparsing 2.4.7; pyrsistent NA; pytz 2020.1; requests 2.23.0; requests_cache 0.5.2; scanpy 1.6.0; scipy 1.5.2; seaborn 0.11.0; send2trash NA; setuptools_scm NA; sinfo 0.3.1; six 1.15.0; sklearn 0.23.2; socks 1.7.1; soupsieve 2.0.1; statsmodels 0.12.0; storemagic NA; tables 3.6.1; terminado 0.8.3; tornado 6.0.4; traitlets 5.0.4; urllib3 1.25.10; wcwidth 0.2.5; wrapt 1.12.1; xlsxwriter 1.3.3; zmq 19.0.2; -----; IPython 7.18.1; jupyter_client 6.1.7; jupyter_core 4.6.3; jupyterlab 2.2.8; notebook 6.1.4; -----; Python 3.8.5 | packaged by conda-forge | (default, Aug 29 2020, 01:22:49) [GCC 7.5.0]; Linux-4.4.0-142-generic-x86_64-with-glibc2.10; 64 logical CPU cores, x86_64; -----; Session information updated at 2020-09-16 11:03; ```. Here is the error message:. ```; ---------------------------------------------------------------------------; InvalidIndexError Traceback (most recent call last); <ipython-input-37-b22ada65a1cd> in <module>; 1 # Create Concatenated anndata object for all timepoints; 2 #alldays = e125.concatenate(e135, e145, e155, uns_merge=""unique""); ----> 3 alldays = e125.concatenate(e135). ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/anndata.py in concatenate(self, join, batch_key, batch_categories, uns_merge, index_unique, fill_value, *adatas); 1696 all_adatas = (self,) + tuple(adatas); 1697 ; -> 1698 out = concat(; 1699 all_adatas,; 1700 axis=0,. ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in concat(adatas, axis, join, merge, uns_merge, label, keys, index_unique, fill_value, pairwise); 799 [dim_indices(a, axis=1 - axis) for a in adatas], join=join; 800 ); --> 801 reindexers = [; 802 gen_reindexer(alt_indices, dim_indices(a, axis=1 -",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1409#issuecomment-693478875:1857,message,message,1857,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409#issuecomment-693478875,1,['message'],['message']
Integrability,"write anndata failed, pearson_residuals_df header message is too large",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2383:50,message,message,50,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2383,1,['message'],['message']
Integrability,"x that contains 8 samples to analyze. My goal is to read count matrix as AnnData object. I am transposing the matrix because I wanted the rows to represent cells (5258) and the columns to represent genes (17143). . ```python; # Read the count matrix ; adata = sc.read_text('/file_path').T #transposed the matrix because I wanted the rows to represent cells (5258) and the columns to represent genes (17143). adata #5258 cells, 17143 genes. # extract the sample name from the column names; adata.obs['sample_name'] = adata.obs.index.str.split('_').str[0]; adata.obs['barcode'] = adata.obs.index.str.split('_').str[1]. # create a list of the sample names; sample_list = ['Pb.F1', 'Pb.M1', 'Ctl.M1', 'Pb.F2', 'Ctl.F1', 'Pb.M2', 'Ctl.M2', 'Ctl.F2']. # iterate through the sample names and create a new AnnData object for each sample; for sample in sample_list:; sample_adata = adata[:, adata.obs['sample_name'] == sample]. ```; However when I run the code for ""iterate through the sample names and create a new AnnData object for each sample"" this is where an error message is generated. ; ```pytb; ---------------------------------------------------------------------------; IndexError Traceback (most recent call last); Input In [5], in <cell line: 2>(); 1 # iterate through the sample names and create a new AnnData object for each sample; 2 for sample in sample_list:; ----> 3 sample_adata = adata[:, adata.obs['sample_name'] == sample]. File ~/opt/anaconda3/lib/python3.9/site-packages/anndata/_core/anndata.py:1113, in AnnData.__getitem__(self, index); 1111 def __getitem__(self, index: Index) -> ""AnnData"":; 1112 """"""Returns a sliced view of the object.""""""; -> 1113 oidx, vidx = self._normalize_indices(index); 1114 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ~/opt/anaconda3/lib/python3.9/site-packages/anndata/_core/anndata.py:1094, in AnnData._normalize_indices(self, index); 1093 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:; -> 1094 return _nor",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2402:1144,message,message,1144,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2402,1,['message'],['message']
Integrability,"x=[f'Gene_{i}' for i in range(data.shape[1])]). # Create the AnnData object; adata = ad.AnnData(X=data, obs=obs, var=var). # Test layer call function; adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test; sc.pl.highest_expr_genes(adata, layer='normalised'); ```. ### Error output. ```pytb; Output exceeds the size limit. Open the full output data in a text editor; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); Cell In[32], line 17; 15 # Test layer call function; 16 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test; ---> 17 sc.pl.highest_expr_genes(adata, layer='normalised'); 19 # Test layer call function; 20 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\scanpy\plotting\_qc.py:100, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds); 98 height = (n_top * 0.2) + 1.5; 99 fig, ax = plt.subplots(figsize=(5, height)); --> 100 sns.boxplot(data=counts_top_genes, orient=""h"", ax=ax, fliersize=1, **kwds); 101 ax.set_xlabel(""% of total counts""); 102 if log:. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\seaborn\categorical.py:1634, in boxplot(data, x, y, hue, order, hue_order, orient, color, palette, saturation, fill, dodge, width, gap, whis, linecolor, linewidth, fliersize, hue_norm, native_scale, log_scale, formatter, legend, ax",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3318:1762,wrap,wrapper,1762,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3318,1,['wrap'],['wrapper']
Integrability,"xception:. TypeError Traceback (most recent call last); <ipython-input-17-f0b30fa7797a> in <module>; ----> 1 gex_matrix.write('/Volumes/Bf110/ct5/raw_data/single_cell/external/GSE156793/GSE156793_GEX_ctl210604.raw.h5ad'). ~/anaconda3/lib/python3.8/site-packages/anndata/_core/anndata.py in write_h5ad(self, filename, compression, compression_opts, force_dense, as_dense); 1903 filename = self.filename; 1904 ; -> 1905 _write_h5ad(; 1906 Path(filename),; 1907 self,. ~/anaconda3/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_h5ad(filepath, adata, force_dense, as_dense, dataset_kwargs, **kwargs); 109 else:; 110 write_attribute(f, ""raw"", adata.raw, dataset_kwargs=dataset_kwargs); --> 111 write_attribute(f, ""obs"", adata.obs, dataset_kwargs=dataset_kwargs); 112 write_attribute(f, ""var"", adata.var, dataset_kwargs=dataset_kwargs); 113 write_attribute(f, ""obsm"", adata.obsm, dataset_kwargs=dataset_kwargs). ~/anaconda3/lib/python3.8/functools.py in wrapper(*args, **kw); 873 '1 positional argument'); 874 ; --> 875 return dispatch(args[0].__class__)(*args, **kw); 876 ; 877 funcname = getattr(func, '__name__', 'singledispatch function'). ~/anaconda3/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_attribute_h5ad(f, key, value, *args, **kwargs); 128 if key in f:; 129 del f[key]; --> 130 _write_method(type(value))(f, key, value, *args, **kwargs); 131 ; 132 . ~/anaconda3/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, key, val, *args, **kwargs); 210 except Exception as e:; 211 parent = _get_parent(elem); --> 212 raise type(e)(; 213 f""{e}\n\n""; 214 f""Above error raised while writing key {key!r} of {type(elem)}"". TypeError: Can't implicitly convert non-string objects to strings. Above error raised while writing key 'Batch' of <class 'h5py._hl.group.Group'> from /. Above error raised while writing key 'obs' of <class 'h5py._hl.files.File'> from /. ```. #### Versions. <details>. WARNING: If you miss a compact list, please try `print_header`!; The `si",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1866:4952,wrap,wrapper,4952,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1866,1,['wrap'],['wrapper']
Integrability,"y. It doesn't seem I can do that without first changing the current working directory outside the line of code. What is the best way to save my plot to a specific directory without having to change it each time using os.chdir? . I have seen this [issue](https://github.com/scverse/scanpy/issues/1508#issue-750736685) from 2 years ago but wondered if any changes have been made since. ### Minimal code sample. ```; output_dir_fig = ""chosen/path/to/directory""; sc.pl.highest_expr_genes(adata, n_top=10, save= f""{output_dir_fig}/highest_expr_genes.png""). ```. ### Error output. ---------------------------------------------------------------------------; FileNotFoundError Traceback (most recent call last); Cell In[85], line 1; ----> 1 sc.pl.highest_expr_genes(adata, n_top=10, save= f""{output_dir_fig}/highest_expr_genes.png""). File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/scanpy/plotting/_qc.py:105, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds); 103 ax.set_xscale(""log""); 104 show = settings.autoshow if show is None else show; --> 105 _utils.savefig_or_show(""highest_expr_genes"", show=show, save=save); 106 if show:; 107 return None. File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/scanpy/plotting/_utils.py:339, in savefig_or_show(writekey, show, dpi, ext, save); 337 show = settings.autoshow if show is None else show; 338 if save:; --> 339 savefig(writekey, dpi=dpi, ext=ext); 340 if show:; ...; -> 2456 fp = builtins.open(filename, ""w+b""); 2458 try:; 2459 save_handler(self, fp, filename). FileNotFoundError: [Errno 2] N",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3276:1408,wrap,wraps,1408,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3276,1,['wrap'],['wraps']
Integrability,"y:363, in partial_reduce(func, x, split_every, keepdims, dtype, name, reduced_meta); 362 try:; --> 363 meta = func(reduced_meta, computing_meta=True); 364 # no meta keyword argument exists for func, and it isn't required. File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/dask/array/reductions.py:685, in mean_combine(pairs, sum, numel, dtype, axis, computing_meta, **kwargs); 684 ns = deepmap(lambda pair: pair[""n""], pairs) if not computing_meta else pairs; --> 685 n = _concatenate2(ns, axes=axis).sum(axis=axis, **kwargs); 687 if computing_meta:. TypeError: _cs_matrix.sum() got an unexpected keyword argument 'keepdims'. During handling of the above exception, another exception occurred:. IndexError Traceback (most recent call last); Cell In[19], line 1; ----> 1 result = sc.pp.highly_variable_genes(adata, inplace=False); 2 result. File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /mnt/workspace/repos/scanpy/scanpy/preprocessing/_highly_variable_genes.py:702, in highly_variable_genes(***failed resolving arguments***); 699 del min_disp, max_disp, min_mean, max_mean, n_top_genes; 701 if batch_key is None:; --> 702 df = _highly_variable_genes_single_batch(; 703 adata, layer=layer, cutoff=cutoff, n_bins=n_bins, flavor=flavor; 704 ); 705 else:; 706 df = _highly_variable_genes_batched(; 707 adata, batch_key, layer=layer, cutoff=cutoff, n_bins=n_bins, flavor=flavor; 708 ). File /mnt/workspace/repos/scanpy/scanpy/preprocessing/_highly_variable_genes.py:274, in _highly_variable_genes_single_batch(adata, layer, cutoff, n_bins, flavor); 271 else:; 272 X = np.expm1(X); --> 274 mean",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2809#issuecomment-1906001725:1415,wrap,wrapper,1415,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2809#issuecomment-1906001725,1,['wrap'],['wrapper']
Integrability,"yes!. 1. make sure you have all of your modified copy somewhere outside of the cloned directory! we’re going to destroy all changes inside of that directory!; 2. Destroy all changes and go back to 1.4: `git reset --hard 1.4` (if it’s really 1.4 and not e.g. 1.4.1); 3. Make a new branch based on 1.4: `git checkout -b weighted-clustering`; 4. Copy your changes over again.; 5. Check if all is right: `git diff` should only tell you about your modified files, not other junk.; 6. Add all files you changed individually (not `git add .` or `git add -A`, but `git add scanpy/file1.py scanpy/file2.py ...`); 7. `git diff` should now say that there’s a few staged files (your modifications), and no other added or modified files.; 8. Commit your changes `git commit -m 'your commit message'` and push them `git push`. Now you can make a new PR with just your changes in it: https://github.com/theislab/scanpy/compare/master...Khalid-Usman:weighted-clustering. Make sure that you only see your changes on the lower part of that page before hitting the “Create Pull Request” button",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/630#issuecomment-492124539:777,message,message,777,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630#issuecomment-492124539,1,['message'],['message']
Integrability,"yes, I'll get to it next week. It didn't seem there was a straightforward way to integrate with the existing implementation given the filtering criterion is different, but I'll try my best.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/993#issuecomment-615957573:81,integrat,integrate,81,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/993#issuecomment-615957573,1,['integrat'],['integrate']
Integrability,"ynndescent`/ `umap` were forcing a `workqueue` backend which is always available. `pynndescent` 0.5.3 recently came out, and started favoring the `tbb` backend. This is a problem since numba thinks this backend is available on our CI (and on my HPC), but it’s actually not. I also can’t get tbb to even install in a way numba sees in on these systems. If tbb isn’t actually available, but numba detects it we get errors with horrible tracebacks anytime parallelized numba code is used https://github.com/lmcinnes/pynndescent/issues/129. These tracebacks are so horrible they break our CI further https://github.com/pytest-dev/pytest-nunit/issues/47. So what do we do?. ## Possible solutions:. * Pin pynndescent below 0.5.3. This makes pynndescent a required dependency. We’ve previously avoided this since it would change results for people using `umap<0.4` (e.g. anyone with `scvelo` installed) who did not explicitly install pynndescent. However, given the lack of complaints around umap results changing as dependencies have increased, this may not be so bad. It would be great if we could constrain the version without having it be a dependency. This would be similar to what's possible with `pip` and [constraints files](https://pip.pypa.io/en/stable/user_guide/#constraints-files), I don't see how one would be able to specify this for a package. I don't think it's possible, but maybe I'm missing something about the version string syntax. * Make sure the numba threading layer is `“workqueue”` after pynndescent is imported. This is tricky. pynndescent<0.5.3 takes a long time to import, so we don’t want to do this at the top level. So we would need to add a check after everytiem pynndescent could possibly be imported to check that it didn’t set the threading backend to anything else. My understanding of the numba threading system is that once you’ve called for parallel compilation, you’re locked into the threading backend for that session. * Make sure the threading layer is workqueue ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1931:1113,depend,dependencies,1113,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1931,1,['depend'],['dependencies']
Integrability,"you have a release candidate of that one installed, so what happened is the only correct behavior: It uninstalled an incompatible version to install a compatible one. If your install’s metadata was outdated and it was in fact a compatible one, then you forgot to refresh the metadata by reinstalling it. That’s annoying but necessary as editable installs are nonstandard and therefore not well integrated into how package metadata works. > Why not just use `pip install -e` here?. Because development installs in general are nonstandard, and `pip install -e` in particular uses the deprecated `setup.py`. Tasks; -----. > - Exclude setup.py from sdist using the standard way, not via .gitignore. sounds good!. > I'm a bit concerned that pinning pip to an old version could lead to problems, especially while pip is going through a lot of changes.; > ...; > - flit mangles the build version part of wheel filenames, in a way that pip just started checking for. . No, as far as I can see, pip arbitrarily decided to not allow local version specifiers in wheel filenames. AFAIK nothing says there can’t be pluses in there, only that you can’t upload packages with local specifiers in their version to PyPI. Which we don’t do here, so pip should chill. If flit decides to work around that quirk, or pip relaxes, we can unpin pip. > - flit symlinked packages seem to be overwritten if a new package is installed which has the symlinked package as a dependency. Seee above. Has nothing to do with flit. What made you thing that anyway?. > - There is a fairly large workaround to make the package version available if the dependencies are not installed. Is it possible to use something more standard like versioneer here?. No. Either we hardcode a string constant in the `__init__.py` or we leave it like it is until flit allows an alternative. That’s the only disadvantage flit has IMHO, but we discussed that at length in the past and found it to not be a problem as the hack is robust and well documented.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1527#issuecomment-781992443:1639,depend,dependency,1639,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-781992443,2,['depend'],"['dependencies', 'dependency']"
Integrability,"ysis on my Intel-core iMac. Surprisingly, when I ran the same line of code (under a similar virtual environment) on my M2-chip laptop, it finished in a flash of time.; ```Running Scrublet; filtered out 1419 genes that are detected in less than 3 cells; normalizing counts per cell; finished (0:00:00); extracting highly variable genes; finished (0:00:00); --> added; 'highly_variable', boolean vector (adata.var); 'means', float vector (adata.var); 'dispersions', float vector (adata.var); 'dispersions_norm', float vector (adata.var); normalizing counts per cell; finished (0:00:00); normalizing counts per cell; finished (0:00:00); Embedding transcriptomes using PCA...; using data matrix X directly; Automatically set threshold at doublet score = 0.42; Detected doublet rate = 0.3%; Estimated detectable doublet fraction = 5.2%; Overall doublet rate:; 	Expected = 5.0%; 	Estimated = 6.6%; Scrublet finished (0:00:14); ```. I'm still not sure what actually caused the problem, but it seems that some dependency inconsistency occurred when performing PCA within the pipeline. Perhaps some package required for the `sc.pp.scrublet()` pipeline needs to be updated to a newer version?. Here are the details of the packages in the virtual environment when I ran the code on my desktop (failed case):; ```; channels:; - pytorch; - plotly; - conda-forge; - bioconda; - defaults; dependencies:; - anndata=0.10.7; - anyio=4.4.0; - appnope=0.1.4; - argcomplete=3.3.0; - argh=0.31.2; - argon2-cffi=23.1.0; - argon2-cffi-bindings=21.2.0; - arpack=3.8.0; - array-api-compat=1.7.1; - arrow=1.3.0; - asttokens=2.4.1; - async-lru=2.0.4; - attrs=23.2.0; - babel=2.14.0; - beautifulsoup4=4.12.3; - biopython=1.83; - blas=2.120; - blas-devel=3.9.0; - bleach=6.1.0; - blosc=1.21.5; - brotli=1.1.0; - brotli-bin=1.1.0; - brotli-python=1.1.0; - bzip2=1.0.8; - c-ares=1.28.1; - c-blosc2=2.14.4; - ca-certificates=2024.6.2; - cached-property=1.5.2; - cached_property=1.5.2; - certifi=2024.6.2; - cffi=1.16.0; - charset-norm",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3116:2267,depend,dependency,2267,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3116,1,['depend'],['dependency']
Integrability,ython-snappy 0.6.1; pytoolconfig 1.2.5; pytz 2023.3.post1; pyviz-comms 2.3.0; PyWavelets 1.4.1; pyxdg 0.27; PyYAML 6.0; pyzmq 23.2.0; QDarkStyle 3.0.2; qstylizer 0.2.2; QtAwesome 1.2.2; qtconsole 5.4.2; QtPy 2.2.0; queuelib 1.5.0; regex 2022.7.9; requests 2.31.0; requests-file 1.5.1; requests-toolbelt 1.0.0; responses 0.13.3; rfc3339-validator 0.1.4; rfc3986-validator 0.1.1; rope 1.7.0; Rtree 1.0.1; ruamel.yaml 0.17.21; ruamel-yaml-conda 0.17.21; s3fs 2023.4.0; safetensors 0.3.2; scikit-image 0.20.0; scikit-learn 1.3.0; scikit-learn-intelex 20230426.111612; scipy 1.11.1; Scrapy 2.8.0; seaborn 0.12.2; SecretStorage 3.3.1; Send2Trash 1.8.0; service-identity 18.1.0; setuptools 68.0.0; sip 6.6.2; six 1.16.0; smart-open 5.2.1; sniffio 1.2.0; snowballstemmer 2.2.0; sortedcontainers 2.4.0; soupsieve 2.4; Sphinx 5.0.2; sphinxcontrib-applehelp 1.0.2; sphinxcontrib-devhelp 1.0.2; sphinxcontrib-htmlhelp 2.0.0; sphinxcontrib-jsmath 1.0.1; sphinxcontrib-qthelp 1.0.3; sphinxcontrib-serializinghtml 1.1.5; spyder 5.4.3; spyder-kernels 2.4.4; SQLAlchemy 1.4.39; stack-data 0.2.0; statsmodels 0.14.0; sympy 1.11.1; tables 3.8.0; tabulate 0.8.10; TBB 0.2; tblib 1.7.0; TELR 1.0; tenacity 8.2.2; terminado 0.17.1; text-unidecode 1.3; textdistance 4.2.1; texttable 1.7.0; threadpoolctl 2.2.0; three-merge 0.1.1; tifffile 2023.4.12; tinycss2 1.2.1; tldextract 3.2.0; tokenizers 0.13.2; toml 0.10.2; tomlkit 0.11.1; toolz 0.12.0; tornado 6.3.2; tqdm 4.65.0; traitlets 5.7.1; transformers 4.32.1; Twisted 22.10.0; typing_extensions 4.7.1; tzdata 2023.3; uc-micro-py 1.0.1; ujson 5.4.0; Unidecode 1.2.0; urllib3 1.26.16; w3lib 1.21.0; watchdog 2.1.6; wcwidth 0.2.5; webencodings 0.5.1; websocket-client 0.58.0; Werkzeug 2.2.3; whatthepatch 1.0.2; wheel 0.38.4; widgetsnbextension 4.0.5; wrapt 1.14.1; wurlitzer 3.0.2; xarray 2023.6.0; xxhash 2.0.2; xyzservices 2022.9.0; y-py 0.5.9; yapf 0.31.0; yarl 1.8.1; ypy-websocket 0.8.2; zict 2.2.0; zipp 3.11.0; zope.interface 5.4.0; zstandard 0.19.0; ```. </details>,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2706:7763,wrap,wrapt,7763,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2706,2,"['interface', 'wrap']","['interface', 'wrapt']"
Integrability,"~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\scanpy\plotting\_tools\paga.py:1255, in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax); 1253 print(X.shape); 1254 if as_heatmap:; -> 1255 img = ax.imshow(X, aspect=""auto"", interpolation=""nearest"", cmap=color_map); 1256 if show_yticks:; 1257 ax.set_yticks(range(len(X))). File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\matplotlib\__init__.py:1459, in _preprocess_data.<locals>.inner(ax, data, *args, **kwargs); 1456 @functools.wraps(func); 1457 def inner(ax, *args, data=None, **kwargs):; 1458 if data is None:; -> 1459 return func(ax, *map(sanitize_sequence, args), **kwargs); 1461 bound = new_sig.bind(ax, *args, **kwargs); 1462 auto_label = (bound.arguments.get(label_namer); 1463 or bound.kwargs.get(label_namer)). File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\matplotlib\axes\_axes.py:5665, in Axes.imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs); 5657 self.set_aspect(aspect); 5658 im = mimage.AxesImage(self, cmap=cmap, norm=norm,; 5659 interpolation=interpolation, origin=origin,; 5660 extent=extent, filternorm=filternorm,; 5661 filterrad=filterrad, resample=resample,; 5662 interpolation_stage=interpolation_stage,; 5663 **kwargs); -> 5665 im.set_data(X); 5666 im.set_alpha(alpha); 5667 if im.get_clip_path() is None:; 5668 # image does not already have clipping set, clip to axes patch. File ~\AppData\Local\Pa",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3025:3300,wrap,wraps,3300,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3025,1,['wrap'],['wraps']
Integrability,"’t intended to be packages, so you shouldn’t import from there. Just think about a module-level `pytest.importorskip(...)` or so. Also if there’s a `__init__.py` somewhere in your `tests` directory when using pytest, you’re doing something wrong. Yes, that means that all packages except for numba are doing it wrong, because numba also has importable test utils in there, and the others just misunderstand how pytest works. I blame setuptools, because `find_packages` finds directories that have `__init__.py`, so people just cargo-cultily started adding it without knowing what they’re doing. Fixture visibility is hierarchical, so a conftest.py in a subfolder is able to override fixtures from higher-up.; So e.g. for testing a package/app that uses celery, you just define your own `celery_config` fixture, then start using the `celery_app` fixture, which will use your config automatically. ## `tests` in the package. I think it’s a good idea to have it in there if you are a huge project and like to physically split up tests into multiple directiories that are close to the source code they test. (like numpy does it). But as long as there’s only one `tests` directory with a structure that doesn’t neatly map to your module hierarchy, having it outside is cleaner because people can’t accidentally import from there. And as you can see from your links and our contributors importing stuff from `test_*` collections, a lot of projects don’t know how to use pytest, so making misuse harder is beneficial. Also we have test data, which wastes space on every user’s machine and bandwidth for people installing scanpy. ## `tests` and `testing`/`test_utils`. Given the above points, I think we should move things out to keep everything clean, and the package small. I also like the separation of concerns: `test_utils` or `testing` (like numpy does) for reusable stuff that other projects depending on you might use and `tests` for actual tests. We can import the reusable fixtures in `conftest.py`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1528#issuecomment-738776290:1967,depend,depending,1967,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1528#issuecomment-738776290,1,['depend'],['depending']
Modifiability,"	with 9992 stored elements in Compressed Sparse Row format>, index\nAAAGCCTGGCTAAC-1 0.023856\nAAATTCGATGCACA-1 0.027458\nAACACGTGGTCTTT-1 0.016819\nAAGTGCACGTGCTA-1 0.011797\nACACGAACGGAGTG-1 0.017277\n ... \nTGGCACCTCCAACA-8 0.008840\nTGTGAGTGCTTTAC-8 0.022068\nTGTTACTGGCGATT-8 0.012821\nTTCAGTACCGGGAA-8 0.014169\nTTGAGGTGGAGAGC-8 0.010886\nName: percent_mito, Length: 700, dtype: float32); E + where <function morans_i at 0x7f354779d9d0> = <module 'scanpy.metrics' from '/opt/hostedtoolcache/Python/3.9.18/x64/lib/python3.9/site-packages/scanpy/metrics/__init__.py'>.morans_i; E + where <module 'scanpy.metrics' from '/opt/hostedtoolcache/Python/3.9.18/x64/lib/python3.9/site-packages/scanpy/metrics/__init__.py'> = sc.metrics; E + and 0.13099293222276967 = <function morans_i at 0x7f354779d9d0>(AnnData object with n_obs × n_vars = 700 × 765\n obs: 'bulk_labels', 'n_genes', 'percent_mito', 'n_counts', 'S_score', 'G2M_score', 'phase', 'louvain'\n var: 'n_counts', 'means', 'dispersions', 'dispersions_norm', 'highly_variable'\n uns: 'bulk_labels_colors', 'louvain', 'louvain_colors', 'neighbors', 'pca', 'rank_genes_groups'\n obsm: 'X_pca', 'X_umap'\n varm: 'PCs'\n layers: 'raw'\n obsp: 'distances', 'connectivities', vals=index\nAAAGCCTGGCTAAC-1 0.023856\nAAATTCGATGCACA-1 0.027458\nAACACGTGGTCTTT-1 0.016819\nAAGTGCACGTGCTA-1 0.011797\nACACGAACGGAGTG-1 0.017277\n ... \nTGGCACCTCCAACA-8 0.008840\nTGTGAGTGCTTTAC-8 0.022068\nTGTTACTGGCGATT-8 0.012821\nTTCAGTACCGGGAA-8 0.014169\nTTGAGGTGGAGAGC-8 0.010886\nName: percent_mito, Length: 700, dtype: float32); E + where <function morans_i at 0x7f354779d9d0> = <module 'scanpy.metrics' from '/opt/hostedtoolcache/Python/3.9.18/x64/lib/python3.9/site-packages/scanpy/metrics/__init__.py'>.morans_i; E + where <module 'scanpy.metrics' from '/opt/hostedtoolcache/Python/3.9.18/x64/lib/python3.9/site-packages/scanpy/metrics/__init__.py'> = sc.metrics. scanpy/tests/test_metrics.py:78: AssertionError; ```. ### Versions. <details>. ```; Package Versi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2688:2642,layers,layers,2642,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2688,1,['layers'],['layers']
Modifiability," 0:; -> 1097 return np.vstack(rptree_leaf_array_parallel(rp_forest)); 1098 else:; 1099 return np.array([[-1]]). ~/.local/lib/python3.8/site-packages/pynndescent/rp_trees.py in rptree_leaf_array_parallel(rp_forest); 1087 ; 1088 def rptree_leaf_array_parallel(rp_forest):; -> 1089 result = joblib.Parallel(n_jobs=-1, require=""sharedmem"")(; 1090 joblib.delayed(get_leaves_from_tree)(rp_tree) for rp_tree in rp_forest; 1091 ). /omics/groups/OE0540/internal/B260/users/olga/.conda/envs/10x_analysis/lib/python3.8/site-packages/joblib/parallel.py in __call__(self, iterable); 1054 ; 1055 with self._backend.retrieval_context():; -> 1056 self.retrieve(); 1057 # Make sure that we get a last message telling us we are done; 1058 elapsed_time = time.time() - self._start_time. /omics/groups/OE0540/internal/B260/users/olga/.conda/envs/10x_analysis/lib/python3.8/site-packages/joblib/parallel.py in retrieve(self); 933 try:; 934 if getattr(self._backend, 'supports_timeout', False):; --> 935 self._output.extend(job.get(timeout=self.timeout)); 936 else:; 937 self._output.extend(job.get()). /omics/groups/OE0540/internal/B260/users/olga/.conda/envs/10x_analysis/lib/python3.8/multiprocessing/pool.py in get(self, timeout); 769 return self._value; 770 else:; --> 771 raise self._value; 772 ; 773 def _set(self, i, obj):. /omics/groups/OE0540/internal/B260/users/olga/.conda/envs/10x_analysis/lib/python3.8/multiprocessing/pool.py in worker(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception); 123 job, i, func, args, kwds = task; 124 try:; --> 125 result = (True, func(*args, **kwds)); 126 except Exception as e:; 127 if wrap_exception and func is not _helper_reraises_exception:. /omics/groups/OE0540/internal/B260/users/olga/.conda/envs/10x_analysis/lib/python3.8/site-packages/joblib/_parallel_backends.py in __call__(self, *args, **kwargs); 593 def __call__(self, *args, **kwargs):; 594 try:; --> 595 return self.func(*args, **kwargs); 596 except KeyboardInterrupt as e:; 597 # We capture the",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2472:4216,extend,extend,4216,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2472,1,['extend'],['extend']
Modifiability," 265 layers = aggregate(; 266 data,; 267 by=categorical,; (...); 270 dof=dof,; 271 ); --> 272 result = AnnData(; 273 layers=layers,; 274 obs=new_label_df,; 275 var=getattr(adata, ""var"" if axis == 0 else ""obs""),; 276 ); 278 if axis == 1:; 279 return result.T. File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/anndata/_core/anndata.py:271, in AnnData.__init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, obsp, varp, oidx, vidx); 269 self._init_as_view(X, oidx, vidx); 270 else:; --> 271 self._init_as_actual(; 272 X=X,; 273 obs=obs,; 274 var=var,; 275 uns=uns,; 276 obsm=obsm,; 277 varm=varm,; 278 raw=raw,; 279 layers=layers,; 280 dtype=dtype,; 281 shape=shape,; 282 obsp=obsp,; 283 varp=varp,; 284 filename=filename,; 285 filemode=filemode,; 286 ). File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/anndata/_core/anndata.py:501, in AnnData._init_as_actual(self, X, obs, var, uns, obsm, varm, varp, obsp, raw, layers, dtype, shape, filename, filemode); 498 self._clean_up_old_format(uns); 500 # layers; --> 501 self._layers = Layers(self, layers). File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/anndata/_core/aligned_mapping.py:331, in Layers.__init__(self, parent, vals); 329 self._data = dict(); 330 if vals is not None:; --> 331 self.update(vals). File <frozen _collections_abc>:949, in update(self, other, **kwds). File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/anndata/_core/aligned_mapping.py:199, in AlignedActualMixin.__setitem__(self, key, value); 198 def __setitem__(self, key: str, value: V):; --> 199 value = self._validate_value(value, key); 200 self._data[key] = value. File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/anndata/_core/aligned_mapping.py:89, in AlignedMapping._validate_value(self, val, key); 83 dims = tuple((""obs"", ""var"")[ax] for ax in self.axes); 84 msg = (; 85 f""Value passed for key {",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2929:2212,layers,layers,2212,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2929,1,['layers'],['layers']
Modifiability," 295 X=X,; --> 296 log1p=log1p,; 297 ); 298 var_metrics = describe_var(. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\preprocessing\_qc.py in describe_obs(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, log1p, inplace, X, parallel); 112 if percent_top:; 113 percent_top = sorted(percent_top); --> 114 proportions = top_segment_proportions(X, percent_top); 115 for i, n in enumerate(percent_top):; 116 obs_metrics[f""pct_{expr_type}_in_top_{n}_{var_type}""] = (. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\preprocessing\_qc.py in top_segment_proportions(mtx, ns); 377 mtx = csr_matrix(mtx); 378 return top_segment_proportions_sparse_csr(; --> 379 mtx.data, mtx.indptr, np.array(ns, dtype=np.int); 380 ); 381 else:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\dispatcher.py in _compile_for_args(self, *args, **kws); 432 e.patch_message('\n'.join((str(e).rstrip(), help_msg))); 433 # ignore the FULL_TRACEBACKS config, this needs reporting!; --> 434 raise e; 435 ; 436 def inspect_llvm(self, signature=None):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\dispatcher.py in _compile_for_args(self, *args, **kws); 365 argtypes.append(self.typeof_pyval(a)); 366 try:; --> 367 return self.compile(tuple(argtypes)); 368 except errors.ForceLiteralArg as e:; 369 # Received request for compiler re-entry with the list of arguments. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\compiler_lock.py in _acquire_compile_lock(*args, **kwargs); 30 def _acquire_compile_lock(*args, **kwargs):; 31 with self:; ---> 32 return func(*args, **kwargs); 33 return _acquire_compile_lock; 34 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, sig); 806 self._cache_misses[sig] += 1; 807 try:; --> 808 cres = self._compiler.compile(args, return_type); 809 except errors.ForceLiteralArg as e:; 810 def folded(args, kws):. ~\AppData\Local\Continuum\anaconda3\lib\site-packag",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1341:6376,config,config,6376,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1341,1,['config'],['config']
Modifiability," ; > obs: 'sample', 'n_genes', 'percent_mito', 'n_counts'; > var: 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm'. > AnnData object with n_obs × n_vars = 14775 × 25386 ; > obs: 'sample', 'n_genes', 'percent_mito', 'n_counts'; > var: 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm'. then on adata2, I cannot add the X_pca field; `sc.tl.pca(adata2, svd_solver='arpack')`. > ---------------------------------------------------------------------------; > ValueError Traceback (most recent call last); > <ipython-input-25-05be375bfc24> in <module>; > 5 print(adata); > 6 print(adata2); > ----> 7 sc.tl.pca(adata2, svd_solver='arpack'); > 8 print(adata2); > ; > ~/miniconda3/lib/python3.7/site-packages/scanpy-1.4+18.gaabe446-py3.7.egg/scanpy/preprocessing/_simple.py in pca(data, n_comps, zero_center, svd_solver, random_state, return_info, use_highly_variable, dtype, copy, chunked, chunk_size); > 504 ; > 505 if data_is_AnnData:; > --> 506 adata.obsm['X_pca'] = X_pca; > 507 if use_highly_variable:; > 508 adata.varm['PCs'] = np.zeros(shape=(adata.n_vars, n_comps)); > ; > ValueError: no field of name X_pca. ```; print(adata2); print(adata); ```. > View of AnnData object with n_obs × n_vars = 14775 × 1999 ; > obs: 'sample', 'n_genes', 'percent_mito', 'n_counts'; > var: 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm'. > AnnData object with n_obs × n_vars = 14775 × 25386 ; > obs: 'sample', 'n_genes', 'percent_mito', 'n_counts'; > var: 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm'. if I do the pca on the original AnnData object for the highly variable genes, it works:; ```; sc.tl.pca(adata, use_highly_variable=True, svd_solver='arpack'); print(adata); ```. > AnnData object with n_obs × n_vars = 14775 × 25386 ; > obs: 'sample', 'n_genes', 'percent_mito', 'n_counts'; > var: 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm'; > uns: 'pca'; > obsm: 'X_pca'; > varm: 'PCs'",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/504#issuecomment-467361094:2555,variab,variable,2555,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/504#issuecomment-467361094,2,['variab'],['variable']
Modifiability," Does `mypy` allow partial typing these days? Also, I haven't found the numpy or pandas type stubs to always be great. Have you run into problems around this?. I think this would also need to wait at least until we can drop python 3.6 for `anndata`, since adding types there currently means circular dependencies. > `rstcheck` to check the syntax of .rst files. I would particularly like a linter for `rst`. I noticed you also had `doc8`, but you'd recommend `rstcheck` check over this? I'm a little worried, considering its last release was over a year ago. Spell check for prose in doc-strings could also be great, but I could see this being overzealous (is there a good way to notify about misspelled words, while not being annoying about technical terms?). I'm a little worried about some custom sphinx extensions we have, and conflicting with this, any experience here?. --------------------------------------------. @Koncopd, I think I agree with your concern, as I said above: it's the worst when you want to fix a bug, but instead have to learn about configuring a linter. I also think it's very easy to add new checks, so someone complaining about new ones is valuable. Per commit, this should always be an option with `git commit --no-verify`, though you could also just not install `pre-commit`. I would like to keep the required checks limited, ideally formatting tasks that can be automated as opposed ""this is poor style"" warnings. I also know these tools can be wrong (e.g. `black` when expression's have many operators, sometimes with chaining) so it would be good to document the escape hatches for this (`# fmt: off` for `black`). That said, we already do require that merged code goes through black before it gets merged, and a benefit of using this would be to not have commit messages like ""formatting"", ""remove unused import"", etc. The pre-commit checks would be a part of CI as well, so it would be *eventually* mandatory – just not on your machine. Does this address your conce",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1563#issuecomment-754352635:1481,config,configuring,1481,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563#issuecomment-754352635,2,['config'],['configuring']
Modifiability," Gaussian kernel width is set to the distance of the; `n_neighbors` neighbor. **n_pcs** : `int` or `None`, optional (default: `None`). Use this many PCs. If `n_pcs==0` use `.X` if `use_rep is None`. **use_rep** : \{`None`, 'X'\} or any key for `.obsm`, optional (default: `None`). Use the indicated representation. If `None`, the representation is chosen; automatically: for `.n_vars` < 50, `.X` is used, otherwise 'X_pca' is used.; If 'X_pca' is not present, it's computed with default parameters. **knn** : bool, optional (default: True). If `True`, use a hard threshold to restrict the number of neighbors to; `n_neighbors`, that is, consider a knn graph. Otherwise, use a Gaussian; Kernel to assign low weights to neighbors more distant than the; `n_neighbors` nearest neighbor. **random_state** : typing.Union[int, mtrand.RandomState, NoneType]. A numpy random seed. **method** : {'umap', 'gauss', `None`} (default: `'umap'`). Use 'umap' [McInnes18]_ or 'gauss' (Gauss kernel following [Coifman05]_; with adaptive width [Haghverdi16]_) for computing connectivities. **metric** : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], float]], optional (default: 'euclidean'). A known metric’s name or a callable that returns a distance. **metric_kwds** : Mapping. Options for the metric. **copy** : bool. Return a copy instead of writing to adata. :Returns:. Depending on `copy`, updates or returns `adata` with the following:. . **connectivities** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Weighted adjacency matrix of the neighborhood graph of data; points. Weights should be interpreted as connectivities. **distances** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Instead of decaying weights, this stores distances for each pair of; neighbors.; File: ~/_hholtz/01_projects/1512_scanpy/scanpy/scanpy/neighbors/__init__.py; Type: function; ```. PS: ; - Already the [docs](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.Neighbors.compute_neighbors.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:6385,adapt,adaptive,6385,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999,2,['adapt'],['adaptive']
Modifiability," I'm getting weird errors when running scanpy on scvelo objects. I first had this issue when running `sc.pp.pca(adata_panc, n_comps=50)`, but managed to solve it by previously setting `adata_panc.X = np.array(adata_panc.X.todense())`. However, I'm now getting the exact same error when running `sc.pp.neighbors(adata_panc)` and I'm not sure which matrix to test. Any advice would be very much appreciated!. ### Minimal code sample (that we can copy&paste without having any data). ```python; adata_panc = scv.datasets.pancreas(); scv.pp.filter_and_normalize(adata_panc, n_top_genes=3000, min_shared_counts=20); del adata_panc.obsm['X_pca']; del adata_panc.obsm['X_umap']; del adata_panc.obsp['distances']; del adata_panc.obsp['connectivities']; adata_panc.X = np.array(adata_panc.X.todense()); sc.pp.pca(adata_panc, n_comps=50); sc.pp.neighbors(adata_panc); ```. ```pytb; Filtered out 20801 genes that are detected 20 counts (shared).; Normalized count data: X, spliced, unspliced.; Extracted 3000 highly variable genes.; Logarithmized X.; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); TypeError: expected dtype object, got 'numpy.dtype[float32]'. The above exception was the direct cause of the following exception:. SystemError Traceback (most recent call last); /hps/scratch/lsf_tmpdir/hl-codon-10-04/ipykernel_2322052/531027197.py in <module>; 7 adata_panc.X = np.array(adata_panc.X.todense()); 8 sc.pp.pca(adata_panc, n_comps=50); ----> 9 sc.pp.neighbors(adata_panc). /hps/software/users/marioni/Leah/miniconda3/envs/scvelo/lib/python3.8/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy); 137 adata._init_as_actual(adata.copy()); 138 neighbors = Neighbors(adata); --> 139 neighbors.compute_neighbors(; 140 n_neighbors=n_neighbors,; 141 knn=knn,. /hps/software/users/marioni/Leah/miniconda3/envs/scvelo/lib/python3",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1983:1331,variab,variable,1331,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1983,1,['variab'],['variable']
Modifiability," Obviously, the signature itself now is a mess for humans to read. But ok, that's fine if the docstring is easy to read.; - There is an error ` <class 'inspect._empty'>`; - The rest looks good to me, except for the superficial stylistic remarks above.; ```; Signature: sc.pp.neighbors(adata:anndata.base.AnnData, n_neighbors:int=15, n_pcs:Union[int, NoneType]=None, use_rep:Union[str, NoneType]=None, knn:bool=True, random_state:Union[int, mtrand.RandomState, NoneType]=0, method:str='umap', metric:Union[str, Callable[[numpy.ndarray, numpy.ndarray], float]]='euclidean', metric_kwds:Mapping[str, Any]={}, copy:bool=False) -> Union[anndata.base.AnnData, NoneType]; Docstring:; Compute a neighborhood graph of observations [McInnes18]_. The neighbor search efficiency of this heavily relies on UMAP [McInnes18]_,; which also provides a method for estimating connectivities of data points -; the connectivity of the manifold (`method=='umap'`). If `method=='diffmap'`,; connectivities are computed according to [Coifman05]_, in the adaption of; [Haghverdi16]_. :Parameters:. **adata** : AnnData, optional (default: <class 'inspect._empty'>). Annotated data matrix. **n_neighbors** : int, optional (default: 15). The size of local neighborhood (in terms of number of neighboring data; points) used for manifold approximation. Larger values result in more; global views of the manifold, while smaller values result in more local; data being preserved. In general values should be in the range 2 to 100.; If `knn` is `True`, number of nearest neighbors to be searched. If `knn`; is `False`, a Gaussian kernel width is set to the distance of the; `n_neighbors` neighbor. **n_pcs** : `int` or `None`, optional (default: `None`). Use this many PCs. If `n_pcs==0` use `.X` if `use_rep is None`. **use_rep** : \{`None`, 'X'\} or any key for `.obsm`, optional (default: `None`). Use the indicated representation. If `None`, the representation is chosen; automatically: for `.n_vars` < 50, `.X` is used, otherwis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:4818,adapt,adaption,4818,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999,2,['adapt'],['adaption']
Modifiability," Put your Error output in this code block (if applicable, else delete the block): -->; ```; In [1]: sc.pp.combat(adata_Combat, key='sample'); /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/anndata/_core/anndata.py:21: FutureWarning: pandas.core.index is deprecated and will be removed in a future version. The public classes are available in the top-level namespace.; from pandas.core.index import RangeIndex; /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).; ""(https://pypi.org/project/six/)."", FutureWarning); scanpy==1.4.6 anndata==0.7.1 umap==0.4.1 numpy==1.18.1 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0; Standardizing Data across genes. Found 11 batches. Found 0 numerical variables:; 	. Fitting L/S model and finding priors. Finding parametric adjustments. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:338: RuntimeWarning: divide by zero encountered in true_divide; change = max((abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max()); Adjusting data. In [2]: np.sum(~np.isnan(adata_Combat.X)); Out[2]: 0. In [3]: np.sum(np.isnan(adata_Combat.X)); Out[3]: 7644442. In [4]: sc.pp.highly_variable_genes(adata_Combat); extracting highly variable genes; Traceback (most recent call last):. File ""<ipython-input-4-a706aaf6f1f8>"", line 1, in <module>; sc.pp.highly_variable_genes(adata_Combat). File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py"", line 235, in highly_variable_genes; flavor=flavor,. File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py"", line 65, in _highly_variabl",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1175:1487,variab,variables,1487,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1175,1,['variab'],['variables']
Modifiability," _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _; ../../miniconda3/envs/scanpy/lib/python3.9/functools.py:888: in wrapper; return dispatch(args[0].__class__)(*args, **kw); scanpy/preprocessing/_simple.py:888: in scale_anndata; X, adata.var[""mean""], adata.var[""std""] = do_scale(; ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args; error_rewrite(e, 'typing'); _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'); issue_type = 'typing'. def error_rewrite(e, issue_type):; """"""; Rewrite and raise Exception `e` with help supplied based on the; specified issue_type.; """"""; if config.SHOW_HELP:; help_msg = errors.error_extras[issue_type]; e.patch_message('\n'.join((str(e).rstrip(), help_msg))); if config.FULL_TRACEBACKS:; raise e; else:; > raise e.with_traceback(None); E numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend); E non-precise type pyobject; E During: typing of argument at /home/zeth/PycharmProjects/scanpy/scanpy/preprocessing/_simple.py (763); E ; E File ""scanpy/preprocessing/_simple.py"", line 763:; E def do_scale(X, maxv, nthr):; E <source elided>; E # t0= time.time(); E s = np.zeros((nthr, X.shape[1])); E ^ ; E ; E This error may have been caused by the following argument(s):; E - argument 0: Cannot determine Numba type of <class 'scipy.sparse._csr.csr_matrix'>. ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:409: TypingError; ```. When trying to use the new flavor with the existing test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2457#issuecomment-1533308183:1713,Rewrite,Rewrite,1713,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457#issuecomment-1533308183,3,"['Rewrite', 'config']","['Rewrite', 'config']"
Modifiability," _; > ../../miniconda3/envs/scanpy/lib/python3.9/functools.py:888: in wrapper; > return dispatch(args[0].__class__)(*args, **kw); > scanpy/preprocessing/_simple.py:888: in scale_anndata; > X, adata.var[""mean""], adata.var[""std""] = do_scale(; > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args; > error_rewrite(e, 'typing'); > _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _; > ; > e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'); > issue_type = 'typing'; > ; > def error_rewrite(e, issue_type):; > """"""; > Rewrite and raise Exception `e` with help supplied based on the; > specified issue_type.; > """"""; > if config.SHOW_HELP:; > help_msg = errors.error_extras[issue_type]; > e.patch_message('\n'.join((str(e).rstrip(), help_msg))); > if config.FULL_TRACEBACKS:; > raise e; > else:; > > raise e.with_traceback(None); > E numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend); > E non-precise type pyobject; > E During: typing of argument at /home/zeth/PycharmProjects/scanpy/scanpy/preprocessing/_simple.py (763); > E ; > E File ""scanpy/preprocessing/_simple.py"", line 763:; > E def do_scale(X, maxv, nthr):; > E <source elided>; > E # t0= time.time(); > E s = np.zeros((nthr, X.shape[1])); > E ^ ; > E ; > E This error may have been caused by the following argument(s):; > E - argument 0: Cannot determine Numba type of <class 'scipy.sparse._csr.csr_matrix'>; > ; > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:409: TypingError; > ```; > ; > When trying to use the new flavor with the existing test. Hi @Zethson ,; We are not able t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2457#issuecomment-1540006717:1893,config,config,1893,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457#issuecomment-1540006717,1,['config'],['config']
Modifiability," ```python; > sc.pl.heatmap(; > pbmc,; > var_names=[""LDHB"", ""LYZ"", ""CD79A""],; > row_groups=""louvain"",; > col_groups=""sampleid""; > ); > ```; > ; > ![image](https://user-images.githubusercontent.com/8238804/144902398-e967c1db-53c1-4b44-bcbf-8dfedcf06e58.png); > ; > What do you think about that?. Thanks @ivirshup !. I like these lines you suggested- perhaps I can adopt to make it more elegant when creating color_df/size_df:; ```; import scanpy as sc, pandas as pd, numpy as np. pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(); pbmc.obs[""sampleid""] = np.repeat([""s1"", ""s2""], pbmc.n_obs / 2); df = sc.get.obs_df(pbmc, [""LDHB"", ""louvain"", ""sampleid""]). summarized = df.pivot_table(; index=[""louvain"", ""sampleid""],; values=""LDHB"",; aggfunc=[np.mean, np.count_nonzero]; ); color_df = summarized[""mean""].unstack(); size_df = summarized[""count_nonzero""].unstack(). # I don't think the var_names or groupby variables are actually important here; sc.pl.DotPlot(; pbmc,; var_names=""LDHB"", groupby=[""louvain"", ""sampleid""], # Just here so it doesn't error; dot_color_df=color_df, dot_size_df=size_df,; ).style(cmap=""Reds"").show(); ```; this is the output:; ![image](https://user-images.githubusercontent.com/10910559/145053489-c550d5a7-a8fe-4a61-b672-9103ccf1d228.png); some work are needed to modify the grid/axis size, legend and scale. Actually this is the reason I work on top of the _dotplot and _baseplot function/ classes to implement the solution- to make the plots the same style with scanpy dotplot without doing too much work on the cosmetics. But I can certainly change grouby_expand from bool to an actual variable `group_cols` as you suggested in #2055 . Or should we call it `col_groups` as you did in your sc.pl.heatmap pseudo code? ; I'd be more than happy to make it more generalized, i.e., to sc.pl.heatmap, but I may need some time to understand sc.pl.heatmap first. The plotting functions are getting really complex- it took me some time to understand _dotplot and _baseplot :). Thanks",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1876#issuecomment-988045664:3878,variab,variable,3878,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1876#issuecomment-988045664,1,['variab'],['variable']
Modifiability," available optional dependencies we run our tests with:. https://github.com/scverse/scanpy/blob/06802b459648a219a10f74243efe4d6c2f912016/scanpy/tests/test_preprocessing_distributed.py#L15-L22. now, the dask tests are enabled and only the zappy tests are disabled:. https://github.com/scverse/scanpy/blob/6b9e734f4979a8ba450c0eaa052451f98b000753/scanpy/tests/test_preprocessing_distributed.py#L18-L31. ### Minimal code sample. ```bash; pytest -v --disable-warnings -k test_normalize_per_cell[dask] --runxfail; ```. ### Error output. ```pytb; ===================================================================================================== test session starts ======================================================================================================; platform linux -- Python 3.8.17, pytest-7.3.1, pluggy-1.0.0 -- /home/phil/Dev/Python/venvs/single-cell/bin/python; cachedir: .pytest_cache; rootdir: /home/phil/Dev/Python/Single Cell/scanpy; configfile: pyproject.toml; testpaths: scanpy; plugins: cov-4.1.0, nunit-1.0.3, memray-1.4.0, xdist-3.3.1; collected 986 items / 985 deselected / 1 selected . scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] FAILED [100%]. =========================================================================================================== FAILURES ===========================================================================================================; __________________________________________________________________________________ TestPreprocessingDistributed.test_normalize_per_cell[dask] __________________________________________________________________________________. self = <scanpy.tests.test_preprocessing_distributed.TestPreprocessingDistributed object at 0x7fdd21f2e9d0>, adata = AnnData object with n_obs × n_vars = 9999 × 1000; obs: 'n_counts'; var: 'gene_ids'; adata_dist = AnnData object with n_obs × n_vars = 9999 × 1000; obs: 'n_counts'; var: 'gene_ids'; uns: 'dist-",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2526:1431,plugin,plugins,1431,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526,1,['plugin'],['plugins']
Modifiability," compression='gzip')` , and then read it back again from disk, the subsetting doesn't work. It only happens after saving to disk, and it seems to only happen after I've run additional analysis (UMAP, rank_genes_groups, etc.... I don't know exactly which entry is the one breaking the writing/reading). ; This is the error I get:. > Traceback (most recent call last):; > File ""<input>"", line 48, in <module>; > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__; > return self._getitem_view(index); > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view; > return AnnData(self, oidx=oidx, vidx=vidx, asview=True); > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 561, in __init__; > self._init_as_view(X, oidx, vidx); > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view; > self._raw = adata_ref.raw[oidx]; > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 344, in __getitem__; > new._varm = self._varm._view(self, vidx); > AttributeError: 'dict' object has no attribute '_view'. Any subsetting (of observations or variables) gives me this error. I tried removing keys from `.uns`, etc. , but nothing seems to work. These are the versions I'm running, all installed with conda:. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.2 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. I also tried using different (or no) compression to save and read, but that didn't help either. I don't know if it's related to the recent problems with h5py, but either way I am using h5py=2.9.0 and hdf5=1.10.5. Hopefully someone can help! Let me know if I should post this in the anndata repository instead.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/884:1537,variab,variables,1537,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/884,1,['variab'],['variables']
Modifiability, conda-forge; numpy 1.24.4 py310ha4c1d20_0 conda-forge; numpyro 0.12.1 pyhd8ed1ab_0 conda-forge; openh264 2.1.1 h780b84a_0 conda-forge; openjpeg 2.5.0 hfec8fc6_2 conda-forge; openpyxl 3.1.2 py310h2372a71_0 conda-forge; openssl 3.1.1 hd590300_1 conda-forge; opt_einsum 3.3.0 pyhd8ed1ab_1 conda-forge; optax 0.1.5 pyhd8ed1ab_0 conda-forge; ordered-set 4.1.0 pyhd8ed1ab_0 conda-forge; orjson 3.9.2 py310h1e2579a_0 conda-forge; packaging 23.1 pyhd8ed1ab_0 conda-forge; pandas 2.0.3 py310h7cbd5c2_1 conda-forge; parso 0.8.3 pyhd8ed1ab_0 conda-forge; patsy 0.5.3 pyhd8ed1ab_0 conda-forge; pcre2 10.40 hc3806b6_0 conda-forge; pexpect 4.8.0 pyh1a96a4e_2 conda-forge; pickleshare 0.7.5 py_1003 conda-forge; pillow 9.4.0 py310h023d228_1 conda-forge; pip 23.2 pyhd8ed1ab_0 conda-forge; pkginfo 1.9.6 pyhd8ed1ab_0 conda-forge; pkgutil-resolve-name 1.3.10 pyhd8ed1ab_0 conda-forge; platformdirs 3.9.1 pyhd8ed1ab_0 conda-forge; poetry 1.5.1 linux_pyhd8ed1ab_0 conda-forge; poetry-core 1.6.1 pyhd8ed1ab_0 conda-forge; poetry-plugin-export 1.4.0 pyhd8ed1ab_0 conda-forge; pooch 1.7.0 pyha770c72_3 conda-forge; prompt-toolkit 3.0.39 pyha770c72_0 conda-forge; prompt_toolkit 3.0.39 hd8ed1ab_0 conda-forge; psutil 5.9.5 py310h1fa729e_0 conda-forge; pthread-stubs 0.4 h36c2ea0_1001 conda-forge; ptyprocess 0.7.0 pyhd3deb0d_0 conda-forge; pure_eval 0.2.2 pyhd8ed1ab_0 conda-forge; pycparser 2.21 pyhd8ed1ab_0 conda-forge; pydantic 2.0.3 pyhd8ed1ab_1 conda-forge; pydantic-core 2.3.0 py310hcb5633a_0 conda-forge; pygments 2.15.1 pyhd8ed1ab_0 conda-forge; pyjwt 2.8.0 pyhd8ed1ab_0 conda-forge; pynndescent 0.5.10 pyh1a96a4e_0 conda-forge; pyopenssl 23.2.0 pyhd8ed1ab_1 conda-forge; pyparsing 3.1.0 pyhd8ed1ab_0 conda-forge; pyproject_hooks 1.0.0 pyhd8ed1ab_0 conda-forge; pyro-api 0.1.2 pyhd8ed1ab_0 conda-forge; pyro-ppl 1.8.4 pyhd8ed1ab_0 conda-forge; pysocks 1.7.1 pyha2e5f31_6 conda-forge; python 3.10.12 hd12c33a_0_cpython conda-forge; python-build 0.10.0 pyhd8ed1ab_1 conda-forge; python-dateutil 2.8.2 pyhd8ed1ab_0 co,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2480#issuecomment-1646783205:17396,plugin,plugin-export,17396,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480#issuecomment-1646783205,2,['plugin'],['plugin-export']
Modifiability," data communication overhead. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in euclidean_distances(X, Y, Y_norm_squared, squared, X_norm_squared); 230 paired_distances : distances betweens pairs of elements of X and Y.; 231 """"""; --> 232 X, Y = check_pairwise_arrays(X, Y); 233 ; 234 # If norms are passed as float32, they are unused. If arrays are passed as. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in check_pairwise_arrays(X, Y, precomputed, dtype); 107 if Y is X or Y is None:; 108 X = Y = check_array(X, accept_sparse='csr', dtype=dtype,; --> 109 estimator=estimator); 110 else:; 111 X = check_array(X, accept_sparse='csr', dtype=dtype,. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/utils/validation.py in check_array(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator); 519 ""Reshape your data either using array.reshape(-1, 1) if ""; 520 ""your data has a single feature or array.reshape(1, -1) ""; --> 521 ""if it contains a single sample."".format(array)); 522 ; 523 # in the future np.flexible dtypes will be handled like object dtypes. ValueError: Expected 2D array, got 1D array instead:; array=[0. 0. 1. ... 0. 3. 0.].; Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.; ```. To reproduce:; ```python; import scanpy as sc. adata = sc.datasets.paul15(); sc.pp.neighbors(adata[:, 0]); ```; Not using `.copy()` doesn't seem to be the cause of the problem, since I've tried that and it still crashes.; My versions are:; ```python; scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.8 numpy==1.16.4 scipy==1.3.1 pandas==0.25.1 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1; ```. When running `sc.pp.neighbors(adata[:, :2])`, it works as expected.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/837:3013,flexible,flexible,3013,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/837,1,['flexible'],['flexible']
Modifiability," datasets, but I also used the data from here, https://github.com/theislab/single-cell-tutorial/blob/master/latest_notebook/Case-study_Mouse-intestinal-epithelium_1906.ipynb, and got the same error. ```; adata_mnn = adata.copy(); adata_list = [adata_mnn[adata_mnn.obs['sample'] == i] for i in adata_mnn.obs['sample'].unique()]; adata_list; ```. ```; [View of AnnData object with n_obs × n_vars = 2267 × 12818; obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'; var: 'gene_id', 'n_cells'; uns: 'log1p'; layers: 'counts',; View of AnnData object with n_obs × n_vars = 1976 × 12818; obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'; var: 'gene_id', 'n_cells'; uns: 'log1p'; layers: 'counts',; View of AnnData object with n_obs × n_vars = 1663 × 12818; obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'; var: 'gene_id', 'n_cells'; uns: 'log1p'; layers: 'counts',; View of AnnData object with n_obs × n_vars = 2356 × 12818; obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'; var: 'gene_id', 'n_cells'; uns: 'log1p'; layers: 'counts',; View of AnnData object with n_obs × n_vars = 2422 × 12818; obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'; var: 'gene_id', 'n_cells'; uns: 'log1p'; layers: 'counts',; View of AnnData object with n_obs × n_vars = 1773 × 12818; obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'; var: 'gene_id', 'n_cells'; uns: 'log1p'; layers: 'counts']; ```. ```; import mnnpy; corrected = mnnpy.mnn_correct(*adata_list, batch_key=""sample""); ```. ```; ---------------------------------------------------------------------------; IndexError Traceback (most recent call last); <ipython-input-35-7ad830fcd907> in <module>; 1 import mnnpy; ----> 2 corrected = mnnpy.mnn_correct(*adata_list, batch_key=""samp",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1367#issuecomment-674176537:575,layers,layers,575,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1367#issuecomment-674176537,6,['layers'],['layers']
Modifiability," emerging and will continue to emerge. Consequently, this necessitates an expansion of the scanpy API. However, I argue that having flat top-level modules makes it difficult to extend scanpy, while maintaining a reasonable API. . Right now there are two ways to introduce new functionality (assuming that it's not something completely unrelated). 1) add a new flavor/method to an existing function (e.g. `sc.pp.highly_variable_genes`, `sc.tl.rank_genes_groups`) or . 2) add a new function with a shared prefix e.g. `sc.pp.neighbors_tsne` (see https://github.com/theislab/scanpy/pull/1561) or `sc.pp.normalize_pearson_residuals` (see https://github.com/berenslab/umi-normalization/issues/1) or `sc.pp.normalize_pearson_residuals_pca()` (see #1715 ). . Since option 1 is more complicated in terms of managing the arguments (esp. method-specific ones), I believe we tend to switch to option 2 now. But given that we already have many functions with common prefixes and that shifting towards option 2 will likely introduce more functions with long underscored names, top layers will get even flatter and wider. Therefore, I think it's time to consider a third option which is to add another layer which makes the API a tiny bit more hierarchical. Some examples I can think of are:. ```java; sc.read.{adata,csv,text,mtx,excel,loom,h5_10x,mtx_10x,...}; sc.pp.neighbors.{umap,gauss,rapids,tsne}; sc.pp.hvg.{seurat,seurat_v3,dispersion}; sc.pp.norm.{tpm,pearson}; sc.pp.filter.{genes,cells,rank_genes,...}; sc.tl.rank_genes.{logreg,wilcoxon,ttest}; sc.tl.cluster.{leiden,louvain}; sc.tl.score.{genes,cell_cycle}; sc.pl.rank_genes.{dotplot,matrixplot,...}; sc.pl.groups.{dot,matrix,violin,...}; sc.pl.embed.{umap,tsne,pca,...}; ```. There are a few issues I can think of. 1. I can imagine some resistance from some developers due to losing a few milliseconds by typing more characters 😄 but if you imagine the long term effects of option 2, I think this might save you some time 😛 . 1. What happens to the fun",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1739:1526,layers,layers,1526,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1739,1,['layers'],['layers']
Modifiability, filelock 3.0.12 pyhd3eb1b0_1 ; flake8 3.9.0 pyhd3eb1b0_0 ; flask 1.1.2 pyhd3eb1b0_0 ; fontconfig 2.13.1 h6c09931_0 ; freetype 2.10.4 h5ab3b9f_0 ; fribidi 1.0.10 h7b6447c_0 ; fsspec 0.9.0 pyhd3eb1b0_0 ; funcargparse 0.2.3 pypi_0 pypi; future 0.18.2 py38_1 ; future_fstrings 1.2.0 py38h32f6830_2 conda-forge; gcc_impl_linux-64 7.3.0 habb00fd_1 ; gcc_linux-64 7.3.0 h553295d_15 ; geosketch 1.2 pypi_0 pypi; get_terminal_size 1.0.0 haa9412d_0 ; get_version 2.1 py_1 conda-forge; gevent 21.1.2 py38h27cfd23_1 ; gfortran_impl_linux-64 7.3.0 hdf63c60_1 ; gfortran_linux-64 7.3.0 h553295d_15 ; glib 2.63.1 h5a9c865_0 ; glob2 0.7 pyhd3eb1b0_0 ; gmp 6.2.1 h2531618_2 ; gmpy2 2.0.8 py38hd5f6e3b_3 ; google-api-core 1.27.0 pypi_0 pypi; google-auth 1.30.0 pypi_0 pypi; googleapis-common-protos 1.53.0 pypi_0 pypi; gpustat 0.6.0 pypi_0 pypi; graphite2 1.3.14 h23475e2_0 ; graphtools 1.5.2 pypi_0 pypi; graphviz 2.40.1 h21bd128_2 ; greenlet 1.1.0 py38h2531618_0 ; grpcio 1.37.1 pypi_0 pypi; gsl 2.4 h14c3975_4 ; gst-plugins-base 1.14.0 hbbd80ab_1 ; gstreamer 1.14.0 hb453b48_1 ; gxx_impl_linux-64 7.3.0 hdf63c60_1 ; gxx_linux-64 7.3.0 h553295d_15 ; h5py 3.2.1 nompi_py38h9915d05_100 conda-forge; harfbuzz 1.8.8 hffaf4a1_0 ; harmonypy 0.0.5 pypi_0 pypi; harmonyts 0.1.4 pypi_0 pypi; hdf5 1.10.6 nompi_h3c11f04_101 conda-forge; heapdict 1.0.1 py_0 ; hiredis 2.0.0 pypi_0 pypi; html5lib 1.1 py_0 ; icu 58.2 he6710b0_3 ; idna 2.10 pyhd3eb1b0_0 ; igraph 0.7.1 h2166141_1005 conda-forge; imageio 2.9.0 pyhd3eb1b0_0 ; imagesize 1.2.0 pyhd3eb1b0_0 ; importlib-metadata 3.10.0 py38h06a4308_0 ; importlib_metadata 3.10.0 hd3eb1b0_0 ; iniconfig 1.1.1 pyhd3eb1b0_0 ; intel-openmp 2021.2.0 h06a4308_610 ; intervaltree 2.1.0 pypi_0 pypi; ipykernel 5.3.4 py38h5ca1d4c_0 ; ipython 7.22.0 py38hb070fc8_0 ; ipython_genutils 0.2.0 pyhd3eb1b0_1 ; ipywidgets 7.6.3 pyhd3eb1b0_1 ; isort 5.8.0 pyhd3eb1b0_0 ; itsdangerous 2.0.1 pyhd3eb1b0_0 ; jbig 2.1 hdba287a_0 ; jdcal 1.4.1 py_0 ; jedi 0.17.2 py38h06a4308_1 ; jeepney 0.6.0 pyhd3eb1b0,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:7925,plugin,plugins-base,7925,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310,2,['plugin'],['plugins-base']
Modifiability," for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python; adata.obs['sex'].cat.categories.tolist(); ```. ```pytb; ['F', 'M', 'U']; ```. ```python; adata.obs['age_groups'].cat.categories.tolist(); ```. ```pytb; ['Old', 'YoungAdult', 'Pediatric', 'Fetal', 'NewBorn']; ```. ```python; sc.pp.combat(adata, key='384plate', covariates=['sex']); ```. ```pytb; Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:; 	sex. Found 0 numerical variables:; 	. Fitting L/S model and finding priors. Finding parametric adjustments. /home/sinhar/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py:340: RuntimeWarning: divide by zero encountered in true_divide; (abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max(); Adjusting data; ```. ```python; sc.pp.combat(adata, key='384plate', covariates=['age_group']); ```. ```pytb; Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:; 	age_group. Found 0 numerical variables:; 	. ---------------------------------------------------------------------------; LinAlgError Traceback (most recent call last); <timed eval> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace); 204 # standardize across genes using a pooled variance estimator; 205 logg.info(""Standardizing Data across genes.\n""); --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key); 207 ; 208 # fitting the parameters on the standardized data. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key); 102 ; 103 # compute pooled variance estimator; --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T); 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]); 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1606:1385,variab,variables,1385,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606,1,['variab'],['variables']
Modifiability," for {key}, ""; 146 f""since a writer for type {type(value)} has not been implemented yet."". NotImplementedError: Failed to write value for uns/umap/params/random_state, since a writer for type <class 'numpy.random.mtrand.RandomState'> has not been implemented yet. The above exception was the direct cause of the following exception:. NotImplementedError Traceback (most recent call last); <ipython-input-2-1dd6b1c7e996> in <module>; 4 pbmc = sc.datasets.pbmc68k_reduced(); 5 sc.tl.umap(pbmc, random_state=np.random.RandomState(10)); ----> 6 pbmc.write(""tmp.h5ad""). ~/github/anndata/anndata/_core/anndata.py in write_h5ad(self, filename, compression, compression_opts, force_dense, as_dense); 1988 compression_opts=compression_opts,; 1989 force_dense=force_dense,; -> 1990 as_dense=as_dense,; 1991 ); 1992 . ~/github/anndata/anndata/_io/h5ad.py in write_h5ad(filepath, adata, force_dense, as_dense, dataset_kwargs, **kwargs); 110 write_attribute(f, ""varp"", adata.varp, dataset_kwargs=dataset_kwargs); 111 write_attribute(f, ""layers"", adata.layers, dataset_kwargs=dataset_kwargs); --> 112 write_attribute(f, ""uns"", adata.uns, dataset_kwargs=dataset_kwargs); 113 if adata.isbacked:; 114 adata.file.open(filepath, ""r+""). /usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/functools.py in wrapper(*args, **kw); 838 '1 positional argument'); 839 ; --> 840 return dispatch(args[0].__class__)(*args, **kw); 841 ; 842 funcname = getattr(func, '__name__', 'singledispatch function'). ~/github/anndata/anndata/_io/h5ad.py in write_attribute_h5ad(f, key, value, *args, **kwargs); 124 if key in f:; 125 del f[key]; --> 126 _write_method(type(value))(f, key, value, *args, **kwargs); 127 ; 128 . ~/github/anndata/anndata/_io/h5ad.py in write_mapping(f, key, value, dataset_kwargs); 284 def write_mapping(f, key, value, dataset_kwargs=MappingProxyType({})):; 285 for sub_key, sub_value in value.items():; --> 286 write_attribute(f, f""{key}/{sub_key}"", sub_value, dataset_kwargs=d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1131:2111,layers,layers,2111,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1131,1,['layers'],['layers']
Modifiability," import filter_genes_dispersion, filter_genes_cv_deprecated; 8 from ._normalization import normalize_total. ~/.conda/envs/scanpy/lib/python3.8/site-packages/scanpy/preprocessing/_simple.py in; 8 from typing import Union, Optional, Tuple, Collection, Sequence, Iterable; 9; ---> 10 import numba; 11 import numpy as np; 12 import scipy as sp. ~/.conda/envs/scanpy/lib/python3.8/site-packages/numba/init.py in; 32; 33 # Re-export decorators; ---> 34 from numba.core.decorators import (cfunc, generated_jit, jit, njit, stencil,; 35 jit_module); 36. ~/.conda/envs/scanpy/lib/python3.8/site-packages/numba/core/decorators.py in; 10; 11 from numba.core.errors import DeprecationError, NumbaDeprecationWarning; ---> 12 from numba.stencils.stencil import stencil; 13 from numba.core import config, extending, sigutils, registry; 14. ~/.conda/envs/scanpy/lib/python3.8/site-packages/numba/stencils/stencil.py in; 9 from llvmlite import ir as lir; 10; ---> 11 from numba.core import types, typing, utils, ir, config, ir_utils, registry; 12 from numba.core.typing.templates import (CallableTemplate, signature,; 13 infer_global, AbstractTemplate). ~/.conda/envs/scanpy/lib/python3.8/site-packages/numba/core/registry.py in; 2; 3 from numba.core.descriptors import TargetDescriptor; ----> 4 from numba.core import utils, typing, dispatcher, cpu; 5; 6 # -----------------------------------------------------------------------------. ~/.conda/envs/scanpy/lib/python3.8/site-packages/numba/core/dispatcher.py in; 13; 14 from numba import _dispatcher; ---> 15 from numba.core import utils, types, errors, typing, serialize, config, compiler, sigutils; 16 from numba.core.compiler_lock import global_compiler_lock; 17 from numba.core.typeconv.rules import default_type_manager. ~/.conda/envs/scanpy/lib/python3.8/site-packages/numba/core/compiler.py in; 11 from numba.core.environment import lookup_environment; 12; ---> 13 from numba.core.compiler_machinery import PassManager; 14; 15 from numba.core.untyped_passes i",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1797:2343,config,config,2343,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1797,1,['config'],['config']
Modifiability," is well defined; 53 return self.accessor_cls; ---> 54 return self.construct_accessor(instance); 55 ; 56 def __set__(self, instance, value):. ~/anaconda3/lib/python3.6/site-packages/pandas/core/categorical.py in _make_accessor(cls, data); 2209 def _make_accessor(cls, data):; 2210 if not is_categorical_dtype(data.dtype):; -> 2211 raise AttributeError(""Can only use .cat accessor with a ""; 2212 ""'category' dtype""); 2213 return CategoricalAccessor(data.values, data.index,. AttributeError: Can only use .cat accessor with a 'category' dtype; ```. Then, I comment out the respective line of code, run the whole thing again, and it works. And when I uncomment the line it works fine again. When I comment the line for the first time, I get a couple of lines displayed in the output saying:; > ... 'donor' was turned into a categorical variable; > ... 'gene_symbols' was turned into a categorical variable. or something like that... My theory is that sanitize_anndata() detects that these variables should be categorical variables and tries to convert them into categoricals. As this sc.pl.scatter call is the first time sanitize_anndata() is called after the variables are read in, this is the first time this conversion would take place. However, I am calling the sc.pl.scatter() on a subsetted anndata object, so it somehow cannot do the conversion. Once I call sc.pl.scatter on a non-subsetted anndata object once, the conversion can take place and I can subsequently call sc.pl.scatter also on a subsetted anndata object. If this is true, I can see why this is happening. However I feel this behaviour will be quite puzzling to a typical user. Maybe sanitize_anndata() should be called before plotting (probably hard to implement), or the plotting functions should have a parameter to plot only a subset of the data. That way sanitize_anndata can be called on the whole anndata object every time as there is no longer a reason to pass a view of the object. You could then test if a view is being pas",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/166:3490,variab,variables,3490,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/166,2,['variab'],['variables']
Modifiability," metric, metric_kwds, verbose); 984 initial_alpha,; 985 negative_sample_rate,; --> 986 verbose=verbose,; 987 ); 988 . /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws); 348 e.patch_message(msg); 349 ; --> 350 error_rewrite(e, 'typing'); 351 except errors.UnsupportedError as e:; 352 # Something unsupported is present in the user code, add help info. /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type); 315 raise e; 316 else:; --> 317 reraise(type(e), e, None); 318 ; 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb); 656 value = tp(); 657 if value.__traceback__ is not tb:; --> 658 raise value.with_traceback(tb); 659 raise value; 660 . TypingError: Failed in nopython mode pipeline (step: nopython frontend); Invalid use of type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)); Known signatures:; * (array(float32, 1d, A), array(float32, 1d, A)) -> float32; * parameterized; [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)); [2] During: typing of call at /opt/conda/lib/python3.7/site-packages/umap/umap_.py (776). File ""../../../opt/conda/lib/python3.7/site-packages/umap/umap_.py"", line 776:; def optimize_layout(; <source elided>. dist_squared = rdist(current, other); ^. This is not usually a problem with Numba itself but instead often caused by; the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:; http://numba.pydata.org/numba-doc/dev/reference/pysupported.html; and; http://numba.pydata.org/numba-doc/dev/reference/numpysupported.html. For more information about typing errors and how to debug them visit:; http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please repor",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/666:2339,parameteriz,parameterized,2339,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666,1,['parameteriz'],['parameterized']
Modifiability," mtx.indptr, np.array(ns, dtype=np.int); 367 ); 368 else:. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws); 399 e.patch_message(msg); 400 ; --> 401 error_rewrite(e, 'typing'); 402 except errors.UnsupportedError as e:; 403 # Something unsupported is present in the user code, add help info. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type); 342 raise e; 343 else:; --> 344 reraise(type(e), e, None); 345 ; 346 argtypes = []. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/six.py in reraise(tp, value, tb); 666 value = tp(); 667 if value.__traceback__ is not tb:; --> 668 raise value.with_traceback(tb); 669 raise value; 670 . TypingError: Failed in nopython mode pipeline (step: nopython mode backend); Failed in nopython mode pipeline (step: nopython frontend); Invalid use of Function(<intrinsic wrap_index>) with argument(s) of type(s): (int32, int64); * parameterized; In definition 0:; ValueError: Argument types for wrap_index must match; raised from /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/array_analysis.py:72; In definition 1:; ValueError: Argument types for wrap_index must match; raised from /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/array_analysis.py:72; This error is usually caused by passing an argument of a type that is unsupported by the named function.; [1] During: resolving callee type: Function(<intrinsic wrap_index>); [2] During: typing of call at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (399). File ""../../../../../packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py"", line 399:; def top_segment_proportions_sparse_csr(data, indptr, ns):; <source elided>; start, end = indptr[i], indptr[i + 1]; sums[i",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/978:2609,parameteriz,parameterized,2609,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978,1,['parameteriz'],['parameterized']
Modifiability," natsort 7.1.1; nbclient 0.6.4; nbconvert 6.5.0; nbformat 5.4.0; nest-asyncio 1.5.5; networkx 2.5; notebook 6.4.11; numba 0.52.0; numexpr 2.7.3; numpy 1.19.5; numpy-groupies 0.9.17; numpyro 0.9.2; oauthlib 3.2.0; openpyxl 3.0.10; opt-einsum 3.3.0; optax 0.1.2; packaging 20.9; pandas 1.2.0; pandocfilters 1.5.0; parso 0.8.2; pathos 0.2.7; patsy 0.5.1; pexpect 4.8.0; pickleshare 0.7.5; Pillow 9.1.1; pip 21.1.1; pox 0.2.9; ppft 1.6.6.3; prometheus-client 0.14.1; prompt-toolkit 3.0.18; protobuf 3.19.0; protobuf3-to-dict 0.1.5; ptyprocess 0.7.0; pyasn1 0.4.8; pyasn1-modules 0.2.8; pycosat 0.6.3; pycparser 2.20; pyDeprecate 0.3.1; Pygments 2.9.0; pyOpenSSL 20.0.1; pyparsing 2.4.7; pyro-api 0.1.2; pyro-ppl 1.8.1; pyrsistent 0.18.1; PySocks 1.7.1; python-dateutil 2.8.1; python-igraph 0.9.1; pytorch-lightning 1.5.10; pytz 2021.1; PyWavelets 1.3.0; PyYAML 6.0; pyzmq 22.0.3; requests 2.25.1; requests-oauthlib 1.3.1; rich 12.4.4; rpy2 3.4.2; rsa 4.8; ruamel-yaml-conda 0.15.80; ruamel.yaml 0.17.21; ruamel.yaml.clib 0.2.6; s3transfer 0.4.2; sagemaker 2.39.0.post0; scanpy 1.6.1; scikit-image 0.19.2; scikit-learn 0.24.2; scikit-misc 0.1.4; scipy 1.6.0; scrublet 0.2.3; scvi-tools 0.16.2; seaborn 0.11.1; Send2Trash 1.8.0; setuptools 59.5.0; setuptools-scm 6.0.1; sinfo 0.3.1; six 1.15.0; smdebug-rulesconfig 1.0.1; soupsieve 2.3.2.post1; spectra 0.0.11; statsmodels 0.12.2; stdlib-list 0.8.0; tables 3.6.1; tensorboard 2.9.0; tensorboard-data-server 0.6.1; tensorboard-plugin-wit 1.8.1; terminado 0.15.0; texttable 1.6.3; threadpoolctl 2.1.0; tifffile 2021.11.2; tinycss2 1.1.1; toolz 0.11.2; torch 1.11.0; torchmetrics 0.9.0; tornado 6.1; tqdm 4.60.0; traitlets 5.2.2.post1; typing-extensions 4.2.0; tzlocal 2.1; umap-learn 0.4.6; urllib3 1.26.4; wcwidth 0.2.5; webencodings 0.5.1; Werkzeug 2.1.2; wheel 0.36.2; widgetsnbextension 3.6.0; yarl 1.7.2; zipp 3.4.1; Note: you may need to restart the kernel to use updated packages."". </details>. Has anyone found any solution to work around this issue?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1351#issuecomment-1146346336:5138,plugin,plugin-wit,5138,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351#issuecomment-1146346336,2,['plugin'],['plugin-wit']
Modifiability," node_size_scale, node_size_power, edge_width_scale, normalize_to_color, title, pos, cmap, frameon, min_edge_width, max_edge_width, export_to_gexf, cax, colorbar, use_raw, cb_kwds, single_component, arrowsize); 612 adata_gene = adata.raw[:, colors]; 613 else:; --> 614 adata_gene = adata[:, colors]; 615 x_color.append(np.mean(adata_gene.X[subset])); 616 colors = x_color. C:\ProgramData\Anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index); 1307 def __getitem__(self, index):; 1308 """"""Returns a sliced view of the object.""""""; -> 1309 return self._getitem_view(index); 1310 ; 1311 def _getitem_view(self, index):. C:\ProgramData\Anaconda3\lib\site-packages\anndata\base.py in _getitem_view(self, index); 1311 def _getitem_view(self, index):; 1312 oidx, vidx = self._normalize_indices(index); -> 1313 return AnnData(self, oidx=oidx, vidx=vidx, asview=True); 1314 ; 1315 def _remove_unused_categories(self, df_full, df_sub, uns):. C:\ProgramData\Anaconda3\lib\site-packages\anndata\base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx); 662 if not isinstance(X, AnnData):; 663 raise ValueError('`X` has to be an AnnData object.'); --> 664 self._init_as_view(X, oidx, vidx); 665 else:; 666 self._init_as_actual(. C:\ProgramData\Anaconda3\lib\site-packages\anndata\base.py in _init_as_view(self, adata_ref, oidx, vidx); 723 self._X = None; 724 else:; --> 725 self._init_X_as_view(); 726 ; 727 self._layers = AnnDataLayers(self, adata_ref=adata_ref, oidx=oidx, vidx=vidx). C:\ProgramData\Anaconda3\lib\site-packages\anndata\base.py in _init_X_as_view(self); 750 shape = (; 751 get_n_items_idx(self._oidx, self._adata_ref.n_obs),; --> 752 get_n_items_idx(self._vidx, self._adata_ref.n_vars); 753 ); 754 if np.isscalar(X):. C:\ProgramData\Anaconda3\lib\site-packages\anndata\utils.py in get_n_items_idx(idx, l); 148 return 1; 149 else:; --> 150 return len(idx). TypeError: object of type 'numpy.int64' has no len(); ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/445:2350,layers,layers,2350,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/445,1,['layers'],['layers']
Modifiability," object with n_obs × n_vars = 29322 × 19860. ```python; >>> tiss[tiss.obs['cell_ontology_class']=='B cell']; ```. ```pytb; IndexError Traceback (most recent call last); <ipython-input-269-28b4524131cb> in <module>(); ----> 1 tiss[tiss.obs['cell_ontology_class']=='B cell']. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index); 1299 def __getitem__(self, index):; 1300 """"""Returns a sliced view of the object.""""""; -> 1301 return self._getitem_view(index); 1302 ; 1303 def _getitem_view(self, index):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index); 1303 def _getitem_view(self, index):; 1304 oidx, vidx = self._normalize_indices(index); -> 1305 return AnnData(self, oidx=oidx, vidx=vidx, asview=True); 1306 ; 1307 def _remove_unused_categories(self, df_full, df_sub, uns):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx); 662 if not isinstance(X, AnnData):; 663 raise ValueError('`X` has to be an AnnData object.'); --> 664 self._init_as_view(X, oidx, vidx); 665 else:; 666 self._init_as_actual(. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _init_as_view(self, adata_ref, oidx, vidx); 713 raise KeyError('Unknown Index type'); 714 # fix categories; --> 715 self._remove_unused_categories(adata_ref.obs, obs_sub, uns_new); 716 self._remove_unused_categories(adata_ref.var, var_sub, uns_new); 717 # set attributes. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _remove_unused_categories(self, df_full, df_sub, uns); 1318 uns[k + '_colors'] = np.array(uns[k + '_colors'])[; 1319 np.where(np.in1d(; -> 1320 all_categories, df_sub[k].cat.categories))[0]]; 1321 ; 1322 def rename_categories(self, key, categories):. IndexError: index 7 is out of bounds for axis 1 with size 7; ```. even though it's part of the set:; ```py; >>> set(tiss.obs['cell_ontology_class']); {'B cell',; 'NA',; '",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/363:1024,layers,layers,1024,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363,1,['layers'],['layers']
Modifiability," object with n_obs × n_vars = 29322 × 19860. ```python; >>> tiss[tiss.obs['cell_ontology_class']=='B cell']; ```. ```pytb; IndexError Traceback (most recent call last); <ipython-input-269-28b4524131cb> in <module>(); ----> 1 tiss[tiss.obs['cell_ontology_class']=='B cell']. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index); 1299 def __getitem__(self, index):; 1300 """"""Returns a sliced view of the object.""""""; -> 1301 return self._getitem_view(index); 1302 ; 1303 def _getitem_view(self, index):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index); 1303 def _getitem_view(self, index):; 1304 oidx, vidx = self._normalize_indices(index); -> 1305 return AnnData(self, oidx=oidx, vidx=vidx, asview=True); 1306 ; 1307 def _remove_unused_categories(self, df_full, df_sub, uns):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx); 662 if not isinstance(X, AnnData):; 663 raise ValueError('`X` has to be an AnnData object.'); --> 664 self._init_as_view(X, oidx, vidx); 665 else:; 666 self._init_as_actual(. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _init_as_view(self, adata_ref, oidx, vidx); 713 raise KeyError('Unknown Index type'); 714 # fix categories; --> 715 self._remove_unused_categories(adata_ref.obs, obs_sub, uns_new); 716 self._remove_unused_categories(adata_ref.var, var_sub, uns_new); 717 # set attributes. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _remove_unused_categories(self, df_full, df_sub, uns); 1318 uns[k + '_colors'] = np.array(uns[k + '_colors'])[; 1319 np.where(np.in1d(; -> 1320 all_categories, df_sub[k].cat.categories))[0]]; 1321 ; 1322 def rename_categories(self, key, categories):. IndexError: index 7 is out of bounds for axis 1 with size 7; ```. even though it's part of the set:; ```py; >>> set(tiss.obs['cell_ontology_class']); {'B cell',; 'NA',; '",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/226#issuecomment-438879520:1024,layers,layers,1024,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/226#issuecomment-438879520,1,['layers'],['layers']
Modifiability," of either cargo culting it or becaue they know that makes setuptools’ `discover_packages` or so find it. > From your description above I had thought you didn't want to emulate [pandas use of conftest](https://github.com/pandas-dev/pandas/blob/main/pandas/conftest.py?rgh-link-date=2022-04-12T13%3A19%3A30Z). What do you mean specifically?. Pandas are defining special pytest functions/variables there and fixtures, which is what it’s for. I’d probably judge that we don’t need all those fixtures for our complete test suite and move some of them to a smaller scope (e.g. `tests/io/conftest.py` or so). > I'd lean towards it, but I fully expect issues like https://github.com/scverse/scanpy/pull/685 to come up. This is why I'd like to see a working example of what you want to work towards. Actually I think we can fix that: [the docs for `pytest_addoption`](https://doc.pytest.org/en/latest/reference/reference.html#pytest.hookspec.pytest_addoption) say it has to be defined at the *tests root directory* which can be configured using the [`rootpath`](https://doc.pytest.org/en/latest/reference/reference.html?highlight=root#pytest.Config.rootpath) config option. > Is it definitely the future default? It looks like they are walking that back. The question is if they remove the others or not, I think: https://github.com/pytest-dev/pytest/issues/7245. ---. My intention here is to make clear which code lives under which laws. Pytest world is very different from Python module world. The presence of `__init__.py` fools people into thinking that we’re dealing with python packages/modules here, but that’s not true. The way pytest works is pretty simple:. 1. it collects all test modules (`test_*.py` files, no directories) and determines which `conftest.py` files, plugins, … apply to which test module; 2. it collects all tests in those modules and checks which fixtures they need; 3. it sets up and tears down fixtures according to the needs of each test and executes the tests. accepting that",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2225#issuecomment-1096900986:1362,config,configured,1362,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2225#issuecomment-1096900986,2,['config'],['configured']
Modifiability," of mitochondrial genes as 'mt'; sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True). print(""Run 1: initial values after simple processing: ""); print('sum of count layer in designated cell: ', adata[cell,:].layers['counts'].sum()); print('obs[total_counts] value in cell: ', adata[cell,:].obs['total_counts'][0]); print('.X.sum() value in cell: ', adata[cell,:].X.sum()); print('sum of count layer of MALAT1 in cell: ', adata[cell,'MALAT1'].layers['counts']); print('.X value of MALAT1 in cell: ', adata[cell,'MALAT1'].X). print(""\nRun 2: after sc.pp.normalize_total: ""); sc.pp.normalize_total(adata, target_sum=1e4); print('sum of count layer in designated cell: ', adata[cell,:].layers['counts'].sum()) # Note that this changed too; print('obs[total_counts] value in cell: ', adata[cell,:].obs['total_counts'][0]); print('.X.sum() value in cell: ', adata[cell,:].X.sum()); print('sum of count layer of MALAT1 in cell: ', adata[cell,'MALAT1'].layers['counts']); print('.X value of MALAT1 in cell: ', adata[cell,'MALAT1'].X). adata = sc.datasets.pbmc3k(); adata.layers['counts'] = adata.X; cell = adata.obs.index[1]; adata.var['mt'] = adata.var_names.str.startswith('MT-') # annotate the group of mitochondrial genes as 'mt'; sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True). print(""\nRun 3: normalization, specifing argument layer=None""); sc.pp.normalize_total(adata, target_sum=1e4, layer = None); print('sum of count layer in designated cell: ', adata[cell,:].layers['counts'].sum()); print('obs[total_counts] value in cell: ', adata[cell,:].obs['total_counts'][0]); print('.X.sum() value in cell: ', adata[cell,:].X.sum()); print('sum of count layer of MALAT1 in cell: ', adata[cell,'MALAT1'].layers['counts']); print('.X value of MALAT1 in cell: ', adata[cell,'MALAT1'].X); ```. ```pytb; #Output:; Run 1: initial values after simple processing: ; sum of count layer in designated cell: 4903.0; obs[total_cou",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2389:2122,layers,layers,2122,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2389,1,['layers'],['layers']
Modifiability," pca(adata, color, use_raw, sort_order, alpha, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, right_margin, size, title, show, save, ax); 114 title=title,; 115 show=False,; --> 116 save=False, ax=ax); 117 utils.savefig_or_show('pca_scatter', show=show, save=save); 118 if show == False: return axs. /usr/local/lib/python3.6/site-packages/scanpy/plotting/anndata.py in scatter(adata, x, y, color, use_raw, layers, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, frameon, right_margin, left_margin, size, title, show, save, ax); 110 show=show,; 111 save=save,; --> 112 ax=ax); 113 elif x is not None and y is not None:; 114 if ((x in adata.obs.keys() or x in adata.var.index). /usr/local/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _scatter_obs(adata, x, y, color, use_raw, layers, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, frameon, right_margin, left_margin, size, title, show, save, ax); 371 c = adata.raw[:, key].X; 372 elif key in adata.var_names:; --> 373 c = adata[:, key].X if layers[2] == 'X' else adata[:, key].layers[layers[2]]; 374 c = c.toarray().flatten() if issparse(c) else c; 375 elif is_color_like(key): # a flat color. /usr/local/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index); 1292 def __getitem__(self, index):; 1293 """"""Returns a sliced view of the object.""""""; -> 1294 return self._getitem_view(index); 1295 ; 1296 def _getitem_view(self, index):. /usr/local/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index); 1296 def _getitem_view(self, index):; 1297 oidx, vidx = self._normalize_indices(index); -> 1298 return AnnData(self, oidx=oidx, vidx=vidx, asview=True); 1299 ; 1300 # this is used in the setter for uns, if a view. /usr/local/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/263:2327,layers,layers,2327,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263,1,['layers'],['layers']
Modifiability," plot_boxplot_cell_fraction(adata, gene, label, title, ax, show); 1 def plot_boxplot_cell_fraction(adata, gene, label, title, ax, show=True):; ----> 2 gene_vals = np.asarray(adata[:, gene].X).flatten(); 3 ; 4 labels = ['3m','24m']; 5 # labels = list(set(adata.obs[label])). ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index); 1299 def __getitem__(self, index):; 1300 """"""Returns a sliced view of the object.""""""; -> 1301 return self._getitem_view(index); 1302 ; 1303 def _getitem_view(self, index):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index); 1303 def _getitem_view(self, index):; 1304 oidx, vidx = self._normalize_indices(index); -> 1305 return AnnData(self, oidx=oidx, vidx=vidx, asview=True); 1306 ; 1307 def _remove_unused_categories(self, df_full, df_sub, uns):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx); 662 if not isinstance(X, AnnData):; 663 raise ValueError('`X` has to be an AnnData object.'); --> 664 self._init_as_view(X, oidx, vidx); 665 else:; 666 self._init_as_actual(. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _init_as_view(self, adata_ref, oidx, vidx); 691 self._varm = ArrayView(adata_ref.varm[vidx_normalized], view_args=(self, 'varm')); 692 # hackish solution here, no copy should be necessary; --> 693 uns_new = deepcopy(self._adata_ref._uns); 694 # need to do the slicing before setting the updated self._n_obs, self._n_vars; 695 self._n_obs = self._adata_ref.n_obs # use the original n_obs here. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil); 178 y = x; 179 else:; --> 180 y = _reconstruct(x, memo, *rv); 181 ; 182 # If is its own copy, don't memoize. ~/anaconda3/lib/python3.6/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy); 278 if state is not None:; 279 if deep:; --> 280 state = deepcopy(state, memo);",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/363#issuecomment-442366170:1651,layers,layers,1651,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363#issuecomment-442366170,1,['layers'],['layers']
Modifiability," ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs; sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(); tr=time.time(); adata = sc.read(input_file); adata.var_names_make_unique(); adata.shape; print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(); # To reduce the number of cells:; USE_FIRST_N_CELLS = 1300000; adata = adata[0:USE_FIRST_N_CELLS]; adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell); sc.pp.filter_cells(adata, max_genes=max_genes_per_cell); sc.pp.filter_genes(adata, min_cells=min_cells_per_gene); sc.pp.normalize_total(adata, target_sum=1e4); print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(); sc.pp.log1p(adata); print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes; sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression; for marker in markers:; adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes; adata = adata[:, adata.var.highly_variable]. ts=time.time(); #Regress out confounding factors (number of counts, mitochondrial gene expression); mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX); n_counts = np.array(adata.X.sum(axis=1)); adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts; adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']); print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(); sc.pp.scale(adata); print(""Total scale time : %s"" % (time.time()-ts)); ```; add timer around _get_mean_var call; https://github.com/scverse/scanpy/blob/706d4ef65e5d65e04b788831e7fd65dbe6b2a61f/scanpy/preprocessing/_scale.py#L167; we can also create _get_mean_var_std function that return std as well so we don't require to compute it in scale function(L168-L169).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3280:2653,variab,variable,2653,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280,1,['variab'],['variable']
Modifiability," reproduce your bug. Hi, everyone:; Many users probably do not rely on pp.normalize_total for downstream analysis, but I found a strange default behavior that I think is worth mentioning.; pp.normalize_total() normalized my .layers['counts'] as well; The documentation is a bit murky; not sure if that is the expected behavior when layer is unspecified, but; such default behavior would undermine anyone who wishes to save the count information before RPKM normalization. ### Minimal code sample (that we can copy&paste without having any data). ```python; # Your code here; adata = sc.datasets.pbmc3k(); adata.layers['counts'] = adata.X; cell = adata.obs.index[1]; adata.var['mt'] = adata.var_names.str.startswith('MT-') # annotate the group of mitochondrial genes as 'mt'; sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True). print(""Run 1: initial values after simple processing: ""); print('sum of count layer in designated cell: ', adata[cell,:].layers['counts'].sum()); print('obs[total_counts] value in cell: ', adata[cell,:].obs['total_counts'][0]); print('.X.sum() value in cell: ', adata[cell,:].X.sum()); print('sum of count layer of MALAT1 in cell: ', adata[cell,'MALAT1'].layers['counts']); print('.X value of MALAT1 in cell: ', adata[cell,'MALAT1'].X). print(""\nRun 2: after sc.pp.normalize_total: ""); sc.pp.normalize_total(adata, target_sum=1e4); print('sum of count layer in designated cell: ', adata[cell,:].layers['counts'].sum()) # Note that this changed too; print('obs[total_counts] value in cell: ', adata[cell,:].obs['total_counts'][0]); print('.X.sum() value in cell: ', adata[cell,:].X.sum()); print('sum of count layer of MALAT1 in cell: ', adata[cell,'MALAT1'].layers['counts']); print('.X value of MALAT1 in cell: ', adata[cell,'MALAT1'].X). adata = sc.datasets.pbmc3k(); adata.layers['counts'] = adata.X; cell = adata.obs.index[1]; adata.var['mt'] = adata.var_names.str.startswith('MT-') # annotate the group of mitochondrial genes",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2389:1385,layers,layers,1385,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2389,1,['layers'],['layers']
Modifiability," return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,; > --> 307 var_group_positions=group_positions, show=show, save=save, **kwds); > 308; > 309 elif plot_type == 'tracksplot':; >; > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds); > 819 if isinstance(var_names, str):; > 820 var_names = [var_names]; > --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer); > 822; > 823 if 'color' in kwds:; >; > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer); > 1983 matrix = adata[:, var_names].layers[layer]; > 1984 elif use_raw:; > -> 1985 matrix = adata.raw[:, var_names].X; > 1986 else:; > 1987 matrix = adata[:, var_names].X; >; > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index); > 510; > 511 def __getitem__(self, index):; > --> 512 oidx, vidx = self._normalize_indices(index); > 513 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]; > 514 else: X = self._adata.file['raw.X'][oidx, vidx]; >; > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_indices(self, packed_index); > 538 obs, var = super(Raw, self)._unpack_index(packed_index); > 539 obs = _normalize_index(obs, self._adata.obs_names); > --> 540 var = _normalize_index(var, self.var_names); > 541 return obs, var; > 542; >; > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_index(index, names); > 270 raise KeyError(; > 271 'Indices ""{}"" contain invalid observation/variables names/indices.'; > --> 272 .format(index)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/438#issuecomment-456735910:2259,layers,layers,2259,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438#issuecomment-456735910,1,['layers'],['layers']
Modifiability," setuptools’ `discover_packages` or so find it. > From your description above I had thought you didn't want to emulate [pandas use of conftest](https://github.com/pandas-dev/pandas/blob/main/pandas/conftest.py?rgh-link-date=2022-04-12T13%3A19%3A30Z). What do you mean specifically?. Pandas are defining special pytest functions/variables there and fixtures, which is what it’s for. I’d probably judge that we don’t need all those fixtures for our complete test suite and move some of them to a smaller scope (e.g. `tests/io/conftest.py` or so). > I'd lean towards it, but I fully expect issues like https://github.com/scverse/scanpy/pull/685 to come up. This is why I'd like to see a working example of what you want to work towards. Actually I think we can fix that: [the docs for `pytest_addoption`](https://doc.pytest.org/en/latest/reference/reference.html#pytest.hookspec.pytest_addoption) say it has to be defined at the *tests root directory* which can be configured using the [`rootpath`](https://doc.pytest.org/en/latest/reference/reference.html?highlight=root#pytest.Config.rootpath) config option. > Is it definitely the future default? It looks like they are walking that back. The question is if they remove the others or not, I think: https://github.com/pytest-dev/pytest/issues/7245. ---. My intention here is to make clear which code lives under which laws. Pytest world is very different from Python module world. The presence of `__init__.py` fools people into thinking that we’re dealing with python packages/modules here, but that’s not true. The way pytest works is pretty simple:. 1. it collects all test modules (`test_*.py` files, no directories) and determines which `conftest.py` files, plugins, … apply to which test module; 2. it collects all tests in those modules and checks which fixtures they need; 3. it sets up and tears down fixtures according to the needs of each test and executes the tests. accepting that makes it easier to reason about how our test suite works.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2225#issuecomment-1096900986:1476,Config,Config,1476,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2225#issuecomment-1096900986,5,"['Config', 'config', 'plugin']","['Config', 'config', 'plugins']"
Modifiability," was a single variable which would be used to fill cell in the plot. As an example:. ```python; pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(); pbmc.obs[""sampleid""] = np.repeat([""s1"", ""s2""], pbmc.n_obs / 2). sc.pl.dotplot(pbmc, var_names='LDHB', groupby=['louvain', 'sampleid'], groupby_expand=True); ```. ![tmpdm8256t1](https://user-images.githubusercontent.com/8238804/144899323-c439785d-5d57-4a18-b6e5-2b12412465f8.PNG). Instead of having an argument which changes the interpretation of the earlier arguments, I would prefer more orthogonal arguments. I think you'd be able to get an output close to what you would currently like with:. ```python; import scanpy as sc, pandas as pd, numpy as np. pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(); pbmc.obs[""sampleid""] = np.repeat([""s1"", ""s2""], pbmc.n_obs / 2); df = sc.get.obs_df(pbmc, [""LDHB"", ""louvain"", ""sampleid""]). summarized = df.pivot_table(; index=[""louvain"", ""sampleid""],; values=""LDHB"",; aggfunc=[np.mean, np.count_nonzero]; ); color_df = summarized[""mean""].unstack(); size_df = summarized[""count_nonzero""].unstack(). # I don't think the var_names or groupby variables are actually important here; sc.pl.DotPlot(; pbmc,; var_names=""LDHB"", groupby=[""louvain"", ""sampleid""], # Just here so it doesn't error; dot_color_df=color_df, dot_size_df=size_df,; ).style(cmap=""Reds"").show(); ```. I think this functionality could be more generic, and inspired by the `pd.pivot_table` function. This could end up looking like:. ```python; # Imaginary implementation:; sc.pl.heatmap(; pbmc,; var_names=""LDHB"",; row_groups=""louvain"",; col_groups=""sampleid""; ); ```. ![image](https://user-images.githubusercontent.com/8238804/144901891-45c3a8aa-1b56-4521-abc1-66f968a59d23.png). ```python; sc.pl.heatmap(; pbmc,; var_names=[""LDHB"", ""LYZ"", ""CD79A""],; row_groups=""louvain"",; col_groups=""sampleid""; ); ```. ![image](https://user-images.githubusercontent.com/8238804/144902398-e967c1db-53c1-4b44-bcbf-8dfedcf06e58.png). What do you think about that?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1876#issuecomment-987049315:1505,variab,variables,1505,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1876#issuecomment-987049315,1,['variab'],['variables']
Modifiability," we all love them. First of all, let's all hold our hands, close our eyes and thank matplotlib developers for implementing them. But when we jointly plot some continuous variables that are not on the exact same scale (e.g. some genes), it's not possible to specify a single vmin/vmax value that fits all variables especially if they have some outliers. This deeply saddens us and forces us to watch a few more episodes of Stranger Things and it certainly doesn't help :(. I would like to hear your thoughts about how to fix that. But before that, as a responsible person, I did some homework and I spent around 34 minutes to understand how color normalization works in matplotlib (https://matplotlib.org/3.1.1/tutorials/colors/colormapnorms.html) and tried to implement a custom normalization class (https://matplotlib.org/3.1.1/tutorials/colors/colormapnorms.html#custom-normalization-manually-implement-two-linear-ranges). . My idea is simply to specify vmin/vmax in terms of quantiles of the color vector which can be shared between variables instead of a specific value. One way, I thought, might be to pass a `norm` argument with a custom normalization object to our lovely `plot_scatter`. However, as far as I understand, it's not possible because in the quantile function in the custom normalization class requires the entire color vector for each continuous variable which is not super convenient because it's too much preprocessing to find different quantile values for each variable and pass a vmin/vmax vector to the plotting function. Not user-friendly and still requires modifications in the code :(. Instead, I added two ugly arguments named `vmin_quantile` and `vmax_quantile` to the `plot_scatter` function which allows me to specify a single quantile value for vmin/vmax which is then translated into real values separately for each variable:. ![image](https://user-images.githubusercontent.com/1140359/62720493-20731c00-b9d8-11e9-9dc9-f91cf052c4e1.png). This solved my problem but I",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/775:1111,variab,variables,1111,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/775,1,['variab'],['variables']
Modifiability," | 297|; | Updated | 14.91 |; | Speedup | 19.91 |. experiment setup : AWS r7i.24xlarge. ```python; import time; import numpy as np. import pandas as pd. import scanpy as sc; from sklearn.cluster import KMeans. import os; import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '); warnings.simplefilter('ignore'); input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):; print('Downloading import file...'); wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes; MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out; markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells; min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed; max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes; min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this; n_top_genes = 4000 # Number of highly variable genes to retain. # PCA; n_components = 50 # Number of principal components to compute. # t-SNE; tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means; k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs; sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(); tr=time.time(); adata = sc.read(input_file); adata.var_names_make_unique(); adata.shape; print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(); # To reduce the number of cells:; USE_FIRST_N_CELLS = 1300000; adata = adata[0:USE_FIRST_N_CELLS]; adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell); sc.pp.filter_cells(adata, max_genes=max_genes_per_cell); sc.pp.filter_genes(adata, min_cells=min_cells_per_gene); sc.pp.normalize_total(adata, target_sum=1e4); print(""Total filter and normalize time ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3110:1773,variab,variable,1773,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3110,1,['variab'],['variable']
Modifiability," |; | Updated | 1.59 |; | Speedup | 7.433962264 |. experiment setup : AWS r7i.24xlarge. ```python; import time; import numpy as np. import pandas as pd. import scanpy as sc; from sklearn.cluster import KMeans. import os; import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '); warnings.simplefilter('ignore'); input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):; print('Downloading import file...'); wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes; MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out; markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells; min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed; max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes; min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this; n_top_genes = 4000 # Number of highly variable genes to retain. # PCA; n_components = 50 # Number of principal components to compute. # t-SNE; tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means; k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs; sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(); tr=time.time(); adata = sc.read(input_file); adata.var_names_make_unique(); adata.shape; print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(); # To reduce the number of cells:; USE_FIRST_N_CELLS = 1300000; adata = adata[0:USE_FIRST_N_CELLS]; adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell); sc.pp.filter_cells(adata, max_genes=max_genes_per_cell); sc.pp.filter_genes(adata, min_cells=min_cells_per_gene); sc.pp.normalize_total(adata, target_sum=1e4); print(""Total filter and normalize time ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3100:1679,variab,variable,1679,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3100,1,['variab'],['variable']
Modifiability,""", line 103 in _multicall; File ""<venv>/lib/python3.12/site-packages/pluggy/_manager.py"", line 120 in _hookexec; File ""<venv>/lib/python3.12/site-packages/pluggy/_hooks.py"", line 513 in __call__; File ""<venv>/lib/python3.12/site-packages/_pytest/runner.py"", line 242 in <lambda>; File ""<venv>/lib/python3.12/site-packages/_pytest/runner.py"", line 341 in from_call; File ""<venv>/lib/python3.12/site-packages/_pytest/runner.py"", line 241 in call_and_report; File ""<venv>/lib/python3.12/site-packages/_pytest/runner.py"", line 132 in runtestprotocol; File ""<venv>/lib/python3.12/site-packages/_pytest/runner.py"", line 113 in pytest_runtest_protocol; File ""<venv>/lib/python3.12/site-packages/pluggy/_callers.py"", line 103 in _multicall; File ""<venv>/lib/python3.12/site-packages/pluggy/_manager.py"", line 120 in _hookexec; File ""<venv>/lib/python3.12/site-packages/pluggy/_hooks.py"", line 513 in __call__; File ""<venv>/lib/python3.12/site-packages/_pytest/main.py"", line 362 in pytest_runtestloop; File ""<venv>/lib/python3.12/site-packages/pluggy/_callers.py"", line 103 in _multicall; File ""<venv>/lib/python3.12/site-packages/pluggy/_manager.py"", line 120 in _hookexec; File ""<venv>/lib/python3.12/site-packages/pluggy/_hooks.py"", line 513 in __call__; File ""<venv>/lib/python3.12/site-packages/_pytest/main.py"", line 337 in _main; File ""<venv>/lib/python3.12/site-packages/_pytest/main.py"", line 283 in wrap_session; File ""<venv>/lib/python3.12/site-packages/_pytest/main.py"", line 330 in pytest_cmdline_main; File ""<venv>/lib/python3.12/site-packages/pluggy/_callers.py"", line 103 in _multicall; File ""<venv>/lib/python3.12/site-packages/pluggy/_manager.py"", line 120 in _hookexec; File ""<venv>/lib/python3.12/site-packages/pluggy/_hooks.py"", line 513 in __call__; File ""<venv>/lib/python3.12/site-packages/_pytest/config/__init__.py"", line 175 in main; File ""<venv>/lib/python3.12/site-packages/_pytest/config/__init__.py"", line 201 in console_main; File ""<venv>/bin/pytest"", line 10 in <module>; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478:7291,config,config,7291,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478,4,['config'],['config']
Modifiability,"""], pbmc.n_obs / 2); > ; > sc.pl.dotplot(pbmc, var_names='LDHB', groupby=['louvain', 'sampleid'], groupby_expand=True); > ```; > ; > ![tmpdm8256t1](https://user-images.githubusercontent.com/8238804/144899323-c439785d-5d57-4a18-b6e5-2b12412465f8.PNG); > ; > Instead of having an argument which changes the interpretation of the earlier arguments, I would prefer more orthogonal arguments.; > ; > I think you'd be able to get an output close to what you would currently like with:; > ; > ```python; > import scanpy as sc, pandas as pd, numpy as np; > ; > pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(); > pbmc.obs[""sampleid""] = np.repeat([""s1"", ""s2""], pbmc.n_obs / 2); > df = sc.get.obs_df(pbmc, [""LDHB"", ""louvain"", ""sampleid""]); > ; > summarized = df.pivot_table(; > index=[""louvain"", ""sampleid""],; > values=""LDHB"",; > aggfunc=[np.mean, np.count_nonzero]; > ); > color_df = summarized[""mean""].unstack(); > size_df = summarized[""count_nonzero""].unstack(); > ; > # I don't think the var_names or groupby variables are actually important here; > sc.pl.DotPlot(; > pbmc,; > var_names=""LDHB"", groupby=[""louvain"", ""sampleid""], # Just here so it doesn't error; > dot_color_df=color_df, dot_size_df=size_df,; > ).style(cmap=""Reds"").show(); > ```; > ; > I think this functionality could be more generic, and inspired by the `pd.pivot_table` function. This could end up looking like:; > ; > ```python; > # Imaginary implementation:; > sc.pl.heatmap(; > pbmc,; > var_names=""LDHB"",; > row_groups=""louvain"",; > col_groups=""sampleid""; > ); > ```; > ; > ![image](https://user-images.githubusercontent.com/8238804/144901891-45c3a8aa-1b56-4521-abc1-66f968a59d23.png); > ; > ```python; > sc.pl.heatmap(; > pbmc,; > var_names=[""LDHB"", ""LYZ"", ""CD79A""],; > row_groups=""louvain"",; > col_groups=""sampleid""; > ); > ```; > ; > ![image](https://user-images.githubusercontent.com/8238804/144902398-e967c1db-53c1-4b44-bcbf-8dfedcf06e58.png); > ; > What do you think about that?. Thanks @ivirshup !. I like these lines you s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1876#issuecomment-988045664:1611,variab,variables,1611,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1876#issuecomment-988045664,1,['variab'],['variables']
Modifiability,"""batch""); ```. ```pytb; >>> import scanpy as sc; g_anndata = sc.pp.log1p(pbmc, copy=True); pbmc.layers['log_transformed'] = log_anndata.X.copy(). # This errors, because X is not normalized and flavor=""seurat"" requires normalizes data.; # ValueError: cannot specify integer `bins` when input data contains infinity; sc.pp.highly_variable_genes(pbmc, flavor=""seurat"", subset=False, inplace=False). # This works, we pass log tranformed data; pbmc.uns['log1p'] = log_anndata.uns['log1p']; sc.pp.highly_variable_genes(pbmc, layer=""log_transformed"", flavor=""seurat"", subset=False, inplace=False). # This raises ValueError again; pbmc.obs['batch'] = 'A'; column_index = pbmc.obs.columns.get_indexer(['batch']); pbmc.obs.iloc[slice(pbmc.n_obs//2, None), column_index] = 'B'; sc.pp.highly_variable_genes(pbmc, layer=""log_transformed"", flavor=""seurat"", subset=False, inplace=False, batch_key=""batch"")>>> pbmc = sc.datasets.pbmc3k(). >>> log_anndata = sc.pp.log1p(pbmc, copy=True); >>> pbmc.layers['log_transformed'] = log_anndata.X.copy(); >>> ; >>> # This errors, because X is not normalized and flavor=""seurat"" requires normalizes data.; >>> # ValueError: cannot specify integer `bins` when input data contains infinity; >>> sc.pp.highly_variable_genes(pbmc, flavor=""seurat"", subset=False, inplace=False); .venv/lib/python3.10/site-packages/scipy/sparse/_data.py:133: RuntimeWarning: overflow encountered in expm1; result = op(self._deduped_data()); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""venv/lib/python3.10/site-packages/scanpy/preprocessing/_highly_variable_genes.py"", line 434, in highly_variable_genes; df = _highly_variable_genes_single_batch(; File "".venv/lib/python3.10/site-packages/scanpy/preprocessing/_highly_variable_genes.py"", line 215, in _highly_variable_genes_single_batch; df['mean_bin'] = pd.cut(df['means'], bins=n_bins); File "".venv/lib/python3.10/site-packages/pandas/core/reshape/tile.py"", line 263, in cut; raise ValueError(; ValueError: cannot",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2396:2364,layers,layers,2364,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2396,1,['layers'],['layers']
Modifiability,"# Minimal code sample (that we can copy&paste without having any data). ```python; # Your code here; adata = sc.datasets.pbmc3k(); adata.layers['counts'] = adata.X; cell = adata.obs.index[1]; adata.var['mt'] = adata.var_names.str.startswith('MT-') # annotate the group of mitochondrial genes as 'mt'; sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True). print(""Run 1: initial values after simple processing: ""); print('sum of count layer in designated cell: ', adata[cell,:].layers['counts'].sum()); print('obs[total_counts] value in cell: ', adata[cell,:].obs['total_counts'][0]); print('.X.sum() value in cell: ', adata[cell,:].X.sum()); print('sum of count layer of MALAT1 in cell: ', adata[cell,'MALAT1'].layers['counts']); print('.X value of MALAT1 in cell: ', adata[cell,'MALAT1'].X). print(""\nRun 2: after sc.pp.normalize_total: ""); sc.pp.normalize_total(adata, target_sum=1e4); print('sum of count layer in designated cell: ', adata[cell,:].layers['counts'].sum()) # Note that this changed too; print('obs[total_counts] value in cell: ', adata[cell,:].obs['total_counts'][0]); print('.X.sum() value in cell: ', adata[cell,:].X.sum()); print('sum of count layer of MALAT1 in cell: ', adata[cell,'MALAT1'].layers['counts']); print('.X value of MALAT1 in cell: ', adata[cell,'MALAT1'].X). adata = sc.datasets.pbmc3k(); adata.layers['counts'] = adata.X; cell = adata.obs.index[1]; adata.var['mt'] = adata.var_names.str.startswith('MT-') # annotate the group of mitochondrial genes as 'mt'; sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True). print(""\nRun 3: normalization, specifing argument layer=None""); sc.pp.normalize_total(adata, target_sum=1e4, layer = None); print('sum of count layer in designated cell: ', adata[cell,:].layers['counts'].sum()); print('obs[total_counts] value in cell: ', adata[cell,:].obs['total_counts'][0]); print('.X.sum() value in cell: ', adata[cell,:].X.sum()); print('sum of co",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2389:1859,layers,layers,1859,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2389,1,['layers'],['layers']
Modifiability,"## Pytest architecture. `tests` directories and `test_*.py` collections aren’t intended to be packages, so you shouldn’t import from there. Just think about a module-level `pytest.importorskip(...)` or so. Also if there’s a `__init__.py` somewhere in your `tests` directory when using pytest, you’re doing something wrong. Yes, that means that all packages except for numba are doing it wrong, because numba also has importable test utils in there, and the others just misunderstand how pytest works. I blame setuptools, because `find_packages` finds directories that have `__init__.py`, so people just cargo-cultily started adding it without knowing what they’re doing. Fixture visibility is hierarchical, so a conftest.py in a subfolder is able to override fixtures from higher-up.; So e.g. for testing a package/app that uses celery, you just define your own `celery_config` fixture, then start using the `celery_app` fixture, which will use your config automatically. ## `tests` in the package. I think it’s a good idea to have it in there if you are a huge project and like to physically split up tests into multiple directiories that are close to the source code they test. (like numpy does it). But as long as there’s only one `tests` directory with a structure that doesn’t neatly map to your module hierarchy, having it outside is cleaner because people can’t accidentally import from there. And as you can see from your links and our contributors importing stuff from `test_*` collections, a lot of projects don’t know how to use pytest, so making misuse harder is beneficial. Also we have test data, which wastes space on every user’s machine and bandwidth for people installing scanpy. ## `tests` and `testing`/`test_utils`. Given the above points, I think we should move things out to keep everything clean, and the package small. I also like the separation of concerns: `test_utils` or `testing` (like numpy does) for reusable stuff that other projects depending on you might use and `te",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1528#issuecomment-738776290:950,config,config,950,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1528#issuecomment-738776290,1,['config'],['config']
Modifiability,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened?. Dear `scanpy` developers, . I was exploring the new features in the latest version of Scanpy, but encountered a prolonged pause when running the `sc.pp.scrublet(adata)`. Initially I thought the problem was due to the large size (~100k cells) of the dataset I was exploring (I let it run for almost a whole week and nothing changed). However, even if I switched to my own dataset (unpublished, around 5k celIs), it paused at the same step. ; ```; Running Scrublet; filtered out 1419 genes that are detected in less than 3 cells; normalizing counts per cell; finished (0:00:00); extracting highly variable genes; finished (0:00:00); --> added; 'highly_variable', boolean vector (adata.var); 'means', float vector (adata.var); 'dispersions', float vector (adata.var); 'dispersions_norm', float vector (adata.var); normalizing counts per cell; finished (0:00:00); normalizing counts per cell; finished (0:00:00); Embedding transcriptomes using PCA...; ```. I was running this analysis on my Intel-core iMac. Surprisingly, when I ran the same line of code (under a similar virtual environment) on my M2-chip laptop, it finished in a flash of time.; ```Running Scrublet; filtered out 1419 genes that are detected in less than 3 cells; normalizing counts per cell; finished (0:00:00); extracting highly variable genes; finished (0:00:00); --> added; 'highly_variable', boolean vector (adata.var); 'means', float vector (adata.var); 'dispersions', float vector (adata.var); 'dispersions_norm', float vector (adata.var); normalizing counts per cell; finished (0:00:00); normalizing counts per cell; finished (0:00:00); Embedding transcriptomes using PCA...; using data matrix X directly; Automatically set threshold at do",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3116:884,variab,variable,884,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3116,1,['variab'],['variable']
Modifiability,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened?. I am working with a set of 2 10x scRNA samples. I read them, concatenated them and then I did basic filtering. I then used ""adata.raw = adata"" to freeze the counts on adata.raw before proceding. Then I ran: ; ```; sc.pp.normalize_total(adata, target_sum=1e4); sc.pp.log1p(adata); ```. To my surprise, when I check the adata.raw I see that the values have been also lognormized (and not only adata). ; Is that how it is supposed to be? Is there any way to avoid this behavior ? I know I can store the raw counts in layers, I just want to understand how it works. . To check the data I used : ; `print(adata.raw.X[1:10,1:10]) `. ### Minimal code sample. ```python; #read the data; Data1_adata= sc.read_10x_mtx(; '/Data_1/filtered_feature_bc_matrix', ; var_names='gene_symbols', index); cache=True) ; #concatenate; adata = Data1_adata.concatenate(Data2_adata); # save raw counts in raw slot.; adata.raw = adata ; # normalize to depth 10 000; sc.pp.normalize_total(adata, target_sum=1e4). # logaritmize; sc.pp.log1p(adata). #check adata.raw ; print(adata.raw.X[1:10,1:10]); ```. ### Error output. _No response_. ### Versions. <details>. ```; anndata 0.10.7; scanpy 1.10.0; -----; PIL 8.4.0; anyio NA; arrow 1.3.0; asttokens NA; attr 23.2.0; attrs 23.2.0; babel 2.14.0; backcall 0.2.0; bottleneck 1.3.7; brotli NA; certifi 2024.02.02; cffi 1.16.0; chardet 5.2.0; charset_normalizer 3.3.2; cloudpickle 3.0.0; colorama 0.4.6; comm 0.2.1; cycler 0.12.1; cython_runtime NA; cytoolz 0.12.3; dask 2024.2.0; dateutil 2.8.2; debugpy 1.8.1; decorator 5.1.1; defusedxml 0.7.1; exceptiongroup 1.2.0; executing 2.0.1; fastjsonschema NA; fqdn NA; h5py 3.7.0; idna 3.6; igraph 0.11.4; importlib_resources NA; ipykernel 6.29.2; ipyw",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3073:803,layers,layers,803,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3073,1,['layers'],['layers']
Modifiability,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened?. I'm working with the `sc.get.aggregate` function from the latest release candidate. In certain combinations of `groupby` variables, some valuese get lost. . I wasn't able to make a minimal reproducible example, but I obfuscated the `obs` table of my real data and can share it here: ; https://www.dropbox.com/scl/fi/jsbrb2ulki7mmih2242kc/adata_aggregate_bug.h5ad?rlkey=qczuaf2v5vlwb00zyuxmzjkix&dl=1. ### Minimal code sample. ```python; >>> test_adata = sc.read_h5ad(""adata_aggregate_bug.h5ad""). >>> test_adata.obs[""patient_id""].nunique(); 69. >>> test_adata.obs.isnull().sum(); patient_id 0; timepoint 0; external_batch_id 0; dtype: int64. >>> pb = sc.get.aggregate(; test_adata,; by=[; ""patient_id"",; ""timepoint"",; ""external_batch_id"",; ],; func=""mean"",; ); pb.obs[""patient_id""].nunique(). 15. >>> pb = sc.get.aggregate(; test_adata,; by=[; ""patient_id"",; ""external_batch_id"",; ],; func=""mean"",; ); pb.obs[""patient_id""].nunique(). 69. >>> pb = sc.get.aggregate(; test_adata,; by=[; ""patient_id"",; ""timepoint"",; ],; func=""mean"",; ); pb.obs[""patient_id""].nunique(). 69; ```. ### Error output. ```pytb; So only if using all three variables, some patient IDs are lost. I don't see why this would be happening.; ```. ### Versions. <details>. ```; Package Version Editable project location; ------------------------- --------------- -------------------------------------------------------------------------------------------------------------------------; aiohttp 3.9.3; aiosignal 1.3.1; anndata 0.10.5.post1; anyio 4.3.0; appdirs 1.4.4; argon2-cffi 23.1.0; argon2-cffi-bindings 21.2.0; array_api_compat 1.5; arrow 1.3.0; asciitree 0.3.3; asttokens 2.4.1; async-lru 2.0.4; async-timeout 4.0.3; attrs 23.2.0; Babel 2.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2964:410,variab,variables,410,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2964,1,['variab'],['variables']
Modifiability,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened?. On version 1.10.1 & manuals for v.1.10.x:. [If None, mpl.rcParams[""axes.prop_cycle""] is used unless the categorical variable already has colors stored in adata.uns[""{var}_colors""]. If provided, values of adata.uns[""{var}_colors""] will be set.](https://scanpy.readthedocs.io/en/stable/api/generated/scanpy.pl.embedding.html). ![image](https://github.com/user-attachments/assets/9d6f328b-3afb-41c5-92ca-0a75bec6ce2c). ### Minimal code sample. Here I have an anndata with a categorical obs variable and having {var}_colors in uns; still, mpl.rcParams[""axes.prop_cycle""] is used:. ```python; ## the type of data.obs['study']: category. Name: study, Length: 48256, dtype: category; Categories (2, object): ['NatGenet', 'HongProj']. data.uns['study_colors']; -> array(['#ff7f0e', '#17becf'], dtype=object); sc.pl.embeddings(data, 'anyembedding', 'study'); ->; ```. ### Error output. _No response_. ### Versions. <details>. ```; scanpy==1.10.1 anndata==0.8.0 umap==0.5.5 numpy==1.26.3 scipy==1.11.4 pandas==1.5.3 scikit-learn==1.4.0 statsmodels==0.14.0 igraph==0.10.3 pynndescent==0.5.8; ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3193:405,variab,variable,405,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3193,2,['variab'],['variable']
Modifiability,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened?. `sc.tl.dpt` was successfully done. But when I want to plot the result of dpt,the error comes out. ### Minimal code sample. ```python; sc.tl.dpt(a1,n_branchings=2); sc.pl.dpt_groups_pseudotime(a1); sc.pl.dpt_timeseries(a1); ```. ### Error output. Error in dpt_timeseries:. ```pytb; WARNING: Plotting more than 100 genes might take some while, consider selecting only highly variable genes, for example.; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); TypeError: float() argument must be a string or a real number, not 'csr_matrix'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last); Cell In[85], line 1; ----> 1 sc.pl.dpt_timeseries(a1). File D:\anaconda\Lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File D:\anaconda\Lib\site-packages\scanpy\plotting\_tools\__init__.py:245, in dpt_timeseries(adata, color_map, show, save, as_heatmap, marker); 242 # only if number of genes is not too high; 243 if as_heatmap:; 244 # plot time series as heatmap, as in Haghverdi et al. (2016), Fig. 1d; --> 245 timeseries_as_heatmap(; 246 adata.X[adata.obs[""dpt_order_indices""].values],; 247 var_names=adata.var_names,; 248 highlights_x=adata.uns[""dpt_changepoints""],; 249 color_map=color_map,; 250 ); 251 else:; 252 # plot time series as gene expression vs time; 2",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3086:662,variab,variable,662,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3086,1,['variab'],['variable']
Modifiability,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. **Note**: This bug seems to have been mentioned on 2/13/2022 in this discourse.scverse.org thread: https://discourse.scverse.org/t/error-in-highly-variable-gene-selection/276. However, I can't seem to access scverse.org so I can't see what the cause/resolution was. **Issue**: I am running highly_variable_genes with flavor='seurat_v3'. When I do not include a batch_key, the function runs fine. When I add a batch_key, I get a numerical error. ### Minimal code sample. ```python; sc.pp.highly_variable_genes(adata, flavor='seurat_v3', n_top_genes=1000, batch_key=""Covariate""); ```. ### Error output. ```pytb; ----> 1 sc.pp.highly_variable_genes(adata, flavor='seurat_v3', n_top_genes=1000, batch_key=""Covariate""). File ~/miniconda3/envs/singlecell/lib/python3.8/site-packages/scanpy/preprocessing/_highly_variable_genes.py:428, in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key, check_values); 422 raise ValueError(; 423 '`pp.highly_variable_genes` expects an `AnnData` argument, '; 424 'pass `inplace=False` if you want to return a `pd.DataFrame`.'; 425 ); 427 if flavor == 'seurat_v3':; --> 428 return _highly_variable_genes_seurat_v3(; 429 adata,; 430 layer=layer,; 431 n_top_genes=n_top_genes,; 432 batch_key=batch_key,; 433 check_values=check_values,; 434 span=span,; 435 subset=subset,; 436 inplace=inplace,; 437 ); 439 if batch_key is None:; 440 df = _highly_variable_genes_single_batch(; 441 adata,; 442 layer=layer,; (...); 449 flavor=flavor,; 450 ). File ~/miniconda3/envs/singlecell/lib/python3.8/site-packages/scanpy/preprocessing/_highly_variable_genes.py:85, in _highly_variable_genes_seurat_v3(adata, la",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2669:438,variab,variable-gene-selection,438,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2669,1,['variab'],['variable-gene-selection']
Modifiability,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. After upgrading annData to 0.10.4 I tried to read in some Visium data with read_10x_mtx(); The resulting table only had one variable (/gene/column), and had it over and over again. ### Minimal code sample. ```python; S = scanpy.read_10x_mtx(mydata); ```. ### Error output. ```pytb; /home/lhw/pkgs/mambaforge/envs/env2/lib/python3.11/site-packages/anndata/_core/anndata.py:1908: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`.; utils.warn_names_duplicates(""var""). In [6]: S.var_names; Out[6]: ; Index(['NOC2L', 'NOC2L', 'NOC2L', 'NOC2L', 'NOC2L', 'NOC2L', 'NOC2L', 'NOC2L',; 'NOC2L', 'NOC2L',; ...; 'NOC2L', 'NOC2L', 'NOC2L', 'NOC2L', 'NOC2L', 'NOC2L', 'NOC2L', 'NOC2L',; 'NOC2L', 'NOC2L'],; dtype='object', length=18085). To test, I ran in another environment with scanpy 1.9.6 but annData 0.10.3.; Below are the results:. In [4]: S = scanpy.read_10x_mtx(mydata). In [5]: S.var_names; Out[5]: ; Index(['SAMD11', 'NOC2L', 'KLHL17', 'PLEKHN1', 'PERM1', 'HES4', 'ISG15',; 'AGRN', 'RNF223', 'C1orf159',; ...; 'MT-ND2', 'MT-CO2', 'MT-ATP6', 'MT-CO3', 'MT-ND3', 'MT-ND4L', 'MT-ND4',; 'MT-ND5', 'MT-ND6', 'MT-CYB'],; dtype='object', length=18085); ```. ### Versions; Scanpy 1.9.6; annData 0.10.3 works, annData 0.10.4 does not; <details>. ```. ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2825:415,variab,variable,415,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2825,2,"['Variab', 'variab']","['Variable', 'variable']"
Modifiability,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. During preprocessing of concatenated adata file for scvi-based label transfer, processing fails when applying ""sc.pp.highly_variable_genes"" function with ""ValueError: b'Extrapolation not allowed with blending'"". ### Minimal code sample. ```python; aadata = aadata.concatenate(ref_data_WT). aadata.X; <15445x13343 sparse matrix of type '<class 'numpy.float64'>'; 	with 107849393 stored elements in Compressed Sparse Row format>. # pre-processing:; aadata.layers[""counts""] = aadata.X.copy(); sc.pp.normalize_total(aadata, target_sum=1e4); sc.pp.log1p(aadata); aadata.raw = aadata. sc.pp.highly_variable_genes(aadata, flavor = 'seurat_v3', n_top_genes=2000,; layer = ""counts"", batch_key=""batch"", subset = True)#, span =0.5; ```. ### Error output. ```pytb; ValueError Traceback (most recent call last); Cell In[37], line 7; 4 sc.pp.log1p(aadata); 5 aadata.raw = aadata; ----> 7 sc.pp.highly_variable_genes(aadata, flavor = 'seurat_v3', n_top_genes=2000,; 8 layer = ""counts"", batch_key=""batch"", subset = True)#, span =0.5. File ~/mambaforge/envs/soupxEnv/lib/python3.10/site-packages/scanpy/preprocessing/_highly_variable_genes.py:441, in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key, check_values); 439 sig = signature(_highly_variable_genes_seurat_v3); 440 n_top_genes = cast(int, sig.parameters[""n_top_genes""].default); --> 441 return _highly_variable_genes_seurat_v3(; 442 adata,; 443 layer=layer,; 444 n_top_genes=n_top_genes,; 445 batch_key=batch_key,; 446 check_values=check_values,; 447 span=span,; 448 subset=subset,; 449 inplace=inplace,; 450 ); 452 if batch_key is None:; 453 df = _highly_variable_genes_single_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2853:745,layers,layers,745,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2853,1,['layers'],['layers']
Modifiability,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. In scanpy version 1.9.3. ### Minimal code sample. ```python; adata = sc.read_visium(; '/data_disk/ST01/User/zhanghh/project/analysis/GSM6252954/outs/',; count_file='filtered_feature_bc_matrix.h5',; source_image_path='/data_disk/ST01/User/zhanghh/project/GSE206391/GSM6252954_11-V19T12-012-V4.jpg',; ); ```. ### Error output. ```pytb; /data_disk/ST01/Software/conda_env/zhhenv/lib/python3.9/site-packages/anndata/_core/anndata.py:1832: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`.; utils.warn_names_duplicates(""var""); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/home/zhanghaihao/.local/lib/python3.9/site-packages/scanpy/readwrite.py"", line 390, in read_visium; raise OSError(f""Could not find '{f}'""); OSError: Could not find '/data_disk/ST01/User/zhanghh/project/analysis/GSM6252954/outs/spatial/tissue_positions_list.csv'; ```. ### Versions. <details>; <summary>Details</summary>. ```; >>> import scanpy; scanpy.logging.print_versions(); -----; anndata 0.9.1; scanpy 1.9.3; -----; PIL 9.4.0; beta_ufunc NA; binom_ufunc NA; cffi 1.15.1; colorama 0.4.6; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; dot_parser NA; gmpy2 2.1.2; h5py 3.7.0; hypergeom_ufunc NA; igraph 0.10.4; importlib_resources NA; invgauss_ufunc NA; joblib 1.1.1; kiwisolver 1.4.4; leidenalg 0.9.1; llvmlite 0.40.0; matplotlib 3.7.1; mpl_toolkits NA; mpmath 1.2.1; natsort 8.3.1; nbinom_ufunc NA; ncf_ufunc NA; nct_ufunc NA; ncx2_ufunc NA; numba 0.57.0; numpy 1.24.3; nvfuser NA; opt_einsum v3.3.0; packaging 23.0; pandas 2.0.1; pkg_resources NA; pydot 1.4.2; pyparsing 3.0.9; pytz 2022.7; scipy 1.10.1; session_info 1.0.0; setuptools 66.0.0; six 1.16.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2565:739,Variab,Variable,739,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2565,1,['Variab'],['Variable']
Modifiability,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. Installation using pip & installation from github repository. ### Minimal code sample. ```python; pip install scanpy; ```. ### Error output. ```pytb; Installing collected packages: tbb, distlib, asciitree, stdlib-list, setuptools-scm, pbr, numcodecs, nodeenv, natsort, igraph, identify, filelock, fasteners, docutils, cfgv, array-api-compat, accessible-pygments, zarr, virtualenv, sphinx, session-info, pytest-nunit, pytest-mock, profimp, mdit-py-plugins, leidenalg, sphinxext-opengraph, sphinx-design, sphinx-copybutton, sphinx-autodoc-typehints, scanpydoc, pynndescent, pydata-sphinx-theme, pre-commit, myst-parser, anndata, umap-learn, sphinx-book-theme, jupyter-cache, scanpy, nbsphinx, myst-nb; Attempting uninstall: tbb; Found existing installation: TBB 0.2; ERROR: Cannot uninstall 'TBB'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.; ```. ### Versions. <details>. ```; Package Version; ----------------------------- ---------------; aiobotocore 2.5.0; aiofiles 22.1.0; aiohttp 3.8.5; aioitertools 0.7.1; aiosignal 1.2.0; aiosqlite 0.18.0; alabaster 0.7.12; anaconda-anon-usage 0.4.2; anaconda-catalogs 0.2.0; anaconda-client 1.12.1; anaconda-cloud-auth 0.1.3; anaconda-navigator 2.5.0; anaconda-project 0.11.1; anyio 3.5.0; appdirs 1.4.4; argon2-cffi 21.3.0; argon2-cffi-bindings 21.2.0; arrow 1.2.3; astroid 2.14.2; astropy 5.1; asttokens 2.0.5; async-timeout 4.0.2; atomicwrites 1.4.0; attrs 22.1.0; Automat 20.2.0; autopep8 1.6.0; Babel 2.11.0; backcall 0.2.0; backports.functools-lru-cache 1.6.4; backports.tempfile 1.0; backports.weakref 1.0.post1; bcrypt 3.2.0; beautifulsoup4 4.12.2; binaryorn",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2706:738,plugin,plugins,738,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2706,1,['plugin'],['plugins']
Modifiability,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. When I install scanpy==1.9.6 with pip (anndata==0.10.4), something wrong and adata.X.nnz is 0.; I changed the version of anndata to 0.9.2, it works normal. ### Minimal code sample. ```python; import numpy as np; import pandas as pd; import scanpy as sc; sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3); sc.logging.print_header(); sc.settings.set_figure_params(dpi=80, facecolor='white'); results_file = 'write/pbmc3k.h5ad' # the file that will store the analysis results; adata = sc.read_10x_mtx(my_sample, # the directory with the `.mtx` file; var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index); cache=False) # write a cache file for faster subsequent reading; # sc.pl.highest_expr_genes(adata, n_top=20, ); adata.X.nnz; ```. ### Error output. _No response_. ### Versions. <details>. ```. -----; anndata 0.9.2; scanpy 1.9.5; -----; PIL 9.5.0; asttokens NA; backcall 0.2.0; bottleneck 1.3.5; cffi 1.16.0; comm 0.1.2; cycler 0.12.0; cython_runtime NA; dateutil 2.8.2; debugpy 1.6.7; decorator 4.4.2; defusedxml 0.7.1; entrypoints 0.4; executing 1.2.0; google NA; h5py 3.7.0; hurry NA; ipykernel 6.25.0; ipython_genutils 0.2.0; ipywidgets 8.0.4; jedi 0.18.1; joblib 1.2.0; kiwisolver 1.4.5; llvmlite 0.41.1; matplotlib 3.8.0; matplotlib_inline 0.1.6; mpl_toolkits NA; natsort 8.4.0; numba 0.58.1; numexpr 2.8.7; numpy 1.26.0; packaging 23.2; pandas 1.5.3; parso 0.8.3; patsy 0.5.6; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; platformdirs 3.10.0; prompt_toolkit 3.0.36; psutil 5.9.0; ptyprocess 0.7.0; pure_eval 0.2.2; pyarrow 13.0.0; pycparser 2.21; pydev_ipython NA; pydevconsole NA; pydevd 2.9.5; pydevd_file_utils NA; pydev",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2822:926,variab,variable,926,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2822,2,['variab'],"['variable', 'variables-axis']"
Modifiability,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened?. Function Rank_genes_groups() does not return a figure -> returns None type; Cannot get Figure via plt.gcf(), plt.gca(). Potential Fix:; https://github.com/scverse/scanpy/blob/main/src/scanpy/plotting/_tools/__init__.py; Line 485: cann be extended to the following as in other functions below:; ```; savefig_or_show(f""{key}_"", show=show, save=save); show = settings.autoshow if show is None else show; if show:; return None; return ax; ```. ### Minimal code sample. ```python; fig = sc.pl.rank_genes_groups(adata, show=False); type(fig); #NoneType; plt.gca() -> empty axes; plt.gcf() -> empty figure; ```. ### Error output. _No response_. ### Versions. <details>. ```; -----; anndata 0.10.8; scanpy 1.10.2; -----; ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3205:527,extend,extended,527,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3205,1,['extend'],['extended']
Modifiability,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened?. HVG can produce more than the number of genes asked for as highly variable. This occurs on these two datasets:. ```; wget https://datasets.cellxgene.cziscience.com/e00ab1f4-28cd-497d-b889-94d45840f423.h5ad; ```. ### Minimal code sample. ```python; import scanpy as sc. adata1 = sc.read('e00ab1f4-28cd-497d-b889-94d45840f423.h5ad'). sc.pp.normalize_total(adata1, target_sum=1e4). sc.pp.log1p(adata1). n_top_gene = 10000; sc.pp.highly_variable_genes(adata1, n_top_genes = n_top_gene). hvg_system1 = set(adata1.var[adata1.var['highly_variable']].index); assert len(hvg_system1) == n_top_gene, f""found {len(hvg_system1)} instead of {n_top_gene}"". ```. ### Error output. ```pytb; AssertionError Traceback (most recent call last); Cell In[12], line 1; ----> 1 assert len(hvg_system1) == n_top_gene, f""found {len(hvg_system1)} instead of {n_top_gene}"". AssertionError: found 13355 instead of 10000; ```. ### Versions. <details>. ```; import scanpy; scanpy.logging.print_versions(); -----; anndata 0.10.8; scanpy 1.10.0rc2.dev85+gb918a23e; -----; ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3157:355,variab,variable,355,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3157,1,['variab'],['variable']
Modifiability,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened?. I am following the tutorial for Scanpy, but when I run the Leiden clustering section it runs for multiple days and doesn't seem to terminate. Is this normal? If so could a note be added to the tutorial in terms of the extended runtime?. ### Minimal code sample. ```python; import scanpy as sc; # Core scverse libraries; import anndata as ad. # Data retrieval; import pooch; # Core scverse libraries; import anndata as ad. # Data retrieval; import pooch; EXAMPLE_DATA = pooch.create(; path=pooch.os_cache(""scverse_tutorials""),; base_url=""doi:10.6084/m9.figshare.22716739.v1/"",; ); EXAMPLE_DATA.load_registry_from_doi(). samples = {; ""s1d1"": ""s1d1_filtered_feature_bc_matrix.h5"",; ""s1d3"": ""s1d3_filtered_feature_bc_matrix.h5"",; }; adatas = {}. for sample_id, filename in samples.items():; path = EXAMPLE_DATA.fetch(filename); sample_adata = sc.read_10x_h5(path); sample_adata.var_names_make_unique(); adatas[sample_id] = sample_adata. adata = ad.concat(adatas, label=""sample""); adata.obs_names_make_unique(); print(adata.obs[""sample""].value_counts()). # mitochondrial genes, ""MT-"" for human, ""Mt-"" for mouse; adata.var[""mt""] = adata.var_names.str.startswith(""MT-""); # ribosomal genes; adata.var[""ribo""] = adata.var_names.str.startswith((""RPS"", ""RPL"")); # hemoglobin genes; adata.var[""hb""] = adata.var_names.str.contains(""^HB[^(P)]""). sc.pp.calculate_qc_metrics(; adata, qc_vars=[""mt"", ""ribo"", ""hb""], inplace=True, log1p=True; ). sc.pl.violin(; adata,; [""n_genes_by_counts"", ""total_counts"", ""pct_counts_mt""],; jitter=0.4,; multi_panel=True,; ). sc.pl.scatter(adata, ""total_counts"", ""n_genes_by_counts"", color=""pct_counts_mt""). sc.pp.filter_cells(adata, min_genes=100); sc.pp.filter_genes(adata, min_cells=3). sc.pp.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3228:507,extend,extended,507,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228,1,['extend'],['extended']
Modifiability,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened?. I have always had a question: do I need to scale my adata before running sc.tl.score_genes?. ### Minimal code sample. ```python; sc.tl.score_genes_cell_cycle(adata_hvg, layers='scaled', s_genes=s_genes, g2m_genes=g2m_genes); ```. ### Error output. _No response_. ### Versions. <details>. ```. ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3080:458,layers,layers,458,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3080,1,['layers'],['layers']
Modifiability,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened?. Leiden and Louvain clustering params are not saved to matching `key_added` key in `uns` dictionary but are ovewritten to hardcoded key instead. One use case is that a user may want to run Leiden/Louvain clustering multiple times with different resolutions / parameters. One may specify different keys to store results under. However, if you do so, the metadata for the parameterization of the clustering algorithms are overwritten because the lines below do not respect the user provided `key_added` parameter. I think the desired behavior is to store data under `adata.uns[key_added][""params""]`. I think I've found the pertinent lines below. Happy to submit a PR if maintainers agree :D.; - https://github.com/scverse/scanpy/blob/91ea0fbb03392795d1506d297d4b4847c646db04/scanpy/tools/_leiden.py#L206; - https://github.com/scverse/scanpy/blob/91ea0fbb03392795d1506d297d4b4847c646db04/scanpy/tools/_louvain.py#L259. ### Minimal code sample. ```python; sc.tl.leiden(adata, resolution=0.8, key_added=""leiden_0.8""); assert ""leiden_0.8"" not in adata.uns; params = adata.uns[""leiden""] . sc.tl.leiden(adata, resolution=1.2, key_added=""leiden_1.2""); assert ""leiden_1.2"" not in adata.uns; overwritten_params = adata.uns[""leiden""] ; assert params == overwritten_params # should fail; ```. ### Error output. _No response_. ### Versions. <details>; Confirmed that params are overwritten in source in main branch. (see permalinks); </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2887:658,parameteriz,parameterization,658,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2887,1,['parameteriz'],['parameterization']
Modifiability,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened?. cc: @Intron7 . The array types returned for the various aggregations in `sc.get.aggregate` are different (see example). This can lead to somewhat confusing behavior downstream, especially while we are using the sparse matrix classes. I would suggest we default to a dense result and consider adding an argument `array_type` that determines the type of the arrays added to `layers`. ### Minimal code sample. ```python; import scanpy as sc. adata = sc.datasets.pbmc3k_processed().raw.to_adata(). aggregated = sc.get.aggregate(adata, ""louvain"", [""sum"", ""count_nonzero""]); type(aggregated.layers[""sum""]); # numpy.ndarray. type(aggregated.layers[""count_nonzero""]); # scipy.sparse._csr.csr_matrix; ```. ### Error output. _No response_. ### Versions. <details>. ```; -----; anndata 0.10.5.post1; scanpy 1.10.0.dev315+gf6d5ac94; -----; IPython 8.20.0; PIL 10.2.0; asciitree NA; asttokens NA; cloudpickle 3.0.0; cycler 0.12.1; cython_runtime NA; dask 2024.1.1; dateutil 2.8.2; decorator 5.1.1; executing 2.0.1; fasteners 0.19; h5py 3.10.0; igraph 0.11.3; jedi 0.19.1; jinja2 3.1.3; joblib 1.3.2; kiwisolver 1.4.5; legacy_api_wrap NA; leidenalg 0.10.2; llvmlite 0.41.1; markupsafe 2.1.4; matplotlib 3.8.2; mpl_toolkits NA; msgpack 1.0.7; natsort 8.4.0; numba 0.58.1; numcodecs 0.12.1; numpy 1.26.3; packaging 23.2; pandas 2.2.0; parso 0.8.3; pexpect 4.9.0; prompt_toolkit 3.0.43; psutil 5.9.8; ptyprocess 0.7.0; pure_eval 0.2.2; pygments 2.17.2; pyparsing 3.1.1; pytz 2023.4; scipy 1.12.0; session_info 1.0.0; six 1.16.0; sklearn 1.4.0; sparse 0.15.1; stack_data 0.6.3; tblib 3.0.0; texttable 1.7.0; threadpoolctl 3.2.0; tlz 0.12.1; toolz 0.12.1; traitlets 5.14.1; wcwidth 0.2.13; yaml 6.0.1; zarr 2.16.1; zipp NA; -----; ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2892:662,layers,layers,662,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2892,3,['layers'],['layers']
Modifiability,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. Hi! I am not sure if this is a bug... ; Every time I rescale the `pbmc3k_processed` matrix using it as input in the scanpy `sc.pp.scale` function, I get a very slightly different matrix in output, enough to generate a different UMAP with each run. But if I rewrite it using numpy in a simple function called `my_scale_function` it outputs the exact same matrix as the input, generating the same UMAP down the line... Could someone explain to me what is happening?; (Note: The matrix is not sparse). ### Minimal code sample. ```python; import scanpy as sc; import numpy as np; ### Loading and preprocessing data; adata = sc.datasets.pbmc3k_processed(). ### Defining scale function; def mean_var(X, axis=0):; mean = np.mean(X, axis=axis, dtype=np.float64); mean_sq = np.multiply(X, X).mean(axis=axis, dtype=np.float64); var = mean_sq - mean**2; # enforce R convention (unbiased estimator) for variance; var *= X.shape[axis] / (X.shape[axis] - 1); return mean, var; def my_scale_function(X, clip=False):; mean, var = mean_var(X, axis=0); X -= mean; std = np.sqrt(var); std[std == 0] = 1; X /= std; if clip:; X = np.clip(X, -10, 10); return np.matrix(X). ### Scanpy scale vs my_scale_function; mtx = adata.X; from scipy.sparse import issparse; print(""mtx is parse="" + str(issparse(np.matrix(mtx))) + ""\n""); print(""Rescaled with my_scale_function:""); mtx_rescaled = my_scale_function(mtx); print((mtx == mtx_rescaled).all()); print(""Rescaled with scanpy:""); mtx_rescaled = sc.pp.scale(mtx, zero_center=True, max_value=None, copy=True); print(str((np.matrix(mtx) == mtx_rescaled).all()) + ""\n""); print(""\nOriginal matrix:""); print(mtx); print(""\nMatrix rescaled with scanpy:""); print(mtx_rescaled); ```. ### Error ou",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2629:548,rewrite,rewrite,548,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2629,1,['rewrite'],['rewrite']
Modifiability,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. Running the highly_variable_genes function produced an error. ### Minimal code sample. ```python; sc.pp.highly_variable_genes(test_adata, min_mean=0.0125, max_mean=3, min_disp=0.25); ```. ### Error output. ```pytb; ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); Cell In[101], line 2; 1 # Identify highly-variable genes and plot; ----> 2 sc.pp.highly_variable_genes(test_adata, min_mean=0.0125, max_mean=3, min_disp=0.25). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/scanpy/preprocessing/_highly_variable_genes.py:440, in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key, check_values); 428 return _highly_variable_genes_seurat_v3(; 429 adata,; 430 layer=layer,; (...); 436 inplace=inplace,; 437 ); 439 if batch_key is None:; --> 440 df = _highly_variable_genes_single_batch(; 441 adata,; 442 layer=layer,; 443 min_disp=min_disp,; 444 max_disp=max_disp,; 445 min_mean=min_mean,; 446 max_mean=max_mean,; 447 n_top_genes=n_top_genes,; 448 n_bins=n_bins,; 449 flavor=flavor,; 450 ); 451 else:; 452 sanitize_anndata(adata). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/scanpy/preprocessing/_highly_variable_genes.py:223, in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor); 219 # retrieve those genes that have nan std, these are the ones where; 220 # only a single gene fell in the bin and implicitly set them to have; 221 # a normalized disperion of 1; 222 one_gene_per_bin = disp_std_bin.isnull(); --> 223 gen_indices = np.where(one_gene_per_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2547:669,variab,variable,669,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2547,1,['variab'],['variable']
Modifiability,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. The test suite keeps failing with a segfault on the `python=3.9` build. I haven't been able to reproduce locally. Interestingly, I haven't seen it error when I rerun the check. It looks like this always happens during the call to `nn_approx`. ### Minimal code sample. ```python; NA; ```. ### Error output. ```pytb; platform linux -- Python 3.9.18, pytest-8.0.1, pluggy-1.4.0 -- /opt/hostedtoolcache/Python/3.9.18/x64/bin/python; cachedir: .pytest_cache; rootdir: /home/vsts/work/1/s; configfile: pyproject.toml; testpaths: scanpy; plugins: nunit-1.0.6, mock-3.12.0; [1mcollecting ... [0mcollected 1474 items. scanpy/_utils/compute/is_constant.py::scanpy._utils.compute.is_constant.is_constant [32mPASSED[0m[32m [ 0%][0m; scanpy/datasets/_ebi_expression_atlas.py::scanpy.datasets._ebi_expression_atlas.ebi_expression_atlas [32mPASSED[0m[32m [ 0%][0m; scanpy/external/pl.py::scanpy.external.pl.phate [33mSKIPPED[0m (needs modul...)[32m [ 0%][0m; scanpy/external/pp/_bbknn.py::scanpy.external.pp._bbknn.bbknn [33mSKIPPED[0m[32m [ 0%][0m; scanpy/external/pp/_harmony_integrate.py::scanpy.external.pp._harmony_integrate.harmony_integrate [32mPASSED[0m[32m [ 0%][0m; scanpy/external/pp/_hashsolo.py::scanpy.external.pp._hashsolo.hashsolo [33mSKIPPED[0m[32m [ 0%][0m; scanpy/external/pp/_magic.py::scanpy.external.pp._magic.magic [32mPASSED[0m[32m [ 0%][0m; scanpy/external/pp/_scanorama_integrate.py::scanpy.external.pp._scanorama_integrate.scanorama_integrate Fatal Python error: Illegal instruction. Thread 0x00007f00347c4640 (most recent call first):; File ""/opt/hostedtoolcache/Python/3.9.18/x64/lib/python3.9/threading.py"", line 316 in wait; File ""/opt/hostedtoolcache/Python/3.9.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2866:775,config,configfile,775,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2866,2,"['config', 'plugin']","['configfile', 'plugins']"
Modifiability,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [x] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened?. When doing HVG selection with batch_key = ""something"" and subset = True, I noticed unexpected genes to be selected in the subset anndata. Upon further investigation, it seems that somehow the inplace subsetting goes wrong. (Though I checked the source code and could not find any issue that may explain it there.). ### Minimal code sample. ```python; import scanpy as sc; import numpy as np; np.random.seed(0). # Get AnnData; adata = sc.datasets.pbmc3k(); adata.layers['counts'] = adata.X.copy().tocsr(); adata.obs[""Age""] = np.random.randint(0, 6, (2700,)); adata.obs[""Age""] = adata.obs[""Age""].astype('category'). # Filter genes, preprocess; sc.pp.filter_genes(adata, min_counts = 10); sc.pp.normalize_total(adata); sc.pp.log1p(adata). # Subset = False; ad_nosub = adata.copy(); sc.pp.highly_variable_genes(ad_nosub, n_top_genes = 1000, batch_key = ""Age"", subset = False). # Subset = False, manual subset afterwards; ad_nosub_subbed = ad_nosub.copy(); ad_nosub_subbed._inplace_subset_var(ad_nosub_subbed.var[""highly_variable""].to_numpy()). # Subset = True; ad_sub = adata.copy(); sc.pp.highly_variable_genes(ad_sub, n_top_genes = 1000, batch_key = ""Age"", subset = True); ```. ### Error output. ```pytb; >>> # As expected; >>> print(np.sum(ad_nosub.var[""highly_variable""])); 1000; >>> ; >>> # As expected; >>> print(np.sum(ad_nosub_subbed.var[""highly_variable""])); 1000; >>> ; >>> # Not as expected; >>> print(np.sum(ad_sub.var[""highly_variable""])); 101; ```. ### Versions. <details>. ``` bash; → conda list | grep scanpy; scanpy 1.10.1 pyhd8ed1ab_0 conda-forge. → conda list | grep anndata; anndata 0.10.7 pyhd8ed1ab_0 conda-forge; ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3027:751,layers,layers,751,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3027,1,['layers'],['layers']
Modifiability,"### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [x] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened?. Trying to store normalised values in a layer 'normalised', then plot from that layer with sc.pl.highest_expr_genes(). But the function fails with the layer parameter. ### Minimal code sample. ```py; import numpy as np; import pandas as pd; import anndata as ad. # Create a small data matrix; data = np.random.rand(10, 5). # Create observation (cell) and variable (gene) annotations; obs = pd.DataFrame(index=[f'Cell_{i}' for i in range(data.shape[0])]); var = pd.DataFrame(index=[f'Gene_{i}' for i in range(data.shape[1])]). # Create the AnnData object; adata = ad.AnnData(X=data, obs=obs, var=var). # Test layer call function; adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test; sc.pl.highest_expr_genes(adata, layer='normalised'); ```. ### Error output. ```pytb; Output exceeds the size limit. Open the full output data in a text editor; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); Cell In[32], line 17; 15 # Test layer call function; 16 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test; ---> 17 sc.pl.highest_expr_genes(adata, layer='normalised'); 19 # Test layer call function; 20 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3318:643,variab,variable,643,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3318,2,"['layers', 'variab']","['layers', 'variable']"
Modifiability,"### What kind of feature would you like to request?. Additional function parameters / changed functionality / changed defaults?. ### Please describe your wishes. ### Issue Description; In examining the `MatrixPlot` class within the provided code, it appears that the documentation for some parameters could be enhanced for clarity and completeness. This improvement is crucial for users to understand how to effectively utilize the class and its functionalities. ### Specific Areas for Improvement; While the overall structure of the documentation is good, certain parameters are not described in detail, which might lead to ambiguity in their application. Notably:. - **Parameters like `use_raw`, `log`, `num_categories`, `categories_order`, etc.**: The existing documentation does not provide enough context or explanation about what each of these parameters does, their expected data types, default values, and how they influence the behavior of the plot. - **Complex Parameters**: Parameters that involve more complex concepts or data structures, such as `var_names`, `groupby`, `var_group_positions`, and `values_df`, would benefit significantly from more detailed descriptions and examples. - **Method `style` and Its Parameters**: The `style` method within the `MatrixPlot` class modifies plot visual parameters, but the implications and use cases of changing parameters like `cmap`, `edge_color`, and `edge_lw` are not well-explained. ### Suggested Improvements; To address these issues, I recommend the following enhancements:. 1. **Detailed Parameter Explanations**: Expand on the description of each parameter, especially those that are complex or not self-explanatory. This should include the type of data expected, default values, and a clear explanation of the parameter’s role and impact. 2. **Include Examples and Use Cases**: For complex parameters, providing examples or typical use cases can be extremely helpful. This could be in the form of small code snippets or scenarios illust",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2766:310,enhance,enhanced,310,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2766,1,['enhance'],['enhanced']
Modifiability,"### What kind of feature would you like to request?. Additional function parameters / changed functionality / changed defaults?. ### Please describe your wishes. #### Summary; Integration of the `polars` and `fast_matrix_market` libraries into Scanpy's data loading functions, specifically `scanpy.read_10x_mtx` and `scanpy.read_mtx`. This will improve the loading speed of `.mtx` and `.csv` files, which is crucial for handling large-scale single-cell datasets more efficiently. #### The problem; The current data loading mechanisms in Scanpy, while effective for small to medium datasets, could be substantially optimized for speed when dealing with larger datasets. #### Expected Impact; - Reduced loading times; - Improving the user experience; - Enhanced scalability. #### Code snipped. ```; import fast_matrix_market; import os; import scanpy as sc; import scipy as sp. def read_10x_faster(; path: str; )-> sc.AnnData:; """"""; Read a sparse matrix in Matrix Market format and two CSV files with gene and cell metadata; into an AnnData object.; ; Args:; path: Path to the directory containing the matrix.mtx, genes.tsv, and barcodes.tsv files.; ; Returns:; An AnnData object with the matrix, gene metadata, and cell metadata. """"""; mtx_file = os.path.join(path, ""matrix.mtx""); gene_info = os.path.join(path, ""genes.tsv""); cell_metadata = os.path.join(path, ""barcodes.tsv""); ; # Read the .mtx file into a sparse matrix using the fast_matrix_market package (faster than scanpy, uses multiprocessing); mtx = fast_matrix_market.mmread(mtx_file). # Convert the sparse matrix to a CSR matrix; # Otherwise you will not be able to use it with scanpy; if isinstance(mtx, sp.sparse.coo.coo_matrix):; mtx = mtx.tocsr(); ; # Create an AnnData object; adata = sc.AnnData(X=mtx.T). # Polars is faster than pandas reading csv files; # Read the gene names and cell names into the AnnData object; adata.var = pl.read_csv(gene_info, separator= '\t', has_header=False).to_pandas(); ; # Read the cell names and cell met",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2846:751,Enhance,Enhanced,751,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2846,1,['Enhance'],['Enhanced']
Modifiability,"### What kind of feature would you like to request?. New plotting function: A kind of plot you would like to seein `sc.pl`?. ### Please describe your wishes. Hello Scanpy,; Thank you for developing this amazing package.; I have 2 categorical variables in `adata.obs`; ```python; adata_sub2.obs['patient']; AAACCTGAGGTTACCT-1-C106_N_1_1_0_c1_v2 C106_N; AAACCTGGTCAGAAGC-1-C106_N_1_1_0_c1_v2 C106_N; AAACCTGGTGCTCTTC-1-C106_N_1_1_0_c1_v2 C106_N; AAACCTGTCCCATTAT-1-C106_N_1_1_0_c1_v2 C106_N; AAACCTGTCGGAGCAA-1-C106_N_1_1_0_c1_v2 C106_N; ... ; TTTGCGCAGACACGAC-1-SMC25T SMC25T; TTTGCGCCATGGAATA-1-SMC25T SMC25T; TTTGCGCCATGTTCCC-1-SMC25T SMC25T; TTTGGTTGTAGGGTAC-1-SMC25T SMC25T; TTTGTCAAGAGGGATA-1-SMC25T SMC25T; Name: patient, Length: 44245, dtype: category; Categories (39, object): ['C106_N', 'C126_N', 'C130_N', 'C133_N', ..., 'C138_T', 'C168_T', 'SMC17T', 'SMC20T']; ```; ```python; adata_sub2.obs['type']; AAACCTGAGGTTACCT-1-C106_N_1_1_0_c1_v2 Normal; AAACCTGGTCAGAAGC-1-C106_N_1_1_0_c1_v2 Normal; AAACCTGGTGCTCTTC-1-C106_N_1_1_0_c1_v2 Normal; AAACCTGTCCCATTAT-1-C106_N_1_1_0_c1_v2 Normal; AAACCTGTCGGAGCAA-1-C106_N_1_1_0_c1_v2 Normal; ... ; TTTGCGCAGACACGAC-1-SMC25T NMCAD; TTTGCGCCATGGAATA-1-SMC25T NMCAD; TTTGCGCCATGTTCCC-1-SMC25T NMCAD; TTTGGTTGTAGGGTAC-1-SMC25T NMCAD; TTTGTCAAGAGGGATA-1-SMC25T NMCAD; Name: type, Length: 44245, dtype: category; Categories (3, object): ['Normal', 'NMCAD', 'MCAD']; ```. I know Scanpy has `sc.pl.matrixplot()`, and we can make matrix plot for gene set score, like; ```python; sc.pl.matrixplot(adata_sub2, var_names=['Normal_signature'], groupby='patient', use_raw=True, dendrogram=False, cmap='inferno', standard_scale='var', swap_axes=True, save='27.pdf'); ```; <img width=""1052"" alt=""image"" src=""https://github.com/scverse/scanpy/assets/75048821/ccde5e13-ae61-4a04-b098-e693e9543103"">. Could you please let me know whether Scanpy can make a matrixplot-like plot showing the 'type' grouped by 'patient'? like:; ```python; sc.pl.matrixplot(adata_sub2, var_na",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2797:242,variab,variables,242,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2797,1,['variab'],['variables']
Modifiability,"### What kind of feature would you like to request?. Other?. ### Please describe your wishes. There are two levels of shared parameters that people could benefit from:. 1. Multiple Notebooks that work with the same data and could benefit from shared configuration, e.g. labels and color maps defined for a certain axis/annotation; 2. Plotting using the same labels, color map or so. This could be achieved using object oriented plotting (todo issue number). Having shared configuration files could be achieved either by direct support in the plotting functions (`x='cell type'`) or by adding a convenience function that loads a config object.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2767:250,config,configuration,250,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2767,3,['config'],"['config', 'configuration']"
Modifiability,"### symlink installs being uninstalled **(most important)**. > No idea why it sees “1.7.0rc2” and decides “I’ll update this even when not asked to update”. Maybe raise this issue with pip?. Yeah, this is super weird. I think it's also blocking for adopting `flit` as recommended way to install scanpy to a dev environment. I also raised this on the call yesterday, and I don't think anyone disagreed with this assessment. I see two paths forward here:. * You're able to solve this in this PR; * We merge mostly as is, but we add back `pip install -e` as a way to make a development environment, and add a `.. note` to the flit installation instructions warning people about this behaviour. I would also want a commitment from you to look into this issue. ### Pinning Pip on CI. > Usually when something does an arbitrary change making our life harder, our approach is pinning it temporarily until it fixed that or the infrastructure has adapted to its whims, right?. I still have the concern that pinning pip to an old version could lead to problems, especially while pip is going through a lot of changes. But we can leave this for now. If getting this wheel issue solved drags on for multiple pip versions, we may need to reconsider. ### PEP stuff. > I see you already commented in `pypa/pip#9628`. I think that conversation is happening in multiple places, so might be hard to track. ### Installing from the repo. As it stands:. ```python; conda create -n scanpyenv python=3.8; https://github.com/theislab/scanpy.git; cd scanpy; pip install .; ```. Will error, unless the commit at the tip of master happens to be tagged with a release version. Right now I don't think this is an issue since I wouldn't expect anyone to install from github unless they were setting up a development environment. And if they are setting up a dev environment, they should be using `pip install -e` or `flit install -s`. . I'm not 100% confident this isn't an issue, and it would be good to get more opinions on this. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1527#issuecomment-783849659:937,adapt,adapted,937,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-783849659,1,['adapt'],['adapted']
Modifiability,"'; obsm: 'X_pca', 'X_umap', 'X_tsne'; varm: 'PCs'. tiss_facs[tiss_facs.obs['cell_ontology_class']=='keratinocyte stem cell']; ---------------------------------------------------------------------------; IndexError Traceback (most recent call last); <ipython-input-82-428532769794> in <module>(); ----> 1 tiss_facs[tiss_facs.obs['cell_ontology_class']=='keratinocyte stem cell']. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index); 1299 def __getitem__(self, index):; 1300 """"""Returns a sliced view of the object.""""""; -> 1301 return self._getitem_view(index); 1302 ; 1303 def _getitem_view(self, index):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index); 1303 def _getitem_view(self, index):; 1304 oidx, vidx = self._normalize_indices(index); -> 1305 return AnnData(self, oidx=oidx, vidx=vidx, asview=True); 1306 ; 1307 def _remove_unused_categories(self, df_full, df_sub, uns):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx); 662 if not isinstance(X, AnnData):; 663 raise ValueError('`X` has to be an AnnData object.'); --> 664 self._init_as_view(X, oidx, vidx); 665 else:; 666 self._init_as_actual(. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _init_as_view(self, adata_ref, oidx, vidx); 713 raise KeyError('Unknown Index type'); 714 # fix categories; --> 715 self._remove_unused_categories(adata_ref.obs, obs_sub, uns_new); 716 self._remove_unused_categories(adata_ref.var, var_sub, uns_new); 717 # set attributes. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _remove_unused_categories(self, df_full, df_sub, uns); 1318 uns[k + '_colors'] = np.array(uns[k + '_colors'])[; 1319 np.where(np.in1d(; -> 1320 all_categories, df_sub[k].cat.categories))[0]]; 1321 ; 1322 def rename_categories(self, key, categories):. IndexError: index 6 is out of bounds for axis 1 with size 6; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/363#issuecomment-440038619:1907,layers,layers,1907,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363#issuecomment-440038619,1,['layers'],['layers']
Modifiability,"'] as well; The documentation is a bit murky; not sure if that is the expected behavior when layer is unspecified, but; such default behavior would undermine anyone who wishes to save the count information before RPKM normalization. ### Minimal code sample (that we can copy&paste without having any data). ```python; # Your code here; adata = sc.datasets.pbmc3k(); adata.layers['counts'] = adata.X; cell = adata.obs.index[1]; adata.var['mt'] = adata.var_names.str.startswith('MT-') # annotate the group of mitochondrial genes as 'mt'; sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True). print(""Run 1: initial values after simple processing: ""); print('sum of count layer in designated cell: ', adata[cell,:].layers['counts'].sum()); print('obs[total_counts] value in cell: ', adata[cell,:].obs['total_counts'][0]); print('.X.sum() value in cell: ', adata[cell,:].X.sum()); print('sum of count layer of MALAT1 in cell: ', adata[cell,'MALAT1'].layers['counts']); print('.X value of MALAT1 in cell: ', adata[cell,'MALAT1'].X). print(""\nRun 2: after sc.pp.normalize_total: ""); sc.pp.normalize_total(adata, target_sum=1e4); print('sum of count layer in designated cell: ', adata[cell,:].layers['counts'].sum()) # Note that this changed too; print('obs[total_counts] value in cell: ', adata[cell,:].obs['total_counts'][0]); print('.X.sum() value in cell: ', adata[cell,:].X.sum()); print('sum of count layer of MALAT1 in cell: ', adata[cell,'MALAT1'].layers['counts']); print('.X value of MALAT1 in cell: ', adata[cell,'MALAT1'].X). adata = sc.datasets.pbmc3k(); adata.layers['counts'] = adata.X; cell = adata.obs.index[1]; adata.var['mt'] = adata.var_names.str.startswith('MT-') # annotate the group of mitochondrial genes as 'mt'; sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True). print(""\nRun 3: normalization, specifing argument layer=None""); sc.pp.normalize_total(adata, target_sum=1e4, layer = None); print('su",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2389:1619,layers,layers,1619,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2389,1,['layers'],['layers']
Modifiability,"(sorry, I messed up my branch, so sending a new PR). I added a new batch_key option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe.; The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via `np.nanmean`. Two new columns are created 1) ""in how many batches a gene is detected as hvg"". 2) intersection of all HVGs across batches. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python; import scanpy as sc; import numpy as np; import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo; ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs); sc.pp.normalize_per_cell(ad); sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'); sc.pp.pca(ad); sc.pp.neighbors(ad); sc.tl.umap(ad); sc.pl.umap(ad, color=[""batch"", ""cell_type""]); ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/622:1344,adapt,adapted,1344,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/622,1,['adapt'],['adapted']
Modifiability,"); 1098 else:; 1099 return np.array([[-1]]). ~/.local/lib/python3.8/site-packages/pynndescent/rp_trees.py in rptree_leaf_array_parallel(rp_forest); 1087 ; 1088 def rptree_leaf_array_parallel(rp_forest):; -> 1089 result = joblib.Parallel(n_jobs=-1, require=""sharedmem"")(; 1090 joblib.delayed(get_leaves_from_tree)(rp_tree) for rp_tree in rp_forest; 1091 ). /omics/groups/OE0540/internal/B260/users/olga/.conda/envs/10x_analysis/lib/python3.8/site-packages/joblib/parallel.py in __call__(self, iterable); 1054 ; 1055 with self._backend.retrieval_context():; -> 1056 self.retrieve(); 1057 # Make sure that we get a last message telling us we are done; 1058 elapsed_time = time.time() - self._start_time. /omics/groups/OE0540/internal/B260/users/olga/.conda/envs/10x_analysis/lib/python3.8/site-packages/joblib/parallel.py in retrieve(self); 933 try:; 934 if getattr(self._backend, 'supports_timeout', False):; --> 935 self._output.extend(job.get(timeout=self.timeout)); 936 else:; 937 self._output.extend(job.get()). /omics/groups/OE0540/internal/B260/users/olga/.conda/envs/10x_analysis/lib/python3.8/multiprocessing/pool.py in get(self, timeout); 769 return self._value; 770 else:; --> 771 raise self._value; 772 ; 773 def _set(self, i, obj):. /omics/groups/OE0540/internal/B260/users/olga/.conda/envs/10x_analysis/lib/python3.8/multiprocessing/pool.py in worker(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception); 123 job, i, func, args, kwds = task; 124 try:; --> 125 result = (True, func(*args, **kwds)); 126 except Exception as e:; 127 if wrap_exception and func is not _helper_reraises_exception:. /omics/groups/OE0540/internal/B260/users/olga/.conda/envs/10x_analysis/lib/python3.8/site-packages/joblib/_parallel_backends.py in __call__(self, *args, **kwargs); 593 def __call__(self, *args, **kwargs):; 594 try:; --> 595 return self.func(*args, **kwargs); 596 except KeyboardInterrupt as e:; 597 # We capture the KeyboardInterrupt and reraise it as. /omics/groups/OE0540/internal",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2472:4283,extend,extend,4283,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2472,1,['extend'],['extend']
Modifiability,"); File ""/home/vsts/work/1/s/scanpy/tools/_draw_graph.py"", line 181, in draw_graph; logg.info(; File ""/home/vsts/work/1/s/scanpy/logging.py"", line 244, in info; return settings._root_logger.info(msg, time=time, deep=deep, extra=extra); File ""/home/vsts/work/1/s/scanpy/logging.py"", line 56, in info; return self.log(INFO, msg, time=time, deep=deep, extra=extra); File ""/home/vsts/work/1/s/scanpy/logging.py"", line 43, in log; super().log(level, msg, extra=extra); Message: "" finished: added\n 'X_draw_graph_fr', graph_drawing coordinates (adata.obsm)""; Arguments: (); --- Logging error ---; Traceback (most recent call last):; File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/logging/__init__.py"", line 1084, in emit; stream.write(msg + self.terminator); ValueError: I/O operation on closed file.; Call stack:; File ""/opt/hostedtoolcache/Python/3.8.8/x64/bin/pytest"", line 8, in <module>; sys.exit(console_main()); File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/_pytest/config/__init__.py"", line 185, in console_main; code = main(); File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/_pytest/config/__init__.py"", line 162, in main; ret: Union[ExitCode, int] = config.hook.pytest_cmdline_main(; File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/pluggy/hooks.py"", line 286, in __call__; return self._hookexec(self, self.get_hookimpls(), kwargs); File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/pluggy/manager.py"", line 93, in _hookexec; return self._inner_hookexec(hook, methods, kwargs); File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/pluggy/manager.py"", line 84, in <lambda>; self._inner_hookexec = lambda hook, methods, kwargs: hook.multicall(; File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/pluggy/callers.py"", line 187, in _multicall; res = hook_impl.function(*args); File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/_pytes",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1736:2871,config,config,2871,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1736,1,['config'],['config']
Modifiability,"); The kind of thing the variables are.; qc_vars : `Container`, optional (default: `()`); Keys for boolean columns of `.var` which identify variables you could ; want to control for (e.g. ""ERCC"" or ""mito"").; percent_top : `Container[int]`, optional (default: `(50, 100, 200, 500)`); Which proportions of top genes to cover. If empty or `None` don't; calculate.; inplace : bool, optional (default: `False`); Whether to place calculated metrics in `.obs` and `.var`. Returns; -------; Union[NoneType, Tuple[pd.DataFrame, pd.DataFrame]]; Depending on `inplace` returns calculated metrics (`pd.DataFrame`) or; updates `adata`'s `obs` and `var`. Observation level metrics include:. * `total_{var_type}_by_{expr_type}`; E.g. ""total_genes_by_counts"". Number of genes with positive counts ; in a cell.; * `total_{expr_type}`; E.g. ""total_counts"". Total number of counts for a cell.; * `pct_{expr_type}_in_top_{n}_{var_type}` - for `n` in `percent_top`; E.g. ""pct_counts_in_top_50_genes"". Cumulative percentage of counts ; for 50 most expressed genes in a cell.; * `total_{expr_type}_{qc_var}` - for `qc_var` in `qc_vars`; E.g. ""total_counts_mito"". Total number of counts for variabes in ; `qc_vars`.; * `pct_{expr_type}_{qc_var}` - for `qc_var` in `qc_vars`; E.g. ""pct_counts_mito"". Proportion of total counts for a cell which ; are mitochondrial. Variable level metrics include:. * `total_{expr_type}`; E.g. ""total_counts"". Sum of counts for a gene.; * `mean_{expr_type}`; E.g. ""mean counts"". Mean expression over all cells.; * `n_cells_by_{expr_type}`; E.g. ""n_cells_by_counts"". Number of cells this expression is ; measured in.; * `pct_dropout_by_{expr_type}`; E.g. ""pct_dropout_by_counts"". Percentage of cells this feature does ; not appear in.; . Example; -------; Calculate qc metrics for visualization. >>> adata = sc.datasets.pbmc3k(); >>> sc.pp.calculate_qc_metrics(adata, inplace=True); >>> sns.jointplot(adata.obs, ""log1p_total_counts"", ""log1p_n_genes_by_counts"", kind=""hex""); """"""; ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/424#issuecomment-454024688:2046,variab,variabes,2046,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/424#issuecomment-454024688,2,"['Variab', 'variab']","['Variable', 'variabes']"
Modifiability,"); ```. ### Error output. ```pytb; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); Cell In[37], line 1; ----> 1 sc.pl.scatter(adata, x='total_counts', y='n_genes_by_counts', color='reference', show=True). File /project/hipaa_ycheng11lab/atlas/CAMR2024/py311env/lib/python3.11/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /project/hipaa_ycheng11lab/atlas/CAMR2024/py311env/lib/python3.11/site-packages/scanpy/plotting/_anndata.py:166, in scatter(adata, x, y, color, use_raw, layers, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, legend_fontoutline, color_map, palette, frameon, right_margin, left_margin, size, marker, title, show, save, ax); 160 raise ValueError(""Either provide a `basis` or `x` and `y`.""); 161 if (; 162 (x in adata.obs.keys() or x in var_index); 163 and (y in adata.obs.keys() or y in var_index); 164 and (color is None or color in adata.obs.keys() or color in var_index); 165 ):; --> 166 return _scatter_obs(**args); 167 if (; 168 (x in adata.var.keys() or x in adata.obs.index); 169 and (y in adata.var.keys() or y in adata.obs.index); 170 and (color is None or color in adata.var.keys() or color in adata.obs.index); 171 ):; 172 adata_T = adata.T. File /project/hipaa_ycheng11lab/atlas/CAMR2024/py311env/lib/python3.11/site-packages/scanpy/plotting/_anndata.py:521, in _scatter_obs(adata, x, y, color, use_raw, layers, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, legend_fontoutline, color_map, palette, frameon, right_margin, left_margin, size, marker",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3102:1630,layers,layers,1630,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3102,1,['layers'],['layers']
Modifiability,"* How about a `var_type` argument which defaults to `""genes""`? Could even replace occurrences of `cells` with a `obs_type` variable; * Agreed, `expr_type` it is.; * I'd considered an option of a suffix argument, but I think there are some issues with it being a string. If `suffix=None, expr_type=""counts""` is it still `total_counts`? If `suffix=""_by_counts""` is it still `total_counts`? A possible solution to this is making suffix a boolean argument, where `True` adds `_by_{expr_type}` as appropriate.; * Additionally `n_genes` in particular is already an overloaded term, as it's already used with multiple different meanings in the codebase (typically number of genes to consider, or number of columns in the data matrix). ; * Could you say a bit more about the conditioning? I'm not sure I follow how it would be confusing here.; * `control_variables` definitely makes more sense here than `variables`, I was just wondering if there's a better word than `control`.; * If there was an equivalent for observations (something like `mean_counts_in_donor1`), what would be a good name for that argument?; * I think I'm happy with `n_...` for some things, `total_...` for others.; * On other convention I'm unsure about, should the range of the `pct_...` values be 0-100 or 0-1?. ---------------. Here's what I'm thinking it'll look like right now (pending `suffix` decision). For obs:. | current | proposed |; | ------- | -------- |; |`total_features_by_{expr_values}` | `n_{var_type}_by_{expr_type}`|; |`total_{expr_values}` | `total_{expr_type}`|; |`pct_{expr_values}_in_top_{n}_features` | `pct_{expr_type}_in_top_{n}_{var_type}`|; |`total_{expr_values}_{feature_control}` | `total_{expr_type}_{control_var}`|; |`pct_{expr_values}_{feature_control}` | `pct_{expr_type}_{control_var}`|. For var:. | current | proposed |; | ------- | -------- |; |`total_{expr_values}` | `total_{expr_type}`|; |`mean_{expr_values}` | `mean_{expr_type}`|; |`n_cells_by_{expr_values}` | `n_cells_by_{expr_type}`|; |`pc",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/316#issuecomment-436498986:123,variab,variable,123,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-436498986,2,['variab'],"['variable', 'variables']"
Modifiability,"**Tests**. For the present changes, these tests are sufficient. Only since recently, Scanpy is becoming properly tested via the example notebooks (previously, I always ran everything manually). So, if you want to test this properly, come up with an interesting use case that uses clustering on weighted graphs, make a pull request for a notebook on `theislab/scanpy_usage`, link to it in the docs (https://scanpy.readthedocs.io/en/latest/examples.html) and commit a corresponding test in `tests/notebooks`. That's a lot of work and I think too much for this present case. Eventually, most of Scanpy's functionality should be tested this way. A lot will also be covered by extending existing notebooks. But I don't think it's possible to meaningfully add the weighted graphs to the present standard Louvain clustering example. **Multiple network representations**. I don't mind as long as the standard user still uses a single one - otherwise people will get confused. But let's not call these grahs ""networks"". The notion ""network"" suggests that nodes ""interact"". The current graphs in Scanpy, however, are all proxies for manifolds. Edges only represent similarity and, through that, have a topological interpretation. I'd really like to reserve the notion ""network"" for cases where nodes possibly or actually interact. Hence, a gene regulatory network or a network of cells with cell-cell interactions. Both are completely different things compared to the ""neighbors"" stuff in Scanpy. **Allow more choice of partition method for louvain-igraph package**. Fine for me as long as the default stays unchanged. I'm not sure whether it adds a lot of value but it will also not be harmful. ---. Could you also fix the docs for the `flavor` parameter and specify the default? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/248#issuecomment-418079395:672,extend,extending,672,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248#issuecomment-418079395,1,['extend'],['extending']
Modifiability,"**The following is what this function does (we can see it with ?sc.pp.recipe_zheng17):**; ```; sc.pp.filter_genes(adata, min_counts=1) # only consider genes with more than 1 count; sc.pp.normalize_per_cell( # normalize with total UMI count per cell; adata, key_n_counts='n_counts_all'); filter_result = sc.pp.filter_genes_dispersion( # select highly-variable genes; adata.X, flavor='cell_ranger', n_top_genes=n_top_genes, log=False); adata = adata[:, filter_result.gene_subset] # subset the genes; sc.pp.normalize_per_cell(adata) # renormalize after filtering; if log: sc.pp.log1p(adata) # log transform: adata.X = log(adata.X + 1); sc.pp.scale(adata) # scale to unit variance and shift to zero mean; ```. **But in the original paper Zheng et al. (2017 (https://www.nature.com/articles/ncomms14049#Sec11), it said:**; Only genes with at least one UMI count detected in at least one cell are used. UMI normalization was performed by first dividing UMI counts by the total UMI counts in each cell, **followed by multiplication with the median of the total UMI counts across cells**. Then, we took the natural log of the UMI counts. Finally, each gene was normalized such that the mean signal for each gene is 0, and standard deviation is 1. **So, comparing these two pipelines, the pipeline implemented in scanpy is not the same with the method described in the original paper, in the paper, there is a step**: _multiplication with the median of the total UMI counts across cells_, but this step was skipped inside the function sc.pp.recipe_zheng17. **Is there anyone who can tell me why they are different?** @flying-sheep",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/905:350,variab,variable,350,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/905,1,['variab'],['variable']
Modifiability,"+1, I have the same error (on different data), which also only seems to appear when I don't filter to leave only the highly variable genes. In my case,. ```; np.any(adata.X.sum(axis=0) == 0); np.any(adata.X.sum(axis=1) == 0); ```; both return `False`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/667#issuecomment-519987040:124,variab,variable,124,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/667#issuecomment-519987040,1,['variab'],['variable']
Modifiability,"+unknown anndata==0.6.9 numpy==1.14.5 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.19.1 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 ; ... storing 'blobs' as categorical. ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-16-4cd21e9edf25> in <module>(); 3 adata = sc.datasets.blobs(); 4 sc.tl.pca(adata); ----> 5 sc.pl.pca(adata, components=['1,2', '2,3']). ~/software/scanpy/scanpy/plotting/tools/__init__.py in pca(adata, color, use_raw, sort_order, alpha, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, right_margin, size, title, show, save, ax); 114 title=title,; 115 show=False,; --> 116 save=False, ax=ax); 117 utils.savefig_or_show('pca_scatter', show=show, save=save); 118 if show == False: return axs. ~/software/scanpy/scanpy/plotting/anndata.py in scatter(adata, x, y, color, use_raw, layers, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, frameon, right_margin, left_margin, size, title, show, save, ax); 110 show=show,; 111 save=save,; --> 112 ax=ax); 113 elif x is not None and y is not None:; 114 if ((x in adata.obs.keys() or x in adata.var.index). ~/software/scanpy/scanpy/plotting/anndata.py in _scatter_obs(adata, x, y, color, use_raw, layers, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, frameon, right_margin, left_margin, size, title, show, save, ax); 291 if components is None: components = '1,2' if '2d' in projection else '1,2,3'; 292 if isinstance(components, str): components = components.split(','); --> 293 components = np.array(components).astype(int) - 1; 294 keys = ['grey'] if color is None else [color] if isinstance(color, str) else color; 295 if title is not None and isinstance(title, str):. ValueError: invalid literal for int() with base 10: '1,2'; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/254:1292,layers,layers,1292,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/254,2,['layers'],['layers']
Modifiability,", :]; zf_48 = zf_48[zf_48.obs.pct_counts_mt < 10, :]; sc.pp.highly_variable_genes(zf_48, flavor='seurat_v3', span=1); ```; Here is the error:; ```pytb; ValueError Traceback (most recent call last); <ipython-input-170-37cd37b7326e> in <module>; 16 zf_48 = zf_48[zf_48.obs.n_genes_by_counts < 2500, :]; 17 zf_48 = zf_48[zf_48.obs.pct_counts_mt < 10, :]; ---> 18 sc.pp.highly_variable_genes(zf_48, flavor='seurat_v3', span=1). ~/opt/anaconda3/lib/python3.8/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key); 413 ; 414 if flavor == 'seurat_v3':; --> 415 return _highly_variable_genes_seurat_v3(; 416 adata,; 417 layer=layer,. ~/opt/anaconda3/lib/python3.8/site-packages/scanpy/preprocessing/_highly_variable_genes.py in _highly_variable_genes_seurat_v3(adata, layer, n_top_genes, batch_key, span, subset, inplace); 59 X = adata.layers[layer] if layer is not None else adata.X; 60 if check_nonnegative_integers(X) is False:; ---> 61 raise ValueError(; 62 ""`pp.highly_variable_genes` with `flavor='seurat_v3'` expects ""; 63 ""raw count data."". ValueError: `pp.highly_variable_genes` with `flavor='seurat_v3'` expects raw count data.; ```. Am I loading the data in wrong? This processing has worked for data loaded in using 'sc.read_10x_mtx()'. #### Versions. <details>. -----; anndata 0.7.5; scanpy 1.6.0; sinfo 0.3.1; -----; PIL 8.0.1; PyObjCTools NA; anndata 0.7.5; anndata2ri 1.0.5; appnope 0.1.2; attr 20.3.0; backcall 0.2.0; bottleneck 1.3.2; cffi 1.14.4; cloudpickle 1.6.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2020.12.0; dateutil 2.8.1; decorator 4.4.2; get_version 2.1; h5py 3.1.0; idna 2.10; igraph 0.8.3; ipykernel 5.4.2; ipython_genutils 0.2.0; ipywidgets 7.5.1; jedi 0.17.2; jinja2 2.11.2; joblib 1.0.0; jsonschema 3.2.0; kiwisolver 1.3.1; legacy_api_wrap 1.2; leidenalg 0.8.3; llvmlite 0.35.0; louvain 0.7.0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1782:2231,layers,layers,2231,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1782,1,['layers'],['layers']
Modifiability,", alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, legend_fontoutline, color_map, palette, frameon, right_margin, left_margin, size, marker, title, show, save, ax); 160 raise ValueError(""Either provide a `basis` or `x` and `y`.""); 161 if (; 162 (x in adata.obs.keys() or x in var_index); 163 and (y in adata.obs.keys() or y in var_index); 164 and (color is None or color in adata.obs.keys() or color in var_index); 165 ):; --> 166 return _scatter_obs(**args); 167 if (; 168 (x in adata.var.keys() or x in adata.obs.index); 169 and (y in adata.var.keys() or y in adata.obs.index); 170 and (color is None or color in adata.var.keys() or color in adata.obs.index); 171 ):; 172 adata_T = adata.T. File /project/hipaa_ycheng11lab/atlas/CAMR2024/py311env/lib/python3.11/site-packages/scanpy/plotting/_anndata.py:521, in _scatter_obs(adata, x, y, color, use_raw, layers, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, legend_fontoutline, color_map, palette, frameon, right_margin, left_margin, size, marker, title, show, save, ax); 517 legend = axs[ikey].legend(; 518 frameon=False, loc=legend_loc, fontsize=legend_fontsize; 519 ); 520 if legend is not None:; --> 521 for handle in legend.legendHandles:; 522 handle.set_sizes([300.0]); 524 # draw a frame around the scatter. AttributeError: 'Legend' object has no attribute 'legendHandles'; ```. ### Versions. <details>. ```; -----; anndata 0.10.7; scanpy 1.10.1; -----; PIL 10.3.0; cffi 1.16.0; cycler 0.12.1; cython_runtime NA; dateutil 2.9.0.post0; defusedxml 0.7.1; dill 0.3.8; h5py 3.11.0; joblib 1.4.2; kiwisolver 1.4.5; legacy_api_wrap NA; llvmlite 0.42.0; matplotlib 3.9.0; mpl_toolkits NA; natsort 8.4.0; numba 0.59.1; numexpr 2.10.0; numpy 1.26.4; packaging 24.0; pandas 2.2.2; psutil 5.9.8; pyparsing 3.1.2; pytz 2024.1; scipy 1.13.1; session_info 1.0.0; six 1.16.0; sklearn 1.5.0; threadpoolctl 3.5.0; torch 2.3.1+cu121; torchgen NA; tqdm 4.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3102:2549,layers,layers,2549,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3102,1,['layers'],['layers']
Modifiability,", engine, **kwds); 1617 self.options[""has_index_names""] = kwds[""has_index_names""]; 1619 self.handles: IOHandles | None = None; -> 1620 self._engine = self._make_engine(f, self.engine). File ~\AppData\Roaming\Python\Python312\site-packages\pandas\io\parsers\readers.py:1880, in TextFileReader._make_engine(self, f, engine); 1878 if ""b"" not in mode:; 1879 mode += ""b""; -> 1880 self.handles = get_handle(; 1881 f,; 1882 mode,; 1883 encoding=self.options.get(""encoding"", None),; 1884 compression=self.options.get(""compression"", None),; 1885 memory_map=self.options.get(""memory_map"", False),; 1886 is_text=is_text,; 1887 errors=self.options.get(""encoding_errors"", ""strict""),; 1888 storage_options=self.options.get(""storage_options"", None),; 1889 ); 1890 assert self.handles is not None; 1891 f = self.handles.handle. File ~\AppData\Roaming\Python\Python312\site-packages\pandas\io\common.py:765, in get_handle(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options); 761 if compression == ""gzip"":; 762 if isinstance(handle, str):; 763 # error: Incompatible types in assignment (expression has type; 764 # ""GzipFile"", variable has type ""Union[str, BaseBuffer]""); --> 765 handle = gzip.GzipFile( # type: ignore[assignment]; 766 filename=handle,; 767 mode=ioargs.mode,; 768 **compression_args,; 769 ); 770 else:; 771 handle = gzip.GzipFile(; 772 # No overload variant of ""GzipFile"" matches argument types; 773 # ""Union[str, BaseBuffer]"", ""str"", ""Dict[str, Any]""; (...); 776 **compression_args,; 777 ). File c:\Program Files\Python312\Lib\gzip.py:192, in GzipFile.__init__(self, filename, mode, compresslevel, fileobj, mtime); 190 mode += 'b'; 191 if fileobj is None:; --> 192 fileobj = self.myfileobj = builtins.open(filename, mode or 'rb'); 193 if filename is None:; 194 filename = getattr(fileobj, 'name', ''). FileNotFoundError: [Errno 2] No such file or directory: 'GSE212966\\GSM6567159_PDAC2_features.tsv.gz'; ```. ### Versions. <details>. ```; '1.10.2'; ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:24317,variab,variable,24317,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['variab'],['variable']
Modifiability,", min_genes=200) #get rid of cells with fewer than 200 genes; sc.pp.filter_genes(adata, min_cells=3) #get rid of genes that are found in fewer than 3 cells; adata.var['mt'] = adata.var_names.str.startswith('MT-') # annotate the group of mitochondrial genes as 'mt'; sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True); upper_lim = np.quantile(adata.obs.n_genes_by_counts.values, .98); lower_lim = np.quantile(adata.obs.n_genes_by_counts.values, .02); adata = adata[(adata.obs.n_genes_by_counts < upper_lim) & (adata.obs.n_genes_by_counts > lower_lim)]; adata = adata[adata.obs.pct_counts_mt < 25]; sc.pp.normalize_total(adata, target_sum=1e4) #normalize every cell to 10,000 UMI; sc.pp.log1p(adata) #change to log counts; sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5) #these are default values; adata.raw = adata #save raw data before processing values and further filtering; adata = adata[:, adata.var.highly_variable] #filter highly variable; sc.pp.regress_out(adata, ['total_counts', 'pct_counts_mt']) #Regress out effects of total counts per cell and the percentage of mitochondrial genes expressed; sc.pp.scale(adata, max_value=10) #scale each gene to unit variance; sc.tl.pca(adata, svd_solver='arpack'); sc.pp.neighbors(adata, n_neighbors=10, n_pcs=20); sc.tl.umap(adata); return adata. adata = sc.read_csv(""./myfile.csv"", first_column_names=True); adata = pp(adata); ```. My computer is Mac book Intel i5. Thanks!; #### Versions. <details>. -----; anndata 0.8.0; scanpy 1.9.1; -----; OpenSSL 22.0.0; PIL 9.2.0; PyObjCTools NA; absl NA; appnope 0.1.2; astunparse 1.6.3; attr 21.4.0; backcall 0.2.0; bcrypt 3.2.0; beta_ufunc NA; binom_ufunc NA; boto3 1.24.28; botocore 1.27.28; bottleneck 1.3.5; brotli NA; certifi 2022.09.24; cffi 1.15.1; chardet 4.0.0; charset_normalizer 2.0.4; chex 0.1.5; cloudpickle 2.0.0; colorama 0.4.5; contextlib2 NA; cryptography 37.0.1; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2359:1736,variab,variable,1736,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2359,1,['variab'],['variable']
Modifiability,", percent_top, layer, use_raw, inplace, parallel); 281 percent_top=percent_top,; 282 inplace=inplace,; --> 283 X=X,; 284 ); 285 var_metrics = describe_var(. ~\anaconda3\lib\site-packages\scanpy\preprocessing\_qc.py in describe_obs(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, X, parallel); 107 if percent_top:; 108 percent_top = sorted(percent_top); --> 109 proportions = top_segment_proportions(X, percent_top); 110 for i, n in enumerate(percent_top):; 111 obs_metrics[f""pct_{expr_type}_in_top_{n}_{var_type}""] = (. ~\anaconda3\lib\site-packages\scanpy\preprocessing\_qc.py in top_segment_proportions(mtx, ns); 364 mtx = csr_matrix(mtx); 365 return top_segment_proportions_sparse_csr(; --> 366 mtx.data, mtx.indptr, np.array(ns, dtype=np.int); 367 ); 368 else:. ~\anaconda3\lib\site-packages\numba\dispatcher.py in _compile_for_args(self, *args, **kws); 418 e.patch_message('\n'.join((str(e).rstrip(), help_msg))); 419 # ignore the FULL_TRACEBACKS config, this needs reporting!; --> 420 raise e; 421 ; 422 def inspect_llvm(self, signature=None):. ~\anaconda3\lib\site-packages\numba\dispatcher.py in _compile_for_args(self, *args, **kws); 351 argtypes.append(self.typeof_pyval(a)); 352 try:; --> 353 return self.compile(tuple(argtypes)); 354 except errors.ForceLiteralArg as e:; 355 # Received request for compiler re-entry with the list of arguments. ~\anaconda3\lib\site-packages\numba\compiler_lock.py in _acquire_compile_lock(*args, **kwargs); 30 def _acquire_compile_lock(*args, **kwargs):; 31 with self:; ---> 32 return func(*args, **kwargs); 33 return _acquire_compile_lock; 34 . ~\anaconda3\lib\site-packages\numba\dispatcher.py in compile(self, sig); 766 self._cache_misses[sig] += 1; 767 try:; --> 768 cres = self._compiler.compile(args, return_type); 769 except errors.ForceLiteralArg as e:; 770 def folded(args, kws):. ~\anaconda3\lib\site-packages\numba\dispatcher.py in compile(self, args, return_type); 75 ; 76 def compile(self, args, return_type):; --->",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1147:6502,config,config,6502,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1147,1,['config'],['config']
Modifiability,",; 274 obs=new_label_df,; 275 var=getattr(adata, ""var"" if axis == 0 else ""obs""),; 276 ); 278 if axis == 1:; 279 return result.T. File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/anndata/_core/anndata.py:271, in AnnData.__init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, obsp, varp, oidx, vidx); 269 self._init_as_view(X, oidx, vidx); 270 else:; --> 271 self._init_as_actual(; 272 X=X,; 273 obs=obs,; 274 var=var,; 275 uns=uns,; 276 obsm=obsm,; 277 varm=varm,; 278 raw=raw,; 279 layers=layers,; 280 dtype=dtype,; 281 shape=shape,; 282 obsp=obsp,; 283 varp=varp,; 284 filename=filename,; 285 filemode=filemode,; 286 ). File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/anndata/_core/anndata.py:501, in AnnData._init_as_actual(self, X, obs, var, uns, obsm, varm, varp, obsp, raw, layers, dtype, shape, filename, filemode); 498 self._clean_up_old_format(uns); 500 # layers; --> 501 self._layers = Layers(self, layers). File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/anndata/_core/aligned_mapping.py:331, in Layers.__init__(self, parent, vals); 329 self._data = dict(); 330 if vals is not None:; --> 331 self.update(vals). File <frozen _collections_abc>:949, in update(self, other, **kwds). File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/anndata/_core/aligned_mapping.py:199, in AlignedActualMixin.__setitem__(self, key, value); 198 def __setitem__(self, key: str, value: V):; --> 199 value = self._validate_value(value, key); 200 self._data[key] = value. File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/anndata/_core/aligned_mapping.py:89, in AlignedMapping._validate_value(self, val, key); 83 dims = tuple((""obs"", ""var"")[ax] for ax in self.axes); 84 msg = (; 85 f""Value passed for key {key!r} is of incorrect shape. ""; 86 f""Values of {self.attrname} must match dimensions {dims} of parent. ""; 87 f""Value had shape {a",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2929:2328,Layers,Layers,2328,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2929,2,"['Layers', 'layers']","['Layers', 'layers']"
Modifiability,"- I have checked that this issue has not already been reported.; - I have confirmed this bug exists on the latest version of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python; import scanpy.api as sc; ```. ```pytb; File ""./scanpy_normalization.py"", line 4, in <module>; import scanpy.api as sc; File ""/usr/local/lib/python3.8/site-packages/scanpy/api/__init__.py"", line 27, in <module>; from . import pl; File ""/usr/local/lib/python3.8/site-packages/scanpy/api/pl.py"", line 1, in <module>; from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot; ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/usr/local/lib/python3.8/site-packages/scanpy/plotting/_anndata.py); ```. This is with the latest version of scanpy. I looked at the code and scanpy/apt/pl.py still has **from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot**, even as the plotting library has been refactored and the dotplot, matrixplot and stacked_violin are now in separate files. I tested this a few days ago and it was working fine then, the update to anndata probably happened in the last couple of days",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1397:1086,refactor,refactored,1086,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397,1,['refactor'],['refactored']
Modifiability,"- [ ] I have checked that this issue has not already been reported.; - [ ] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python; sc.pp.highly_variable_genes(adata, flavor='cell_ranger', n_top_genes=4000); print('\n','Number of highly variable genes: {:d}'.format(np.sum(adata.var['highly_variable']))); ```. ```pytb; ValueError Traceback (most recent call last); <ipython-input-46-616fc10e63ff> in <module>; ----> 1 sc.pp.highly_variable_genes(adata, flavor='cell_ranger', n_top_genes=4000); 2 print('\n','Number of highly variable genes: {:d}'.format(np.sum(adata.var['highly_variable']))). ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key); 424 ; 425 if batch_key is None:; --> 426 df = _highly_variable_genes_single_batch(; 427 adata,; 428 layer=layer,. ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor); 242 from statsmodels import robust; 243 ; --> 244 df['mean_bin'] = pd.cut(; 245 df['means'],; 246 np.r_[-np.inf, np.percentile(df['means'], np.arange(10, 105, 5)), np.inf],. ~\anaconda3\lib\site-packages\pandas\core\reshape\tile.py in cut(x, bins, right, labels, retbins, precision, include_lowest, duplicates, ordered); 273 raise ValueError(""bins must increase monotonically.""); 274 ; --> 275 fac, bins = _bins_to_cuts(; 276 x,; 277 bins,. ~\anaconda3\lib\site-packages\pandas\core\reshape\tile.py in _bi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1560:604,variab,variable,604,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1560,2,['variab'],['variable']
Modifiability,"- [ ] I have checked that this issue has not already been reported.; - [ ] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data); ```; the simple simulation dataset(filter_gene_dis.h5,which is the adata in the simulation code) is; AnnData object with n_obs × n_vars = 3 × 50; [[ 8. 10. 7. 11. 14. 12. 11. 9. 14. 11. 8. 2. 8. 10. 7. 12. 11. 12.; 12. 10. 3. 11. 13. 7. 8. 11. 14. 9. 11. 9. 5. 13. 8. 13. 9. 15.; 11. 8. 7. 7. 5. 12. 9. 12. 11. 8. 11. 6. 10. 11.]; [19. 22. 19. 23. 20. 16. 13. 26. 20. 29. 22. 16. 19. 22. 24. 20. 19. 15.; 17. 25. 23. 19. 18. 18. 24. 18. 25. 22. 25. 16. 25. 23. 27. 22. 14. 21.; 24. 23. 16. 15. 14. 27. 23. 24. 21. 27. 17. 20. 20. 12.]; [27. 31. 32. 30. 29. 31. 24. 29. 29. 33. 29. 29. 26. 38. 27. 32. 21. 24.; 28. 27. 25. 19. 28. 24. 23. 23. 30. 39. 29. 42. 34. 28. 25. 26. 27. 32.; 28. 35. 34. 26. 27. 22. 24. 42. 30. 32. 29. 28. 29. 34.]]```; ```. ```python; import scanpy as sc; import logging; sc.settings.verbosity = 3 ; adata=sc.read(""filter_gene_dis.h5"")#a simulation dataset; #when n_top_genes=10,it returns 3*10; #when n_top_genes=20,it returns 3*19; sc.pp.filter_genes_dispersion(adata,n_top_genes=20); print(adata); ```. ```pytb; If you pass `n_top_genes`, all cutoffs are ignored.; extracting highly variable genes; finished (0:00:00); AnnData object with n_obs × n_vars = 3 × 19; var: 'means', 'dispersions', 'dispersions_norm'; ```. #### Versions. <details>; scanpy==1.6.0 anndata==0.7.4 umap==0.4.6 numpy==1.18.5 scipy==1.5.0 pandas==1.0.5 scikit-learn==0.23.1 statsmodels==0.11.1 python-igraph==0.8.2 louvain==0.7.0 leidenalg==0.8.1; </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1457:1600,variab,variable,1600,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1457,1,['variab'],['variable']
Modifiability,"- [ x] I have checked that this issue has not already been reported.; - [ ] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---; I have tried to plot `umap` `embedding` with the `groups` parameters set to 4 out of ~200 groups (since palettes with ~200 colors cant be distinguished and scanpy anyways assigns gray to everything). However, even when we have only 5 elements in the legend (NA and 4 groups) they still remain all gray. I thus tried to specify the `palette` as dictionary of group_names:colors (for the four group), but then I get the error that groups that are in fact not gonna be plotted are missing in the palette dict input. . I think it would be nice that when using `groups` the `palette` argument would be flexible enough to take only the colors for these groups.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2380:827,flexible,flexible,827,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2380,1,['flexible'],['flexible']
Modifiability,"- [ x] I have checked that this issue has not already been reported.; - [ x] I have confirmed this bug exists on the latest version of scanpy.; - [ x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python; adata.obs['sex'].cat.categories.tolist(); ```. ```pytb; ['F', 'M', 'U']; ```. ```python; adata.obs['age_groups'].cat.categories.tolist(); ```. ```pytb; ['Old', 'YoungAdult', 'Pediatric', 'Fetal', 'NewBorn']; ```. ```python; sc.pp.combat(adata, key='384plate', covariates=['sex']); ```. ```pytb; Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:; 	sex. Found 0 numerical variables:; 	. Fitting L/S model and finding priors. Finding parametric adjustments. /home/sinhar/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py:340: RuntimeWarning: divide by zero encountered in true_divide; (abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max(); Adjusting data; ```. ```python; sc.pp.combat(adata, key='384plate', covariates=['age_group']); ```. ```pytb; Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:; 	age_group. Found 0 numerical variables:; 	. ---------------------------------------------------------------------------; LinAlgError Traceback (most recent call last); <timed eval> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace); 204 # standardize across genes using a pooled variance estimator; 205 logg.info(""Standardizing Data across genes.\n""); --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key); 207 ; 208 # fitting the parameters on the standardized data. ~/.local/lib/python3.8/site-pa",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1606:868,variab,variables,868,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606,2,['variab'],['variables']
Modifiability,"- [ ✔] I have checked that this issue has not already been reported.; - [✔ ] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python 3.8; # Your code here; ```sc.tl.pca(adata, svd_solver='arpack'); sc.pp.neighbors(adata, n_pcs=50, knn=True); sc.tl.leiden(adata, resolution=10); sc.tl.umap(adata); sc.pl.umap(adata, color=['leiden'], legend_loc='on data', frameon=False, title='', use_raw=False). ```pytb; computing PCA; on highly variable genes; with n_comps=50; finished (0:00:07); computing neighbors; using 'X_pca' with n_pcs = 50; finished: added to `.uns['neighbors']`; `.obsp['distances']`, distances for each pair of neighbors; `.obsp['connectivities']`, weighted adjacency matrix (0:00:10); running Leiden clustering; finished: found 137 clusters and added; 'leiden', the cluster labels (adata.obs, categorical) (0:00:02); computing UMAP; finished: added; 'X_umap', UMAP coordinates (adata.obsm) (0:00:07); the obs value 'leiden' has more than 103 categories. Uniform 'grey' color will be used for all categories.; ```. #### Versions. <details>. scanpy==1.8.2 anndata==0.7.8 umap==0.5.2 numpy==1.20.3 scipy==1.7.2 pandas==1.3.4 scikit-learn==1.0.1 statsmodels==0.13.1 python-igraph==0.9.8 pynndescent==0.5.5. </details>. Hello Scanpy,; When I make >100 clusters, the 'leiden' becomes gray and cannot be changed back in the same notebook.; Could you please help me to solve this issue?; Thanks!; Best; YJ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2058:797,variab,variable,797,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2058,1,['variab'],['variable']
Modifiability,"- [X ] I have checked that this issue has not already been reported.; - [X ] I have confirmed this bug exists on the latest version of scanpy.; - [X ] (optional) I have confirmed this bug exists on the master branch of scanpy. I'm working on comit `63b42e4b` (latest master).; I'm not sure if intended or not but it seems like it would be usefull if one were able to ingest data that don't share 100% of all features. ### Minimal code sample (that we can copy&paste without having any data). ```python; adata = sc.datasets.paul15(); sc.pp.pca(adata_ref); sc.pp.neighbors(adata_ref); sc.tl.leiden(adata_ref); adata = adata_ref[:, :1000].copy() # assume adata_ref has more than 1000 genes.; sc.tl.ingest(adata, adata_ref, obs='leiden'); ```. Error message; ```pytb; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-37-b3cd11e67810> in <module>; ----> 1 sc.tl.ingest(adata, adata_ref, obs='leiden'). ~/projects/scanpy/scanpy/tools/_ingest.py in ingest(adata, adata_ref, obs, embedding_method, labeling_method, neighbors_key, inplace, **kwargs); 125 ; 126 ing = Ingest(adata_ref, neighbors_key); --> 127 ing.fit(adata); 128 ; 129 for method in embedding_method:. ~/projects/scanpy/scanpy/tools/_ingest.py in fit(self, adata_new); 437 ; 438 if not ref_var_names.equals(new_var_names):; --> 439 raise ValueError(; 440 'Variables in the new adata are different '; 441 'from variables in the reference adata'. ValueError: Variables in the new adata are different from variables in the reference adata; ```. --- . #### Versions. <details>. sc.logging.print_header(); scanpy==1.8.0.dev78+gc488909a anndata==0.7.6 umap==0.5.0 numpy==1.19.4 scipy==1.5.4 pandas==1.1.4 scikit-learn==0.23.2 statsmodels==0.12.1 python-igraph==0.8.3 leidenalg==0.8.3 pynndescent==0.5.1. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2001:1402,Variab,Variables,1402,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2001,4,"['Variab', 'variab']","['Variables', 'variables']"
Modifiability,"- [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy. ---. Hi, . Trying to run `scVI` to analyse my data using the latest `scanpy+scvi-tools` workflow, as described [here](https://www.scvi-tools.org/en/stable/user_guide/notebooks/harmonization.html). However, I'm running into a weird issue with the new `seurat_v3` flavour to call HVGs. When I run this:. ```python; sc.pp.highly_variable_genes(; adata,; flavor = ""seurat_v3"",; n_top_genes = 7000,; layer = ""counts"",; batch_key = ""combined"",; subset = True; ); ```. I get the following error:. ```pytb; If you pass `n_top_genes`, all cutoffs are ignored.; extracting highly variable genes; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-19-3748de5bacdc> in <module>; 5 layer = ""counts"",; 6 batch_key = ""combined"",; ----> 7 subset = True; 8 ); 9 adata. /opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key); 420 span=span,; 421 subset=subset,; --> 422 inplace=inplace,; 423 ); 424 . /opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py in _highly_variable_genes_seurat_v3(adata, layer, n_top_genes, batch_key, span, subset, inplace); 82 x = np.log10(mean[not_const]); 83 model = loess(x, y, span=span, degree=2); ---> 84 model.fit(); 85 estimat_var[not_const] = model.outputs.fitted_values; 86 reg_std = np.sqrt(10 ** estimat_var). _loess.pyx in _loess.loess.fit(). ValueError: b'There are other near singularities as well. 0.090619\n'; ```. While looking for a solution, I came across [this](https://github.com/YosefLab/scvi-tools/issues/727#issuecomment-718717033) issue that reports a similar problem. Any ideas of what this may be? . Thanks . #### Versions. <deta",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1504:711,variab,variable,711,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1504,1,['variab'],['variable']
Modifiability,"- [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. Hello, according to the documentation, the simulation of doublets with scrublet works best with un-normalized counts. The current wrapper implementation however uses a layer to hold the raw data, which then gets normalized anyway. . Please have in mind, that this is the first issue I have ever opened. If I made any grievous mistakes please inform me of such. . ---. ### Minimal code sample:. ```python; import scanpy; adata = scanpy.datasets.pbmc3k(); scanpy.external.pp.scrublet(adata); ```; and around line 177 in scanpy.external.pp.scrublet we need to 'observe' the values:; ```python; adata_obs.layers['raw'] = adata_obs.X; print(adata_obs.layers['raw']); pp.normalize_total(adata_obs) <--- currently normalizes all layers and X; print(adata_obs.layers['raw']); ```; ---; ### Impact; The 'raw' layer is later used in line 194 of scanpy.external.pp.scrublet to simulate doublets, however, it doesn't contain raw data anymore. ```python; adata_sim = scrublet_simulate_doublets(; adata_obs,; layer='raw',; sim_doublet_ratio=sim_doublet_ratio,; synthetic_doublet_umi_subsampling=synthetic_doublet_umi_subsampling,; ); ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1957:830,layers,layers,830,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1957,4,['layers'],['layers']
Modifiability,"- [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. This is probably a bug in my thinking, but naively I thought that `sc.pp.normalize_total()` normalizes counts per cell, thus allowing comparison of different cells by correcting for variable sequencing depth. However, the log transformation applied after normalisation seems to upset this relationship, example below. Why is this not problematic?. Incidentally, I first noticed this on my real biological dataset, not the toy example below. Edit: [relevant paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6215955/). > We can show, mathematically, that if we normalize expression profiles to have the same mean across cells, the mean after the equation [log] transformation used for RNA-Seq data will not be the same, and it will depend on the detection rate... And this [one](https://www.biorxiv.org/content/10.1101/404962v1.full):. > One issue of particular interest is that the mean of the log-counts is not generally the same as the log-mean count [1]. This is problematic in scRNA-seq contexts where the log-transformation is applied to normalized expression data. ---. ### Minimal code sample. ```python; >>> from anndata import AnnData; >>> import scanpy as sc; >>> import numpy as np; >>> adata = AnnData(np.array([[3, 3, 3, 6, 6],[1, 1, 1, 2, 2],[1, 22, 1, 2, 2], ])); >>> X_norm = sc.pp.normalize_total(adata, target_sum=1, inplace=False)['X']; >>> X_norm_log = np.log1p(X_norm); >>> X_norm_again = np.expm1(X_norm_log); >>> adata.X.sum(axis=1); array([21., 7., 28.], dtype=float32) # Different counts for each cell; >>> X_norm.sum(axis=1); array([1., 1., 1.], dtype=float32) # Normalisation means same counts for each cell; >>> X_norm_log.sum(axis=1); array([0.90322304, 0.90322304, 0.7879869 ], dtype=float32) # <<< Interested in this! Different counts for each ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1364:406,variab,variable,406,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364,1,['variab'],['variable']
Modifiability,"- [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy.; Latest on pip at least scanpy-1.6.0; ---. I'm using the sc.pl.dendrogram multiple times different lists of genes on my dataset (incrementing number of highly variable genes basically). The outputted dendrogram is alway the same (I guess it's taking into account all the genes because it's using something like 32go of ram....). ### Minimal code sample (that we can copy&paste without having any data). ```python; hvegene_sets = [sc.pp.highly_variable_genes(adata, inplace=False, subset=False, n_top_genes=nhvg)[""highly_variable""] for nhvg in [500,1000,2000, 3000,4000, 5000]]; ```; then; ```python; [sum(hvgene) for hvgene in hvegene_sets]; ```; outputs:; [499, 1000, 1999, 2999, 4000, 4999] (so i have my different genesets). then ; ```python; dendro1 = sc.tl.dendrogram(adata, ; var_names=adata.var_names[hvegene_sets[1]].values, ; optimal_ordering=True,; cor_method=""spearman"", linkage_method=""complete"", inplace=False,; groupby=""Annotation""); dendro2 = sc.tl.dendrogram(adata, ; var_names=adata.var_names[hvegene_sets[5]].values, ; optimal_ordering=True,; cor_method=""spearman"", linkage_method=""complete"", inplace=False,; groupby=""Annotation""); [dendro1[key] ==dendro2[key] for key in dendro1.keys()] ; ```; outputs: ; ```code; [array([[ True, True, True, True],; [ True, True, True, True],; [ True, True, True, True],; [ True, True, True, True],; [ True, True, True, True],; [ True, True, True, True],; [ True, True, True, True],; [ True, True, True, True],; [ True, True, True, True],; [ True, True, True, True],; [ True, True, True, True],; [ True, True, True, True],; [ True, True, True, True],; [ True, True, True, True],; [ True, True, True, True],; [ True, True, True, True],; [ True, True, True, True],; [ True, True, True, True],; [ True, True, True, True]]),; Tr",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1549:383,variab,variable,383,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1549,1,['variab'],['variable']
Modifiability,"- [x ] I have checked that this issue has not already been reported.; - [ x] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. Selection of highly variable genes works fine in default settings, but I get an error when I try to use seurat_v3 flavor. ```python; adata2.layers[""counts""] = adata2.X.copy(); adata2.raw = adata2 # keep full dimension safe; sc.pp.normalize_total(adata2, target_sum=1e4); sc.pp.log1p(adata2); sc.pp.highly_variable_genes(; adata2,; flavor=""seurat_v3"",; n_top_genes=3000,; layer=""counts"",; batch_key=""Sample"",; subset=True; ); ```. ```pytb; ValueError Traceback (most recent call last); <ipython-input-18-64d280f5029c> in <module>; 3 sc.pp.normalize_total(adata2, target_sum=1e4); 4 sc.pp.log1p(adata2); ----> 5 sc.pp.highly_variable_genes(; 6 adata2,; 7 flavor=""seurat_v3"",. /data04/projects04/MarianaBoroni/lbbc_members/lib/conda_envs/diogoamb/lib/python3.9/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key, check_values); 417 ; 418 if flavor == 'seurat_v3':; --> 419 return _highly_variable_genes_seurat_v3(; 420 adata,; 421 layer=layer,. /data04/projects04/MarianaBoroni/lbbc_members/lib/conda_envs/diogoamb/lib/python3.9/site-packages/scanpy/preprocessing/_highly_variable_genes.py in _highly_variable_genes_seurat_v3(adata, layer, n_top_genes, batch_key, check_values, span, subset, inplace); 83 x = np.log10(mean[not_const]); 84 model = loess(x, y, span=span, degree=2); ---> 85 model.fit(); 86 estimat_var[not_const] = model.outputs.fitted_values; 87 reg_std = np.sqrt(10 ** estimat_var). _loess.pyx in _loess.loess.fit(). ValueError: b'svddc failed in l2fit.'; ```. #### Versions; 0.10.00",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2034:251,variab,variable,251,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2034,2,"['layers', 'variab']","['layers', 'variable']"
Modifiability,"- [x ] I have checked that this issue has not already been reported.; - [ x] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---; I am trying to ingest a CITEseq dataset into another clustered dataset. These datasets have different numbers of cells but I ran neighbors(n_neighbors=30) for both prior to running umap. I have confirmed that both datasets have the same variable names and the same number of variable names (38). Both objects look identical when a call adata.var. . I receive the error: ""all input arrays must have the same shape"". . ```; sc.pp.neighbors(CODEX_sub, n_neighbors=30) ; sc.tl.umap(CODEX_sub); sc.pp.neighbors(adata_sub, n_neighbors = 30); sc.tl.umap(adata_sub); sc.tl.ingest(CODEX_sub, adata_sub, obs='leiden', embedding_method='umap'); ```. ```pytb; ValueError Traceback (most recent call last); <ipython-input-214-01a03312d3df> in <module>; ----> 1 sc.tl.ingest(CODEX_sub, adata_sub, obs='leiden', embedding_method='umap'). ~\anaconda3\envs\scenv\lib\site-packages\scanpy\tools\_ingest.py in ingest(adata, adata_ref, obs, embedding_method, labeling_method, neighbors_key, inplace, **kwargs); 124 labeling_method = labeling_method * len(obs); 125 ; --> 126 ing = Ingest(adata_ref, neighbors_key); 127 ing.fit(adata); 128 . ~\anaconda3\envs\scenv\lib\site-packages\scanpy\tools\_ingest.py in __init__(self, adata, neighbors_key); 383 ; 384 if neighbors_key in adata.uns:; --> 385 self._init_neighbors(adata, neighbors_key); 386 else:; 387 raise ValueError(. ~\anaconda3\envs\scenv\lib\site-packages\scanpy\tools\_ingest.py in _init_neighbors(self, adata, neighbors_key); 349 else:; 350 self._neigh_random_state = neighbors['params'].get('random_state', 0); --> 351 self._init_pynndescent(neighbors['distances']); 352 ; 353 def _init_pca(self, adata):. ~\anaconda3\envs\scenv\lib\site-packages\scanpy\tools\_ingest.py in _init_pynndescent(self, distances); 284 ; 285 first_c",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2085:468,variab,variable,468,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2085,2,['variab'],['variable']
Modifiability,"- [x ] I have checked that this issue has not already been reported.; - [x ] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python; # create new env; conda install -c pytorch pytorch; conda install -c pytorch cudatoolkit=11.3. conda install -c bioconda scanpy; Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: \; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed. UnsatisfiableError: The following specifications were found to be incompatible with each other:. Output in format: Requested package -> Available versionsThe following specifications were found to be incompatible with your system:. - feature:/linux-64::__glibc==2.31=0; - feature:|@/linux-64::__glibc==2.31=0. Your installed version is: 2.31; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2282:758,flexible,flexible,758,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2282,2,['flexible'],['flexible']
Modifiability,"- [x] Additional function parameters / changed functionality / changed defaults?. At the moment when we are plotting data points in e.g., `sc.pl.umap()` with `color='covariate'` we determine the plotting order in two ways:; 1. if `'covariate'` is continuous the highest values are plotted on top, to showcase the peaks of the distribution;; 2. if `'covariate'` is a categorical variable, the order of `adata.obs_names` is used (i believe). As we often concatenate datasets after integration or loading from multiple sources, covariates we plot are usually not randomly ordered here. I think the first case is fine (and it can be turned off), but we should probably not be doing case 2. Instead, it would be good if the default was to plot in a random order unless the covariate is ordered internally (I believe this is already taken into account, but not sure). I have come across this issue several times now, and we're not solving this in a good way imo. Fabian has mentioned this to me several times as well. What do you think @fidelram @ivirshup ?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1263:378,variab,variable,378,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1263,1,['variab'],['variable']
Modifiability,"- [x] I have checked that this issue has not already been reported. ; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---; ; I am reproducing [3k PBMCs tutorial](https://github.com/scverse/scanpy-tutorials/blob/master/pbmc3k.ipynb), which I run with no problem previously in Google Colab. But suddenly this error pop up during plotting which I believe due to latest update 1.9.0. I verified by installing previous version, this is not an issue:. `!pip install scanpy==1.8.2 # work fine`. ### Error code . ```python; # scatter plot in the PCA coordinates, but we will not use that later on.; sc.pl.pca(adata, color='CST3') # <-- this produces the error; ```. ```pytb; computing PCA; on highly variable genes; with n_comps=50; finished (0:00:00); ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); [<ipython-input-10-615d33c5cea9>](https://localhost:8080/#) in <module>(); 3 ; 4 # scatter plot in the PCA coordinates, but we will not use that later on.; ----> 5 sc.pl.pca(adata, color='CST3'); 6 ; 7 '''. 5 frames; [/usr/local/lib/python3.7/dist-packages/scanpy/plotting/_tools/scatterplots.py](https://localhost:8080/#) in pca(adata, annotate_var_explained, show, return_fig, save, **kwargs); 870 if not annotate_var_explained:; 871 return embedding(; --> 872 adata, 'pca', show=show, return_fig=return_fig, save=save, **kwargs; 873 ); 874 else:. [/usr/local/lib/python3.7/dist-packages/scanpy/plotting/_tools/scatterplots.py](https://localhost:8080/#) in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, dimensions, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, colorbar_loc, vmax, vmin, vcenter, ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2208:799,variab,variable,799,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2208,1,['variab'],['variable']
Modifiability,"- [x] I have checked that this issue has not already been reported.; - [ ] I have confirmed this bug exists on the latest version of scanpy.; - [x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ```python; sc.tl.pca(adata, svd_solver='arpack', use_highly_variable=True, chunked=True, chunk_size=1000); ```. ```pytb; TypeError: unsupported operand type(s) for -: 'datetime.datetime' and 'int'; ```. I believe the error is due overwriting of `start` variable.; It is declared in [line 116](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/_pca.py#L116) but is overwritten by loop variable of same name in [line 172](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/_pca.py#L172). #### Versions. <details>. -----; anndata 0.7.5; scanpy 1.6.0; sinfo 0.3.1; -----; PIL 8.1.0; anndata 0.7.5; asciitree NA; cffi 1.14.3; cloudpickle 1.6.0; constants NA; cycler 0.10.0; cython_runtime NA; dask 2020.12.0; dateutil 2.8.1; fasteners NA; get_version 2.1; h5py 3.1.0; highs_wrapper NA; igraph 0.8.3; joblib 1.0.0; kiwisolver 1.3.1; legacy_api_wrap 1.2; leidenalg 0.8.3; llvmlite 0.35.0; matplotlib 3.3.3; mpl_toolkits NA; msgpack 1.0.2; natsort 7.1.0; numba 0.52.0; numcodecs 0.7.2; numexpr 2.7.2; numpy 1.19.4; packaging 20.8; pandas 1.2.0; pkg_resources NA; psutil 5.8.0; pyparsing 2.4.7; pytz 2020.5; scanpy 1.6.0; scipy 1.6.0; setuptools_scm NA; sinfo 0.3.1; six 1.15.0; sklearn 0.24.0; sparse 0.11.2; tables 3.6.1; tblib 1.7.0; texttable 1.6.3; tlz 0.11.1; toolz 0.11.1; typing_extensions NA; yaml 5.3.1; zarr 2.6.1; -----; Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]; Linux-3.10.0-1062.4.1.el7.x86_64-x86_64-with-glibc2.10; 2 logical CPU cores, x86_64; -----. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1590:482,variab,variable,482,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1590,2,['variab'],['variable']
Modifiability,"- [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python; import scanpy as sc; pbmc = sc.datasets.pbmc3k(); log_anndata = sc.pp.log1p(pbmc, copy=True); pbmc.layers['log_transformed'] = log_anndata.X.copy(). # This errors, because X is not normalized and flavor=""seurat"" requires normalizes data.; # ValueError: cannot specify integer `bins` when input data contains infinity; sc.pp.highly_variable_genes(pbmc, flavor=""seurat"", subset=False, inplace=False). # This works, we pass log tranformed data; pbmc.uns['log1p'] = log_anndata.uns['log1p']; sc.pp.highly_variable_genes(pbmc, layer=""log_transformed"", flavor=""seurat"", subset=False, inplace=False). # This raises ValueError again; pbmc.obs['batch'] = 'A'; column_index = pbmc.obs.columns.get_indexer(['batch']); pbmc.obs.iloc[slice(pbmc.n_obs//2, None), column_index] = 'B'; sc.pp.highly_variable_genes(pbmc, layer=""log_transformed"", flavor=""seurat"", subset=False, inplace=False, batch_key=""batch""); ```. ```pytb; >>> import scanpy as sc; g_anndata = sc.pp.log1p(pbmc, copy=True); pbmc.layers['log_transformed'] = log_anndata.X.copy(). # This errors, because X is not normalized and flavor=""seurat"" requires normalizes data.; # ValueError: cannot specify integer `bins` when input data contains infinity; sc.pp.highly_variable_genes(pbmc, flavor=""seurat"", subset=False, inplace=False). # This works, we pass log tranformed data; pbmc.uns['log1p'] = log_anndata.uns['log1p']; sc.pp.highly_variable_genes(pbmc, layer=""log_transformed"", flavor=""seurat"", subset=False, inplace=False). # This raises ValueError a",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2396:598,layers,layers,598,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2396,1,['layers'],['layers']
Modifiability,"- [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. Hi, everyone:; Many users probably do not rely on pp.normalize_total for downstream analysis, but I found a strange default behavior that I think is worth mentioning.; pp.normalize_total() normalized my .layers['counts'] as well; The documentation is a bit murky; not sure if that is the expected behavior when layer is unspecified, but; such default behavior would undermine anyone who wishes to save the count information before RPKM normalization. ### Minimal code sample (that we can copy&paste without having any data). ```python; # Your code here; adata = sc.datasets.pbmc3k(); adata.layers['counts'] = adata.X; cell = adata.obs.index[1]; adata.var['mt'] = adata.var_names.str.startswith('MT-') # annotate the group of mitochondrial genes as 'mt'; sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True). print(""Run 1: initial values after simple processing: ""); print('sum of count layer in designated cell: ', adata[cell,:].layers['counts'].sum()); print('obs[total_counts] value in cell: ', adata[cell,:].obs['total_counts'][0]); print('.X.sum() value in cell: ', adata[cell,:].X.sum()); print('sum of count layer of MALAT1 in cell: ', adata[cell,'MALAT1'].layers['counts']); print('.X value of MALAT1 in cell: ', adata[cell,'MALAT1'].X). print(""\nRun 2: after sc.pp.normalize_total: ""); sc.pp.normalize_total(adata, target_sum=1e4); print('sum of count layer in designated cell: ', adata[cell,:].layers['counts'].sum()) # Note that this changed too; print('obs[total_counts] value in cell: ', adata[cell,:].obs['total_counts'][0]); print(",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2389:618,layers,layers,618,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2389,1,['layers'],['layers']
Modifiability,"- [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. I have the following anndata; ```; AnnData object with n_obs × n_vars = 28752 × 22603; obs: 'batch', 'cluster1_psc', 'all_fib_types', 'psc_batch_temp', 'batch_combined', 'overall', 'cluster1_kpc', 'kpc_batch_temp', 'n_genes', 'n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt'; var: 'gene_ids', 'feature_types', 'n_cells', 'mt', 'n_cells_by_counts', 'mean_counts', 'pct_dropout_by_counts', 'total_counts'; uns: 'genome'; ```. When I run leiden clustering it shows that the parameters were added. ```computing PCA; on highly variable genes; with n_comps=30; finished (0:01:25); computing neighbors; finished: added to `.uns['neighbors']`; `.obsp['distances']`, distances for each pair of neighbors; `.obsp['connectivities']`, weighted adjacency matrix (0:00:04); computing UMAP; finished: added; 'X_umap', UMAP coordinates (adata.obsm) (0:00:18); running Leiden clustering; finished: found 23 clusters and added; 'NoBatchCorr', the cluster labels (adata.obs, categorical) (0:00:04); ```; But when I check my anndata, none present. As such if I try to generate a umap image I get the following error; ```; AnnData object with n_obs × n_vars = 28752 × 22603; obs: 'batch', 'cluster1_psc', 'all_fib_types', 'psc_batch_temp', 'batch_combined', 'overall', 'cluster1_kpc', 'kpc_batch_temp', 'n_genes', 'n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt', 'Norm_Factor'; var: 'gene_ids', 'feature_types', 'n_cells', 'mt', 'n_cells_by_counts', 'mean_counts', 'pct_dropout_by_counts', 'total_counts', 'highly_variable', 'means', 'dispersions', 'dispersions",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2330:959,variab,variable,959,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2330,1,['variab'],['variable']
Modifiability,"- [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. I am running the standard scrnaseq pipeline on some data and am experiencing an issue. When I run. ```py; sc.pp.highly_variable_genes(adata,n_top_genes=4000, batch_key='batch'); ```. I get the error. ```; If you pass `n_top_genes`, all cutoffs are ignored.; extracting highly variable genes; ZeroDivisionError: division by zero; ```. I've unfortunately never seen this before, and i'm not sure how to address it. I would love if someone could help with this. Some additional information on my data. ```pycon; >>> adata.X.shape; Out[21]: (3433, 16836). >>> adata.X; array([[0., 0., 0., ..., 0., 0., 0.],; [0., 0., 0., ..., 0., 0., 0.],; [0., 0., 0., ..., 0., 0., 0.],; ...,; [0., 0., 0., ..., 0., 0., 0.],; [0., 0., 0., ..., 0., 0., 0.],; [0., 0., 0., ..., 0., 0., 0.]], dtype=float32); ```. <details>. -----; anndata 0.8.0; scanpy 1.9.1; -----; AlexFunctions NA; JonFunctions NA; PIL 9.1.0; PyQt5 NA; atomicwrites 1.4.0; autoreload NA; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; bs4 4.11.1; cffi 1.15.0; chardet 4.0.0; cloudpickle 2.0.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; debugpy 1.6.0; decorator 5.1.1; defusedxml 0.7.1; entrypoints 0.4; h5py 3.6.0; hypergeom_ufunc NA; igraph 0.9.1; import_all NA; ipykernel 6.13.0; jedi 0.18.1; joblib 1.1.0; kiwisolver 1.4.2; leidenalg 0.8.4; llvmlite 0.38.0; matplotlib 3.5.1; matplotlib_inline NA; mpl_toolkits NA; natsort 8.1.0; nbinom_ufunc NA; numba 0.55.1; numpy 1.21.6; packaging 21.3; pandas 1.4.2; params NA; parso 0.8.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; plotly 5.7.0; prompt_toolkit 3.0.29; psutil 5.9.0; ptyprocess 0.7.0; pycparser 2.21; pydev_ipython NA; pydevconsole NA; pydevd 2.8.0; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.11.2; pynnd",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2236:505,variab,variable,505,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2236,1,['variab'],['variable']
Modifiability,"- [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. In short `sc.pp.scale()` throws an error when `adata.X` is a dask array solely because `sc.pp._simple.scale` doesn't have the `sc.pp._simple.scale_array` function registered for a `dask.array.Array` type. Adding `@scale.register(da.Array)` [here](https://github.com/scverse/scanpy/blob/f03c5b407412de447480733a1a1a0e33e0c871d2/scanpy/preprocessing/_simple.py#LL758C7-L758C7) would fix that, but considering `dask.array` is [only an optional dependency,](https://github.com/scverse/scanpy/blob/f03c5b407412de447480733a1a1a0e33e0c871d2/scanpy/preprocessing/_simple.py#L32) there would have to be a conditional to wrap the function decoration. This brings me to the larger issue, is scanpy supposed to or working toward supporting dask arrays completely? I'm new to scanpy and I'm not sure if this is a bug report or an enhancement request. I could submit a pull request for this one issue but I'm curious if I'll run into many such issues as I dive in further and trying to figure out if there's existing momentum in this direction or whether I should be following some other parallelization strategy. I got here by following [AnnData's guide on using dask and zarr](https://anndata.readthedocs.io/en/latest/tutorials/notebooks/%7Bread%2Cwrite%7D_dispatched.html) to try to parallelize processing of a large scRNA-seq file. I come from a microscopy data analysis and ML background where my image data is stored in S3 hosted zarr arrays (in an OME-NGFF schema), handled by using dask arrays wrapped in xarray DataArrays, and parallelized across compute using ray. It would be nice to use a similar stack (dropping in anndata for xarray) for sc-seq analyses. ### Minimal code sample (that we can copy&paste without having any data). ```python; import zarr; import anndata as a",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2491:1046,enhance,enhancement,1046,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2491,1,['enhance'],['enhancement']
Modifiability,"- [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. To do some trajectory analysis, I wanted to first try some tutorials to adapt to the workflow of scanpy, after calling sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True) i got the valueerror below. I also tried different Drosophila Datasets and it always happens, and its due to a gene called ""nan"" in the features.tsv or genes.tsv, if this gene is renamed, scanpy works as intended. ```pytb; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-6-455e630e3278> in <module>; 1 adata.var['mt'] = adata.var_names.str.startswith('mt:') # annotate the group of mitochondrial genes as 'mt'; ----> 2 sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True). C:\ProgramData\Anaconda3\lib\site-packages\scanpy\preprocessing\_qc.py in calculate_qc_metrics(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, log1p, parallel); 286 X.eliminate_zeros(); 287 ; --> 288 obs_metrics = describe_obs(; 289 adata,; 290 expr_type=expr_type,. C:\ProgramData\Anaconda3\lib\site-packages\scanpy\preprocessing\_qc.py in describe_obs(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, log1p, inplace, X, parallel); 119 for qc_var in qc_vars:; 120 obs_metrics[f""total_{expr_type}_{qc_var}""] = (; --> 121 X[:, adata.var[qc_var].values].sum(axis=1); 122 ); 123 if log1p:. C:\ProgramData\Anaconda3\lib\site-packages\scipy\sparse\_index.py in __getitem__(self, key); 49 return self._get_sliceXslice(row, col); 50 elif col.ndim == 1:; ---> 51 return self._get_sliceXarray(row, col); 52 raise IndexError('index results in >2 dimensions'); 53 elif row.ndim == 1:. C:\ProgramData\Anaconda3\lib\site-p",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1708:301,adapt,adapt,301,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1708,1,['adapt'],['adapt']
Modifiability,"- [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---; When trying to plot the PAGA graph some of the nodes don't show up in the graph. The nodes/clusters don't show up specifically for color=dpt_pseudotime. The nodes are still visible with categorical variables, and with other continuous variables.; Even when copying dpt_pseudotime column, the color=dpt_pseudotime_copy does not show up correctly. ### Minimal code sample ; ```python; # preprocessing; sc.pp.recipe_zheng17(adata); adata_wt= adata[adata.obs[""genotype""].isin([""WT""])]; adata_pca = sc.tl.pca(adata_wt, svd_solver='arpack', copy=True); adata_n = sc.pp.neighbors(adata_pca, n_neighbors=4, n_pcs=20, copy=True); adata_graph = sc.tl.draw_graph(adata_n, copy=True); # paga; adata_full = sc.tl.paga(adata_graph, groups='final_bulk_labels', copy=True); # dpt; adata_full.uns['iroot'] = np.flatnonzero(adata_full.obs['final_bulk_labels'] == 'HSC')[1000]; adata_paga_dpt_nonan = sc.tl.diffmap(adata_full, copy=True, n_comps=10); adata_paga_dpt_nonan = sc.tl.dpt(adata_paga_dpt_nonan, copy=True). adata_paga_dpt_nonan.obs[""dpt_pseudotime_copy""]=adata_paga_dpt_nonan.obs[""dpt_pseudotime""]. sc.pl.paga(adata_paga_dpt_nonan, ; threshold=0.05, ; color=['dpt_pseudotime', 'final_bulk_labels', 'dpt_pseudotime_copy', 'total_counts'],; ; # layout: Optional[_IGraphLayout] = None,; # layout_kwds: Mapping[str, Any] = MappingProxyType({}),; # init_pos: Optional[np.ndarray] = None,; # root: Union[int, str, Sequence[int], None] = 0,; # labels: Union[str, Sequence[str], Mapping[str, str], None] = None,; single_component = True,; solid_edges= 'connectivities',; # dashed_edges: Optional[str] = None,; # transitions: Optional[str] = None,; fontsize = 5,. fontweight='light', ; # fontoutline=2, ; # text_kwds: Mapping[str, Any] = MappingProxyType({}),; node_size_scale = 3, ; node_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2292:427,variab,variables,427,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2292,2,['variab'],['variables']
Modifiability,"- [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. Hey, I've noticed another potential problem within the `seurat_v3` flavor of `sc.pp.highly_variable_genes()`. The documentation of the `batch_key` argument says on how the genes are ranked. >For all flavors, genes are first sorted by how many batches they are a HVG. For dispersion-based flavors ties are broken by normalized dispersion. If `flavor = 'seurat_v3'`, ties are broken by the median (across batches) rank based on within-batch normalized variance. However, when genes are sorted after computing everything, the `seurat_v3` method sorts first by the median ranks and then by how many batches a gene is highly variable (contrary to what the docstring says):. https://github.com/theislab/scanpy/blob/da66e15eee137b51868a6fb85603cfca9de557a7/scanpy/preprocessing/_highly_variable_genes.py#L139-L144. For comparison, the other flavors sort the other way around (as the docstring says):. https://github.com/theislab/scanpy/blob/da66e15eee137b51868a6fb85603cfca9de557a7/scanpy/preprocessing/_highly_variable_genes.py#L505-L510. Not sure which sorting would be correct here to make `seurat_v3` match the original behavior in Seurat, but I found these comments by @adamgayoso in #1204 that mention that when using `batch_key` in the current implementation, results don't match Seurat.. maybe the sorting order is the problem here?. https://github.com/theislab/scanpy/pull/1204#issuecomment-645700601; https://github.com/theislab/scanpy/pull/1204#issuecomment-663879113. Here is a piece of code that shows the top5 genes after `seurat_v3` gene selection and sorting by `highly_variable_nbatches` (which should then all be HVG, but they are not). <details>. ```python; import numpy as np; import scanpy as sc; import anndata . import sys; sys.path.append(""scanpy/preproce",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1733:849,variab,variable,849,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1733,1,['variab'],['variable']
Modifiability,"- [x] It would be better to refer to the anndata docs for the AnnData class (http://anndata.readthedocs.io/en/latest/anndata.AnnData.html) whenever `:class:~scanpy.api.AnnData` appears. `:class:AnnData <http://anndata.readthedocs.io/en/latest/anndata.AnnData.html>` has the correct css style, but does not hyperlink. Probably a solution via http://www.sphinx-doc.org/en/1.5.1/ext/extlinks.html together with the definition of an `:extclass:` role that inherits the `:class:` properties would be the correct way to do it.; - [x] A few references, like ""[Traag1723]"" are not rendered correctly... Who knows what's going on there. I couldn't figure it out with a few tests... Let's see.; - [x] the Neighbors class docstring doesn't render properly; - [ ] changing to the slim docstring style from the numpy docstring style messes up readability when calling the docstring lookup in jupyter or other IDEs, hence I'd advocate for maintaining this information. ## AnnData; - [x] `__init__` method appears in `AnnData`; - [x] `attributes` appear after `methods`",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/58:452,inherit,inherits,452,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/58,1,['inherit'],['inherits']
Modifiability,"- [x] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [x] External tools: Do you know an existing package that should go into `sc.external.*`?. Hi, thanks for developing such a fantastic framework for analyzing single-cell data, it really helps a lot in my research. I'm the developer of [Heatgraphy](https://github.com/Heatgraphy/heatgraphy), which is a python package to visualize multi-dimensional data using the x-layout system. You may think of it as python's version of complexHeatmap, but Heatgraphy can do much more. Personally, I think Heatgraphy can help visualize the AnnData intuitively. Here is [an example](https://heatgraphy.readthedocs.io/en/latest/auto_examples/plot_pbmc3k.html) of visualization of the PBMC3K dataset using Heatgraphy. The structure of AnnData fits the structure of this visualization quite well. We can plot the `.X` as the heatmap, and other attributes in `.obs`/`.var` or `.obsm`/`.varm` as side plots. ![](https://heatgraphy.readthedocs.io/en/latest/_images/sphx_glr_plot_pbmc3k_001_2_0x.png). Therefore, I propose adding a new API using Heatgraphy to help visualize AnnData. If adding to `sc.pl`, the API may look like this:. ```python; sc.pl.heatgraphy(adata, left=[(""cell_type"", ""color""), (""cell_type"", ""label"")], right=[(""gene_name"", ""label"")]); ```. If added as an extra class, the API can be more flexible and offer much more customization. It may look like this:. ```python; viz = AnnDataViz(adata); viz.add_left(key=""cell_type"", plot=""color"", cmap=""Set2"") # use a key from .obs and plot as color strip; viz.add_left(key=""cell_type"", plot=""label""); viz.render(); ```. It can also be applied to specific visualization for analysis.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2444:1368,flexible,flexible,1368,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444,1,['flexible'],['flexible']
Modifiability,"-------------------------------------------------------------------; Exception Traceback (most recent call last); <ipython-input-26-3a0e0ee3248f> in <module>(); ----> 1 loom_file=sc.read_loom('Aerts_Fly_AdultBrain_Filtered_57k.loom',validate=False). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\readwrite\read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype, **kwargs); 184 var=var,; 185 layers=layers,; --> 186 dtype=dtype); 187 return adata; 188 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx); 670 layers=layers,; 671 dtype=dtype, shape=shape,; --> 672 filename=filename, filemode=filemode); 673 ; 674 def _init_as_view(self, adata_ref: 'AnnData', oidx: Index, vidx: Index):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, layers, dtype, shape, filename, filemode); 848 # annotations; 849 self._obs = _gen_dataframe(obs, self._n_obs,; --> 850 ['obs_names', 'row_names', 'smp_names']); 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']); 852 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _gen_dataframe(anno, length, index_names); 285 _anno = pd.DataFrame(; 286 anno, index=anno[index_name],; --> 287 columns=[k for k in anno.keys() if k != index_name]); 288 break; 289 else:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\frame.py in __init__(self, data, index, columns, dtype, copy); 390 dtype=dtype, copy=copy); 391 elif isinstance(data, dict):; --> 392 mgr = init_dict(data, index, columns, dtype=dtype); 393 elif isinstance(data, ma.MaskedArray):; 394 import numpy.ma.mrecords as mrecords. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py in init_dict(data, index, columns, dtype); 210 arrays = [data[k] ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/924:1481,layers,layers,1481,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/924,1,['layers'],['layers']
Modifiability,"--------------------------------------------------; KeyError Traceback (most recent call last); <ipython-input-138-e642551f77de> in <module>(); ----> 1 sc.pl.dotplot(adata, marker_genes1, groupby='louvain'). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, color_map, dot_max, dot_min, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds); 1350 if isinstance(var_names, str):; 1351 var_names = [var_names]; -> 1352 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer); 1353 ; 1354 # for if category defined by groupby (if any) compute for each var_name. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer); 1983 matrix = adata[:, var_names].layers[layer]; 1984 elif use_raw:; -> 1985 matrix = adata.raw[:, var_names].X; 1986 else:; 1987 matrix = adata[:, var_names].X. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index); 495 ; 496 def __getitem__(self, index):; --> 497 oidx, vidx = self._normalize_indices(index); 498 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]; 499 else: X = self._adata.file['raw.X'][oidx, vidx]. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_indices(self, packed_index); 523 obs, var = super()._unpack_index(packed_index); 524 obs = _normalize_index(obs, self._adata.obs_names); --> 525 var = _normalize_index(var, self.var_names); 526 return obs, var; 527 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_index(index, names); 268 raise KeyError(; 269 'Indices ""{}"" contain invalid observation/variables names/indices.'; --> 270 .format(index)); 271 return positions.values; 272 else:. KeyEr",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/593:1137,layers,layers,1137,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/593,1,['layers'],['layers']
Modifiability,"---------------; TypeError Traceback (most recent call last); <ipython-input-698-58a5366a0f70> in <module>; 1 adata = sc.datasets.paul15(); ----> 2 sc.pl.scatter(adata, ""Cma1"", ""Irf8"", color='paul15_clusters', palette=""Set2""). ~/.local/lib/python3.7/site-packages/scanpy/plotting/_anndata.py in scatter(adata, x, y, color, use_raw, layers, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, legend_fontoutline, color_map, palette, frameon, right_margin, left_margin, size, title, show, save, ax); 126 and (color is None or color in adata.obs.keys() or color in adata.var.index); 127 ):; --> 128 return _scatter_obs(**args); 129 if (; 130 (x in adata.var.keys() or x in adata.obs.index). ~/.local/lib/python3.7/site-packages/scanpy/plotting/_anndata.py in _scatter_obs(adata, x, y, color, use_raw, layers, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, legend_fontoutline, color_map, palette, frameon, right_margin, left_margin, size, title, show, save, ax); 273 palettes = [palette for _ in range(len(keys))]; 274 for i, palette in enumerate(palettes):; --> 275 palettes[i] = _utils.default_palette(palette); 276 ; 277 if basis is not None:. TypeError: 'str' object does not support item assignment; ```. I get no error if I use any of `sc.pl.palettes`. I also get no error setting `palette=""Set2""` in `sc.pl.umap`, `sc.pl.draw_graph` etc... #### Versions. <details>. -----; anndata 0.7.4; scanpy 1.6.0; sinfo 0.3.1; -----; MulticoreTSNE NA; PIL 7.2.0; anndata 0.7.4; appdirs 1.4.4; atac_utils NA; atomicwrites 1.3.0; attr 20.2.0; backcall 0.2.0; brotli NA; cellrank 1.0.0-rc.10; certifi 2020.06.20; cffi 1.14.3; chardet 3.0.4; colorama 0.4.3; cycler 0.10.0; cython_runtime NA; datacache 1.1.5; dateutil 2.8.1; decorator 4.4.2; docrep 0.3.1; fcsparser 0.2.1; future_fstrings NA; get_version 2.1; google NA; gtfparse 1.2.0; h5py 2.10.0; idna 2.10; igraph 0.8.2; importlib_metadata 0.23; i",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1438:1299,layers,layers,1299,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1438,1,['layers'],['layers']
Modifiability,"--> 116 save=False, ax=ax); 117 utils.savefig_or_show('pca_scatter', show=show, save=save); 118 if show == False: return axs. /usr/local/lib/python3.6/site-packages/scanpy/plotting/anndata.py in scatter(adata, x, y, color, use_raw, layers, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, frameon, right_margin, left_margin, size, title, show, save, ax); 110 show=show,; 111 save=save,; --> 112 ax=ax); 113 elif x is not None and y is not None:; 114 if ((x in adata.obs.keys() or x in adata.var.index). /usr/local/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _scatter_obs(adata, x, y, color, use_raw, layers, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, frameon, right_margin, left_margin, size, title, show, save, ax); 371 c = adata.raw[:, key].X; 372 elif key in adata.var_names:; --> 373 c = adata[:, key].X if layers[2] == 'X' else adata[:, key].layers[layers[2]]; 374 c = c.toarray().flatten() if issparse(c) else c; 375 elif is_color_like(key): # a flat color. /usr/local/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index); 1292 def __getitem__(self, index):; 1293 """"""Returns a sliced view of the object.""""""; -> 1294 return self._getitem_view(index); 1295 ; 1296 def _getitem_view(self, index):. /usr/local/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index); 1296 def _getitem_view(self, index):; 1297 oidx, vidx = self._normalize_indices(index); -> 1298 return AnnData(self, oidx=oidx, vidx=vidx, asview=True); 1299 ; 1300 # this is used in the setter for uns, if a view. /usr/local/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx); 674 if not isinstance(X, AnnData):; 675 raise ValueError('`X` has to be an AnnData object.'); --> 676 self._init_as_view(X, oidx, vidx);",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/263:2621,layers,layers,2621,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263,1,['layers'],['layers']
Modifiability,". # t-SNE; tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means; k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs; sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(); tr=time.time(); adata = sc.read(input_file); adata.var_names_make_unique(); adata.shape; print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(); # To reduce the number of cells:; USE_FIRST_N_CELLS = 1300000; adata = adata[0:USE_FIRST_N_CELLS]; adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell); sc.pp.filter_cells(adata, max_genes=max_genes_per_cell); sc.pp.filter_genes(adata, min_cells=min_cells_per_gene); sc.pp.normalize_total(adata, target_sum=1e4); print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(); sc.pp.log1p(adata); print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes; sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression; for marker in markers:; adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes; adata = adata[:, adata.var.highly_variable]. #Regress out confounding factors (number of counts, mitochondrial gene expression); mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX); n_counts = np.array(adata.X.sum(axis=1)); adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts; adata.obs['n_counts'] = n_counts. ts=time.time(); sc.pp.regress_out(adata, ['n_counts', 'percent_mito']); print(""Total regress out time : %s"" % (time.time()-ts)); ```; <!-- Please check (“- [x]”) and fill in the following boxes -->; - [ ] Closes #; - [ ] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3284:2731,variab,variable,2731,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3284,1,['variab'],['variable']
Modifiability,.0; intervaltree 3.1.0; ipykernel 6.25.2; ipython 8.16.1; ipython-genutils 0.2.0; ipywidgets 8.1.1; isoduration 20.11.0; isort 5.12.0; itemadapter 0.8.0; itemloaders 1.1.0; itsdangerous 2.1.2; jaraco.classes 3.3.0; jedi 0.18.1; jeepney 0.8.0; jellyfish 1.0.1; Jinja2 3.1.2; jmespath 1.0.1; joblib 1.3.2; json5 0.9.14; jsonpatch 1.33; jsonpointer 2.4; jsonschema 4.19.1; jsonschema-specifications 2023.7.1; jupyter 1.0.0; jupyter_client 8.3.1; jupyter-console 6.6.3; jupyter_core 5.3.2; jupyter-events 0.7.0; jupyter-lsp 2.2.0; jupyter_server 2.7.3; jupyter_server_terminals 0.4.4; jupyterlab 4.0.6; jupyterlab-pygments 0.2.2; jupyterlab_server 2.25.0; jupyterlab-widgets 3.0.9; keyring 24.2.0; kiwisolver 1.4.5; lazy_loader 0.3; lazy-object-proxy 1.9.0; leidenalg 0.9.1; libarchive-c 5.0; libmambapy 1.5.1; linkify-it-py 2.0.0; llvmlite 0.40.1; locket 1.0.0; lxml 4.9.2; lz4 4.3.2; Markdown 3.5; markdown-it-py 3.0.0; MarkupSafe 2.1.3; matplotlib 3.8.0; matplotlib-inline 0.1.6; mccabe 0.7.0; mdit-py-plugins 0.4.0; mdurl 0.1.0; mistune 3.0.1; mkl-service 2.4.0; more-itertools 10.1.0; mpmath 1.3.0; msgpack 1.0.6; multidict 6.0.4; multipledispatch 0.6.0; multiprocess 0.70.15; munkres 1.1.4; mypy-extensions 1.0.0; natsort 8.4.0; natsort 8.4.0; navigator-updater 0.4.0; nbclient 0.8.0; nbconvert 7.9.2; nbformat 5.9.2; nest-asyncio 1.5.6; networkx 3.1; nltk 3.8.1; notebook 7.0.4; notebook_shim 0.2.3; numba 0.57.1; numexpr 2.8.7; numpy 1.24.4; numpydoc 1.5.0; openpyxl 3.1.2; overrides 7.4.0; packaging 23.2; pandas 2.1.1; pandocfilters 1.5.0; panel 1.2.3; parallel-fastq-dump 0.6.7; param 1.13.0; parsel 1.8.1; parso 0.8.3; partd 1.4.1; pathspec 0.11.2; patsy 0.5.3; pep8 1.7.1; pexpect 4.8.0; pickleshare 0.7.5; Pillow 10.0.1; pip 23.2.1; pkce 1.0.3; pkginfo 1.9.6; pkgutil_resolve_name 1.3.10; plac 1.3.5; platformdirs 3.11.0; plotly 5.17.0; pluggy 1.3.0; ply 3.11; pooch 1.7.0; prometheus-client 0.17.1; prompt-toolkit 3.0.39; Protego 0.3.0; psutil 5.9.5; ptyprocess 0.7.0; PuLP 2.7.0; pure-eval,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2680:6796,plugin,plugins,6796,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2680,1,['plugin'],['plugins']
Modifiability,.8; intervaltree 3.1.0; ipykernel 6.25.0; ipython 8.15.0; ipython-genutils 0.2.0; ipywidgets 8.0.4; isort 5.9.3; itemadapter 0.3.0; itemloaders 1.0.4; itsdangerous 2.0.1; jaraco.classes 3.2.1; jedi 0.18.1; jeepney 0.7.1; jellyfish 1.0.1; Jinja2 3.1.2; jinja2-time 0.2.0; jmespath 0.10.0; joblib 1.2.0; json5 0.9.6; jsonpatch 1.32; jsonpointer 2.1; jsonschema 4.17.3; jupyter 1.0.0; jupyter_client 7.4.9; jupyter-console 6.6.3; jupyter_core 5.3.0; jupyter-events 0.6.3; jupyter-server 1.23.4; jupyter_server_fileid 0.9.0; jupyter_server_ydoc 0.8.0; jupyter-ydoc 0.2.4; jupyterlab 3.6.3; jupyterlab-pygments 0.1.2; jupyterlab_server 2.22.0; jupyterlab-widgets 3.0.5; kaleido 0.2.1; keyring 23.13.1; kiwisolver 1.4.4; lazy_loader 0.2; lazy-object-proxy 1.6.0; libarchive-c 2.9; libmambapy 1.5.1; linkify-it-py 2.0.0; llvmlite 0.40.0; lmdb 1.4.1; locket 1.0.0; lxml 4.9.3; lz4 4.3.2; Markdown 3.4.1; markdown-it-py 2.2.0; MarkupSafe 2.1.1; matplotlib 3.7.2; matplotlib-inline 0.1.6; mccabe 0.7.0; mdit-py-plugins 0.3.0; mdurl 0.1.0; mistune 0.8.4; mkl-fft 1.3.8; mkl-random 1.2.4; mkl-service 2.4.0; more-itertools 8.12.0; mpmath 1.3.0; msgpack 1.0.3; multidict 6.0.2; multipledispatch 0.6.0; multiprocess 0.70.14; munkres 1.1.4; mypy-extensions 1.0.0; navigator-updater 0.4.0; nbclassic 0.5.5; nbclient 0.5.13; nbconvert 6.5.4; nbformat 5.9.2; nest-asyncio 1.5.6; networkx 3.1; nltk 3.8.1; notebook 6.5.4; notebook_shim 0.2.2; numba 0.57.1; numexpr 2.8.4; numpy 1.24.3; numpydoc 1.5.0; openpyxl 3.0.10; packaging 23.1; pandas 2.0.3; pandocfilters 1.5.0; panel 1.2.3; param 1.13.0; parsel 1.6.0; parso 0.8.3; partd 1.4.0; pathlib 1.0.1; pathspec 0.10.3; patsy 0.5.3; pep8 1.7.1; pexpect 4.8.0; pickleshare 0.7.5; Pillow 9.4.0; pip 23.2.1; pkce 1.0.3; pkginfo 1.9.6; platformdirs 3.10.0; plotly 5.9.0; pluggy 1.0.0; ply 3.11; poyo 0.5.0; prometheus-client 0.14.1; prompt-toolkit 3.0.36; Protego 0.1.16; psutil 5.9.0; ptyprocess 0.7.0; pure-eval 0.2.2; py-cpuinfo 8.0.0; pyarrow 11.0.0; pyasn1 0.4.8; pyasn1,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2706:4437,plugin,plugins,4437,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2706,1,['plugin'],['plugins']
Modifiability,"/local/lib/python3.6/site-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds); 438 ; 439 # Create the parser.; --> 440 parser = TextFileReader(filepath_or_buffer, **kwds); 441 ; 442 if chunksize or iterator:. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, f, engine, **kwds); 785 self.options['has_index_names'] = kwds['has_index_names']; 786 ; --> 787 self._make_engine(self.engine); 788 ; 789 def close(self):. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in _make_engine(self, engine); 1012 def _make_engine(self, engine='c'):; 1013 if engine == 'c':; -> 1014 self._engine = CParserWrapper(self.f, **self.options); 1015 else:; 1016 if engine == 'python':. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, src, **kwds); 1706 kwds['usecols'] = self.usecols; 1707 ; -> 1708 self._reader = parsers.TextReader(src, **kwds); 1709 ; 1710 passed_names = self.names is None. pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader.__cinit__(). EmptyDataError: No columns to parse from file; ```. </details>. But it seems to work no matter what server I choose in R, here's the test code. ```R; library(biomaRt). wwwmart = useMart(biomart=""ensembl"", dataset=""hsapiens_gene_ensembl"", host=""www.ensembl.org""); asiamart = useMart(biomart=""ensembl"", dataset=""hsapiens_gene_ensembl"", host=""asia.ensembl.org""); useastmart = useMart(biomart=""ensembl"", dataset=""hsapiens_gene_ensembl"", host=""useast.ensembl.org""). wwwresult = getBM(attributes=c(""hgnc_symbol"", ""chromosome_name""), mart=wwwmart); asiaresult = getBM(attributes=c(""hgnc_symbol"", ""chromosome_name""), mart=asiamart); useastresult = getBM(attributes=c(""hgnc_symbol"", ""chromosome_name""), mart=useastmart). assertthat::assert_that(all(wwwresult == asiaresult)); assertthat::assert_that(all(useastresult == asiaresult)); ```. I don't think the issue is with ensembl's servers. Is it possible you have a config for this `bioservices` module you've done anything with?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/242#issuecomment-416814067:3778,config,config,3778,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242#issuecomment-416814067,1,['config'],['config']
Modifiability,"0].__class__)(*args, **kw); > scanpy/preprocessing/_simple.py:888: in scale_anndata; > X, adata.var[""mean""], adata.var[""std""] = do_scale(; > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args; > error_rewrite(e, 'typing'); > _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _; > ; > e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'); > issue_type = 'typing'; > ; > def error_rewrite(e, issue_type):; > """"""; > Rewrite and raise Exception `e` with help supplied based on the; > specified issue_type.; > """"""; > if config.SHOW_HELP:; > help_msg = errors.error_extras[issue_type]; > e.patch_message('\n'.join((str(e).rstrip(), help_msg))); > if config.FULL_TRACEBACKS:; > raise e; > else:; > > raise e.with_traceback(None); > E numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend); > E non-precise type pyobject; > E During: typing of argument at /home/zeth/PycharmProjects/scanpy/scanpy/preprocessing/_simple.py (763); > E ; > E File ""scanpy/preprocessing/_simple.py"", line 763:; > E def do_scale(X, maxv, nthr):; > E <source elided>; > E # t0= time.time(); > E s = np.zeros((nthr, X.shape[1])); > E ^ ; > E ; > E This error may have been caused by the following argument(s):; > E - argument 0: Cannot determine Numba type of <class 'scipy.sparse._csr.csr_matrix'>; > ; > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:409: TypingError; > ```; > ; > When trying to use the new flavor with the existing test. Hi @Zethson ,; We are not able to see this issue with the latest commit. Can you please retry with the latest commit in scale branch.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2457#issuecomment-1540006717:2022,config,config,2022,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457#issuecomment-1540006717,1,['config'],['config']
Modifiability,"1229 ; 1230 @cbook.deprecated(""3.3"", alternative=""update_normal""). c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\matplotlib\cbook\deprecation.py in wrapper(*args, **kwargs); 449 ""parameter will become keyword-only %(removal)s."",; 450 name=name, obj_type=f""parameter of {func.__name__}()""); --> 451 return func(*args, **kwargs); 452 ; 453 return wrapper. c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\matplotlib\colorbar.py in __init__(self, ax, cmap, norm, alpha, values, boundaries, orientation, ticklocation, extend, spacing, ticks, format, drawedges, filled, extendfrac, extendrect, label); 489 else:; 490 self.formatter = format # Assume it is a Formatter or None; --> 491 self.draw_all(); 492 ; 493 def _extend_lower(self):. c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\matplotlib\colorbar.py in draw_all(self); 506 # sets self._boundaries and self._values in real data units.; 507 # takes into account extend values:; --> 508 self._process_values(); 509 # sets self.vmin and vmax in data units, but just for the part of the; 510 # colorbar that is not part of the extend patch:. c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\matplotlib\colorbar.py in _process_values(self, b); 961 expander=0.1); 962 ; --> 963 b = self.norm.inverse(self._uniform_y(self.cmap.N + 1)); 964 ; 965 if isinstance(self.norm, (colors.PowerNorm, colors.LogNorm)):. c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\matplotlib\colors.py in inverse(self, value); 1218 if not self.scaled():; 1219 raise ValueError(""Not invertible until scaled""); -> 1220 self._check_vmin_vmax(); 1221 vmin, vmax = self.vmin, self.vmax; 1222 . c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\matplotlib\colors.py in _check_vmin_vmax(self); 1179 raise ValueError(""minvalue must be less than or equal to maxvalue""); 1180 elif self.vmin <= 0:; -> 1181 raise ValueError",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2003:3430,extend,extend,3430,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2003,1,['extend'],['extend']
Modifiability,"18.49 |; | Updated | 3.97 |; | Speedup | 4.65743073 |. experiment setup : AWS r7i.24xlarge. ```py; import time; import numpy as np. import pandas as pd. import scanpy as sc; from sklearn.cluster import KMeans. import os; import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '); warnings.simplefilter('ignore'); input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):; print('Downloading import file...'); wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes; MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out; markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells; min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed; max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes; min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this; n_top_genes = 4000 # Number of highly variable genes to retain. # PCA; n_components = 50 # Number of principal components to compute. # t-SNE; tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means; k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs; sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(); tr=time.time(); adata = sc.read(input_file); adata.var_names_make_unique(); adata.shape; print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(); # To reduce the number of cells:; USE_FIRST_N_CELLS = 1300000; adata = adata[0:USE_FIRST_N_CELLS]; adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell); sc.pp.filter_cells(adata, max_genes=max_genes_per_cell); sc.pp.filter_genes(adata, min_cells=min_cells_per_gene); sc.pp.normalize_total(adata, target_sum=1e4); print(""Total filter and normalize time ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3099:1415,variab,variable,1415,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3099,1,['variab'],['variable']
Modifiability,"18:; ```; conda create -n temp_env_scanpy; conda activate temp_env_scanpy; (temp_env_scanpy) giov@vm:~$ conda install -c bioconda scanpy. Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: -; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed. UnsatisfiableError: The following specifications were found to be incompatible with your CUDA driver:. - feature:/linux-64::__cuda==9.1=0. Your installed CUDA driver is: 9.1; ```; Interestingly, this error is not thrown all the time, e.g. in a VM centos 7 without cuda:; ```; Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: \; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed. UnsatisfiableError:; ```; Another student working with me had the same issue in windows. His error was:; ```; UnsatisfiableError: The following specifications were found to be incompatible with your CUDA driver:. - feature:/win-64::__cuda==10.2=0. Your installed CUDA driver is: 10.2; ```; But on a mac, no problem at all. In all situations, I have at least another environment with scanpy installed.; In all cases, conda was `4.8.3`.; I cannot rule out completely the possibility that my conda in those 2 vms are messed up.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1142:1164,flexible,flexible,1164,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1142,2,['flexible'],['flexible']
Modifiability,"2"",; ""TI1-P3"", 'TM1-P3', 'TI2-P3', 'TM2-P3',; ""TI-P4"",'TS1-P4','TS2-P4',; 'lung_P2', 'lung_P5', 'lung_P8', 'lung_P9',; 'lung_P13', 'lung_P16',; 'lung_P20', 'lung_P21', 'lung_P24', 'lung_P28', 'lung_P29',; 'lung_P32', 'lung_P33', 'lung_P34', 'lung_P35', 'lung_P38', 'lung_P39'],; join='outer'). print('Begin of post doublets removal and QC plot'); sc.pp.scrublet(alldata, n_neighbors=10); alldata = alldata[alldata.obs['predicted_doublet']==False, :].copy(); n1 = alldata.shape[0]; print(f'Cells retained after scrublet: {n1}, {n0-n1} removed.'); print(f'End of post doublets removal and QC plots.'); ```. ### Error output. ```pytb; Begin of post doublets removal and QC plot; Running Scrublet; normalizing counts per cell. C:\ProgramData\Anaconda3\envs\dl\lib\site-packages\scanpy\preprocessing\_normalization.py:233: UserWarning: Some cells have zero counts; warn(UserWarning(""Some cells have zero counts"")). finished (0:00:00); WARNING: adata.X seems to be already log-transformed.; extracting highly variable genes. C:\ProgramData\Anaconda3\envs\dl\lib\site-packages\scanpy\preprocessing\_simple.py:377: RuntimeWarning: invalid value encountered in log1p; np.log1p(X, out=X). ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); Cell In[59], line 2; 1 print('Begin of post doublets removal and QC plot'); ----> 2 sc.pp.scrublet(alldata, n_neighbors=10); 3 alldata = alldata[alldata.obs['predicted_doublet']==False, :].copy(); 4 n1 = alldata.shape[0]. File C:\ProgramData\Anaconda3\envs\dl\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File C:\ProgramData\Anaconda3\envs\dl\lib\site-packages\scanpy\preproc",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3070:6499,variab,variable,6499,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3070,1,['variab'],['variable']
Modifiability,"2`. I can however load it again with that version. But when I downgrade to 1.3.7 (recommendation from @mbuttner who had the same cellxgene issue) I can no longer load the object and get the above error. Back in the 1.4.3 dev version scanpy it no longer writes the object after loading, and gives me the following error:; ```; In [23]: adata.write(""cellxgene.h5ad"") ; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-23-33b15d710f71> in <module>; ----> 1 adata.write(""cellxgene.h5ad""). ~/new_anndata/anndata/anndata/core/anndata.py in write_h5ad(self, filename, compression, compression_opts, force_dense); 2222 compression=compression,; 2223 compression_opts=compression_opts,; -> 2224 force_dense=force_dense,; 2225 ); 2226 . ~/new_anndata/anndata/anndata/readwrite/h5ad.py in write_h5ad(filepath, adata, force_dense, dataset_kwargs, **kwargs); 90 write_attribute(f, ""varp"", adata.varp, dataset_kwargs); 91 write_attribute(f, ""layers"", adata.layers, dataset_kwargs); ---> 92 write_attribute(f, ""uns"", adata.uns, dataset_kwargs); 93 write_attribute(f, ""raw"", adata.raw, dataset_kwargs); 94 if adata.isbacked:. ~/new_anndata/anndata/anndata/readwrite/h5ad.py in write_attribute(f, key, value, dataset_kwargs); 103 if key in f:; 104 del f[key]; --> 105 _write_method(type(value))(f, key, value, dataset_kwargs); 106 ; 107 . ~/new_anndata/anndata/anndata/readwrite/h5ad.py in write_mapping(f, key, value, dataset_kwargs); 203 def write_mapping(f, key, value, dataset_kwargs=MappingProxyType({})):; 204 for sub_key, sub_value in value.items():; --> 205 write_attribute(f, f""{key}/{sub_key}"", sub_value, dataset_kwargs); 206 ; 207 . ~/new_anndata/anndata/anndata/readwrite/h5ad.py in write_attribute(f, key, value, dataset_kwargs); 103 if key in f:; 104 del f[key]; --> 105 _write_method(type(value))(f, key, value, dataset_kwargs); 106 ; 107 . ~/new_anndata/anndata/anndata/readwrite/h5ad.py in write_mapping(f, key, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/832#issuecomment-544968526:1171,layers,layers,1171,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832#issuecomment-544968526,1,['layers'],['layers']
Modifiability,"34 cb = Colorbar(cax, mappable, **kwargs); 1735 ; 1736 cid = mappable.callbacksSM.connect('changed', cb.update_normal). c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\matplotlib\colorbar.py in __init__(self, ax, mappable, **kwargs); 1226 if isinstance(mappable, martist.Artist):; 1227 _add_disjoint_kwargs(kwargs, alpha=mappable.get_alpha()); -> 1228 ColorbarBase.__init__(self, ax, **kwargs); 1229 ; 1230 @cbook.deprecated(""3.3"", alternative=""update_normal""). c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\matplotlib\cbook\deprecation.py in wrapper(*args, **kwargs); 449 ""parameter will become keyword-only %(removal)s."",; 450 name=name, obj_type=f""parameter of {func.__name__}()""); --> 451 return func(*args, **kwargs); 452 ; 453 return wrapper. c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\matplotlib\colorbar.py in __init__(self, ax, cmap, norm, alpha, values, boundaries, orientation, ticklocation, extend, spacing, ticks, format, drawedges, filled, extendfrac, extendrect, label); 489 else:; 490 self.formatter = format # Assume it is a Formatter or None; --> 491 self.draw_all(); 492 ; 493 def _extend_lower(self):. c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\matplotlib\colorbar.py in draw_all(self); 506 # sets self._boundaries and self._values in real data units.; 507 # takes into account extend values:; --> 508 self._process_values(); 509 # sets self.vmin and vmax in data units, but just for the part of the; 510 # colorbar that is not part of the extend patch:. c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\matplotlib\colorbar.py in _process_values(self, b); 961 expander=0.1); 962 ; --> 963 b = self.norm.inverse(self._uniform_y(self.cmap.N + 1)); 964 ; 965 if isinstance(self.norm, (colors.PowerNorm, colors.LogNorm)):. c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\matplotlib\colors.py in inverse(se",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2003:3002,extend,extend,3002,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2003,3,['extend'],"['extend', 'extendfrac', 'extendrect']"
Modifiability,"7251754bb/scanpy/neighbors/__init__.py#L105; https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/neighbors/__init__.py#L258. There is a chance that this can also be solved with an import from UMAP.; https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/tools/_umap.py#L107. As just discussed, @Koncopd, can you look into this and make a PR that gets rid of the umap legacy code?. Thank you so much!; Alex. PS: Just wrote an explanation for the reasons why I intorduced the duplicated code in the first place.; > The duplicated code in Scanpy came about as I wanted to very quickly move forward with a version 1.0 of Scanpy about a year ago. UMAP was just becoming available on GitHub and there wasn’t even a preprint, I think. It changed very quickly and there were dramatic bugs every now and then. Nonetheless it was clear that it’s a major improvement over existing solutions, both in terms of computational performance, quality of the result and ease of installation and use. I wanted to achieve two things: (i) I had to rewrite some parts of UMAP so that I could decompose it a neighbors computing and a dedicated embedding step; you know that in Scanpy, the neighborhood graph is used for many other things other than for the embedding (clustering and trajectory inference). I also added the Gaussian kernel solution that I had before switching to a “UMAP backend” for `pp.neighbors`; which was needed so that results for DPT could be reproduced. All of this would have been quite a discussion with Leland. Until we would have had settled on the “Scanpy needs” that certainly weren’t aligned with the development of an independent young package, PRs would have been integrated to much time would have been lost. Finally, I wanted absolute reproducibility for Scanpy users, which could only be achieved by “freezing the code”. So, I asked Leland whether he is OK if I add a frozen version of umap as an intermediate solution.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/522:1429,rewrite,rewrite,1429,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/522,1,['rewrite'],['rewrite']
Modifiability,"9 |; | Updated | 3.97 |; | Speedup | 4.65743073 |. experiment setup : AWS r7i.24xlarge; ```python; import time; import numpy as np. import pandas as pd. import scanpy as sc; from sklearn.cluster import KMeans. import os; import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '); warnings.simplefilter('ignore'); input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):; print('Downloading import file...'); wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes; MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out; markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells; min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed; max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes; min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this; n_top_genes = 4000 # Number of highly variable genes to retain. # PCA; n_components = 50 # Number of principal components to compute. # t-SNE; tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means; k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs; sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(); tr=time.time(); adata = sc.read(input_file); adata.var_names_make_unique(); adata.shape; print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(); # To reduce the number of cells:; USE_FIRST_N_CELLS = 1300000; adata = adata[0:USE_FIRST_N_CELLS]; adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell); sc.pp.filter_cells(adata, max_genes=max_genes_per_cell); sc.pp.filter_genes(adata, min_cells=min_cells_per_gene); sc.pp.normalize_total(adata, target_sum=1e4); print(""Total filter and normalize time ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3280:1312,variab,variable,1312,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280,1,['variab'],['variable']
Modifiability,"; 0 FCGR3A 47.682064 5.891937 3.275554e-141 3.579712e-139; 1 FTL 45.653259 2.497682 9.003150e-208 6.887410e-205; ```. it also works for multiple groups:. ```python; print(sc.get.rank_genes_groups_df(adata, None, n_top_genes=2)); ```; ```; group names scores logfoldchanges pvals pvals_adj; 0 0 CD3D 26.250046 3.859759 4.379061e-75 2.233321e-73; 1 0 LDHB 21.207499 2.134979 1.488480e-67 5.993089e-66; 2 1 FCGR3A 47.682064 5.891937 3.275554e-141 3.579712e-139; 3 1 FTL 45.653259 2.497682 9.003150e-208 6.887410e-205; 4 2 LYZ 38.981312 5.096991 1.697105e-172 1.298285e-169; 5 2 CST3 34.241749 4.388617 1.448193e-149 5.539337e-147; 6 3 NKG7 34.214161 6.089183 2.356710e-55 2.575547e-53; 7 3 CTSW 24.584066 5.091688 2.026294e-39 9.118324e-38; 8 4 CD79A 52.583344 6.626956 4.032974e-84 7.713062e-82; 9 4 CD79B 32.102913 4.990217 1.958507e-51 1.872822e-49; 10 5 FTL 26.084383 1.844273 1.236398e-74 2.364611e-72; 11 5 LST1 25.554073 3.170759 5.653851e-81 4.325196e-78; 12 6 LYZ 31.497107 4.328516 9.041131e-106 6.916466e-103; 13 6 CST3 23.850258 3.281016 2.491629e-83 9.530482e-81; 14 7 CST3 33.024582 4.195395 5.768439e-136 4.412856e-133; 15 7 LYZ 31.264187 4.267053 9.712334e-101 1.485987e-98; 16 8 PPIB 39.260998 3.990153 7.159966e-47 3.651583e-45; 17 8 MZB1 33.305500 8.979518 7.611322e-26 1.878278e-24; 18 9 STMN1 27.133045 5.936039 4.998127e-18 8.312102e-17; 19 9 HMGB2 15.229477 5.016804 3.184879e-12 4.060720e-11; 20 10 HNRNPA1 18.405415 2.040915 1.570832e-12 1.560632e-11; 21 10 NPM1 14.230449 2.183721 3.424469e-10 3.046185e-09; ```. This also extends to enrichment queries (this is what I wanted originally):. ```python; sc.queries.enrich(adata, ""1"", n_top_genes=10); ```. For enrichment queries, I added to the doc string that a pval threshold of 0.05 is used. Previously, this was not obvious to me (and for cluster marker genes, this might not always be sensible). I didn't add anything to `docs/release-notes/`, yet. I first wanted to get your opinion. Is it useful, what is still needed here?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2145:2223,extend,extends,2223,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2145,1,['extend'],['extends']
Modifiability,"; 36. ~/.conda/envs/scanpy/lib/python3.8/site-packages/numba/core/decorators.py in; 10; 11 from numba.core.errors import DeprecationError, NumbaDeprecationWarning; ---> 12 from numba.stencils.stencil import stencil; 13 from numba.core import config, extending, sigutils, registry; 14. ~/.conda/envs/scanpy/lib/python3.8/site-packages/numba/stencils/stencil.py in; 9 from llvmlite import ir as lir; 10; ---> 11 from numba.core import types, typing, utils, ir, config, ir_utils, registry; 12 from numba.core.typing.templates import (CallableTemplate, signature,; 13 infer_global, AbstractTemplate). ~/.conda/envs/scanpy/lib/python3.8/site-packages/numba/core/registry.py in; 2; 3 from numba.core.descriptors import TargetDescriptor; ----> 4 from numba.core import utils, typing, dispatcher, cpu; 5; 6 # -----------------------------------------------------------------------------. ~/.conda/envs/scanpy/lib/python3.8/site-packages/numba/core/dispatcher.py in; 13; 14 from numba import _dispatcher; ---> 15 from numba.core import utils, types, errors, typing, serialize, config, compiler, sigutils; 16 from numba.core.compiler_lock import global_compiler_lock; 17 from numba.core.typeconv.rules import default_type_manager. ~/.conda/envs/scanpy/lib/python3.8/site-packages/numba/core/compiler.py in; 11 from numba.core.environment import lookup_environment; 12; ---> 13 from numba.core.compiler_machinery import PassManager; 14; 15 from numba.core.untyped_passes import (ExtractByteCode, TranslateByteCode,. ~/.conda/envs/scanpy/lib/python3.8/site-packages/numba/core/compiler_machinery.py in; 5 from numba.core.compiler_lock import global_compiler_lock; 6 from numba.core import errors, config, transforms; ----> 7 from numba.core.utils import add_metaclass; 8 from numba.core.tracing import event; 9 from numba.core.postproc import PostProcessor. ImportError: cannot import name 'add_metaclass' from 'numba.core.utils' (/home/qsw5175/.conda/envs/scanpy/lib/python3.8/site-packages/numba/core/utils.py)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1797:2952,config,config,2952,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1797,2,['config'],['config']
Modifiability,"; binutils-common (= 2.37-8),; binutils-x86-64-linux-gnu (= 2.37-8),; blt (= 2.5.3+dfsg-4.1),; bsdextrautils (= 2.37.2-4),; bsdutils (= 1:2.37.2-4),; build-essential (= 12.9),; bzip2 (= 1.0.8-4),; ca-certificates (= 20211016),; coreutils (= 8.32-4.1),; cpp (= 4:11.2.0-2),; cpp-11 (= 11.2.0-10),; dash (= 0.5.11+git20210903+057cd650a4ed-3),; dbus (= 1.12.20-3),; dbus-bin (= 1.12.20-3),; dbus-daemon (= 1.12.20-3),; dbus-session-bus-common (= 1.12.20-3),; dbus-system-bus-common (= 1.12.20-3),; dbus-user-session (= 1.12.20-3),; dconf-gsettings-backend (= 0.40.0-2),; dconf-service (= 0.40.0-2),; debconf (= 1.5.79),; debhelper (= 13.5.2),; debianutils (= 5.5-1),; dh-autoreconf (= 20),; dh-python (= 5.20211105),; dh-strip-nondeterminism (= 1.12.0-2),; diffutils (= 1:3.7-5),; dmsetup (= 2:1.02.175-2.1),; docutils-common (= 0.17.1+dfsg-2),; dpkg (= 1.20.9),; dpkg-dev (= 1.20.9),; dwz (= 0.14-1),; file (= 1:5.39-3),; findutils (= 4.8.0-1),; flit (= 3.0.0-1),; fontconfig (= 2.13.1-4.2),; fontconfig-config (= 2.13.1-4.2),; fonts-font-awesome (= 5.0.10+really4.7.0~dfsg-4.1),; fonts-lato (= 2.0-2.1),; fonts-lyx (= 2.3.6-1),; g++ (= 4:11.2.0-2),; g++-11 (= 11.2.0-10),; gcc (= 4:11.2.0-2),; gcc-11 (= 11.2.0-10),; gcc-11-base (= 11.2.0-10),; gettext (= 0.21-4),; gettext-base (= 0.21-4),; gir1.2-atk-1.0 (= 2.36.0-2),; gir1.2-freedesktop (= 1.70.0-2),; gir1.2-gdkpixbuf-2.0 (= 2.42.6+dfsg-2),; gir1.2-glib-2.0 (= 1.70.0-2),; gir1.2-gtk-3.0 (= 3.24.30-3),; gir1.2-harfbuzz-0.0 (= 2.7.4-1),; gir1.2-pango-1.0 (= 1.48.10+ds1-1),; grep (= 3.7-1),; groff-base (= 1.22.4-7),; gtk-update-icon-cache (= 3.24.30-3),; gzip (= 1.10-4),; hicolor-icon-theme (= 0.17-2),; hostname (= 3.23),; imagemagick (= 8:6.9.11.60+dfsg-1.3),; imagemagick-6-common (= 8:6.9.11.60+dfsg-1.3),; imagemagick-6.q16 (= 8:6.9.11.60+dfsg-1.3),; init-system-helpers (= 1.60),; intltool-debian (= 0.35.0+20060710.5),; libacl1 (= 2.3.1-1),; libaec0 (= 1.0.6-1),; libamd2 (= 1:5.10.1+dfsg-2),; libaom3 (= 3.2.0-1),; libapparmor1 (= 3.0.3",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616:2277,config,config,2277,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616,2,['config'],['config']
Modifiability,"<!-- Please give a clear and concise description of what the bug is: -->; ...When run bbknn on adata which has been calculated the pca, umap, and leiden, the AttributeError shows 'tuple' object has no attribute 'tocsr'. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; sc.pp.pca(adata); sc.pp.neighbors(adata); sc.tl.umap(adata); ...; computing PCA; on highly variable genes; with n_comps=50; finished (0:00:27); computing neighbors; using 'X_pca' with n_pcs = 50; finished: added to `.uns['neighbors']`; `.obsp['distances']`, distances for each pair of neighbors; `.obsp['connectivities']`, weighted adjacency matrix (0:00:24); computing UMAP; finished: added; 'X_umap', UMAP coordinates (adata.obsm) (0:01:27). %%time; sc.external.pp.bbknn(adata, batch_key='batch'); ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```; computing batch balanced neighbors; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-9-9b24f504f73c> in <module>(); ----> 1 get_ipython().run_cell_magic('time', '', ""sc.external.pp.bbknn(adata, batch_key='batch')""). 6 frames; <decorator-gen-60> in time(self, line, cell, local_ns). <timed eval> in <module>(). /usr/local/lib/python3.6/dist-packages/bbknn/__init__.py in compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity); 63 distances = get_sparse_matrix_from_indices_distances_umap(knn_indices, knn_dists, n_obs, n_neighbors); 64 ; ---> 65 return distances, connectivities.tocsr(); 66 ; 67 def create_tree(data,approx,metric,use_faiss,n_trees):. AttributeError: 'tuple' object has no attribute 'tocsr'; ```. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; > ...; scanpy==1.5.1 anndata==0.7.3 umap==0.4.3 numpy==1.18.4 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.22.2.post1 statsmodels==0.10.2 python-igraph==0.8.2 ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1249:405,variab,variable,405,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1249,1,['variab'],['variable']
Modifiability,"<!-- Please give a clear and concise description of what the bug is: -->; I am calculating custom connectivities using hsnw on rep 'X', I don't want to calculate PCA, I want to compute UMAP using these connectivities. ; sc.tl.umap falls back to pca in:; https://github.com/theislab/scanpy/blob/5bc37a2b10f40463f1d90ea1d61dc599bbea2cd0/scanpy/tools/_umap.py#L153; https://github.com/theislab/scanpy/blob/5bc37a2b10f40463f1d90ea1d61dc599bbea2cd0/scanpy/tools/_utils.py#L23. how to get sc.tl.umap to run on the precomputed 'X 'connectivities?; ... <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```python; import scanpy as sc; from scvelo.pp import neighbors; adata; #AnnData object with n_obs × n_vars = 4329 × 192; #obs: 'BARCODE', 'sample', 'detectable.features'; #var: 'gene_ids', 'feature_types'; #layers: 'normalized.counts'. neighbors(adata, n_neighbors = 20, use_rep = ""X"",knn = True,random_state = 0,method = 'hnsw',metric = ""euclidean"",metric_kwds = {""M"":20,""ef"":200,""ef_construction"":200},num_threads=1). adata.uns[""neighbors""]['params']; #{'n_neighbors': 20, 'method': 'hnsw', 'metric': 'euclidean', 'n_pcs': None}. sc.tl.umap(adata). #WARNING: .obsp[""connectivities""] have not been computed using umap; #WARNING: You’re trying to run this on 192 dimensions of `.X`, if you really want this, set `use_rep='X'`.; # Falling back to preprocessing with `sc.pp.pca` and default params. ...; ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```pytb; #WARNING: .obsp[""connectivities""] have not been computed using umap; #WARNING: You’re trying to run this on 192 dimensions of `.X`, if you really want this, set `use_rep='X'`.; # Falling back to preprocessing with `sc.pp.pca` and default params. ...; ```. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; scanpy==1.5.1 anndata==0.7.3 umap==0.4.4 numpy==1.19.0 scipy==1.5.0 pandas==1.0.5 scikit-learn==0.23.1 statsmodels==0.11.1 python-",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1318:847,layers,layers,847,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1318,1,['layers'],['layers']
Modifiability,"<!-- Please give a clear and concise description of what the bug is: -->; Loading data using `adata = sc.datasets.visium_sge('V1_Human_Lymph_Node')` with Anndata<0.7rc1 leads to error `'AnnData' object has no attribute 'is_view'`.; The reason is that the function name changed in version 0.7rc1 from `isview` -> `is_view`. I propose two possible solutions:; **Solution A**: Change requirements to `anndata>=0.7rc1`; **Solution B**: Add function to anndata:; ```python; def isview(self):; return self.is_view(); ```; I think solution B is preferable as it provides back-compatibility of anndata. ---; <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```python; pip install git+https://github.com/theislab/scanpy.git@spatial; import scanpy as sc; adata = sc.datasets.visium_sge('V1_Human_Lymph_Node'); ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```pytb; Variable names are not unique. To make them unique, call `.var_names_make_unique`.; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-2-59eff31dcd22> in <module>; 1 get_ipython().system('pip install git+https://github.com/theislab/scanpy.git@spatial'); 2 import scanpy as sc; ----> 3 adata = sc.datasets.visium_sge('V1_Human_Lymph_Node'). /opt/conda/lib/python3.7/site-packages/scanpy/datasets/__init__.py in visium_sge(sample_id); 368 ; 369 # read h5 file; --> 370 adata = read_10x_h5(files['counts']); 371 adata.var_names_make_unique(); 372 . /opt/conda/lib/python3.7/site-packages/scanpy/readwrite.py in read_10x_h5(filename, genome, gex_only); 169 if gex_only:; 170 adata = adata[:, list(map(lambda x: x == 'Gene Expression', adata.var['feature_types']))]; --> 171 if adata.is_view:; 172 return adata.copy(); 173 else:. AttributeError: 'AnnData' object has no attribute 'is_view'; ```. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; >",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1027:950,Variab,Variable,950,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1027,1,['Variab'],['Variable']
Modifiability,"<!-- Please give a clear and concise description of what the bug is: -->; Not able to install with conda and no info about the source of error.; <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```bash; (scrna) $ conda install -c bioconda scanpy. Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: | ; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Output in format: Requested package -> Available versions; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1190:420,flexible,flexible,420,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1190,2,['flexible'],['flexible']
Modifiability,"<!-- Please give a clear and concise description of what the bug is: -->; When I use `sc.pp.log1p(adata)` and then `sc.pp.log1p(adata, layer='other')` it warns me that the data has already been logged even though I am logging a layer as opposed to adata.X. Would be nice to flag logging for each layer instead of when anything is logged. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```python; import scanpy as sc. adata = sc.datasets.pbmc3k_processed(); adata.layers['other'] = adata.X; sc.pp.log1p(adata, layer='other'); sc.pp.log1p(adata); ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```pytb; WARNING: adata.X seems to be already log-transformed.; ```. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; > scanpy==1.5.2.dev5+ge5d246aa anndata==0.7.3 umap==0.3.10 numpy==1.18.5 scipy==1.5.0 pandas==1.0.5 scikit-learn==0.23.1 statsmodels==0.11.1 python-igraph==0.7.1 louvain==0.6.1 leidenalg==0.7.0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1333:510,layers,layers,510,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1333,1,['layers'],['layers']
Modifiability,"<!-- Please give a clear and concise description of what the bug is: -->; scanpy.pp.recipe_seurat and scanpy.pp.recipe_zheng17 indicate that they expect non log-transformed data. This leads both functions to do by default the highly variable gene (HVG) selection on non log-transformed data. This seems contrary to the scanpy and seurat clustering tutorials, which perform HVG selection after log-transform. It also seems contrary to the new function scanpy.pp.highly_variable_genes which expects log-transformed inputs. scanpy version : 1.5.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1251:233,variab,variable,233,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1251,1,['variab'],['variable']
Modifiability,<!-- What kind of feature would you like to request? -->; - [ ] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; ...; How to generate two umaps for one gene split by a condition[one variable in obs] ?; so you can compare where gene expressed in cell types and how they differ in the two conditions. could we add option split by to umap scanpy function?; https://github.com/theislab/scanpy/issues/759,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1879:538,variab,variable,538,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1879,1,['variab'],['variable']
Modifiability,<!-- What kind of feature would you like to request? -->; - [ ] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; ...; is there a scanpy method to do a correlation between cell types and continuous variables stored in .obs?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1855:553,variab,variables,553,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1855,1,['variab'],['variables']
Modifiability,"<!-- What kind of feature would you like to request? -->; - [ ] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [x] Other?. <!-- Please describe your wishes below: -->; ... I'm adapting ScanPy for my compositional data analysis (CoDA) methodologies in the same vein as https://github.com/scverse/scanpy/issues/2475 . In this, I would like to perform Aitchison PCA but I'd also like to keep my Anndata object in counts form instead of creating another one. . For example: . ```python; import numpy as np; import pandas as pd; from typing import Union. def clr(x:Union[np.ndarray, pd.Series], multiplicative_replacement:Union[None,str,float,int]=""auto"") -> Union[np.ndarray, pd.Series]:; """"""; http://scikit-bio.org/docs/latest/generated/skbio.stats.composition.clr.html#skbio.stats.composition.clr; """"""; assert np.all(x >= 0); if multiplicative_replacement == ""auto"":; if np.any(x == 0):; multiplicative_replacement = 1/(len(x)**2); if multiplicative_replacement is None:; multiplicative_replacement = 0; x = x.copy() + multiplicative_replacement; x = x/x.sum(); log_x = np.log(x); geometric_mean = log_x.mean(); return log_x - geometric_mean. # Add CLR to layers; adata.layers[""clr""] = adata.to_df().apply(clr, axis=1) # Not the fastest way but just to show example. sc.tl.pca(adata, svd_solver='arpack', layer=""clr"") # -> Return addata object where PCA is performed on CLR. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2476:477,adapt,adapting,477,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2476,3,"['adapt', 'layers']","['adapting', 'layers']"
Modifiability,<!-- What kind of feature would you like to request? -->; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?. <!-- Please describe your wishes below: -->; Computes a hierarchical clustering for the variables(genes) in sc.pl.matrixplot or sc.pl.dotplot.Thanks!,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2433:224,variab,variables,224,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2433,1,['variab'],['variables']
Modifiability,"<!-- What kind of feature would you like to request? -->; - [ x] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; When determining colours of categorical variables in uns they are based on alphabetical order (if I am not mistaken) - being represented just as an ordered list. Thus it is a bit inconvenient to change colours, especially if categories change during the analysis - the whole order changes and the mapping breaks. Would it be possible to use a dictionary of colours as values and categories as keys (with a default for any categories missing colours)?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1340:510,variab,variables,510,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1340,1,['variab'],['variables']
Modifiability,"<!-- What kind of feature would you like to request? -->; - [X] Additional function parameters / changed functionality / changed defaults?. <!-- Please describe your wishes below: -->; Sorry to file two issues in a row! I've been using Scanpy + scVI for my analysis recently, and am really enjoying it!. Currently, if use_rep is set in `sc.pp.neighbors`, then n_pcs is ignored. These seems like sane default behaviour to me - if we aren't using 'X_pca', then n_pcs doesn't really make sense. . This can actually be limiting, as I recently discovered. If you calculate a latent representation with scVI with `n_latent = 50`, you can't then do... ```python; sc.pp.neighbors(adata, use_rep='X_scvi', n_pcs = 25); ```. ...as the neighborhood graph is calculated on all 50 latent variables. If I want to use only 25 - say, as a point of comparison to see which best represents my data - I have to recalculate the latent representation with scVI with `n_latent = 25`. Basically, if you aren't using PCA, you have to calculate a full reduction for every number of dimensions you are interested in. . I don't think the fix would be that major. The source seems to be in the `_choose_representation` function, with the directly relevant snippet below:. https://github.com/theislab/scanpy/blob/5bc37a2b10f40463f1d90ea1d61dc599bbea2cd0/scanpy/tools/_utils.py#L48-L58. I think the only change needed would be to have the `n_pcs is not None` catch in all cases, not just when `use_rep = 'X_pca'`. Might also generalise the variable name to make it more clear it refers to any reduction, not just PCA. Admittedly, I only just started exploring the code base, so I'm not sure where else `_choose_representation` is called, or what other impacts this could have. . I have some blue sky time tomorrow, so I'll fork it then and see if I can whip up a pull request. Thanks for the excellent product!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1281:775,variab,variables,775,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1281,2,['variab'],"['variable', 'variables']"
Modifiability,"<!-- What kind of feature would you like to request? -->; - [x] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->. Especially when we visualize large datasets with multiple categorical variables (e.g. patient, disease, cell type) using `sc.pl.dotplot`, and we use a sequence in the `groupby` argument (`e.g. sc.pl.dotplot(ad, 'genex', groupby=['individual', 'disease_status', 'cell type'])`), sometimes we end up with too few cells in some rows, in which summary statistics like fraction of nonzero expressors or mean expression are not very robust. To avoid that, I think it'd be cool to have a minimum observation cutoff in the function, where e.g. `min_cells=5` would show `groupby` combinations with at least 5 cells. Without this option, this sort of filtering becomes an annoying pandas exercise (which some might enjoy but possibly not everyone).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1829:539,variab,variables,539,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1829,1,['variab'],['variables']
Modifiability,"<!-- What kind of feature would you like to request? -->; - [x] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; As more and more technologies allow multimodal characterization of single cells it could be useful to exploit some functionalities of scanpy's toolkit to perform, at least, some rough integrative analysis. Assuming we have to modalities on different layers (say RNA and ATAC), one could create two knn graphs for both layers and use `leidenalg.find_partition_multiplex` to perform a joint call of partitions handling the two (or more) graphs as a multiplex. I have tested myself this approach, described in [leidenalg documentation](https://leidenalg.readthedocs.io/en/latest/multiplex.html), it works and it is highly configurable. ; We can take care of the implementation of enhancement (as `leiden_multiplex()` function?), I just want to be sure that it is not already on the development roadmap and that it is ok to have it into scanpy and not as an external tool.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1107:719,layers,layers,719,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1107,4,"['config', 'enhance', 'layers']","['configurable', 'enhancement', 'layers']"
Modifiability,"<!-- What kind of feature would you like to request? -->; - [x] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; I find using the `adata.layers` really useful to compare normalisation strategies. I'd like to also be able to seamlessly run `sc.tl.pca` on data stored in different layers of the same `anndata` object. . At the moment my workaround is to set `adata.X` before PCA and changing the key to the `adata.obsm` element after:; ```python; adata.X = adata.layers[""mylayer""]; sc.tl.pca(adata); adata.obsm[""mylayer_pca""] = adata.obsm[""X_pca""]; ```; Ideally I'd like to just be able to set a `layer` argument in `sc.tl.pca`, as in the plotting functions. . Any plans on linking data layers to dimensionality reductions?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1301:493,layers,layers,493,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1301,4,['layers'],['layers']
Modifiability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [ X] Closes # (New Feature); - [ X] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [X ] Release notes not necessary because:. I thought I would give a shot at contributing to this awesome tool! I added a function to Scanpy's plotting API that I use in my own research for creating stacked barplots for visualizing compositions of cell groups. An example is depicted below:. ![image](https://github.com/scverse/scanpy/assets/5004419/21885a72-e15f-4f94-b1e5-84c1de8ca954). Specifically, this function enables one to plot the fraction of each cell group (e.g., cluster) that are labelled with a specific categorical variable. For example, if the cell groups are clusters, then one might be interested in examining the fraction of each cluster that originates from each ""batch"" of cells or each patient sample. This function also enables ordering of the groups according to a specific value (e.g., a patient number or batch ID) or by agglomerative clustering. An example of ordering the groups based on agglomerative clustering is shown below:. ![image](https://github.com/scverse/scanpy/assets/5004419/bfde8173-4f0d-483f-b37e-849446b65153). The function supplies an argument to specify whether the dendrogram should or should not be plotted. Please let me know if this feature is of interest and if so, what else needs to be adjusted or fixed prior to merging. Thanks!. Matt",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2873:986,variab,variable,986,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2873,1,['variab'],['variable']
Modifiability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [ ] Closes # _no existing issue_; - [ ] Tests included or not required because: _No new tests_; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because: _I did not write release notes_. Hi :). I am proposing a change that speeds up `filter_cells` (x1000 speedup) and `filter_genes` (x2 speedup) for CSR sparse matrices. On my personal machine for 1M cells, `sc.pp.filter_cells(adata, min_genes=xx)` runs in 1ms instead of 10s currently. The speedup should be even stronger on sparser modalities like ATAC. In spirit, this simply replaces `np.sum(X > 0, axis=axis)` with `X.getnnz(axis=axis)`, which is much more efficient. But the axis argument in `getnnz` in `csr_array` may be deprecated. I think it should still be fine with `csr_matrix`, but since I don't know for sure I manually implemented it for the CSR case as in https://github.com/scipy/scipy/issues/19405 . What do you think?. Regarding `getnnz`: Of course it would be nicer to be able to write `.getnnz(axis=axis)`, which extends beyond CSR to other sparse matrices. Can we assume that we're getting sparse matrices and not sparse arrays ?. Pinging @dschult from the Scipy issue liked above, who mentioned: . > I'm pretty sure that a reasonable and commonly occuring use-case would be enough to make the developers include this feature somehow. (edited because I confused `csr_array` and `csr_matrix`)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2772:1348,extend,extends,1348,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2772,1,['extend'],['extends']
Modifiability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Adresses #2088 and adresses #1733; <!-- Only check the following box if you did not include release notes -->; - [x] Tests included or not required because: They are required and some suggested; - [x] Release notes; - [x] Doc update - depending on feedback here; - [x] Doc update - guidance scanpy vs seurat. **Context**; As discussed in issues #2088 and #1733, `sc.pp.highly_variable_genes(adata, flavor=""seurat_v3"", batch_key=SOME_KEY)` potentially differs in the implementation of how HVGs are ranked from its Seurat counterpart:; - either by sorting by number-of-batches-in-which-genes-are-highly-variable and then breaking ties with median-rank-in-batches (this is described in [Stuart et al. 2019](https://www.cell.com/cell/pdf/S0092-8674(19)30559-8.pdf), and implemented in Seurat's [`SelectIntegrationFeatures`](https://satijalab.org/seurat/reference/selectintegrationfeatures)*).; - OR by sorting first by median-rank-in-batches and breaking ties with number-of-batches-in-which-genes-are-highly-variable (this is how `""seurat_v3""` in scanpy is currently implemented); ; causing quite some discrepancy in the results. *I am not an R expert, so this might not be correct: Digging into the code of `SelectIntegrationFeatures`, I suspect the genes _above_ a treshold level of batches in which they are HVGs are [ordered by their median rank](https://github.com/satijalab/seurat/blob/41d19a8a55350bff444340d6ae7d7e03417d4173/R/integration.R#L2988), in contrary to the textual description in Stuart et al.; and only the genes displaying this threshold of number of batches in which they are highly variable are ranked by their median rank - to decide which are kept as highly variable. This w",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2792:906,variab,variable,906,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2792,1,['variab'],['variable']
Modifiability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #3027; - [x] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because: they are, added. This PR fixes the bug reported in the linked issue. A new test which spots the erroneous computations has been added. I would use this chance to refactor the `_highly_variable_genes.py`, rather than using the 2-lines fix suggested in the first commit:; Doing the multi-batch hvg flagging differently for seurat_v3 and seurat/cell_ranger is what made this bug hard to spot in the first place I think.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3042:647,refactor,refactor,647,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3042,1,['refactor'],['refactor']
Modifiability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #3114; - [x] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:. Other things done:. - Rename some variables and reorder functions so the diff between the metrics is minimal for a future unification; - Skip seurat v3 tests with numpy 2 because skmisc isn’t ready: https://github.com/has2k1/scikit-misc/issues/34. This will skip the seurat v3 tests for all but the minimum versions test job for now, but I think that’s OK?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3115:520,variab,variables,520,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3115,1,['variab'],['variables']
Modifiability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #761, closes #2322, closes #2229, closes #2267; - [x] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:. Also. - documents the parameter better, see #1502; - remove duplicated tests for embedding plots with continuous variables: `legend_loc` does nothing there (yet)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3163:640,variab,variables,640,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3163,1,['variab'],['variables']
Modifiability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Tests included or not required because: it's formating; <!-- Only check the following box if you did not include release notes -->; - [x] Release notes not necessary because: it's formatting. Why did we ever configure black?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2701:513,config,configure,513,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2701,1,['config'],['configure']
Modifiability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. This is a very small pull request to add `str` to the possible arguments for saving a figure from [`scanpy.pl.rank_genes_groups`][rank-genes-groups]. This addition matches other `save=` arguments, such as from [`scanpy.plotting.highly_variable_genes`][highly-variable-genes], [`sc.plotting.pca_variance_ratio`][pca-variance-ratio], and [`scanpy.plotting.umap`][umap]. [rank-genes-groups]: https://github.com/scverse/scanpy/blob/main/scanpy/plotting/_tools/__init__.py; [highly-variable-genes]: https://github.com/scverse/scanpy/blob/main/scanpy/plotting/_preprocessing.py; [pca-variance-ratio]: https://github.com/scverse/scanpy/blob/main/scanpy/plotting/_tools/__init__.py; [umap]: https://github.com/scverse/scanpy/blob/main/scanpy/plotting/_tools/scatterplots.py. I have not included tests or release notes due to the single-line change nature of this pull request",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3076:493,variab,variable-genes,493,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3076,2,['variab'],['variable-genes']
Modifiability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->; Modified (for `scanpy`) version of https://github.com/scverse/anndata/pull/564. Fixes https://github.com/scverse/anndata/issues/556. Big points of change:; 1. No more tuple-indices and related functionality (i.e., scoring pairwise); 2. Allow for `obs` and `var` group-by +`varm`, `obsm`, `layers` as options for data to aggregate; 3. Output is `AnnData` object instead of `DataFrame`; 4. `scanpy`-style public API. ## TODO (by @ivirshup):. Necessary:. - [x] Docs; - [x] Aggregate along other axis; - [x] Keep grouping cols in result; - [x] Reconsider API for non-anndata version (maybe return a dict of arrays?); - [ ] Decide on naming convention for `""nonzero""` variations, should this be `""nonzero_count""` so it's a little like `""nanmean""`. Optional, can do later:. - [ ] Weighted (although.... Idk, maybe can skip. Does ""weights"" affect ""count_nonzero""?); - [ ] Option for keeping around unseen groups, probably needs `fill_value` argument for those values; - [x] Support for `obsm`, `varm`; - [ ] Directly pass Series to groupby; - [ ] More aggregation functions (mean_nonzero, min, max, std, `nan*` variations); - [ ] Mask argument; - [ ] Dask support",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2590:523,layers,layers,523,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2590,1,['layers'],['layers']
Modifiability,"<!--; ⚠ If you need help using Scanpy, please ask in https://scanpy.discourse.group/ instead ⚠; If you want to know about design decisions and the like, please ask below:; -->; ...; Hi ; if samples contribute a different number of cells to my object, how to control for variability among samples? ; How to make sure that any difference between conditions I found is caused by biology and not because of samples variation? . downsampling, upsampling, bootstrapping, robustness test . Appreciate any feedback and any references for this issue. ![image](https://user-images.githubusercontent.com/23288387/155648374-f0d6178f-7024-4ecd-88c0-37547c5e7e19.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2155:270,variab,variability,270,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2155,1,['variab'],['variability']
Modifiability,"<!--; ⚠ If you need help using Scanpy, please ask in https://scanpy.discourse.group/ instead ⚠; If you want to know about design decisions and the like, please ask below:; -->; Hi, . In the original Coifman/Lafon paper on diffusion maps they introduce a family of kernels index by some alpha. I'm just wondering what value of alpha is implemented in scanpy. I assume it's alpha=0, based on the documentation but its a bit unclear. Is there any plans to extend it to other values of alpha? Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2115:453,extend,extend,453,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2115,1,['extend'],['extend']
Modifiability,"<!--; ⚠ If you need help using Scanpy, please ask in https://scanpy.discourse.group/ instead ⚠; If you want to know about design decisions and the like, please ask below:; -->; I was wondering, what is the intended way to temporarily set plotting parameters, e.g. figsize?. Say I want to increase the figsize just for a single UMAP plot. . I could either temporarily `set_figure_params` and reset it manually afterwards; ```python; sc.set_figure_params(figsize=(8, 8)); sc.pl.umap(adata, color=""cell_type""); sc.set_figure_params(figsize=(4, 4)) # or whatever it was before...; ```. Or create the figure separately:; ```python; _, ax = plt.subplots(figsize=(8, 8)); sc.pl.umap(adata, color=""cell_type"", ax=ax); ```; <p><br></p>. Would it make sense to extend `set_figure_params` to act as a context manager?. ```python; with sc.set_figure_params(figsize=(8, 8), frameon=False):; sc.pl.umap(adata, color=""cell_type""); ```. But maybe there's a comparable way already?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1648:751,extend,extend,751,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1648,1,['extend'],['extend']
Modifiability,"=raise_errors); 71 typemap, restype, calltypes = infer.unify(raise_errors=raise_errors); 72 . ~/.local/lib/python3.9/site-packages/numba/core/typeinfer.py in propagate(self, raise_errors); 1069 if isinstance(e, ForceLiteralArg)]; 1070 if not force_lit_args:; -> 1071 raise errors[0]; 1072 else:; 1073 raise reduce(operator.or_, force_lit_args). TypingError: Failed in nopython mode pipeline (step: nopython frontend); Failed in nopython mode pipeline (step: nopython frontend); Failed in nopython mode pipeline (step: nopython mode backend); Failed in nopython mode pipeline (step: nopython mode backend); Failed in nopython mode pipeline (step: nopython frontend); No implementation of function Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>) found for signature:; ; >>> run_quicksort(array(int32, 1d, C)); ; There are 2 candidate implementations:; - Of which 2 did not match due to:; Overload in function 'register_jitable.<locals>.wrap.<locals>.ov_wrap': File: numba/core/extending.py: Line 150.; With argument(s): '(array(int32, 1d, C))':; Rejected as the implementation raised a specific error:; UnsupportedError: Failed in nopython mode pipeline (step: analyzing bytecode); Use of unsupported opcode (LOAD_ASSERTION_ERROR) found; ; File ""../../../../.local/lib/python3.9/site-packages/numba/misc/quicksort.py"", line 180:; def run_quicksort(A):; <source elided>; while high - low >= SMALL_QUICKSORT:; assert n < MAX_STACK; ^; ; raised from /home/gabriel/.local/lib/python3.9/site-packages/numba/core/byteflow.py:269. During: resolving callee type: Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>); During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/numba/np/arrayobj.py (5007). File ""../../../../.local/lib/python3.9/site-packages/numba/np/arrayobj.py"", line 5007:; def array_sort_impl(arr):; <source elided>; # Note we clobber the return value; sort_func(arr); ^. During: lowering ""$14call_method.5 = cal",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1652:8805,extend,extending,8805,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652,1,['extend'],['extending']
Modifiability,"> ## Question; > ; > Does this interact with group colors and dendrograms at all?. Dendrogram and colors seem not affected. > ## Test change; > ; > All of the plots will start failing because this will change the output for every test. I have a few concerns here:; > ; > * I'm worried about repo bloat from the plotting tests. Ideally we could just store the reference images outside of git (git lfs maybe?). Updating all the plots with `tight_layout` would increase repo size by 10%; > ; > * Is `tight_layout` deterministic ([matplotlib/matplotlib#11809 (comment)](https://github.com/matplotlib/matplotlib/issues/11809#issuecomment-432726600))? Also, is matplotlib trying to replace it with [`constrained_layout`](https://matplotlib.org/stable/tutorials/intermediate/constrainedlayout_guide.html)?; > ; > * Does globally adding `tight_layout` add to test times? My impression was that it basically rendered the plot, fixed the borders, then rendered it again.; > ; > ; > Proposed solution:; > ; > Can we just explicitly extend the borders for this test? At a later point we can move plots to a different storage system, then have much more freedom in making changes to how they render. Sounds great, I think I managed to do that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1735#issuecomment-796812772:1021,extend,extend,1021,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1735#issuecomment-796812772,1,['extend'],['extend']
Modifiability,"> 272 result = AnnData(; 273 layers=layers,; 274 obs=new_label_df,; 275 var=getattr(adata, ""var"" if axis == 0 else ""obs""),; 276 ); 278 if axis == 1:; 279 return result.T. File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/anndata/_core/anndata.py:271, in AnnData.__init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, obsp, varp, oidx, vidx); 269 self._init_as_view(X, oidx, vidx); 270 else:; --> 271 self._init_as_actual(; 272 X=X,; 273 obs=obs,; 274 var=var,; 275 uns=uns,; 276 obsm=obsm,; 277 varm=varm,; 278 raw=raw,; 279 layers=layers,; 280 dtype=dtype,; 281 shape=shape,; 282 obsp=obsp,; 283 varp=varp,; 284 filename=filename,; 285 filemode=filemode,; 286 ). File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/anndata/_core/anndata.py:501, in AnnData._init_as_actual(self, X, obs, var, uns, obsm, varm, varp, obsp, raw, layers, dtype, shape, filename, filemode); 498 self._clean_up_old_format(uns); 500 # layers; --> 501 self._layers = Layers(self, layers). File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/anndata/_core/aligned_mapping.py:331, in Layers.__init__(self, parent, vals); 329 self._data = dict(); 330 if vals is not None:; --> 331 self.update(vals). File <frozen _collections_abc>:949, in update(self, other, **kwds). File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/anndata/_core/aligned_mapping.py:199, in AlignedActualMixin.__setitem__(self, key, value); 198 def __setitem__(self, key: str, value: V):; --> 199 value = self._validate_value(value, key); 200 self._data[key] = value. File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/anndata/_core/aligned_mapping.py:89, in AlignedMapping._validate_value(self, val, key); 83 dims = tuple((""obs"", ""var"")[ax] for ax in self.axes); 84 msg = (; 85 f""Value passed for key {key!r} is of incorrect shape. ""; 86 f""Values of {self.attrname} must match dimensions {",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2929:2297,layers,layers,2297,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2929,1,['layers'],['layers']
Modifiability,"> 79 characters); scanpy/preprocessing/_highly_variable_genes.py:713:80: E501 line too long (88 > 79 characters); scanpy/preprocessing/_highly_variable_genes.py:735:80: E501 line too long (82 > 79 characters); scanpy/preprocessing/_highly_variable_genes.py:737:80: E501 line too long (83 > 79 characters); scanpy/preprocessing/_highly_variable_genes.py:742:80: E501 line too long (80 > 79 characters). ```. `git status` and `git diff` show the automatic changes pre-commit makes:. ```; jlause@8b38045532aa:~/libs/scanpy/scanpy/preprocessing$ git status; On branch pearson_residuals_1.7; Changes to be committed:; (use ""git reset HEAD <file>..."" to unstage). 	modified: _highly_variable_genes.py. Changes not staged for commit:; (use ""git add <file>..."" to update what will be committed); (use ""git checkout -- <file>..."" to discard changes in working directory). 	modified: _highly_variable_genes.py. Untracked files:; (use ""git add <file>..."" to include in what will be committed). 	../../.pre-commit-config.yaml. jlause@8b38045532aa:~/libs/scanpy/scanpy/preprocessing$ git diff _highly_variable_genes.py ; diff --git a/scanpy/preprocessing/_highly_variable_genes.py b/scanpy/preprocessing/_highly_variable_genes.py; index 03b01940..e2851f50 100644; --- a/scanpy/preprocessing/_highly_variable_genes.py; +++ b/scanpy/preprocessing/_highly_variable_genes.py; @@ -15,7 +15,8 @@ from ._utils import _get_mean_var; from ._distributed import materialize_as_ndarray; from ._simple import filter_genes; ; -#testedit; +# testedit; +; ; def _highly_variable_genes_seurat_v3(; adata: AnnData,; @@ -98,7 +99,9 @@ def _highly_variable_genes_seurat_v3(; else:; clip_val_broad = np.broadcast_to(clip_val, batch_counts.shape); np.putmask(; - batch_counts, batch_counts > clip_val_broad, clip_val_broad,; + batch_counts,; + batch_counts > clip_val_broad,; + clip_val_broad,; ); ; if sp_sparse.issparse(batch_counts):; @@ -173,6 +176,7 @@ def _highly_variable_genes_seurat_v3(; df = df.drop(['highly_variable_nbatche",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-794148562:8658,config,config,8658,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-794148562,1,['config'],['config']
Modifiability,"> > ooh, this time the benchmark shows really nicely how much faster it is!; > ; > Looks like preprocessing_log.time_regress_out('pbmc68k_reduced') , regress out those variables that is not inside it. It should regress_out ['n_counts', 'percent_mito'] instead of [""total_counts"", ""pct_counts_mt""]. For the both commit it fails so report the same time. @flying-sheep , can you look at the this benchmark test?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3110#issuecomment-2181072184:168,variab,variables,168,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3110#issuecomment-2181072184,1,['variab'],['variables']
Modifiability,"> @WeilerP, do you think this would be more appropriate in `scvelo`? (Side note, I have thought that tutorial; > of going from BAMs through `scvelo` would be quite useful). Hm, not sure if the functionality would match the expectation. In `scvelo`, we'd store only unspliced and spliced counts (spliced both in `adata.X` and `adata.layers`). Based on the proposed code snippet in [alexdobin/STAR#774 (comment)](https://github.com/alexdobin/STAR/issues/774#issuecomment-850477636), the expected output would be to read all of the available information?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1860#issuecomment-873990253:332,layers,layers,332,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1860#issuecomment-873990253,1,['layers'],['layers']
Modifiability,"> @flyingsheep I can assure you, that's the normal case in academic HPC systems. I agree that this is a huge and common problem in many HPC systems. I usually install conda and R packages to non-home directories with bigger space to avoid issues on servers. One can fill up hundreds of MB by just installing a single package e.g. human genome from Bioconductor 😄 . > Do you have a user home? Is there a canonical cache directory outside of the user home? Is there a way to detect that we are on such a system or a environment variable pointing to the canonical cache directory?. There is a user home and the cache is `~/.cache` and $XDG_CACHE_HOME is undefined (at least in my case). Some pip wheel files are there for example. . Although it's painful to work in such systems, I believe it's user's responsibility to fix this. One idea might be to print a warning when the cache directory is created for the first time along with the path itself to inform the user about where files are.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-476797878:526,variab,variable,526,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-476797878,1,['variab'],['variable']
Modifiability,"> @zhangguy, adding on to some thoughts from your PR [#2055 (comment)](https://github.com/theislab/scanpy/pull/2055#issuecomment-987012001); > ; > From my reading of that PR, you added a boolean argument `groupby_expand` which, when `True`, assumed `group_by` had two values: a grouping variable for the rows of the plot and a grouping variable for the columns of the plot. It also assumed `var_names` was a single variable which would be used to fill cell in the plot. As an example:; > ; > ```python; > pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(); > pbmc.obs[""sampleid""] = np.repeat([""s1"", ""s2""], pbmc.n_obs / 2); > ; > sc.pl.dotplot(pbmc, var_names='LDHB', groupby=['louvain', 'sampleid'], groupby_expand=True); > ```; > ; > ![tmpdm8256t1](https://user-images.githubusercontent.com/8238804/144899323-c439785d-5d57-4a18-b6e5-2b12412465f8.PNG); > ; > Instead of having an argument which changes the interpretation of the earlier arguments, I would prefer more orthogonal arguments.; > ; > I think you'd be able to get an output close to what you would currently like with:; > ; > ```python; > import scanpy as sc, pandas as pd, numpy as np; > ; > pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(); > pbmc.obs[""sampleid""] = np.repeat([""s1"", ""s2""], pbmc.n_obs / 2); > df = sc.get.obs_df(pbmc, [""LDHB"", ""louvain"", ""sampleid""]); > ; > summarized = df.pivot_table(; > index=[""louvain"", ""sampleid""],; > values=""LDHB"",; > aggfunc=[np.mean, np.count_nonzero]; > ); > color_df = summarized[""mean""].unstack(); > size_df = summarized[""count_nonzero""].unstack(); > ; > # I don't think the var_names or groupby variables are actually important here; > sc.pl.DotPlot(; > pbmc,; > var_names=""LDHB"", groupby=[""louvain"", ""sampleid""], # Just here so it doesn't error; > dot_color_df=color_df, dot_size_df=size_df,; > ).style(cmap=""Reds"").show(); > ```; > ; > I think this functionality could be more generic, and inspired by the `pd.pivot_table` function. This could end up looking like:; > ; > ```python; ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1876#issuecomment-988045664:287,variab,variable,287,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1876#issuecomment-988045664,3,['variab'],['variable']
Modifiability,"> AFAICT, I think the parallelization you're seeing will be due to the underlying calls in statsmodels. If you turn down the number of threads blas can use, do you see the same utilization?. FYI more n_jobs seems to be slower for regress_out if I don't disable the BLAS multi threading:; ```; sc.pp.regress_out(adata, ['percent_mito'], n_jobs=1); ```; ```; regressing out ['percent_mito']; sparse input is densified and may lead to high memory use; finished (0:04:05); ```; ```; sc.pp.regress_out(adata, ['percent_mito'], n_jobs=24); ```; ```; regressing out ['percent_mito']; sparse input is densified and may lead to high memory use; finished (0:07:41); ```. I'm using scanpy 1.5.2.dev104+g8611dba1. Indeed after disabling BLAS multi threading sc.pp.regress_out will only use one core if setting n_jobs = 1. But it has to be disable by exporting these environmental variables before starting python ..., but I guess it is not a good thing because other scanpy functions may be affected?; ```; export MKL_NUM_THREADS=1; export NUMEXPR_NUM_THREADS=1; export OMP_NUM_THREADS=1; ```; More interesting is that regress_out becomes lightning fast when n_jobs = 24 and with BLAS multi threading disabled:; ```; sc.pp.regress_out(adata, ['percent_mito'], n_jobs=24); ```; ```; regressing out ['percent_mito']; sparse input is densified and may lead to high memory use; finished (0:00:23); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1396#issuecomment-684103610:868,variab,variables,868,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1396#issuecomment-684103610,1,['variab'],['variables']
Modifiability,"> About the commit process: That's far far too much work to do it like you suggested. I don't have the time for this. As a general point about this PR: to me, the fair amount of the work of turning on flake8 deciding on the rules. Perhaps we should start with a subset of files then? I realize you did not come to the meeting where we talked about this, so perhaps there is a difference of expectations here, but we agreed to be conservative about the rules we turned on in `pre-commit`. Going through everything to make sure changes are correctly reverted is also takes a lot of time for me as the reviewer. I'd also like to limit that. ----------------. You said you used some automated tools to get faster compliance. What were these? In general, I would prefer to have a formatter that automatically ran than a tool that told me I formatted something wrong. -----------------. > `@ivirshup` I would keep the noqas. They are very easily searchable across the whole project and can be fixed later. I'm pretty strongly against this. `noqa`s just look like the formatter/ linter was wrong, and I'm not accepting that having no plan to address bugs. I think this should be a discussion with a broader set of the team. > ""Whats up with removing leading #s from comments?"" Not my choice either. What we have now is pep8 and flake8 compliant. If you're not happy with this we can ignore the rule. Yes, lets ignore this. >> ""I don't like replacing x == False with not x in all cases. Sometimes a variable could be a container, and an error should be thrown. I think cases have to be evaluated for this.""; >; > This should be covered by tests. In any case it is not good style and a violation. I will try and take a closer look at these changes. I'm particularly concerned that there will be cases where possible values are `None`, `True`, and `False`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1689#issuecomment-785871670:1491,variab,variable,1491,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689#issuecomment-785871670,1,['variab'],['variable']
Modifiability,"> Ah I think I see the issue! Feature branches should be based off `master` and directing the pull request there! I think what's happening is that a pre-commit hook was installed, but the config only exists on the `master` branch.; > ; > I think this should largely be manageable by rebasing onto master (e.g. `git rebase --onto master 1.7.x`) and changing the branch the PR is targeting via the github interface:. Thanks a lot, I rebased and changed the PR target to `master` so I hope everything is on track now! ; The pre-commit style checks were working as expected now (auto-edits only in the files / parts I edited). > Side note: We're considering separating the highly_variable_genes interface into multiple functions, since the arguments to the different methods don't always overlap in meaningful or intuitive ways. There's nothing you need to do about this right now, but just a heads up to keep the logic for this method separate from the main function. Sounds good!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-795469189:188,config,config,188,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-795469189,2,['config'],['config']
Modifiability,"> An alternative solution to this would be `pyplot.imshow(..., origin='lower')`; > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L301; > by default `pyplot.imshow(..., origin='upper')` which makes y-axis flipped compared to the scatterplot. Good point, but if `img_key=None` shouldn't we assume that there is no image? So nothing to `pl.imshow`. ; Now the point would be:; * passing a dummy array as ""image"" and then plot the spots as `circles`, so keeping the normal behavior; * plot the spatial data as a simple scatterplot. . My reasoning is that, since there is no image to show:; * we probably should assume that also `scalefactors` are empty.; * the correct size of the spots (given by `scalefactors`, and plotted with `circles`) does not matter anymore, since their size is only important in the presence of an image in the background, so a normal scatterplot would do.; * If the user does not want an image in the background, but wants to retain the size of the spots (because `scalefactors` are not empty), then `alpha_img=0` should do the job. . Because of these reasons, I would go for the second option above, passing the `coords` as basis to the scatterplot fun. Probably an even better option would be to plot the spots as hexagons, as originally suggested by @flying-sheep , but would wait for that after the plotting module is refactored. > @giovp conditionally doing that would probably be cleaner and wouldn’t involve flipping the data right?. The reason for flipping is that the coords from space ranger are given with upper origin. ```python; sc.pl.embedding(adata_spatial, basis = ""coords""); ```; ![image](https://user-images.githubusercontent.com/25887487/79198789-55bbed80-7e34-11ea-9db6-66da7d700cd2.png). Happy to discuss and change the behavior, I could have missed a crucial point.; And thank you @vitkl for feedback, we are in dire need of spatial transcriptomics scanpy users !",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1149#issuecomment-613283777:1383,refactor,refactored,1383,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1149#issuecomment-613283777,2,['refactor'],['refactored']
Modifiability,"> And scipy is also some 100 MB right?. Scipy is actually under `~/.cache` on my mac, ¯\\\_(ツ)_/¯. > Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. > miniconda is somewhere else for me by default, and it contains everything. I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. > You'd not notice it much, because datasets are just being re-downloaded on demand. So the compute nodes on this HPC have limited internet connectivity. One of the use cases I'd had for adding the expression atlas was to be able to easily try a method across a bunch of test datasets. If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. > My favorite command line interfaces have the ability to query options and set options globally by writing to a config file. I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-478212804:1227,config,config,1227,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-478212804,4,['config'],"['config', 'configs']"
Modifiability,"> As a general approach to this kind of problem, I write functions like this:; > ; > ```python; > def grouped_obs_mean(adata, group_key, layer=None, gene_symbols=None):; > if layer is not None:; > getX = lambda x: x.layers[layer]; > else:; > getX = lambda x: x.X; > if gene_symbols is not None:; > new_idx = adata.var[idx]; > else:; > new_idx = adata.var_names; > ; > grouped = adata.obs.groupby(group_key); > out = pd.DataFrame(; > np.zeros((adata.shape[1], len(grouped)), dtype=np.float64),; > columns=list(grouped.groups.keys()),; > index=adata.var_names; > ); > ; > for group, idx in grouped.indices.items():; > X = getX(adata[idx]); > out[group] = np.ravel(X.mean(axis=0, dtype=np.float64)); > return out; > ```; > ; > Swapping out the last 8 lines or so depending on what I'm calculating. To use a set of marker genes I'd call it as `grouped_obs_mean(adata[:, marker_genes], ...)`.; > ; > At some point we might have `groupby` for `AnnData`s, but that'll require figuring out how to be consistent about the returned type. Thanks. But need to make a tiny amendment to make it work now:; ```python; out[group] = np.ravel(X.mean(axis=0, dtype=np.float64)).tolist(); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/181#issuecomment-1871723078:216,layers,layers,216,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/181#issuecomment-1871723078,1,['layers'],['layers']
Modifiability,"> Awesome, Gokcen, thank you! 😁; >. Thank you!; ; > Also, adding an export utility for Gephi was on the list already before. Cool that you found a simple solution for this.; > . Ah ok, didn't know that. Here is what I used so far for gephi:. ```python; # python-igraph from master branch is required; # see https://github.com/igraph/python-igraph/issues/115; from igraph.remote.gephi import GephiConnection, GephiGraphStreamer. sc.tl.draw_graph(adata); # would be also nice have access to igraph object right after sc.tl.draw_graph; g = sc.utils.get_igraph_from_adjacency(adata.uns['data_graph_norm_weights']). # then install latest Gephi and the streaming plugin:; # https://gephi.org/plugins/#/plugin/graphstreaming; # and start the Gephi master server; streamer = GephiGraphStreamer(); conn = GephiConnection(workspace=1). # igraph cannot serialize numpy float32 to json, so it must be converted to float64; g.es['weight'] = [float(x) for x in g.es['weight']]; g.vs['groups'] = adata.obs['louvain_groups'].tolist(); streamer.post(g, conn); ```. Here is the Yifan Hu layout for 3K PBMC:. ![image](https://user-images.githubusercontent.com/1140359/34961174-384c5658-fa0c-11e7-8597-db4e77cbf4e3.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/68#issuecomment-357787075:657,plugin,plugin,657,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/68#issuecomment-357787075,6,['plugin'],"['plugin', 'plugins']"
Modifiability,"> CCA does not have code in python, which will make it difficult to integrate, pySCENIC is probably easier but I would rather ask the developers.; > ; > @falexwolf We should consider a way to facilitate scanpy 'plugins'. A quick search shows me that this could be possible: https://packaging.python.org/guides/creating-and-discovering-plugins/ but honestly I don't know how it works. Nevertheless, given the number of tools that continue to appear we should consider a scheme that facilitate how developers can take advantage of scanpy preprocessing, storing, analysis and visualization tools. Pyscenic has been integrated into scanpy now! Here is the hyper link:; https://github.com/aertslab/pySCENIC/blob/master/notebooks/pySCENIC%20-%20Integration%20with%20scanpy.ipynb",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/265#issuecomment-509063881:211,plugin,plugins,211,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-509063881,4,['plugin'],['plugins']
Modifiability,"> Can we call this epi_sc_expression_atlas instead of expression_atlas?. Definitely agree it's good to specify it's from EBI. Would `ebi_expression_atlas` be alright? `ebi_sc_expression_atlas` feels a little verbose for me. I think it's implied it's single cell, plus it's explicit in the doc-string. > For the time being, can we make this settings.datasetsdir instead of settings.dataset_dir and add it here:. Changed the name. It looks like the main docs aren't being generated from `scanpy/scanpy/api/__init__.py`, but from `docs/api/index.rst` instead. Which is correct?. > Can we point it to the home directory by default, I'd say ~/scanpy-datasets/?. Changed. Does this mean config changes should also happen in this PR? I think this may cause trouble (HPC environments with small `~` allocations) without allowing default configuration at the same time. I had previously figured that setting up a config file could be factored out to a separate PR. To be able to put off adding the config for now, we could temporarily special case a `SCANPY_DATASETDIR` environment variable.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/573#issuecomment-478414057:681,config,config,681,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573#issuecomment-478414057,5,"['config', 'variab']","['config', 'configuration', 'variable']"
Modifiability,"> Can you point to a package whose test organization you would like our tests to emulate?. - pytest: https://github.com/pytest-dev/pytest/tree/main/testing; - loompy: https://github.com/linnarsson-lab/loompy/tree/master/tests. The others have their tests in the package, and just have that useless `__init__.py` in the tests directory because of either cargo culting it or becaue they know that makes setuptools’ `discover_packages` or so find it. > From your description above I had thought you didn't want to emulate [pandas use of conftest](https://github.com/pandas-dev/pandas/blob/main/pandas/conftest.py?rgh-link-date=2022-04-12T13%3A19%3A30Z). What do you mean specifically?. Pandas are defining special pytest functions/variables there and fixtures, which is what it’s for. I’d probably judge that we don’t need all those fixtures for our complete test suite and move some of them to a smaller scope (e.g. `tests/io/conftest.py` or so). > I'd lean towards it, but I fully expect issues like https://github.com/scverse/scanpy/pull/685 to come up. This is why I'd like to see a working example of what you want to work towards. Actually I think we can fix that: [the docs for `pytest_addoption`](https://doc.pytest.org/en/latest/reference/reference.html#pytest.hookspec.pytest_addoption) say it has to be defined at the *tests root directory* which can be configured using the [`rootpath`](https://doc.pytest.org/en/latest/reference/reference.html?highlight=root#pytest.Config.rootpath) config option. > Is it definitely the future default? It looks like they are walking that back. The question is if they remove the others or not, I think: https://github.com/pytest-dev/pytest/issues/7245. ---. My intention here is to make clear which code lives under which laws. Pytest world is very different from Python module world. The presence of `__init__.py` fools people into thinking that we’re dealing with python packages/modules here, but that’s not true. The way pytest works is pretty simple:.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2225#issuecomment-1096900986:728,variab,variables,728,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2225#issuecomment-1096900986,2,['variab'],['variables']
Modifiability,"> Concatenating obsm without touching uns puts the object in an unstable state somehow from diffmap point of view. Sure, but this should only ever effect `diffmap`. . Arguably it also puts the object in an unstable state from a PCA point of view since there's no promise that observation loadings correspond to the variable loadings. I don't think users should have the expectation that meaning is preserved by concatenation, but I'm not sure if this is something people would believe. > I'm not entirely sure. Less experienced users might concatenate things and plot a UMAP without running sc.tl.umap on the new concatenated object and see some super weird things. Have users reported that this is confusing?. > It'd be cool to print a warning in such cases somehow, that concatenated obsms are not compatible or so. I think a note in the docstring for concatenation should be sufficient. My expectation is that it's much more common for our users to be familiar with what similar methods (like `np.concatenate` and `pd.concat`) do, and to have the right expectations about this. Bioconductor's `SummarizedExperiment` classes also do not warn about this, and concatenate along their `reducedDims`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1021#issuecomment-582736183:315,variab,variable,315,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1021#issuecomment-582736183,1,['variab'],['variable']
Modifiability,"> Could any noqas added in this PR get something searchable added to them (like # noqa: {rule} TODO: fix me) so we know why it was added?. The noqas already state what they are ignoring. I for now would not try to differentiate between noqas that we want to keep and noqas that we want to get rid of. We want to get rid of all of them in the follow up issue and only when examining all of them we will figure out which ones we want to keep. > Document how to turn off these checks in dev docs. What do you mean? How to ignore a single line? How to fully ignore whole checks? I would always refer to the flake8 documentation, because it will certainly maintained better than the dev documentation. > Add autopep8 to precommit. If things can be fixed automatically, they should be. autopep8 should be able to get it's rules from the flake8 config. I wish it were that easy. autopep8 does not take its configuration from the flake8 config file nor can it fix all pep8 violations nor do Black and autopep8 always work nicely together. Black is an **opiniated** formatter. It formats consistently, but not necessarily compatible with other tools. I would not add autopep8, since I do not see any further benefit to the Black & flake8 combination, only more potential for issues and confused developers. I agree with your other comments and will take care of them as soon as I got your answers :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1689#issuecomment-787424021:838,config,config,838,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689#issuecomment-787424021,3,['config'],"['config', 'configuration']"
Modifiability,"> FIXED: Updating adata.X to a scipy csr sparse matrix using `adata.X = scipy.sparse.csr_matrix(adata.X)` fixed this error.; > ; > I still get `RuntimeWarning: invalid value encountered in sqrt std = np.sqrt(var)` when running `sc.pp.scale(adata, max_value=10)` even after forcing to a csr matrix, but doesn't seem to affect downstream results... hi Rebecca, I have been trying to process scRNA (converted seurat to h5ad format) in python (processing like QC, normalisation, scaling, high variables, clustering etc) and have been getting stuck at the highly variable genes. Can you please help me out with it?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/391#issuecomment-1149911935:489,variab,variables,489,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391#issuecomment-1149911935,2,['variab'],"['variable', 'variables']"
Modifiability,"> For example, before scaling, you can just store a copy of your data by e.g., calling adata.layers['normalized_unscaled] = adata.X. Why is .copy() not necessary here?. ie, adata.layers['normalized_unscaled] = adata.X.copy()",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1650#issuecomment-1413256189:93,layers,layers,93,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1650#issuecomment-1413256189,2,['layers'],['layers']
Modifiability,"> Greatest advantage at first sight for me: scanpy.api.AnnData is now anndata.AnnData. to be fair, this was a simple consequence of adding intersphinx and would have been possible without the rest. > Also, you don't seem to have to mingle around with autodoc anymore, which seems a good thing... yes, but by now i added another, less invasive hack to get the parameter doc style the way you want them. it would be nice to have numpydoc-style parameter rendering as a separate extension or sphinx option. :skull_and_crossbones: the hack is also not finished, as in its current form, it’ll break docstrings with indentation (code blocks, lists, …). optimally the hack would be rewritten as a sphinx extension that can be loaded after the others. (it’s only a hack because it piggypacks on another extension just to ensure it runs last). > it's going to be a lot of work to rewrite all the docstrings... We don’t lose anything if we do it gradually: Unconverted Docstrings just render as they do now. > there might be some danger of introducing bugs as one needs to rewrite the function headers. i don’t think it’s possible to get bugs this way: we’re just adding type annotations, we don’t change the defaults or the order or anything.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/119#issuecomment-379833230:871,rewrite,rewrite,871,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/119#issuecomment-379833230,4,['rewrite'],['rewrite']
Modifiability,"> Hey, sorry about the late response!. No worries!. > Would you mind separating out the bug fix and feature addition? That was the bug fix can be released more quickly. OK, will do- see https://github.com/theislab/scanpy/pull/2023, https://github.com/theislab/scanpy/pull/2025. > Question about the main idea here: what kind of batches are you expecting to handle here?; > ; > If they were from completely separate sequencing experiments, would you want to have variable expected doublet rates between batches?; > ; > If the batches are multiple samples that were barcoded and multiplexed, would you want to allow for the possibility of multiplets across batches?. So, really, I just want to be able to follow best practice as per the [Scrublet docs](https://github.com/swolock/scrublet#best-practices), to be able to run Scrublet on cells from different samples separately, perhaps batches is the wrong term. Do you have a preferred alternative, or should I just clarify the help text?. > Could you also merge master into this? CI should be fixed now. Done",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1965#issuecomment-953614277:462,variab,variable,462,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1965#issuecomment-953614277,1,['variab'],['variable']
Modifiability,"> Hi, It's not available in scanpy at the moment, but I wrote a wrapper for it via `rpy2` and `anndata2ri` which is available here:; > https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py. Hi,. I have been trying to use this wrapper, but seems like there's some error during the conversion process:. RRuntimeError: Error in validObject(.Object) : ; invalid class “dgCMatrix” object: 1: invalid object for slot ""i"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer""; invalid class “dgCMatrix” object: 2: invalid object for slot ""p"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer""; invalid class “dgCMatrix” object: 3: invalid object for slot ""Dim"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer""; invalid class “dgCMatrix” object: 4: invalid object for slot ""x"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""numeric"". Any pointers to get around this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1068#issuecomment-866121061:495,extend,extend,495,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-866121061,4,['extend'],['extend']
Modifiability,"> How about a var_type argument which defaults to ""genes""? Could even replace occurrences of cells with a obs_type variable. Sorry about the late response. The week passes so quickly and depending on how it goes, I can only go back to Scanpy during the weekends these days... I like your new table much better than the previous one. If you'd put that forward in a PR, I'd be super happy!. PS: I think `control_variables` is fine, maybe `focus_variables` better reflects that it's not only about ""control"", but just a subset of variables (or observations) of interest. Let's not move forward on the suffix argument, your imposed structure will make things cleaner for people! :smile: `pct` is ok if in 0 - 100. No `obs_type` needed right now...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/316#issuecomment-437734137:115,variab,variable,115,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-437734137,2,['variab'],"['variable', 'variables']"
Modifiability,"> How do we xfail stuff from dev?. [`pytest.mark.xfail`](https://docs.pytest.org/en/6.2.x/reference.html#pytest-mark-xfail) takes a condition:. ```py; xfail_if_dev_tests = pytest.mark.xfail(; os.environ.get(""DEPENDENCIES_VERSION"", ""latest"") == ""pre-release"",; reason=""..."",; ). @xfail_if_dev_tests; def test_xzy(): ...; ```. You probably need to change the tests so it makes the CI variable visible as an env variable, I’m not an Azure expert so I don’t know if it already is. > Codecov, I think, is outright wrong aklthough that might have to do with the failing dev test. Yeah, maybe, let’s see once everything passes. I’m also OK with lowering the percentage, I just set it to 75% to have some indication if codecov is broken or working. (Before it would report 20% for a PR and there would be no visual indication that that’s a problem)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3048#issuecomment-2114691148:382,variab,variable,382,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3048#issuecomment-2114691148,2,['variab'],['variable']
Modifiability,"> I could not find a n_jobs argument in scanpy.pp.pca. Can you elaborate a little on the single threaded, multi-threaded bit?. The blas library used by numpy is multithreaded by default. You can change this by setting an environment variable. This might have to happen before numpy is imported. Here's how you'd do that:. ```python; import os; os.environ[""MKL_NUM_THREADS""] = ""1"" # If you're using MKL blas; os.environ[""OPENBLAS_NUM_THREADS""] = ""1"" # If you're using open blas; ```. Using sc.datasets.pbmc3k:. <details>; <summary> Single threaded </summary>. ```python; %time sc.pp.pca(pbmc, pca_sparse=True) ; CPU times: user 4.36 s, sys: 57.2 ms, total: 4.42 s; Wall time: 4.43 s. %time sc.pp.pca(pbmc) ; CPU times: user 15.7 s, sys: 127 ms, total: 15.8 s; Wall time: 15.8 s; ```. </details>. <details>; <summary> Multithreaded </summary>. ```python; %time sc.pp.pca(pbmc, pca_sparse=True) ; CPU times: user 28.9 s, sys: 5.44 s, total: 34.4 s; Wall time: 2.39 s. %time sc.pp.pca(pbmc) ; CPU times: user 1min 37s, sys: 23.6 s, total: 2min 1s; Wall time: 9.92 s; ```. </details>. > I noticed that if zero_center=False then TruncatedSVD does not accept the svd_solver argument and defaults to the randomized solver. Good catch! I'm pretty sure that should be passed the solver.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1066#issuecomment-589921438:233,variab,variable,233,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-589921438,1,['variab'],['variable']
Modifiability,"> I don't like the use of branches with `isinstance` because it breaks polymorphism, which is a key part of pythonic code to me. That’s what ABCs are for. `isinstance(thing, Mapping)` works beautifully for everything that has the `Mapping` protocol, no matter if it’s a `dict` or not.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-444816086:71,polymorphi,polymorphism,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-444816086,1,['polymorphi'],['polymorphism']
Modifiability,"> I don't understand this, why would this belong on sc.pp.neighbors? The graph weighing should go into the sc.tl.tsne call. Are the UMAP weights assigned to the graph in sc.pp.neighbors?. Yes, this is my understanding of how it works in scanpy. See https://scanpy.readthedocs.io/en/stable/api/scanpy.pp.neighbors.html:. ```; method : {‘umap’, ‘gauss’, ‘rapids’}, None (default: 'umap'). Use ‘umap’ [McInnes18] or ‘gauss’ (Gauss kernel following [Coifman05] with ; adaptive width [Haghverdi16]) for computing connectivities. Use ‘rapids’ for the; RAPIDS implementation of UMAP (experimental, GPU only).; ```. > If I understand option 2 correctly, we would normalize the 15 neighbors to essentially perplexity=5. That's not what I meant. I meant taking UMAP's weights for k=15 and normalizing the matrix so that it sums to 1, as in t-SNE. That said, I would also prefer option 1 because I don't want anything that sounds like it's a UMAP/t-SNE hybrid.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1233#issuecomment-748685742:464,adapt,adaptive,464,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1233#issuecomment-748685742,1,['adapt'],['adaptive']
Modifiability,"> I figured it'd be good to with everything being consistent. Plus documentation for the settings is now available through ?sc.settings.{setting}!. Of course, it's much better to be consistent. I was just suggesting a quick solution that wouldn't have been worse than the current one... ;). A complete automatic documentation is now also available from ; https://scanpy.readthedocs.io/en/latest/api/scanpy._settings.ScanpyConfig.html; under; https://scanpy.readthedocs.io/en/latest/api/index.html#settings. scanpy config: At some point almost 2 years ago, I removed a lot of stuff that I didn't think was essential to clean up the project. It was just an element of that. I'm very happy to introduce it again; these days, the project is much more major and indeed has many config options (there were only few at the time) and hence it would indeed merit having a config file.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/573#issuecomment-479750272:514,config,config,514,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573#issuecomment-479750272,3,['config'],['config']
Modifiability,"> I for now would not try to differentiate between noqas that we want to keep and noqas that we want to get rid of. We want to get rid of all of them in the follow up issue and only when examining all of them we will figure out which ones we want to keep. After this merges new ignore messages can be added for reasons like ""this rule is generally good, but not in this specific case"". Each of these will go through PR review, so will be vetted. The ones added here largely have not been vetted, and are just being added so we don't get a failure. I would like to be able to distinguish between these cases. Once more `noqa` cases are added, it gets more complicated to find cases that haven't been vetted if they don't have some associated label. --------------------------. > What do you mean? How to ignore a single line? How to fully ignore whole checks?. How to disable flake8 errors for a line or file. > I would always refer to the flake8 documentation, because it will certainly maintained better than the dev documentation. A link to the section of the flake8 docs on this would be great. -------------------------. > I wish it were that easy. autopep8 does not take its configuration from the flake8 config file . `autopep8` says it does this: https://github.com/hhatto/autopep8#configuration. > It formats consistently, but not necessarily compatible with other tools. I would like changes that are automatically applicable to be automatically applied. I'm thinking of things like white space in docstrings. Is there another way to automate these you can suggest?. ---------. BTW, I've added a few more points to the checklist above. I would recommend trying to build the package and build the docs in the directory you're working in to see what files get generated so they can be added to the `ignore`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1689#issuecomment-787426782:1180,config,configuration,1180,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689#issuecomment-787426782,3,['config'],"['config', 'configuration']"
Modifiability,"> I had the same issue, and it turns out setting up channels solves the problem as follows:; > ; > ```; > conda config --add channels defaults; > conda config --add channels bioconda; > conda config --add channels conda-forge; > ```; > ; > Ref:; > https://bioconda.github.io/recipes/scanpy/README.html; > https://bioconda.github.io/user/install.html#set-up-channels. Thanks, this also worked for me!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-762368917:112,config,config,112,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-762368917,3,['config'],['config']
Modifiability,"> I have got what I want with the following code adapted from dotplot():; > ; > gene_ids = adata.raw.var.index.values clusters = adata.obs['louvain'].cat.categories obs = adata.raw[:,gene_ids].X.toarray() obs = pd.DataFrame(obs,columns=gene_ids,index=adata.obs['louvain']) average_obs = obs.groupby(level=0).mean() obs_bool = obs.astype(bool) fraction_obs = obs_bool.groupby(level=0).sum()/obs_bool.groupby(level=0).count() average_obs.T.to_csv(""average.csv"") fraction_obs.T.to_csv(""fraction.csv""). Love this! Thanks a lot!! ; Just one question, is there a way to get the average expression in different cell types (cluster label 1 ) in different sample (cluster label 2 ) from an integrated object?? ; to get something roughly like this:. Gene 1 Gene 2 ; sample1 sample2 sample3 sample1 sample2 sample3 ..... ....... ....; T-cell; B-cell ; .....; ..... I am not sure if this makes sense, but I have been trying to do this for a while and nothing worked!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/336#issuecomment-1334674713:49,adapt,adapted,49,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/336#issuecomment-1334674713,1,['adapt'],['adapted']
Modifiability,"> I haven't worked much with h5py or tables, is it time-consuming to refactor these functions? It seems like moving to anndata is the most straightforward solution at least logically to me. In this case, I think it should be fine. It might not happen too soon if we're left to our own devices, so a PR is welcome. > I could just see a standalone package being widely used and community driven. What formats that aren't in `anndata` would you see in this package? I'm trying to get an idea of the kind of scope you're thinking of here. I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1387#issuecomment-683586582:69,refactor,refactor,69,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-683586582,1,['refactor'],['refactor']
Modifiability,"> I know all that. :wink:. And I know that you know! I just like to be comprehensive when presenting my arguments!. > But numpy, pandas, scikit learn, tensorflow, seaborn all have the comma-separated list as a convention and I'd really like to stick to that convention. OK. I’d prefer “a, b, or c”, but I’ll concede. It would also be no problem to change it later since all will be automated :+1: . > No, the optional keyword always means that a parameter has a default. Very often, people forget to append ""or None"" (, None) to the list of possible types. Well, when I open scanpy in PyCharm and someone forgot that in a type annotation, it highlights that fact to me. Pretty nice. > As mentioned before, there is no point in using set-theoretic/logical notions like union or intersection as the topic is so simple that it doesn't need it (no need for an intersection, it's not even clear what that would mean; if you're stringent about it, it's also not clear for union). Oh, then you didn’t hear of type theory. It’s a branch of logic: Type systems are formal systems, and in most of them the terms I used are well defined. The kinds of composite types I mentioned are:. - `Union` of types / Sum Type / [Tagged Union](https://en.wikipedia.org/wiki/Tagged_union): Variables with one of those have one of several fixed types.; - Subtype / [Intersection Type](https://en.wikipedia.org/wiki/Type_system#Intersection_types): Variables have all the properties of the supertypes.; - `Tuple` / [Product Type](https://en.wikipedia.org/wiki/Product_type): Variables contain multiple entries that each have one corresponding type.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-442007106:1266,Variab,Variables,1266,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-442007106,3,['Variab'],['Variables']
Modifiability,"> I know exactly that in PCA I can interpret a component based on its rank (and/or variance contribution). Ah, I meant more specifically that it may be easier to biologically interpret an ICA. > That would say I should try as many decompositions as possible to see when I get a good result. I'm a little unsure of your meaning here. Do you mean decompositions like decomposition techniques? If so, I don't think this is the right conclusion. I think it means: probably PCA for clustering, probably NMF for finding gene modules. I would also suspect something which finds sparser variable loadings like ICA or NMF could be more robust for cross dataset classification. If you mean, if the results are unstable how do we know which to trust – I did ask that question. I think it's the usual: have a validation dataset, maybe some ensemble/ robustness method, or do some sort of enrichment. It's an open question, but a lot of our analysis pipeline is.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/941#issuecomment-560313033:579,variab,variable,579,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/941#issuecomment-560313033,1,['variab'],['variable']
Modifiability,"> I might be missiong something, though. I personally have only had problems with the intel backends on my MacBook. I see that pinning the parallel backend would be a good way to make the code work for more people. Especially if this used to be worse. I think the capabilities of the different threading layers have also been tweaked over time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1931#issuecomment-874674744:304,layers,layers,304,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1931#issuecomment-874674744,1,['layers'],['layers']
Modifiability,"> I see them more as TODOs for later since the ""bad code"" is in master at the moment anyways. But if you want to discuss that - sure. I think it's hard to tell the difference between a `noqa` that was added because: ""most of the time, this rule is right, this time it is wrong"" vs. ""added to get rid of warning"". Could any `noqa`s added in this PR get something searchable added to them (like `# noqa: {rule} TODO: fix me`) so we know why it was added?. In a future meeting we can discuss with the whole team how we will actually fix these. I think it's a good task for a hackathon/ sprint. ------------. Looking at this again, I think this is pretty close to done. Just a few documentation/ minor rule changes left. - [x] Document how to turn off these checks in dev docs; - [x] Document the rules that are turned off (similar to [pandas](https://github.com/pandas-dev/pandas/blob/879cd22dd58b0574cdcaa7a26e396d5ec71a615a/setup.cfg#L71-L79)); - [x] Add autopep8 to precommit. If things can be fixed automatically, they should be. autopep8 should be able to get it's rules from the flake8 config.; - [x] Add annotation to `noqa`s.; - [x] Also add `TODO` annotation to `except Exception`s that have been added.; - [x] Turn off E731; - [x] Turn off the rule that doesn't allow multiple leading `#` in comments; - [x] Turn off F811 for tests (rule violated by using fixture as test argument); - Possible solution: add noqa for this to all files under `tests`, separate check that all files under tests have this.; - [x] Ignore `build` `docs/_build` directories",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1689#issuecomment-787412099:1089,config,config,1089,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689#issuecomment-787412099,1,['config'],['config']
Modifiability,"> I think we should have a 'cookbook' where we can keep this and other information. I've been trying to be organized about keeping notebooks around for this ([here](https://github.com/ivirshup/notebooks/tree/master/plotting)). Of course, I rarely get the notebooks clean enough to push 😆. > > In the end it's about showing which cells are represented per pixel/pixel bin.; >; > I would argue that this would be fair. In the end it's about showing which cells are represented per pixel/pixel bin. Is it fair if coloring by batch and one dataset had fewer samples? Wouldn't you want to know that multiple batches were showing up in this region? I'm fairly convinced there is no good way to show this in one plot, other than telling users some information is hidden. > We could do a quick fix based on random order for now. I'm trying to think of the simplest way to implement this. I would like to keep the behaviour of `sort_order=False` just using the order from the anndata object. Some options:. * `sort_order=""random""`, this would make the order random, but we might need to add a seed argument. Also, do we still plot over null values?; * `sort_order=order_array` where `order_array: np.ndarray[1, int]`. Basically, the user can pass whatever order they like. For random order it would be `np.random.choice(adata.n_obs, adata.n_obs, repeat=False)`. This is pretty flexible since it allows whatever order you want to be used without sorting the object. > larger update that would have to do with updating scanpy plotting to larger cell numbers?. I think this might be worth a separate package, at least to start out. At least with how I'm handling it now, there would be a large number of dependencies. Plus, I think overplottting like this is an unsolved problem, so freedom to experiment in important.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1263#issuecomment-761745895:1368,flexible,flexible,1368,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1263#issuecomment-761745895,2,['flexible'],['flexible']
Modifiability,"> I wasn't really expecting this feature PR to also include such a large refactor. It would have been necessary for the Dask Dataframe version. Now I 1. did the work and 2. improved readability, so it would be counter productive to undo it. > I'm still not 100% convinced the behaviour here is exactly the same as before. I have done a few tests, which have been okay, but I haven't tried much parameterization. I'm ~80% convinced the results should be the same. If you have any specific things in mind, you should probably make a PR that adds tests for the properties you think we should preserve. We can then merge that one, update this one, and see if it actually breaks something. I can’t check for speculative differences if I have no idea where those could be. > I would note that the dataframe returned when inplace=False has a different index than it did previously. Yup, now it actually matches instead of discarding the original Index and replacing it with a RangeIndex for no reason. > Apart from the comments, can we get a regression test for ""cell_ranger"" (e.g. generate results with an older version)? I don't think we have one in the test suite. Sure! That’s a concrete thing I can do. I’ll do that on thursday, I did the rest of what you asked today",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2809#issuecomment-1930104931:73,refactor,refactor,73,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2809#issuecomment-1930104931,4,"['parameteriz', 'refactor']","['parameterization', 'refactor']"
Modifiability,"> I wouldn’t call that situation fine, doing things at import time or even just requiring a certain value as configurable global state is bad behavior. It was fine in that it made UMAP work more places than it would have otherwise. > This means our solution for the second shouldn’t be that we hardcode a threading layer to use here. We could make it configurable on our end or something, but no import time global state change. I don't really want to have to touch it, especially since I believe it's immutable after any parallel code is called – so we can't actually control in. Our hands are a little tied by having a dependency do this. I would like to know why this is still considered necessary for pynndescent.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1931#issuecomment-874661678:109,config,configurable,109,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1931#issuecomment-874661678,2,['config'],['configurable']
Modifiability,"> I'd like the following sequence of commands ... to produce a reasonable t-SNE result that could be called ""t-SNE"" in publications. ; > I am worried that it may be a bit weird to refer to this as ""t-SNE"" in publications. I share the same worry, but am not qualified to answer when something becomes ""t-SNE"". I think it would be sufficient for `sc.tl.tsne` to warn users if the graph it was passed looks unexpected (or if it could tell it was generated by a different method). > What you suggest (t-SNE on normalized UMAP affinities) could maybe achieve that. From an API point of view, we don't control weights at the `sc.tl.umap` call, so I think it would be strange to control weights at the `sc.tl.tsne` call. I'm also not sure if binarizing the graph would be closer to ""t-SNE"". ----------------------. About `sc.pp.neighbors` vs `sc.pp.neighbors_tsne`. > This is just a question of API, and is less important for me personally. I agree that it could be better to have neighbors() compute kNN adjacency matrix without computing any weights, but this is refactoring beyond the scope of this PR. I think for backwards compatibility I would like to keep neighbors pretty much as is. I think new functions like `distance_neighbors`, `umap_neighbors`, `tsne_neighbors` could be reasonable to add. It's also possible we could add a `""tsne""` method to `neighbors`, but I think the implementation can look very similar to having a `tsne_neighbors` function, so this can be kicked down the road a bit.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-762595475:1058,refactor,refactoring,1058,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-762595475,1,['refactor'],['refactoring']
Modifiability,"> I'm curious about how much the backend changes the runtime and results of nearest neighbors methods. You can see some quick comparisons between Pynndescent and Annoy here: https://github.com/pavlin-policar/openTSNE/issues/101#issuecomment-597178379. But I have not investigated it very thoroughly. Anyway, returning to the main conversation:. I think switching to openTSNE makes sense even if nothing else that we are discussing is implemented. It's A LOT faster than Mutlicore t-SNE for large datasets: https://opentsne.readthedocs.io/en/latest/benchmarks.html. It is also more flexible, actively supported, conveniently packaged/distributed, etc. I don't see any possible disadvantage. You could potentially keep all the default parameters as you have now in scanpy (even though I would not recommend it, see below). However, what I said about using pre-build kNN graph requires some thinking. T-SNE uses perplexity=30 by default and uses kNN graph with k=3*perplexity, so that's 90 by default. UMAP uses k=15 and that's what you use in scanpy by default too. I can see three options here:. i) Let openTSNE do its own thing and ignore the kNN graph built in scanpy. Advantage: that's what you do now. Disadvantage: not very consistent architecture IMHO. . ii) Use the kNN graph built in scanpy and query() it to get 90 neighbors. Disadvantage: can be a bit slow. But I think it's better than (i). iii) Run t-SNE using 15 neighbors. Turns out, t-SNE with uniform affinities across 15 neigbours is *extremely* similar to t-SNE with perplexity 30. Evidence: https://twitter.com/hippopedoid/status/1232698023253303298. So you could run this version of t-SNE with uniform kernel. This will be very fast. Regarding default parameters: learning rate = 1000 that you use by default is simply not enough for large data (sample size in millions), as shown in that Nat Comms paper in detail. If you want to keep it for compatibility reasons, that's your choice, but be aware that you are getting suboptimal t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1233#issuecomment-633735833:581,flexible,flexible,581,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1233#issuecomment-633735833,2,['flexible'],['flexible']
Modifiability,"> I'm not sure I know what the pytest execution model is like, but does it ever start new processes for different tests?. Apparently [it needs a plugin](https://pypi.org/project/pytest-parallel/) for that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/724#issuecomment-513132199:145,plugin,plugin,145,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/724#issuecomment-513132199,1,['plugin'],['plugin']
Modifiability,"> I'm not sure what t-SNE implementation is currently used in scanpy, but would it make sense to switch it to openTSNE? It's a Cython re-implementation of FIt-SNE, it's available on conda and should be very easy to depend on. We use `MulticoreTSNE` if it's installed, but fall back to `sklearn`. > As far as I understand the scanpy architecture, it builds a kNN graph and then runs downstream analysis. Right now, we tend to use a connectivity graph built by UMAP, but are working on making this more generic. We're thinking about allowing the UMAP embedding to be generated on graphs we provide as well. > 1. switch scanpy to using openTSNE for tSNE, using already constructed kNN graph. I think I'd like to see this. That package is much more actively maintained than our current backend, and looks interesting. I would like it if the TSNE was flexible about the graph that was used. I'm not sure that I'll get to this, but a PR would be welcome. I'd have to see some performance/ results before thinking about changing the defaults, or whether this would go into a major or minor version change. > 2. add tSNE support for ingest using openTSNE functionality. @Koncopd do you have any thoughts on this?. > 3. change default tSNE parameters (n_iter, learning rate, initialization) following openTSNE defaults. Again, I'd have to think about backwards compatibility. Maybe this could start as a `sc.tl.opentsne` function?. > 4. add some tSNE ""recipes"". I'd be interested in this. Skimming that paper now, I really like the idea of showing regions of uncertainty for projection would be very useful. I'd be interested in how these ""recipes"" could be wrapped in a function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1233#issuecomment-631235395:846,flexible,flexible,846,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1233#issuecomment-631235395,2,['flexible'],['flexible']
Modifiability,"> I'm pretty sure I took your position back when this behaviour was added. The closest issue I could find was [theislab/anndata#115](https://github.com/theislab/anndata/issues/115), but I remember having a longer discussion with @flying-sheep about this.; > ; > However, I'm now pretty convinced that strings as categoricals is pretty necessary for computational reasons. There isn't a good ""array of strings"" in python, so all operations on those kinds are reaaaaally slow. Also, most of the time strings really are encoding a categorical variable. If they aren't, they should be unique (so we don't convert). I would still argue it's better to complain and get the user to fix it or convert to a categorical internally for the purposes of the function. It's not the conversion that I find the issue it's that there is no way to control it. I would be totally fine with including `sanitize_andata()` in examples, just as a function called by the user rather than internally. An example of a non-unique non-categorical variable would be alternative gene annotations. It's relatively common for more than one ENSEMBL idea to map to a gene symbol but that's not really a ""category"" that's useful for anything. Maybe the threshold of one repeated value is too high and it should be like 10%? That would probably become too unpredictable though. > I would generally agree with this. I don't like that `highly_variable_genes` will add mean and variance measures to the object. I guess I don't see converting strings to categoricals as being a big change, since most operations on them will have identical results. I wouldn't say the means/variances are side effects, more of an intermediate value. That's what the function says it's calculating and they are useful to have for later. Changing unrelated columns is what bugs me. > I do see how this would cause problems with `R` since this isn't the case with `factors`. I wonder if there this could be solved in the converter?. I don't think there is any g",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1747#issuecomment-801731499:540,variab,variable,540,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747#issuecomment-801731499,1,['variab'],['variable']
Modifiability,"> I've observed that it may introduce unexpected changes (not shown here) in the shape of the distribution that is different from all of the other transforms mentioned, so these need to be validated biologically against some ""ground truth"". Here's some selected examples (skipping the raw and geometric mean for reasons stated earlier) of the additional aspect introduced by CLR, beyond linearization of the signal, which illustrate how one might want to decide on a case by case basis which is biologically true:. Some *potential* artifacts:; - discreteness at low values (reflected in the histograms earlier), and a ""kink"" near there in the contour that doesn't match with a 2D-gaussian; - skewing of the ""absence"" of a marker depending on presence of another marker; - a weird double-positive tail that extends along the diagonal. These types of effects are reminiscent of [flow cytometry artifacts](https://docs.flowjo.com/flowjo/graphs-and-gating/gw-transform-overview/gw-transform-digital/). However, without proving which one is ground truth, we don't know for sure which one is true. At least initially, I would think that the CLR plots look more plausible. ![image](https://user-images.githubusercontent.com/20694664/83360046-51985080-a34c-11ea-9ec0-2057301ae4fc.png). ![image](https://user-images.githubusercontent.com/20694664/83360065-74c30000-a34c-11ea-9e0b-d28cea53993e.png). ![image](https://user-images.githubusercontent.com/20694664/83360079-84dadf80-a34c-11ea-9026-4256d8a3199b.png). I used a neutral word earlier: that CLR ""injects"" additional changes, but now it seems that may be a positive thing because many of these empirical cases seem believable from a biological standpoint -- a more systematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1117#issuecomment-636513215:806,extend,extends,806,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-636513215,2,['extend'],['extends']
Modifiability,"> If I understand the .raw removal alternative correctly, then you would want to add masks to every operation in scanpy that is not DE and work with .layers?. Pretty much every function where you would want to use `highly_variable`. > It seems to me that adding masking like this would be quite a large endeavour, no?. I think a similarly sized endeavor to adding `highly_variable`, except we can use the `highly_variable` code where it's been implemented. I would expect this to be less effort than supporting `raw`, which is a constant maintenance burden, especially for `anndata`. I think this logic could be added to the `_get_obs_rep`, and `_set_obs_rep` functions. --------------. > If you assume anything filtered out was removed because it was predominantly 0. I'm not sure I like having this assumption. Especially when a collaborator asks ""what about gene X"", but it just wasn't in the table I received. Maybe it's an annotation issue, maybe it wasn't expressed, or maybe it wasn't expressed globally at a high enough level – but could have been expressed in the cells of interest. > you can assume it would not be in the HVG intersection for that dataset and if you add it,. Is intersection the way to go? If you have cell types which are only present in some datasets, wouldn't you want to take the union?. > Typically there is sufficient gene-gene covariance that you still keep this signal somehow. I would agree that it is unlikely that this would have a huge effect on analyses like PCA or UMAP. When it comes time to do differential expression or show expression on an embedding, then it starts to be an issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1798#issuecomment-822937305:150,layers,layers,150,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-822937305,1,['layers'],['layers']
Modifiability,"> In the help documentation of sc.pp.scale, it is said ""zero_center If `False`, omit zero-centering variables, which allows to handle sparse input efficiently. I am still confused about zero_center. If zero_center=False, what will sc.pp.scale do ? Could you give a simple example ? For example, [1,2,3] would be [-1.22,0,1.22] after scaling, but what if zero_center=False ?. Just the data will be only scaled by stds, the means wouldn't be subtracted.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2164#issuecomment-1370694921:100,variab,variables,100,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2164#issuecomment-1370694921,2,['variab'],['variables']
Modifiability,"> Is there a widely used processing pipeline which does not adhere to this file naming?. STARsolo generates cell-ranger compatible output, and when multiple multi-mapper resolution strategies are enabled, it will write multiple matrix.mtx.gz files, with different names. e.g: `STARsolo ... --soloMultiMappers Unique EM PropUnique Rescue Uniform` yields:. ```; barcodes.tsv.gz; features.tsv.gz; matrix.mtx.gz; UniqueAndMult-EM.mtx.gz; UniqueAndMult-PropUnique.mtx.gz; UniqueAndMult-Rescue.mtx.gz; UniqueAndMult-Uniform.mtx.gz; ```. Each of these `*.mtx.gz` files matches the same format as `matrix.mtx.gz` and can be read in the same way. (They all share the `*.tsv.gz` files). . A 3-parameter version of the `read_10x_mtx()` function would be my vote as the most flexible option.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/882#issuecomment-2002523593:763,flexible,flexible,763,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882#issuecomment-2002523593,1,['flexible'],['flexible']
Modifiability,"> It doesn't have to be a TIFF image - in my experience slide scanners save JPEG images internally, so there is no value in converting that to TIFF. . This is interesting to know. > Also, it would be cool to use sc.pl.spatial for other technologies - say to overlay single cell spatial over the microscopy image image. . Can you elaborate on this? What tipe of technology and plot do you have in mind?. > I am wondering if you could add support for a fullres slot with size factor 1 and explain which variables need to be set for it to work in the tutorial. Mmh, I still think that the added value for looking at the fullres instead of the png in the context of overlaying spots to image is very little. In the hires png, even when cropping, the underlying resulting image is still quite good. Maybe not enough for analysis purpose, but for visualization should do the job no? I'm interested to hear your thoughts on this. The reason for not supporting it in the same way it's done now is that the image can be quite big (several GBs for fluorescent visium for instance) and so maybe it's not a good idea to load it in the anndata. I'll think about including a small section on this in the tutorial.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1436#issuecomment-703211980:501,variab,variables,501,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1436#issuecomment-703211980,1,['variab'],['variables']
Modifiability,"> It looks like running adata.raw.to_adata() does not preserve the var table, so that won't work either:. That makes sense though right? Since it's a different set of variables. That said, I do suspect there is some weirdness about handling of `use_raw` here that doesn't actually make sense when you think about it, but could break some code to remove.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2027#issuecomment-964290470:167,variab,variables,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2027#issuecomment-964290470,1,['variab'],['variables']
Modifiability,"> I’m not 100% up to date, but if you want more fancy differential expression analysis than what `rank_genes_groups` provides, you should give https://github.com/theislab/diffxpy a shot!; > ; > @davidsebfischer it’s maintained, right?; > ; > I’m going to close this unless I’m wrong and we want to enhance `rank_genes_groups` after all. Hi Philipp, I will try diffxpy. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2550#issuecomment-1640417629:298,enhance,enhance,298,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2550#issuecomment-1640417629,1,['enhance'],['enhance']
Modifiability,"> Let’s pay attention to not include any new function without * anymore, OK?. OK!. > Oh it’s definitely OK! I meant that it needs a bit of imagination to put other things in there than actual gene symbols, but I feel like it’s faster to see!. Yes, but I hardly imagine that you need two layers of annotations for variables outside the specific case of ENSids and gene symbols. So it's good to be verbose about this specific case and people wont miss such functionality in other cases.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/376#issuecomment-441598874:287,layers,layers,287,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376#issuecomment-441598874,2,"['layers', 'variab']","['layers', 'variables']"
Modifiability,"> Looks good, thanks for all the work!; > ; > We should add a release note for this at some point, I'm just not sure where yet, probably a section for dev practices. Could you suggest a line for that?; > ; > I was unsure about the variable naming for PAGA, so I've decided to revert that. I couldn't get flake8 to call it a redefinition. :tada: ; Maybe ""Enabled flake8 (https://flake8.pycqa.org/en/latest/) pre-commit to run code style checks""?; Everything else might just be details that people will uncover anyways since the workflows might complain :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1689#issuecomment-801763494:231,variab,variable,231,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689#issuecomment-801763494,1,['variab'],['variable']
Modifiability,"> Moving 10x reading functions to anndata. I haven't worked much with h5py or tables, is it time-consuming to refactor these functions? It seems like moving to anndata is the most straightforward solution at least logically to me. > scanpy as a requirement. I like scanpy, but the only thing we really *require* in scvi is the data loading part. A user could take their scvi outputs and go use Seurat if that makes them happy. And then like the data loading functions are simple enough that we could just implement them ourselves. I'm sure a lot of people are currently doing this, which inspired the idea to have a standalone package. > Splitting off new modules. Your questions are very valid. I don't really have good answers for them. I could just see a standalone package being widely used and community driven, especially if there is some scanpy backing + maybe optional dependencies/functionality to get your objects ready for R analysis pipelines.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1387#issuecomment-680188365:110,refactor,refactor,110,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-680188365,2,['refactor'],['refactor']
Modifiability,"> My opinion would be that you need to write `adata.raw = adata.copy()` if you want a copy to be made, since almost all assignments do not create a copy of the assigned object in anndata. But we should look into whether this is a change that was made deliberately or not. That makes python-sense. This is absolutely a change in convention though, see:. 1. The [original scanpy tutorial](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html); 2. The [scVI tutorial](https://docs.scvi-tools.org/en/0.6.8/tutorials/scanpy.html) (where they discuss needing to retain counts in raw); 3. Most notably in the [anndata API](https://anndata.readthedocs.io/en/latest/generated/anndata.AnnData.raw.html). In addition, both sc.pl.umap and sc.pl.paga_path() come to mind as functions that default to using the .raw layer. > If we don't change it, we could maybe warn if we're mutating `adata.X` and `adata.raw.X` also refers to the same thing?. I think that's a good idea. In general, it would be _very_ helpful to preserve in the anndata structure some record of the major transformations to .X (or any layer). > Overall, I would recommend that you use `adata.layers[""counts""] = adata.X.copy()` instead of using `.raw` at all though. This seems like good practice and the workaround we'll apply for now. I do wonder if some change was made after [this conversation](https://github.com/scverse/scanpy/issues/1798) which you were a part of. Thank you by the way, this package is an amazing tool.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3073#issuecomment-2151178325:1157,layers,layers,1157,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3073#issuecomment-2151178325,1,['layers'],['layers']
Modifiability,"> Oh, then you didn’t hear of type theory. It’s a branch of logic:. Indeed, I have never heard of that. But I doubt that it would be considered a branch of logic. 1. `Union type` is a pretty bad descriptor for a variable that can take _one_ of a set of fixed types. A union usually denotes a composition of multiple sets giving rise to a new set that contains all elements from these sets.; 2. `Subtype` is a great descriptor for a type that has properties of supertypes.; 3. `Intersection type` is an insanely bad descriptor for a variable that denotes the intersection of _properties_ of supertypes; the concept of such a subtype might be something useful in some languages and some cases and it might deserve a special name as it's the converse behavior of subclassing. But I have no idea how such a type would be useful in Python and in all cases that I've encountered. The [example on Wikipedia](https://en.wikipedia.org/wiki/Type_system#Intersection_types) already constructs a highly artificial case, whose relevance is opaque to me even though Scanpy features it in many instances: functions that overload parameters and have different overloading-dependent return types (standard example is passing an array instead of an AnnData, which triggers the automatic return of the computed annotation). What do you think about 3, @flying-sheep?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-443072359:212,variab,variable,212,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-443072359,2,['variab'],['variable']
Modifiability,"> Or should we call it col_groups as you did in your sc.pl.heatmap pseudo code?. That could be up to you. It depends on what the user is trying to achieve, which makes more sense. For instance, I'm not sure if it makes sense to allow splitting the columns by both variables and groups, or if that's the wrong abstraction. > I'd be more than happy to make it more generalized, i.e., to sc.pl.heatmap, but I may need some time to understand sc.pl.heatmap first. The plotting functions are getting really complex- it took me some time to understand _dotplot and _baseplot :). This code could definitely be a lot more simple. Would definitely appreciate help here! I think some of the concepts used in `seaborn` could be quite useful here, though it looks like they're under heavy refactoring at the moment ([relevant seaborn branch](https://github.com/mwaskom/seaborn/tree/skunkworks/features)). Maybe a good first step would be to fix how so the dotplot would look right if the user provides the dot size and dot color dataframes? Would make these plots possible, and gives an interface to try later approaches with.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1876#issuecomment-988956524:264,variab,variables,264,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1876#issuecomment-988956524,4,"['refactor', 'variab']","['refactoring', 'variables']"
Modifiability,"> Scipy is actually under ~/.cache on my mac, ¯\\_(ツ)_/¯. Sorry, I was too terse here: What I meant is that a wheel cached by pip (such as scipy) ends up in ~/.cache. And since some of those wheels are big, you need to clean that directory from time to time anyway if you have little space. > I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. My idea was that showing it every time would help people discover this. But the default scanpy log level is INFO anyway, right? So it would get shown by default if we info-log it?. > I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right?. The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-478230940:779,config,configs,779,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-478230940,6,['config'],"['config', 'configs']"
Modifiability,"> So I guess basic difference is that this one can convert starfish to anndata ""online"" without having to read from disk the anndata object. Sure. I was also wondering why use literal values here instead of the constant variables that starfish uses (e.g. `""cells""` vs. `Features.CELLS`). > this could also be fixed by sending a PR there and modifying what is saved in anndata of the expression matrix. I like the idea of having just one version of conversion, instead of them implementing a `to_anndata` method and us implementing an independent `from_starfish` method. Are either of you already in communication with the starfish developers? . Additionally, it seems like having a `to_anndata` method would be a pretty trivial modification of their code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1362#issuecomment-671896754:220,variab,variables,220,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1362#issuecomment-671896754,1,['variab'],['variables']
Modifiability,"> Some pip wheel files are there for example. And scipy is also some 100 MB right?. > Totally agree it's the user's responsibility. I would say that it's the devs responsibility to make it as easy as possible for the user. That's exactly my stance as well. > How about printing the absolute path of the data's destination on download?. I thought that too. Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. And put help on how to change the cache dir in the settings docs. > I thought the older ones would just be deleted, right?. Since those systems aren't configured well, probably not. On those systems, it would just be another directory. But on a laptop with a common Linux distribution, there would be a pop-up once your disk space gets low, which allows you to clear that directory with a click. > If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. You'd not notice it much, because datasets are just being re-downloaded on demand. That's a feature!. > [We don't have XDG_CACHE_HOME set]. Yes, because you only need it if you want your cache files to not be in `~/.cache`. > When I think about example datasets that are available through scientific computing packages I think of […]. I'm on mobile, so I don't want to check all of those, but. - miniconda is somewhere else for me by default, and it contains everything, not just data; - nltk pops up a window asking you to where to put stuff, and [recommends /use/local/share/nltk_data](https://www.nltk.org/data.html) for global installs, with no recommendation for per-user installs. I have a lot more stuff in my cache dir, not just applications. And as said: for good reason, because the OS often knows about this, which helps the user to delete the stuff with one click if needed. ---. My pe",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-477102890:629,config,configured,629,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-477102890,2,['config'],['configured']
Modifiability,"> Splitting off new modules; Finally, the idea of having IO functions go into their own package. I think this is a much bigger change, and I'd like to see a more fleshed out case for it. This would add a fair bit of complexity to development, so I'd want to be sure it's worth it. Some general questions I have:. > What are the advantages/ disadvantages of having smaller sub-packages?. method developer would just depend on those instead of (multiple) analysis package. > How does this impact users vs. developers?. user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. > Is IO special, or should more parts go into sub-packages?. it kind of is imho, it's all about having whatever data there is in an anndata/mudata shape. I must say that I'd also think plotting could be it's own separate package but it would probably require a lot of refactoring across packages (thinking about duplication of scanpy/scvelo code). > What gets re-exported from ""main"" modules?. didn't get this sorry. > Who manages the sub-packages?. the IO subpackage? everyone 😅 . so beside being in favour, it might also be that other issues arise. For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059346815:900,refactor,refactoring,900,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059346815,1,['refactor'],['refactoring']
Modifiability,"> Thanks for the update. Now is clear.; > ; > We do not offer that possibility as most of those functions are based on seaborn, thus, simply passing the relevant data to seaborn will get you the image that you want.; > ; > Nevertheless, I would like to take a look. How do you think this should work. Just add a variable to show the genes that you would like to see. Or you mean a more generic function just to make split plots between any two categories for the genes that you want to see?. Thanks for your attention. Yes it would be nice if I could compare two .obs categories with regard to expression distributions of a list of genes I supply. . Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1448#issuecomment-707563433:312,variab,variable,312,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1448#issuecomment-707563433,2,['variab'],['variable']
Modifiability,"> That's a great idea. It might require some reorganization, though, because currently use_raw is checked two places: once in sc.pl.scatter(), because it needs to know whether to look for variables in raw or not when deciding how to call _scatter_obs(), and again in _scatter_obs() itself. Would it be possible to not call it again in `scatter_obs`? E.g. could `_scatter_obs` not even need to know about the `raw` field?. > On another note, some pytests that are in files I did not edit are now failing because they can't find anndata.tests to import. I'm not sure if I messed something up by adding tests to test_plotting.py or whether this is a different issue. Aww crap, I think that was me making a new release. On the plus side it means our build system is now working as it's supposed to.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2027#issuecomment-964279124:188,variab,variables,188,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2027#issuecomment-964279124,1,['variab'],['variables']
Modifiability,"> This is not a pleasant way to navigate files:. <details>; <summary>You can fix that with the .ignore plugin!</summary>. Right click the Project view and select “Hide Ignored Files”. ![grafik](https://user-images.githubusercontent.com/291575/113685021-1c743180-96c6-11eb-8f73-eb7630857009.png). </details>. > I would be happy to have docs.scanpy.org/en/latest/api/dotplot.html be the canonical url, but not if it requires mixing generated and source files. That would be perfect if it’s possible!. > Since (AFAIK) the generated html page has to have the same name as the rst file, this would break links (as you mentioned above). While we could do a redirect from here, what do we redirect too?. We create manual redirects for the 2 APIs where we failed to add an underscore to the function name, and don’t do that again. The non-domain parts of URLs *are* case sensitive, therefore having a redirect `.../dotplot.html` → `.../dot_plot.html` works perfectly fine in the presence of `.../DotPlot.html`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1753#issuecomment-813954380:103,plugin,plugin,103,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1753#issuecomment-813954380,1,['plugin'],['plugin']
Modifiability,"> We have original radius dimension but it can be handy to modify it according to cropping/zooming, or simply for visualization purposes. Cropping/zooming won’t make a difference if you plot circles in data space. So there’s our problem: We have the original radius in data space, but you’re plotting markers, whose size is in figure space (i.e. their center position in the final figure is determined and then they’re plotted as circles right into the graphic). So you need to switch from `ax.scatter` to a `circles` function that does what we need: https://stackoverflow.com/questions/9081553/python-scatter-plot-size-and-style-of-the-marker/24567352#24567352. We can just adapt that one (throw out what we don’t need), make it so the `scatter(...)` calls in “embedding” work with it, and do `scatter = ax.scatter if img_key is None else partial(circles, ax=ax)`. This means that we don’t have to do difficult math when cropping/zooming, as the spots will always just be the correct size. We can also get rid of `spot_size` and make `size` a scale factor in the image case (1=normal size, 0.8=slightly smaller than in the data, 1.2=slightly larger than in the data)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1012#issuecomment-580144894:675,adapt,adapt,675,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1012#issuecomment-580144894,2,['adapt'],['adapt']
Modifiability,"> Went with properties for everything except private variables. Took a little longer than 10 minutes, but I think it's mostly there. Yes, now that you made everything a property, I would have expected it to take much longer than 10 minutes. It's great that you did!. I'll make some tiny additions and merge.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/573#issuecomment-479736498:53,variab,variables,53,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573#issuecomment-479736498,1,['variab'],['variables']
Modifiability,"> What I noticed was that if I didn't have the same ID columns in my adata.var when setting adata.raw I couldn't use gene_symbols. Ah, if you're using the values in `raw` for differential expression the column used for `gene_symbols` should be in `raw.var` this is because `raw` can have a different set of variables than the main object. So this case, at least, is expected.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1758#issuecomment-816352446:307,variab,variables,307,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1758#issuecomment-816352446,1,['variab'],['variables']
Modifiability,"> What do you think of using the | separator to describe adata.X | adata.layers[layer] e.g. [here](https://icb-scanpy--2742.com.readthedocs.build/en/2742/generated/scanpy.pp.regress_out.html)?. works for me!. > the inconsistent and mixed use of inplace and copy (effort: lot of work). yeah, we definitely need to do this … in a dedicated PR. > Might raise smaller issues in the future for these specific things rather than bloating this purpose-driven PR up?. Yeah! please add them here: https://github.com/orgs/scverse/projects/18/views/1",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2742#issuecomment-1814024532:73,layers,layers,73,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2742#issuecomment-1814024532,1,['layers'],['layers']
Modifiability,"> Why did we ever configure black?. because Alex liked `'` more. I think at the time we didn’t know of `pre-commit-hooks`’ `double-quote-string-fixer`, or it didn‘t exist yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2701#issuecomment-1772240411:18,config,configure,18,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2701#issuecomment-1772240411,1,['config'],['configure']
Modifiability,"> Why is this PR getting a build if there is no `pr` trigger entry in the yaml?. See 3 paragraphs down:. > If no pr triggers appear in your YAML file, pull request validations are automatically enabled for all branches, as if you wrote the following pr trigger. This configuration triggers a build when any pull request is created, and when commits come into the source branch of any active pull request.; > ; > ```; > pr:; > branches:; > include:; > - '*' # must quote since ""*"" is a YAML reserved character; we want a string; > ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1516#issuecomment-737862275:267,config,configuration,267,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1516#issuecomment-737862275,1,['config'],['configuration']
Modifiability,"> Yes, they are in that file: https://github.com/theislab/scanpy/blob/master/scanpy/plotting/palettes.py; > ; > Of course, user palettes are also accepted. Let me know if you need more. We'll extend this in the future. I have the data for 120 clusters, is it possible that I can have 120 colors?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/156#issuecomment-496532324:192,extend,extend,192,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/156#issuecomment-496532324,1,['extend'],['extend']
Modifiability,"> adata.uns['log1p'][""base""] = None. Thank you. I also had this error when calculating highly variable genes `sc.pp.highly_variable_genes(Adult,batch_key='batch')`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2239#issuecomment-1184556213:94,variab,variable,94,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2239#issuecomment-1184556213,1,['variab'],['variable']
Modifiability,> copy paste. please never say those words when speaking about code again :stuck_out_tongue_winking_eye: . No but seriously: There’s at least 6 reasons not to do that and to introduce a second (temporary) variable instead: https://github.com/theislab/scanpy/pull/557#issuecomment-476512533,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/458#issuecomment-476513549:205,variab,variable,205,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458#issuecomment-476513549,1,['variab'],['variable']
Modifiability,"> if we imagine a notebook with e.g. five sc.pl.dotplot calls. To me, italicizing the genes is more of a ""customizing plots for publication"" than ""looking at data"", so I think it's okay for it to have some extra steps as it happens less often. I personally have used illustrator for this kind of step. Alternatively, I tend to have functions like:. ```python; def customized_dotplot(adata, ..., dotplot_kwargs={}):; ... # Preprocess data, maybe add some fields; ret = sc.pl.dotplot(adata, show=False, ..., **dotplot_kwargs); ... # do additional things; ```. That said, I'm not against also having a global config for it, I would just want to see the structure for handling this global config for extra setting options sketched out in more detail. Also a PR implementing it 😉. > Last suggestion is like theme(axis_text_x=element_text(angle=90, hjust=1)). I do think it would make sense to have this outside of `set_figure_params`. `set_figure_params` does some global setting through the matplotlib config. Were you thinking of this as `DotPlot(...).theme(...)` or `sc.settings.theme(...)`? Maybe both, with the method overriding the config?. 1. I don't really like that `set_figure_params` modifies plots not generated by scanpy; 2. matplotlib doesn't have a concept of the ""variable axis"", so we'd have to handle this ourselves.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1913#issuecomment-873735647:606,config,config,606,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1913#issuecomment-873735647,5,"['config', 'variab']","['config', 'variable']"
Modifiability,"> it kind of is imho, it's all about having whatever data there is in an anndata/mudata shape. I must say that I'd also think plotting could be it's own separate package but it would probably require a lot of refactoring across packages (thinking about duplication of scanpy/scvelo code). @giovp and I are synced on these ideas :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059349278:209,refactor,refactoring,209,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059349278,1,['refactor'],['refactoring']
Modifiability,"> like pip install .[dev,test$(test_extras))], and run things once with test_extras='' and once with test_extras=',leiden,magic,harmony,scrublet,scanorama,skmisc'. Yeah, I was thinking something like this. Except we could just reduce `test` to include the barebones needed to make tests run, and separately have optional dependencies. The hard part here is structuring the tests so they can run without optional dependencies being present. We'd need to establish patterns for optional dependencies in fixtures and parameterized tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2211#issuecomment-1088721539:514,parameteriz,parameterized,514,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2211#issuecomment-1088721539,1,['parameteriz'],['parameterized']
Modifiability,"> ooh, this time the benchmark shows really nicely how much faster it is!. Looks like preprocessing_log.time_regress_out('pbmc68k_reduced') , regress out those variables that is not inside it. It should regress_out ['n_counts', 'percent_mito'] instead of [""total_counts"", ""pct_counts_mt""]. For the both commit it fails so report the same time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3110#issuecomment-2177815754:160,variab,variables,160,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3110#issuecomment-2177815754,1,['variab'],['variables']
Modifiability,"> scvelo depends on scanpy>=1.5, so I don't think this is the cause. Do you know why this is happening, and can you provide counter examples where it doesn't happen?. Ah weird. Pip tries to resolve the dependencies, and for that purpose gets all the installed packages’ metadata, then tries to figure out a configuration of upgrades that makes things work. No idea why it sees “1.7.0rc2” and decides “I’ll update this even when not asked to update”. Maybe raise this issue with pip?. > I'm not sure what you mean by this. Does flit install -s --deps=develop not count as reinstalling? Are you counting flit install -s as a development install?. Yes, that’s a reinstall in some development mode. My point was that if a scanpy pre-1.7 version really was installed, maybe pip was correct to update to 1.7 for some reason. However since we’re past 1.7 now, unless you haven’t git-pulled yet, I assume your dev install’s metadata has gone stale. I guess pip wouldn’t uninstall your dev install if you had run `git pull && flit install -s`, therefore updating the metadata. But I could be wrong, as I have no idea why pip thought it necessary to touch scanpy when installing scvelo. > I think pinning pip to an old version is worse than using a common, even if non-standard, installation method. Our setup.py is a compatibility shim solely for fallback use, not something to be relied upon in any part of our process. Usually when something does an arbitrary change making our life harder, our approach is pinning it temporarily until it fixed that or the infrastructure has adapted to its whims, right?. > It looks like the direction the discussion is headed is PEP 427 is wrong, and pip is right. Accepted PEPs are specs, so only pip and flit can be right or wrong (as they implement it). If people decide that what pip does happens to be *better* than the currently spec-compliant behavior, the spec can be changed accordingly. Until then pip is wrong, so we should pin its version to one that accepts sp",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1527#issuecomment-783309298:307,config,configuration,307,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-783309298,1,['config'],['configuration']
Modifiability,"> sparse_indicator doesn’t have its weights branches hit at all, maybe we should remove that? Or will this be used at some point?. I think it will be used at some point, but also happy to remove. I think parameterizing `test_aggregate_axis_specification` is overkill for what the test does.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2590#issuecomment-1953840990:204,parameteriz,parameterizing,204,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2590#issuecomment-1953840990,1,['parameteriz'],['parameterizing']
Modifiability,"> the main hpc I'm on 1gb of space where appdirs would put these files. That's a misconfigured server, not a normal case. We should use appdirs as default, catch a IOError on write, and send a nice message like. > Error: Cannot write to your cache directory. Please make sure there's space in {cache_dir!r} or override the cache directory by setting one of the $SCANPY_CACHE_DIR or $XDG_CACHE_DIR environment variables. All linux-based systems should set $XDG_CACHE_DIR if there's a better place than ~/.cache for such files.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-476675808:409,variab,variables,409,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-476675808,1,['variab'],['variables']
Modifiability,"> there is still a large amount of changes to the dataframe code here. Not really changes: it’s almost all refactoring, because the code was spaghetti with quite some duplication. I’m doing nothing more than. 1. I introduce helper functions so code gets more readable, e.g. a clean `disp_cut_off = _nth_highest(dispersion_norm, n_top_genes)` instead of a large inline code block that has to be decyphered line by line to figure out that it finds the nth highest value. This is especially necessary for the huge main pile of spaghetti that used to be the `if flavor == ""seurat"":`/`elif flavor == ""cell_ranger"":` branches. I simply put their contents into a `_get_mean_bins` helper and two helpers `_stats_seurat` and `_stats_cell_ranger` (while deduplicating shared code); 2. Making sure pandas indices match up while removing `.to_numpy()`. That way instead of having `.to_numpy()` potentially copy and and convert data in extension arrays, the series are just used directly. Not to mention that three `.to_numpy()` per line make things hard to read.; 3. refactor the 5 cutoff parameters into one value `cutoff` for clarity, less inline code, and better type information (we either have either `n_top_genes: int` or a `_Cutoffs` instance, never both. This way, the type system knows). and that’s it. <ins>potentially</ins> faster, much more maintainable, and almost dask-compatible. After my changes, it should be easier to further refactor things so the seurat_v3 flavor is integrated into the overall structure instead of doing its own thing.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2809#issuecomment-1929321140:107,refactor,refactoring,107,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2809#issuecomment-1929321140,8,"['maintainab', 'refactor']","['maintainable', 'refactor', 'refactoring']"
Modifiability,"> this is not a test of `filter_rank_genes_groups`. To me the point of the test is that `filter_rank_genes_groups` followed by `.pl.rank_genes_groups_*` returns the right image by comparing it to a reference. I mainly prefer that the reference is generated by as orthogonal a method as possible, since then we're closer to testing functionality than implementation. I also think that this strategy forms a more flexible implementation that can be extended to test other properties. For example, it's easy to modify this to check that the `sc.pl.rank_genes_groups_*(..., min_logfoldchange=)` argument plays well with `filter_rank_genes_groups`. I also like having an example of an equivalent implementation in the test suite.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1942#issuecomment-880411601:411,flexible,flexible,411,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1942#issuecomment-880411601,2,"['extend', 'flexible']","['extended', 'flexible']"
Modifiability,">=2.0.4 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from legacy-api-wrap->scanpy) (2.1); Requirement already satisfied: setuptools in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from legacy-api-wrap->scanpy) (42.0.2.post20191203); Requirement already satisfied: numexpr>=2.6.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from tables->scanpy) (2.7.0); Requirement already satisfied: more-itertools in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata>=0.7; python_version < ""3.8""->scanpy) (7.2.0); ```. ```; conda install -c bioconda scanpy; ```. ```; Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: |; /; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed. UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package pandas conflicts for:; scanpy -> pandas[version='>=0.21']; Package tqdm conflicts for:; scanpy -> tqdm; Package setuptools conflicts for:; scanpy -> setuptools; Package patsy conflicts for:; scanpy -> patsy; Package seaborn conflicts for:; scanpy -> seaborn; Package pytables conflicts for:; scanpy -> pytables; Package umap-learn conflicts for:; scanpy -> umap-learn[version='>=0.3.0']; Package networkx conflicts for:; scanpy -> networkx; Package readline conflicts for:; python=3.7 -> readline[version='>=7.0,<8.0a0']; Package joblib conflicts for:; scanpy -> joblib; Package importlib-metadata conflicts for:; scanpy -> importlib-metadata; Package tk conflicts for:; python=3.7 -> tk[version='>=8.6.7,<8.7.0a0|>=8.6.8,<8.7.0a0']; P",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:11199,flexible,flexible,11199,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452,2,['flexible'],['flexible']
Modifiability,"@Koncopd Currently breaking test for me:. ```pytb; $ pytest -k test_ingest; ===================================================== test session starts =====================================================; platform darwin -- Python 3.7.6, pytest-5.3.5, py-1.8.0, pluggy-0.12.0; rootdir: /Users/isaac/github/scanpy, inifile: pytest.ini, testpaths: scanpy/tests/; plugins: pylama-7.7.1, parallel-0.0.10, cov-2.7.1, black-0.3.7, hypothesis-5.6.0; collected 393 items / 389 deselected / 4 skipped . scanpy/tests/test_ingest.py ...F [100%]. ========================================================== FAILURES ===========================================================; _______________________________________________ test_ingest_map_embedding_umap ________________________________________________. def test_ingest_map_embedding_umap():; adata_ref = sc.AnnData(X); adata_new = sc.AnnData(T); ; sc.pp.neighbors(; adata_ref, method='umap', use_rep='X', n_neighbors=4, random_state=0; ); sc.tl.umap(adata_ref, random_state=0); ; ing = sc.tl.Ingest(adata_ref); ing.fit(adata_new); ing.map_embedding(method='umap'); ; reducer = UMAP(min_dist=0.5, random_state=0, n_neighbors=4); reducer.fit(X); umap_transformed_t = reducer.transform(T); ; > assert np.allclose(ing._obsm['X_umap'], umap_transformed_t); E assert False; E + where False = <function allclose at 0x119616b00>(array([[16.566338, 20.174282],\n [15.368203, 20.291983]], dtype=float32), array([[16.502459, 20.157679],\n [15.581459, 20.302881]], dtype=float32)); E + where <function allclose at 0x119616b00> = np.allclose. scanpy/tests/test_ingest.py:140: AssertionError; ---------------------------------------------------- Captured stderr call -----------------------------------------------------; computing neighbors; finished: added to `.uns['neighbors']`; 'distances', distances for each pair of neighbors; 'connectivities', weighted adjacency matrix (0:00:00); computing UMAP; finished: added; 'X_umap', UMAP coordinates (adata.obsm) (0:00:00); ``",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1036#issuecomment-599469073:361,plugin,plugins,361,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036#issuecomment-599469073,2,['plugin'],['plugins']
Modifiability,"@Koncopd has looked at refactoring the `rank_genes_groups` methods, but in the big picture we don't really love the output format that `rank_genes_groups` uses. Maybe an easier path forward would be to be able to directly pass values into the various plotting functions? You can already generate mostly similar plots from `sc.pl.rank_genes_groups_{plot_func}` and `sc.pl.{plot_func}` apart from using logfc and pvalues. If we allowed passing those in, it would be simple enough to make the same plots/ add a wrapper that generates the plots into `diffxpy`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1955#issuecomment-886408954:23,refactor,refactoring,23,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1955#issuecomment-886408954,2,['refactor'],['refactoring']
Modifiability,@Koncopd it is a correlation between two continuous variables as celltypes are continuous and age is also continuous. how to correlate X with continuous variables stored in .obs ?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1845#issuecomment-848118263:52,variab,variables,52,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1845#issuecomment-848118263,2,['variab'],['variables']
Modifiability,"@Koncopd pre-commit doesn’t *have* to be configured, you can choose not to enable it. Of course tests will fail then. regarding mypy: I guess it’s possible to make everything return `-> t.Any: # TODO fix typing`. I like isort!. Also we should get #1527 in before doing any big restructuring: It’s been through too many rounds of delays and I had to resolve conflicts so many times.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1563#issuecomment-757907537:41,config,configured,41,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563#issuecomment-757907537,1,['config'],['configured']
Modifiability,"@LisaSikkema, current behavior just changes the groups which are tested (I'd call this the ""left hand side"" in `group vs reference`) not what they are tested against. That is controlled by the `reference` argument. I agree this could be more clear. It would also be nice if `reference` was more flexible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1519#issuecomment-743963259:295,flexible,flexible,295,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1519#issuecomment-743963259,2,['flexible'],['flexible']
Modifiability,"@LuckyMD . I can replicate that with:. ```python; import scanpy as sc; pbmc = sc.datasets.pbmc68k_reduced(); pbmc.write(""tmp.h5ad""); fromdisk = sc.read(""tmp.h5ad"") # Do we read okay; fromdisk.write(pbmc) # Can we round trip; ```. Some context around this, and my current thinking on a solution:. * h5py doesn't do fixed length unicode strings; * h5py does do variable length unicode strings, pretty much anywhere; * zarr doesn't do variable length strings in structured arrays; * We probably don't actually want to use fixed length unicode strings much. Bytestrings, more likely.; * We can probably just add another element type to allow special handling for these. I think it'd be fine to not do `np.str_` type arrays.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/832#issuecomment-545345438:359,variab,variable,359,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832#issuecomment-545345438,2,['variab'],['variable']
Modifiability,@LuckyMD . Thank you for the whole in-depth discussion. It makes a lot of sense! :smile:. To your question: Scanpy has used Welch's adaption of Student's t-test from the very beginning. @davidsebfischer . Thank you! I guess it would be nice to have a single-cell tutorial in diffxpy that shows higher sensitivity by accounting for technical covariates.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/397#issuecomment-450025261:132,adapt,adaption,132,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397#issuecomment-450025261,1,['adapt'],['adaption']
Modifiability,@LuckyMD Can you maybe check if the new `highly_variable_nbatches` variable in `adata.var` is useful for finding the intersections? . `adata.var['highly_variable'] = adata.var['highly_variable_nbatches'] == len(adata.obs.batch.cat.categories)`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/614#issuecomment-486188826:67,variab,variable,67,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614#issuecomment-486188826,1,['variab'],['variable']
Modifiability,@LuckyMD should be easy to extend the `Plot` class to add this. I will think about it.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1127#issuecomment-607236910:27,extend,extend,27,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127#issuecomment-607236910,1,['extend'],['extend']
Modifiability,"@SamueleSoraggi you are right, the layers contain the same genes as the adata.X matrix. I assume that in your case, you did a highly variable gene selection which affects both adata.X and adata.layers but not adata.raw. The solution is to mark highly variable genes without removing the other less variable genes. This functionality was added some few months ago and may not be properly reflected on the documentation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/438#issuecomment-459668085:35,layers,layers,35,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438#issuecomment-459668085,5,"['layers', 'variab']","['layers', 'variable']"
Modifiability,"@VolkerBergen on the top of my head here are some features that were added recently:; * adjustment of multiple plots distance using `wspace` and `hspace` (indeed I have never used `left_margin` and `right_margin` which I think are not necessary); * multiple plots by a combination of `color` and `components`. E.g. This will produce four plots: `sc.pl.pca(adata, color=['CD14', 'CD8'], components=['1,2', '2,3']` ; * support for layers; * when the marker size given by the user is a vector, this is also subjected to any sorting of the cells (eg. `sort_order=True`, `groups=['<group name']` ); * added the option to outline the plot using the parameter `add_outline` #794 ; * added vmin and vmax as percentiles #794 ; * when using the `groups` parameter, plot those groups on top #891",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/617#issuecomment-555927399:429,layers,layers,429,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617#issuecomment-555927399,1,['layers'],['layers']
Modifiability,"@adamgayoso, have you had any thoughts here about how we manage the `key` for variable loadings?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1861#issuecomment-1081974157:78,variab,variable,78,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1861#issuecomment-1081974157,1,['variab'],['variable']
Modifiability,"@atarashansky sorry for getting back to you late! I played around with the implementation, thanks a lot for the notebooks!. few questions before opening the PR:; - do you think it's worth it to allow users to add other variables (beside log_umi) ?; - do you think it would be useful to add other models other than poisson?; - there are other outputs provided by R implementation other than pearson residuals. Do you think it's worth to include them?; - testing: how do you think it should be best tested? we thought about saving results from original implementation in R and test against those (as it's done for others seurat re-implementation like highly variable genes). looking forward to hear what you think! thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1643#issuecomment-783547730:219,variab,variables,219,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643#issuecomment-783547730,2,['variab'],"['variable', 'variables']"
Modifiability,"@biskra Just my two cents on (3), I mostly use histograms or hex bins for viewing qc metrics. I've been meaning to write this up as a tutorial for a while, but I think you can lean on whole pyviz ecosystem pretty heavily here. Something static and pretty:. ```python; # Joint distribution with marginal histograms:; sns.jointplot(; x=""log1p_total_counts"",; y=""log1p_n_genes_by_counts"",; data=adata.obs,; kind=""hex""; ). # For a single variable; sns.distplot(adata.obs[""log1p_total_counts""]). # To facet the distplot by some columns of obs, like ""batch"":; g = sns.FacetGrid(adata.obs, row=""batch"", height=1.7, aspect=3) ; g.map(sns.distplot, ""pct_counts_mito"") # Sadly, this doesn't work for joint grids; # I do have a PR for a ridge plot based on this in the works; ```. If you'd like it to be interactive:. ```python; import hvplot.pandas. adata.obs.hvplot.hexbin(""log1p_total_counts"", ""log1p_n_genes_by_counts""); adata.obs.hvplot.hist(""log1p_total_counts""). # Faceting; adata.obs.hvplot.hist(""pct_counts_mito"", by=""batch"", subplots=True).cols(1); ```. Or, if you want to play with cool most-biggest-data toys:. ```python; adata.obs.hvplot.scatter(; ""log1p_total_counts"",; ""log1p_n_genes_by_counts"",; datashade=True,; dynspread=True; ); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/510#issuecomment-473506882:434,variab,variable,434,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510#issuecomment-473506882,1,['variab'],['variable']
Modifiability,"@carmensandoval, there's no implicit copying so one should make a copy explicitly. . Please see the following code for more details:. ```py; import numpy as np; from anndata import AnnData; from jax import random. adata = AnnData(X=np.array(random.normal(random.PRNGKey(0), (100, 10)))). print(id(adata.X)); # => 5393766064. adata.layers[""X""] = adata.X; adata.layers[""X_copy""] = adata.X.copy(). for layer in (""X"", ""X_copy""):; print(f""{layer}: "", id(adata.layers[layer])); # => X: 5393766064; # => X_copy: 5393773552. print(adata.X[0, 0]); # => -1.5721827. adata.X[0, 0] = 0.0; for layer in (""X"", ""X_copy""):; print(f""{layer}: "", adata.layers[layer][0, 0]); # => X: 0.0; # => X_copy: -1.5721827; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2261#issuecomment-1413733462:331,layers,layers,331,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2261#issuecomment-1413733462,4,['layers'],['layers']
Modifiability,"@cdpolt, is there are specific change (""new behavior"") you're referring to?. > Storing things in layers sequentially, I just end up with a bunch of layers that all are identically fully processed . Would the code in the previous message be helpful to understand why that happens and how to fix that?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2261#issuecomment-2071131186:97,layers,layers,97,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2261#issuecomment-2071131186,2,['layers'],['layers']
Modifiability,"@chris-rands my two cents here: Using two many PCs (50 or 100) might not be such a good idea because most PCs above the first 15-20 (rough figure) are likely to represent non-biological variability (e.g., batch effects), so their inclusion in the analysis might lead to the identification of clusters that are not biologically-relevant but are rather technical noise. That's why it is recommended to use either a Jackstraw plot or an elbow plot to identify the optimal number of PCs to avoid including PCs that correspond to technical variation rather than biological heterogeneity. Therefore, overshooting the number of PCs used from 10 to 20 is not that big a deal, but using 50 is NOT ideal by any means.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/872#issuecomment-822057877:186,variab,variability,186,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872#issuecomment-822057877,1,['variab'],['variability']
Modifiability,"@chris-rands, that section of the Seurat tutorial also ends in these points:. > * We encourage users to repeat downstream analyses with a different number of PCs (10, 15, or even 50!). As you will observe, the results often do not differ dramatically.; > * We advise users to err on the higher side when choosing this parameter. For example, performing downstream analyses with only 5 PCs does significantly and adversely affect results. > Can anyone explain/show literature on if/why the scanpy default of 50 PCs works well?. I think there is enough to show that PCA works well. I'm not sure if I can show you a paper that says either choosing a high cutoff, or using jackstraw/ elbow plots gives better downstream results. I'd note that the [cited paper](https://www.cell.com/fulltext/S0092-8674(15)00549-8) for the Seurat tutorial doesn't seem to evaluate this. ---------------. @wolf5996, I'm not sure I agree with your point that lower PCs are more likely to contain non biological variability. I don't think that a component which explains more variability in the dataset would necessarily represent biological variability. As an example, if we have a dataset with two evenly sized batches, and a rare cell type which makes up ~1% of the population, wouldn't a PC representing the batch explain much more variability than a PC corresponding to the rare cell type?. Anecdotally, I can say batch effects can show up in high principal components.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/872#issuecomment-822286073:987,variab,variability,987,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872#issuecomment-822286073,4,['variab'],['variability']
Modifiability,"@coh-racng I would like to add that for your specific intention the best way is to load the `plot_scatter` function that accepts `basis` as parameter and works well with layers. The code should be:; ```PYTHON; from scanpy._plotting.scatterplots import plot_scatter`; plot_scatter(adata, basis='<name>'....); ```. @ivirshup, @falexwolf I think we should add `plot_scatter` to the API maybe renaming it `plot_embedding` to help users like @coh-racng. Currently we have two different ways to make scatter plots: One for embeddings (`plot_scatter`) and other more generic for obs and vars (`sc.pl.scatter`) that accepts `x` and `y` parameters.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/762#issuecomment-517618114:170,layers,layers,170,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/762#issuecomment-517618114,1,['layers'],['layers']
Modifiability,"@dawe What if you use the 'batch' field in `adata.obs` together with the index that might contain duplicates to get the uniqueness that your're missing. If that's not enough. I would do the same thing as [pandas.concat](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.concat.html), namely generating a multi-index. This would be just a one-line edit for `AnnData.concatenate()` and we should do this; we simply need to make sure that the indexing behavior of AnnData as a whole remains consistent with the following. @flying-sheep, could you do this? ; ```; import pandas as pd; s1 = pd.DataFrame({'v1': ['a', 'b'], 'v2': [2, 3]}); s2 = pd.DataFrame({'v1': ['c', 'd'], 'v2': [3, 4]}); s = pd.concat([s1, s2], keys=['s1', 's2']); print('... the concatenated annotations'); print(s); print('... all observations corresponding to ""s1""'); print(s.loc['s1']); print('... a single observation'); print(s.loc['s1', 0]); print('... values of a single observation'); # this is what we do not want in AnnData, only the behavior of the next line; # that is, a multi-index should be indexed with lists or tuples; print(s.loc['s1', 0].values); print('... single observation and a single variable'); print(s.loc[['s1', 0], 'v1']); print('... single observation and a single variable'); print(s.loc[('s1', 0), 'v1']); ```; gives; ```; ... the concatenated annotations; v1 v2; s1 0 a 2; 1 b 3; s2 0 c 3; 1 d 4; ... all observations corresponding to ""s1""; v1 v2; 0 a 2; 1 b 3; ... a single observation; v1 a; v2 2; Name: (s1, 0), dtype: object; ... values of a single observation; ['a' 2]; ... single observation and a single variable; s1 0 a; 1 b; Name: v1, dtype: object; ... single observation and a single variable; a; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/55#issuecomment-354903240:1188,variab,variable,1188,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/55#issuecomment-354903240,8,['variab'],['variable']
Modifiability,"@fabianrost84 The 'layers' were not added to anndata until recently and had not been implemented in all functions. However, adding this functionality to the different scatter plot functions is straightforward. Let me prepare a quick PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/458#issuecomment-476095057:19,layers,layers,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458#issuecomment-476095057,1,['layers'],['layers']
Modifiability,@falexwolf @fidelram how to we change the color palette for numerical variables? currently setting `palette = 'Oranges'` only works for the categorical ones,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/156#issuecomment-441815667:70,variab,variables,70,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/156#issuecomment-441815667,1,['variab'],['variables']
Modifiability,"@falexwolf I added `_utils_clustering.py` since I think it's the more maintainable way to do it (e.g. if in the future, new clustering methods are added).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/586#issuecomment-480280193:70,maintainab,maintainable,70,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586#issuecomment-480280193,1,['maintainab'],['maintainable']
Modifiability,"@falexwolf I think it is worth to have a `scanpy.plugin` or `scanpy.extension` or something shorter like `scanpy.pg` or `scanpy.ext` that aggregates all plugins. First, this clarifies for the user that the tool he/she is using is not directly developed by scanpy. Second, this allows plugins to be installed separately without having to update scanpy's code. The idea is that scanpy will be able to discover any plugins installed. On the side of developers, this could facilitate integration with scanpy. We can get inspired by flask extensions: http://flask.pocoo.org/docs/1.0/extensiondev/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/265#issuecomment-423915827:49,plugin,plugin,49,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-423915827,4,['plugin'],"['plugin', 'plugins']"
Modifiability,"@falexwolf I try to answer where I can. I should probably have clarified a bit above. I would argue that most real data DE tests benefit from accounting for technical covariates. For example, you should probably not perform batch correction on your data and then do a wilcoxon rank sum test, but instead take the normalized (and log transformed) data or the raw counts and include a batch covariate in the test. This also holds for technical covariates that describe the complexity of the data (such as size factors or n_genes). Often these factors are not sufficiently accounted for by simple normalization techniques (especially for plate-based data), and are thus included in the DE testing framework. This is done in MAST (and MAST performs better with this `detRate` covariate in the Soneson & Robinson paper you cite above), and it is also done in a recent negative binomial DE test from [Mayer et al, Nature 2018](http://www.nature.com/doifinder/10.1038/nature25999). When you are not able to fit the background variability in your model, you will have a lower sensitivity. Accounting for covariates is obviously not possible with t-tests or wilcoxon rank sum tests. Hence my statement about lower sensitivity. They did perform comparatively well in the DE method comparison, which is why I'd argue that they're useful for first pass exploratory applications (and marker gene detection when you don't want to use more fancy approaches like [this](https://www.biorxiv.org/content/early/2018/11/05/463265)). However, if you can account for technical covariates, that's probably a good approach to use. Also, according to the comparison paper you mention, there are not more false positives when using MAST or limma compared to t-tests or Wilcoxon rank sum tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/397#issuecomment-447865088:1019,variab,variability,1019,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397#issuecomment-447865088,2,['variab'],['variability']
Modifiability,"@falexwolf any update on the plug-ins idea for scanpy?. On Sun, Feb 3, 2019 at 4:50 PM Alex Wolf <notifications@github.com> wrote:. > Merged #457 <https://github.com/theislab/scanpy/pull/457> into master.; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/pull/457#event-2114366554>, or mute; > the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1auPPM2VVhh2E_5Gwd8djTqP9ltAks5vJwVLgaJpZM4agJQ1>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/457#issuecomment-460063532:29,plug-in,plug-ins,29,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/457#issuecomment-460063532,1,['plug-in'],['plug-ins']
Modifiability,"@falexwolf thanks for the feedback. :). I agree with your comments on the `sc.tl.dendrogram`. Similar reasoning originally motivated me to separate and expose the implementation of the function. I expect that now, is easier to extend the creation of a correlation matrix to other methods and groupings as you suggest. Currently, by default `sc.tl.dendrogram` uses PCA by recycling the function used by `sc.tl.neighbors` (`tools._utils.choose_representation()`). Any other embedding in `.obsm` can be used (as is the case by `sc.tl.neighbors`. Also, any group of genes can be given as parameter . What tl.dendrogram does not do is to use the underlying network to compute a distance matrix as I think seurat does and apparently you also do in PAGA. . For me, what is important is that the plotting functions get the dendrogram data from `.uns` and thus the generation of the hierarchical clustering is separated and can be computed by any other method.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/425#issuecomment-456065730:227,extend,extend,227,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425#issuecomment-456065730,2,['extend'],['extend']
Modifiability,"@falexwolf we just tried the solution you posted and it reveals a bug: when `ax` is not `None` you don't create the variable `axs` and thus throw an error here: https://github.com/theislab/scanpy/blob/master/scanpy/plotting/anndata.py#L634. Should be a simple fix (I think):. ```python; if ax is None:; axs, _, _, _ = setup_axes(ax=ax, panels=['x'] if groupby is None else keys, show_ticks=True, right_margin=0.3); else:; axs = [ax]; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/137#issuecomment-413354154:116,variab,variable,116,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/137#issuecomment-413354154,2,['variab'],['variable']
Modifiability,"@fidelram . As noted in #1632, dotplot labels can often extend outside of the plotted area. Whether the full labels can be seen is dependent on how the plots are being output. It would be great if this always worked. <details>; <summary> Example from the docs: </summary>. ![](https://user-images.githubusercontent.com/8238804/107312688-122e2080-6ae5-11eb-8a7e-f61c51a8392c.png). </details>. Could possibly be solved by using `matplotlib`'s `constrained_layout` or `tight_layout`. I think these would require modifying how the grid spec and axes are created.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1705:56,extend,extend,56,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1705,1,['extend'],['extend']
Modifiability,"@fidelram Thanks a lot! I was getting confused why my dot sizes weren't working (only sometimes, strangely...). If the dot sizes are sorted according to the `color` variable it should hopefully work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/543#issuecomment-475287691:165,variab,variable,165,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/543#issuecomment-475287691,1,['variab'],['variable']
Modifiability,"@fidelram Thanks, no further changes. I agree about the arbitrariness but I think it's good to provide a configurable implementation with reasonable defaults. This, I believe, is also what we do in other functions like pp.neighbors and pp.highly_variable_genes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/538#issuecomment-474786478:105,config,configurable,105,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/538#issuecomment-474786478,1,['config'],['configurable']
Modifiability,"@fidelram Yes, makes sense. Let's see whether we manage to organize it this way. There will be a few plugins coming soon and I'll talk with the one doing it about this. @wangjiawen2013 The Seurat developers did a bit more than simply fitting a standard CCA. So I'd assume that it'd be some work to wrap sklearn's CCA or pyrcca so that it performs similar to Seurat's CCA on single cell data...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/265#issuecomment-424548158:101,plugin,plugins,101,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-424548158,2,['plugin'],['plugins']
Modifiability,"@fidelram, I really like your plotting gallery! Would be cool to have that as part of the tutorials or even integrated in the main documentation (enhance each plotting function with an example image?)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/369#issuecomment-441069177:146,enhance,enhance,146,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369#issuecomment-441069177,1,['enhance'],['enhance']
Modifiability,"@flying-sheep As always, thank you for your thorough thoughts on the topic! And as always, my ""hacking-numerics"" perspective likely is not a path that is long term sustainable. With what I wrote at the very beginning of this thread, I simply wanted to express that I thought that we shouldn't transition quickly and immediately; for the cosmetic reasons and for the reason of staying away from creating entry hurdles. I still don't think that scanpy needs to precede major packages like numpy and many others in adapting type annotations. But, in essence, I trust you and if you want to push this further I'm fine if scanpy becomes somewhat a field of experimentation for how to deal with type annotations in scientific and numerics-centered software. . @ivirshup Thank you very much for your remarks, too! I agree with your concerns and examples, but wouldn't have been able to summarize them as neatly. *Conclusion:* @flying-sheep if you feel you have bandwidth for improving the cosmetics (thanks for what you did already, also the PR to ipython) that lead to more homogeneous docstrings (I'd say: `Union[a, b]` → `a, b`), of course, please go ahead. If people make PRs with old-school docstrings and without type annotations, I'd still not trouble them, for now. When we have converged on new docstrings and canonical type annotations so that at least people who really know what they're doing (@ivirshup) don't feel things are ambiguous anymore (say in a year), we can start to rigorously ask for them. PS: Thanks for the hints about Jedi etc. @flying-sheep. But likely, I'll keep playing around and reading documentation of packages using shift-tab in jupyter and develop using emacs relatively plain (there were times when I worked with quite some extensions, but these days, I'm back to almost plain for performance reasons - I know that's probably not smart, but anyways)...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-441472798:512,adapt,adapting,512,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441472798,2,['adapt'],['adapting']
Modifiability,"@flying-sheep That does look useful, though I definitely see a lot of toolkits using something like `~/.{toolkitname}` on my mac. Personally, I'd look under my home directory first. I think if things are going to move, there should be a sensible default, but the location should be user configurable (probably through an environment variable). For example, the main hpc I'm on only gives me about 1gb of space where `appdirs` would put these files. @maximilianh Currently I don't get coordinates along with some other computed values like SC3 clustering. I also believe it's only a subset of the metadata. I think there's more through the array express api, but that was more of a pain to navigate. Was there anything in particular you wanted to see included?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-476555082:287,config,configurable,287,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-476555082,2,"['config', 'variab']","['configurable', 'variable']"
Modifiability,"@flying-sheep This is generally the kind of simplification I was hoping we could do with plotting. ; It's not much, and is more about the dotplot, heatmap, etc. plotting methods. Also, the test errors I was running into are still happening. Another example would using a function to choose representations of X the same way for each function. Something like:. ```python; def _choose_obs_rep(adata, *, use_raw=False, layer=None, obsm=None, obsp=None):; """"""; Choose array aligned with obs annotation.; """"""; is_layer = layer is not None; is_raw = use_raw is not False; is_obsm = obsm is not None; is_obsp = obsp is not None; choices_made = sum((is_layer, is_raw, is_obsm, is_obsp)); assert choices_made <= 1; if choices_made == 0:; return adata.X; elif is_layer:; return adata.layers[layer]; elif use_raw:; return adata.raw.X; elif is_obsm:; return adata.obsm[obsm]; elif is_obsp:; return adata.obsp[obsp]; else:; assert False, (; ""That was unexpected. Please report this bug at:\n\n\t""; "" https://github.com/theislab/scanpy/issues""; ); ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1109:774,layers,layers,774,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1109,1,['layers'],['layers']
Modifiability,"@flying-sheep To answer your question. Honestly, I am not so familiar with Plotnine, Plotly or Altair. However, after a quick revision I would say that Altair seems quite interesting and possibly were I could had reused/extended some code. Yet, at the moment in scanpy we use matplotlib extensively and I didn't even think about the other APIs. Looking closely at Altair I realized that I have a lot to catch up regarding Vega, Vega-lite and the idiosyncrasies specific to Altair before I could start using it. . Thus, the current effort only integrates the idea of 'chaining' seen in Altair (or in other context in Pandas). In Plotly or Plotnine the 'chaining' is achieved differently but I don't find it as nice or straightforward:. **Plotly:**; ```PYTHON; import plotly.graph_objects as go; fig = go.Figure(; data=[go.Bar(x=[1, 2, 3], y=[1, 3, 2])],; layout=go.Layout(; title=go.layout.Title(text=""A Bar Chart""); ); ); fig.show(); ```; **Plotnine:**; ```PYTHON. from plotnine import ggplot, geom_point, aes, stat_smooth, facet_wrap; from plotnine.data import mtcars. (ggplot(mtcars, aes('wt', 'mpg', color='factor(gear)')); + geom_point(); + stat_smooth(method='lm'); + facet_wrap('~gear')); ```; **Altair:**; ```PYTHON; import altair as alt; from vega_datasets import data. source = data.cars(). alt.Chart(source).mark_circle(size=60).encode(; x='Horsepower',; y='Miles_per_Gallon',; color='Origin',; tooltip=['Name', 'Origin', 'Horsepower', 'Miles_per_Gallon']; ).interactive(); ```; The current solution, although using method chaining, is very *ad hoc* for a specific type of graphs that have predetermined features, like 'dendrogram' or totals for categories or 'brackets' to highlight features.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1127#issuecomment-607888729:220,extend,extended,220,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127#issuecomment-607888729,1,['extend'],['extended']
Modifiability,@flying-sheep suggested to write a hatch-plugin that can automatically create the respective @overload type hints,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2583#issuecomment-1664197260:41,plugin,plugin,41,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2583#issuecomment-1664197260,1,['plugin'],['plugin']
Modifiability,"@flying-sheep, I'm pretty sure the logical conclusion of any long discussion about types is that everything should be done in Haskell. I don't like the use of branches with `isinstance` because it breaks polymorphism, which is a key part of pythonic code to me. @falexwolf, I completely agree with you on ""what makes a good docstring"". The knowledge overhead for numeric python doesn't include type theory, so the docs should be interpretable without them. Ideally, interfaces are simple and the documentation makes the expected behavior clear. I'm still not sure I totally understand what the intent of the ""type"" vs. ""class"" system is in python, so I'm often a little unsure what to do with heavily typed code. That said, if expected behaviors could be encapsulated (both formally and intuitively) with some abstract types (representing interfaces or traits) that would be a nicer solution. I don't think we're near that point in python. ## Lattices. Sorry about not giving some info on lattices, I'd thought you didn't want to get into it. It's the [partially ordered set kind](https://en.wikipedia.org/wiki/Lattice_(order)) of lattice, where each type is an element or subset. I'll give a short python based example (ignoring that `Union[]` can't be instantiated). <details>. <summary>The code:</summary>. ```python; from typing import Any, Union. class A():; pass. class B(A):; pass. class C(A):; pass. class D():; pass. class E(D):; pass; ```. </details>. that defines a lattice, which can be represented as a DAG like this:. ```; Any; / \; A D; / \ |; B C E; \ | /; Union[]; ```. It's partially ordered in that you can't say A contains E or vice-versa, but you can say things like A is contains B, and `Any` is a supertype of (contains) everything else. I think that how you're viewing it is pretty close, except the elements are types instead of their properties. My mental model has types being a collection of properties, and being a subtype means an object inherits it's supertypes properti",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-444715545:204,polymorphi,polymorphism,204,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-444715545,2,['polymorphi'],['polymorphism']
Modifiability,"@flyingsheep I can assure you, that's the normal case in academic HPC; systems. On Tue, Mar 26, 2019 at 3:37 PM Philipp A. <notifications@github.com> wrote:. > the main hpc I'm on 1gb of space where appdirs would put these files; >; > That's a misconfigured server, not a normal case. We should use appdirs as; > default, catch a IOError on write, and send a nice message like; >; > Your cache directory is full. Please make sure there's space in; > {cache_dir} or override the cache directory by setting the; > $SCANPY_CACHE_DIR environment variable.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/558#issuecomment-476675808>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AAS-TQPrmr3LWdmwNL5O6XPnRdSAcl_1ks5vajC0gaJpZM4cKXC7>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-476677167:542,variab,variable,542,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-476677167,1,['variab'],['variable']
Modifiability,@giovp I want to make correlation plot between cell types and the continuous variables stored in .obs,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1845#issuecomment-848077091:77,variab,variables,77,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1845#issuecomment-848077091,1,['variab'],['variables']
Modifiability,"@gokceneraslan A few thoughts on this:. 1. In general, I don't like messing with global configurations for other packages.; 2. I generate many plots, but only a few are actually going to go to a manuscript. I'm not sure it's justified to increase all PDF sizes so that they're all editable. Maybe it should be opt-in? And do we need to mess with global settings to do this, or can we just make finding out about this easier for users?. What if there was an ""Plotting output options"" tutorial, which went over saving of figures? This could include a section on how to export for publication.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1720#issuecomment-792236673:88,config,configurations,88,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1720#issuecomment-792236673,1,['config'],['configurations']
Modifiability,"@gokceneraslan Hey, sorry for my long silence on this. I've been using @Hoohm's [https://github.com/Hoohm/CITE-seq-Count](CITE-seq-Count) for ADT/HTO tag counting which produces (in recent versions) a 10X v3 style `mtx` directory for both reads and UMIs. In some cases, I'll load these in as their own AnnData object with reads and counts as different `layers` which is helpful in computing per-cell or per-tag ""sequencing saturation"" and other metrics involving both reads and counts. This is especially helpful for investigating some pilot experiments (lipid tags, cholesterol tags, etc.) we've been doing. However, most of the time I'll just load the tags matrix in as a pandas dataframe and run them through a demuxing function that'll modify `adata.obs`. A couple challenges/ideas to consider:. * at our facility, we're typically building the same Illumina i7 index (`ATTACTCG`) into all tag libraries. This leads to some tricky situations when using a NovaSeq for sequencing since the multiple tag libraries (with disjoint sets of tags) may be run on the same sequencing flowcell lane. This results in a single set of FASTQ files and thus a single barcode-tag matrix for all tag libraries on that lane. Therefore, the mapping between transcriptome AnnData objects <-> tag library matrices is not always 1-to-1.; * in my experience, HTO libraries have a large variance in quality, so for the most part I've been using the transcriptome as my ""ground truth"" as to what is a cell. However, I imagine others use HTOs to ""rescue"" cells that were not called by their pipeline of choice (and I hope to do this once I build enough trust in the data). In that case, one would want to intersect the HTO classifications with the raw cell-gene matrix.; * not all tags are antibody based, so I'd vote for naming all related functions `*hashtags()`. I'd therefore vote for something like the following design:; ```{python}; # htos is a AnnData object; htos = sc.read_hashtags(filename) . # classify_hashtags a",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/351#issuecomment-543387900:353,layers,layers,353,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351#issuecomment-543387900,1,['layers'],['layers']
Modifiability,"@gokceneraslan I like the idea, that would save quite some space. However, as a partial color blind person, I tend to avoid legends based on color because I can not map them reliably back to the figure. But otherwise, I think it is easy to adapt the current code to add such feature.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/228#issuecomment-411039951:240,adapt,adapt,240,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/228#issuecomment-411039951,1,['adapt'],['adapt']
Modifiability,"@gokceneraslan If I follow you correctly, the idea would be to add a palette argument to heatmap. That indeed may solve the issue and should not be difficult to achieve. I will put it on my list of future enhancements.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/548#issuecomment-477997099:205,enhance,enhancements,205,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/548#issuecomment-477997099,1,['enhance'],['enhancements']
Modifiability,"@gokceneraslan Thanks for pointing this out! The `relative_luminance` function required for this can be easily imported from seaborn, therefore, I imagine that the code can be adapted easily.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1210#issuecomment-648000250:176,adapt,adapted,176,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1210#issuecomment-648000250,1,['adapt'],['adapted']
Modifiability,"@gokceneraslan Totally agree it's the user's responsibility. I would say that it's the devs responsibility to make it as easy as possible for the user. How about printing the absolute path of the data's destination on download?. @flying-sheep Would there necessarily be an error if space ran out? I could probably fit a few datasets in 2gb. From your previous depiction, I thought the older ones would just be deleted, right? If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. Also here's the [docs](https://opus.nci.org.au/display/Help/Filesystems+User+Guide#FilesystemsUserGuide-DiskQuotaPolicy) for my HPCs filesystem. I don't have an `XDG_CACHE_HOME` variable set when I log in. I'm also not sure scanpy fits the app model. When I look in my `~/Library/Caches/` I see things like Illustrator, VSCode, and Slack. When I think about example datasets that are available through scientific computing packages I think of:. * `scikit-learn` – `~/scikit_learn_data`; * `seaborn` – `~/seaborn-data`; * `NLTK` – `~/nltk_data`; * `keras` and `tensorflow` – `~/.keras/datasets`; * `conda` – `~/miniconda3/`; * `intake` – `~/.intake/cache/` (specifically for caching feature); * CRAN and bioconductor data packages – same place as packages I think",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-476943448:832,variab,variable,832,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-476943448,2,['variab'],['variable']
Modifiability,"@gokceneraslan, how about a tuple like `({obsm_key}, {obsm_value_column})`? It's more verbose, but I think more flexible, particularly if `obsm` starts to allow more kinds of values. Here's an example (using some stuff on personal branches):. <img width=""703"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/56583661-6c62a680-661d-11e9-8314-ca0b0bf8a309.png"">",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/613#issuecomment-485800110:112,flexible,flexible,112,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/613#issuecomment-485800110,1,['flexible'],['flexible']
Modifiability,"@gokceneraslan. > I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". As a point about this, I don't think `raw` completley solves this problem. There's two reasons for this:. ### Only a different set of variables. Raw only differs from the main object by variables. But we just as often want to remove observations (doublet detection for example). To account for this, I think it makes sense to just have two different anndata objects. ### absolutely everything. I don't think we really can expect to have everything. There are always going to be analyses that require going back to the BAM. If ""single file"" is the issue, we could definitely allow something like:. ```python; with h5py.File(""analysis.h5"") as f:; processed = ad.read_h5ad(f[""processed""]); raw = ad.read_h5ad(f[""raw""]); ```. -----------------------------. @LuckyMD . > Integration works better with HVGs typically. I'm thinking of the case where I have a few datasets saved as `h5ad` that I want to integrate. What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset?. I feel like it's helpful to have the all the measured genes present, so that when you do gather your datasets together you can select features from the full set. > > This does run into memory usage problems if want do a densifying transform on the data; > Don't understand this entirely... I was thinking about what happens if you do something like `sc.pp.scale`, where you don't have any 0s in your expression matrix anymore, so it has to be stored as a dense matrix. I believe this is why `raw` was even introduced originally, since the normalization workflow then was feature selection -> scale. It was wasteful to store the entire s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472:285,variab,variables,285,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472,4,['variab'],['variables']
Modifiability,"@ivirshup . I'd recommend these:; - `mypy` for type checking (I've caught a few bugs using it); - `autoflake` to remove unused variables/imports (by default removes only imports from Python's standard library); - `yesqa` to automatically remove unused `# noqa` for flake8; - `check-yaml` and `pretty-format-yaml` I also found useful; - `requirements-txt-fixer` to sort the reqs.; - `check-added-large-files` have prevented my from commiting some `.h5ad` files that shouldn't be there; - `rstcheck` to check the syntax of .rst files; - some flake8: `pytest-style` (enforces some pytest conventions, like non-returning fixtures beginning with `_`), `blind-except` (no blanket exceptions) and `comprehensions` (helps you rewrite comprehensions). Here's also an exhaustive list from which I picked the ones I use: https://pre-commit.com/hooks.html. As for any problems, some of them came from `rstcheck` as; `docs/source/classes.rst:9: (INFO/1) No directive entry for ""autoclass"" in module ""docutils.parsers.rst.languages.en"".`, that's why `.rstcheck.cfg` might be necessary. Also fixing types for `mypy` takes a while, I'd do it as last.; Also `flake8-comprehensions` forces you to abandon `dict(foo=..., bar=...)` calls, unless you disable it.; And `[tool.isort]` in `pyproject.toml` should have `profile = ""black""`. I had also planed to use `pylint` to enforce naming conventions, etc., but it has large overlap with `flake8` + the config file can be rather large, see https://gist.github.com/dkatz23238/08d79a76911ddaaa6171dff9cddd5772",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1563#issuecomment-754065509:127,variab,variables,127,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563#issuecomment-754065509,3,"['config', 'rewrite', 'variab']","['config', 'rewrite', 'variables']"
Modifiability,"@ivirshup ; Yeah, it was the same data as the privious plot. I tried calling sc.tl.umap(sp, init_pos=""paga"") but meet an error. I just use the get_init_pos_from_paga function to solve this error as mention in #769 .Thanks!; ```; TypingError: Failed in nopython mode pipeline (step: nopython frontend); Invalid use of type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)); Known signatures:; * (array(float32, 1d, A), array(float32, 1d, A)) -> float32; * parameterized; [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)); [2] During: typing of call at /datc/dh_data/.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py (797). File ""../../../../.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py"", line 797:; def optimize_layout(; <source elided>. dist_squared = rdist(current, other); ^. This is not usually a problem with Numba itself but instead often caused by; the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:; http://numba.pydata.org/numba-doc/latest/reference/pysupported.html; and; http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:; http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message; and traceback, along with a minimal reproducer at:; https://github.com/numba/numba/issues/new",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/918#issuecomment-555516223:519,parameteriz,parameterized,519,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918#issuecomment-555516223,1,['parameteriz'],['parameterized']
Modifiability,"@ivirshup @dkobak I've fixed up this PR, so now it implements what I mentioned in my comment above. I've left a couple of comments on the code, commenting on anything noteworthy. The tests and everything will fail until I release a new version of openTSNE, which I'll do in the coming days. But please look through the changes and let me know if there's anything you'd like me to change, so we can get this merged. Also, I haven't updated the docstrings at all. The most glaring thing is `neighbors_tsne`. Over 90% of the code here is identical to `neighbors`. Really, the only difference is that I changed the `n_neighbors` parameter to `perplexity`. But there was no elegant way to incorporate that into `neighbors`. I've also tried refactoring the duplicated code that saves the settings into `adata.uns`, but doingt that would also make the code pretty messy. Obviously, it's not a good idea to have duplicated code like this. What do you think would be the best way to handle this?. Functionally, this now works as agreed. Let me know how you want to proceed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-822033944:735,refactor,refactoring,735,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-822033944,1,['refactor'],['refactoring']
Modifiability,"@ivirshup @flying-sheep @falexwolf . I would like to merge this branch soon and I would like your opinion on the following:; * Extended functionality and fine tuning of the plots is achieved by using the new plot objects. However, I don't know how they can be documented in readthedocs (or if we want that). Note: the object methods are well documented and can be accessed, for example in Jupyter notebooks. * I am using `return_fig` as an argument to return the plot object in `sc.pl.dotplot` etc. Is this a good name choice?. As mentioned previously, the current PR tries to keep the current functionality with minimal changes to the way functions like `sc.pl.dotplot` are called. On the background, the code was refactored to remove much repetition as possible and allow new functionally. . The current progress on the visualization tutorial is here: https://nbviewer.jupyter.org/github/fidelram/scanpy-tutorials/blob/marker_genes_vis_tutorial/visualizing-marker-genes.ipynb",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1210#issuecomment-636014104:127,Extend,Extended,127,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1210#issuecomment-636014104,2,"['Extend', 'refactor']","['Extended', 'refactored']"
Modifiability,"@ivirshup @pavlin-policar I'd like to get back to this issue. I think maybe we should postpone discussing `ingest` until later (as well as ""recipes"" based on our Nat Comms paper, and other topics raised above) and focus on switching to openTSNE first. Scanpy's architecture is to compute kNN graph by calling `sc.pp.neighbors` and then run dimensionality reduction (UMAP) and clustering (Leiden) on this graph. I think this is very neat and makes everything consistent and other functions should follow this approach as much as possible. So IMHO if it's possible to run t-SNE on the kNN graph with k=15, then that's what we should do. And luckily it is possible! I can even see two approaches. (1) Either use k=15 kNN graph with the uniform similarity kernel. As I said, and as Pavlin knows, this yields result that is *very* similar to using perplexity=30. (2) Or use k=15 kNN graph with UMAP weights, normalize it as t-SNE expects it to be normalized and use that. My expectation is that it would yield very similar results, but I haven't actually tried it. Admittedly, this is not the ""vanilla"" t-SNE. But it's very close. And I think advantages outweigh the disadvantages. Actually this is quite a bit faster than the standard t-SNE, because it only uses k=15 instead of k=90 (3 times perplexity=30). Moreover, we could make the standard t-SNE available by extending `sc.pp.neighors` with `method=""tsne""` (there are several `method`s there already). What I mean is that . ```; sc.pp.neighors(); sc.tl.tsne(); ```; would use let's say uniform kernel on k=15 kNN neighbor graph (and maybe print a warning about it, but I am not even sure it's needed), while. ```; sc.pp.neighbors(method=""tsne"", perplexity=30); sc.tl.tsne(); ```; would construct k=90 weighted kNN graph as standard t-SNE does and then use that. Either way, `sc.tl.tsne()` runs openTSNE with the pre-defined affinity matrix. Thoughts?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1233#issuecomment-748543070:1361,extend,extending,1361,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1233#issuecomment-748543070,1,['extend'],['extending']
Modifiability,"@ivirshup Do you mind adding as part of the PR an extended description of the function? I don't think that everyone is familiar with `calculateQCMetrics` and thus, the output of this function is unclear.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/424#issuecomment-453959469:50,extend,extended,50,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/424#issuecomment-453959469,1,['extend'],['extended']
Modifiability,"@ivirshup I don't think so, unless there's work towards https://github.com/scverse/anndata/issues/244. To follow the ideas in https://github.com/scverse/anndata/issues/706, seems like the steps would be:. - [ ] add an attribute `._X_layer` to store which layer `.X` references;; - [ ] use `.X` to reference `.layers[._X_layer]`;; - [ ] add `in_layer=` and `out_layer=` arguments to scanpy's `.pp` functions;; - [ ] these functions will also alter `._X_layer`. The second to last point can actually be implemented irrespective of the AnnData change as `in_layer=None` will mean taking `.X`. ; The question is, should we consider changing the defaults right away, e.g. `in_layer=""counts"", out_layer=""lognorm""`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2261#issuecomment-1157056525:309,layers,layers,309,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2261#issuecomment-1157056525,1,['layers'],['layers']
Modifiability,"@ivirshup I hope that I caught all of your comments.; The flake8 configuration is now minimal and pretty much only contains black violations or what you requested. I added and then removed autopep8 again, because it has other opinions on formatting than the opinionated formatter black. Yes, even with the flake8 configuration file. Black formatted the code then autopep8 and this cycle continues forever.; Added TODOs to exceptions and noqas are still easily searchable and mention what rule they ignore anyways. I want to get this merged asap since the merge conflicts will just pile up.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1689#issuecomment-799425430:65,config,configuration,65,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689#issuecomment-799425430,2,['config'],['configuration']
Modifiability,@ivirshup The test failures are a bug exposed by the fixture refactoring. The tests were relying on `adata['uns']['pos']` being left over from a previous test run. Can you help me fix it?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2235#issuecomment-1099069308:61,refactor,refactoring,61,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2235#issuecomment-1099069308,1,['refactor'],['refactoring']
Modifiability,@ivirshup are you looking for default colors for boolean variables?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1646#issuecomment-1156714165:57,variab,variables,57,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646#issuecomment-1156714165,1,['variab'],['variables']
Modifiability,"@ivirshup coloring by boolean values `(True, False)` is now possible:. 1) The solution is based on casting the boolean columns to string columns, so that they can be colored in a categorical way. Actual columns in the anndata object are not modified. 2) I was thinking about the case where True is 1 and False is 0. Current behaviour: colorbar is plotted, since 0 and 1 are treated like continuous variables. Does it make sense to handle this case in scanpy?. 3) Tests fail due to new pandas version, they do not fail locally though. . What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2460#issuecomment-1500883671:398,variab,variables,398,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2460#issuecomment-1500883671,1,['variab'],['variables']
Modifiability,"@ivirshup this looks fine to me, but i just took the cell ranger version for the refactored `highly_variable_genes ` from the last version, so also not an expert. But again, this looks right.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/705#issuecomment-506889693:81,refactor,refactored,81,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/705#issuecomment-506889693,1,['refactor'],['refactored']
Modifiability,"@ivirshup, @JBreunig using the absolute counts isn't a problem per se. It's simply that the scvelo paper used the spliced counts in `adata.X` based on which the highly variable genes are selected and PCA, neighbor graph and UMAP embedding are calculated.; @JBreunig, shouldn't be a problem to put spliced into `adata.X` as the dimensions of spliced and total counts are the same.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1860#issuecomment-878300735:168,variab,variable,168,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1860#issuecomment-878300735,2,['variab'],['variable']
Modifiability,"@lazappi. > Users should have control over what type things are (unless this is required for computational reasons). I'm pretty sure I took your position back when this behaviour was added. The closest issue I could find was https://github.com/theislab/anndata/issues/115, but I remember having a longer discussion with @flying-sheep about this. However, I'm now pretty convinced that strings as categoricals is pretty necessary for computational reasons. There isn't a good ""array of strings"" in python, so all operations on those kinds are reaaaaally slow. Also, most of the time strings really are encoding a categorical variable. If they aren't, they should be unique (so we don't convert). > Functions shouldn't have side effects (i.e. I expect the highly_variable_genes() function to calculate the highly variable genes, not do that AND modify a bunch of unrelated columns in obs/var). I would generally agree with this. I don't like that `highly_variable_genes` will add mean and variance measures to the object. I guess I don't see converting strings to categoricals as being a big change, since most operations on them will have identical results. . I do see how this would cause problems with `R` since this isn't the case with `factors`. I wonder if there this could be solved in the converter? Maybe there is a better implementation of categorical values out there in the `R` ecosystem? Does `stringr` play nicer with factors?. -------------------------. @grst, that's bug, we definitely allow null values in categoricals, but it looks like they're being lost in conversion from strings.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1747#issuecomment-801601445:624,variab,variable,624,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747#issuecomment-801601445,2,['variab'],['variable']
Modifiability,"@michalk8 thanks for the extensive recommendations!. I think I'd like to keep the number of tools used small. It's the worst when you want to fix a bug, but instead have to learn about configuring a linter. More tools means more configurations people need to be familiar with, and the goal is reducing cognitive load. > Also fixing types for `mypy` takes a while, I'd do it as last. Yeah, I figured this would be the case. Does `mypy` allow partial typing these days? Also, I haven't found the numpy or pandas type stubs to always be great. Have you run into problems around this?. I think this would also need to wait at least until we can drop python 3.6 for `anndata`, since adding types there currently means circular dependencies. > `rstcheck` to check the syntax of .rst files. I would particularly like a linter for `rst`. I noticed you also had `doc8`, but you'd recommend `rstcheck` check over this? I'm a little worried, considering its last release was over a year ago. Spell check for prose in doc-strings could also be great, but I could see this being overzealous (is there a good way to notify about misspelled words, while not being annoying about technical terms?). I'm a little worried about some custom sphinx extensions we have, and conflicting with this, any experience here?. --------------------------------------------. @Koncopd, I think I agree with your concern, as I said above: it's the worst when you want to fix a bug, but instead have to learn about configuring a linter. I also think it's very easy to add new checks, so someone complaining about new ones is valuable. Per commit, this should always be an option with `git commit --no-verify`, though you could also just not install `pre-commit`. I would like to keep the required checks limited, ideally formatting tasks that can be automated as opposed ""this is poor style"" warnings. I also know these tools can be wrong (e.g. `black` when expression's have many operators, sometimes with chaining) so it would be goo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1563#issuecomment-754352635:185,config,configuring,185,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563#issuecomment-754352635,4,['config'],"['configurations', 'configuring']"
Modifiability,"@moqri bit difficult to answer this question. What are you referring to?. if you are referring to this: https://scanpy.readthedocs.io/en/stable/api/scanpy.pp.filter_genes_dispersion.html. then the docs are pretty clear: ; ```; The normalized dispersion is obtained by scaling with the mean and standard deviation of the dispersions for genes falling into a given bin for mean expression of genes. This means that for each bin of mean expression, highly variable genes are selected.; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1803#issuecomment-826766301:453,variab,variable,453,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1803#issuecomment-826766301,2,['variab'],['variable']
Modifiability,"@mxposed It may be worth noting that scanpy's sc.pp.highly_variable_genes takes an argument `flavor` which defaults to the original [2015 Seurat paper](https://www.nature.com/articles/nbt.3192). To Obtain the same set of Highly Variable Genes as produced by modern versions of Seurat [2019 Stuart et al. paper](https://www.sciencedirect.com/science/article/pii/S0092867419305598), it is necessary to pass 'seurat_v3' for this value. You will need to install scikit-misc for this method to work:; ```sh; pip install --user scikit-misc; ```; But there is another wrinkle... the seurat3 algorithm needs count data. therefore it is necessary to rearrange the normalization in scanpy:; ```py; # find the highly variable genes...; # Since we are using seurat_v3 as the flavor,; # we have to do this before normalization; sc.pp.highly_variable_genes(sc96, flavor='seurat_v3', ; n_top_genes=2000). # Normalize and log transform (over all genes); sc.pp.normalize_total(sc96, target_sum=1e4); sc.pp.log1p(sc96). # it is necessary to do the Normalization before selecting; # to just the highly variable genes else our normalization ; # for reads will only be counting the subset. # now select the subset; sc96 = sc96[:,sc96.var.highly_variable]; ```; With these steps scanpy selects the exact same set of HGV and the Normalized log1p data in scanpy `sc96.X` is equal to `sc96$RNA@data)[VariableFeatures(object=sc96),]` in Seurat to about 6 decimal places in my dataset. And thanks for sharing your notebook link, I am trying to perform a similar comparison.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1531#issuecomment-1079775692:228,Variab,Variable,228,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1531#issuecomment-1079775692,4,"['Variab', 'variab']","['Variable', 'VariableFeatures', 'variable']"
Modifiability,"@sjfleming I am currently facing the same issue. I was able to load the h5 output with this input function you stated here https://lightrun.com/answers/broadinstitute-cellbender-read_10x_h5-error-in-scanpy-191. But after further analysis and I wanted to save the adata object with the write function to h5ad format, I am not able to read that saved h5ad object with scanpy again with error ; **test.h5ad contains more than one genome. For legacy 10x h5 files you must specify the genome if more than one is present. Available genomes are: ['X', 'layers', 'obs', 'obsm', 'obsp', 'raw', 'uns', 'var', 'varm', 'varp']**. For some reason, the genome ""GRCh38"" is not showing up in the available genomes options. And when I tried to use command test = sc.read_10x_h5 ('test.h5ad', genome = ""GRCh38), this error shows up again ; **Could not find genome 'GRCh38' in 'test.h5ad'. Available genomes are: ['X', 'layers', 'obs', 'obsm', 'obsp', 'raw', 'uns', 'var', 'varm', 'varp'].** . Do you know if this error is related to this pull request? and is there any fix to it so that I can save processed h5ad after cellbender and able to read it again? Thank you very much!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2246#issuecomment-1247444051:546,layers,layers,546,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2246#issuecomment-1247444051,2,['layers'],['layers']
Modifiability,"@stefanpeidli's code gives this error. `ValueError: Cannot take a larger sample than population when 'replace=False'`. If a group has less than required number observations, it shouldn't subsample. ```python; target_cells = 1000; cluster_key = ""cell_type"". grouped = adata.obs.groupby(cluster_key); downsampled_indices = []. for _, group in grouped:; if len(group) > target_cells:; downsampled_indices.extend(group.sample(target_cells).index); else:; downsampled_indices.extend(group.index). adata_downsampled = adata[downsampled_indices]; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/987#issuecomment-1397060295:402,extend,extend,402,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/987#issuecomment-1397060295,2,['extend'],['extend']
Modifiability,"@wflynny `hashsolo` allows you to set a prior for your expected rate negatives, singlets, and doublets, which helps quite a bit with the issue you described despite modeling the log CMO counts as a normal distribution. Additionally, you can also add cell types if you've done cell-type annotation or even leiden clustering labels to help with cell type variability with CMO counts. This helped me quite a bit in kidney where NK cells had far fewer CMO counts than other cells despite being apparently live cells, e.g. good gene diversity and low mitochondrial gene percentage. @fidelram I'd be happy to add a visualization tool like you suggested if you have the code laying around.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/351#issuecomment-759575072:353,variab,variability,353,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351#issuecomment-759575072,1,['variab'],['variability']
Modifiability,"@xie186, are the variable names within each of your objects are unique?. I would guess that would be the problem. Here's a simple case that would throw this error:. ```python; import anndata as ad, numpy as np, pandas as pd. a = ad.AnnData(np.ones((3, 2)), var=pd.DataFrame(index=[""a"", ""a""])); b = ad.AnnData(np.ones((3, 3)), var=pd.DataFrame(index=[""a"", ""b"", ""c""])). a.concatenate(b); ```. I think our merge operation for the variables is only well defined if variable names are unique with each of the objects. I'm not sure there's a good default result here other than throwing an error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1409#issuecomment-694683604:17,variab,variable,17,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409#issuecomment-694683604,6,['variab'],"['variable', 'variables']"
Modifiability,"@zhangguy, adding on to some thoughts from your PR https://github.com/theislab/scanpy/pull/2055#issuecomment-987012001. From my reading of that PR, you added a boolean argument `groupby_expand` which, when `True`, assumed `group_by` had two values: a grouping variable for the rows of the plot and a grouping variable for the columns of the plot. It also assumed `var_names` was a single variable which would be used to fill cell in the plot. As an example:. ```python; pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(); pbmc.obs[""sampleid""] = np.repeat([""s1"", ""s2""], pbmc.n_obs / 2). sc.pl.dotplot(pbmc, var_names='LDHB', groupby=['louvain', 'sampleid'], groupby_expand=True); ```. ![tmpdm8256t1](https://user-images.githubusercontent.com/8238804/144899323-c439785d-5d57-4a18-b6e5-2b12412465f8.PNG). Instead of having an argument which changes the interpretation of the earlier arguments, I would prefer more orthogonal arguments. I think you'd be able to get an output close to what you would currently like with:. ```python; import scanpy as sc, pandas as pd, numpy as np. pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(); pbmc.obs[""sampleid""] = np.repeat([""s1"", ""s2""], pbmc.n_obs / 2); df = sc.get.obs_df(pbmc, [""LDHB"", ""louvain"", ""sampleid""]). summarized = df.pivot_table(; index=[""louvain"", ""sampleid""],; values=""LDHB"",; aggfunc=[np.mean, np.count_nonzero]; ); color_df = summarized[""mean""].unstack(); size_df = summarized[""count_nonzero""].unstack(). # I don't think the var_names or groupby variables are actually important here; sc.pl.DotPlot(; pbmc,; var_names=""LDHB"", groupby=[""louvain"", ""sampleid""], # Just here so it doesn't error; dot_color_df=color_df, dot_size_df=size_df,; ).style(cmap=""Reds"").show(); ```. I think this functionality could be more generic, and inspired by the `pd.pivot_table` function. This could end up looking like:. ```python; # Imaginary implementation:; sc.pl.heatmap(; pbmc,; var_names=""LDHB"",; row_groups=""louvain"",; col_groups=""sampleid""; ); ```. ![ima",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1876#issuecomment-987049315:260,variab,variable,260,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1876#issuecomment-987049315,3,['variab'],['variable']
Modifiability,"A few things!. 1. What version is UMAP and numba?. 2. Can you post your full workflow? How are you generating the AnnData objects in `adatalist`? The convergence scores in your SAM output are suspicious. They should be something like 0.4, 0.1, 0.001,... Usually I get convergence scores like yours when I have an issue with the preprocessing of the data.; ; 3. The tricky bit about applying SAM to batch-corrected data is that batch correction results in negative values in the data, whereas SAM requires non-negative data to calculate dispersion. In order to account for this, I use `adata.X` to calculate the manifold, and `adata.layers['X_disp']` to calculate the dispersions. The 'X_disp' layer should store the nonnegative expression values. If you want to run SAM on batch corrected data, make sure that you set `adata.layers['X_disp']` to be the original non-negative expression values. If you don't assign `adata.layers['X_disp']` yourself, then it is automatically assigned to be equal to `adata.X`. If `adata.X` has negative expression values due to batch correction, then that'll cause problems for SAM. Not sure if it's related to this specific bug, but it's good to know anyway.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1157#issuecomment-614700185:632,layers,layers,632,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1157#issuecomment-614700185,3,['layers'],['layers']
Modifiability,"About `use_raw` with `sc.get.var_df`: I didn't include this because the semantics differ significantly from `sc.get.obs_df`, as `raw` can have a different number of variables. I think it makes more sense for a user to call `sc.get.var_df(adata.raw, ...)`, since it's much more explicit that `adata.raw.var` and `adata.raw.varm` will be used.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1499#issuecomment-730348174:165,variab,variables,165,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1499#issuecomment-730348174,1,['variab'],['variables']
Modifiability,"About the commit process: That's far far too much work to do it like you suggested. I don't have the time for this. . About the rules: . 1. ""I don't like replacing `x == False` with `not x` in all cases. Sometimes a variable could be a container, and an error should be thrown. I think cases have to be evaluated for this."" . This should be covered by tests. In any case it is not good style and a violation. 2. ""Whats with changing from single letter variables inside expressions? Seems fine to me."". They are redefinitions of earlier variables and trip up flake8. We can call them whatever we want as long it s not `l` again. . 3. ""`lambda's also are generally fine."". See comment at the section. 4. ""Whats up with removing leading `#`s from comments?"" Not my choice either. What we have now is pep8 and flake8 compliant. If you're not happy with this we can ignore the rule. 5. ""So, some of the things you've adding a `# noqa` to look like bugs. I think we need to have a plan in place for doing something about these. Do you have any suggestions?"". The noqa ignore a rule for a specific line. I did not want to ""fix"" these things myself since Python is a dynamic language and you never know what happens :) Ideally we eventually get rid of all noqas, but not in this PR and not by me. I don't know the internals well enough to know whether this could have any side effects.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1689#issuecomment-785831068:216,variab,variable,216,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689#issuecomment-785831068,3,['variab'],"['variable', 'variables']"
Modifiability,Adapt name of UMAP coordinates according to used representation,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1403:0,Adapt,Adapt,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1403,1,['Adapt'],['Adapt']
Modifiability,Adapted the css to enforce the consistent styling of headings on each page: https://github.com/theislab/scanpy/commit/1f579f6f745d01c599a39f38abadb8a32f95c12b,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/610#issuecomment-484432495:0,Adapt,Adapted,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610#issuecomment-484432495,1,['Adapt'],['Adapted']
Modifiability,Add azure pipelines plugin,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2111:20,plugin,plugin,20,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2111,1,['plugin'],['plugin']
Modifiability,Added visualizations for variables ordered by observations. E.g. marker genes ordered by cluster,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/175:25,variab,variables,25,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/175,1,['variab'],['variables']
Modifiability,"Adding an expression atlas downloader to `sc.datasets` (proposed in #489). I've punted on replacing where datasets are downloaded by just making it a variable in settings, since it seems contentious where datasets should be downloaded by default #558. @flying-sheep when I build the docs locally, the link to the expression atlas doesn't format properly on the main `API` page, but does on it's own page. Any ideas on if we can get that to work?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/573:150,variab,variable,150,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573,1,['variab'],['variable']
Modifiability,Adding uns to a view fails if there is only one variable,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/323:48,variab,variable,48,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323,1,['variab'],['variable']
Modifiability,"After going through the comment, _Originally posted by @LuckyMD in https://github.com/theislab/scanpy/issues/220#issuecomment-408332060_ , I have opened this issue. I am facing problem with `sc.pl.highest_expr_genes(adata_orig, n_top=20)` as well. Attaching some details:; I started by reading in the file as:. `adata_orig = sc.read_h5ad('covid_portal_210320_with_raw.h5ad', backed = 'r')`. After printing the `adata_orig` object, i get the following output:. ```; AnnData object with n_obs × n_vars = 647366 × 24929 backed at 'covid_portal_210320_with_raw.h5ad'; obs: 'sample_id', 'n_genes', 'n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt', 'full_clustering', 'initial_clustering', 'Resample', 'Collection_Day', 'Sex', 'Age_interval', 'Swab_result', 'Status', 'Smoker', 'Status_on_day_collection', 'Status_on_day_collection_summary', 'Days_from_onset', 'Site', 'time_after_LPS', 'Worst_Clinical_Status', 'Outcome', 'patient_id'; var: 'feature_types'; uns: 'hvg', 'leiden', 'neighbors', 'pca', 'umap'; obsm: 'X_pca', 'X_pca_harmony', 'X_umap'; layers: 'raw'; ```. After this, when I tried running the command:; `sc.pl.highest_expr_genes(adata_orig, n_top=20, )`. I get the following output:; ```; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-22-c4ab6dadfa42> in <module>; ----> 1 sc.pl.highest_expr_genes(adata_orig ). C:\ProgramData\Anaconda3\lib\site-packages\scanpy\plotting\_qc.py in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds); 65 ; 66 # compute the percentage of each gene per cell; ---> 67 norm_dict = normalize_total(adata, target_sum=100, inplace=False); 68 ; 69 # identify the genes with the highest mean. C:\ProgramData\Anaconda3\lib\site-packages\scanpy\preprocessing\_normalization.py in normalize_total(adata, target_sum, exclude_highly_expressed, max_fraction, key_added, layer, layers, layer_norm, inplace, copy); 174 counts_per_cel",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2147:1068,layers,layers,1068,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2147,1,['layers'],['layers']
Modifiability,"After imputation you don't have your initial variability in your data. Ideally you have only the biologically relevant variability, but that's another question. Also, imputation methods take different data as input. Our DCA method takes count data, but MAGIC takes pre-processed data I think.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/653#issuecomment-494742140:45,variab,variability,45,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653#issuecomment-494742140,2,['variab'],['variability']
Modifiability,"After studying the PAGA tutorial, I tried to apply it to my data (40,000 cells, 8,000 highly variable genes). I ran the following commands:; ```python; sc.pp.neighbors(adata_hvg); sc.tl.louvain(adata_hvg); sc.tl.draw_graph(adata_hvg); ```; Till here, everything works nicely, but then I try to get the PAGA representation:. ```python; sc.tl.paga(adata_hvg, groups=""louvain""); ```. This returns the following error:. ```; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-249-7cc787ba28f9> in <module>; ----> 1 sc.tl.paga(adata_hvg, groups=""louvain""). ~/miniconda3/envs/wot/lib/python3.6/site-packages/scanpy/tools/_paga.py in paga(adata, groups, use_rna_velocity, model, copy); 93 adata.uns['paga'] = {}; 94 if not use_rna_velocity:; ---> 95 paga.compute_connectivities(); 96 adata.uns['paga']['connectivities'] = paga.connectivities; 97 adata.uns['paga']['connectivities_tree'] = paga.connectivities_tree. ~/miniconda3/envs/wot/lib/python3.6/site-packages/scanpy/tools/_paga.py in compute_connectivities(self); 127 def compute_connectivities(self):; 128 if self._model == 'v1.2':; --> 129 return self._compute_connectivities_v1_2(); 130 elif self._model == 'v1.0':; 131 return self._compute_connectivities_v1_0(). ~/miniconda3/envs/wot/lib/python3.6/site-packages/scanpy/tools/_paga.py in _compute_connectivities_v1_2(self); 161 if scaled_value > 1:; 162 scaled_value = 1; --> 163 connectivities[i, j] = scaled_value; 164 expected_n_edges[i, j] = expected_random_null; 165 # set attributes. ~/miniconda3/envs/wot/lib/python3.6/site-packages/scipy/sparse/_index.py in __setitem__(self, key, x); 67 if x.size != 1:; 68 raise ValueError('Trying to assign a sequence to an item'); ---> 69 self._set_intXint(row, col, x.flat[0]); 70 return; 71 . ~/miniconda3/envs/wot/lib/python3.6/site-packages/scipy/sparse/compressed.py in _set_intXint(self, row, col, x); 795 def _set_intXint(self, row, col, x):; 796 i, j = self._",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/695:93,variab,variable,93,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/695,1,['variab'],['variable']
Modifiability,"After the highly variable genes information was added to .var `pl.pca_loadings` no longer works. This is an example that reproduces the problem:; ```PYTHON; import scanpy.api as sc; import numpy as np; import pandas as pd. N = 1000; M = 2000. adata = sc.AnnData(; X=np.random.random_sample((N, M)); ). sc.pp.filter_genes_dispersion(adata, subset=False); sc.tl.pca(adata); sc.pl.pca_loadings(adata); ```. If ` subset=True` then the pca_loadings works as expected.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/315:17,variab,variable,17,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/315,1,['variab'],['variable']
Modifiability,"Agreed! . On another note, we currently lack a method for HVG selection that works on scaled/regressed out data. Could rewrite `regress_out` to not just output the residuals, but also add the intercept again.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/993#issuecomment-615137108:119,rewrite,rewrite,119,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/993#issuecomment-615137108,1,['rewrite'],['rewrite']
Modifiability,"Ah I think I see the issue! Feature branches should be based off `master` and directing the pull request there! I think what's happening is that a pre-commit hook was installed, but the config only exists on the `master` branch. I think this should largely be manageable by rebasing onto master (e.g. `git rebase --onto master 1.7.x`) and changing the branch the PR is targeting via the github interface:. <img width=""300"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/110570131-9093e600-81a9-11eb-9223-5b7bc233d75c.png"">. --------------. Side note: We're considering separating the `highly_variable_genes` interface into multiple functions, since the arguments to the different methods don't always overlap in meaningful or intuitive ways. There's nothing you need to do about this right now, but just a heads up to keep the logic for this method separate from the main function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-794790768:186,config,config,186,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-794790768,2,['config'],['config']
Modifiability,"Ah, it looks like `use_raw` was being set to True, even if `layers` was passed. . https://github.com/theislab/scanpy/blob/c748b3558b38e908f00b16b0c18e2846d3599e5c/scanpy/plotting/_tools/scatterplots.py#L74-L76. Any idea why this didn't trigger this test?. https://github.com/theislab/scanpy/blob/c748b3558b38e908f00b16b0c18e2846d3599e5c/scanpy/tests/test_plotting.py#L294-L298. Edit: I'm guessing vmin.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/730#issuecomment-510467988:60,layers,layers,60,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730#issuecomment-510467988,1,['layers'],['layers']
Modifiability,"Ahh, see what you mean though. Think I'd mis-interpreted the [docs](https://scanpy.readthedocs.io/en/stable/generated/scanpy.pp.normalize_total.html), where talks about normalising layers:. ```; Layer to normalize instead of X. If None, X is normalized.; ```. Implies that .X is the only thing that gets normalised, if you don't specify layers. . **Edit:** . Actually this is just due to references I think:. ```; # Done as per current code; >>> ad = sc.read(""read.h5ad""); >>> ad[1:5,1:5].X.todense(); matrix([[1., 4., 0., 0.],; [0., 0., 0., 2.],; [0., 1., 0., 0.],; [0., 0., 0., 0.]], dtype=float32); >>> ad.layers['raw'] = ad.X; >>> sc.pp.normalize_total(ad); >>> ad[1:5,1:5].X.todense(); matrix([[0.07691469, 0.30765876, 0. , 0. ],; [0. , 0. , 0. , 0.16940302],; [0. , 0.10096962, 0. , 0. ],; [0. , 0. , 0. , 0. ]], dtype=float32); >>> ad[1:5,1:5].layers['raw'].todense(); matrix([[0.07691469, 0.30765876, 0. , 0. ],; [0. , 0. , 0. , 0.16940302],; [0. , 0.10096962, 0. , 0. ],; [0. , 0. , 0. , 0. ]], dtype=float32); ; # If we actually copy the matrix; >>> ad = sc.read(""read.h5ad""); >>> ad.layers['raw'] = ad.X.copy()); >>> sc.pp.normalize_total(ad); >>> ad[1:5,1:5].X.todense(); matrix([[0.07691469, 0.30765876, 0. , 0. ],; [0. , 0. , 0. , 0.16940302],; [0. , 0.10096962, 0. , 0. ],; [0. , 0. , 0. , 0. ]], dtype=float32); >>> ad[1:5,1:5].layers['raw'].todense(); matrix([[1., 4., 0., 0.],; [0., 0., 0., 2.],; [0., 1., 0., 0.],; [0., 0., 0., 0.]], dtype=float32); ```. Shall I add the fix as part of what I'm doing in https://github.com/theislab/scanpy/pull/1965 @ivirshup ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1957#issuecomment-889156596:181,layers,layers,181,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1957#issuecomment-889156596,6,['layers'],['layers']
Modifiability,"All of this is correct, I think you didn’t take into account that shuffling `var` means shuffling `var.index` and therefore renaming the variables. You should do this instead:. ```py; rna_ann = rna_ann[:, (-rna_ann.var['nCount']).argsort()]; ```. it reorders both `.X` and `.var`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3106#issuecomment-2426582045:137,variab,variables,137,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3106#issuecomment-2426582045,1,['variab'],['variables']
Modifiability,"All right, fair points. > Poetry is great! But i remember two problems:; > ; > no good way to editably install into some env: python-poetry/poetry#34; > doesn’t support plugins yet so only hardcoded versions in static metadata: python-poetry/poetry#140; > . I also stumbled upon the editably install issue. This is not an issue that Poetry can solve at the moment as explained in the thread. I do however understand that this is an issue for scanpy (considering the strong anndata dependency etc). Regarding plugins - they are on the roadmap and should appear at some point. Considering that the community is very active whereas the main developer is not anymore this may solve the editable install with some hack as well. I agree with your points and Poetry is not yet the solution that we should currently use, but I think it is the proper solution that we should aim for. > I'd also be worried using poetry would hamper contributions from people unfamiliar with it, and I don't think bioinformaticians are going to be familiar with it. I don't think that anybody is familiar with flit either, but it is slightly less intrusive and does not fundamentally change so many things like Poetry does. However, many things that Poetry does change make a lot of sense and solve other issues that we did not discuss here yet. So yeah, I personally would wait for Poetry to get it's plugin system and for the editable install issue to get a proper PEP. But if you don't feel like waiting Flit might be fun :). Cheers",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1527#issuecomment-765253434:169,plugin,plugins,169,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-765253434,3,['plugin'],"['plugin', 'plugins']"
Modifiability,Already fixed in 6c3e92924ea09ef288e422b283c6e03410d64a0b. > This may result in passing an empty `adj_tree` to the `_compute_pos()` function. empty? you mean an unset variable.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/483#issuecomment-463602100:167,variab,variable,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/483#issuecomment-463602100,1,['variab'],['variable']
Modifiability,"Alright, if you would like to achieve reproducibility the next things to play around with would probably be [these CPU feature flag variables](https://numba.pydata.org/numba-doc/dev/reference/envvars.html#compilation-options) for numba. In particular: `NUMBA_CPU_NAME=generic`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2014#issuecomment-946714555:132,variab,variables,132,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2014#issuecomment-946714555,1,['variab'],['variables']
Modifiability,"Also `groups_names` was undefined in the same function, I changed it to `groups_order` but I'm not sure if it's the correct variable.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/93#issuecomment-367725475:124,variab,variable,124,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/93#issuecomment-367725475,1,['variab'],['variable']
Modifiability,Also move the `MPLBACKEND=agg` to variable definitions. Threading layer docs: https://numba.pydata.org/numba-doc/latest/user/threading-layer.html,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1933:34,variab,variable,34,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1933,1,['variab'],['variable']
Modifiability,"Also two api thoughts:. For `sc.metrics.gearys_c(a: ""array"", b: ""array"")`, where `b` is 2d is expected to have a shape like: `(variable, number_of_cells)` – the ufunc shape signature would be: `(m,m)(n,m)->(n,)`. This is because it needs fast access to each variable, so they correspond to rows. Also the length of the returned array depends on the first axis of the passed input. Is this intuitive, or should the input be transposed?. Second, for `confusion_matrix`, I'm thinking I should make it singly dispatched on the first argument. This way if a dataframe is passed, the next two arguments could correspond to keys in that dataframe. Otherwise, vectors can be passed directly. Under that, these calls would be equivalent:. ```python; sc.metrics.confusion_matrix(adata.obs, ""sample_labels"", ""leiden""); sc.metrics.confusion_matrix(adata.obs[""sample_labels""], adata.obs[""leiden""]); ```. Right now it has the seaborn style argument handling shown at the top of this PR. I'm not sure that's really caught on in other packages or fits with scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/915#issuecomment-559928610:127,variab,variable,127,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-559928610,4,['variab'],['variable']
Modifiability,"Also, I just found time to read the links you posted @davidsebfischer. Shouldn't we always use a welch t-test instead of a t-test in marker gene detection according to your second stackexchange link? They state that If you don't have a good reason to assume equal variances in the groups, then use the Welch correction... if we have a `group` vs `rest` type of setup as we do in `rank_genes_groups()` at the moment, then we would definitely not expect a single cluster to have an equal variance to the combination of all other cells in other clusters. I think the default is currently `t-test-overestimate-var`... being oblivious to exactly how that works, might it not be better to adapt that to a `welch-t-test-overestimate-var` or something like that @falexwolf?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/397#issuecomment-449358857:683,adapt,adapt,683,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397#issuecomment-449358857,1,['adapt'],['adapt']
Modifiability,"An example with real data:. ![counts](https://user-images.githubusercontent.com/20436557/89998524-f957c600-dc8d-11ea-9036-a5d165d6bad5.png). Code:; ```py; # Load the PBMC 3k data; adata = sc.read_10x_mtx(; os.path.join(; save_path, ""filtered_gene_bc_matrices/hg19/""; ), # the directory with the `.mtx` file; var_names=""gene_symbols"", # use gene symbols for the variable names (variables-axis index); ); adata.var_names_make_unique(). # Get counts; adata.obs[""n_counts""] = adata.X.sum(axis=1).A1; sc.pp.normalize_per_cell(adata, counts_per_cell_after=1e4); adata.obs[""n_counts_normalized""] = adata.X.sum(axis=1).A1; sc.pp.log1p(adata); adata.obs[""n_counts_normalized_log""] = adata.X.sum(axis=1).A1. # Dim reduction; sc.tl.pca(adata, svd_solver=""arpack""); sc.pp.neighbors(adata); sc.tl.umap(adata); sc.pl.umap(adata, color=[""n_counts"", ""n_counts_normalized"", ""n_counts_normalized_log""]); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1364#issuecomment-672762269:361,variab,variable,361,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364#issuecomment-672762269,2,['variab'],"['variable', 'variables-axis']"
Modifiability,"An implementation of highly deviant gene identification from the 2019 GLMPCA paper. I'm rather fond of the method, as it's a straightforward statistical measure, and comes with significance testing as a form of data-driven cutoff. I put it in a new `highly_deviant_genes()` function, as:; - it comes with a number of unique parameters, and there's only so many different algorithms `highly_variable_genes()` can house; - the paper argues that highly deviant is different from highly variable. I acknowledge that there are no tests, I'm hoping to get some assistance with that if possible.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1765:483,variab,variable,483,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1765,1,['variab'],['variable']
Modifiability,"And in terms of the `sc.pp.highly_variable_genes` function. We typically don't use the `max_mean` and `disperson` based parametrization anymore, but instead just select `n_top_genes`, which avoids this problem altogether. That being said, there is a PR with the VST-based highly-variable genes implementation from Seurat that will be added into scanpy soon. If you would like to reproduce an updated pbmc3k tutorial from Seurat using scanpy functions, that would be very welcome of course!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1338#issuecomment-665746348:279,variab,variable,279,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1338#issuecomment-665746348,1,['variab'],['variable']
Modifiability,"And, in which step should I execute MNN batch effect correction ? Is it still necessary to regress out some variables ( n_counts, percent_mito et al.,) when I execute MNN ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/41#issuecomment-395337961:108,variab,variables,108,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/41#issuecomment-395337961,1,['variab'],['variables']
Modifiability,"Another option is to set fewer components to use in sc.tl.pca, option n_comps should be set to at most number of variable genes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/432#issuecomment-499561060:113,variab,variable,113,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/432#issuecomment-499561060,1,['variab'],['variable']
Modifiability,Apologies for the bug but this didn't seem like an enhancement and strikes me as something close to a bug.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2839#issuecomment-1917273580:51,enhance,enhancement,51,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2839#issuecomment-1917273580,1,['enhance'],['enhancement']
Modifiability,"Apologies for the late response @hawaiiki! I married and moved to the US with twin babies last week. And in between, I spilled something over my laptop... Yes, unfortunately, there were two half-cooked anndata releases out there. 😒 All these issues are fixed on GitHub and in anndata 0.6.10. anndata is now able to fully handle loom's layers, which it wasn't before and hence gained quite some additional functionality, thanks to @Koncopd and @VolkerBergen.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/247#issuecomment-418059347:335,layers,layers,335,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/247#issuecomment-418059347,1,['layers'],['layers']
Modifiability,"Are celltypes really continuous? How does this variable look like?; for continuous you can do; `from scipy.stats import pearsonr`; `r, _ = pearsonr(adata.obs[""celltypes""], adata.obs[""age""])`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1845#issuecomment-849646102:47,variab,variable,47,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1845#issuecomment-849646102,1,['variab'],['variable']
Modifiability,"Are the variable names in your anndata object unique? If not, there could be an issue with ambiguous gene names being used (e.g. two genes are given the same name, so we can't tell which column to use). If this is the issue, making the variable names unique with `adata.var_names_make_unique()` should solve the issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1862#issuecomment-861376891:8,variab,variable,8,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1862#issuecomment-861376891,2,['variab'],['variable']
Modifiability,Are these layers for the velocyto implementation? I.e. spliced and unspliced count data layers. And is this then also usable for different layers of data processing?. Or am I missing the point entirely here?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/236#issuecomment-414600952:10,layers,layers,10,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/236#issuecomment-414600952,6,['layers'],['layers']
Modifiability,"Are you sure that the genes are in `adata.var_names` in the gene symbol format that you are using to subset the object? In other words, is `'Ada' in adata.var_names` `True`? I'd just like to check whether you don't have e.g., Ensembl IDs as your variable names by chance. Regarding normalization... there are other normalization methods. I believe a method was recently added to scanpy to use only a particular fraction of genes to calculate size factors (avoiding genes that make up >5% of the total counts). Otherwise, we have recently compiled a best practices pipeline in the group, which uses Scran's pooling strategy to normalize the data. This is implemented in R, but can easily be used in a python-based workflow via [`anndata2ri`](www.github.com/flying-sheep/anndata2ri). A case study using the best practices (with scran and anndata2ri) is available [here](www.github.com/theislab/single-cell-tutorial).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/510#issuecomment-488011785:246,variab,variable,246,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510#issuecomment-488011785,1,['variab'],['variable']
Modifiability,"As a general approach to this kind of problem, I write functions like this:. ```python. def grouped_obs_mean(adata, group_key, layer=None, gene_symbols=None):; if layer is not None:; getX = lambda x: x.layers[layer]; else:; getX = lambda x: x.X; if gene_symbols is not None:; new_idx = adata.var[idx]; else:; new_idx = adata.var_names. grouped = adata.obs.groupby(group_key); out = pd.DataFrame(; np.zeros((adata.shape[1], len(grouped)), dtype=np.float64),; columns=list(grouped.groups.keys()),; index=adata.var_names; ). for group, idx in grouped.indices.items():; X = getX(adata[idx]); out[group] = np.ravel(X.mean(axis=0, dtype=np.float64)); return out; ```. Swapping out the last 8 lines or so depending on what I'm calculating. To use a set of marker genes I'd call it as `grouped_obs_mean(adata[:, marker_genes], ...)`. At some point we might have `groupby` for `AnnData`s, but that'll require figuring out how to be consistent about the returned type.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/181#issuecomment-534867254:202,layers,layers,202,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/181#issuecomment-534867254,1,['layers'],['layers']
Modifiability,"As an update, I've been using this helper function to consistently handle this:. ```python. def _choose_obs_rep(adata, *, use_raw=False, layer=None, obsm=None, obsp=None):; """"""; Choose array aligned with obs annotation.; """"""; is_layer = layer is not None; is_raw = use_raw is not False; is_obsm = obsm is not None; is_obsp = obsp is not None; choices_made = sum((is_layer, is_raw, is_obsm, is_obsp)); assert choices_made <= 1; if choices_made == 0:; return adata.X; elif is_layer:; return adata.layers[layer]; elif use_raw:; return adata.raw.X; elif is_obsm:; return adata.obsm[obsm]; elif is_obsp:; return adata.obsp[obsp]; else:; assert False, (; ""That was unexpected. Please report this bug at:\n\n\t""; "" https://github.com/theislab/scanpy/issues""; ); ```. This could use support for variable masks like `use_highly_variable`. Also the error message should be better. I think a collection of helper functions like this should go in to a utils module (`sc.utils.argutils`?) which could be public so it's easier to use in `scanpy`-like packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/828#issuecomment-560072919:495,layers,layers,495,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/828#issuecomment-560072919,2,"['layers', 'variab']","['layers', 'variable']"
Modifiability,"As discussed in issue #313, score_genes function returns different values on various machines. This is due to using float32 dtype in the `np.nanmean` calls. This PR fixes this behaviour by changing the dtype to float64 in the relevant sections of code ie. functions `gene_score()` and `_sparse_nanmean`. Following the suggestion of @ivirshup the returned value is now also float64. I also adapted the tests `test_add_score` and `test_npnanmean_vs_sparsemean` to use and expect float64.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1890:389,adapt,adapted,389,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1890,1,['adapt'],['adapted']
Modifiability,"As explained in plotting docs; > The color map can also be set individually for each value in adata.obs and adata.var, by setting `adata.uns[""{var}_cmap""]`. The individual values overwrite `color_map`. I think it's very useful when plotting multiple .obs or .var variables using ""color"", as the cmaps can then be defined for each embedding individually.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1489:263,variab,variables,263,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1489,1,['variab'],['variables']
Modifiability,"As for `randomized_svd`, looking at [this](https://github.com/scikit-learn/scikit-learn/blob/7fe3413475bf50683f821d296c2ca6cb525a7714/sklearn/utils/extmath.py#L120) it seems that is should work properly if a class for lazy evaluation inherits from standard sparse class and implements \_\_mul\_\_ and \_\_rmul\_\_.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/393#issuecomment-446377673:234,inherit,inherits,234,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393#issuecomment-446377673,2,['inherit'],['inherits']
Modifiability,"As this issue is not closed I'll add a question here. . Is it possible, or if not could it be added, that the cells in the heatmaps are sorted within the groupby variables. Either by pseudotime if availible or just clustered simply by hierarchical clustering. This could add a more visually and intuitive pleasing ordering of cells. For example as in my figure above the groupby miss some property of the data with pattern over cells. If the cell ordering was random or default this pattern could not be seen.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/349#issuecomment-460428844:162,variab,variables,162,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/349#issuecomment-460428844,2,['variab'],['variables']
Modifiability,"As we are refactoring scvi, I'm wondering what the utility would be to spin off the i/o part of scanpy into it's own lightweight package that's more general for reading single cell pipeline outputs into anndata. For example, we'd like to use the scanpy read from 10x/visium functions, but don't necessarily want to have scanpy be a full requirement. It's also a bit confusing why something like `read_umi_tools` is in anndata but not `read_10x_h5`. The same goes for loom to some extent. I could imagine either moving such functionality to anndata or a standalone package that could be expanded to include support for other technologies like scATAC-seq. . This overall could be a big benefit to methods developers who would like to build on anndata.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1387:10,refactor,refactoring,10,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387,1,['refactor'],['refactoring']
Modifiability,"As we don't have any extensions anymore we can close this for now. Also, many people start adapting numba. In the context of Scanpy this should usually be enough... let's talk again in case we need it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/19#issuecomment-380415001:91,adapt,adapting,91,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/19#issuecomment-380415001,1,['adapt'],['adapting']
Modifiability,"Assuming you're on a debian based linux, please check the following:; - `echo $PATH` shows your PATH variable.; - `which git` shows you the location of your git installation. If nothing is shown, you need to install it.; - `apt install git` if you haven't installed it yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1257#issuecomment-636457516:101,variab,variable,101,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1257#issuecomment-636457516,1,['variab'],['variable']
Modifiability,"Awesome, thanks for the suggestion to look in the .uns variable! Just to confirm, are the available palettes options that can be used with the palette keyword in the call to the sc.pl.tsne() function listed in the palettes.py file? For example things like vega_20_scanpy, zeileis_26, and godsnot_64?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/156#issuecomment-390299178:55,variab,variable,55,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/156#issuecomment-390299178,1,['variab'],['variable']
Modifiability,"Back on the topic of getting dot plots a bit more flexible, I've been working on an approach that could work. You can check it out in [this binder environment](https://mybinder.org/v2/gh/ivirshup/scanpy-interactive/master?filepath=notebooks%2Fflexible_de.ipynb), but it's based on two main ideas:. 1. It'd be nice if there were an easy way to get aggregated values for groups, so I've added a crude `groupby` to `AnnData`; 2. Our differential expression results are like a 3d array, with axes `[""genes"", ""group"", ""values""]` where values are things like p-values and mean expression. Here's a quick example of the output:. ![image](https://user-images.githubusercontent.com/8238804/56495993-0dc4fc00-653b-11e9-8831-a830b2ead841.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/562#issuecomment-485384100:50,flexible,flexible,50,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562#issuecomment-485384100,1,['flexible'],['flexible']
Modifiability,Backport PR #1789 on branch 1.7.x (Fix malformed flake8 config file),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1790:56,config,config,56,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1790,1,['config'],['config']
Modifiability,Backport PR #1789: Fix malformed flake8 config file,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1790:40,config,config,40,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1790,1,['config'],['config']
Modifiability,"Backport PR #2027 on branch 1.8.x (Fix finding variables with sc.pl.scatter(..., use_raw=True))",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2049:47,variab,variables,47,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2049,1,['variab'],['variables']
Modifiability,"Backport PR #2027: Fix finding variables with sc.pl.scatter(..., use_raw=True)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2049:31,variab,variables,31,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2049,1,['variab'],['variables']
Modifiability,Backport PR #3031 on branch 1.10.x (Extend benchmarks from basic tutorial),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3056:36,Extend,Extend,36,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3056,1,['Extend'],['Extend']
Modifiability,Backport PR #3031: Extend benchmarks from basic tutorial,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3056:19,Extend,Extend,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3056,1,['Extend'],['Extend']
Modifiability,Backport PR #3170 on branch 1.10.x (Refactor score_genes),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3171:36,Refactor,Refactor,36,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3171,1,['Refactor'],['Refactor']
Modifiability,Backport PR #3182 on branch 1.10.x (Some refactoring ahead of key_added),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3185:41,refactor,refactoring,41,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3185,1,['refactor'],['refactoring']
Modifiability,Backport PR #3182: Some refactoring ahead of key_added,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3185:24,refactor,refactoring,24,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3185,1,['refactor'],['refactoring']
Modifiability,Backport PR #3316 on branch 1.10.x (Refactor regress_out),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3322:36,Refactor,Refactor,36,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3322,1,['Refactor'],['Refactor']
Modifiability,Backport PR #3316: Refactor regress_out,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3322:19,Refactor,Refactor,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3322,1,['Refactor'],['Refactor']
Modifiability,But the error comes from your variable names being tuples. The following fixes it.; ```; adata.var_names = [i[0] for i in adata.var_names]; ```. Let me think where it would be best to output a warning. I'm not quite sure where else it would lead to collisions. Do you need tuples in the index?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/365#issuecomment-440431843:30,variab,variable,30,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/365#issuecomment-440431843,1,['variab'],['variable']
Modifiability,"CCA does not have code in python, which will make it difficult to integrate, pySCENIC is probably easier but I would rather ask the developers. @falexwolf We should consider a way to facilitate scanpy 'plugins'. A quick search shows me that this could be possible: https://packaging.python.org/guides/creating-and-discovering-plugins/ but honestly I don't know how it works. Nevertheless, given the number of tools that continue to appear we should consider a scheme that facilitate how developers can take advantage of scanpy preprocessing, storing, analysis and visualization tools.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/265#issuecomment-423514211:202,plugin,plugins,202,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-423514211,4,['plugin'],['plugins']
Modifiability,CPUDispatcher error with highly variable genes,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1995:32,variab,variable,32,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1995,1,['variab'],['variable']
Modifiability,"CT_sub2 = sc.read('C:/Users/Park_Lab/Documents/ACT_sub2.h5ad') # Scanpy proceeded data; ACT_sub2; AnnData object with n_obs × n_vars = 2636 × 5000; obs: 'leiden', 'n_genes', 'n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt', 'total_counts_rpl', 'pct_counts_rpl', 'total_counts_rps', 'pct_counts_rps'; var: 'Accession', 'Chromosome', 'End', 'Start', 'Strand', 'n_cells', 'mt', 'rpl', 'rps', 'n_cells_by_counts', 'mean_counts', 'pct_dropout_by_counts', 'total_counts', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'mean', 'std'; uns: 'hvg', 'leiden', 'leiden_colors', 'neighbors', 'pca', 'rank_genes_groups', 'umap'; obsm: 'X_pca', 'X_umap'; varm: 'PCs'; layers: 'ambiguous', 'matrix', 'spliced', 'unspliced'; obsp: 'connectivities', 'distances'. adata = sc.read_loom(filename='C:/Users/Park_Lab/Documents/cellsorted_Apc_Cracd_Tumor_KPVDV.loom') # raw data; adata.var_names_make_unique(); adata; AnnData object with n_obs × n_vars = 13499 × 32285; var: 'Accession', 'Chromosome', 'End', 'Start', 'Strand'; layers: 'matrix', 'ambiguous', 'spliced', 'unspliced'. adata.var['highly_variable']=ACT_sub2.var['highly_variable']; adata = adata[:, adata.var.highly_variable] # subset the raw data by ACT_sub2's highly variable genes. KeyError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_9544/4098982234.py in <module>; ----> 1 adata = adata[:, adata.var.highly_variable]; 2 adata. ~\anaconda3\envs\Python3812\lib\site-packages\anndata\_core\anndata.py in __getitem__(self, index); 1114 def __getitem__(self, index: Index) -> ""AnnData"":; 1115 """"""Returns a sliced view of the object.""""""; -> 1116 oidx, vidx = self._normalize_indices(index); 1117 return AnnData(self, oidx=oidx, vidx=vidx, asview=True); 1118 . ~\anaconda3\envs\Python3812\lib\site-packages\anndata\_core\anndata.py in _normalize_indices(self, index); 1095 ; 1096 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:; -> 1097 return _normalize_indices(index, se",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2095:1831,layers,layers,1831,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2095,1,['layers'],['layers']
Modifiability,Can you extend scanpy functions so that I can show gene expression level on plot generated by sc.pl.diffmap? just like that monocle2 does.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/41#issuecomment-395336870:8,extend,extend,8,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/41#issuecomment-395336870,1,['extend'],['extend']
Modifiability,"Can you extend scanpy functions so that I can show gene expression level on plot generated by sc.pl.diffmap? just like that monocle2 does. And, in which step should I execute MNN batch effect correction ? Is it still necessary to regress out some variables ( n_counts, percent_mito, cell cycle et al.,) when I execute MNN ?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/168:8,extend,extend,8,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168,2,"['extend', 'variab']","['extend', 'variables']"
Modifiability,"Check out the PR. I’m removing the GitHub actions workflow, not pre-commit’s config file. I’m talking about having two checks that do the same thing, so let’s keep the faster one:. <img width=""830"" alt=""image"" src=""https://user-images.githubusercontent.com/291575/161737449-5d10a522-9c6d-41fc-bd55-5a88518f0aee.png"">",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2206#issuecomment-1088547896:77,config,config,77,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2206#issuecomment-1088547896,1,['config'],['config']
Modifiability,"Checking locally, this PR causes to docs to be fully rebuilt with no other changes. I think the issue is that `typehints_formatter` is being changed after config init, not `typehints_default`. ```; updating environment: [config changed ('typehints_formatter')] 317 added, 0 changed, 0 removed; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2204#issuecomment-1087568170:155,config,config,155,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2204#issuecomment-1087568170,2,['config'],['config']
Modifiability,Coloring by boolean variables,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2460:20,variab,variables,20,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2460,1,['variab'],['variables']
Modifiability,Computes a hierarchical clustering for the variables(genes),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2433:43,variab,variables,43,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2433,1,['variab'],['variables']
Modifiability,"Conda installation fails silently with no error. Installation command:; ```; conda install -c bioconda scanpy; ```. Output:; ```; Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: / ; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Output in format: Requested package -> Available versions; ```. Increasing the verbosity did not help. Using older python version did not helpeither.. It looks like the metadata are not correct but I am not able to validate this. I tried miniconda anaconda clean installs and I had no luck whatsoever. Pip install works fine.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1298:258,flexible,flexible,258,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1298,2,['flexible'],['flexible']
Modifiability,"Cool! . > * Mask out genes which aren't expressed in the compared groups (since there's not too much point in getting and correcting a pvalue for them). I think masking out might be problematic because, `n_genes=adata.n_vars` should return all genes in any case. . > * Revert change (would bring back issue of genes with variance of 0). I feel like using scipy function will slightly increase the maintainability (and simplicity) of the code, so I'm fine with keeping the scipy switch. > * Wrap the t-test with something like `np.errstate` to hide the warning. This sounds good. Replacing weird scipy warning with a proper scanpy warning would also make sense.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/629#issuecomment-489105754:397,maintainab,maintainability,397,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/629#issuecomment-489105754,2,['maintainab'],['maintainability']
Modifiability,"Cool, @Koncopd! I'll rewrite a recipe for this with the new functions, which should be a lot more memory effective.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/511#issuecomment-470859467:21,rewrite,rewrite,21,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/511#issuecomment-470859467,1,['rewrite'],['rewrite']
Modifiability,"Cool, very interesting! :smile: Greatest advantage at first sight for me: `scanpy.api.AnnData` is now `anndata.AnnData`. Also, you don't seem to have to mingle around with autodoc anymore, which seems a good thing... The simpler docstrings are also nice... but it's going to be a lot of work to rewrite all the docstrings... also, there might be some danger of introducing bugs as one needs to rewrite the function headers. Hm, ... I'm a bit hesitant to just do this right away... Let's discuss! :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/119#issuecomment-379778109:295,rewrite,rewrite,295,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/119#issuecomment-379778109,4,['rewrite'],['rewrite']
Modifiability,Could you add Moran's I calculation to Scanpy? It could be used in scIB and to also find variable genes across embedding (could be an alternative to SEMITONES that takes a while to be computed).,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1698:89,variab,variable,89,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698,1,['variab'],['variable']
Modifiability,"Currently it’s almost a MVP. What do you people think (especially @mbuttner)? Possible improvements:. - [x] **Heuristic for neighborhood size**; - [x] Expose more data: E.g. a column in `.obs` with the corrected p values; - [ ] **Better docs**; - [ ] **groupby**; - [ ] Mean of multiple samples instead of whole data; - [ ] Adapt to cells that appear in no neighborhoods; - [ ] Speedups?. I guess at least the heuristic and the docs have to go in!. I also don’t know if relying on the previous `neighbors` call is a good idea. Its default is 15 Neighbors, and even for a small (200 cells) dataset, the number the heuristic determined was 75.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/364:324,Adapt,Adapt,324,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/364,1,['Adapt'],['Adapt']
Modifiability,"Currently, test collection takes about 11 seconds. This seemed a little long so I played around with the config a bit, and found if all the test files names are prepended with `test_`, and I set `python_files = test_*.py`, collection takes about 1 second. Mind if I make that change?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/326:105,config,config,105,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/326,1,['config'],['config']
Modifiability,De layers,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/801:3,layers,layers,3,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/801,1,['layers'],['layers']
Modifiability,"Dear author,. Can bbknn integrate multiple variables？such as Platform and Individual. Looking forward your reply; Siyu. >>> sc.external.pp.bbknn(mydata, batch_key=[""Platform"",""Individual_2""],n_pcs=50,set_op_mix_ratio=ratio). computing batch balanced neighbors; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/home/scCell2/miniconda3/lib/python3.8/site-packages/scanpy/external/pp/_bbknn.py"", line 134, in bbknn; return bbknn(; File ""/home/scCell2/miniconda3/lib/python3.8/site-packages/bbknn/__init__.py"", line 110, in bbknn; if batch_key not in adata.obs:; File ""/home/scCell2/miniconda3/lib/python3.8/site-packages/pandas/core/generic.py"", line 1721, in __contains__; return key in self._info_axis; File ""/home/scCell2/miniconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py"", line 4071, in __contains__; hash(key); TypeError: unhashable type: 'list",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2004:43,variab,variables,43,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2004,1,['variab'],['variables']
Modifiability,"Dear people,. In our plotting functions, vmin and vmax are great arguments, we all love them. First of all, let's all hold our hands, close our eyes and thank matplotlib developers for implementing them. But when we jointly plot some continuous variables that are not on the exact same scale (e.g. some genes), it's not possible to specify a single vmin/vmax value that fits all variables especially if they have some outliers. This deeply saddens us and forces us to watch a few more episodes of Stranger Things and it certainly doesn't help :(. I would like to hear your thoughts about how to fix that. But before that, as a responsible person, I did some homework and I spent around 34 minutes to understand how color normalization works in matplotlib (https://matplotlib.org/3.1.1/tutorials/colors/colormapnorms.html) and tried to implement a custom normalization class (https://matplotlib.org/3.1.1/tutorials/colors/colormapnorms.html#custom-normalization-manually-implement-two-linear-ranges). . My idea is simply to specify vmin/vmax in terms of quantiles of the color vector which can be shared between variables instead of a specific value. One way, I thought, might be to pass a `norm` argument with a custom normalization object to our lovely `plot_scatter`. However, as far as I understand, it's not possible because in the quantile function in the custom normalization class requires the entire color vector for each continuous variable which is not super convenient because it's too much preprocessing to find different quantile values for each variable and pass a vmin/vmax vector to the plotting function. Not user-friendly and still requires modifications in the code :(. Instead, I added two ugly arguments named `vmin_quantile` and `vmax_quantile` to the `plot_scatter` function which allows me to specify a single quantile value for vmin/vmax which is then translated into real values separately for each variable:. ![image](https://user-images.githubusercontent.com/1140359/627204",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/775:245,variab,variables,245,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/775,2,['variab'],['variables']
Modifiability,"Deduplication always makes sense. I often use speaking variable names instead of comments to clarify what I’m doing. Here the 6 reasons why I’m convinced of the way I suggested:. 1\) If I look at your change as it is, I have no idea what parameters are missing compared to `doc_scatter_bulk`. If you used variables, we could see it at a glance. 2) You could add a comment explaining that this one is just temporary (Hard to do in-line in a docstring). If one makes changes to the parameters, 3) they only have to make them once and 4) can’t forget to make them twice. 5) Also the diff becomes much smaller and 6) git will be able to track doc changes that are made in the mean time. So do it please.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/557#issuecomment-476512533:55,variab,variable,55,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/557#issuecomment-476512533,2,['variab'],"['variable', 'variables']"
Modifiability,"Definitely been an abstract todo for a while. Tracking for 1.9. Some questions:. * What about methods where more than one element is added to the AnnData? E.g. for PCA we also add the variable loadings to `varm`; * How do these parameters get tracked in the `uns` metadata? Currently the key added there is largely fixed, but maybe it should vary too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1861#issuecomment-867334904:184,variab,variable,184,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1861#issuecomment-867334904,1,['variab'],['variable']
Modifiability,"Do you have values in `adata.raw`? Raw can have a different set of variables than `adata.var`, which could be causing the issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/817#issuecomment-528735020:67,variab,variables,67,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/817#issuecomment-528735020,1,['variab'],['variables']
Modifiability,Docs Enhancements,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/58:5,Enhance,Enhancements,5,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/58,1,['Enhance'],['Enhancements']
Modifiability,Does using `adata.var_names_make_unique()` also makes the variable names of `adata.X` unique?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2685#issuecomment-1763972254:58,variab,variable,58,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2685#issuecomment-1763972254,1,['variab'],['variable']
Modifiability,"Dotplot / Matrixplot Bug/Suggestion [Key Error] Because ""var_group_labels"" & ""categories_order"" using the same variable (memory), mostly happened when ""swap_axes=True""",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3081:111,variab,variable,111,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3081,1,['variab'],['variable']
Modifiability,Dotplot labels can extend off plot,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1705:19,extend,extend,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1705,1,['extend'],['extend']
Modifiability,"Enhance scanpy.tl.rank_gene_groups with additional filters (min_pct, etc.,)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3159:0,Enhance,Enhance,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3159,1,['Enhance'],['Enhance']
Modifiability,Enhancement,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/627:0,Enhance,Enhancement,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/627,1,['Enhance'],['Enhancement']
Modifiability,Enhancement of Parameter Descriptions in MatrixPlot Class Documentation,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2766:0,Enhance,Enhancement,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2766,1,['Enhance'],['Enhancement']
Modifiability,"Every time I build the docs locally, they do a complete rebuild. This is painfully slow (especially with our examples that run on each build) and really discourages editing the docs. This is happening because the sphinx sees the config being modified. There are two causes of this:. * The version being set dynamically – at each commit the version string changes.; * `scanpydoc.elegant_typehints` sets some properties of the config after it's loaded. E.g.:. ```; updating environment: [config changed ('typehints_formatter')] 317 added, 0 changed, 0 removed; ```. ### Solution. Version being set dynamically does really add that much value for us, so I just removed that part of the version string. `scanpydoc.elegant_typehints` does make the doc-strings nicer, but it is not worth a five minute build to update the docs. Ideally it can be implemented in a way that doesn't make sphinx think the config has changed, but I am disabling it until then.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2199:229,config,config,229,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2199,4,['config'],['config']
Modifiability,Extend benchmarks from basic tutorial,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3031:0,Extend,Extend,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3031,1,['Extend'],['Extend']
Modifiability,Extend functionality of tl.rank_genes_groups(),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2317:0,Extend,Extend,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2317,1,['Extend'],['Extend']
Modifiability,Extended gex_only function to visium. <!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #3113; - [x] Tests included or not required because: It's a straightforward argument pass from visium to read_10x_h5; <!-- Only check the following box if you did not include release notes -->; - [x] Release notes not necessary because: not a big change,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3278:0,Extend,Extended,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3278,1,['Extend'],['Extended']
Modifiability,Extending gex_only option to Visium function,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3278:0,Extend,Extending,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3278,1,['Extend'],['Extending']
Modifiability,"Feature selection refers to excluding uninformative genes such as those which exhibit no meaningful biological variation across samples. Since scRNA-Seq experiments usually examine cells within a single tissue, only a small fraction of genes are expected to be informative since many genes are biologically variable only across different tissues (adopted from https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6).; But, in fact some experimental design are very complex, such single-cell RNAseq of tissues from different development stage. The tissues can vary a log along the development timeline.; I find that the number of HVGs can affect data integration and batch effects correction. I've integrated seven cell samples collected at different development stage(1day, 2 day, 3 day, 4day, 5 day, 6 day, 7day after fertilization) with SCVI-tools, using 2000 HVGs, which then shows no ""batch effect"" (cells were mixed with no correlation among samples) left; on the other hand, using all genes, which shows still some extent of ""batch effect"" (some cells were clustered by time obviously) left. This could definitely affect the biological explaination, because the ""batch effect"" can be regarded as the difference of true biological difference at different development stage. The tissues are undergoing intensive differentiation process, so that the cell population are changing a lot during this process. Using only HVGs might lost these development process. ; In sum, HVGs are good for batch effect correction. The ""batch effects"" become less obvious when using less genes and more obvious when using more genes. However, more genes are good for discovery of new cell population. Does this make sense ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1578#issuecomment-764494020:307,variab,variable,307,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578#issuecomment-764494020,1,['variab'],['variable']
Modifiability,"File ~/miniconda3/envs/test/lib/python3.10/site-packages/numba/parfors/parfor.py:2926, in ParforPass.run(self); 2924 # Validate reduction in parfors.; 2925 for p in parfors:; -> 2926 get_parfor_reductions(self.func_ir, p, p.params, self.calltypes); 2928 # Validate parameters:; 2929 for p in parfors:. File ~/miniconda3/envs/test/lib/python3.10/site-packages/numba/parfors/parfor.py:3549, in get_parfor_reductions(func_ir, parfor, parfor_params, calltypes, reductions, reduce_varnames, param_uses, param_nodes, var_to_param); 3547 if param_name in used_vars and param_name not in reduce_varnames:; 3548 param_nodes[param].reverse(); -> 3549 reduce_nodes = get_reduce_nodes(param, param_nodes[param], func_ir); 3550 # Certain kinds of ill-formed Python (like potentially undefined; 3551 # variables) in combination with SSA can make things look like; 3552 # reductions except that they don't have reduction operators.; 3553 # If we get to this point but don't find a reduction operator; 3554 # then assume it is this situation and just don't treat this; 3555 # variable as a reduction.; 3556 if reduce_nodes is not None:. File ~/miniconda3/envs/test/lib/python3.10/site-packages/numba/parfors/parfor.py:3637, in get_reduce_nodes(reduction_node, nodes, func_ir); 3635 defs[lhs.name] = rhs; 3636 if isinstance(rhs, ir.Var) and rhs.name in defs:; -> 3637 rhs = lookup(rhs); 3638 if isinstance(rhs, ir.Expr):; 3639 in_vars = set(lookup(v, True).name for v in rhs.list_vars()). File ~/miniconda3/envs/test/lib/python3.10/site-packages/numba/parfors/parfor.py:3627, in get_reduce_nodes.<locals>.lookup(var, varonly); 3625 val = defs.get(var.name, None); 3626 if isinstance(val, ir.Var):; -> 3627 return lookup(val); 3628 else:; 3629 return var if (varonly or val is None) else val. File ~/miniconda3/envs/test/lib/python3.10/site-packages/numba/parfors/parfor.py:3627, in get_reduce_nodes.<locals>.lookup(var, varonly); 3625 val = defs.get(var.name, None); 3626 if isinstance(val, ir.Var):; -> 3627 return l",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2191:10120,variab,variable,10120,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2191,1,['variab'],['variable']
Modifiability,"First, very nice job on this package! It is a welcome replacement to seurat for python programmers! Yay!. I have a suggestion to improve sc.pl.dotplot(). I would like to be able to adjust the limits of the color_map used for sc.pl.dotplot(). The documentation states that kwargs get passed to matplotlib.pyplot.scatter(). But the dotplot does not change when vmin and vmax are included as kwargs. In the source code for anndata.dotplot includes the following lines:. import matplotlib.colors; normalize = matplotlib.colors.Normalize(vmin=min(mean_flat), vmax=max(mean_flat)); colors = [cmap(normalize(value)) for value in mean_flat]. I believe these lines are the issue. Because vmin and vmax are calculated from the mean_flat variable the vmin and vmax provided as parameters are ignored. Perhaps rather than determining the scale using mean_flat, the vmin and vmax should be used when provided by the user. This would allow the user to set the color_bar scale as they need. Currently, there is no way to change the color_bar and the documentation is misleading, since it suggests that it can be done.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/388:727,variab,variable,727,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388,1,['variab'],['variable']
Modifiability,Fix `layers` parameter in `score_genes` with `.raw`,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3155:5,layers,layers,5,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3155,1,['layers'],['layers']
