quality_attribute,sentence,source,author,repo,version,id,keyword,matched_word,match_idx,filename,wiki,url,total_similar,target_keywords,target_matched_words
Performance,"onal restrictions, described later in this; document. It is undefined behavior if a computation history featuring a send of; ``retain`` followed by a send of ``release`` to the same object, with no; intervening ``release`` on that object, is not equivalent under the high-level; semantics to a computation history in which these sends are removed. Note that; this implies that these methods may not raise exceptions. It is undefined behavior if a computation history features any use whatsoever; of an object following the completion of a send of ``release`` that is not; preceded by a send of ``retain`` to the same object. The behavior of ``autorelease`` must be equivalent to sending ``release`` when; one of the autorelease pools currently in scope is popped. It may not throw an; exception. When the semantics call for performing one of these operations on a retainable; object pointer, if that pointer is ``null`` then the effect is a no-op. All of the semantics described in this document are subject to additional; :ref:`optimization rules <arc.optimization>` which permit the removal or; optimization of operations based on local knowledge of data flow. The; semantics describe the high-level behaviors that the compiler implements, not; an exact sequence of operations that a program will be compiled into. .. _arc.objects.operands:. Retainable object pointers as operands and arguments; ----------------------------------------------------. In general, ARC does not perform retain or release operations when simply using; a retainable object pointer as an operand within an expression. This includes:. * loading a retainable pointer from an object with non-weak :ref:`ownership; <arc.ownership>`,; * passing a retainable pointer as an argument to a function or method, and; * receiving a retainable pointer as the result of a function or method call. .. admonition:: Rationale. While this might seem uncontroversial, it is actually unsafe when multiple; expressions are evaluated in ""paral",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst:15066,optimiz,optimization,15066,interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,1,['optimiz'],['optimization']
Performance,"onality while the specific behaviours for one or multiple dimensions are implemented in; `ROOT::Math::VirtualIntegratorOneDim` and `ROOT::Math::VirtualIntegratorMultiDim`.; These interfaces define the integrator functionality with abstract methods to set the function, to compute the integral or to set the integration tolerance.; These methods must be implemented in the concrete classes existing for the different integration algorithms.; The user cannot create directly these virtual integrator interfaces. He needs to create the; `ROOT::Math::IntegratorOneDim` class for integrating one-dimensional functions and `ROOT::Math::IntegratorMultiDim` for multi-dimensional functions.; Through the ROOT Plug-In Manager, the user can initialize `ROOT::Math::IntegratorOneDim` or `ROOT::Math::IntegratorMultiDim` with; any of the concrete integration classes without dealing with them directly.; These two classes provide the same interface as in `VirtualIntegratorOneDim` and `VirtualIntegratorMultiDim`, but with the possibility to choose in the constructor,; which method will be used to perform the integration. The method to set the function to be integrated, must be of the function interface type described before.; `ROOT::Math::IBaseFunctionOneDimFunction` is used for `ROOT::Math::IBaseFunctionMultiDim` and; The only difference between the `ROOT::Math::IntegratorOneDim` and `ROOT::Math::IntegratorMultiDim` resides; in the dimensionality of that function and some specific that will be seen afterwards for the one dimensional one. ![ROOT::Math Numerical Integrator classes](pictures/Integration.png). The rest of the classes shown above in the diagram are the specialized classes provided. Each one implements a different method that will be explained in detail. It is important; to notice that the two grayed classes (the one which name starts by GSL) are part of the *MathMore* library.; We will later show in more detail the differences between the implementations. ### Integration of One-di",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/MathLibraries.md:49540,perform,perform,49540,documentation/users-guide/MathLibraries.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/MathLibraries.md,1,['perform'],['perform']
Performance,"oncurrency model of the host language, happens-after; the generation of the pointer and happens-before a release of that; object (possibly via an aliasing pointer or indirectly due to; destruction of a different object). .. admonition:: Rationale. There is very little point in trying to guarantee correctness in the; presence of race conditions. ARC does not have a stack-scanning; garbage collector, and guaranteeing the atomicity of every load and; store operation would be prohibitive and preclude a vast amount of; optimization. ARC may assume that non-ARC code engages in sensible balancing; behavior and does not rely on exact or minimum retain count values; except as guaranteed by ``__strong`` object invariants or +1 transfer; conventions. For example, if an object is provably double-retained; and double-released, ARC may eliminate the inner retain and release;; it does not need to guard against code which performs an unbalanced; release followed by a ""balancing"" retain. .. _arc.optimization.liveness:. Object liveness; ---------------. ARC may not allow a retainable object ``X`` to be deallocated at a; time ``T`` in a computation history if:. * ``X`` is the value stored in a ``__strong`` object ``S`` with; :ref:`precise lifetime semantics <arc.optimization.precise>`, or. * ``X`` is the value stored in a ``__strong`` object ``S`` with; imprecise lifetime semantics and, at some point after ``T`` but; before the next store to ``S``, the computation history features a; load from ``S`` and in some way depends on the value loaded, or. * ``X`` is a value described as being released at the end of the; current full-expression and, at some point after ``T`` but before; the end of the full-expression, the computation history depends; on that value. .. admonition:: Rationale. The intent of the second rule is to say that objects held in normal; ``__strong`` local variables may be released as soon as the value in; the variable is no longer being used: either the variable stops; be",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst:78641,optimiz,optimization,78641,interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,1,['optimiz'],['optimization']
Performance,"ond, label %guarded, label %deopt. for this branch. Here `%any_other_cond` is an arbitrarily chosen; well-defined `i1` value. By making guard widening, we may; impose stricter conditions on `guarded` block and bail to the; deopt when the new condition is not met. Lowering:; """""""""""""""""". Default lowering strategy is replacing the result of; call of ``@llvm.experimental.widenable.condition`` with; constant `true`. However it is always correct to replace; it with any other `i1` value. Any pass can; freely do it if it can benefit from non-default lowering. '``llvm.load.relative``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare ptr @llvm.load.relative.iN(ptr %ptr, iN %offset) nounwind memory(argmem: read). Overview:; """""""""""""""""". This intrinsic loads a 32-bit value from the address ``%ptr + %offset``,; adds ``%ptr`` to that value and returns it. The constant folder specifically; recognizes the form of this intrinsic and the constant initializers it may; load from; if a loaded constant initializer is known to have the form; ``i32 trunc(x - %ptr)``, the intrinsic call is folded to ``x``. LLVM provides that the calculation of such a constant initializer will; not overflow at link time under the medium code model if ``x`` is an; ``unnamed_addr`` function. However, it does not provide this guarantee for; a constant initializer folded into a function body. This intrinsic can be; used to avoid the possibility of overflows when loading from such a constant. .. _llvm_sideeffect:. '``llvm.sideeffect``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare void @llvm.sideeffect() inaccessiblememonly nounwind willreturn. Overview:; """""""""""""""""". The ``llvm.sideeffect`` intrinsic doesn't perform any operation. Optimizers; treat it as having side effects, so it can be inserted into a loop to; indicate that the loop shouldn't be assumed to terminate (which could; potentially lead to the loop being optimized away entirely), even if it's; an infi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:951147,load,load,951147,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,2,['load'],"['load', 'loaded']"
Performance,one as ``*_vol`` to only invalidate the volatile cache lines. The code sequences used to implement the memory model for GFX90A are defined; in table :ref:`amdgpu-amdhsa-memory-model-code-sequences-gfx90a-table`. .. table:: AMDHSA Memory Model Code Sequences GFX90A; :name: amdgpu-amdhsa-memory-model-code-sequences-gfx90a-table. ============ ============ ============== ========== ================================; LLVM Instr LLVM Memory LLVM Memory AMDGPU AMDGPU Machine Code; Ordering Sync Scope Address GFX90A; Space; ============ ============ ============== ========== ================================; **Non-Atomic**; ------------------------------------------------------------------------------------; load *none* *none* - global - !volatile & !nontemporal; - generic; - private 1. buffer/global/flat_load; - constant; - !volatile & nontemporal. 1. buffer/global/flat_load; glc=1 slc=1. - volatile. 1. buffer/global/flat_load; glc=1; 2. s_waitcnt vmcnt(0). - Must happen before; any following volatile; global/generic; load/store.; - Ensures that; volatile; operations to; different; addresses will not; be reordered by; hardware. load *none* *none* - local 1. ds_load; store *none* *none* - global - !volatile & !nontemporal; - generic; - private 1. buffer/global/flat_store; - constant; - !volatile & nontemporal. 1. buffer/global/flat_store; glc=1 slc=1. - volatile. 1. buffer/global/flat_store; 2. s_waitcnt vmcnt(0). - Must happen before; any following volatile; global/generic; load/store.; - Ensures that; volatile; operations to; different; addresses will not; be reordered by; hardware. store *none* *none* - local 1. ds_store; **Unordered Atomic**; ------------------------------------------------------------------------------------; load atomic unordered *any* *any* *Same as non-atomic*.; store atomic unordered *any* *any* *Same as non-atomic*.; atomicrmw unordered *any* *any* *Same as monotonic atomic*.; **Monotonic Atomic**; --------------------------------------------------,MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:242823,load,load,242823,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"one parent and several daughters. For a better; understanding of the hierarchy, have a look at TGeoManage. Just close now the `X3D` window and focus at the wire frame picture; drawn in a pad. Activate Options/Event Status. Moving the mouse in the; pad, you will notice that objects are sometimes changing color to red.; Volumes are highlighted in this way whenever the mouse pointer is close; enough to one of its vertices. When this happens, the corresponding; volume is selected and you will see in the bottom right size of the %ROOT; canvas its name, shape type and corresponding path in the physical tree.; Right clicking on the screen when a volume is selected will also open; its context menu (picking). Note that there are several actions that can; be performed both at view (no volume selected) and volume level. TView (mouse not selecting any volume):. - Click-and-drag rotates the view.; - Pressing some keys perform different actions:; - J/K - zoom / unzoom; - H, L, U, I - move the viewpoint; - Right click + `SetParallel` `()/SetPerspective` `()` - switch from; parallel to perspective view.; - Right click + `ShowAxis()` - show coordinate axes.; - Right click + `Centered/Left/Side/Top` - change view direction. TGeoVolume (mouse selecting a volume):. - Double click will focus the corresponding volume.; - Right click + `CheckOverlaps()` - run overlap checker on current; volume.; - Right click + `Draw` `()` - draw that volume according current; global visualization options; - Right click + `DrawOnly()` - draw only the selected volume.; - Right click + `InspectShape/Material()` - print info about shape or; material.; - Right click + `Raytrace()` - initiate a ray tracing algorithm on; current view.; - Right click + `RandomPoints/Rays()` - shoot random points or rays; inside the bounding box of the clicked volume and display only those; inside visible volumes.; - Right click + `Weight()` - estimates the weight of a volume within a; given precision. Note that there are several",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md:14226,perform,perform,14226,geom/geom/doc/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md,1,['perform'],['perform']
Performance,"one pointer each contained within the Location.; Both Locations in a pair can be assumed to be of the same size. Note that the Locations used in each section may describe the same; physical location. e.g. A stack slot may appear as a deopt location,; a gc base pointer, and a gc derived pointer. The LiveOut section of the StkMapRecord will be empty for a statepoint; record. Safepoint Semantics & Verification; ==================================. The fundamental correctness property for the compiled code's; correctness w.r.t. the garbage collector is a dynamic one. It must be; the case that there is no dynamic trace such that an operation; involving a potentially relocated pointer is observably-after a; safepoint which could relocate it. 'observably-after' is this usage; means that an outside observer could observe this sequence of events; in a way which precludes the operation being performed before the; safepoint. To understand why this 'observable-after' property is required,; consider a null comparison performed on the original copy of a; relocated pointer. Assuming that control flow follows the safepoint,; there is no way to observe externally whether the null comparison is; performed before or after the safepoint. (Remember, the original; Value is unmodified by the safepoint.) The compiler is free to make; either scheduling choice. The actual correctness property implemented is slightly stronger than; this. We require that there be no *static path* on which a; potentially relocated pointer is 'observably-after' it may have been; relocated. This is slightly stronger than is strictly necessary (and; thus may disallow some otherwise valid programs), but greatly; simplifies reasoning about correctness of the compiled code. By construction, this property will be upheld by the optimizer if; correctly established in the source IR. This is a key invariant of; the design. The existing IR Verifier pass has been extended to check most of the; local restrictions on the intri",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Statepoints.rst:23906,perform,performed,23906,interpreter/llvm-project/llvm/docs/Statepoints.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Statepoints.rst,1,['perform'],['performed']
Performance,"one* - global - !volatile & !nontemporal; - generic; - private 1. buffer/global/flat_store; - constant; - !volatile & nontemporal. 1. buffer/global/flat_store; glc=1 slc=1 dlc=1. - If GFX10, omit dlc=1. - volatile. 1. buffer/global/flat_store; dlc=1. - If GFX10, omit dlc=1. 2. s_waitcnt vscnt(0). - Must happen before; any following volatile; global/generic; load/store.; - Ensures that; volatile; operations to; different; addresses will not; be reordered by; hardware. store *none* *none* - local 1. ds_store; **Unordered Atomic**; ------------------------------------------------------------------------------------; load atomic unordered *any* *any* *Same as non-atomic*.; store atomic unordered *any* *any* *Same as non-atomic*.; atomicrmw unordered *any* *any* *Same as monotonic atomic*.; **Monotonic Atomic**; ------------------------------------------------------------------------------------; load atomic monotonic - singlethread - global 1. buffer/global/flat_load; - wavefront - generic; load atomic monotonic - workgroup - global 1. buffer/global/flat_load; - generic glc=1. - If CU wavefront execution; mode, omit glc=1. load atomic monotonic - singlethread - local 1. ds_load; - wavefront; - workgroup; load atomic monotonic - agent - global 1. buffer/global/flat_load; - system - generic glc=1 dlc=1. - If GFX11, omit dlc=1. store atomic monotonic - singlethread - global 1. buffer/global/flat_store; - wavefront - generic; - workgroup; - agent; - system; store atomic monotonic - singlethread - local 1. ds_store; - wavefront; - workgroup; atomicrmw monotonic - singlethread - global 1. buffer/global/flat_atomic; - wavefront - generic; - workgroup; - agent; - system; atomicrmw monotonic - singlethread - local 1. ds_atomic; - wavefront; - workgroup; **Acquire Atomic**; ------------------------------------------------------------------------------------; load atomic acquire - singlethread - global 1. buffer/global/ds/flat_load; - wavefront - local; - generic; load atomic acqu",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:345229,load,load,345229,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"one* 1. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate; (see comment for; previous fence).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - Must happen before; the following; buffer_inv.; - Ensures that the; fence-paired atomic; has completed; before invalidating; the; cache. Therefore; any following; locations read must; be no older than; the value read by; the; fence-paired-atomic. 2. buffer_inv sc0=1 sc1=1. - Must happen before any; following global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. **Release Atomic**; ------------------------------------------------------------------------------------; store atomic release - singlethread - global 1. GFX940, GFX941; - wavefront - generic buffer/global/flat_store; sc0=1 sc1=1; GFX942; buffer/global/flat_store. store atomic release - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; be used.*. 1. ds_store; store atomic release - workgroup - global 1. s_waitcnt lgkm/vmcnt(0); - generic; - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL, omit lgkmcnt(0).; - s_waitcnt vmcnt(0); must happen after; any",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:306189,cache,cache,306189,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['cache'],['cache']
Performance,"one* 1. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate; (see comment for; previous fence).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - Must happen before; the following; buffer_inv.; - Ensures that the; fence-paired atomic; has completed; before invalidating; the; cache. Therefore; any following; locations read must; be no older than; the value read by; the; fence-paired-atomic. 2. buffer_inv sc1=1. - Must happen before any; following global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. fence acquire - system *none* 1. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate; (see comment for; previous fence).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; u",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:304819,cache,cache,304819,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['cache'],['cache']
Performance,"one-eabi target""``; * ``-DCMAKE_AR=/path/to/llvm-ar``; * ``-DCMAKE_NM=/path/to/llvm-nm``; * ``-DCMAKE_RANLIB=/path/to/llvm-ranlib``; * ``-DCOMPILER_RT_BAREMETAL_BUILD=ON``; * ``-DCOMPILER_RT_DEFAULT_TARGET_ONLY=ON``; * ``-DLLVM_CONFIG_PATH=/path/to/llvm-config``; * ``-DCMAKE_C_FLAGS=""build-c-flags""``; * ``-DCMAKE_ASM_FLAGS=""build-c-flags""``; * ``-DCOMPILER_RT_EMULATOR=""qemu-arm -L /path/to/armv7-A/sysroot""``; * ``-DCOMPILER_RT_INCLUDE_TESTS=ON``; * ``-DCOMPILER_RT_TEST_COMPILER=""/path/to/clang""``; * ``-DCOMPILER_RT_TEST_COMPILER_CFLAGS=""test-c-flags""``. The Armv6-M builtins will use the soft-float ABI. When compiling the tests for; Armv7-A we must include ``""-mthumb -mfloat-abi=soft -mfpu=none""`` in the; test-c-flags. We must use an Armv7-A soft-float abi sysroot for ``qemu-arm``. Depending on the linker used for the test cases you may encounter BuildAttribute; mismatches between the M-profile objects from compiler-rt and the A-profile; objects from the test. The lld linker does not check the profile; BuildAttribute so it can be used to link the tests by adding -fuse-ld=lld to the; ``COMPILER_RT_TEST_COMPILER_CFLAGS``. Alternative using a cmake cache; -------------------------------; If you wish to build, but not test compiler-rt for Armv6-M, Armv7-M or Armv7E-M; the easiest way is to use the BaremetalARM.cmake recipe in clang/cmake/caches. You will need a bare metal sysroot such as that provided by the GNU ARM; Embedded toolchain. The libraries can be built with the cmake options:. * ``-DBAREMETAL_ARMV6M_SYSROOT=/path/to/bare/metal/toolchain/arm-none-eabi``; * ``-DBAREMETAL_ARMV7M_SYSROOT=/path/to/bare/metal/toolchain/arm-none-eabi``; * ``-DBAREMETAL_ARMV7EM_SYSROOT=/path/to/bare/metal/toolchain/arm-none-eabi``; * ``-C /path/to/llvm/source/tools/clang/cmake/caches/BaremetalARM.cmake``; * ``/path/to/llvm``. **Note** that for the recipe to work the compiler-rt source must be checked out; into the directory llvm/runtimes. You will also need clang and lld checked out. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToCrossCompileBuiltinsOnArm.rst:12431,cache,cache,12431,interpreter/llvm-project/llvm/docs/HowToCrossCompileBuiltinsOnArm.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToCrossCompileBuiltinsOnArm.rst,3,['cache'],"['cache', 'caches']"
Performance,"oned above); whenever feasible.; - Include comments about what is tested/expected in a particular test. If there; are relevant issues in the bug tracker, add references to those bug reports; (for example, ""See PR999 for more details"").; - Avoid undefined behavior and poison/undef values unless necessary. For; example, do not use patterns like ``br i1 undef``, which are likely to break; as a result of future optimizations.; - Minimize tests by removing unnecessary instructions, metadata, attributes,; etc. Tools like ``llvm-reduce`` can help automate this.; - Outside PhaseOrdering tests, only run a minimal set of passes. For example,; prefer ``opt -S -passes=instcombine`` over ``opt -S -O3``.; - Avoid unnamed instructions/blocks (such as ``%0`` or ``1:``), because they may; require renumbering on future test modifications. These can be removed by; running the test through ``opt -S -passes=instnamer``.; - Try to give values (including variables, blocks and functions) meaningful; names, and avoid retaining complex names generated by the optimization; pipeline (such as ``%foo.0.0.0.0.0.0``). Extra files; -----------. If your test requires extra files besides the file containing the ``RUN:`` lines; and the extra files are small, consider specifying them in the same file and; using ``split-file`` to extract them. For example,. .. code-block:: llvm. ; RUN: split-file %s %t; ; RUN: llvm-link -S %t/a.ll %t/b.ll | FileCheck %s. ; CHECK: ... ;--- a.ll; ...; ;--- b.ll; ... The parts are separated by the regex ``^(.|//)--- <part>``. If you want to test relative line numbers like ``[[#@LINE+1]]``, specify; ``--leading-lines`` to add leading empty lines to preserve line numbers. If the extra files are large, the idiomatic place to put them is in a subdirectory ``Inputs``.; You can then refer to the extra files as ``%S/Inputs/foo.bar``. For example, consider ``test/Linker/ident.ll``. The directory structure is; as follows::. test/; Linker/; ident.ll; Inputs/; ident.a.ll; ident.b.ll.",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TestingGuide.rst:14574,optimiz,optimization,14574,interpreter/llvm-project/llvm/docs/TestingGuide.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TestingGuide.rst,1,['optimiz'],['optimization']
Performance,"only be; - ``--offload-arch`` loaded and executed in a process that has a; matching setting for SRAMECC. If not specified for code object V2 to V3, generate; code that can be loaded and executed in a process; with SRAMECC enabled. If not specified for code object V4 or above, generate; code that can be loaded and executed in a process; with either setting of SRAMECC. tgsplit ``-m[no-]tgsplit`` Enable/disable generating code that assumes; work-groups are launched in threadgroup split mode.; When enabled the waves of a work-group may be; launched in different CUs. wavefrontsize64 - ``-m[no-]wavefrontsize64`` Control the wavefront size used when; generating code for kernels. When disabled; native wavefront size 32 is used, when enabled; wavefront size 64 is used. xnack - ``-mcpu`` If specified, generate code that can only be; - ``--offload-arch`` loaded and executed in a process that has a; matching setting for XNACK replay. If not specified for code object V2 to V3, generate; code that can be loaded and executed in a process; with XNACK replay enabled. If not specified for code object V4 or above, generate; code that can be loaded and executed in a process; with either setting of XNACK replay. XNACK replay can be used for demand paging and; page migration. If enabled in the device, then if; a page fault occurs the code may execute; incorrectly unless generated with XNACK replay; enabled, or generated for code object V4 or above without; specifying XNACK replay. Executing code that was; generated with XNACK replay enabled, or generated; for code object V4 or above without specifying XNACK replay,; on a device that does not have XNACK replay; enabled will execute correctly but may be less; performant than code generated for XNACK replay; disabled.; =============== ============================ ==================================================. .. _amdgpu-target-id:. Target ID; ---------. AMDGPU supports target IDs. See `Clang Offload Bundler; <https://clang.llvm.org/docs",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:18236,load,loaded,18236,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['loaded']
Performance,"only supports a limited number of processors and has fixed; settings for target features. See; :ref:`amdgpu-elf-note-record-supported_processors-v2-table` for a list of; processors and the corresponding target ID. In the table the note record ISA; name is a concatenation of the vendor name, architecture name, major, minor,; and stepping separated by a "":"". The target ID column shows the processor name and fixed target features used; by the LLVM compiler. The LLVM compiler does not generate a; ``NT_AMD_HSA_HSAIL`` note record. A code object generated by the Finalizer also uses code object V2 and always; generates a ``NT_AMD_HSA_HSAIL`` note record. The processor name and; ``sramecc`` target feature is as shown in; :ref:`amdgpu-elf-note-record-supported_processors-v2-table` but the ``xnack``; target feature is specified by the ``EF_AMDGPU_FEATURE_XNACK_V2`` ``e_flags``; bit. ``NT_AMD_HSA_ISA_NAME``; Specifies the target ISA name as a non-NUL terminated string. This note record is not used by the HSA runtime loader. See the ``NT_AMD_HSA_ISA_VERSION`` note record description of the code object; V2's limited support of processors and fixed settings for target features. See :ref:`amdgpu-elf-note-record-supported_processors-v2-table` for a mapping; from the string to the corresponding target ID. If the ``xnack`` target; feature is supported and enabled, the string produced by the LLVM compiler; will may have a ``+xnack`` appended. The Finlizer did not do the appending and; instead used the ``EF_AMDGPU_FEATURE_XNACK_V2`` ``e_flags`` bit. ``NT_AMD_HSA_METADATA``; Specifies extensible metadata associated with the code objects executed on HSA; [HSA]_ compatible runtimes (see :ref:`amdgpu-os`). It is required when the; target triple OS is ``amdhsa`` (see :ref:`amdgpu-target-triples`). See; :ref:`amdgpu-amdhsa-code-object-metadata-v2` for the syntax of the code object; metadata string. .. table:: AMDGPU Code Object V2 Supported Processors and Fixed Target Feature Settings; :name",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:73795,load,loader,73795,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['loader']
Performance,"only* loop; in a given function, it is not touched. This is a pass most useful for; debugging via bugpoint. ``loop-reduce``: Loop Strength Reduction; ----------------------------------------. This pass performs a strength reduction on array references inside loops that; have as one or more of their components the loop induction variable. This is; accomplished by creating a new value to hold the initial value of the array; access for the first iteration, and then creating a new GEP instruction in the; loop to increment the value by the appropriate amount. .. _passes-loop-rotate:. ``loop-rotate``: Rotate Loops; -----------------------------. A simple loop rotation transformation. A summary of it can be found in; :ref:`Loop Terminology for Rotated Loops <loop-terminology-loop-rotate>`. .. _passes-loop-simplify:. ``loop-simplify``: Canonicalize natural loops; ---------------------------------------------. This pass performs several transformations to transform natural loops into a; simpler form, which makes subsequent analyses and transformations simpler and; more effective. A summary of it can be found in; :ref:`Loop Terminology, Loop Simplify Form <loop-terminology-loop-simplify>`. Loop pre-header insertion guarantees that there is a single, non-critical entry; edge from outside of the loop to the loop header. This simplifies a number of; analyses and transformations, such as :ref:`LICM <passes-licm>`. Loop exit-block insertion guarantees that all exit blocks from the loop (blocks; which are outside of the loop that have predecessors inside of the loop) only; have predecessors from inside of the loop (and are thus dominated by the loop; header). This simplifies transformations such as store-sinking that are built; into LICM. This pass also guarantees that loops will have exactly one backedge. Note that the :ref:`simplifycfg <passes-simplifycfg>` pass will clean up blocks; which are split out but end up being unnecessary, so usage of this pass should; not pessimize gene",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:26704,perform,performs,26704,interpreter/llvm-project/llvm/docs/Passes.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst,1,['perform'],['performs']
Performance,"ons access a single vector L1 cache shared by all; SIMDs a CU. Therefore, no special action is required for coherence between the; lanes of a single wavefront, or for coherence between wavefronts in the same; work-group. A ``buffer_wbinvl1_vol`` is required for coherence between; wavefronts executing in different work-groups as they may be executing on; different CUs.; * The scalar memory operations access a scalar L1 cache shared by all wavefronts; on a group of CUs. The scalar and vector L1 caches are not coherent. However,; scalar operations are used in a restricted way so do not impact the memory; model. See :ref:`amdgpu-amdhsa-memory-spaces`.; * The vector and scalar memory operations use an L2 cache shared by all CUs on; the same agent.; * The L2 cache has independent channels to service disjoint ranges of virtual; addresses.; * Each CU has a separate request queue per channel. Therefore, the vector and; scalar memory operations performed by wavefronts executing in different; work-groups (which may be executing on different CUs) of an agent can be; reordered relative to each other. A ``s_waitcnt vmcnt(0)`` is required to; ensure synchronization between vector memory operations of different CUs. It; ensures a previous vector memory operation has completed before executing a; subsequent vector memory or LDS operation and so can be used to meet the; requirements of acquire and release.; * The L2 cache can be kept coherent with other agents on some targets, or ranges; of virtual addresses can be set up to bypass it to ensure system coherence. Scalar memory operations are only used to access memory that is proven to not; change during the execution of the kernel dispatch. This includes constant; address space and global address space for program scope ``const`` variables.; Therefore, the kernel machine code does not have to maintain the scalar cache to; ensure it is coherent with the vector caches. The scalar and vector caches are; invalidated between kernel dispat",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:208664,perform,performed,208664,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['perform'],['performed']
Performance,"ons and potentially long data dependencies; which may limit the ILP. Last row, ``<total>``, shows a global average over all; instructions measured. Note that :program:`llvm-mca`, by default, assumes at; least 1cy between the dispatch event and the issue event. When the performance is limited by data dependencies and/or long latency; instructions, the number of cycles spent while in the *ready* state is expected; to be very small when compared with the total number of cycles spent in the; scheduler's queue. The difference between the two counters is a good indicator; of how large of an impact data dependencies had on the execution of the; instructions. When performance is mostly limited by the lack of hardware; resources, the delta between the two counters is small. However, the number of; cycles spent in the queue tends to be larger (i.e., more than 1-3cy),; especially when compared to other low latency instructions. Bottleneck Analysis; ^^^^^^^^^^^^^^^^^^^; The ``-bottleneck-analysis`` command line option enables the analysis of; performance bottlenecks. This analysis is potentially expensive. It attempts to correlate increases in; backend pressure (caused by pipeline resource pressure and data dependencies) to; dynamic dispatch stalls. Below is an example of ``-bottleneck-analysis`` output generated by; :program:`llvm-mca` for 500 iterations of the dot-product example on btver2. .. code-block:: none. Cycles with backend pressure increase [ 48.07% ]; Throughput Bottlenecks:; Resource Pressure [ 47.77% ]; - JFPA [ 47.77% ]; - JFPU0 [ 47.77% ]; Data Dependencies: [ 0.30% ]; - Register Dependencies [ 0.30% ]; - Memory Dependencies [ 0.00% ]. Critical sequence based on the simulation:. Instruction Dependency Information; +----< 2. vhaddps %xmm3, %xmm3, %xmm4; |; | < loop carried >; |; | 0. vmulps %xmm0, %xmm1, %xmm2; +----> 1. vhaddps %xmm2, %xmm2, %xmm3 ## RESOURCE interference: JFPA [ probability: 74% ]; +----> 2. vhaddps %xmm3, %xmm3, %xmm4 ## REGISTER dependency: %x",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst:26539,bottleneck,bottleneck-analysis,26539,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,3,"['bottleneck', 'perform']","['bottleneck-analysis', 'bottlenecks', 'performance']"
Performance,"ons are 0s. :ref:`llvm.get.rounding<int_get_rounding>` AMDGPU supports two separately controllable rounding; modes depending on the floating-point type. One; controls float, and the other controls both double and; half operations. If both modes are the same, returns; one of the standard return values. If the modes are; different, returns one of :ref:`12 extended values; <amdgpu-rounding-mode-enumeration-values-table>`; describing the two modes. To nearest, ties away from zero is not a supported; mode. The raw rounding mode values in the MODE; register do not exactly match the FLT_ROUNDS values,; so a conversion is performed. llvm.amdgcn.wave.reduce.umin Performs an arithmetic unsigned min reduction on the unsigned values; provided by each lane in the wavefront.; Intrinsic takes a hint for reduction strategy using second operand; 0: Target default preference,; 1: `Iterative strategy`, and; 2: `DPP`.; If target does not support the DPP operations (e.g. gfx6/7),; reduction will be performed using default iterative strategy.; Intrinsic is currently only implemented for i32. llvm.amdgcn.wave.reduce.umax Performs an arithmetic unsigned max reduction on the unsigned values; provided by each lane in the wavefront.; Intrinsic takes a hint for reduction strategy using second operand; 0: Target default preference,; 1: `Iterative strategy`, and; 2: `DPP`.; If target does not support the DPP operations (e.g. gfx6/7),; reduction will be performed using default iterative strategy.; Intrinsic is currently only implemented for i32. llvm.amdgcn.udot2 Provides direct access to v_dot2_u32_u16 across targets which; support such instructions. This performs unsigned dot product; with two v2i16 operands, summed with the third i32 operand. The; i1 fourth operand is used to clamp the output. llvm.amdgcn.udot4 Provides direct access to v_dot4_u32_u8 across targets which; support such instructions. This performs unsigned dot product; with two i32 operands (holding a vector of 4 8bit values), s",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:38588,perform,performed,38588,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['perform'],['performed']
Performance,"ons called in a small number of call sites; ------------------------------------------------; We may extend the above approach to cases where `f()`; is called more than once (but still a small number of times).; With LTO we know all possible values of `RA` and we check them; one-by-one (or using binary search) against the value on stack.; If the match is found, we `JMP` to the known constant address, otherwise abort. This protection is *near-precise*, i.e. it guarantees that the control flow will; be transferred to one of the valid return addresses for this function,; but not necessary to the point of the most recent `CALL`. General case; ------------; For functions called multiple times a *return jump table* is constructed; in the same manner as jump tables for indirect function calls (see above).; The correct jump table entry (or its index) is passed by `CALL` to `f()`; (as an extra argument) and then spilled to stack.; The `RET` instruction is replaced with a load of the jump table entry,; jump table range check, and `JMP` to the jump table entry. This protection is also *near-precise*. Returns from functions called indirectly; ----------------------------------------. If a function is called indirectly, the return jump table is constructed for the; equivalence class of functions instead of a single function. Cross-DSO calls; ---------------; Consider two instrumented DSOs, `A` and `B`. `A` defines `f()` and `B` calls it. This case will be handled similarly to the cross-DSO scheme using the slow path callback. Non-goals; ---------. RCFI does not protect `RET` instructions:; * in non-instrumented DSOs,; * in instrumented DSOs for functions that are called from non-instrumented DSOs,; * embedded into other instructions (e.g. `0f4fc3 cmovg %ebx,%eax`). .. _SafeStack: https://clang.llvm.org/docs/SafeStack.html; .. _RFG: https://xlab.tencent.com/en/2016/11/02/return-flow-guard; .. _Intel CET: https://software.intel.com/en-us/blogs/2016/06/09/intel-release-new-technolo",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ControlFlowIntegrityDesign.rst:26740,load,load,26740,interpreter/llvm-project/clang/docs/ControlFlowIntegrityDesign.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ControlFlowIntegrityDesign.rst,1,['load'],['load']
Performance,"ons cannot overlap. Also, overlapping regions; cannot have the same name. There is no support for marking regions from high-level source code, like C or; C++. As a workaround, inline assembly directives may be used:. .. code-block:: c++. int foo(int a, int b) {; __asm volatile(""# LLVM-MCA-BEGIN foo"":::""memory"");; a += 42;; __asm volatile(""# LLVM-MCA-END"":::""memory"");; a *= b;; return a;; }. However, this interferes with optimizations like loop vectorization and may have; an impact on the code generated. This is because the ``__asm`` statements are; seen as real code having important side effects, which limits how the code; around them can be transformed. If users want to make use of inline assembly; to emit markers, then the recommendation is to always verify that the output; assembly is equivalent to the assembly generated in the absence of markers.; The `Clang options to emit optimization reports <https://clang.llvm.org/docs/UsersManual.html#options-to-emit-optimization-reports>`_; can also help in detecting missed optimizations. INSTRUMENT REGIONS; ------------------. An InstrumentRegion describes a region of assembly code guarded by; special LLVM-MCA comment directives. .. code-block:: none. # LLVM-MCA-<INSTRUMENT_TYPE> <data>; ... ## asm. where `INSTRUMENT_TYPE` is a type defined by the target and expects; to use `data`. A comment starting with substring `LLVM-MCA-<INSTRUMENT_TYPE>`; brings data into scope for llvm-mca to use in its analysis for; all following instructions. If a comment with the same `INSTRUMENT_TYPE` is found later in the; instruction list, then the original InstrumentRegion will be; automatically ended, and a new InstrumentRegion will begin. If there are comments containing the different `INSTRUMENT_TYPE`,; then both data sets remain available. In contrast with an AnalysisRegion,; an InstrumentRegion does not need a comment to end the region. Comments that are prefixed with `LLVM-MCA-` but do not correspond to; a valid `INSTRUMENT_TYPE` for t",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst:10673,optimiz,optimization-reports,10673,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,2,['optimiz'],"['optimization-reports', 'optimizations']"
Performance,"ons of different CUs. It ensures a previous vector; memory operation has completed before executing a subsequent vector memory; or LDS operation and so can be used to meet the requirements of acquire and; release.; * An L2 cache can be kept coherent with other L2 caches by using the MTYPE RW; (read-write) for memory local to the L2, and MTYPE NC (non-coherent) with; the PTE C-bit set for memory not local to the L2. * Any local memory cache lines will be automatically invalidated by writes; from CUs associated with other L2 caches, or writes from the CPU, due to; the cache probe caused by the PTE C-bit.; * XGMI accesses from the CPU to local memory may be cached on the CPU.; Subsequent access from the GPU will automatically invalidate or writeback; the CPU cache due to the L2 probe filter.; * To ensure coherence of local memory writes of CUs with different L1 caches; in the same agent a ``buffer_wbl2`` is required. It does nothing if the; agent is configured to have a single L2, or will writeback dirty L2 cache; lines if configured to have multiple L2 caches.; * To ensure coherence of local memory writes of CUs in different agents a; ``buffer_wbl2 sc1`` is required. It will writeback dirty L2 cache lines.; * To ensure coherence of local memory reads of CUs with different L1 caches; in the same agent a ``buffer_inv sc1`` is required. It does nothing if the; agent is configured to have a single L2, or will invalidate non-local L2; cache lines if configured to have multiple L2 caches.; * To ensure coherence of local memory reads of CUs in different agents a; ``buffer_inv sc0 sc1`` is required. It will invalidate non-local L2 cache; lines if configured to have multiple L2 caches. * PCIe access from the GPU to the CPU can be kept coherent by using the MTYPE; UC (uncached) which bypasses the L2. Scalar memory operations are only used to access memory that is proven to not; change during the execution of the kernel dispatch. This includes constant; address space and global a",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:288992,cache,cache,288992,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['cache'],"['cache', 'caches']"
Performance,"ons). ``aarch64_pstate_za_new``; is used for functions with ``__attribute__((arm_new_za))``. ``aarch64_pstate_za_shared``; is used for functions with ``__attribute__((arm_shared_za))``. ``aarch64_pstate_za_preserved``; is used for functions with ``__attribute__((arm_preserves_za))``. ``aarch64_expanded_pstate_za``; is used for functions with ``__attribute__((arm_new_za))``. Clang must ensure that the above attributes are added both to the; function's declaration/definition as well as to their call-sites. This is; important for calls to attributed function pointers, where there is no; definition or declaration available. 2. Handling PSTATE.SM; =====================. When changing PSTATE.SM the execution of FP/vector operations may be transferred; to another processing element. This has three important implications:. * The runtime SVE vector length may change. * The contents of FP/AdvSIMD/SVE registers are zeroed. * The set of allowable instructions changes. This leads to certain restrictions on IR and optimizations. For example, it; is undefined behaviour to share vector-length dependent state between functions; that may operate with different values for PSTATE.SM. Front-ends must honour; these restrictions when generating LLVM IR. Even though the runtime SVE vector length may change, for the purpose of LLVM IR; and almost all parts of CodeGen we can assume that the runtime value for; ``vscale`` does not. If we let the compiler insert the appropriate ``smstart``; and ``smstop`` instructions around call boundaries, then the effects on SVE; state can be mitigated. By limiting the state changes to a very brief window; around the call we can control how the operations are scheduled and how live; values remain preserved between state transitions. In order to control PSTATE.SM at this level of granularity, we use function and; callsite attributes rather than intrinsics. Restrictions on attributes; --------------------------. * It is undefined behaviour to pass or return (p",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AArch64SME.rst:2157,optimiz,optimizations,2157,interpreter/llvm-project/llvm/docs/AArch64SME.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AArch64SME.rst,1,['optimiz'],['optimizations']
Performance,"ons. Given:. .. code-block:: c. typedef int (*pointerToFunctionThatReturnsIntWithCharArg)(char);; pointerToFunctionThatReturnsIntWithCharArg functionPointer;; ^ pointerToFunctionThatReturnsIntWithCharArg (float x) { return functionPointer; }. and:. .. code-block:: c. ^ int ((*)(float x))(char) { return functionPointer; }. are equivalent expressions, as is:. .. code-block:: c. ^(float x) { return functionPointer; }. [returnfunctionptr.c]. The compound statement body establishes a new lexical scope within; that of its parent. Variables used within the scope of the compound; statement are bound to the Block in the normal manner with the; exception of those in automatic (stack) storage. Thus one may access; functions and global variables as one would expect, as well as static; local variables. [testme]. Local automatic (stack) variables referenced within the compound; statement of a Block are imported and captured by the Block as const; copies. The capture (binding) is performed at the time of the Block; literal expression evaluation. The compiler is not required to capture a variable if it can prove; that no references to the variable will actually be evaluated.; Programmers can force a variable to be captured by referencing it in a; statement at the beginning of the Block, like so:. .. code-block:: c. (void) foo;. This matters when capturing the variable has side-effects, as it can; in Objective-C or C++. The lifetime of variables declared in a Block is that of a function;; each activation frame contains a new copy of variables declared within; the local scope of the Block. Such variable declarations should be; allowed anywhere [testme] rather than only when C99 parsing is; requested, including for statements. [testme]. Block literal expressions may occur within Block literal expressions; (nest) and all variables captured by any nested blocks are implicitly; also captured in the scopes of their enclosing Blocks. A Block literal expression may be used as the initializa",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/BlockLanguageSpec.rst:4695,perform,performed,4695,interpreter/llvm-project/clang/docs/BlockLanguageSpec.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/BlockLanguageSpec.rst,1,['perform'],['performed']
Performance,"ons; ===========. Marking Functions as Kernels; ----------------------------. In PTX, there are two types of functions: *device functions*, which are only; callable by device code, and *kernel functions*, which are callable by host; code. By default, the back-end will emit device functions. Metadata is used to; declare a function as a kernel function. This metadata is attached to the; ``nvvm.annotations`` named metadata object, and has the following format:. .. code-block:: text. !0 = !{<function-ref>, metadata !""kernel"", i32 1}. The first parameter is a reference to the kernel function. The following; example shows a kernel function calling a device function in LLVM IR. The; function ``@my_kernel`` is callable from host code, but ``@my_fmad`` is not. .. code-block:: llvm. define float @my_fmad(float %x, float %y, float %z) {; %mul = fmul float %x, %y; %add = fadd float %mul, %z; ret float %add; }. define void @my_kernel(float* %ptr) {; %val = load float, float* %ptr; %ret = call float @my_fmad(float %val, float %val, float %val); store float %ret, float* %ptr; ret void; }. !nvvm.annotations = !{!1}; !1 = !{void (float*)* @my_kernel, !""kernel"", i32 1}. When compiled, the PTX kernel functions are callable by host-side code. .. _address_spaces:. Address Spaces; --------------. The NVPTX back-end uses the following address space mapping:. ============= ======================; Address Space Memory Space; ============= ======================; 0 Generic; 1 Global; 2 Internal Use; 3 Shared; 4 Constant; 5 Local; ============= ======================. Every global variable and pointer type is assigned to one of these address; spaces, with 0 being the default address space. Intrinsics are provided which; can be used to convert pointers between the generic and non-generic address; spaces. As an example, the following IR will define an array ``@g`` that resides in; global device memory. .. code-block:: llvm. @g = internal addrspace(1) global [4 x i32] [ i32 0, i32 1, i32 2, i32 3",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:1690,load,load,1690,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,1,['load'],['load']
Performance,"ons; to global and local; have completed; before performing; the atomicrmw that; is being released. 3. buffer/global/flat_atomic sc1=1; atomicrmw release - system - global 1. buffer_wbl2 sc0=1 sc1=1; - generic; - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at system scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to memory and the L2; writeback have; completed before; performing the; store that is being; released. 3. buffer/global/flat_atomic; sc0=1 sc1=1; fence release - singlethread *none* *none*; - wavefront; fence release - workgroup *none* 1. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate. If; fence had an; address space then; set to address; space of OpenCL; fence flag, or to; generic if both; local and global; flags are; specified.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/; load atomic/store atomic/; atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Must happen before; any following store; a",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:312624,perform,performing,312624,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['perform'],['performing']
Performance,"onservatively; always generate; (see comment for; previous fence).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - Must happen before; the following; buffer_wbinvl1_vol.; - Ensures that the; fence-paired atomic; has completed; before invalidating; the; cache. Therefore; any following; locations read must; be no older than; the value read by; the; fence-paired-atomic. 2. buffer_wbinvl1_vol. - Must happen before any; following global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. **Release Atomic**; ------------------------------------------------------------------------------------; store atomic release - singlethread - global 1. buffer/global/ds/flat_store; - wavefront - local; - generic; store atomic release - workgroup - global 1. s_waitcnt lgkmcnt(0); - generic; - If OpenCL, omit.; - Must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; store.; - Ensures that all; memory operations; to local have; completed before; performing the; store that is being; released. 2. buffer/global/flat_store; store atomic release - workgroup - local 1. ds_store; store atomic release - agent - global 1. s_waitcnt lgkmcnt(0) &; - system - generic vmcnt(0). - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_w",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:218781,load,load,218781,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"onservatively; always generate; (see comment for; previous fence).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - Must happen before; the following; buffer_wbinvl1_vol.; - Ensures that the; fence-paired atomic; has completed; before invalidating; the; cache. Therefore; any following; locations read must; be no older than; the value read by; the; fence-paired-atomic. 2. buffer_wbinvl1_vol. - Must happen before any; following global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. fence acquire - system *none* 1. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate; (see comment for; previous fence).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unord",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:255121,load,load,255121,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"onst double *x, const double *p) calls now TF1::InitArgs in the case of CINT functions.; Fixed a bug in using the TF1::GetMinimum(), TF1::GetMaximum(), TF1::GetMinimumX, TF1::GetMaximumX, TF1::GetX with default arguments. ; Fixed a bug when copying functions obtained from member functions of interpreted classes; . THStack. In THStack::Paint() replace; fHistogram->Paint(""axissame""); by; gPad->RedrawAxis(); in order to fix the bug described here:; https://savannah.cern.ch/bugs/?41423 .; The simple following macro was enough to show the problem:. {; TH1D h(""h"", ""h"", 10., 0., 1.); h.Fill(.5);; THStack s(""s"", ""s""); s.Add(&h);; TCanvas canvas(""canvas"");; frame = canvas.DrawFrame(-1., 0., 2., 2.);; frame.SetLabelSize(0.05, ""XY"");; frame.Draw(); s.Draw(""same"");; }. Make the data member fHistogram persistent in order to save the; axis attributes which may have been changed during a root session (like,; for instance, the axis titles).; When a THStack is drawn with the option ""pads"", the number of lines is; now optimized to make sure there is no empty line. . TUnfold. Introduces this new class for solving inverse problems:. data histograms with Gaussian errors are decomposed into; several template distributions (""generator level"" bins). The result are new normalisation constants for the template; distributions (the unfolded ""generator level"" distribution). The solution can be tuned by properly adjusting the; regularisation parameter tau. A standard method, the L-curve scan is; implemented to help finding a good choice of this parameter. Two example tutorials are included to show the usage of this class: tutorials/math/testUnfiold1.C and tutorials/math/testUnfiold2.C. FitPanel; Add a new revised version of the Fit Panel with the following functionality:. Add support now for fitting, in addition to the TH1 and TGraph; also for TH2, TH3, TMultiGraph and TGraph2D and TTree (with un-binned; fits); Add possibility to select the data object directly from the Fit; panel. The Fit Panel",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/hist/doc/v522/index.html:7300,optimiz,optimized,7300,hist/doc/v522/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/hist/doc/v522/index.html,2,['optimiz'],['optimized']
Performance,"onstructed. If for whatever reason it is necessary to change the; system of units later, this is feasible disabling the otherwise fatal exception:. ``` {.cpp}; TGeoManager::LockDefaultUnits(kFALSE);; ```. followed later by a corresponding call to again lock the system of units:. ``` {.cpp}; TGeoManager::LockDefaultUnits(kTRUE);; ```. \anchor GP01; ## Geometry Creation. A given geometry can be built in various ways, but one has to follow; some mandatory steps. Even if we might use some terms that will be; explained later, here are few general rules:. - Volumes need media and shapes in order to be created.; - Both containers and contained volumes must be created before linking; them together, and the relative transformation matrix must be; provided.; - Any volume have to be positioned somewhere otherwise it will not be; considered as part of the geometry.; - Visibility or tracking properties of volumes can be provided both at; build time or after geometry is closed, but global visualization; settings (see section: ""The Drawing Package"") should not be provided; at build time, otherwise the drawing package will be loaded. There is also a list of specific rules:. - Positioned volumes should not extrude their container or intersect; with others within this unless it is specified (see section:; Overlapping Volumes).; - The top volume (containing all geometry trees) must be specified; before closing the geometry and must not be positioned - it; represents the global reference frame.; - After building the full geometry tree, the geometry must be closed; (see the method **`TGeoManager::CloseGeometry()`**). Voxelization; can be redone per volume after this process. The list is much bigger and we will describe in more detail the geometry; creation procedure in the following sections. Provided that geometry was; successfully built and closed, the **`TGeoManager`** class will register; itself to ROOT and the logical/physical structures will become; immediately browsable. \anchor ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md:17828,load,loaded,17828,geom/geom/doc/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md,1,['load'],['loaded']
Performance,"onvention. We could define a new ABI and/or calling convention to explicitly pass the; predicate state in and out of functions. This may be interesting if none of the; alternatives have adequate performance, but it makes deployment and adoption; dramatically more complex, and potentially infeasible. ## High-Level Alternative Mitigation Strategies. There are completely different alternative approaches to mitigating variant 1; attacks. [Most](https://lwn.net/Articles/743265/); [discussion](https://lwn.net/Articles/744287/) so far focuses on mitigating; specific known attackable components in the Linux kernel (or other kernels) by; manually rewriting the code to contain an instruction sequence that is not; vulnerable. For x86 systems this is done by either injecting an `lfence`; instruction along the code path which would leak data if executed speculatively; or by rewriting memory accesses to have branch-less masking to a known safe; region. On Intel systems, `lfence` [will prevent the speculative load of secret; data](https://newsroom.intel.com/wp-content/uploads/sites/11/2018/01/Intel-Analysis-of-Speculative-Execution-Side-Channels.pdf).; On AMD systems `lfence` is currently a no-op, but can be made; dispatch-serializing by setting an MSR, and thus preclude misspeculation of the; code path ([mitigation G-2 +; V1-1](https://developer.amd.com/wp-content/resources/Managing-Speculation-on-AMD-Processors.pdf)). However, this relies on finding and enumerating all possible points in code; which could be attacked to leak information. While in some cases static; analysis is effective at doing this at scale, in many cases it still relies on; human judgement to evaluate whether code might be vulnerable. Especially for; software systems which receive less detailed scrutiny but remain sensitive to; these attacks, this seems like an impractical security model. We need an; automatic and systematic mitigation strategy. ### Automatic `lfence` on Conditional Edges. A natural way to sca",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md:44134,load,load,44134,interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,1,['load'],['load']
Performance,"ooHashTable`, `RooNameSet`, `RooSetPair`, and `RooList` are removed. Please use STL container classes instead, like `std::unordered_map`, `std::set`, and `std::vector`.; - The `RooFit::FitOptions(const char*)` command to steer [RooAbsPdf::fitTo()](https://root.cern.ch/doc/v628/classRooAbsPdf.html) with an option string was removed. This way of configuring the fit was deprecated since at least since ROOT 5.02.; Subsequently, the `RooMinimizer::fit(const char*)` function and the [RooMCStudy](https://root.cern.ch/doc/v628/classRooMCStudy.html) constructor that takes an option string were removed as well.; - The overload of `RooAbsData::createHistogram` that takes integer parameters for the bin numbers is now deprecated and will be removed in ROOT 6.30.; This was done to avoid confusion with inconsistent behavior when compared to other `createHistogram` overloads.; Please use the verson of `createHistogram` that takes RooFit command arguments.; - The `RooAbsData::valid()` method to cache valid entries in the variable range; was removed. It was not implemented in RooDataSet, so it never worked as; intended. Related to it was the `RooDataHist::cacheValidEntries()` function, which is removed as well.; The preferred way to reduce RooFit datasets to subranges is [RooAbsData::reduce()](https://root.cern.ch/doc/v628/classRooAbsData.html#acfa7b31e5cd751eec1bc4e95d2796390).; - The longtime-deprecated `RooStats::HistFactory::EstimateSummary` class is removed, including the functions that use it. The information that it was meant to store is managed by the `RooStats::HistFactory::Measurement` object since many years.; - The `RooSuperCategory::MakeIterator()` function that was deprecated since 6.22 is now removed. Please use range-based loops to iterate over the category states.; - The `HybridCalculatorOriginal` and `HypoTestInverterOriginal` classes in RooStats that were deprecated for a very long time aleady are removed. Please use `HybridCalculator` and `HypoTestInverter`.; - The",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v628/index.md:3270,cache,cache,3270,README/ReleaseNotes/v628/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v628/index.md,1,['cache'],['cache']
Performance,"ooks like. You just need to run; the example and you will get the following picture that you can rotate; using the mouse; or you can zoom / move it around (see what the Help; menu of the GL window displays). ``` {.cpp}; % root rootgeom.C; ```. ![](pictures/020001B1.jpg). Now let us browse the hierarchy that was just created. Start a browser; and double-click on the item simple1 representing the; ***`gGeoManager`*** object. Note that right click opens the context menu; of the manager class where several global methods are available. ``` {.cpp}; root[] new TBrowser;; ```. ![](pictures/020001B2.jpg). The folders `Materials`, `Media` and `Local transformations` are in fact; the containers where the geometry manager stores the corresponding; objects. The `Illegal overlaps` folder is empty but can be filled after; performing a geometry validity check (see section: ""Checking the; Geometry""). If tracking is performed using **`TGeo`**, the folder; `Tracks` might contain user-defined tracks that can be; visualized/animated in the geometry context (see section: ""Creating and; Visualizing Tracks""). Since for the time being we are interested more in; the geometrical hierarchy, we will focus on the last two displayed items; `TOP `and `TOP_1`. These are the top volume and the corresponding top; node in the hierarchy. Double clicking on the `TOP` volume will unfold all different volumes; contained by the top volume. In the right panel, we will see all the; volumes contained by `TOP` (if the same is positioned 4 times we will; get 4 identical items). This rule will apply to any clicked volume in; the hierarchy. Note that right clicking a volume item activates the; volume context menu containing several specific methods. We will call; the volume hierarchy developed in this way as the; `logical geometry graph`. The volume objects are nodes inside this graph; and the same volume can be accessed starting from different branches. On the other hand, the real geometrical objects that are s",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md:9357,perform,performed,9357,documentation/users-guide/Geometry.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md,1,['perform'],['performed']
Performance,"ookup within a; given declaration (e.g., find the member named ``x`` in a structure) and; iterate over the declarations stored within a context (e.g., iterate over all; of the fields of a structure for structure layout). In Clang's AST file format, deserializing a declaration that is a; ``DeclContext`` is a separate operation from deserializing all of the; declarations stored within that declaration context. Therefore, Clang will; deserialize the translation unit declaration without deserializing the; declarations within that translation unit. When required, the declarations; stored within a declaration context will be deserialized. There are two; representations of the declarations within a declaration context, which; correspond to the name-lookup and iteration behavior described above:. * When the front end performs name lookup to find a name ``x`` within a given; declaration context (for example, during semantic analysis of the expression; ``p->x``, where ``p``'s type is defined in the precompiled header), Clang; refers to an on-disk hash table that maps from the names within that; declaration context to the declaration IDs that represent each visible; declaration with that name. The actual declarations will then be; deserialized to provide the results of name lookup.; * When the front end performs iteration over all of the declarations within a; declaration context, all of those declarations are immediately; de-serialized. For large declaration contexts (e.g., the translation unit),; this operation is expensive; however, large declaration contexts are not; traversed in normal compilation, since such a traversal is unnecessary.; However, it is common for the code generator and semantic analysis to; traverse declaration contexts for structs, classes, unions, and; enumerations, although those contexts contain relatively few declarations in; the common case. Statements and Expressions; ^^^^^^^^^^^^^^^^^^^^^^^^^^. Statements and expressions are stored in the AST file",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/PCHInternals.rst:15188,perform,performs,15188,interpreter/llvm-project/clang/docs/PCHInternals.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/PCHInternals.rst,1,['perform'],['performs']
Performance,"ookup; performance. See the subsection `How to manage symbol strings`_. 4. IR layers require ThreadSafeModule instances, rather than; std::unique_ptr<Module>s. ThreadSafeModule is a wrapper that ensures that; Modules that use the same LLVMContext are not accessed concurrently.; See `How to use ThreadSafeModule and ThreadSafeContext`_. 5. Symbol lookup is no longer handled by layers. Instead, there is a; ``lookup`` method on JITDylib that takes a list of JITDylibs to scan. .. code-block:: c++. ExecutionSession ES;; JITDylib &JD1 = ...;; JITDylib &JD2 = ...;. auto Sym = ES.lookup({&JD1, &JD2}, ES.intern(""_main""));. 6. The removeModule/removeObject methods are replaced by; ``ResourceTracker::remove``.; See the subsection `How to remove code`_. For code examples and suggestions of how to use the ORCv2 APIs, please see; the section `How-tos`_. How-tos; =======. How to manage symbol strings; ----------------------------. Symbol strings in ORC are uniqued to improve lookup performance, reduce memory; overhead, and allow symbol names to function as efficient keys. To get the; unique ``SymbolStringPtr`` for a string value, call the; ``ExecutionSession::intern`` method:. .. code-block:: c++. ExecutionSession ES;; /// ...; auto MainSymbolName = ES.intern(""main"");. If you wish to perform lookup using the C/IR name of a symbol you will also; need to apply the platform linker-mangling before interning the string. On; Linux this mangling is a no-op, but on other platforms it usually involves; adding a prefix to the string (e.g. '_' on Darwin). The mangling scheme is; based on the DataLayout for the target. Given a DataLayout and an; ExecutionSession, you can create a MangleAndInterner function object that; will perform both jobs for you:. .. code-block:: c++. ExecutionSession ES;; const DataLayout &DL = ...;; MangleAndInterner Mangle(ES, DL);. // ... // Portable IR-symbol-name lookup:; auto Sym = ES.lookup({&MainJD}, Mangle(""main""));. How to create JITDylibs and set up linkage rel",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:22086,perform,performance,22086,interpreter/llvm-project/llvm/docs/ORCv2.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst,1,['perform'],['performance']
Performance,"ool chain for C family; programming languages. In order to assemble a complete toolchain,; additional tools and runtime libraries are required. Clang is designed; to interoperate with existing tools and libraries for its target; platforms, and the LLVM project provides alternatives for a number; of these components. This document describes the required and optional components in a; complete toolchain, where to find them, and the supported versions; and limitations of each option. .. warning::. This document currently describes Clang configurations on POSIX-like; operating systems with the GCC-compatible ``clang`` driver. When; targeting Windows with the MSVC-compatible ``clang-cl`` driver, some; of the details are different. Tools; =====. .. FIXME: Describe DWARF-related tools. A complete compilation of C family programming languages typically; involves the following pipeline of tools, some of which are omitted; in some compilations:. * **Preprocessor**: This performs the actions of the C preprocessor:; expanding #includes and #defines.; The ``-E`` flag instructs Clang to stop after this step. * **Parsing**: This parses and semantically analyzes the source language and; builds a source-level intermediate representation (""AST""), producing a; :ref:`precompiled header (PCH) <usersmanual-precompiled-headers>`,; preamble, or; :doc:`precompiled module file (PCM) <Modules>`,; depending on the input.; The ``-precompile`` flag instructs Clang to stop after this step. This is; the default when the input is a header file. * **IR generation**: This converts the source-level intermediate representation; into an optimizer-specific intermediate representation (IR); for Clang, this; is LLVM IR.; The ``-emit-llvm`` flag instructs Clang to stop after this step. If combined; with ``-S``, Clang will produce textual LLVM IR; otherwise, it will produce; LLVM IR bitcode. * **Compiler backend**: This converts the intermediate representation; into target-specific assembly code.; The ``-S`` ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/Toolchain.rst:1179,perform,performs,1179,interpreter/llvm-project/clang/docs/Toolchain.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/Toolchain.rst,1,['perform'],['performs']
Performance,"oolkit. - `libTree` is the TTree object container system. - `libTreePlayer` is the TTree drawing classes. - `libTreeViewer` is the graphical TTree query interface. #### Library Dependencies. ![ROOT libraries dependencies](pictures/03000005.png). The libraries are designed and organized to minimize dependencies,; such that you can load just enough code for the task at hand rather; than having to load all libraries or one monolithic chunk. The core; library (`libCore.so`) contains the essentials; it is a part of all; ROOT applications. In the Figure 1-2 you see that libCore.so is made; up of base classes, container classes, meta information classes,; operating system specific classes, and the ZIP algorithm used for; compression of the ROOT files. The Cling library (`libCling.so`) is also needed in all ROOT; applications, and even by `libCore`. A; program referencing only **`TObject`** only needs `libCore`;; `libCling` will be opened automatically. To add the ability to read and write; ROOT objects one also has to load `libRIO`. As one would expect, none of that; depends on graphics or the GUI. Library dependencies have different consequences; depending on whether; you try to build a binary, or you just try to access a class that is; defined in a library. #### Linktime Library Dependencies. When building your own executable you will have to link against the; libraries that contain the classes you use. The ROOT reference guide; states the library a class is reference guide defined in. Almost all; relevant classes can be found in libraries returned by; `root-config -glibs`; the graphics libraries are retuned by; `root-config --libs`. These commands are commonly used in `Makefiles`.; Using `root-config` instead of enumerating the libraries by hand; allows you to link them in a platform independent way. Also, if ROOT; library names change you will not need to change your Makefile. A batch program that does not have a graphic display, which creates,; fills, and saves histog",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Introduction.md:16802,load,load,16802,documentation/users-guide/Introduction.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Introduction.md,1,['load'],['load']
Performance,ools-extra/clang-tidy/performance/MoveConstructorInitCheck.h; clang-tools-extra/clang-tidy/performance/NoAutomaticMoveCheck.cpp; clang-tools-extra/clang-tidy/performance/NoAutomaticMoveCheck.h; clang-tools-extra/clang-tidy/performance/NoexceptMoveConstructorCheck.cpp; clang-tools-extra/clang-tidy/performance/NoexceptMoveConstructorCheck.h; clang-tools-extra/clang-tidy/performance/NoIntToPtrCheck.cpp; clang-tools-extra/clang-tidy/performance/NoIntToPtrCheck.h; clang-tools-extra/clang-tidy/performance/PerformanceTidyModule.cpp; clang-tools-extra/clang-tidy/performance/TriviallyDestructibleCheck.cpp; clang-tools-extra/clang-tidy/performance/TriviallyDestructibleCheck.h; clang-tools-extra/clang-tidy/performance/TypePromotionInMathFnCheck.cpp; clang-tools-extra/clang-tidy/performance/TypePromotionInMathFnCheck.h; clang-tools-extra/clang-tidy/performance/UnnecessaryCopyInitialization.cpp; clang-tools-extra/clang-tidy/performance/UnnecessaryValueParamCheck.cpp; clang-tools-extra/clang-tidy/performance/UnnecessaryValueParamCheck.h; clang-tools-extra/clang-tidy/plugin/ClangTidyPlugin.cpp; clang-tools-extra/clang-tidy/portability/PortabilityTidyModule.cpp; clang-tools-extra/clang-tidy/portability/RestrictSystemIncludesCheck.cpp; clang-tools-extra/clang-tidy/portability/SIMDIntrinsicsCheck.cpp; clang-tools-extra/clang-tidy/readability/AvoidConstParamsInDecls.h; clang-tools-extra/clang-tidy/readability/BracesAroundStatementsCheck.cpp; clang-tools-extra/clang-tidy/readability/BracesAroundStatementsCheck.h; clang-tools-extra/clang-tidy/readability/ConstReturnTypeCheck.cpp; clang-tools-extra/clang-tidy/readability/ContainerContainsCheck.cpp; clang-tools-extra/clang-tidy/readability/ContainerContainsCheck.h; clang-tools-extra/clang-tidy/readability/ContainerDataPointerCheck.cpp; clang-tools-extra/clang-tidy/readability/ContainerDataPointerCheck.h; clang-tools-extra/clang-tidy/readability/ContainerSizeEmptyCheck.h; clang-tools-extra/clang-tidy/readability/ConvertMemberFunctionsToStat,MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/tools/clang-formatted-files.txt:66314,perform,performance,66314,interpreter/llvm-project/clang/docs/tools/clang-formatted-files.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/tools/clang-formatted-files.txt,1,['perform'],['performance']
Performance,"oop attributes the epilogue will have. The; epilogue is not vectorized and is executed when either the vectorized; loop is not known to preserve semantics (because e.g., it processes two; arrays that are found to alias by a runtime check) or for the last; iterations that do not fill a complete set of vector lanes. See; :ref:`Transformation Metadata <transformation-metadata>` for details. '``llvm.loop.vectorize.followup_all``' Metadata; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Attributes in the metadata will be added to both the vectorized and; epilogue loop.; See :ref:`Transformation Metadata <transformation-metadata>` for details. '``llvm.loop.unroll``'; ^^^^^^^^^^^^^^^^^^^^^^. Metadata prefixed with ``llvm.loop.unroll`` are loop unrolling; optimization hints such as the unroll factor. ``llvm.loop.unroll``; metadata should be used in conjunction with ``llvm.loop`` loop; identification metadata. The ``llvm.loop.unroll`` metadata are only; optimization hints and the unrolling will only be performed if the; optimizer believes it is safe to do so. '``llvm.loop.unroll.count``' Metadata; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. This metadata suggests an unroll factor to the loop unroller. The; first operand is the string ``llvm.loop.unroll.count`` and the second; operand is a positive integer specifying the unroll factor. For; example:. .. code-block:: llvm. !0 = !{!""llvm.loop.unroll.count"", i32 4}. If the trip count of the loop is less than the unroll count the loop; will be partially unrolled. '``llvm.loop.unroll.disable``' Metadata; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. This metadata disables loop unrolling. The metadata has a single operand; which is the string ``llvm.loop.unroll.disable``. For example:. .. code-block:: llvm. !0 = !{!""llvm.loop.unroll.disable""}. '``llvm.loop.unroll.runtime.disable``' Metadata; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. This metadata disables runtime loop unrolling. The metadata has a single; operand which is the s",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:300985,optimiz,optimization,300985,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,3,"['optimiz', 'perform']","['optimization', 'optimizer', 'performed']"
Performance,"oop passes, new pass manager.; - Monthly, 2nd Tuesdays, 10.00am PT/7:00pm CET, for 30 minutes.; `ics <https://calendar.google.com/calendar/ical/c_pm6e7160iq7n5fcm1s6m3rjhh4%40group.calendar.google.com/public/basic.ics>`__; `gcal <https://calendar.google.com/calendar/embed?src=c_pm6e7160iq7n5fcm1s6m3rjhh4%40group.calendar.google.com>`__; - `GoogleMeet <https://meet.google.com/hhk-xpdj-gvx>`__; - English, Romanian; * - Aaron Ballman (he/him); - Clang internals; frontend attributes; clang-tidy; clang-query; AST matchers; - Monthly, 2nd Monday and 3rd Friday of the month at 10:00am Eastern and again at 2:00pm Eastern, for 60 minutes.; `ics <https://calendar.google.com/calendar/ical/npgke5dug0uliud0qapptmps58%40group.calendar.google.com/public/basic.ics>`__; `gcal <https://calendar.google.com/calendar/embed?src=npgke5dug0uliud0qapptmps58%40group.calendar.google.com>`__; - `GoogleMeet <https://meet.google.com/xok-iqne-gmi>`__; - English, Norwegian (not fluently); * - Johannes Doerfert (he/him); - OpenMP, LLVM-IR, interprocedural optimizations, Attributor, workshops, research, ...; - Every week, Wednesdays 9:30am (Pacific Time), for 1 hour.; `ics <https://drive.google.com/file/d/1E_QkRvirmdJzlXf2EKBUX-v8Xj7-eW3v/view?usp=sharing>`__; - `MS Teams <https://teams.microsoft.com/l/meetup-join/19%3ameeting_MTMxNzU4MWYtYzViNS00OTM2LWJmNWQtMjg5ZWFhNGVjNzgw%40thread.v2/0?context=%7b%22Tid%22%3a%22a722dec9-ae4e-4ae3-9d75-fd66e2680a63%22%2c%22Oid%22%3a%22885bda30-ce8e-46db-aa7e-15de0474831a%22%7d>`__; - English, German; * - Tobias Grosser; - General questions on how to contribute to LLVM/MLIR, Polly, Loop Optimization, FPL, Research in LLVM, PhD in CS, Summer of Code.; - Monthly, last Monday of the month at 18:00 London time (typically 9am PT), for 30 minutes.; - `Video Call <https://meet.grosser.science/LLVMOfficeHours>`__; - English, German, Spanish, French; * - Anastasia Stulova; - Clang internals for C/C++ language extensions and dialects, OpenCL, GPU, SPIR-V, how to contribute, w",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GettingInvolved.rst:12291,optimiz,optimizations,12291,interpreter/llvm-project/llvm/docs/GettingInvolved.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GettingInvolved.rst,1,['optimiz'],['optimizations']
Performance,"oops if; there are no instructions in the loop that modifies the memory loaded. * It uses mod/ref information to hoist function calls out of loops that do not; write to memory and are loop-invariant. * It uses alias information to promote memory objects that are loaded and stored; to in loops to live in a register instead. It can do this if there are no may; aliases to the loaded/stored memory location. The ``-argpromotion`` pass; ^^^^^^^^^^^^^^^^^^^^^^^^^^. The ``-argpromotion`` pass promotes by-reference arguments to be passed in; by-value instead. In particular, if pointer arguments are only loaded from it; passes in the value loaded instead of the address to the function. This pass; uses alias information to make sure that the value loaded from the argument; pointer is not modified between the entry of the function and any load of the; pointer. The ``-gvn``, ``-memcpyopt``, and ``-dse`` passes; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. These passes use AliasAnalysis information to reason about loads and stores. .. _the clients:. Clients for debugging and evaluation of implementations; -------------------------------------------------------. These passes are useful for evaluating the various alias analysis; implementations. You can use them with commands like:. .. code-block:: bash. % opt -ds-aa -aa-eval foo.bc -disable-output -stats. The ``-print-alias-sets`` pass; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. The ``-print-alias-sets`` pass is exposed as part of the ``opt`` tool to print; out the Alias Sets formed by the `AliasSetTracker`_ class. This is useful if; you're using the ``AliasSetTracker`` class. To use it, use something like:. .. code-block:: bash. % opt -ds-aa -print-alias-sets -disable-output. The ``-aa-eval`` pass; ^^^^^^^^^^^^^^^^^^^^^. The ``-aa-eval`` pass simply iterates through all pairs of pointers in a; function and asks an alias analysis whether or not the pointers alias. This; gives an indication of the precision of the alias analysis. Statist",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AliasAnalysis.rst:28820,load,loads,28820,interpreter/llvm-project/llvm/docs/AliasAnalysis.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AliasAnalysis.rst,1,['load'],['loads']
Performance,"oothNum (for regions with less signal). . Configuration of the PDF parameters from the option string moved to PDF class,; allowing the user to define all the PDF functionalities in every classifier; the PDF is used (i.e., also for the MVA PDFs). The reading of these variables; was removed from MethodBase and MethodLikelihood. This also allows improved ; (full) PDF configuration of MVA output via the ""CreateMvaPdf"" option.; (Work by Or Cohen, CERN & Weizmann); ; New generalisation methods:. ; MethodCompositeBase: combines more than one; classifier within one. MethodBoost: boosts/bags any classifier; type. A special booking procedure for it was added to; Factory class. MethodDT: a classifier composed of a single; decision tree, boosted using MethodBoost. Results are; compatible with BDT, but BDT remains the default for boosted; decision trees, because it has pruning (among other; additional features). . Other improvements . Improved handling of small Likelihood values such that; Likelihood performance increases in analyses with many variables; (~>10). Thanks to Ralph Schaefer (Bonn U.) for reporting this. Nicer plotting: custom variable titles and units can be; assigned in ""AddVariable"" call. Introduced the inverse transformation InverseTransform for; the variable transformations into the framework. While this is; not necessary for classification, it is necessary for; regression. The inverse transformation of the normalization; transformation has been implemented. Started to extend the variable transformations to the; regression targets as well. MethodCuts now produces the 'optimal-cut' histograms needed; by macro mvaeffs.C. (macro 5a of TMVAGui.C); ; MsgLogger can be silenced in order to prevent excess output; during boosting. Third dataset type added centrally (Training, Validation; and Testing). The validation data is split off the original; training data set. Update of GUI and other Macros according to the new; features of PDF and the addition of MethodBoost.; ; U",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tmva/doc/v524/index.html:5110,perform,performance,5110,tmva/doc/v524/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tmva/doc/v524/index.html,2,['perform'],['performance']
Performance,"op attributes the vectorized loop will; have. See :ref:`transformation-metadata` for details. '``llvm.loop.vectorize.followup_epilogue``' Metadata; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. This metadata defines which loop attributes the epilogue will have. The; epilogue is not vectorized and is executed when either the vectorized; loop is not known to preserve semantics (because e.g., it processes two; arrays that are found to alias by a runtime check) or for the last; iterations that do not fill a complete set of vector lanes. See; :ref:`Transformation Metadata <transformation-metadata>` for details. '``llvm.loop.vectorize.followup_all``' Metadata; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Attributes in the metadata will be added to both the vectorized and; epilogue loop.; See :ref:`Transformation Metadata <transformation-metadata>` for details. '``llvm.loop.unroll``'; ^^^^^^^^^^^^^^^^^^^^^^. Metadata prefixed with ``llvm.loop.unroll`` are loop unrolling; optimization hints such as the unroll factor. ``llvm.loop.unroll``; metadata should be used in conjunction with ``llvm.loop`` loop; identification metadata. The ``llvm.loop.unroll`` metadata are only; optimization hints and the unrolling will only be performed if the; optimizer believes it is safe to do so. '``llvm.loop.unroll.count``' Metadata; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. This metadata suggests an unroll factor to the loop unroller. The; first operand is the string ``llvm.loop.unroll.count`` and the second; operand is a positive integer specifying the unroll factor. For; example:. .. code-block:: llvm. !0 = !{!""llvm.loop.unroll.count"", i32 4}. If the trip count of the loop is less than the unroll count the loop; will be partially unrolled. '``llvm.loop.unroll.disable``' Metadata; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. This metadata disables loop unrolling. The metadata has a single operand; which is the string ``llvm.loop.unroll.disable``. For example:. .. code-block:: llvm",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:300784,optimiz,optimization,300784,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['optimiz'],['optimization']
Performance,"op attributes, a new ``MDNode``; must be created and assigned as the new ``llvm.loop`` metadata. Any; connection between the old ``MDNode`` and the loop is lost. The; ``llvm.loop`` node is also used as LoopID (``Loop::getLoopID()``), i.e.; the loop effectively gets a new identifier. For instance,; ``llvm.mem.parallel_loop_access`` references the LoopID. Therefore, if; the parallel access property is to be preserved after adding/removing; loop attributes, any ``llvm.mem.parallel_loop_access`` reference must be; updated to the new LoopID. Transformation Metadata Structure; =================================. Some attributes describe code transformations (unrolling, vectorizing,; loop distribution, etc.). They can either be a hint to the optimizer; that a transformation might be beneficial, instruction to use a specific; option, , or convey a specific request from the user (such as; ``#pragma clang loop`` or ``#pragma omp simd``). If a transformation is forced but cannot be carried-out for any reason,; an optimization-missed warning must be emitted. Semantic information; such as a transformation being safe (e.g.; ``llvm.mem.parallel_loop_access``) can be unused by the optimizer; without generating a warning. Unless explicitly disabled, any optimization pass may heuristically; determine whether a transformation is beneficial and apply it. If; metadata for another transformation was specified, applying a different; transformation before it might be inadvertent due to being applied on a; different loop or the loop not existing anymore. To avoid having to; explicitly disable an unknown number of passes, the attribute; ``llvm.loop.disable_nonforced`` disables all optional, high-level,; restructuring transformations. The following example avoids the loop being altered before being; vectorized, for instance being unrolled. .. code-block:: llvm. br i1 %exitcond, label %for.exit, label %for.header, !llvm.loop !0; ...; !0 = distinct !{!0, !1, !2}; !1 = !{!""llvm.loop.vectorize.enab",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TransformMetadata.rst:2563,optimiz,optimization-missed,2563,interpreter/llvm-project/llvm/docs/TransformMetadata.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TransformMetadata.rst,1,['optimiz'],['optimization-missed']
Performance,"op of our CompileLayer. We initialize our OptimizeLayer with a reference to; the ExecutionSession and output layer (standard practice for layers), along with; a *transform function*. For our transform function we supply our classes; optimizeModule static method. .. code-block:: c++. // ...; return cantFail(OptimizeLayer.addModule(std::move(M),; std::move(Resolver)));; // ... Next we need to update our addModule method to replace the call to; ``CompileLayer::add`` with a call to ``OptimizeLayer::add`` instead. .. code-block:: c++. static Expected<ThreadSafeModule>; optimizeModule(ThreadSafeModule M, const MaterializationResponsibility &R) {; // Create a function pass manager.; auto FPM = std::make_unique<legacy::FunctionPassManager>(M.get());. // Add some optimizations.; FPM->add(createInstructionCombiningPass());; FPM->add(createReassociatePass());; FPM->add(createGVNPass());; FPM->add(createCFGSimplificationPass());; FPM->doInitialization();. // Run the optimizations over all functions in the module being added to; // the JIT.; for (auto &F : *M); FPM->run(F);. return M;; }. At the bottom of our JIT we add a private method to do the actual optimization:; *optimizeModule*. This function takes the module to be transformed as input (as; a ThreadSafeModule) along with a reference to a reference to a new class:; ``MaterializationResponsibility``. The MaterializationResponsibility argument; can be used to query JIT state for the module being transformed, such as the set; of definitions in the module that JIT'd code is actively trying to call/access.; For now we will ignore this argument and use a standard optimization; pipeline. To do this we set up a FunctionPassManager, add some passes to it, run; it over every function in the module, and then return the mutated module. The; specific optimizations are the same ones used in `Chapter 4 <LangImpl04.html>`_; of the ""Implementing a language with LLVM"" tutorial series. Readers may visit; that chapter for a more in-depth disc",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:4792,optimiz,optimizations,4792,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,1,['optimiz'],['optimizations']
Performance,"op1=99 op2=55/>; <Remark hotness codeid=7 abbrevid=6 op0=999999999/>; <Argument with debug location codeid=8 abbrevid=7 op0=4 op1=5 op2=6 op3=11 op4=66/>; </Remark>. opt-viewer; ==========. The ``opt-viewer`` directory contains a collection of tools that visualize and; summarize serialized remarks. The tools only support the ``yaml`` format. .. _optviewerpy:. opt-viewer.py; -------------. Output a HTML page which gives visual feedback on compiler interactions with; your program. :Examples:. ::. $ opt-viewer.py my_yaml_file.opt.yaml. ::. $ opt-viewer.py my_build_dir/. opt-stats.py; ------------. Output statistics about the optimization remarks in the input set. :Example:. ::. $ opt-stats.py my_yaml_file.opt.yaml. Total number of remarks 3. Top 10 remarks by pass:; inline 33%; asm-printer 33%; prologepilog 33%. Top 10 remarks:; asm-printer/InstructionCount 33%; inline/NoDefinition 33%; prologepilog/StackSize 33%. opt-diff.py; -----------. Produce a new YAML file which contains all of the changes in optimizations; between two YAML files. Typically, this tool should be used to do diffs between:. * new compiler + fixed source vs old compiler + fixed source; * fixed compiler + new source vs fixed compiler + old source. This diff file can be displayed using :ref:`opt-viewer.py <optviewerpy>`. :Example:. ::. $ opt-diff.py my_opt_yaml1.opt.yaml my_opt_yaml2.opt.yaml -o my_opt_diff.opt.yaml; $ opt-viewer.py my_opt_diff.opt.yaml. .. _remarkssection:. Emitting remark diagnostics in the object file; ==============================================. A section containing metadata on remark diagnostics will be emitted for the; following formats:. * ``yaml-strtab``; * ``bitstream``. This can be overridden by using the flag ``-remarks-section=<bool>``. The section is named:. * ``__LLVM,__remarks`` (MachO). C API; =====. LLVM provides a library that can be used to parse remarks through a shared; library named ``libRemarks``. The typical usage through the C API is like the following:. ..",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Remarks.rst:15597,optimiz,optimizations,15597,interpreter/llvm-project/llvm/docs/Remarks.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Remarks.rst,1,['optimiz'],['optimizations']
Performance,"opAnalysisManager>();; TheFAM = std::make_unique<FunctionAnalysisManager>();; TheCGAM = std::make_unique<CGSCCAnalysisManager>();; TheMAM = std::make_unique<ModuleAnalysisManager>();; ThePIC = std::make_unique<PassInstrumentationCallbacks>();; TheSI = std::make_unique<StandardInstrumentations>(*TheContext,; /*DebugLogging*/ true);; TheSI->registerCallbacks(*ThePIC, TheMAM.get());; ... After initializing the global module ``TheModule`` and the FunctionPassManager,; we need to initialize other parts of the framework. The four AnalysisManagers; allow us to add analysis passes that run across the four levels of the IR; hierarchy. PassInstrumentationCallbacks and StandardInstrumentations are; required for the pass instrumentation framework, which allows developers to; customize what happens between passes. Once these managers are set up, we use a series of ""addPass"" calls to add a; bunch of LLVM transform passes:. .. code-block:: c++. // Add transform passes.; // Do simple ""peephole"" optimizations and bit-twiddling optzns.; TheFPM->addPass(InstCombinePass());; // Reassociate expressions.; TheFPM->addPass(ReassociatePass());; // Eliminate Common SubExpressions.; TheFPM->addPass(GVNPass());; // Simplify the control flow graph (deleting unreachable blocks, etc).; TheFPM->addPass(SimplifyCFGPass());. In this case, we choose to add four optimization passes.; The passes we choose here are a pretty standard set; of ""cleanup"" optimizations that are useful for a wide variety of code. I won't; delve into what they do but, believe me, they are a good starting place :). Next, we register the analysis passes used by the transform passes. .. code-block:: c++. // Register analysis passes used in these transform passes.; PassBuilder PB;; PB.registerModuleAnalyses(*TheMAM);; PB.registerFunctionAnalyses(*TheFAM);; PB.crossRegisterProxies(*TheLAM, *TheFAM, *TheCGAM, *TheMAM);; }. Once the PassManager is set up, we need to make use of it. We do this by; running it after our newly created fun",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:7072,optimiz,optimizations,7072,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,1,['optimiz'],['optimizations']
Performance,"ope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global have; completed before; performing the; atomicrmw that is; being released. 3. buffer/global_atomic; 4. s_waitcnt vmcnt(0). - Must happen before; following; buffer_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; cache. 5. buffer_inv sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acq_rel - system - global 1. buffer_wbl2 sc0=1 sc1=1. - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at system scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global and L2 writeback; have completed before; performing the; atomicrmw that is; being released. 3. buffer/global_atomic; sc1=1; 4. s_wait",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:320727,load,loads,320727,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['loads']
Performance,"operations, a major performance problem.; Fortunately for us, the LLVM optimizer has a highly-tuned optimization; pass named ""mem2reg"" that handles this case, promoting allocas like this; into SSA registers, inserting Phi nodes as appropriate. If you run this; example through the pass, for example, you'll get:. .. code-block:: bash. $ llvm-as < example.ll | opt -passes=mem2reg | llvm-dis; @G = weak global i32 0; @H = weak global i32 0. define i32 @test(i1 %Condition) {; entry:; br i1 %Condition, label %cond_true, label %cond_false. cond_true:; %X.0 = load i32, i32* @G; br label %cond_next. cond_false:; %X.1 = load i32, i32* @H; br label %cond_next. cond_next:; %X.01 = phi i32 [ %X.1, %cond_false ], [ %X.0, %cond_true ]; ret i32 %X.01; }. The mem2reg pass implements the standard ""iterated dominance frontier""; algorithm for constructing SSA form and has a number of optimizations; that speed up (very common) degenerate cases. The mem2reg optimization; pass is the answer to dealing with mutable variables, and we highly; recommend that you depend on it. Note that mem2reg only works on; variables in certain circumstances:. #. mem2reg is alloca-driven: it looks for allocas and if it can handle; them, it promotes them. It does not apply to global variables or heap; allocations.; #. mem2reg only looks for alloca instructions in the entry block of the; function. Being in the entry block guarantees that the alloca is only; executed once, which makes analysis simpler.; #. mem2reg only promotes allocas whose uses are direct loads and stores.; If the address of the stack object is passed to a function, or if any; funny pointer arithmetic is involved, the alloca will not be; promoted.; #. mem2reg only works on allocas of `first; class <../../LangRef.html#first-class-types>`_ values (such as pointers,; scalars and vectors), and only if the array size of the allocation is; 1 (or missing in the .ll file). mem2reg is not capable of promoting; structs or arrays to registers. Note that ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:7568,optimiz,optimization,7568,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,1,['optimiz'],['optimization']
Performance,"opied in this way. These requirements are followed automatically for objects whose; initialization and deinitialization are under the control of ARC:. * objects of static, automatic, and temporary storage duration; * instance variables of Objective-C objects; * elements of arrays where the array object's initialization and; deinitialization are under the control of ARC; * fields of Objective-C struct types where the struct object's; initialization and deinitialization are under the control of ARC; * non-static data members of Objective-C++ non-union class types; * Objective-C++ objects and arrays of dynamic storage duration created; with the ``new`` or ``new[]`` operators and destroyed with the; corresponding ``delete`` or ``delete[]`` operator. They are not followed automatically for these objects:. * objects of dynamic storage duration created in other memory, such as; that returned by ``malloc``; * union members. .. admonition:: Rationale. ARC must perform special operations when initializing an object and; when destroying it. In many common situations, ARC knows when an; object is created and when it is destroyed and can ensure that these; operations are performed correctly. Otherwise, however, ARC requires; programmer cooperation to establish its initialization invariants; because it is infeasible for ARC to dynamically infer whether they; are intact. For example, there is no syntactic difference in C between; an assignment that is intended by the programmer to initialize a variable; and one that is intended to replace the existing value stored there,; but ARC must perform one operation or the other. ARC chooses to always; assume that objects are initialized (except when it is in charge of; initializing them) because the only workable alternative would be to; ban all code patterns that could potentially be used to access; uninitialized memory, and that would be too limiting. In practice,; this is rarely a problem because programmers do not generally need to; wor",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst:46938,perform,perform,46938,interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,1,['perform'],['perform']
Performance,"opt - LLVM optimizer; ====================. .. program:: opt. SYNOPSIS; --------. :program:`opt` [*options*] [*filename*]. DESCRIPTION; -----------. The :program:`opt` command is the modular LLVM optimizer and analyzer. It takes; LLVM source files as input, runs the specified optimizations or analyses on it,; and then outputs the optimized file. The optimizations available via; :program:`opt` depend upon what libraries were linked into it as well as any; additional libraries that have been loaded with the :option:`-load` option. Use; the :option:`-help` option to determine what optimizations you can use. If ``filename`` is omitted from the command line or is ""``-``"", :program:`opt`; reads its input from standard input. Inputs can be in either the LLVM assembly; language format (``.ll``) or the LLVM bitcode format (``.bc``). If an output filename is not specified with the :option:`-o` option,; :program:`opt` writes its output to the standard output. OPTIONS; -------. .. option:: -f. Enable binary output on terminals. Normally, :program:`opt` will refuse to; write raw bitcode output if the output stream is a terminal. With this option,; :program:`opt` will write raw bitcode regardless of the output device. .. option:: -help. Print a summary of command line options. .. option:: -o <filename>. Specify the output filename. .. option:: -S. Write output in LLVM intermediate language (instead of bitcode). .. option:: -{passname}. :program:`opt` provides the ability to run any of LLVM's optimization or; analysis passes in any order. The :option:`-help` option lists all the passes; available. The order in which the options occur on the command line are the; order in which they are executed (within pass constraints). .. option:: -strip-debug. This option causes opt to strip debug information from the module before; applying other optimizations. It is essentially the same as `-strip`; but it ensures that stripping of debug information is done first. .. option:: -verify-each. Thi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/opt.rst:11,optimiz,optimizer,11,interpreter/llvm-project/llvm/docs/CommandGuide/opt.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/opt.rst,8,"['load', 'optimiz']","['load', 'loaded', 'optimizations', 'optimized', 'optimizer']"
Performance,"optimization level and target triple:; bin/clang-proto-fuzzer CORPUS_DIR -ignore_remaining_args=1 -O3 -triple \; arm64apple-ios9. To translate a clang-proto-fuzzer corpus output to C++:; bin/clang-proto-to-cxx CORPUS_OUTPUT_FILE. ===================; llvm-proto-fuzzer; ===================; Like, clang-proto-fuzzer, llvm-proto-fuzzer is also a protobuf-mutator based; fuzzer. It receives as input a cxx_loop_proto which it then converts into a; string of valid LLVM IR: a function with either a single loop or two nested; loops. It then creates a new string of IR by running optimization passes over; the original IR. Currently, it only runs a loop-vectorize pass but more passes; can easily be added to the fuzzer. Once there are two versions of the input; function (optimized and not), llvm-proto-fuzzer uses LLVM's JIT Engine to; compile both functions. Lastly, it runs both functions on a suite of inputs and; checks that both functions behave the same on all inputs. In this way,; llvm-proto-fuzzer can find not only compiler crashes, but also miscompiles; originating from LLVM's optimization passes. llvm-proto-fuzzer is built very similarly to clang-proto-fuzzer. You can run the; fuzzer with the following command:; bin/clang-llvm-proto-fuzzer CORPUS_DIR. To translate a cxx_loop_proto file into LLVM IR do:; bin/clang-loop-proto-to-llvm CORPUS_OUTPUT_FILE; To translate a cxx_loop_proto file into C++ do:; bin/clang-loop-proto-to-cxx CORPUS_OUTPUT_FILE. Note: To get a higher number of executions per second with llvm-proto-fuzzer it; helps to build it without ASan instrumentation and with the -O2 flag. Because; the fuzzer is not only compiling code, but also running it, as the inputs get; large, the time necessary to fuzz one input can get very high.; Example:; cmake .. -GNinja -DCMAKE_C_COMPILER=clang -DCMAKE_CXX_COMPILER=clang++ \; -DCLANG_ENABLE_PROTO_FUZZER=ON -DLLVM_USE_SANITIZE_COVERAGE=YES \; -DCMAKE_CXX_FLAGS=""-O2""; ninja clang-llvm-proto-fuzzer clang-loop-proto-to-llvm; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/clang-fuzzer/README.txt:4953,optimiz,optimization,4953,interpreter/llvm-project/clang/tools/clang-fuzzer/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/clang-fuzzer/README.txt,1,['optimiz'],['optimization']
Performance,"optimized compiler. **stage2-check-llvm**; Depends on stage2 and runs check-llvm using the stage2 compiler. **stage2-check-clang**; Depends on stage2 and runs check-clang using the stage2 compiler. **stage2-check-all**; Depends on stage2 and runs check-all using the stage2 compiler. **stage2-test-suite**; Depends on stage2 and runs the test-suite using the stage2 compiler (requires; in-tree test-suite). BOLT; ====. `BOLT <https://github.com/llvm/llvm-project/blob/main/bolt/README.md>`_; (Binary Optimization and Layout Tool) is a tool that optimizes binaries; post-link by profiling them at runtime and then using that information to; optimize the layout of the final binary among other optimizations performed; at the binary level. There are also CMake caches available to build; LLVM/Clang with BOLT. To configure a single-stage build that builds LLVM/Clang and then optimizes; it with BOLT, use the following CMake configuration:. .. code-block:: console. $ cmake <path to source>/llvm -C <path to source>/clang/cmake/caches/BOLT.cmake. Then, build the BOLT-optimized binary by running the following ninja command:. .. code-block:: console. $ ninja clang-bolt. If you're seeing errors in the build process, try building with a recent; version of Clang/LLVM by setting the CMAKE_C_COMPILER and; CMAKE_CXX_COMPILER flags to the appropriate values. It is also possible to use BOLT on top of PGO and (Thin)LTO for an even more; significant runtime speedup. To configure a three stage PGO build with ThinLTO; that optimizes the resulting binary with BOLT, use the following CMake; configuration command:. .. code-block:: console. $ cmake -G Ninja <path to source>/llvm \; -C <path to source>/clang/cmake/caches/BOLT-PGO.cmake \; -DBOOTSTRAP_LLVM_ENABLE_LLD=ON \; -DBOOTSTRAP_BOOTSTRAP_LLVM_ENABLE_LLD=ON \; -DPGO_INSTRUMENT_LTO=Thin. Then, to build the final optimized binary, build the stage2-clang-bolt target:. .. code-block:: console. $ ninja stage2-clang-bolt. 3-Stage Non-Determinism; =======",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AdvancedBuilds.rst:10430,cache,caches,10430,interpreter/llvm-project/llvm/docs/AdvancedBuilds.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AdvancedBuilds.rst,1,['cache'],['caches']
Performance,"option(LLVM_INCLUDE_DXIL_TESTS ""Include DXIL tests"" Off); mark_as_advanced(LLVM_INCLUDE_DXIL_TESTS). if (NOT LLVM_INCLUDE_DXIL_TESTS); return(); endif (). if (NOT ""DirectX"" IN_LIST LLVM_TARGETS_TO_BUILD); message(FATAL_ERROR ""Building dxil-dis tests is unsupported without the DirectX target""); endif (). if (CMAKE_HOST_UNIX); set(LLVM_LINK_OR_COPY create_symlink); else (); set(LLVM_LINK_OR_COPY copy); endif (). if (DXIL_DIS); add_custom_target(dxil-dis; COMMAND ${CMAKE_COMMAND} -E ${LLVM_LINK_OR_COPY} ""${DXIL_DIS}"" ""${LLVM_RUNTIME_OUTPUT_INTDIR}/dxil-dis${CMAKE_EXECUTABLE_SUFFIX}""); return(); endif (). include(ExternalProject). set(SOURCE_DIR ${CMAKE_CURRENT_BINARY_DIR}/DXC-src); set(BINARY_DIR ${CMAKE_CURRENT_BINARY_DIR}/DXC-bins); set(GIT_SETTINGS GIT_REPOSITORY https://github.com/microsoft/DirectXShaderCompiler.git). if (DXC_SOURCE_DIR); set(SOURCE_DIR ${DXC_SOURCE_DIR}); unset(GIT_SETTINGS); endif (). ExternalProject_Add(DXC; ${GIT_SETTINGS}; SOURCE_DIR ${SOURCE_DIR}; BINARY_DIR ${BINARY_DIR}; CMAKE_ARGS -C ${SOURCE_DIR}/cmake/caches/PredefinedParams.cmake -DLLVM_INCLUDE_TESTS=On; BUILD_COMMAND ${CMAKE_COMMAND} --build ${BINARY_DIR} --target llvm-dis; BUILD_BYPRODUCTS ${BINARY_DIR}/bin/llvm-dis; INSTALL_COMMAND """"; ). add_custom_target(dxil-dis; COMMAND ${CMAKE_COMMAND} -E ${LLVM_LINK_OR_COPY} ""${BINARY_DIR}/bin/llvm-dis${CMAKE_EXECUTABLE_SUFFIX}"" ""${LLVM_RUNTIME_OUTPUT_INTDIR}/dxil-dis${CMAKE_EXECUTABLE_SUFFIX}""; DEPENDS DXC; ); ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/dxil-dis/CMakeLists.txt:1046,cache,caches,1046,interpreter/llvm-project/llvm/tools/dxil-dis/CMakeLists.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/dxil-dis/CMakeLists.txt,2,['cache'],['caches']
Performance,"option="""");; ```. The drawing/animation time range is a global variable that can be; directly set:. ``` {.cpp}; gGeoManager->SetTminTmax(tmin, tmax);; // without arguments resets the time range to the maximum value; ```. Once set, the time range will be active both for individual or global; track drawing. For animation, this range is divided to the desired; number of frames and will be automatically updated at each frame in; order to get the animation effect. The option provided to all track-drawing methods can trigger different; track selections:. `default: `A track (or all primary tracks) drawn without daughters. `/D:` Track and first level descendents only are drawn. `/*: ` Track and all descendents are drawn. `/Ntype:` All tracks having `name=type` are drawn. Generally several options can be concatenated in the same string (E.g.; `""/D /Npion-""`). For animating tracks, additional options can be added:. `/G:`Geometry animate. Generally when drawing or animating tracks, one; has to first perform a normal drawing of the geometry as convenient. The; tracks will be drawn over the geometry. The geometry itself will be; animated (camera moving and rotating in order to ""catch"" the majority of; current track segments.). `/S:`Save all frames in gif format in the current folder. This option; allows creating a movie based on individual frames. ## Checking the Geometry. Several checking methods are accessible from the context menu of volume; objects or of the manager class. They generally apply only to the; visible parts of the drawn geometry in order to ease geometry checking,; and their implementation is in the **`TGeoChecker`** class. The checking; package contains an overlap checker and several utility methods that; generally have visualization outputs. ### The Overlap Checker. An overlap is any region in the Euclidian space being contained by more; than one positioned volume. Due to the containment scheme used by the; modeller, all points inside a volume have to be also ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md:128970,perform,perform,128970,documentation/users-guide/Geometry.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md,1,['perform'],['perform']
Performance,"opying them and giving them new names. For example,; the PredicateInfo utility uses it to build Extended SSA form, and; attach various forms of information to operands that dominate specific; uses. It is not meant for general use, only for building temporary; renaming forms that require value splits at certain points. .. _type.test:. '``llvm.type.test``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare i1 @llvm.type.test(ptr %ptr, metadata %type) nounwind memory(none). Arguments:; """""""""""""""""""". The first argument is a pointer to be tested. The second argument is a; metadata object representing a :doc:`type identifier <TypeMetadata>`. Overview:; """""""""""""""""". The ``llvm.type.test`` intrinsic tests whether the given pointer is associated; with the given type identifier. .. _type.checked.load:. '``llvm.type.checked.load``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare {ptr, i1} @llvm.type.checked.load(ptr %ptr, i32 %offset, metadata %type) nounwind memory(argmem: read). Arguments:; """""""""""""""""""". The first argument is a pointer from which to load a function pointer. The; second argument is the byte offset from which to load the function pointer. The; third argument is a metadata object representing a :doc:`type identifier; <TypeMetadata>`. Overview:; """""""""""""""""". The ``llvm.type.checked.load`` intrinsic safely loads a function pointer from a; virtual table pointer using type metadata. This intrinsic is used to implement; control flow integrity in conjunction with virtual call optimization. The; virtual call optimization pass will optimize away ``llvm.type.checked.load``; intrinsics associated with devirtualized calls, thereby removing the type; check in cases where it is not needed to enforce the control flow integrity; constraint. If the given pointer is associated with a type metadata identifier, this; function returns true as the second element of its return value. (Note that; the function may also return true if th",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:937825,load,load,937825,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,"or B4, the ``Stmt*`` for the second; statement refers to the actual expression in the AST for ``(x > 2)``. Thus; pointers to subclasses of ``Expr`` can appear in the list of statements in a; block, and not just subclasses of ``Stmt`` that refer to proper C statements. The terminator of block B4 is a pointer to the ``IfStmt`` object in the AST.; The pretty-printer outputs ``if [B4.2]`` because the condition expression of; the if-statement has an actual place in the basic block, and thus the; terminator is essentially *referring* to the expression that is the second; statement of block B4 (i.e., B4.2). In this manner, conditions for; control-flow (which also includes conditions for loops and switch statements); are hoisted into the actual basic block. .. Implicit Control-Flow; .. ^^^^^^^^^^^^^^^^^^^^^. .. A key design principle of the ``CFG`` class was to not require any; .. transformations to the AST in order to represent control-flow. Thus the; .. ``CFG`` does not perform any ""lowering"" of the statements in an AST: loops; .. are not transformed into guarded gotos, short-circuit operations are not; .. converted to a set of if-statements, and so on. Constant Folding in the Clang AST; ---------------------------------. There are several places where constants and constant folding matter a lot to; the Clang front-end. First, in general, we prefer the AST to retain the source; code as close to how the user wrote it as possible. This means that if they; wrote ""``5+4``"", we want to keep the addition and two constants in the AST, we; don't want to fold to ""``9``"". This means that constant folding in various; ways turns into a tree walk that needs to handle the various cases. However, there are places in both C and C++ that require constants to be; folded. For example, the C standard defines what an ""integer constant; expression"" (i-c-e) is with very precise and specific requirements. The; language then requires i-c-e's in a lot of places (for example, the size of a; bitfiel",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/InternalsManual.rst:117963,perform,perform,117963,interpreter/llvm-project/clang/docs/InternalsManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/InternalsManual.rst,1,['perform'],['perform']
Performance,"or RAW files; by; default cache is OFF for these files, but there maybe cases in which the cache can; improve performances.; Remove call to XrdClient::Sync in SysStat. Correctly honor the create/recreate options coming from TFile::Open(); Allow the size of the (written) file to be retrieved after the Close (solves several reported file size mismatches).; . TXNetSystem:; ; Fix problem with GetDirEntry: the entry object was; going out-of-scope so; that the returned string was meaningless.; Reset; the list if dir entries in FreeDirectory.; Fix problem affecting repeated calls. The implementation of TFile throughput and info sending was; just sending 'regular' samples about the activity of the single TFile; instance that happened to trigger an activity in the right moment.; Now TMonaLisaWriter keeps internally track of every; activity; and regularly sends summaries valid for all the files which had; activity in the last time interval.; Additionally, it's now finalized the infrastructure able to; measure; and keep track of the file Open latency. A packet is sent for each; successful Open, sending the measures of the latencies for the; various phases of the open. Currently exploited fully by TAlienFile; and TXNetFile. Easy to report from other TFiles too.; Now, the hook for the Close() func triggers sending of a; packet containing various information about the performance related to; that file only.; Added support also for performance monitoring when writing. RGLITE: A ROOT GRID interface. RGLite plug-in - a ROOT plug-in module, which implements the ROOT Grid; interface and offers to ROOT users possibilities to perform a number of; operations using gLite middleware from within ROOT. Supported features:. Workload Management System operations:; ; job submission – normal, DAG and parametric; jobs (gLite; WMProxy API), ; smart look-up algorithm for WMP-Endpoints, ; job status querying (gLite LB API), ; job output retrieving (Globus GridFTP). . File Catalog operations (gLite/L",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/net/doc/v520/index.html:2604,latency,latency,2604,net/doc/v520/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/net/doc/v520/index.html,2,['latency'],['latency']
Performance,"or a 4-byte C++ ``int``, the template may be instantiated with,; for example, an ``int64_t`` instead (if available on the platform).; Since Python does not have unsigned types, the instantiation mechanism; strongly prefers signed types.; However, if an argument value is too large to fit in a signed integer type,; but would fit in an unsigned type, then that will be used. If it is important that a specific overload is selected, then use the; ``__overload__`` method to match a specific function signature.; An optional boolean second parameter can be used to restrict the selected; method to be const (if ``True``) or non-const (if ``False``).; The return value of which is a first-class callable object, that can be; stored to by-pass the overload resolution:. .. code-block:: python. >>> gf_double = global_function.__overload__('double'); >>> gf_double(1) # int implicitly promoted; 2.718281828459045; >>>. The ``__overload__`` method only does a lookup; it performs no (implicit); conversions and the types in the signature to match should be the fully; resolved ones (no typedefs).; To see all overloads available for selection, use ``help()`` on the function; or look at its ``__doc__`` string:. .. code-block:: python. >>> print(global_function.__doc__); int ::global_function(int); double ::global_function(double); >>>. For convenience, the ``:any:`` signature allows matching any overload, for; example to reduce a method to its ``const`` overload only, use:. .. code-block:: python. MyClass.some_method = MyClass.some_method.__overload__(':any:', True). `Overloads and exceptions`; --------------------------. Python error reporting is done using exceptions.; Failed argument conversion during overload resolution can lead to different; types of exceptions coming from respective attempted overloads.; The final error report issued if all overloads fail, is a summary of the; individual errors, but by Python language requirements it has to have a; single exception type.; If all the exc",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/functions.rst:9484,perform,performs,9484,bindings/pyroot/cppyy/cppyy/doc/source/functions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/functions.rst,1,['perform'],['performs']
Performance,"or class.; . Fumili. Add implementation of Minimizer interface using TFumili.; ; Minuit. In TMinuitMinimizer: do not delete the contained TMinuit reference, but maintain it alive, and accessible outside as gMinuit. It can then be used after fitting, for example for drawing contour plots. Add also support for Scan and Contour plots.; ; TLinearMinimizer: add support for robust fitting; . Minuit2. Add support to perform parallel minimization using a thread for each gradient calculation with openMP. In the ROOT environment the Minuit2 library can be built using openMP ( -fopenmp compilation flag for gcc) if the environment variables USE_PARALLEL_MINUIT2 and USE_OPENMP are set.; In the Minuit2 standalone built libraries (using autoconf) support for openMP is automatically enabled, whenever the compiler supports it (for example for gcc version >= 4.2). Some small changes have been applied in Minuit2 to make it thread safe. For example, when transforming from internal to external values, the parameter values are not cached anymore in MnUserTransformation class.; DavidonErrorUpdator: add an additional check to avoid a division by zero.; In Minuit2Minimizer fill the status information according to the minimizer result; Add Scan and Contour methods in the Minuit2Minimizer class; ; GenVector. Change the way the exception are thrown in the package (class GenVector_exception). Now, the GenVector_exception class is created only when the throwing of exception is enabled. This avoids the allocation of an un-needed std::string. This problem was observed in CMS when converting from 4D-vectors based on mass to standard (x,y,z,t) vectors, when the mass is zero. In this case, a numerical error creates artificially small negative masses returned by the (x,y,z,t) vector. Eventually a protection could be added when calculating M2(), to avoid negative values due to numerical rounding.; ; Fix a problem in the assignment operator of the ROOT::Math::PxPyPzM4D class. Avoid having nan when conve",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/math/doc/v522/index.html:3722,cache,cached,3722,math/doc/v522/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/math/doc/v522/index.html,2,['cache'],['cached']
Performance,"or ctrl+D to quit *; *****************************************; [cling]$. Statements and expressions could take more than one input line. The interactive prompt changes from ""[cling]$"" to ""[cling]$ ?"".; ; [cling]$ #include ""math.h""; [cling]$ #include ""stdio.h""; [cling]$ for (unsigned i = 0; i < 5; ++i) {; [cling]$ ? printf(""%f\n"", sin(i));; [cling]$ ? }; 0.000000; 0.841471; 0.909297; 0.141120; -0.756802; Grammar; Cling is able to parse everything that clang can. Current clang status can be found here. At the moment, there are use cases only for C++ that's why cling is best in working with C++. Clang has support of C, objC, objC++ and we are looking forward to having more use-cases and extend our tool in that direction.; Cling has internal commands, which can change its behavior at runtime. Those commands usually start with dot (.):; .I <path> - Adds an include path;; .x <filename> - #include-s the filename; and calls function called filename(); ; .L <libname> - Loads libname or #include-s the libname if libname is file;; .@ - Cancels the multiline input;; .printAST - (DEBUG ONLY) Turns on the printing of the compiler's abstract syntax tree (AST);; .dynamicExtensions - Turns on cling's dynamic extensions. This in turn enables the dynamic lookup and the late resolving of the identifier. With that option cling tries to heal the compile-time failed lookups at runtime;. Details; Command line. The interactive prompt supports an emacs-like command line editor, just like bash terminal, which makes it easy to integrate and use. Cling uses TextInput and doesn't depend on ncurses.; . Autocompletion should be coming soon!; ; #Include Declarations. Cling allows #include-s to be not only before the declarations. The includes could be mixed with other declarations. For example:; [cling]$ #include ""math.h""; [cling]$ sin(1); (double const) 8.414710e-01; [cling]$ #include ""stdio.h""; [cling]$ printf(""%f\n"", sin(1));; 0.841471. More statements could be combined using semicolon (;). Thi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/www/old/index.html:2311,Load,Loads,2311,interpreter/cling/www/old/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/www/old/index.html,1,['Load'],['Loads']
Performance,"or details. '``llvm.licm.disable``' Metadata; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. This metadata indicates that loop-invariant code motion (LICM) should not be; performed on this loop. The metadata has a single operand which is the string; ``llvm.licm.disable``. For example:. .. code-block:: llvm. !0 = !{!""llvm.licm.disable""}. Note that although it operates per loop it isn't given the llvm.loop prefix; as it is not affected by the ``llvm.loop.disable_nonforced`` metadata. '``llvm.access.group``' Metadata; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. ``llvm.access.group`` metadata can be attached to any instruction that; potentially accesses memory. It can point to a single distinct metadata; node, which we call access group. This node represents all memory access; instructions referring to it via ``llvm.access.group``. When an; instruction belongs to multiple access groups, it can also point to a; list of accesses groups, illustrated by the following example. .. code-block:: llvm. %val = load i32, ptr %arrayidx, !llvm.access.group !0; ...; !0 = !{!1, !2}; !1 = distinct !{}; !2 = distinct !{}. It is illegal for the list node to be empty since it might be confused; with an access group. The access group metadata node must be 'distinct' to avoid collapsing; multiple access groups by content. An access group metadata node must; always be empty which can be used to distinguish an access group; metadata node from a list of access groups. Being empty avoids the; situation that the content must be updated which, because metadata is; immutable by design, would required finding and updating all references; to the access group node. The access group can be used to refer to a memory access instruction; without pointing to it directly (which is not possible in global; metadata). Currently, the only metadata making use of it is; ``llvm.loop.parallel_accesses``. '``llvm.loop.parallel_accesses``' Metadata; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. The ``llvm.loop.parallel_accesses`` metadata re",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:310477,load,load,310477,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,"or efficiency. If ARC is not; aware of methods that return such ""interior"" pointers, its optimizations can; cause the owning object to be reclaimed too soon. This attribute informs ARC; that it must tread lightly. The extension rules are somewhat intentionally vague. The autorelease pool; limit is there to permit a simple implementation to simply retain and; autorelease the receiver. The other limit permits some amount of; optimization. The phrase ""derived from"" is intended to encompass the results; both of pointer transformations, such as casts and arithmetic, and of loading; from such derived pointers; furthermore, it applies whether or not such; derivations are applied directly in the calling code or by other utility code; (for example, the C library routine ``strchr``). However, the implementation; never need account for uses after a return from the code which calls the; method returning an interior pointer. As an exception, no extension is required if the receiver is loaded directly; from a ``__strong`` object with :ref:`precise lifetime semantics; <arc.optimization.precise>`. .. admonition:: Rationale. Implicit autoreleases carry the risk of significantly inflating memory use,; so it's important to provide users a way of avoiding these autoreleases.; Tying this to precise lifetime semantics is ideal, as for local variables; this requires a very explicit annotation, which allows ARC to trust the user; with good cheer. .. _arc.misc.c-retainable:. C retainable pointer types; --------------------------. A type is a :arc-term:`C retainable pointer type` if it is a pointer to; (possibly qualified) ``void`` or a pointer to a (possibly qualifier) ``struct``; or ``class`` type. .. admonition:: Rationale. ARC does not manage pointers of CoreFoundation type (or any of the related; families of retainable C pointers which interoperate with Objective-C for; retain/release operation). In fact, ARC does not even know how to; distinguish these types from arbitrary C pointer ty",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst:101435,load,loaded,101435,interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,1,['load'],['loaded']
Performance,"or example with:. ``` {.cpp}; #pragma link C++ ioctortype UserClass1;; #pragma link C++ ioctortype UserClass2;; ```. We look for the first existing public constructor in the following; order:. ``` {.cpp}; MyClass(UserClass1*);; MyClass(UserClass2*);; MyClass(TRootIOCtor*);; MyClass(); // Or a constructor with all its arguments defaulted.; ```. ## rootcling: The Cling Dictionary Generator. A way in which dictionaries can be generated is via the `rootcling`; utility. This tool generates takes as input a set of headers and; generates in output the dictionary C++ code and a `pcm` file.; This latter file is fundamental for the correct functioning of the; dictionary at runtime. It should be located in the directory where; the shared library is installed in which the compiled dictionary; resides. NOTA BENE: the dictionaries that will be used within the same project; must have unique names. In other words, compiled object files relative; to dictionary source files cannot reside in the same library or in; two libraries loaded by the same application if the original source; files have the same name.; This loose limitation is imposed by the registration mechanism ROOT; has in place to keep track of dynamically loaded libraries. In the following example, we walk through the steps necessary to; generate a dictionary, I/O, and inspect member functions. Let's start; with a **`TEvent`** class, which contains a collection of **`TTracks`**. The `TEvent.h` header is:. ``` {.cpp}; #ifndef __TEvent__; #define __TEvent__; #include ""TObject.h""; #include ""TCollection.h"". class TTrack;. class TEvent : public TObject {; private:; Int_t fId; // event sequential id; Float_t fTotalMom; // total momentum; TCollection *fTracks; // collection of tracks; public:; TEvent() { fId = 0; fTotalMom = 0; fTracks = nullptr; }; TEvent(Int_t id);; ~TEvent();; void AddTrack(TTrack *t);; Int_t GetId() const { return fId; }; Int_t GetNoTracks() const;; void Print(Option_t *opt="""");; Float_t TotalMomentum();. Cla",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/AddingaClass.md:13947,load,loaded,13947,documentation/users-guide/AddingaClass.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/AddingaClass.md,1,['load'],['loaded']
Performance,"or float *y) {; if (vec_all_eq(*x,*y)) return 3245; ; else return 12;; }. A predicate compare being used in a select_cc should have the same peephole; applied to it as a predicate compare used by a br_cc. There should be no; mfcr here:. _foo:; mfspr r2, 256; oris r5, r2, 12288; mtspr 256, r5; li r5, 12; li r6, 3245; lvx v2, 0, r4; lvx v3, 0, r3; vcmpeqfp. v2, v3, v2; mfcr r3, 2; rlwinm r3, r3, 25, 31, 31; cmpwi cr0, r3, 0; bne cr0, LBB1_2 ; entry; LBB1_1: ; entry; mr r6, r5; LBB1_2: ; entry; mr r3, r6; mtspr 256, r2; blr. //===----------------------------------------------------------------------===//. CodeGen/PowerPC/vec_constants.ll has an and operation that should be; codegen'd to andc. The issue is that the 'all ones' build vector is; SelectNodeTo'd a VSPLTISB instruction node before the and/xor is selected; which prevents the vnot pattern from matching. //===----------------------------------------------------------------------===//. An alternative to the store/store/load approach for illegal insert element ; lowering would be:. 1. store element to any ol' slot; 2. lvx the slot; 3. lvsl 0; splat index; vcmpeq to generate a select mask; 4. lvsl slot + x; vperm to rotate result into correct slot; 5. vsel result together. //===----------------------------------------------------------------------===//. Should codegen branches on vec_any/vec_all to avoid mfcr. Two examples:. #include <altivec.h>; int f(vector float a, vector float b); {; int aa = 0;; if (vec_all_ge(a, b)); aa |= 0x1;; if (vec_any_ge(a,b)); aa |= 0x2;; return aa;; }. vector float f(vector float a, vector float b) { ; if (vec_any_eq(a, b)) ; return a; ; else ; return b; ; }. //===----------------------------------------------------------------------===//. We should do a little better with eliminating dead stores.; The stores to the stack are dead since %a and %b are not needed. ; Function Attrs: nounwind; define <16 x i8> @test_vpmsumb() #0 {; entry:; %a = alloca <16 x i8>, align 16; %b = alloca <16 x",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt:5157,load,load,5157,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt,2,['load'],['load']
Performance,"or is a null pointer. * ``__bidi_indexable`` casts to ``__indexable`` if the pointer does not have an; underflow. The compiler may insert run-time checks to ensure the pointer is; not below the lower bound. * ``__indexable`` casts to ``__bidi_indexable``. The resulting; ``__bidi_indexable`` gets the lower bound same as the pointer value. * A type conversion may involve both a bitcast and a bounds annotation cast. For; example, casting from ``int *__bidi_indexable`` to ``char *__single`` involve; a bitcast (``int *`` to ``char *``) and a bounds annotation cast; (``__bidi_indexable`` to ``__single``). In this case, the compiler performs; the bitcast and then converts the bounds annotation. This means, ``int; *__bidi_indexable`` will be converted to ``char *__bidi_indexable`` and then; to ``char *__single``. * ``__terminated_by(T)`` cannot cast to any safe pointer type without the same; ``__terminated_by(T)`` attribute. To perform the cast, programmers can use an; intrinsic function such as ``__unsafe_terminated_by_to_indexable(P)`` to force; the conversion. * ``__terminated_by(T)`` can cast to ``__unsafe_indexable``. * Any type without ``__terminated_by(T)`` cannot cast to ``__terminated_by(T)``; without explicitly using an intrinsic function to allow it. + ``__unsafe_terminated_by_from_indexable(T, PTR [, PTR_TO_TERM])`` casts any; safe pointer PTR to a ``__terminated_by(T)`` pointer. ``PTR_TO_TERM`` is an; optional argument where the programmer can provide the exact location of the; terminator. With this argument, the function can skip reading the entire; array in order to locate the end of the pointer (or the upper bound).; Providing an incorrect ``PTR_TO_TERM`` causes a run-time trap. + ``__unsafe_forge_terminated_by(T, P, E)`` creates ``T __terminated_by(E)``; pointer given any pointer ``P``. Tmust be a pointer type. Portability with toolchains that do not support the extension; -------------------------------------------------------------. The language model is d",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/BoundsSafety.rst:44173,perform,perform,44173,interpreter/llvm-project/clang/docs/BoundsSafety.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/BoundsSafety.rst,1,['perform'],['perform']
Performance,"or tests *cond2* and returns *val2* if the result is; true. And so forth. An error is reported if no conditions are true. This example produces the sign word for an integer::. !cond(!lt(x, 0) : ""negative"", !eq(x, 0) : ""zero"", true : ""positive""). ``!dag(``\ *op*\ ``,`` *arguments*\ ``,`` *names*\ ``)``; This operator creates a DAG node with the given operator and; arguments. The *arguments* and *names* arguments must be lists; of equal length or uninitialized (``?``). The *names* argument; must be of type ``list<string>``. Due to limitations of the type system, *arguments* must be a list of items; of a common type. In practice, this means that they should either have the; same type or be records with a common parent class. Mixing ``dag`` and; non-``dag`` items is not possible. However, ``?`` can be used. Example: ``!dag(op, [a1, a2, ?], [""name1"", ""name2"", ""name3""])`` results in; ``(op a1-value:$name1, a2-value:$name2, ?:$name3)``. ``!div(``\ *a*\ ``,`` *b*\ ``)``; This operator performs signed division of *a* by *b*, and produces the quotient.; Division by 0 produces an error. Division of INT64_MIN by -1 produces an error. ``!empty(``\ *a*\ ``)``; This operator produces 1 if the string, list, or DAG *a* is empty; 0 otherwise.; A dag is empty if it has no arguments; the operator does not count. ``!eq(`` *a*\ `,` *b*\ ``)``; This operator produces 1 if *a* is equal to *b*; 0 otherwise.; The arguments must be ``bit``, ``bits``, ``int``, ``string``, or; record values. Use ``!cast<string>`` to compare other types of objects. ``!exists<``\ *type*\ ``>(``\ *name*\ ``)``; This operator produces 1 if a record of the given *type* whose name is *name*; exists; 0 otherwise. *name* should be of type *string*. ``!filter(``\ *var*\ ``,`` *list*\ ``,`` *predicate*\ ``)``. This operator creates a new ``list`` by filtering the elements in; *list*. To perform the filtering, TableGen binds the variable *var* to each; element and then evaluates the *predicate* expression, which presumably",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:62240,perform,performs,62240,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,1,['perform'],['performs']
Performance,"or this identifier within; its :ref:`identifier table <pchinternals-ident-table>` to load any top-level; declarations or macros associated with that identifier. ``ExternalASTSource``; This abstract interface is associated with the ``ASTContext`` class, and is; used whenever the abstract syntax tree nodes need to loaded from the AST; file. It provides the ability to de-serialize declarations and types; identified by their numeric values, read the bodies of functions when; required, and read the declarations stored within a declaration context; (either for iteration or for name lookup). ``ExternalSemaSource``; This abstract interface is associated with the ``Sema`` class, and is used; whenever semantic analysis needs to read information from the :ref:`global; method pool <pchinternals-method-pool>`. .. _pchinternals-chained:. Chained precompiled headers; ---------------------------. Chained precompiled headers were initially intended to improve the performance; of IDE-centric operations such as syntax highlighting and code completion while; a particular source file is being edited by the user. To minimize the amount; of reparsing required after a change to the file, a form of precompiled header; --- called a precompiled *preamble* --- is automatically generated by parsing; all of the headers in the source file, up to and including the last; ``#include``. When only the source file changes (and none of the headers it; depends on), reparsing of that source file can use the precompiled preamble and; start parsing after the ``#include``\ s, so parsing time is proportional to the; size of the source file (rather than all of its includes). However, the; compilation of that translation unit may already use a precompiled header: in; this case, Clang will create the precompiled preamble as a chained precompiled; header that refers to the original precompiled header. This drastically; reduces the time needed to serialize the precompiled preamble for use in; reparsing. Chained pre",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/PCHInternals.rst:23648,perform,performance,23648,interpreter/llvm-project/clang/docs/PCHInternals.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/PCHInternals.rst,1,['perform'],['performance']
Performance,"or; dynamically) equal to or larger than the integer bit width of the arguments,; the result is a :ref:`poison value <poisonvalues>`. If the arguments are; vectors, each vector element of ``a`` is shifted by the corresponding shift; amount in ``b``. Semantics:; """""""""""""""""""". The maximum value this operation can clamp to is the largest unsigned value; representable by the bit width of the arguments. Examples; """""""""""""""""". .. code-block:: llvm. %res = call i4 @llvm.ushl.sat.i4(i4 2, i4 1) ; %res = 4; %res = call i4 @llvm.ushl.sat.i4(i4 3, i4 3) ; %res = 15. Fixed Point Arithmetic Intrinsics; ---------------------------------. A fixed point number represents a real data type for a number that has a fixed; number of digits after a radix point (equivalent to the decimal point '.').; The number of digits after the radix point is referred as the `scale`. These; are useful for representing fractional values to a specific precision. The; following intrinsics perform fixed point arithmetic operations on 2 operands; of the same scale, specified as the third argument. The ``llvm.*mul.fix`` family of intrinsic functions represents a multiplication; of fixed point numbers through scaled integers. Therefore, fixed point; multiplication can be represented as. .. code-block:: llvm. %result = call i4 @llvm.smul.fix.i4(i4 %a, i4 %b, i32 %scale). ; Expands to; %a2 = sext i4 %a to i8; %b2 = sext i4 %b to i8; %mul = mul nsw nuw i8 %a2, %b2; %scale2 = trunc i32 %scale to i8; %r = ashr i8 %mul, i8 %scale2 ; this is for a target rounding down towards negative infinity; %result = trunc i8 %r to i4. The ``llvm.*div.fix`` family of intrinsic functions represents a division of; fixed point numbers through scaled integers. Fixed point division can be; represented as:. .. code-block:: llvm. %result call i4 @llvm.sdiv.fix.i4(i4 %a, i4 %b, i32 %scale). ; Expands to; %a2 = sext i4 %a to i8; %b2 = sext i4 %b to i8; %scale2 = trunc i32 %scale to i8; %a3 = shl i8 %a2, %scale2; %r = sdiv i8 %a3, %b2 ; this i",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:619281,perform,perform,619281,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['perform']
Performance,orDataLoader). endif(). #--- CPU tests. ----------------------------; if ((BLAS_FOUND OR mathmore) AND imt AND tmva-cpu). ROOT_EXECUTABLE(testIm2ColCpu TestIm2ColCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-CNN-Im2Col-CPU COMMAND testIm2ColCpu). ROOT_EXECUTABLE(testPoolingLayerCpu TestPoolingLayerCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-CNN-PoolingLayer-CPU COMMAND testPoolingLayerCpu). ROOT_EXECUTABLE(testConvLayerCpu TestConvLayerCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-CNN-ConvLayer-CPU COMMAND testConvLayerCpu). ROOT_EXECUTABLE(testRotWeightsCpu TestRotateWeightsCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-CNN-RotWeights-CPU COMMAND testRotWeightsCpu). ROOT_EXECUTABLE(testForwardPassCpu TestForwardPassCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-CNN-Forward-CPU COMMAND testForwardPassCpu). ROOT_EXECUTABLE(testConvNetLossCpu TestConvNetLossCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-CNN-Loss-CPU COMMAND testConvNetLossCpu). ROOT_EXECUTABLE(testConvNetPredCpu TestConvNetPredictionCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-CNN-Pred-CPU COMMAND testConvNetPredCpu). ROOT_EXECUTABLE(testReshapeCpu TestReshapeCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-CNN-Reshape-CPU COMMAND testReshapeCpu). #-- need to be fixed; #ROOT_EXECUTABLE(testTensorDataLoaderCpu TestTensorDataLoaderCpu.cxx LIBRARIES ${Libraries}); #ROOT_ADD_TEST(TMVA-DNN-Tensor-Data-Loader-CPU COMMAND testTensorDataLoaderCpu). #ROOT_EXECUTABLE(testDLMinimizationCpu TestMinimizationCpu.cxx LIBRARIES ${Libraries}); #ROOT_ADD_TEST(TMVA-DNN-CNN-Minimization-CPU COMMAND testDLMinimizationCpu). ROOT_EXECUTABLE(testConvBackpropagationCpu TestConvBackpropagation.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-CNN-Backpropagation-CPU COMMAND testConvBackpropagationCpu). ROOT_EXECUTABLE(testMethodDLCNN TestMethodDLCNN.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-CNN-MethodDL COMMAND testMethodDLCNN). endif (); ,MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tmva/tmva/test/DNN/CNN/CMakeLists.txt:6180,Load,Loader-CPU,6180,tmva/tmva/test/DNN/CNN/CMakeLists.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tmva/tmva/test/DNN/CNN/CMakeLists.txt,1,['Load'],['Loader-CPU']
Performance,"ord64`` S + A; Dynamic; ``R_AMDGPU_REL32`` Static 4 ``word32`` S + A - P; ``R_AMDGPU_REL64`` Static 5 ``word64`` S + A - P; ``R_AMDGPU_ABS32`` Static, 6 ``word32`` S + A; Dynamic; ``R_AMDGPU_GOTPCREL`` Static 7 ``word32`` G + GOT + A - P; ``R_AMDGPU_GOTPCREL32_LO`` Static 8 ``word32`` (G + GOT + A - P) & 0xFFFFFFFF; ``R_AMDGPU_GOTPCREL32_HI`` Static 9 ``word32`` (G + GOT + A - P) >> 32; ``R_AMDGPU_REL32_LO`` Static 10 ``word32`` (S + A - P) & 0xFFFFFFFF; ``R_AMDGPU_REL32_HI`` Static 11 ``word32`` (S + A - P) >> 32; *reserved* 12; ``R_AMDGPU_RELATIVE64`` Dynamic 13 ``word64`` B + A; ``R_AMDGPU_REL16`` Static 14 ``word16`` ((S + A - P) - 4) / 4; ========================== ======= ===== ========== ==============================. ``R_AMDGPU_ABS32_LO`` and ``R_AMDGPU_ABS32_HI`` are only supported by; the ``mesa3d`` OS, which does not support ``R_AMDGPU_ABS64``. There is no current OS loader support for 32-bit programs and so; ``R_AMDGPU_ABS32`` is not used. .. _amdgpu-loaded-code-object-path-uniform-resource-identifier:. Loaded Code Object Path Uniform Resource Identifier (URI); ---------------------------------------------------------. The AMD GPU code object loader represents the path of the ELF shared object from; which the code object was loaded as a textual Uniform Resource Identifier (URI).; Note that the code object is the in memory loaded relocated form of the ELF; shared object. Multiple code objects may be loaded at different memory; addresses in the same process from the same ELF shared object. The loaded code object path URI syntax is defined by the following BNF syntax:. .. code::. code_object_uri ::== file_uri | memory_uri; file_uri ::== ""file://"" file_path [ range_specifier ]; memory_uri ::== ""memory://"" process_id range_specifier; range_specifier ::== [ ""#"" | ""?"" ] ""offset="" number ""&"" ""size="" number; file_path ::== URI_ENCODED_OS_FILE_PATH; process_id ::== DECIMAL_NUMBER; number ::== HEX_NUMBER | DECIMAL_NUMBER | OCTAL_NUMBER. **number**; Is a C integral",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:82660,load,loaded-code-object-path-uniform-resource-identifier,82660,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['loaded-code-object-path-uniform-resource-identifier']
Performance,"ore atomic/; atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; have; completed before; performing the; atomicrmw that is; being released. 2. buffer/global_atomic; 3. s_waitcnt vmcnt(0). - If not TgSplit execution; mode, omit.; - Must happen before; the following; buffer_wbinvl1_vol.; - Ensures any; following global; data read is no; older than the; atomicrmw value; being acquired. 4. buffer_wbinvl1_vol. - If not TgSplit execution; mode, omit.; - Ensures that; following; loads will not see; stale data. atomicrmw acq_rel - workgroup - local *If TgSplit execution mode,; local address space cannot; be used.*. 1. ds_atomic; 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than the local load; atomic value being; acquired. atomicrmw acq_rel - workgroup - generic 1. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL, omit; lgkmcnt(0).; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load/store/; load atomic/store atomic/; atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; have; completed before; performing the; atomicrmw that is; being released. 2. flat_atomic; 3. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If not TgSplit execution; mode, omit vmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Must happen before; the following; buffer_wbinvl1_vol and; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:267886,load,load,267886,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"ore the; function and do not draw it. ### Advances Options. The advance option button is enabled only after having performed the fit and provides; additional drawing options that can be used after having done the fit. These new drawing tools,; which can be selected by the ""Advanced Drawing Tool"" panel that pops up when clicking the ""Advanced"" button, are:. * *Contour*: to plot the confidence contour of two chosen parameters. One can select the number of points to draw the contour; (more points might require more time to compute it), the parameters and the desired confidence level . * *Scan* : to plot a scan of the minimization function (likelihood or chi-squared) around the minimum as function of the chosen parameter. * *Conf Interval* : to plot the confidence interval of the fitted function as a filled coloured band around its central value.; One can select the desired confidence level for the band to be plotted. ### Print Options. This set of options specifies the amount of feedback printed on the; root command line after performed fits. *‘Verbose'* - prints fit results after each iteration. *‘Quiet'* - no fit information is printed. *‘Default'* - between Verbose and Quiet. ### Command Buttons. *Fit button* - performs a fit taking different option settings via the; Fit Panel interface. *Reset* - sets the GUI elements and related fit settings to the; default ones. *Close* - closes the Fit panel window. ### Minimization Options. With this tab one can select specific options for minimization. These include. * The minimizer library ( *Minuit*, *Minuit2*, *Fumili*, *GSL*, *Genetics* ); * The method (algorithm) for minimization. For example for Minuit one can choose between (*Migrad*, *Simplex* or *Scan*); * Error definition; * Minimization tolerance; * Number of iterations/function calls; * Print Level: (*Default*, *Verbose* or *Quiet*). ## New ROOT::Fit classes. The fitting of the data objects in ROOT, histograms, graphs and tree is performed via some common classes,; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/FittingHistograms.md:25476,perform,performed,25476,documentation/users-guide/FittingHistograms.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/FittingHistograms.md,1,['perform'],['performed']
Performance,"ore, which; eliminates a constant pool load. For example, consider:. define i64 @ccosf(float %z.0, float %z.1) nounwind readonly {; entry:; %tmp6 = fsub float -0.000000e+00, %z.1		; <float> [#uses=1]; %tmp20 = tail call i64 @ccoshf( float %tmp6, float %z.0 ) nounwind readonly; ret i64 %tmp20; }; declare i64 @ccoshf(float %z.0, float %z.1) nounwind readonly. This currently compiles to:. LCPI1_0:					# <4 x float>; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; _ccosf:; 	subl	$12, %esp; 	movss	16(%esp), %xmm0; 	movss	%xmm0, 4(%esp); 	movss	20(%esp), %xmm0; 	xorps	LCPI1_0, %xmm0; 	movss	%xmm0, (%esp); 	call	L_ccoshf$stub; 	addl	$12, %esp; 	ret. Note the load into xmm0, then xor (to negate), then store. In PIC mode,; this code computes the pic base and does two loads to do the constant pool ; load, so the improvement is much bigger. The tricky part about this xform is that the argument load/store isn't exposed; until post-legalize, and at that point, the fneg has been custom expanded into ; an X86 fxor. This means that we need to handle this case in the x86 backend; instead of in target independent code. //===---------------------------------------------------------------------===//. Non-SSE4 insert into 16 x i8 is atrociously bad. //===---------------------------------------------------------------------===//. <2 x i64> extract is substantially worse than <2 x f64>, even if the destination; is memory. //===---------------------------------------------------------------------===//. INSERTPS can match any insert (extract, imm1), imm2 for 4 x float, and insert; any number of 0.0 simultaneously. Currently we only use it for simple; insertions. See comments in LowerINSERT_VECTOR_ELT_SSE4. //===---------------------------------------------------------------------===//. On a random note, SSE2 should declare insert/extract of 2 x f64 as legal, not; Custom. All combinations of insert/extract reg-reg, reg-m",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:13445,load,load,13445,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,2,['load'],['load']
Performance,"ore/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; buffer_wbinvl1_vol.; - Ensures that the; preceding; global/local/generic; load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; acquire-fence-paired-atomic); has completed; before invalidating; the cache. This; satisfies the; requirements of; acquire.; - Ensures that all; previous memory; operations have; completed before a; following; global/local/generic; store; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; release-fence-paired-atomic).; This satisfies the; requirements of; release. 2. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. This; satisfies the; requirements of; acquire. **Sequential Consistent Atomic**; ------------------------------------------------------------------------------------; load atomic seq_cst - singlethread - global *Same as corresponding; - wavefront - local load atomic acquire,; - generic except must generate; all instructions even; for OpenCL.*; load atomic seq_cst - workgroup - global 1. s_waitcnt lgkmcnt(0); - generic. - Must; happen after; preceding; local/generic load; atomic/store; atomic/atomicrmw; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; lgkmcnt(0) and so do; not need to be; considered.); - Ensures any; preceding; sequential; consistent local; memory instructions; have completed; before executing; this sequentially; consistent; instruction. This; prevents reordering; a seq_cst store; followed by a; seq_cst load. (Note; that seq_cst is; stronger than; acquire/relea",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:230044,load,loads,230044,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['loads']
Performance,"ore/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; buffer_wbinvl1_vol.; - Ensures that the; preceding; global/local/generic; load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; acquire-fence-paired-atomic); has completed; before invalidating; the cache. This; satisfies the; requirements of; acquire.; - Ensures that all; previous memory; operations have; completed before a; following; global/local/generic; store; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; release-fence-paired-atomic).; This satisfies the; requirements of; release. 2. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. This; satisfies the; requirements of; acquire. fence acq_rel - system *none* 1. buffer_wbl2. - If OpenCL and; address space is; local, omit.; - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at system scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate; (see comment for; previous fence).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atom",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:277702,load,loads,277702,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['loads']
Performance,"ore; than once header basic blocks.) If ``irr_loop`` metadata is attached to the; terminator instruction of a basic block that is not really an irreducible loop; header, the behavior is undefined. The intent of this metadata is to improve the; accuracy of the block frequency propagation. For example, in the code below, the; block ``header0`` may have a loop header weight (relative to the other headers of; the irreducible loop) of 100:. .. code-block:: llvm. header0:; ...; br i1 %cmp, label %t1, label %t2, !irr_loop !0. ...; !0 = !{""loop_header_weight"", i64 100}. Irreducible loop header weights are typically based on profile data. .. _md_invariant.group:. '``invariant.group``' Metadata; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. The experimental ``invariant.group`` metadata may be attached to; ``load``/``store`` instructions referencing a single metadata with no entries.; The existence of the ``invariant.group`` metadata on the instruction tells; the optimizer that every ``load`` and ``store`` to the same pointer operand; can be assumed to load or store the same; value (but see the ``llvm.launder.invariant.group`` intrinsic which affects; when two pointers are considered the same). Pointers returned by bitcast or; getelementptr with only zero indices are considered the same. Examples:. .. code-block:: llvm. @unknownPtr = external global i8; ...; %ptr = alloca i8; store i8 42, ptr %ptr, !invariant.group !0; call void @foo(ptr %ptr). %a = load i8, ptr %ptr, !invariant.group !0 ; Can assume that value under %ptr didn't change; call void @foo(ptr %ptr). %newPtr = call ptr @getPointer(ptr %ptr); %c = load i8, ptr %newPtr, !invariant.group !0 ; Can't assume anything, because we only have information about %ptr. %unknownValue = load i8, ptr @unknownPtr; store i8 %unknownValue, ptr %ptr, !invariant.group !0 ; Can assume that %unknownValue == 42. call void @foo(ptr %ptr); %newPtr2 = call ptr @llvm.launder.invariant.group.p0(ptr %ptr); %d = load i8, ptr %newPtr2, !invariant.group !0 ; Ca",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:316135,optimiz,optimizer,316135,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,3,"['load', 'optimiz']","['load', 'optimizer']"
Performance,"oreg``; DBG_VALUE, indicating that the variable takes on an undefined location.; * The final dbg.value has its Value placed in virtual register ``%1``. Instruction Scheduling; ----------------------. A number of passes can reschedule instructions, notably instruction selection; and the pre-and-post RA machine schedulers. Instruction scheduling can; significantly change the nature of the program -- in the (very unlikely) worst; case the instruction sequence could be completely reversed. In such; circumstances LLVM follows the principle applied to optimizations, that it is; better for the debugger not to display any state than a misleading state.; Thus, whenever instructions are advanced in order of execution, any; corresponding DBG_VALUE is kept in its original position, and if an instruction; is delayed then the variable is given an undefined location for the duration; of the delay. To illustrate, consider this pseudo-MIR:. .. code-block:: text. %1:gr32 = MOV32rm %0, 1, $noreg, 4, $noreg, debug-location !5 :: (load 4 from %ir.addr1); DBG_VALUE %1, $noreg, !1, !2; %4:gr32 = ADD32rr %3, %2, implicit-def dead $eflags; DBG_VALUE %4, $noreg, !3, !4; %7:gr32 = SUB32rr %6, %5, implicit-def dead $eflags; DBG_VALUE %7, $noreg, !5, !6. Imagine that the SUB32rr were moved forward to give us the following MIR:. .. code-block:: text. %7:gr32 = SUB32rr %6, %5, implicit-def dead $eflags; %1:gr32 = MOV32rm %0, 1, $noreg, 4, $noreg, debug-location !5 :: (load 4 from %ir.addr1); DBG_VALUE %1, $noreg, !1, !2; %4:gr32 = ADD32rr %3, %2, implicit-def dead $eflags; DBG_VALUE %4, $noreg, !3, !4; DBG_VALUE %7, $noreg, !5, !6. In this circumstance LLVM would leave the MIR as shown above. Were we to move; the DBG_VALUE of virtual register %7 upwards with the SUB32rr, we would re-order; assignments and introduce a new state of the program. Whereas with the solution; above, the debugger will see one fewer combination of variable values, because; ``!3`` and ``!5`` will change value at the same ti",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst:33518,load,load,33518,interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,1,['load'],['load']
Performance,"org; D: Support for packed types. N: Rod Kay; E: rkay@auroraux.org; D: Author of LLVM Ada bindings. N: Erich Keane; E: erich.keane@intel.com; D: A variety of Clang contributions including function multiversioning, regcall/vectorcall.; I: ErichKeane. N: Eric Kidd; W: http://randomhacks.net/; D: llvm-config script. N: Anton Korobeynikov; E: anton at korobeynikov dot info; D: Mingw32 fixes, cross-compiling support, stdcall/fastcall calling conv.; D: x86/linux PIC codegen, aliases, regparm/visibility attributes; D: Switch lowering refactoring. N: Sumant Kowshik; E: kowshik@uiuc.edu; D: Author of the original C backend. N: Benjamin Kramer; E: benny.kra@gmail.com; D: Miscellaneous bug fixes. N: Michael Kuperstein; E: mkuper@google.com; D: Loop Vectorizer. N: Sundeep Kushwaha; E: sundeepk@codeaurora.org; D: Implemented DFA-based target independent VLIW packetizer. N: Christopher Lamb; E: christopher.lamb@gmail.com; D: aligned load/store support, parts of noalias and restrict support; D: vreg subreg infrastructure, X86 codegen improvements based on subregs; D: address spaces. N: Jim Laskey; E: jlaskey@apple.com; D: Improvements to the PPC backend, instruction scheduling; D: Debug and Dwarf implementation; D: Auto upgrade mangler; D: llvm-gcc4 svn wrangler. N: Chris Lattner; E: sabre@nondot.org; W: http://nondot.org/~sabre/; D: Primary architect of LLVM. N: Tanya Lattner (Tanya Brethour); E: tonic@nondot.org; W: http://nondot.org/~tonic/; D: The initial llvm-ar tool, converted regression testsuite to dejagnu; D: Modulo scheduling in the SparcV9 backend; D: Release manager (1.7+). N: Sylvestre Ledru; E: sylvestre@debian.org; W: http://sylvestre.ledru.info/; W: https://apt.llvm.org/; D: Debian and Ubuntu packaging; D: Continuous integration with jenkins. N: Andrew Lenharth; E: alenhar2@cs.uiuc.edu; W: http://www.lenharth.org/~andrewl/; D: Alpha backend; D: Sampling based profiling. N: Nick Lewycky; E: nicholas@mxc.ca; D: PredicateSimplifier pass. N: Tony Linthicum, et. al.; E: ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/CREDITS.TXT:6920,load,load,6920,interpreter/llvm-project/llvm/CREDITS.TXT,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/CREDITS.TXT,1,['load'],['load']
Performance,"orkgroup - local 1. s_waitcnt vmcnt(0) & vscnt(0). - If CU wavefront execution; mode, omit.; - If OpenCL, omit.; - Could be split into; separate s_waitcnt; vmcnt(0) and s_waitcnt; vscnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load/load; atomic/; atomicrmw-with-return-value.; - s_waitcnt vscnt(0); must happen after; any preceding; global/generic; store/store atomic/; atomicrmw-no-return-value.; - Must happen before; the following; store.; - Ensures that all; global memory; operations have; completed before; performing the; store that is being; released. 2. ds_atomic; 3. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; the following; buffer_gl0_inv.; - Ensures any; following global; data read is no; older than the local load; atomic value being; acquired. 4. buffer_gl0_inv. - If CU wavefront execution; mode, omit.; - If OpenCL omit.; - Ensures that; following; loads will not see; stale data. atomicrmw acq_rel - workgroup - generic 1. s_waitcnt lgkmcnt(0) &; vmcnt(0) & vscnt(0). - If CU wavefront execution; mode, omit vmcnt(0) and; vscnt(0).; - If OpenCL, omit lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0), s_waitcnt; vscnt(0) and s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load/load; atomic/; atomicrmw-with-return-value.; - s_waitcnt vscnt(0); must happen after; any preceding; global/generic; store/store; atomic/; atomicrmw-no-return-value.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; have; completed before; performing the; atomicrmw that is; being released. 2. flat_atomic; 3. s_waitcnt lgkmcnt(0) &; vmcnt(0) & vscnt(0). - If CU wavefr",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:365491,load,loads,365491,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['loads']
Performance,"orks by; speculatively storing over the return address after a bounds check bypass. This; speculative store then ends up being used by the CPU during speculative; execution of the return, potentially directing speculative execution to; arbitrary gadgets in the binary. Let's look at an example.; ```; unsigned char local_buffer[4];; unsigned char *untrusted_data_from_caller = ...;; unsigned long untrusted_size_from_caller = ...;; if (untrusted_size_from_caller < sizeof(local_buffer)) {; // Speculative execution enters here with a too-large size.; memcpy(local_buffer, untrusted_data_from_caller,; untrusted_size_from_caller);; // The stack has now been smashed, writing an attacker-controlled; // address over the return address.; minor_processing(local_buffer);; return;; // Control will speculate to the attacker-written address.; }; ```. However, this can be mitigated by hardening the load of the return address just; like any other load. This is sometimes complicated because x86 for example; *implicitly* loads the return address off the stack. However, the; implementation technique below is specifically designed to mitigate this; implicit load by using the stack pointer to communicate misspeculation between; functions. This additionally causes a misspeculation to have an invalid stack; pointer and never be able to read the speculatively stored return address. See; the detailed discussion below. For variant #1.2, the attacker speculatively stores into the vtable or jump; table used to implement an indirect call or indirect jump. Because this is; speculative, this will often be possible even when these are stored in; read-only pages. For example:; ```; class FancyObject : public BaseObject {; public:; void DoSomething() override;; };; void f(unsigned long attacker_offset, unsigned long attacker_data) {; FancyObject object = getMyObject();; unsigned long *arr[4] = getFourDataPointers();; if (attacker_offset < 4) {; // We have bypassed the bounds check speculatively.; unsign",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md:12328,load,loads,12328,interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,1,['load'],['loads']
Performance,"orkspace - Add private expensive object cache to workspace; RooBinningCategory - New real-to-category function that maps values of input RooRealVar to categories with labels that correspond to bins of input RooRealVar. See rf405_realtocatfuncs.C for details . RooStats; This is a new package introduced in this version for statistical tools built on top of RooFit. It is a joint effort between the LHC experiments and the ROOT team (see the RooStats Wiki page).; ; This version contains the interfaces for performing the statistical calculations and dealing with the obtained results and concrete classes implementing the statistical methods.; ; All the classes and functions in RooStats are provided in the namespace RooStats.; ; RooStats interfaces. ConfInterval: interface for describing a confidence interval. ; IntervalCalculator: interface for a statistical tool producing confidence intervals (class ConfInterval).; HypoTestResult: interface for representing results of a hypothesis test; HypoTestCalculator: interface for a statistical tool performing an hypothesis test. ; CombinedCalculator: interface for a statistical tool which can produce both hypothesis test results and confidence intervals. ; RooStats concrete classes. The concrete classes describing statistical tools implementing the above interfaces are:; ; ProfileLikelihoodCalculator: it is an implementation of a CombinedCalculator using the profile likelihood ratio as a test statistics. After configuring the calculator, one only needs to ask GetHypoTest() (which will return a HypoTestResult pointer) or GetInterval() (which will return an ConfInterval pointer).; LikelihoodInterval: concrete implementation of a ConfInterval interface. It implements connected N-dimensional intervals based on the contour of a likelihood ratio. The boundary of the interval is equivalent to a MINUIT/MINOS contour about the maximum likelihood estimator. . HybridCalculator: hypothesis test calculator using a Bayesian-frequentist hybrid me",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v522/index.html:10124,perform,performing,10124,roofit/doc/v522/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v522/index.html,2,['perform'],['performing']
Performance,"orm libA's full descriptor. A.h,; potentially only part of the descriptor of libA, expands to more than 26000; lines of code. ```cpp; // Main.cpp; #include ""A.h""; int main() {; do();; return 0;; }. ```; Main.cpp, reuses code from libA by including libA's descriptor and links against; libA. The full descriptor can contain thousands of files expanding to millions; of lines of code -- a common case for framework libraries, for instance. ROOT goes further and enhances C++ by allowing the following code to work without; explicitly requiring to `#include <A.h>`. Currently, ROOT's lack of support of; line `#5` is a long-standing, known limitation that is lifted with modules. ```cpp; // ROOT prompt; root [] AStruct<float> S0; // #1: implicit loading of libA. Full descriptor required.; root [] AStruct<float>* S1; // #2: implicit loading of libA. No full descriptor required.; root [] if (gFile) S1->doIt(); // #3: implicit loading of libA. Full descriptor required.; root [] gSystem->Load(""libA""); // #4: explicit loading of libA. No full descriptor required.; root [] do(); // #5: error: implicit loading of libA is currently unsupported. ```. This pattern is not only used in the ROOT prompt but in I/O hotspots such as; `ShowMembers` and `TClass::IsA`. A naive implementation of this feature would require inclusion of all reachable; library descriptors (aka header files) at ROOT startup time. Of course this is; not feasible and ROOT inserts a set of optimizations to fence itself from the; costly full header inclusion. Unfortunately, several of them are home-grown and; in a few cases inaccurate (eg line #5) causing a noticeable technical debt. Here we will briefly describe the three common layers of optimizations: ROOT PCH,; ROOTMAP and RDICT. The ROOT precompiled header (PCH) reduces the CPU and memory cost for ROOT's; most used libraries. The precompiled header technology is well-understood since; decades [[4]]. It is an efficient on-disk representation of the state of the; compil",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/README.CXXMODULES.md:5127,load,loading,5127,README/README.CXXMODULES.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/README.CXXMODULES.md,1,['load'],['loading']
Performance,"ormalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed). H No False − Print method-specific help message. CreateMVAPdfs No False − Create PDFs for classifier outputs (signal and background). IgnoreNegWeightsInTraining No False − Events with negative weights are ignored in the training (but are included for testing and performance evaluation). Gamma No 1 − RBF kernel parameter: Gamma (size of the Kernel). C No 1 − Cost parameter. Tol No 0.01 − Tolerance parameter. MaxIter No 1000 − Maximum number of training loops. Configuration options for MVA method :. Configuration options reference for MVA method: CFMlpANN. Option Array Default value Predefined values Description. V No False − Verbose output (short form of VerbosityLevel below - overrides the latter one). VerbosityLevel No Default Default, Debug, Verbose, Info, Warning, Error, Fatal Verbosity level. VarTransform No None − List of variable transformations performed before training, e.g., D_Background,P_Signal,G,N_AllClasses for: Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed). H No False − Print method-specific help message. CreateMVAPdfs No False − Create PDFs for classifier outputs (signal and background). IgnoreNegWeightsInTraining No False − Events with negative weights are ignored in the training (but are included for testing and performance evaluation). NCycles No 3000 − Number of training cycles. HiddenLayers No N,N-1 − Specification of hidden layer architecture. Configuration options for MVA method :. Configuration options reference for MVA method: KNN. Option Array Default value Predefined values Description. V No False − Verbose output (short form of VerbosityLevel below - overrides the latter one). VerbosityLevel No Default Default, Debug, Verbose, Info, Warnin",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/tmva/UsersGuide/optionRef.html:8736,perform,performed,8736,documentation/tmva/UsersGuide/optionRef.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/tmva/UsersGuide/optionRef.html,2,['perform'],['performed']
Performance,ormalizationCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-BatchNormalization-Cpu COMMAND testBatchNormalizationCpu). # DNN - Optimization CPU; ROOT_EXECUTABLE(testOptimizationCpu TestOptimizationCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-Optimization-Cpu COMMAND testOptimizationCpu). # DNN - MethodDL SGD Optimization CPU; ROOT_EXECUTABLE(testMethodDLSGDOptimizationCpu TestMethodDLSGDOptimizationCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-MethodDL-SGD-Optimization-Cpu COMMAND testMethodDLSGDOptimizationCpu). # DNN - MethodDL Adam Optimization CPU; ROOT_EXECUTABLE(testMethodDLAdamOptimizationCpu TestMethodDLAdamOptimizationCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-MethodDL-Adam-Optimization-Cpu COMMAND testMethodDLAdamOptimizationCpu TIMEOUT 1800). # DNN - MethodDL Adagrad Optimization CPU; ROOT_EXECUTABLE(testMethodDLAdagradOptimizationCpu TestMethodDLAdagradOptimizationCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-MethodDL-Adagrad-Optimization-Cpu COMMAND testMethodDLAdagradOptimizationCpu). # DNN - MethodDL RMSProp Optimization CPU; ROOT_EXECUTABLE(testMethodDLRMSPropOptimizationCpu TestMethodDLRMSPropOptimizationCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-MethodDL-RMSProp-Optimization-Cpu COMMAND testMethodDLRMSPropOptimizationCpu). # DNN - MethodDL Adadelta Optimization CPU; ROOT_EXECUTABLE(testMethodDLAdadeltaOptimizationCpu TestMethodDLAdadeltaOptimizationCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-MethodDL-Adadelta-Optimization-Cpu COMMAND testMethodDLAdadeltaOptimizationCpu). # DNN - Regression CPU; ROOT_EXECUTABLE(testRegressionCpu TestRegressionMethodDL.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-Regression-Cpu COMMAND testRegressionCpu). #( old-dnn-test ); # DNN - DataLoader CPU; ROOT_EXECUTABLE(testDataLoaderCpu TestDataLoaderCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-Data-Loader-Cpu COMMAND testDataLoaderCpu). # DNN - Minimization CPU; ROOT_EXECUTABLE(,MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tmva/tmva/test/DNN/CMakeLists.txt:6244,Optimiz,Optimization-Cpu,6244,tmva/tmva/test/DNN/CMakeLists.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tmva/tmva/test/DNN/CMakeLists.txt,1,['Optimiz'],['Optimization-Cpu']
Performance,"ormals are assumed to be preserved. Running LLVM code in an environment where these assumptions are not met can lead; to undefined behavior. The ``strictfp`` and ``denormal-fp-math`` attributes as; well as :ref:`Constrained Floating-Point Intrinsics <constrainedfp>` can be used; to weaken LLVM's assumptions and ensure defined behavior in non-default; floating-point environments; see their respective documentation for details. .. _floatnan:. Behavior of Floating-Point NaN values; -------------------------------------. A floating-point NaN value consists of a sign bit, a quiet/signaling bit, and a; payload (which makes up the rest of the mantissa except for the quiet/signaling; bit). LLVM assumes that the quiet/signaling bit being set to ``1`` indicates a; quiet NaN (QNaN), and a value of ``0`` indicates a signaling NaN (SNaN). In the; following we will hence just call it the ""quiet bit"". The representation bits of a floating-point value do not mutate arbitrarily; in; particular, if there is no floating-point operation being performed, NaN signs,; quiet bits, and payloads are preserved. For the purpose of this section, ``bitcast`` as well as the following operations; are not ""floating-point math operations"": ``fneg``, ``llvm.fabs``, and; ``llvm.copysign``. These operations act directly on the underlying bit; representation and never change anything except possibly for the sign bit. For floating-point math operations, unless specified otherwise, the following; rules apply when a NaN value is returned: the result has a non-deterministic; sign; the quiet bit and payload are non-deterministically chosen from the; following set of options:. - The quiet bit is set and the payload is all-zero. (""Preferred NaN"" case); - The quiet bit is set and the payload is copied from any input operand that is; a NaN. (""Quieting NaN propagation"" case); - The quiet bit and payload are copied from any input operand that is a NaN.; (""Unchanged NaN propagation"" case); - The quiet bit is set an",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:158218,perform,performed,158218,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performed']
Performance,"ormance Overhead. The performance overhead of this style of comprehensive mitigation is very; high. However, it compares very favorably with previously recommended; approaches such as the `lfence` instruction. Just as users can restrict the; scope of `lfence` to control its performance impact, this mitigation technique; could be restricted in scope as well. However, it is important to understand what it would cost to get a fully; mitigated baseline. Here we assume targeting a Haswell (or newer) processor and; using all of the tricks to improve performance (so leaves the low 2gb; unprotected and +/- 2gb surrounding any PC in the program). We ran both; Google's microbenchmark suite and a large highly-tuned server built using; ThinLTO and PGO. All were built with `-march=haswell` to give access to BMI2; instructions, and benchmarks were run on large Haswell servers. We collected; data both with an `lfence`-based mitigation and load hardening as presented; here. The summary is that mitigating with load hardening is 1.77x faster than; mitigating with `lfence`, and the overhead of load hardening compared to a; normal program is likely between a 10% overhead and a 50% overhead with most; large applications seeing a 30% overhead or less. | Benchmark | `lfence` | Load Hardening | Mitigated Speedup |; | -------------------------------------- | -------: | -------------: | ----------------: |; | Google microbenchmark suite | -74.8% | -36.4% | **2.5x** |; | Large server QPS (using ThinLTO & PGO) | -62% | -29% | **1.8x** |. Below is a visualization of the microbenchmark suite results which helps show; the distribution of results that is somewhat lost in the summary. The y-axis is; a log-scale speedup ratio of load hardening relative to `lfence` (up -> faster; -> better). Each box-and-whiskers represents one microbenchmark which may have; many different metrics measured. The red line marks the median, the box marks; the first and third quartiles, and the whiskers mark the min and m",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md:47468,load,load,47468,interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,1,['load'],['load']
Performance,"ormation`. 2.17 Allow MD5 Checksums to be Optionally Present; -------------------------------------------------. In DWARF Version 5 the file timestamp and file size can be optional, but if the; MD5 checksum is present it must be valid for all files. This is a problem if; using link time optimization to combine compilation units where some have MD5; checksums and some do not. Therefore, sSupport to allow MD5 checksums to be; optionally present in the line table is added. See :ref:`amdgpu-dwarf-line-number-information`. 2.18 Add the HIP Programing Language; ------------------------------------. The HIP programming language [:ref:`HIP <amdgpu-dwarf-HIP>`], which is supported; by the AMDGPU, is added. See :ref:`amdgpu-dwarf-language-names-table`. 2.19 Support for Source Language Optimizations that Result in Concurrent Iteration Execution; --------------------------------------------------------------------------------------------. A compiler can perform loop optimizations that result in the generated code; executing multiple iterations concurrently. For example, software pipelining; schedules multiple iterations in an interleaved fashion to allow the; instructions of one iteration to hide the latencies of the instructions of; another iteration. Another example is vectorization that can exploit SIMD; hardware to allow a single instruction to execute multiple iterations using; vector registers. Note that although this is similar to SIMT execution, the way a client debugger; uses the information is fundamentally different. In SIMT execution the debugger; needs to present the concurrent execution as distinct source language threads; that the user can list and switch focus between. With iteration concurrency; optimizations, such as software pipelining and vectorized SIMD, the debugger; must not present the concurrency as distinct source language threads. Instead,; it must inform the user that multiple loop iterations are executing in parallel; and allow the user to select bet",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst:32857,perform,perform,32857,interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst,3,"['concurren', 'optimiz', 'perform']","['concurrently', 'optimizations', 'perform']"
Performance,"ormed to have a *single* canonical induction variable; which starts at zero and steps by one.; * The canonical induction variable is guaranteed to be the first PHI node in; the loop header block.; * Any pointer arithmetic recurrences are raised to use array subscripts. If the trip count of a loop is computable, this pass also makes the following; changes:. * The exit condition for the loop is canonicalized to compare the induction; value against the exit value. This turns loops like:. .. code-block:: c++. for (i = 7; i*i < 1000; ++i). into. .. code-block:: c++. for (i = 0; i != 25; ++i). * Any use outside of the loop of an expression derived from the indvar is; changed to compute the derived value outside of the loop, eliminating the; dependence on the exit value of the induction variable. If the only purpose; of the loop is to compute the exit value of some derived expression, this; transformation will make the loop dead. This transformation should be followed by strength reduction after all of the; desired loop transformations have been performed. Additionally, on targets; where it is profitable, the loop could be transformed to count down to zero; (the ""do loop"" optimization). ``inline``: Function Integration/Inlining; -----------------------------------------. Bottom-up inlining of functions into callees. .. _passes-instcombine:. ``instcombine``: Combine redundant instructions; -----------------------------------------------. Combine instructions to form fewer, simple instructions. This pass does not; modify the CFG. This pass is where algebraic simplification happens. This pass combines things like:. .. code-block:: llvm. %Y = add i32 %X, 1; %Z = add i32 %Y, 1. into:. .. code-block:: llvm. %Z = add i32 %X, 2. This is a simple worklist driven algorithm. This pass guarantees that the following canonicalizations are performed on the; program:. #. If a binary operator has a constant operand, it is moved to the right-hand; side.; #. Bitwise operators with constant op",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:18991,perform,performed,18991,interpreter/llvm-project/llvm/docs/Passes.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst,1,['perform'],['performed']
Performance,"ort local frame objects through the; **`TVirtualViewer3D`** interface method: `PreferLocalFrame()`. If this; returns `kTRUE` you can make repeated calls to `AddObject()`, with; **`TBuffer3D`** containing the same `fID`, and different `fLocalMaster`; placements. For viewers supporting logical/physical objects, the TBuffer3D content; refers to the properties of the logical object, with the exception of:. - `fLocalMaster` transform. - `fColor `. - `fTransparency`. attributes, which can be varied for **each** physical object. As **a minimum requirement** all clients must be capable of filling the; raw tessellation of the object buffer, in the master reference frame.; Conversely viewers must always be capable of displaying the object; described by this buffer. If either does not meet this requirement the; object may not be displayed. #### Scene Rebuilds. `TBuffer3D::AddObject` is not an explicit command to the viewer - it may; for various reasons decide to ignore it:. - It already has the object internally cached. - The object falls outside some 'interest' limits of the viewer; camera. - The object is too small to be worth drawing. In all these cases `TBuffer3D::AddObject()` returns kNone, as it does; for successful addition, indicating it does not require further; information about this object. Hence you should not try to make any; assumptions about what the viewer did with the object. The viewer may; decide to force the client to rebuild (republish) the scene, obtaining a; different collection of objects, if the internal viewer state changes; .e.g. significant camera move. It does this presently by forcing a; repaint on the attached **`TPad`** object - hence you should attach you; master geometry object to the pad (via `TObject::Draw()`), and perform; the publishing to the viewer in response to **`TObject::Paint()`**. #### Physical IDs. TVirtualViewer3D provides for two methods of object addition:. ``` {.cpp}; virtual Int_t AddObject(const TBuffer3D &buffer,; Bool_t * a",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Graphics.md:136706,cache,cached,136706,documentation/users-guide/Graphics.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Graphics.md,1,['cache'],['cached']
Performance,"ory forms of; ADD LOGICAL WITH CARRY and SUBTRACT LOGICAL WITH BORROW, so the high; part of 128-bit memory operations would probably need to be done; via a register.). --. We don't use ICM, STCM, or CLM. --. We don't use ADD (LOGICAL) HIGH, SUBTRACT (LOGICAL) HIGH,; or COMPARE (LOGICAL) HIGH yet. --. DAGCombiner doesn't yet fold truncations of extended loads. Functions like:. unsigned long f (unsigned long x, unsigned short *y); {; return (x << 32) | *y;; }. therefore end up as:. sllg %r2, %r2, 32; llgh %r0, 0(%r3); lr %r2, %r0; br %r14. but truncating the load would give:. sllg %r2, %r2, 32; lh %r2, 0(%r3); br %r14. --. Functions like:. define i64 @f1(i64 %a) {; %and = and i64 %a, 1; ret i64 %and; }. ought to be implemented as:. lhi %r0, 1; ngr %r2, %r0; br %r14. but two-address optimizations reverse the order of the AND and force:. lhi %r0, 1; ngr %r0, %r2; lgr %r2, %r0; br %r14. CodeGen/SystemZ/and-04.ll has several examples of this. --. Out-of-range displacements are usually handled by loading the full; address into a register. In many cases it would be better to create; an anchor point instead. E.g. for:. define void @f4a(i128 *%aptr, i64 %base) {; %addr = add i64 %base, 524288; %bptr = inttoptr i64 %addr to i128 *; %a = load volatile i128 *%aptr; %b = load i128 *%bptr; %add = add i128 %a, %b; store i128 %add, i128 *%aptr; ret void; }. (from CodeGen/SystemZ/int-add-08.ll) we load %base+524288 and %base+524296; into separate registers, rather than using %base+524288 as a base for both. --. Dynamic stack allocations round the size to 8 bytes and then allocate; that rounded amount. It would be simpler to subtract the unrounded; size from the copy of the stack pointer and then align the result.; See CodeGen/SystemZ/alloca-01.ll for an example. --. If needed, we can support 16-byte atomics using LPQ, STPQ and CSDG. --. We might want to model all access registers and use them to spill; 32-bit values. --. We might want to use the 'overflow' condition of eg. AR to suppo",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt:2679,load,loading,2679,interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt,2,['load'],['loading']
Performance,"ory is located. - `/vctoolsdir:`; - `/vctoolsversion:`. If `/winsysroot:` is not specified, the `/vctoolsdir:` argument is consulted; as a location to identify where the Visual C++ Tools are located. If; `/vctoolsversion:` is specified, that version is preferred, otherwise, the; highest version detected is used. 2. Consult the environment. - `/external:[VARIABLE]`. This specifies a user identified environment variable which is treated as; a path delimiter (`;`) separated list of paths to map into `-imsvc`; arguments which are treated as `-isystem`. - `INCLUDE` and `EXTERNAL_INCLUDE`. The path delimiter (`;`) separated list of paths will be mapped to; `-imsvc` arguments which are treated as `-isystem`. - `LIB` (indirectly). The linker `link.exe` or `lld-link.exe` will honour the environment; variable `LIB` which is a path delimiter (`;`) set of paths to consult for; the import libraries to use when linking the final target. The following environment variables will be consulted and used to form paths; to validate and load content from as appropriate:. - `VCToolsInstallDir`; - `VCINSTALLDIR`; - `Path`. 3. Consult `ISetupConfiguration` [Windows Only]. Assuming that the toolchain is built with `USE_MSVC_SETUP_API` defined and; is running on Windows, the Visual Studio COM interface `ISetupConfiguration`; will be used to locate the installation of the MSVC toolset. 4. Fallback to the registry [DEPRECATED]. The registry information is used to help locate the installation as a final; fallback. This is only possible for pre-VS2017 installations and is; considered deprecated. Restrictions and Limitations compared to Clang; ----------------------------------------------. Strict Aliasing; ^^^^^^^^^^^^^^^. Strict aliasing (TBAA) is always off by default in clang-cl. Whereas in clang,; strict aliasing is turned on by default for all optimization levels. To enable LLVM optimizations based on strict aliasing rules (e.g., optimizations; based on type of expressions in C/C++), user wil",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:196660,load,load,196660,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,1,['load'],['load']
Performance,"ory ordering is not meaningful on load atomic instructions and is; treated a non-atomic. Acquire-release memory ordering is not meaningful on load or store atomic; instructions and is treated as acquire and release respectively. The memory order also adds the single thread optimization constraints defined in; table; :ref:`amdgpu-amdhsa-memory-model-single-thread-optimization-constraints-table`. .. table:: AMDHSA Memory Model Single Thread Optimization Constraints; :name: amdgpu-amdhsa-memory-model-single-thread-optimization-constraints-table. ============ ==============================================================; LLVM Memory Optimization Constraints; Ordering; ============ ==============================================================; unordered *none*; monotonic *none*; acquire - If a load atomic/atomicrmw then no following load/load; atomic/store/store atomic/atomicrmw/fence instruction can be; moved before the acquire.; - If a fence then same as load atomic, plus no preceding; associated fence-paired-atomic can be moved after the fence.; release - If a store atomic/atomicrmw then no preceding load/load; atomic/store/store atomic/atomicrmw/fence instruction can be; moved after the release.; - If a fence then same as store atomic, plus no following; associated fence-paired-atomic can be moved before the; fence.; acq_rel Same constraints as both acquire and release.; seq_cst - If a load atomic then same constraints as acquire, plus no; preceding sequentially consistent load atomic/store; atomic/atomicrmw/fence instruction can be moved after the; seq_cst.; - If a store atomic then the same constraints as release, plus; no following sequentially consistent load atomic/store; atomic/atomicrmw/fence instruction can be moved before the; seq_cst.; - If an atomicrmw/fence then same constraints as acq_rel.; ============ ==============================================================. The code sequences used to implement the memory model are defined in the; following sec",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:204920,load,load,204920,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"osed step is; negative. In this case, one can subsequently call; `TGeoManager::ComputeNormalFast()` to get the normal vector to the; crossed surface, after propagating the current point with the; `TGeoManager::GetStep()` value. This propagation can be done like:. ``` {.cpp}; Double_t *current_point = gGeoManager->GetCurrentPoint();; Double_t *current_dir = gGeoManager->GetCurrentDirection();; for (Int_t i=0; i<3; i++); current_point[i] += step * current_dir[I];; ```. Note: The method `TGeoManager::FindNextBoundary()` does not modify the; current point/direction nor the current volume/state. The returned node; is the next crossed one, but the physical path (state) AFTER crossing; the boundary is not determined. In order to find out this new state, one; has to propagate the point with a distance slightly bigger that the; computed step value (which is accurate within numerical precision). A; method that performs this task finding the next location is; `TGeoManager::Step()`, described in "" Making a Step "", but users may; implement more precise methods to insure post-step boundary crossing. ## Geometry Graphical User Interface. The geombuilder package allows you to create and edit geometries. The; package provides a library of all GUI classes related to geometry. Each; editable geometry class **`TGeoXXX`** have a correspondent editor; **`TGeoXXXEditor`** that provides a graphics user interface allowing to; edit some (or all) parameters of a geometry object. The editable objects; are geometry manager, volumes, nodes, shapes, media, materials and; matrices. The interfaces provide also access to specific functionality; of geometry objects. The editing mechanism is based on ROOT GED; (Graphics Editors) functionality and the library is loaded using the; plug-in mechanism. ### Editing a Geometry. There are two different use cases having different ways of invoking the; geometry editors. The first one applies when starting with geometry from; scratch and using the builder functi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md:165008,perform,performs,165008,documentation/users-guide/Geometry.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md,1,['perform'],['performs']
Performance,"osition can be set using; **`SetCurrentPosition(x,y,z)`** method of the manager class, in which; case **`FindNode()`** can be called without arguments. The method; returns a pointer to the *deepest node* that geometrically contains *P*; (in our case let us suppose it is *B\_3*). Since a node is just a; positioned volume, we can then get a pointer to the volume, medium or; material objects related to it. *Deepest* means that *B\_3* still; contains point *P* (as well as *A\_1* and *TOP\_1*), but none of the; daughters of volume **B** does. After finding out the node containing; the particle, one can check if the geometry state is different compared; to the last located point:. ``` {.cpp}; Bool_t *TGeoManager::IsSameLocation(); ```. The algorithm for finding where a point is located in geometry is; presented in the figure 17-36. It always starts by checking if the last computed modeller state is the; answer. This optimizes the search when continuously tracking a particle.; The main actions performed are:. - moving up and down in the logical node tree while updating the; current node and its global matrix; - converting the global position into the local frame of the current; node/volume; - checking whether the local position lies within the geometrical; shape of the current volume - if this is the case continue the; search downwards for the daughters of the current node, otherwise; search upwards its containers until the top level is reached.; - the number of candidate nodes to be checked at a given level is; minimized by an additional optimization structure: voxels. This is; effective even in case there is only one daughter of the current; volume.; - in case the current node is declared as possibly overlapping, the; method FindInCluster() is invoked. This method checks all different; possibilities within the cluster of overlapping candidates. One of; the candidates is prioritized if one of the following conditions id; fulfilled (in order):; - Is declared as non-overlap",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md:157193,perform,performed,157193,documentation/users-guide/Geometry.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md,1,['perform'],['performed']
Performance,"ossible to; use typed pointers using a number of opt-in flags. For users of the clang driver interface, it is possible to temporarily restore; the old default using the ``-DCLANG_ENABLE_OPAQUE_POINTERS=OFF`` cmake option,; or by passing ``-Xclang -no-opaque-pointers`` to a single clang invocation. For users of the clang cc1 interface, ``-no-opaque-pointers`` can be passed.; Note that the ``CLANG_ENABLE_OPAQUE_POINTERS`` cmake option has no effect on; the cc1 interface. Usage for LTO can be disabled by passing ``-Wl,-plugin-opt=no-opaque-pointers``; to the clang driver. For users of LLVM as a library, opaque pointers can be disabled by calling; ``setOpaquePointers(false)`` on the ``LLVMContext``. For users of LLVM tools like opt, opaque pointers can be disabled by passing; ``-opaque-pointers=0``. Version Support; ===============. **LLVM 14:** Supports all necessary APIs for migrating to opaque pointers and deprecates/removes incompatible APIs. However, using opaque pointers in the optimization pipeline is **not** fully supported. This release can be used to make out-of-tree code compatible with opaque pointers, but opaque pointers should **not** be enabled in production. **LLVM 15:** Opaque pointers are enabled by default. Typed pointers are still; supported. **LLVM 16:** Opaque pointers are enabled by default. Typed pointers are; supported on a best-effort basis only and not tested. **LLVM 17:** Only opaque pointers are supported. Typed pointers are not; supported. Transition State; ================. As of July 2023:. Typed pointers are **not** supported on the ``main`` branch. The following typed pointer functionality has been removed:. * The ``CLANG_ENABLE_OPAQUE_POINTERS`` cmake flag is no longer supported.; * The ``-no-opaque-pointers`` cc1 clang flag is no longer supported.; * The ``-opaque-pointers`` opt flag is no longer supported.; * The ``-plugin-opt=no-opaque-pointers`` LTO flag is no longer supported.; * C APIs that do not support opaque pointers (like ``",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:11845,optimiz,optimization,11845,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst,1,['optimiz'],['optimization']
Performance,"ot clear. Which headers belong to a particular; library, and in what order should those headers be included to; guarantee that they compile correctly? Are the headers C, C++,; Objective-C++, or one of the variants of these languages? What; declarations in those headers are actually meant to be part of the; API, and what declarations are present only because they had to be; written as part of the header file?. Semantic import; ---------------; Modules improve access to the API of software libraries by replacing the textual preprocessor inclusion model with a more robust, more efficient semantic model. From the user's perspective, the code looks only slightly different, because one uses an ``import`` declaration rather than a ``#include`` preprocessor directive:. .. code-block:: c. import std.io; // pseudo-code; see below for syntax discussion. However, this module import behaves quite differently from the corresponding ``#include <stdio.h>``: when the compiler sees the module import above, it loads a binary representation of the ``std.io`` module and makes its API available to the application directly. Preprocessor definitions that precede the import declaration have no impact on the API provided by ``std.io``, because the module itself was compiled as a separate, standalone module. Additionally, any linker flags required to use the ``std.io`` module will automatically be provided when the module is imported [#]_; This semantic import model addresses many of the problems of the preprocessor inclusion model:. * **Compile-time scalability**: The ``std.io`` module is only compiled once, and importing the module into a translation unit is a constant-time operation (independent of module system). Thus, the API of each software library is only parsed once, reducing the *M x N* compilation problem to an *M + N* problem. * **Fragility**: Each module is parsed as a standalone entity, so it has a consistent preprocessor environment. This completely eliminates the need for ``__",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/Modules.rst:4429,load,loads,4429,interpreter/llvm-project/clang/docs/Modules.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/Modules.rst,1,['load'],['loads']
Performance,"ot natively an interpreted; language! There is much more to say: chapter is indeed dedicated to; macros. ## Controlling ROOT ##. One more remark at this point: as every command you type into ROOT is; usually interpreted by Cling, an ""escape character"" is needed to pass; commands to ROOT directly. This character is the dot at the beginning of; a line:. ``` {.cpp}; root [1] .<command>; ```. This is a selection of the most common commands. - **quit root**, simply type `.q` or `.quit` or `.exit`. - obtain the full **list of commands**, use `.?` or `.help`. - **access the shell** of the operating system, type `.!<OS_command>`;; try, e.g. `.!ls` or `.!pwd`. - **execute a macro**, enter `.x <file_name>`; in the above example,; you might have used `.x slits.C` at the ROOT prompt. - **load a macro**, type `.L <file_name>`; in the above example, you; might instead have used the command `.L slits.C` followed by the; function call `slits();`. Note that after loading a macro all; functions and procedures defined therein are available at the ROOT; prompt. - **compile a macro**, type `.L <file_name>+`; ROOT is able to manage; for you the `C++` compiler behind the scenes and to produce machine; code starting from your macro. One could decide to compile a macro; in order to obtain better performance or to get nearer to the; production environment. ## Plotting Measurements ##. To display measurements in ROOT, including errors, there exists a; powerful class `TGraphErrors` with different types of constructors. In; the example here, we use data from the file `ExampleData.txt` in text; format:. ``` {.cpp}; root [0] TGraphErrors gr(""ExampleData.txt"");; root [1] gr.Draw(""AP"");; ```. You should see the output shown in Figure [2.2](#f22). [f22]: figures/TGraphErrors_Example.png ""f22""; <a name=""f22""></a>. ![Visualisation of data points with errors using the class TGraphErrors. \label{f22}][f22]. Make sure the file `ExampleData.txt` is available in the directory from; which you started ROOT. I",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/primer/ROOT_as_calculator.md:8400,load,loading,8400,documentation/primer/ROOT_as_calculator.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/primer/ROOT_as_calculator.md,1,['load'],['loading']
Performance,"ot naturally encode a needed element type. This is also; used for inline assembly. Note that some of the methods mentioned above only exist to support both typed; and opaque pointers at the same time, and will be dropped once the migration; has completed. For example, ``isOpaqueOrPointeeTypeEquals()`` becomes; meaningless once all pointers are opaque. While direct usage of pointer element types is immediately apparent in code,; there is a more subtle issue that opaque pointers need to contend with: A lot; of code assumes that pointer equality also implies that the used load/store; type or GEP source element type is the same. Consider the following examples; with typed and opaque pointers:. .. code-block:: llvm. define i32 @test(i32* %p) {; store i32 0, i32* %p; %bc = bitcast i32* %p to i64*; %v = load i64, i64* %bc; ret i64 %v; }. define i32 @test(ptr %p) {; store i32 0, ptr %p; %v = load i64, ptr %p; ret i64 %v; }. Without opaque pointers, a check that the pointer operand of the load and; store are the same also ensures that the accessed type is the same. Using a; different type requires a bitcast, which will result in distinct pointer; operands. With opaque pointers, the bitcast is not present, and this check is no longer; sufficient. In the above example, it could result in store to load forwarding; of an incorrect type. Code making such assumptions needs to be adjusted to; check the accessed type explicitly:; ``LI->getType() == SI->getValueOperand()->getType()``. Frontends; ---------. Frontends need to be adjusted to track pointee types independently of LLVM,; insofar as they are necessary for lowering. For example, clang now tracks the; pointee type in the ``Address`` structure. Frontends using the C API through an FFI interface should be aware that a; number of C API functions are deprecated and will be removed as part of the; opaque pointer transition::. LLVMBuildLoad -> LLVMBuildLoad2; LLVMBuildCall -> LLVMBuildCall2; LLVMBuildInvoke -> LLVMBuildInvoke2; LLVM",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:9195,load,load,9195,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst,1,['load'],['load']
Performance,"ot provide any general synchronization. It; essentially guarantees that if you take all the operations affecting a specific; address, a consistent ordering exists. Relevant standard; This corresponds to the C++/C ``memory_order_relaxed``; see those; standards for the exact definition. Notes for frontends; If you are writing a frontend which uses this directly, use with caution. The; guarantees in terms of synchronization are very weak, so make sure these are; only used in a pattern which you know is correct. Generally, these would; either be used for atomic operations which do not protect other memory (like; an atomic counter), or along with a ``fence``. Notes for optimizers; In terms of the optimizer, this can be treated as a read+write on the relevant; memory location (and alias analysis will take advantage of that). In addition,; it is legal to reorder non-atomic and Unordered loads around Monotonic; loads. CSE/DSE and a few other optimizations are allowed, but Monotonic; operations are unlikely to be used in ways which would make those; optimizations useful. Notes for code generation; Code generation is essentially the same as that for unordered for loads and; stores. No fences are required. ``cmpxchg`` and ``atomicrmw`` are required; to appear as a single operation. Acquire; -------. Acquire provides a barrier of the sort necessary to acquire a lock to access; other memory with normal loads and stores. Relevant standard; This corresponds to the C++/C ``memory_order_acquire``. It should also be; used for C++/C ``memory_order_consume``. Notes for frontends; If you are writing a frontend which uses this directly, use with caution.; Acquire only provides a semantic guarantee when paired with a Release; operation. Notes for optimizers; Optimizers not aware of atomics can treat this like a nothrow call. It is; also possible to move stores from before an Acquire load or read-modify-write; operation to after it, and move non-Acquire loads from before an Acquire; operati",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst:11108,optimiz,optimizations,11108,interpreter/llvm-project/llvm/docs/Atomics.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst,2,['optimiz'],['optimizations']
Performance,"ot, load the Quad class and create a heap object:. ``` {.cpp}; root[] .L Quad.cxx; root[] Quad *my_objptr = new Quad(1.,2.,-3.);; ```. To delete the object:. ``` {.cpp}; root[] delete my_objptr;; root[] my_objptr = 0;; ```. You should see the print out from its destructor. Setting the pointer; to zero afterwards is not strictly necessary (and Cling does it; automatically), but the object is no more accessible, and any attempt; to use the pointer again will, as has already been stated, cause; grief. So much for heap objects, but how are stack objects deleted? In; C++, a stack object is deleted as soon as control leaves the innermost; compound statement that encloses it. Therefore, it is singularly; futile to do something like:. ``` {.cpp}; root[] { Quad my_object(1.,2.,-3.); }; ```. Cling does not follow this rule; if you type in the above line, you; will not see the destructor message. As explained in the Script; lesson, you can load in compound statements, which would be a bit; pointless if everything disappeared as soon as it was loaded! Instead,; to reset the stack you have to type:. ``` {.cpp}; root[] gROOT->Reset();; ```. This sends the Reset message via the global pointer to the ROOT; object, which, amongst its many roles, acts as a resource manager.; Start ROOT again and type in the following:. ``` {.cpp}; root[] .L Quad.cxx; root[] Quad my_object(1.,2.,-3.);; root[] Quad *my_objptr = new Quad(4.,5.,-6.);; root[] gROOT->Reset();; ```. You will see that this deletes the first object but not the second. We; have also painted ourselves into a corner, as `my_objptr` was also on; the stack. This command will fail. ``` {.cpp}; root[] my_objptr->Solve();; ```. Cling no longer knows what `my_objptr` is. This is a great example of a; memory leak; the heap object exists but we have lost our way to access; it. In general, this is not a problem. If any object will outlive the; compound statement in which it was created then a more permanent; pointer will point to it, whic",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/ALittleC++.md:13824,load,load,13824,documentation/users-guide/ALittleC++.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/ALittleC++.md,2,['load'],"['load', 'loaded']"
Performance,"ot-project/root/issues/14302)] - The command ""root --notebook"" is not allowed on Windows 11; * [[#14277](https://github.com/root-project/root/issues/14277)] - Cling triggers a huge number of `openat` calls when loading libraries; * [[#14263](https://github.com/root-project/root/issues/14263)] - [tmva] When using DNN_USE_CBLAS, CMakeLists should link publicly to gsl instead of privately; * [[#14256](https://github.com/root-project/root/issues/14256)] - TAxis::GetTicks and TAxis::SetTicks are inconsistent. Significantly so.; * [[#14244](https://github.com/root-project/root/issues/14244)] - String comparison operators defined in TString.h should be defined as constexpr; * [[#14229](https://github.com/root-project/root/issues/14229)] - [6.30] root-config --git-revision broken; * [[#14225](https://github.com/root-project/root/issues/14225)] - [RF] Segmentation fault in ROOT 6.30 workspace creation; * [[#14223](https://github.com/root-project/root/issues/14223)] - Extremely long startup time when loading dictionaries with pyroot; * [[#14219](https://github.com/root-project/root/issues/14219)] - [cling] Use deduction guides for llvm::ArrayRef; * [[#14211](https://github.com/root-project/root/issues/14211)] - Implement value printing for `std::source_location`; * [[#14205](https://github.com/root-project/root/issues/14205)] - [cling] Replace llvm::Optional and llvm::None with std::optional; * [[#14199](https://github.com/root-project/root/issues/14199)] - Memory hoarding triggered by the TPluginManager; * [[#14188](https://github.com/root-project/root/issues/14188)] - cmake find_package ROOT 6.30 broken: it requires nlohmann-json; * [[#14163](https://github.com/root-project/root/issues/14163)] - cmake find_package ROOT broken with 6.30, nlohmann and vdt are builtin but not found; * [[#14162](https://github.com/root-project/root/issues/14162)] - `RooFFTConvPdf` is not working for ROOT 6.30/02; * [[#14157](https://github.com/root-project/root/issues/14157)] - Minuit2 standalon",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v632/index.md:34435,load,loading,34435,README/ReleaseNotes/v632/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v632/index.md,1,['load'],['loading']
Performance,"ot; natively supported. The introduction of the Legalize phases allows all of the; canonicalization patterns to be shared across targets, and makes it very easy to; optimize the canonicalized code because it is still in the form of a DAG. .. _optimizations:; .. _Optimize SelectionDAG:; .. _selector:. SelectionDAG Optimization Phase: the DAG Combiner; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. The SelectionDAG optimization phase is run multiple times for code generation,; immediately after the DAG is built and once after each legalization. The first; run of the pass allows the initial code to be cleaned up (e.g. performing; optimizations that depend on knowing that the operators have restricted type; inputs). Subsequent runs of the pass clean up the messy code generated by the; Legalize passes, which allows Legalize to be very simple (it can focus on making; code legal instead of focusing on generating *good* and legal code). One important class of optimizations performed is optimizing inserted sign and; zero extension instructions. We currently use ad-hoc techniques, but could move; to more rigorous techniques in the future. Here are some good papers on the; subject:. ""`Widening integer arithmetic <http://www.eecs.harvard.edu/~nr/pubs/widen-abstract.html>`_"" :raw-html:`<br>`; Kevin Redwine and Norman Ramsey :raw-html:`<br>`; International Conference on Compiler Construction (CC) 2004. ""`Effective sign extension elimination <http://portal.acm.org/citation.cfm?doid=512529.512552>`_"" :raw-html:`<br>`; Motohiro Kawahito, Hideaki Komatsu, and Toshio Nakatani :raw-html:`<br>`; Proceedings of the ACM SIGPLAN 2002 Conference on Programming Language Design; and Implementation. .. _Select instructions from DAG:. SelectionDAG Select Phase; ^^^^^^^^^^^^^^^^^^^^^^^^^. The Select phase is the bulk of the target-specific code for instruction; selection. This phase takes a legal SelectionDAG as input, pattern matches the; instructions supported by the target to this DAG, and",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst:45794,optimiz,optimizations,45794,interpreter/llvm-project/llvm/docs/CodeGenerator.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst,3,"['optimiz', 'perform']","['optimizations', 'optimizing', 'performed']"
Performance,"otential issues, consider the following example:. .. code-block:: llvm. define i32 @foo(i32 %bar, i1 %cond) {; entry:; call @llvm.dbg.value(metadata i32 0, metadata !1, metadata !2); br i1 %cond, label %truebr, label %falsebr; truebr:; %tval = add i32 %bar, 1; call @llvm.dbg.value(metadata i32 %tval, metadata !1, metadata !2); %g1 = call i32 @gazonk(); br label %exit; falsebr:; %fval = add i32 %bar, 2; call @llvm.dbg.value(metadata i32 %fval, metadata !1, metadata !2); %g2 = call i32 @gazonk(); br label %exit; exit:; %merge = phi [ %tval, %truebr ], [ %fval, %falsebr ]; %g = phi [ %g1, %truebr ], [ %g2, %falsebr ]; call @llvm.dbg.value(metadata i32 %merge, metadata !1, metadata !2); call @llvm.dbg.value(metadata i32 %g, metadata !3, metadata !2); %plusten = add i32 %merge, 10; %toret = add i32 %plusten, %g; call @llvm.dbg.value(metadata i32 %toret, metadata !1, metadata !2); ret i32 %toret; }. Containing two source-level variables in ``!1`` and ``!3``. The function could,; perhaps, be optimized into the following code:. .. code-block:: llvm. define i32 @foo(i32 %bar, i1 %cond) {; entry:; %g = call i32 @gazonk(); %addoper = select i1 %cond, i32 11, i32 12; %plusten = add i32 %bar, %addoper; %toret = add i32 %plusten, %g; ret i32 %toret; }. What ``llvm.dbg.value`` intrinsics should be placed to represent the original variable; locations in this code? Unfortunately the second, third and fourth; dbg.values for ``!1`` in the source function have had their operands; (%tval, %fval, %merge) optimized out. Assuming we cannot recover them, we; might consider this placement of dbg.values:. .. code-block:: llvm. define i32 @foo(i32 %bar, i1 %cond) {; entry:; call @llvm.dbg.value(metadata i32 0, metadata !1, metadata !2); %g = call i32 @gazonk(); call @llvm.dbg.value(metadata i32 %g, metadata !3, metadata !2); %addoper = select i1 %cond, i32 11, i32 12; %plusten = add i32 %bar, %addoper; %toret = add i32 %plusten, %g; call @llvm.dbg.value(metadata i32 %toret, metadata !1, metadat",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst:20870,optimiz,optimized,20870,interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,1,['optimiz'],['optimized']
Performance,"other hand sounds like a good idea, and the; > implementation seems fairly language-independent so it doesn't have the; > problems with malloc listed above. Okay, once we get the above stuff figured out, I'll put it all in the; spec. > About indirect call:; > Your option #2 sounded good to me. I'm not sure I understand your; > concern about an explicit 'icall' instruction?. I worry too much. :) The other alternative has been removed. 'icall' is; now up in the instruction list next to 'call'. > I believe tail calls are relatively easy to identify; do you know why; > .NET has a tailcall instruction?. Although I am just guessing, I believe it probably has to do with the fact; that they want languages like Haskell and lisp to be efficiently runnable; on their VM. Of course this means that the VM MUST implement tail calls; 'correctly', or else life will suck. :) I would put this into a future; feature bin, because it could be pretty handy... > A pair of important synchronization instr'ns to think about:; > load-linked; > store-conditional. What is 'load-linked'? I think that (at least for now) I should add these; to the 'possible extensions' section, because they are not immediately; needed... > Other classes of instructions that are valuable for pipeline; > performance:; > conditional-move ; > predicated instructions. Conditional move is effectly a special case of a predicated; instruction... and I think that all predicated instructions can possibly; be implemented later in LLVM. It would significantly change things, and; it doesn't seem to be very necessary right now. It would seem to; complicate flow control analysis a LOT in the virtual machine. I would; tend to prefer that a predicated architecture like IA64 convert from a; ""basic block"" representation to a predicated rep as part of it's dynamic; complication phase. Also, if a basic block contains ONLY a move, then; that can be trivally translated into a conditional move... > I agree that we need a static data space",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt:5909,load,load-linked,5909,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt,1,['load'],['load-linked']
Performance,"ou are done using the LLVM APIs, you should call ``llvm_shutdown()`` to; deallocate memory used for internal structures. .. _managedstatic:. Lazy Initialization with ``ManagedStatic``; ------------------------------------------. ``ManagedStatic`` is a utility class in LLVM used to implement static; initialization of static resources, such as the global type tables. In a; single-threaded environment, it implements a simple lazy initialization scheme.; When LLVM is compiled with support for multi-threading, however, it uses; double-checked locking to implement thread-safe lazy initialization. .. _llvmcontext:. Achieving Isolation with ``LLVMContext``; ----------------------------------------. ``LLVMContext`` is an opaque class in the LLVM API which clients can use to; operate multiple, isolated instances of LLVM concurrently within the same; address space. For instance, in a hypothetical compile-server, the compilation; of an individual translation unit is conceptually independent from all the; others, and it would be desirable to be able to compile incoming translation; units concurrently on independent server threads. Fortunately, ``LLVMContext``; exists to enable just this kind of scenario!. Conceptually, ``LLVMContext`` provides isolation. Every LLVM entity; (``Module``\ s, ``Value``\ s, ``Type``\ s, ``Constant``\ s, etc.) in LLVM's; in-memory IR belongs to an ``LLVMContext``. Entities in different contexts; *cannot* interact with each other: ``Module``\ s in different contexts cannot be; linked together, ``Function``\ s cannot be added to ``Module``\ s in different; contexts, etc. What this means is that is safe to compile on multiple; threads simultaneously, as long as no two threads operate on entities within the; same context. In practice, very few places in the API require the explicit specification of a; ``LLVMContext``, other than the ``Type`` creation/lookup APIs. Because every; ``Type`` carries a reference to its owning context, most other entities can; de",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ProgrammersManual.rst:122978,concurren,concurrently,122978,interpreter/llvm-project/llvm/docs/ProgrammersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ProgrammersManual.rst,1,['concurren'],['concurrently']
Performance,"ou can now load the; dictionary:. .. code-block:: python. >>> import cppyy; >>> cppyy.load_reflection_info('MyClassDict'); >>> cppyy.gbl.MyClass(42); <cppyy.gbl.MyClass object at 0x7ffb9f230950>; >>> print(_.get_int()); 42; >>>. and use the selected C++ entities as if the header was loaded. The dictionary shared library can be relocated, as long as it can be found; by the dynamic loader (e.g. through ``LD_LIBRARY_PATH``) and the header file; is fully embedded or still accessible (e.g. through a path added to; ``cppyy.add_include_path`` at run-time, or with ``-I`` to; ``rootcling``/``genreflex`` during build time).; When relocating the shared library, move the .pcm with it.; Once support for C++ modules is fully fleshed out, access to the header file; will no longer be needed. .. _`rootcling manual`: https://root.cern.ch/root/html/guides/users-guide/AddingaClass.html#the-linkdef.h-file; .. _`helper script`: https://github.com/wlav/cppyy/blob/master/test/make_dict_win32.py. Class loader; ^^^^^^^^^^^^. Explicitly loading dictionaries is fine if this is hidden under the hood of; a Python package and thus transparently done on ``import``.; Otherwise, the automatic class loader is more convenient, as it allows direct; use without having to manually find and load dictionaries (assuming these are; locatable by the dynamic loader). The class loader utilizes so-called rootmap files, which by convention should; live alongside the dictionary shared library (and C++ module file).; These are simple text files, which map C++ entities (such as classes) to the; dictionaries and other libraries that need to be loaded for their use. With ``genreflex``, the mapping file can be automatically created with; ``--rootmap-lib=MyClassDict``, where ""MyClassDict"" is the name of the shared; library (without the extension) build from the dictionary file.; With ``rootcling``, create the same mapping file with; ``-rmf MyClassDict.rootmap -rml MyClassDict``.; It is necessary to provide the final lib",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/utilities.rst:8159,load,loader,8159,bindings/pyroot/cppyy/cppyy/doc/source/utilities.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/utilities.rst,1,['load'],['loader']
Performance,"ough an intermediate package such as; HippoDraw @bib-HippoDraw, then the user's $\mbox{FCN}$ may be; supplied by the this package. The name of the user's class to implement the FCNBase interface may be; chosen freely (in documentation we give it the generic name; $\mbox{FCN}$). ### FCNBase::operator()(const std::vector$<$double$>$&) ###. The meaning of the vector of parameters std::vector$<$double$>$ in the; argument of FCNBase::operator() are of course defined by the user, who; uses the values of those parameters to calculate their function value. The; order and the position of these parameters is strictly the one specified; by the user when supplying the starting values for minimization. The starting values must be specified by the user, either via an; std::vector$<$double$>$ or the MnUserParameters (see [api:parameters]); supplied as input to the M minimizers such as VariableMetricMinimizer or; MnMigrad (see [api:migrad]). Later values are determined by M as it; searches for the minimum or performs whatever analysis is requested by; the user. ### FCNBase::up() ###. [howto:errordef] Returns the value of $\mbox{up}$ (default value; $= 1.$), defining parameter errors. M defines parameter errors as the; change in parameter value required to change the function value by; $\mbox{up}$. Normally, for chisquared fits $\mbox{up = 1}$, and; for negative log likelihood, $\mbox{up = 0.5}$. ### $\mbox{FCN}$ function with gradient ###. By default first derivatives are calculated numerically by M . In case; the user wants to supply their own gradient calculator (e.g. analytical; derivatives), they need to implement the FCNGradientBase interface. ![](figures/fcngradientbase.png). The size of the output vector is the same as of the input one. The same; is true for the position of the elements (first derivative of the; function with respect to the $n_\mathrm{th}$ variable has index $n$ in; the output vector). ## M parameters ##. Interaction with the parameters of the function are e",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/minuit2/Minuit2.md:29318,perform,performs,29318,documentation/minuit2/Minuit2.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/minuit2/Minuit2.md,1,['perform'],['performs']
Performance,"ould check that the argument pointer is in-bounds,; and is properly aligned, and if the checks fail it will either trap (in monolithic scheme); or call the slow path function (cross-DSO scheme).; The bit vector lookup is probably too complex for a hardware implementation. .. code-block:: none. // This instruction checks that 'Ptr'; // * is aligned by (1 << kAlignment) and; // * is inside [kRangeBeg, kRangeBeg+(kRangeSize<<kAlignment)); // and if the check fails it jumps to the given target (slow path).; //; // 'Ptr' is a register, pointing to the virtual function table; // or to the function which we need to check. We may require an explicit; // fixed register to be used.; // 'kAlignment' is a 4-bit constant.; // 'kRangeSize' is a ~20-bit constant.; // 'kRangeBeg' is a PC-relative constant (~28 bits); // pointing to the beginning of the allowed range for 'Ptr'.; // 'kFailedCheckTarget': is a PC-relative constant (~28 bits); // representing the target to branch to when the check fails.; // If kFailedCheckTarget==0, the process will trap; // (monolithic binary scheme).; // Otherwise it will jump to a handler that implements `CFI_SlowPath`; // (cross-DSO scheme).; CFI_Check(Ptr, kAlignment, kRangeSize, kRangeBeg, kFailedCheckTarget) {; if (Ptr < kRangeBeg ||; Ptr >= kRangeBeg + (kRangeSize << kAlignment) ||; Ptr & ((1 << kAlignment) - 1)); Jump(kFailedCheckTarget);; }. An alternative and more compact encoding would not use `kFailedCheckTarget`,; and will trap on check failure instead.; This will allow us to fit the instruction into **8-9 bytes**.; The cross-DSO checks will be performed by a trap handler and; performance-critical ones will have to be black-listed and checked using the; software-only scheme. Note that such hardware extension would be complementary to checks; at the callee side, such as e.g. **Intel ENDBRANCH**.; Moreover, CFI would have two benefits over ENDBRANCH: a) precision and b); ability to protect against invalid casts between polymorphic types.; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ControlFlowIntegrityDesign.rst:29873,perform,performed,29873,interpreter/llvm-project/clang/docs/ControlFlowIntegrityDesign.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ControlFlowIntegrityDesign.rst,2,['perform'],"['performance-critical', 'performed']"
Performance,"ould generated. Options are: ``asm``; for textual assembly ( ``'.s'``), ``obj`` for native object files (``'.o'``); and ``null`` for not emitting anything (for performance testing). Note that not all targets support all options. .. option:: -mattr=a1,+a2,-a3,... Override or control specific attributes of the target, such as whether SIMD; operations are enabled or not. The default set of attributes is set by the; current CPU. For a list of available attributes, use:. .. code-block:: none. llvm-as < /dev/null | llc -march=xyz -mattr=help. .. option:: --frame-pointer. Specify effect of frame pointer elimination optimization (all,non-leaf,none). .. option:: --disable-excess-fp-precision. Disable optimizations that may produce excess precision for floating point.; Note that this option can dramatically slow down code on some systems; (e.g. X86). .. option:: --enable-no-infs-fp-math. Enable optimizations that assume no Inf values. .. option:: --enable-no-nans-fp-math. Enable optimizations that assume no NAN values. .. option:: --enable-no-signed-zeros-fp-math. Enable FP math optimizations that assume the sign of 0 is insignificant. .. option:: --enable-no-trapping-fp-math. Enable setting the FP exceptions build attribute not to use exceptions. .. option:: --enable-unsafe-fp-math. Enable optimizations that make unsafe assumptions about IEEE math (e.g. that; addition is associative) or may not work for all input ranges. These; optimizations allow the code generator to make use of some instructions which; would otherwise not be usable (such as ``fsin`` on X86). .. option:: --stats. Print statistics recorded by code-generation passes. .. option:: --time-passes. Record the amount of time needed for each pass and print a report to standard; error. .. option:: --load=<dso_path>. Dynamically load ``dso_path`` (a path to a dynamically shared object) that; implements an LLVM target. This will permit the target name to be used with; the :option:`-march` option so that code can be ge",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llc.rst:3522,optimiz,optimizations,3522,interpreter/llvm-project/llvm/docs/CommandGuide/llc.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llc.rst,1,['optimiz'],['optimizations']
Performance,"ould increase the code size. Versioning LICM; ---------------. The pass hoists code out of loops that are only loop-invariant when; dynamic conditions apply. For instance, it transforms the loop. .. code-block:: c. for (int i = 0; i < n; i+=1) // original loop; A[i] = B[0];. into:. .. code-block:: c. if (rtc) {; auto b = B[0];; for (int i = 0; i < n; i+=1) // versioned loop; A[i] = b;; } else {; for (int i = 0; i < n; i+=1) // unversioned loop; A[i] = B[0];; }. The runtime condition (``rtc``) checks that the array ``A`` and the; element `B[0]` do not alias. Currently, this transformation does not support followup-attributes. Loop Interchange; ----------------. Currently, the ``LoopInterchange`` pass does not use any metadata. Ambiguous Transformation Order; ==============================. If there multiple transformations defined, the order in which they are; executed depends on the order in LLVM's pass pipeline, which is subject; to change. The default optimization pipeline (anything higher than; ``-O0``) has the following order. When using the legacy pass manager:. - LoopInterchange (if enabled); - SimpleLoopUnroll/LoopFullUnroll (only performs full unrolling); - VersioningLICM (if enabled); - LoopDistribute; - LoopVectorizer; - LoopUnrollAndJam (if enabled); - LoopUnroll (partial and runtime unrolling). When using the legacy pass manager with LTO:. - LoopInterchange (if enabled); - SimpleLoopUnroll/LoopFullUnroll (only performs full unrolling); - LoopVectorizer; - LoopUnroll (partial and runtime unrolling). When using the new pass manager:. - SimpleLoopUnroll/LoopFullUnroll (only performs full unrolling); - LoopDistribute; - LoopVectorizer; - LoopUnrollAndJam (if enabled); - LoopUnroll (partial and runtime unrolling). Leftover Transformations; ========================. Forced transformations that have not been applied after the last; transformation pass should be reported to the user. The transformation; passes themselves cannot be responsible for this reporting b",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TransformMetadata.rst:13684,optimiz,optimization,13684,interpreter/llvm-project/llvm/docs/TransformMetadata.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TransformMetadata.rst,1,['optimiz'],['optimization']
Performance,"ould like to generate name lookup tables that can be mapped into memory; from disk, and used as is, with little or no up-front parsing. We would also; be able to control the exact content of these different tables so they contain; exactly what we need. The Name Accelerator Tables were designed to fix these; issues. In order to solve these issues we need to:. * Have a format that can be mapped into memory from disk and used as is; * Lookups should be very fast; * Extensible table format so these tables can be made by many producers; * Contain all of the names needed for typical lookups out of the box; * Strict rules for the contents of tables. Table size is important and the accelerator table format should allow the reuse; of strings from common string tables so the strings for the names are not; duplicated. We also want to make sure the table is ready to be used as-is by; simply mapping the table into memory with minimal header parsing. The name lookups need to be fast and optimized for the kinds of lookups that; debuggers tend to do. Optimally we would like to touch as few parts of the; mapped table as possible when doing a name lookup and be able to quickly find; the name entry we are looking for, or discover there are no matches. In the; case of debuggers we optimized for lookups that fail most of the time. Each table that is defined should have strict rules on exactly what is in the; accelerator tables and documented so clients can rely on the content. Hash Tables; ^^^^^^^^^^^. Standard Hash Tables; """""""""""""""""""""""""""""""""""""""". Typical hash tables have a header, buckets, and each bucket points to the; bucket contents:. .. code-block:: none. .------------.; | HEADER |; |------------|; | BUCKETS |; |------------|; | DATA |; `------------'. The BUCKETS are an array of offsets to DATA for each hash:. .. code-block:: none. .------------.; | 0x00001000 | BUCKETS[0]; | 0x00002000 | BUCKETS[1]; | 0x00002200 | BUCKETS[2]; | 0x000034f0 | BUCKETS[3]; | | ...; | 0xXXXXXXXX | BUCKE",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst:60308,optimiz,optimized,60308,interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,1,['optimiz'],['optimized']
Performance,"oup fitpanel ROOT Fit Panel; \ingroup gui; \brief Classes forming the user interface of the Fit Panel in ROOT. ## The Fit Panel. \image html fitpanel.png. To display the Fit Panel right click on a histogram to pop up the; context menu, and then select the menu entry Fit Panel. By design, this user interface is planned to contain two tabs:; ""General"" and ""Minimization"". Currently, the ""General"" tab provides; user interface elements for setting the fit function, fit method and; different fit, draw, print options.; The ""Minimization tab"" provides the option to set the Minimizer to use in the fit and; its specific options. The fit panel is a modeless dialog, i.e. when opened, it does not; prevent users from interacting with other windows. Its first prototype; is a singleton application. When the Fit Panel is activated, users can; select an object for fitting in the usual way, i.e. by left-mouse; click on it. If the selected object is suitable for fitting, the fit; panel is connected with this object and users can perform fits by; setting different parameters and options. ### Function Choice and Settings. *‘Predefined' combo box* - contains a list of predefined functions in; ROOT. You have a choice of several polynomials, a Gaussian, a Landau,; and an Exponential function. The default one is Gaussian. *‘Operation' radio button group* defines the selected operational mode; between functions:. *Nop* - no operation (default);. *Add* - addition;. *Conv* - convolution (will be implemented in the future). Users can enter the function expression into the text entry field; below the ‘Predefined' combo box. The entered string is checked after; the Enter key was pressed and an error message shows up, if the; function string is not accepted. ‘*Set Parameters*' button opens a dialog for parameters settings,; which will be explained later. ### Fitter Settings. *‘Method' combo box* currently provides only two fit model choices:; Chi-square and Binned Likelihood. The default one is Chi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/gui/fitpanel/doc/index.md:1031,perform,perform,1031,gui/fitpanel/doc/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/gui/fitpanel/doc/index.md,1,['perform'],['perform']
Performance,"oup referred to by ``llvm.loop.parallel_accesses``, then the loop must; not be considered trivially parallel. Additional; memory dependence analysis is required to make that determination. As a fail; safe mechanism, this causes loops that were originally parallel to be considered; sequential (if optimization passes that are unaware of the parallel semantics; insert new memory instructions into the loop body). Example of a loop that is considered parallel due to its correct use of; both ``llvm.access.group`` and ``llvm.loop.parallel_accesses``; metadata types. .. code-block:: llvm. for.body:; ...; %val0 = load i32, ptr %arrayidx, !llvm.access.group !1; ...; store i32 %val0, ptr %arrayidx1, !llvm.access.group !1; ...; br i1 %exitcond, label %for.end, label %for.body, !llvm.loop !0. for.end:; ...; !0 = distinct !{!0, !{!""llvm.loop.parallel_accesses"", !1}}; !1 = distinct !{}. It is also possible to have nested parallel loops:. .. code-block:: llvm. outer.for.body:; ...; %val1 = load i32, ptr %arrayidx3, !llvm.access.group !4; ...; br label %inner.for.body. inner.for.body:; ...; %val0 = load i32, ptr %arrayidx1, !llvm.access.group !3; ...; store i32 %val0, ptr %arrayidx2, !llvm.access.group !3; ...; br i1 %exitcond, label %inner.for.end, label %inner.for.body, !llvm.loop !1. inner.for.end:; ...; store i32 %val1, ptr %arrayidx4, !llvm.access.group !4; ...; br i1 %exitcond, label %outer.for.end, label %outer.for.body, !llvm.loop !2. outer.for.end: ; preds = %for.body; ...; !1 = distinct !{!1, !{!""llvm.loop.parallel_accesses"", !3}} ; metadata for the inner loop; !2 = distinct !{!2, !{!""llvm.loop.parallel_accesses"", !3, !4}} ; metadata for the outer loop; !3 = distinct !{} ; access group for instructions in the inner loop (which are implicitly contained in outer loop as well); !4 = distinct !{} ; access group for instructions in the outer, but not the inner loop. .. _langref_llvm_loop_mustprogress:. '``llvm.loop.mustprogress``' Metadata; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:313571,load,load,313571,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,"ource spectrum. The calling program should fill; in the input parameters of the `one_dim_fit` structure. The fitted parameters; are written into structure pointed by `one_dim_fit` structure pointer; and fitted data are written into source spectrum. Function parameters:. - **`source`**: pointer to the vector of the source spectrum; - **`p`**: pointer to the `one_dim_fit` structure pointer; - **`size`**: length of the source spectrum. The `one_dim_fit` structure has the form of. ``` {.cpp}; class TSpectrumOneDimFit{. public:. int number_of_peaks; // input parameter, should be >0; int number_of_iterations; // input parameter, should be >0; int xmin; // first fitted channel; int xmax; // last fitted channel; double alpha; // convergence coefficient, input parameter, it should be a positive number and <=1; double chi; // here the function returns the resulting chi-square; int statistic_type; // type of statistics, possible values are:; // FIT1_OPTIM_CHI_COUNTS (chi square statistics with counts as weighting coefficients),; // FIT1_OPTIM_CHI_FUNC_VALUES (chi square statistics with function values as weighting coefficients); // FIT1_OPTIM_MAX_LIKELIHOOD; int alpha_optim; // optimization of convergence coefficients, possible values are:; // FIT1_ALPHA_HALVING,; // FIT1_ALPHA_OPTIMAL; int power; // possible values FIT1_FIT_POWER2,4,6,8,10,12; int fit_taylor; // order of Taylor expansion, possible values; // FIT1_TAYLOR_ORDER_FIRST, FIT1_TAYLOR_ORDER_SECOND. double position_init[MAX_NUMBER_OF_PEAKS1]; // initial values of peaks positions, input parameters; double position_calc[MAX_NUMBER_OF_PEAKS1]; // calculated values of fitted positions, output parameters; double position_err[MAX_NUMBER_OF_PEAKS1]; // position errors; bool fix_position[MAX_NUMBER_OF_PEAKS1]; // logical vector which allows to fix appropriate positions (not fit). However they are present in the estimated functional; double amp_init[MAX_NUMBER_OF_PEAKS1]; // initial values of peaks amplitudes, input parameters",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/spectrum/Spectrum.md:42047,optimiz,optimization,42047,documentation/spectrum/Spectrum.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/spectrum/Spectrum.md,1,['optimiz'],['optimization']
Performance,"ource-level state of the program**, regardless of which LLVM; optimizations have been run. :doc:`HowToUpdateDebugInfo` specifies how debug; info should be updated in various kinds of code transformations to avoid; breaking this guarantee, and how to preserve as much useful debug info as; possible. Note that some optimizations may impact the ability to modify the; current state of the program with a debugger, such as setting program; variables, or calling functions that have been deleted. * As desired, LLVM optimizations can be upgraded to be aware of debugging; information, allowing them to update the debugging information as they; perform aggressive optimizations. This means that, with effort, the LLVM; optimizers could optimize debug code just as well as non-debug code. * LLVM debug information does not prevent optimizations from; happening (for example inlining, basic block reordering/merging/cleanup,; tail duplication, etc). * LLVM debug information is automatically optimized along with the rest of; the program, using existing facilities. For example, duplicate; information is automatically merged by the linker, and unused information; is automatically removed. Basically, the debug information allows you to compile a program with; ""``-O0 -g``"" and get full debug information, allowing you to arbitrarily modify; the program as it executes from a debugger. Compiling a program with; ""``-O3 -g``"" gives you full debug information that is always available and; accurate for reading (e.g., you get accurate stack traces despite tail call; elimination and inlining), but you might lose the ability to modify the program; and call functions which were optimized out of the program, or inlined away; completely. The :doc:`LLVM test-suite <TestSuiteMakefileGuide>` provides a framework to; test the optimizer's handling of debugging information. It can be run like; this:. .. code-block:: bash. % cd llvm/projects/test-suite/MultiSource/Benchmarks # or some other level; % make TEST=d",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst:4934,optimiz,optimized,4934,interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,1,['optimiz'],['optimized']
Performance,"out of sequence.; By setting TTree::SetClusterPrefetch(), an entire clusters will be loaded into memory, rather than single baskets.; By setting the MaxVirtualSize of the tree to a negative value, previous clusters will be retained; (the absolute value of MaxVirtualSize indicates how many additional clusters will be kept in memory).; - Added ```TBranchProxy::GetEntries``` to support leaflist variable size array and added ```TBranchProxy::GetArrayLength```.; - In ```TBranch::Streamer``` insured that we never steam already basket already written to disk. ### TDataFrame. #### New features; - Add `Alias`, a facility to specify an alternative name for a given column: `auto histo = mytdf.Alias(""myAlias"", ""myColumn"").Histo1D(""myAlias"");`. Especially useful for pyROOT users to deal with column names that are not valid C++ identifiers (e.g. `Filter(""1branch > 0"") --> Alias(""1branch"", ""branch1"").Filter(""branch1 > 0"")`.; - Add `Cache`, a facility to cache `TDataFrame`s in memory. All or some columns can be cached. Two versions of the method are proposed: one which allows to explicitly list the types of the columns and another one allowing to let the system infer them (the same mechanism of the `Snapshot` method). Only columns containing instances of classes which have a copy constructor can be cached.; - Add `DefineSlot`, a `Define` transformation that is aware of the multi-threading slot where the workload is executed; - Add `DefineSlotEntry`, a `Define` transformation that is aware of the multi-threading slot and of the current entry number; - Add `GetColumnsNames`: users can now get the names of the available columns coming from trees, data sources or `Define`d columns; - Add `OnPartialResult` and `OnPartialResultSlot`: users can now register one or more functions to be executed on partial results of TDF actions during the event loop.; This mechanism is meant to be used to inspect partial results of the analysis or print useful debug information.; For example, both in single",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v612/index.md:11036,cache,cached,11036,README/ReleaseNotes/v612/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v612/index.md,1,['cache'],['cached']
Performance,"ove`` call; below can not be instantiated using a Python string, but the; ``std::string::value_type`` must be used instead:. .. code-block:: python. >>> cppstr = cppyy.gbl.std.string; >>> n = cppstr('this is a C++ string'); >>> print(n); this is a C++ string; >>> n.erase(cppyy.gbl.std.remove(n.begin(), n.end(), cppstr.value_type(' '))); <cppyy.gbl.__wrap_iter<char*> object at 0x7fba35d1af50>; >>> print(n); thisisaC++stringing; >>>. `Reduced typing`; ----------------. Note: ``from cppyy.interactive import *`` is no longer supported for CPython; 3.11 and later because the ``dict`` object features it relies on have been; removed. Typing ``cppyy.gbl`` all the time gets old rather quickly, but the dynamic; nature of ``cppyy`` makes something like ``from cppyy.gbl import *``; impossible.; For example, classes can be defined dynamically after that statement and then; they would be missed by the import.; In scripts, it is easy enough to rebind names to achieve a good amount of; reduction in typing (and a modest performance improvement to boot, because of; fewer dictionary lookups), e.g.:. .. code-block:: python. import cppyy; std = cppyy.gbl.std; v = std.vector[int](range(10)). But even such rebinding becomes annoying for (brief) interactive sessions. For CPython only (and not with tools such as IPython or in IDEs that replace; the interactive prompt), there is a fix, using; ``from cppyy.interactive import *``.; This makes lookups in the global dictionary of the current frame also; consider everything under ``cppyy.gbl``.; This feature comes with a performance `penalty` and is not meant for; production code.; Example usage:. .. code-block:: python. >>> from cppyy.interactive import *; >>> v = std.vector[int](range(10)); >>> print(list(v)); [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]; >>>; >>> cppdef(""struct SomeStruct {};""); True; >>> s = SomeStruct() # <- dynamically made available; >>> s; <cppyy.gbl.SomeStruct object at 0x7fa9b8624320>; >>>. For PyPy, IPython, etc. ``cppyy.gbl`` is sim",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/misc.rst:4526,perform,performance,4526,bindings/pyroot/cppyy/cppyy/doc/source/misc.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/misc.rst,1,['perform'],['performance']
Performance,"over the snippet directly, first duplicate it so that the; loop body contains at least this many instructions. This potentially results; in loop body being cached in the CPU Op Cache / Loop Cache, which allows to; which may have higher throughput than the CPU decoders. .. option:: --max-configs-per-opcode=<value>. Specify the maximum configurations that can be generated for each opcode.; By default this is `1`, meaning that we assume that a single measurement is; enough to characterize an opcode. This might not be true of all instructions:; for example, the performance characteristics of the LEA instruction on X86; depends on the value of assigned registers and immediates. Setting a value of; `-max-configs-per-opcode` larger than `1` allows `llvm-exegesis` to explore; more configurations to discover if some register or immediate assignments; lead to different performance characteristics. .. option:: --benchmarks-file=</path/to/file>. File to read (`analysis` mode) or write (`latency`/`uops`/`inverse_throughput`; modes) benchmark results. ""-"" uses stdin/stdout. .. option:: --analysis-clusters-output-file=</path/to/file>. If provided, write the analysis clusters as CSV to this file. ""-"" prints to; stdout. By default, this analysis is not run. .. option:: --analysis-inconsistencies-output-file=</path/to/file>. If non-empty, write inconsistencies found during analysis to this file. `-`; prints to stdout. By default, this analysis is not run. .. option:: --analysis-filter=[all|reg-only|mem-only]. By default, all benchmark results are analysed, but sometimes it may be useful; to only look at those that to not involve memory, or vice versa. This option; allows to either keep all benchmarks, or filter out (ignore) either all the; ones that do involve memory (involve instructions that may read or write to; memory), or the opposite, to only keep such benchmarks. .. option:: --analysis-clustering=[dbscan,naive]. Specify the clustering algorithm to use. By default DBSCAN will b",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-exegesis.rst:14559,latency,latency,14559,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-exegesis.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-exegesis.rst,1,['latency'],['latency']
Performance,"overloaded intrinsic. You can use ``llvm.smul.fix``; on any integer bit width or vectors of integers. ::. declare i16 @llvm.smul.fix.i16(i16 %a, i16 %b, i32 %scale); declare i32 @llvm.smul.fix.i32(i32 %a, i32 %b, i32 %scale); declare i64 @llvm.smul.fix.i64(i64 %a, i64 %b, i32 %scale); declare <4 x i32> @llvm.smul.fix.v4i32(<4 x i32> %a, <4 x i32> %b, i32 %scale). Overview; """""""""""""""""". The '``llvm.smul.fix``' family of intrinsic functions perform signed; fixed point multiplication on 2 arguments of the same scale. Arguments; """""""""""""""""""". The arguments (%a and %b) and the result may be of integer types of any bit; width, but they must have the same bit width. The arguments may also work with; int vectors of the same length and int size. ``%a`` and ``%b`` are the two; values that will undergo signed fixed point multiplication. The argument; ``%scale`` represents the scale of both operands, and must be a constant; integer. Semantics:; """""""""""""""""""". This operation performs fixed point multiplication on the 2 arguments of a; specified scale. The result will also be returned in the same scale specified; in the third argument. If the result value cannot be precisely represented in the given scale, the; value is rounded up or down to the closest representable value. The rounding; direction is unspecified. It is undefined behavior if the result value does not fit within the range of; the fixed point type. Examples; """""""""""""""""". .. code-block:: llvm. %res = call i4 @llvm.smul.fix.i4(i4 3, i4 2, i32 0) ; %res = 6 (2 x 3 = 6); %res = call i4 @llvm.smul.fix.i4(i4 3, i4 2, i32 1) ; %res = 3 (1.5 x 1 = 1.5); %res = call i4 @llvm.smul.fix.i4(i4 3, i4 -2, i32 1) ; %res = -3 (1.5 x -1 = -1.5). ; The result in the following could be rounded up to -2 or down to -2.5; %res = call i4 @llvm.smul.fix.i4(i4 3, i4 -3, i32 1) ; %res = -5 (or -4) (1.5 x -1.5 = -2.25). '``llvm.umul.fix.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax; """""""""""""". This is an overloaded intrinsic. You can use ``llv",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:622174,perform,performs,622174,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performs']
Performance,"ow various primitives in the refactoring API; can be used to implement different refactoring actions. The :doc:`LibTooling`; library provides several other APIs that are used when developing a; refactoring action. Refactoring engine can be used to implement local refactorings that are; initiated using a selection in an editor or an IDE. You can combine; :doc:`AST matchers<LibASTMatchers>` and the refactoring engine to implement; refactorings that don't lend themselves well to source selection and/or have to; query ASTs for some particular nodes. We assume basic knowledge about the Clang AST. See the :doc:`Introduction; to the Clang AST <IntroductionToTheClangAST>` if you want to learn more; about how the AST is structured. .. FIXME: create new refactoring action tutorial and link to the tutorial. Introduction; ------------. Clang's refactoring engine defines a set refactoring actions that implement; a number of different source transformations. The ``clang-refactor``; command-line tool can be used to perform these refactorings. Certain; refactorings are also available in other clients like text editors and IDEs. A refactoring action is a class that defines a list of related refactoring; operations (rules). These rules are grouped under a common umbrella - a single; ``clang-refactor`` command. In addition to rules, the refactoring action; provides the action's command name and description to ``clang-refactor``.; Each action must implement the ``RefactoringAction`` interface. Here's an; outline of a ``local-rename`` action:. .. code-block:: c++. class LocalRename final : public RefactoringAction {; public:; StringRef getCommand() const override { return ""local-rename""; }. StringRef getDescription() const override {; return ""Finds and renames symbols in code with no indexer support"";; }. RefactoringActionRules createActionRules() const override {; ...; }; };. Refactoring Action Rules; ------------------------. An individual refactoring action is responsible for creating",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/RefactoringEngine.rst:1211,perform,perform,1211,interpreter/llvm-project/clang/docs/RefactoringEngine.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/RefactoringEngine.rst,1,['perform'],['perform']
Performance,"ow; many physical registers are available for register renaming purposes. A value; of zero for this flag means ""unlimited number of physical registers"". .. option:: -iterations=<number of iterations>. Specify the number of iterations to run. If this flag is set to 0, then the; tool sets the number of iterations to a default value (i.e. 100). .. option:: -noalias=<bool>. If set, the tool assumes that loads and stores don't alias. This is the; default behavior. .. option:: -lqueue=<load queue size>. Specify the size of the load queue in the load/store unit emulated by the tool.; By default, the tool assumes an unbound number of entries in the load queue.; A value of zero for this flag is ignored, and the default load queue size is; used instead. .. option:: -squeue=<store queue size>. Specify the size of the store queue in the load/store unit emulated by the; tool. By default, the tool assumes an unbound number of entries in the store; queue. A value of zero for this flag is ignored, and the default store queue; size is used instead. .. option:: -timeline. Enable the timeline view. .. option:: -timeline-max-iterations=<iterations>. Limit the number of iterations to print in the timeline view. By default, the; timeline view prints information for up to 10 iterations. .. option:: -timeline-max-cycles=<cycles>. Limit the number of cycles in the timeline view, or use 0 for no limit. By; default, the number of cycles is set to 80. .. option:: -resource-pressure. Enable the resource pressure view. This is enabled by default. .. option:: -register-file-stats. Enable register file usage statistics. .. option:: -dispatch-stats. Enable extra dispatch statistics. This view collects and analyzes instruction; dispatch events, as well as static/dynamic dispatch stall events. This view; is disabled by default. .. option:: -scheduler-stats. Enable extra scheduler statistics. This view collects and analyzes instruction; issue events. This view is disabled by default. .. option:: -retir",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst:4662,queue,queue,4662,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,1,['queue'],['queue']
Performance,"ower than ``double``.; * ``extended`` The compiler uses ``long double`` as the floating-point evaluation method for all float expressions of type that is narrower than ``long double``. .. option:: -f[no-]protect-parens. This option pertains to floating-point types, complex types with; floating-point components, and vectors of these types. Some arithmetic; expression transformations that are mathematically correct and permissible; according to the C and C++ language standards may be incorrect when dealing; with floating-point types, such as reassociation and distribution. Further,; the optimizer may ignore parentheses when computing arithmetic expressions; in circumstances where the parenthesized and unparenthesized expression; express the same mathematical value. For example (a+b)+c is the same; mathematical value as a+(b+c), but the optimizer is free to evaluate the; additions in any order regardless of the parentheses. When enabled, this; option forces the optimizer to honor the order of operations with respect; to parentheses in all circumstances.; Defaults to ``-fno-protect-parens``. Note that floating-point contraction (option `-ffp-contract=`) is disabled; when `-fprotect-parens` is enabled. Also note that in safe floating-point; modes, such as `-ffp-model=precise` or `-ffp-model=strict`, this option; has no effect because the optimizer is prohibited from making unsafe; transformations. .. option:: -fexcess-precision:. The C and C++ standards allow floating-point expressions to be computed as if; intermediate results had more precision (and/or a wider range) than the type; of the expression strictly allows. This is called excess precision; arithmetic.; Excess precision arithmetic can improve the accuracy of results (although not; always), and it can make computation significantly faster if the target lacks; direct hardware support for arithmetic in a particular type. However, it can; also undermine strict floating-point reproducibility. Under the standards, as",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:65809,optimiz,optimizer,65809,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,1,['optimiz'],['optimizer']
Performance,"owing global; data read is no; older than a local load; atomic value being; acquired. 3. buffer_wbinvl1_vol. - If not TgSplit execution; mode, omit.; - Ensures that; following; loads will not see; stale data. load atomic acquire - agent - global 1. buffer/global_load; glc=1; 2. s_waitcnt vmcnt(0). - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the load; has completed; before invalidating; the cache. 3. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale global data. load atomic acquire - system - global 1. buffer/global/flat_load; glc=1; 2. s_waitcnt vmcnt(0). - Must happen before; following buffer_invl2 and; buffer_wbinvl1_vol.; - Ensures the load; has completed; before invalidating; the cache. 3. buffer_invl2;; buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale L1 global data,; nor see stale L2 MTYPE; NC global data.; MTYPE RW and CC memory will; never be stale in L2 due to; the memory probes. load atomic acquire - agent - generic 1. flat_load glc=1; 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL omit; lgkmcnt(0).; - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the flat_load; has completed; before invalidating; the cache. 3. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. load atomic acquire - system - generic 1. flat_load glc=1; 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL omit; lgkmcnt(0).; - Must happen before; following; buffer_invl2 and; buffer_wbinvl1_vol.; - Ensures the flat_load; has completed; before invalidating; the caches. 3. buffer_invl2;; buffer_wbinvl1_vol. - Must happen before; any following;",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:247524,load,loads,247524,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['loads']
Performance,"owing; loads will not see; stale global data. load atomic acquire - system - global 1. buffer/global/flat_load; sc0=1 sc1=1; 2. s_waitcnt vmcnt(0). - Must happen before; following; buffer_inv.; - Ensures the load; has completed; before invalidating; the cache. 3. buffer_inv sc0=1 sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale MTYPE NC global data.; MTYPE RW and CC memory will; never be stale due to the; memory probes. load atomic acquire - agent - generic 1. flat_load sc1=1; 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL omit; lgkmcnt(0).; - Must happen before; following; buffer_inv.; - Ensures the flat_load; has completed; before invalidating; the cache. 3. buffer_inv sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. load atomic acquire - system - generic 1. flat_load sc0=1 sc1=1; 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL omit; lgkmcnt(0).; - Must happen before; the following; buffer_inv.; - Ensures the flat_load; has completed; before invalidating; the caches. 3. buffer_inv sc0=1 sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale MTYPE NC global data.; MTYPE RW and CC memory will; never be stale due to the; memory probes. atomicrmw acquire - singlethread - global 1. buffer/global/flat_atomic; - wavefront - generic; atomicrmw acquire - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; be used.*. 1. ds_atomic; atomicrmw acquire - workgroup - global 1. buffer/global_atomic; 2. s_waitcnt vmcnt(0). - If not TgSplit execution; mode, omit.; - Must happen before the; following buffer_inv.; - Ensures the atomicrmw; has completed; before inval",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:298315,load,load,298315,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"p - global 1. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL, omit; lgkmcnt(0).; - Must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load/store/; load atomic/store atomic/; atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; have; completed before; performing the; atomicrmw that is; being released. 2. buffer/global_atomic; 3. s_waitcnt vmcnt(0). - If not TgSplit execution; mode, omit.; - Must happen before; the following; buffer_inv.; - Ensures any; following global; data read is no; older than the; atomicrmw value; being acquired. 4. buffer_inv sc0=1. - If not TgSplit execution; mode, omit.; - Ensures that; following; loads will not see; stale data. atomicrmw acq_rel - workgroup - local *If TgSplit execution mode,; local address space cannot; be used.*. 1. ds_atomic; 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than the local load; atomic value being; acquired. atomicrmw acq_rel - workgroup - generic 1. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL, omit; lgkmcnt(0).; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load/store/; load atomic/store atomic/; atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; have; completed before; performing the; atomicrmw that is; being rele",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:318028,load,loads,318028,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['loads']
Performance,"p annotations. Instruction Annotations; =======================. .. _contextual markups:. Contextual markups; ------------------. Annotated assembly display will supply contextual markup to help clients more; efficiently implement things like pretty printers. Most markup will be target; independent, so clients can effectively provide good display without any target; specific knowledge. Annotated assembly goes through the normal instruction printer, but optionally; includes contextual tags on portions of the instruction string. An annotation; is any '<' '>' delimited section of text(1). .. code-block:: bat. annotation: '<' tag-name tag-modifier-list ':' annotated-text '>'; tag-name: identifier; tag-modifier-list: comma delimited identifier list. The tag-name is an identifier which gives the type of the annotation. For the; first pass, this will be very simple, with memory references, registers, and; immediates having the tag names ""mem"", ""reg"", and ""imm"", respectively. The tag-modifier-list is typically additional target-specific context, such as; register class. Clients should accept and ignore any tag-names or tag-modifiers they do not; understand, allowing the annotations to grow in richness without breaking older; clients. For example, a possible annotation of an ARM load of a stack-relative location; might be annotated as:. .. code-block:: text. ldr <reg gpr:r0>, <mem regoffset:[<reg gpr:sp>, <imm:#4>]>. 1: For assembly dialects in which '<' and/or '>' are legal tokens, a literal token is escaped by following immediately with a repeat of the character. For example, a literal '<' character is output as '<<' in an annotated assembly string. C API Details; -------------. The intended consumers of this information use the C API, therefore the new C; API function for the disassembler will be added to provide an option to produce; disassembled instructions with annotations, ``LLVMSetDisasmOptions()`` and the; ``LLVMDisassembler_Option_UseMarkup`` option (see above).; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MarkedUpDisassembly.rst:2646,load,load,2646,interpreter/llvm-project/llvm/docs/MarkedUpDisassembly.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MarkedUpDisassembly.rst,1,['load'],['load']
Performance,"p is to compute the exit value of some derived expression, this; transformation will make the loop dead. This transformation should be followed by strength reduction after all of the; desired loop transformations have been performed. Additionally, on targets; where it is profitable, the loop could be transformed to count down to zero; (the ""do loop"" optimization). ``inline``: Function Integration/Inlining; -----------------------------------------. Bottom-up inlining of functions into callees. .. _passes-instcombine:. ``instcombine``: Combine redundant instructions; -----------------------------------------------. Combine instructions to form fewer, simple instructions. This pass does not; modify the CFG. This pass is where algebraic simplification happens. This pass combines things like:. .. code-block:: llvm. %Y = add i32 %X, 1; %Z = add i32 %Y, 1. into:. .. code-block:: llvm. %Z = add i32 %X, 2. This is a simple worklist driven algorithm. This pass guarantees that the following canonicalizations are performed on the; program:. #. If a binary operator has a constant operand, it is moved to the right-hand; side.; #. Bitwise operators with constant operands are always grouped so that shifts; are performed first, then ``or``\ s, then ``and``\ s, then ``xor``\ s.; #. Compare instructions are converted from ``<``, ``>``, ``≤``, or ``≥`` to; ``=`` or ``≠`` if possible.; #. All ``cmp`` instructions on boolean values are replaced with logical; operations.; #. ``add X, X`` is represented as ``mul X, 2`` ⇒ ``shl X, 1``; #. Multiplies with a constant power-of-two argument are transformed into; shifts.; #. … etc. This pass can also simplify calls to specific well-known function calls (e.g.; runtime library functions). For example, a call ``exit(3)`` that occurs within; the ``main()`` function can be transformed into simply ``return 3``. Whether or; not library calls are simplified is controlled by the; :ref:`-function-attrs <passes-function-attrs>` pass and LLVM's knowledge o",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:19786,perform,performed,19786,interpreter/llvm-project/llvm/docs/Passes.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst,1,['perform'],['performed']
Performance,"p, ``0 <= i < count``, hence ``1 <= i + 1 <= count``.; * Pointer arithmetic ``p + count`` in the if-condition doesn’t wrap.; * ``-fbounds-safety`` treats pointer arithmetic overflow as deterministically; two’s complement computation, not an undefined behavior. Therefore,; getelementptr does not typically have inbounds keyword. However, the compiler; does emit inbounds for ``p + count`` in this case because; ``__counted_by(count)`` has the invariant that p has at least as many as; elements as count. Using this information, ``ConstraintElimination`` is able; to determine ``p + count`` doesn’t wrap.; * Accordingly, ``p + i`` and ``p + i + 1`` also don’t wrap.; * Therefore, ``p <= p + i`` and ``p + i + 1 <= p + count``.; * The if-condition simplifies to false and becomes dead code that the subsequent; optimization passes can remove. ``OptRemarks`` can be utilized to provide insights into performance tuning. It; has the capability to report on checks that it cannot eliminate, possibly with; reasons, allowing programmers to adjust their code to unlock further; optimizations. Debugging; =========. Internal bounds annotations; ---------------------------. Internal bounds annotations change a pointer into a wide pointer. The debugger; needs to understand that wide pointers are essentially pointers with a struct; layout. To handle this, a wide pointer is described as a record type in the; debug info. The type name has a special name prefix (e.g.,; ``__bounds_safety$bidi_indexable``) which can be recognized by a debug info; consumer to provide support that goes beyond showing the internal structure of; the wide pointer. There are no DWARF extensions needed to support wide pointers.; In our implementation, LLDB recognizes wide pointer types by name and; reconstructs them as wide pointer Clang AST types for use in the expression; evaluator. External bounds annotations; ---------------------------. Similar to internal bounds annotations, external bound annotations are described; a",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/BoundsSafetyImplPlans.rst:8185,optimiz,optimizations,8185,interpreter/llvm-project/clang/docs/BoundsSafetyImplPlans.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/BoundsSafetyImplPlans.rst,1,['optimiz'],['optimizations']
Performance,"p.parallel_accesses`` list. If all memory-accessing instructions in a loop have; ``llvm.access.group`` metadata that each refer to one of the access; groups of a loop's ``llvm.loop.parallel_accesses`` metadata, then the; loop has no loop carried memory dependences and is considered to be a; parallel loop. Note that if not all memory access instructions belong to an access; group referred to by ``llvm.loop.parallel_accesses``, then the loop must; not be considered trivially parallel. Additional; memory dependence analysis is required to make that determination. As a fail; safe mechanism, this causes loops that were originally parallel to be considered; sequential (if optimization passes that are unaware of the parallel semantics; insert new memory instructions into the loop body). Example of a loop that is considered parallel due to its correct use of; both ``llvm.access.group`` and ``llvm.loop.parallel_accesses``; metadata types. .. code-block:: llvm. for.body:; ...; %val0 = load i32, ptr %arrayidx, !llvm.access.group !1; ...; store i32 %val0, ptr %arrayidx1, !llvm.access.group !1; ...; br i1 %exitcond, label %for.end, label %for.body, !llvm.loop !0. for.end:; ...; !0 = distinct !{!0, !{!""llvm.loop.parallel_accesses"", !1}}; !1 = distinct !{}. It is also possible to have nested parallel loops:. .. code-block:: llvm. outer.for.body:; ...; %val1 = load i32, ptr %arrayidx3, !llvm.access.group !4; ...; br label %inner.for.body. inner.for.body:; ...; %val0 = load i32, ptr %arrayidx1, !llvm.access.group !3; ...; store i32 %val0, ptr %arrayidx2, !llvm.access.group !3; ...; br i1 %exitcond, label %inner.for.end, label %inner.for.body, !llvm.loop !1. inner.for.end:; ...; store i32 %val1, ptr %arrayidx4, !llvm.access.group !4; ...; br i1 %exitcond, label %outer.for.end, label %outer.for.body, !llvm.loop !2. outer.for.end: ; preds = %for.body; ...; !1 = distinct !{!1, !{!""llvm.loop.parallel_accesses"", !3}} ; metadata for the inner loop; !2 = distinct !{!2, !{!""llvm.loop.parallel",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:313194,load,load,313194,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,"p_sdiv:. '``llvm.vp.sdiv.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x i32> @llvm.vp.sdiv.v16i32 (<16 x i32> <left_op>, <16 x i32> <right_op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x i32> @llvm.vp.sdiv.nxv4i32 (<vscale x 4 x i32> <left_op>, <vscale x 4 x i32> <right_op>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x i64> @llvm.vp.sdiv.v256i64 (<256 x i64> <left_op>, <256 x i64> <right_op>, <256 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". Predicated, signed division of two vectors of integers. Arguments:; """""""""""""""""""". The first two operands and the result have the same vector of integer type. The; third operand is the vector mask and has the same number of elements as the; result vector type. The fourth operand is the explicit vector length of the; operation. Semantics:; """""""""""""""""""". The '``llvm.vp.sdiv``' intrinsic performs signed division (:ref:`sdiv <i_sdiv>`); of the first and second vector operand on each enabled lane. The result on; disabled lanes is a :ref:`poison value <poisonvalues>`. Examples:; """""""""""""""""". .. code-block:: llvm. %r = call <4 x i32> @llvm.vp.sdiv.v4i32(<4 x i32> %a, <4 x i32> %b, <4 x i1> %mask, i32 %evl); ;; For all lanes below %evl, %r is lane-wise equivalent to %also.r. %t = sdiv <4 x i32> %a, %b; %also.r = select <4 x i1> %mask, <4 x i32> %t, <4 x i32> poison. .. _int_vp_udiv:. '``llvm.vp.udiv.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x i32> @llvm.vp.udiv.v16i32 (<16 x i32> <left_op>, <16 x i32> <right_op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x i32> @llvm.vp.udiv.nxv4i32 (<vscale x 4 x i32> <left_op>, <vscale x 4 x i32> <right_op>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x i64> @llvm.vp.udiv.v256i64 (<256 x i64> <left_op>, <256 x i64> <right_op>, <256 x i1> <mask>, i32 <vector_length>). Overvi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:702578,perform,performs,702578,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performs']
Performance,"p`` is the union of the label of ``p`` and; the label of ``v``. If the flag is false, the label of ``*p`` is the label of; just ``v``. * ``-dfsan-combine-offset-labels-on-gep`` -- Controls whether to propagate; labels of offsets in GEP instructions. Its default value is true. For example:. .. code-block:: c++. p += i;. If the flag is true, the label of ``p`` is the union of the label of ``p`` and; the label of ``i``. If the flag is false, the label of ``p`` is unchanged. * ``-dfsan-track-select-control-flow`` -- Controls whether to track the control; flow of select instructions. Its default value is true. For example:. .. code-block:: c++. v = b? v1: v2;. If the flag is true, the label of ``v`` is the union of the labels of ``b``,; ``v1`` and ``v2``. If the flag is false, the label of ``v`` is the union of the; labels of just ``v1`` and ``v2``. * ``-dfsan-event-callbacks`` -- An experimental feature that inserts callbacks for; certain data events. Currently callbacks are only inserted for loads, stores,; memory transfers (i.e. memcpy and memmove), and comparisons. Its default value; is false. If this flag is set to true, a user must provide definitions for the; following callback functions:. .. code-block:: c++. void __dfsan_load_callback(dfsan_label Label, void* Addr);; void __dfsan_store_callback(dfsan_label Label, void* Addr);; void __dfsan_mem_transfer_callback(dfsan_label *Start, size_t Len);; void __dfsan_cmp_callback(dfsan_label CombinedLabel);. * ``-dfsan-conditional-callbacks`` -- An experimental feature that inserts; callbacks for control flow conditional expressions.; This can be used to find where tainted values can control execution. In addition to this compilation flag, a callback handler must be registered; using ``dfsan_set_conditional_callback(my_callback);``, where my_callback is; a function with a signature matching; ``void my_callback(dfsan_label l, dfsan_origin o);``.; This signature is the same when origin tracking is disabled - in this case; th",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/DataFlowSanitizer.rst:7520,load,loads,7520,interpreter/llvm-project/clang/docs/DataFlowSanitizer.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/DataFlowSanitizer.rst,1,['load'],['loads']
Performance,"p``' intrinsic converts its signed integer operand to the; :ref:`floating-point <t_floating>` return type. The operation has a mask and; an explicit vector length parameter. Arguments:; """""""""""""""""""". The '``llvm.vp.sitofp``' intrinsic takes a value to cast as its first operand.; The value to cast must be vector of :ref:`integer <t_integer>` type. The; return type is the type to cast the value to. The return type must be a vector; of :ref:`floating-point <t_floating>` type. The second operand is the vector; mask. The return type, the value to cast, and the vector mask have the same; number of elements. The third operand is the explicit vector length of the; operation. Semantics:; """""""""""""""""""". The '``llvm.vp.sitofp``' intrinsic interprets its first operand as a signed; integer quantity and converts it to the corresponding floating-point value. If; the value cannot be exactly represented, it is rounded using the default; rounding mode. The conversion is performed on lane positions below the; explicit vector length and where the vector mask is true. Masked-off lanes are; ``poison``. Examples:; """""""""""""""""". .. code-block:: llvm. %r = call <4 x float> @llvm.vp.sitofp.v4f32.v4i32(<4 x i32> %a, <4 x i1> %mask, i32 %evl); ;; For all lanes below %evl, %r is lane-wise equivalent to %also.r. %t = sitofp <4 x i32> %a to <4 x float>; %also.r = select <4 x i1> %mask, <4 x float> %t, <4 x float> poison. .. _int_vp_ptrtoint:. '``llvm.vp.ptrtoint.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x i8> @llvm.vp.ptrtoint.v16i8.v16p0(<16 x ptr> <op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x i8> @llvm.vp.ptrtoint.nxv4i8.nxv4p0(<vscale x 4 x ptr> <op>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x i64> @llvm.vp.ptrtoint.v16i64.v16p0(<256 x ptr> <op>, <256 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". The '``llvm.vp.ptrtoint``' intrinsic converts its pointer to the integer ret",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:812364,perform,performed,812364,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performed']
Performance,"pace overhead for; instructions and bit vectors and increased overhead in the form of padding. We; therefore limit the amount of padding so that we align to no more than 128; bytes. This number was found experimentally to provide a good tradeoff. Eliminating Bit Vector Checks for All-Ones Bit Vectors; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. If the bit vector is all ones, the bit vector check is redundant; we simply; need to check that the address is in range and well aligned. This is more; likely to occur if the virtual tables are padded. Forward-Edge CFI for Virtual Calls by Interleaving Virtual Tables; -----------------------------------------------------------------. Dimitar et. al. proposed a novel approach that interleaves virtual tables in [1]_.; This approach is more efficient in terms of space because padding and bit vectors are no longer needed.; At the same time, it is also more efficient in terms of performance because in the interleaved layout; address points of the virtual tables are consecutive, thus the validity check of a virtual; vtable pointer is always a range check. At a high level, the interleaving scheme consists of three steps: 1) split virtual table groups into; separate virtual tables, 2) order virtual tables by a pre-order traversal of the class hierarchy; and 3) interleave virtual tables. The interleaving scheme implemented in LLVM is inspired by [1]_ but has its own; enhancements (more in `Interleave virtual tables`_). .. [1] `Protecting C++ Dynamic Dispatch Through VTable Interleaving <https://cseweb.ucsd.edu/~lerner/papers/ivtbl-ndss16.pdf>`_. Dimitar Bounov, Rami Gökhan Kıcı, Sorin Lerner. Split virtual table groups into separate virtual tables; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. The Itanium C++ ABI glues multiple individual virtual tables for a class into a combined virtual table (virtual table group).; The interleaving scheme, however, can only work with individual virtual tables so it must split",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ControlFlowIntegrityDesign.rst:9916,perform,performance,9916,interpreter/llvm-project/clang/docs/ControlFlowIntegrityDesign.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ControlFlowIntegrityDesign.rst,1,['perform'],['performance']
Performance,"paired-atomic. **Acquire-Release Atomic**; ------------------------------------------------------------------------------------; atomicrmw acq_rel - singlethread - global 1. buffer/global/flat_atomic; - wavefront - generic; atomicrmw acq_rel - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; be used.*. 1. ds_atomic; atomicrmw acq_rel - workgroup - global 1. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL, omit; lgkmcnt(0).; - Must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load/store/; load atomic/store atomic/; atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; have; completed before; performing the; atomicrmw that is; being released. 2. buffer/global_atomic; 3. s_waitcnt vmcnt(0). - If not TgSplit execution; mode, omit.; - Must happen before; the following; buffer_inv.; - Ensures any; following global; data read is no; older than the; atomicrmw value; being acquired. 4. buffer_inv sc0=1. - If not TgSplit execution; mode, omit.; - Ensures that; following; loads will not see; stale data. atomicrmw acq_rel - workgroup - local *If TgSplit execution mode,; local address space cannot; be used.*. 1. ds_atomic; 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than the local load; atomic value being; acquired. atomicrmw acq_rel - workgroup - generic 1. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL, omit; lgkmcnt(0).; - s_waitcnt vmcnt",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:317650,perform,performing,317650,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['perform'],['performing']
Performance,"paired-atomic. **Acquire-Release Atomic**; ------------------------------------------------------------------------------------; atomicrmw acq_rel - singlethread - global 1. buffer/global/flat_atomic; - wavefront - generic; atomicrmw acq_rel - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; be used.*. 1. ds_atomic; atomicrmw acq_rel - workgroup - global 1. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL, omit; lgkmcnt(0).; - Must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load/store/; load atomic/store atomic/; atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; have; completed before; performing the; atomicrmw that is; being released. 2. buffer/global_atomic; 3. s_waitcnt vmcnt(0). - If not TgSplit execution; mode, omit.; - Must happen before; the following; buffer_wbinvl1_vol.; - Ensures any; following global; data read is no; older than the; atomicrmw value; being acquired. 4. buffer_wbinvl1_vol. - If not TgSplit execution; mode, omit.; - Ensures that; following; loads will not see; stale data. atomicrmw acq_rel - workgroup - local *If TgSplit execution mode,; local address space cannot; be used.*. 1. ds_atomic; 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than the local load; atomic value being; acquired. atomicrmw acq_rel - workgroup - generic 1. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL, omit; lgkmcnt(0).; - s_wai",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:267127,perform,performing,267127,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['perform'],['performing']
Performance,"parameters, that is, find those values of the coefficients which; give the lowest value of chisquare. The user must therefore supply, in addition to the function to be; analyzed, via a set or sequence of M applications the instructions which; analysis is wanted. The instructions are coded in in the calling program; (main.cpp), which allows looping, conditional execution, and all the; other possibilities of , but not interactivity, since it must be; compiled before execution. ## Design aspects of M in ##. What M is:. - platform independent. - written in an object-oriented way using standard. - independent of any external package. The maintainability should be guaranteed with the choice of a modern; computer language. Choosing object-oriented technology M should profit; from an increased flexibility and functionality and make it also; extendable (recursiveness, new algorithms, new functionality). What M does not:. - histogramming. - data handling. - graphics. M is kept as a low-level package with optimal performance. The main usages of M are. - from a user's program (such as int main()...). - from a graphical data analysis tool such as HippoDraw@bib-HippoDraw. The most important goals of M in are. - its numerical accuracy (equivalent to its Fortran version). - its computational performance (equivalent to its Fortran version). For the design of the application programming interface (API) of M a; two-way strategy was imposed:. - a minimal required interface with minimum interaction with M objects; and with appropriate usage of the standard library (STL): the user's; implementation of the FCNBase class, initial parameter values and; uncertainties are provided by the to M user via std::vectors. - a rich interface which provides the user with more functionality; such as interaction with parameters. The core of the minimization functionality and related tools (the kernel; of M ) should be clearly separated from the user, who is interfacing via; defined user interfaces (the A",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/minuit2/Minuit2.md:4001,perform,performance,4001,documentation/minuit2/Minuit2.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/minuit2/Minuit2.md,1,['perform'],['performance']
Performance,"partial dead case:; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=12395. //===---------------------------------------------------------------------===//. Scalar PRE hoists the mul in the common block up to the else:. int test (int a, int b, int c, int g) {; int d, e;; if (a); d = b * c;; else; d = b - c;; e = b * c + g;; return d + e;; }. It would be better to do the mul once to reduce codesize above the if.; This is GCC PR38204. //===---------------------------------------------------------------------===//; This simple function from 179.art:. int winner, numf2s;; struct { double y; int reset; } *Y;. void find_match() {; int i;; winner = 0;; for (i=0;i<numf2s;i++); if (Y[i].y > Y[winner].y); winner =i;; }. Compiles into (with clang TBAA):. for.body: ; preds = %for.inc, %bb.nph; %indvar = phi i64 [ 0, %bb.nph ], [ %indvar.next, %for.inc ]; %i.01718 = phi i32 [ 0, %bb.nph ], [ %i.01719, %for.inc ]; %tmp4 = getelementptr inbounds %struct.anon* %tmp3, i64 %indvar, i32 0; %tmp5 = load double* %tmp4, align 8, !tbaa !4; %idxprom7 = sext i32 %i.01718 to i64; %tmp10 = getelementptr inbounds %struct.anon* %tmp3, i64 %idxprom7, i32 0; %tmp11 = load double* %tmp10, align 8, !tbaa !4; %cmp12 = fcmp ogt double %tmp5, %tmp11; br i1 %cmp12, label %if.then, label %for.inc. if.then: ; preds = %for.body; %i.017 = trunc i64 %indvar to i32; br label %for.inc. for.inc: ; preds = %for.body, %if.then; %i.01719 = phi i32 [ %i.01718, %for.body ], [ %i.017, %if.then ]; %indvar.next = add i64 %indvar, 1; %exitcond = icmp eq i64 %indvar.next, %tmp22; br i1 %exitcond, label %for.cond.for.end_crit_edge, label %for.body. It is good that we hoisted the reloads of numf2's, and Y out of the loop and; sunk the store to winner out. However, this is awful on several levels: the conditional truncate in the loop; (-indvars at fault? why can't we completely promote the IV to i64?). Beyond that, we have a partially redundant load in the loop: if ""winner"" (aka ; %i.01718) isn't updated, we reload Y[winner].y ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:30267,load,load,30267,interpreter/llvm-project/llvm/lib/Target/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt,2,['load'],['load']
Performance,"parts of the pipeline. Pass plugins can also add passes into default pipelines. Different tools have; different ways of loading dynamic pass plugins. For example, ``opt; -load-pass-plugin=path/to/plugin.so`` loads a pass plugin into ``opt``. For; information on writing a pass plugin, see :doc:`WritingAnLLVMNewPMPass`. Using Analyses; ==============. LLVM provides many analyses that passes can use, such as a dominator tree.; Calculating these can be expensive, so the new pass manager has; infrastructure to cache analyses and reuse them when possible. When a pass runs on some IR, it also receives an analysis manager which it can; query for analyses. Querying for an analysis will cause the manager to check if; it has already computed the result for the requested IR. If it already has and; the result is still valid, it will return that. Otherwise it will construct a; new result by calling the analysis's ``run()`` method, cache it, and return it.; You can also ask the analysis manager to only return an analysis if it's; already cached. The analysis manager only provides analysis results for the same IR type as; what the pass runs on. For example, a function pass receives an analysis; manager that only provides function-level analyses. This works for many; passes which work on a fixed scope. However, some passes want to peek up or; down the IR hierarchy. For example, an SCC pass may want to look at function; analyses for the functions inside the SCC. Or it may want to look at some; immutable global analysis. In these cases, the analysis manager can provide a; proxy to an outer or inner level analysis manager. For example, to get a; ``FunctionAnalysisManager`` from a ``CGSCCAnalysisManager``, you can call. .. code-block:: c++. FunctionAnalysisManager &FAM =; AM.getResult<FunctionAnalysisManagerCGSCCProxy>(InitialC, CG); .getManager();. and use ``FAM`` as a typical ``FunctionAnalysisManager`` that a function pass; would have access to. To get access to an outer level IR anal",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:7486,cache,cached,7486,interpreter/llvm-project/llvm/docs/NewPassManager.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst,1,['cache'],['cached']
Performance,"path should still be matched correctly. Miscellaneous. Repaired the behavior of TTreeCache when the TTree has a dramatic dynamic range with a lots of very small entriesat the beginning and very large entries at the end, the size in bytes of the cluster for the later entries will be very large (because of the cluster size in entries is large!). TTreeCache::FillBuffer was always attempting to load complete clusters not matter the; size (even with the size was larger than 2GB!). This patch resolves the issue by limiting the amount of memory used to:. The requested size if more than one cluster fits in the cache.; Twice the requested size if at least one basket per branch fits in the cache.; Four time the requested size in the case where the cache can not even hold one basket per branch. The filling will restart at the next cluster boundary in the case a) and will; restart at the maximum of entry number read in the cache in the case b) and c).; Baskets that are below this boundary and did not fit in the cache will be read; individually.; Repaired the basket flushing frequency when the TTree has already more than one cluster size.; Repaired binning of string histogram generated by TTree::Draw.; Many bug fixes and fix for issues discovery by Coverity, see change log for more details.; In TTree::MakeProxy add proper support for top level stl collection of objects and for stl collection of objects that are 'empty' in the file (and thus we know nothing about its content).; Avoid deficiency in hadd when the resulting TTree is longer than the AutoSave length *and* the TFileMerger needs to handle the input files in more than one pass for example when there is more than 1000 input files or the -n option is passed to hadd.; Fix support for emulated class that derived from an abstract class.; This can happen when reading a file containing an ancient; class layout where the derived class is no longer provided in the; compiled code but the abstract base class is still provided. It al",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/doc/v534/index.html:3660,cache,cache,3660,tree/doc/v534/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/doc/v534/index.html,2,['cache'],['cache']
Performance,"path>, -r. Specify a file which contains a remapping from symbol names in the input; profile to the symbol names that should be used in the output profile. The; file should consist of lines of the form ``<input-symbol> <output-symbol>``.; Blank lines and lines starting with ``#`` are skipped. The :doc:`llvm-cxxmap <llvm-cxxmap>` tool can be used to generate the symbol; remapping file. .. option:: --instr (default). Specify that the input profile is an instrumentation-based profile. .. option:: --sample. Specify that the input profile is a sample-based profile. The format of the generated file can be generated in one of three ways:. .. option:: --binary (default). Emit the profile using a binary encoding. For instrumentation-based profile; the output format is the indexed binary format. .. option:: --extbinary. Emit the profile using an extensible binary encoding. This option can only; be used with sample-based profile. The extensible binary encoding can be; more compact with compression enabled and can be loaded faster than the; default binary encoding. .. option:: --text. Emit the profile in text mode. This option can also be used with both; sample-based and instrumentation-based profile. When this option is used; the profile will be dumped in the text format that is parsable by the profile; reader. .. option:: --gcc. Emit the profile using GCC's gcov format (Not yet supported). .. option:: --sparse[=true|false]. Do not emit function records with 0 execution count. Can only be used in; conjunction with -instr. Defaults to false, since it can inhibit compiler; optimization during PGO. .. option:: --num-threads=<N>, -j. Use N threads to perform profile merging. When N=0, llvm-profdata auto-detects; an appropriate number of threads to use. This is the default. .. option:: --failure-mode=[any|all]. Set the failure mode. There are two options: 'any' causes the merge command to; fail if any profiles are invalid, and 'all' causes the merge command to fail; only if all prof",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-profdata.rst:3264,load,loaded,3264,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-profdata.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-profdata.rst,1,['load'],['loaded']
Performance,"pe and all previously; loaded code is available.; If the statements are declarations, the effect is the same as ``cppdef``,; but ``cppexec`` also accepts executable lines.; Example::. >>> cppyy.cppexec(r""""""std::string hello = ""Hello, World!"";""""""); True; >>> cppyy.cppexec(""std::cout << hello << std::endl;""); Hello, World!; True; >>> . * ``include``: load declarations into the interpreter.; This function accepts C++ declarations from a file, typically a header.; Files are located through include paths given to the Cling.; Example::. >>> cppyy.include(""vector"") # equivalent to ""#include <vector>""; True; >>> . * ``c_include``: load declarations into the interpreter.; This function accepts C++ declarations from a file, typically a header.; Name mangling is an important difference between C and C++ code.; The use of ``c_include`` instead of ``include`` prevents mangling. * ``load_library``: load compiled C++ into the interpreter.; This function takes the name of a shared library and loads it into current; process, exposing all external symbols to Cling.; Libraries are located through load paths given to Cling, either through the; ""-L"" compiler flag or the dynamic search path environment variable (system; dependent).; Any method that brings symbols into the process (including normal linking,; e.g. when embedding Python in a C++ application) is suitable to expose; symbols.; An alternative for ``load_library`` is for example ``ctypes.CDLL``, but; that function does not respect dynamic load paths on all platforms. If a compilation error occurs during JITing of C++ code in any of the above; helpers, a Python ``SyntaxError`` exception is raised.; If a compilation warning occurs, a Python warning is issued. `Configuring Cling`; -------------------. It is often convenient to add additional search paths for Cling to find; headers and libraries when loading a module (Python does not have standard; locations to place headers and libraries, but their locations can usually; be inferre",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/toplevel.rst:2682,load,loads,2682,bindings/pyroot/cppyy/cppyy/doc/source/toplevel.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/toplevel.rst,1,['load'],['loads']
Performance,"pe. The operation has a mask and an explicit vector length parameter. Arguments:; """""""""""""""""""". The '``llvm.vp.fptrunc``' intrinsic takes a value to cast as its first operand.; The return type is the type to cast the value to. Both types must be vector of; :ref:`floating-point <t_floating>` type. The bit size of the value must be; larger than the bit size of the return type. This implies that; '``llvm.vp.fptrunc``' cannot be used to make a *no-op cast*. The second operand; is the vector mask. The return type, the value to cast, and the vector mask have; the same number of elements. The third operand is the explicit vector length of; the operation. Semantics:; """""""""""""""""""". The '``llvm.vp.fptrunc``' intrinsic casts a ``value`` from a larger; :ref:`floating-point <t_floating>` type to a smaller :ref:`floating-point; <t_floating>` type.; This instruction is assumed to execute in the default :ref:`floating-point; environment <floatenv>`. The conversion is performed on lane positions below the; explicit vector length and where the vector mask is true. Masked-off lanes are; ``poison``. Examples:; """""""""""""""""". .. code-block:: llvm. %r = call <4 x float> @llvm.vp.fptrunc.v4f32.v4f64(<4 x double> %a, <4 x i1> %mask, i32 %evl); ;; For all lanes below %evl, %r is lane-wise equivalent to %also.r. %t = fptrunc <4 x double> %a to <4 x float>; %also.r = select <4 x i1> %mask, <4 x float> %t, <4 x float> poison. .. _int_vp_fpext:. '``llvm.vp.fpext.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x double> @llvm.vp.fpext.v16f64.v16f32 (<16 x float> <op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x double> @llvm.vp.fpext.nxv4f64.nxv4f32 (<vscale x 4 x float> <op>, <vscale x 4 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". The '``llvm.vp.fpext``' intrinsic extends its first operand to the return; type. The operation has a mask and an explicit vector length parameter. Arguments:; """""""""""""""""""". T",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:802494,perform,performed,802494,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performed']
Performance,"pe; level, location. As the printed output can reach a considerable size,; several selection options, enable printing of specific elements. The pattern matching can ignore the case (:option:`--select-nocase`); and be extended to use regular expressions (:option:`--select-regex`). ELEMENTS; ^^^^^^^^; The following options allow printing of elements that match the given; <pattern>, offset <value> or an element <condition>. .. option:: --select=<pattern>. Print all elements whose name or line number matches the given <pattern>. .. option:: --select-offsets=<value[,value,...]>. Print all elements whose offset matches the given values. See; :option:`--attribute` option. .. option:: --select-elements=<condition[,condition,...]>. Print all elements that satisfy the given <condition>. With **condition**; being one of the options in the following list. .. code-block:: text. =discarded: Discarded elements by the linker.; =global: Element referenced across Compile Units.; =optimized: Optimized inlined abstract references. .. option:: --select-nocase. Pattern matching is case-insensitive when using :option:`--select`. .. option:: --select-regex. Treat any <pattern> strings as regular expressions when selecting with; :option:`--select` option. If :option:`--select-nocase` is specified,; the regular expression becomes case-insensitive. If the <pattern> criteria is too general, a more selective option can; be specified to target a particular category of elements:; lines (:option:`--select-lines`), scopes (:option:`--select-scopes`),; symbols (:option:`--select-symbols`) and types (:option:`--select-types`).; These options require knowledge of the debug information format (DWARF,; CodeView, COFF), as the given **kind** describes a very specific type; of element. LINES; ^^^^^; The following options allow printing of lines that match the given <kind>.; The given criteria describes the debug line state machine registers. .. option:: --select-lines=<kind[,kind,...]>. With **kind** bein",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-debuginfo-analyzer.rst:15415,optimiz,optimized,15415,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-debuginfo-analyzer.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-debuginfo-analyzer.rst,1,['optimiz'],['optimized']
Performance,"pe= 67, offset= 0, len=1, method=0; i= 1, TAttLine type= 0, offset= 28, len=1, method=142484480; i= 2, TAttFill type= 0, offset= 40, len=1, method=142496992; i= 3, TAttMarker type= 0, offset= 48, len=1, method=142509704; i= 4, fNcells type= 3, offset= 60, len=1, method=0; i= 5, fXaxis type= 61, offset= 64, len=1, method=1081287424; i= 6, fYaxis type= 61, offset=192, len=1, method=1081287548; i= 7, fZaxis type= 61, offset=320, len=1, method=1081287676; i= 8, fBarOffset type= 22, offset=448, len=2, method=0; i= 9, fEntries type= 28, offset=452, len=8, method=0; i=10, fContour type= 62, offset=516, len=1, method=1081287804; i=11, fSumw2 type= 62, offset=528, len=1, method=1081287924; i=12, fOption type= 65, offset=540, len=1, method=1081288044; i=13, fFunctions type= 63, offset=548, len=1, method=1081288164; ```. ### Optimized StreamerInfo. The entries starting with ""`i = 0`"" is the optimized format of the; `StreamerInfo`. Consecutive data members of the same simple type and; size are collapsed and read at once into an array for performance; optimization. ``` {.cpp}; i= 0, TNamed type= 67, offset= 0, len=1, method=0; i= 1, TAttLine type= 0, offset= 28, len=1, method=142484480; i= 2, TAttFill type= 0, offset= 40, len=1, method=142496992; i= 3, TAttMarker type= 0, offset= 48, len=1, method=142509704; ```. For example, the five data members beginning with `fEnties `and the; three data members beginning with `fMaximum`, are put into an array; called `fEntries` (`i = 9`) with the length 8. ``` {.cpp}; i= 9, fEntries type= 28, offset=452, len=8, method=0; ```. Only simple type data members are combined, object data members are not; combined. For example the three axis data members remain separate. The; ""method"" is a handle to the method that reads the object. ### Automatic Schema Evolution. When a class is defined in ROOT, it must include the `ClassDef` macro as; the last line in the header file inside the class definition. The syntax; is:. ``` {.cpp}; ClassDef(<ClassName>,<V",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/InputOutput.md:75040,perform,performance,75040,documentation/users-guide/InputOutput.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/InputOutput.md,2,"['optimiz', 'perform']","['optimization', 'performance']"
Performance,"pe=65 histogram options; TList* fFunctions offset=548 type=63 ->Pointer to list of functions; i= 0, TNamed type= 67, offset= 0, len=1, method=0; i= 1, TAttLine type= 0, offset= 28, len=1, method=142484480; i= 2, TAttFill type= 0, offset= 40, len=1, method=142496992; i= 3, TAttMarker type= 0, offset= 48, len=1, method=142509704; i= 4, fNcells type= 3, offset= 60, len=1, method=0; i= 5, fXaxis type= 61, offset= 64, len=1, method=1081287424; i= 6, fYaxis type= 61, offset=192, len=1, method=1081287548; i= 7, fZaxis type= 61, offset=320, len=1, method=1081287676; i= 8, fBarOffset type= 22, offset=448, len=2, method=0; i= 9, fEntries type= 28, offset=452, len=8, method=0; i=10, fContour type= 62, offset=516, len=1, method=1081287804; i=11, fSumw2 type= 62, offset=528, len=1, method=1081287924; i=12, fOption type= 65, offset=540, len=1, method=1081288044; i=13, fFunctions type= 63, offset=548, len=1, method=1081288164; ```. ### Optimized StreamerInfo. The entries starting with ""`i = 0`"" is the optimized format of the; `StreamerInfo`. Consecutive data members of the same simple type and; size are collapsed and read at once into an array for performance; optimization. ``` {.cpp}; i= 0, TNamed type= 67, offset= 0, len=1, method=0; i= 1, TAttLine type= 0, offset= 28, len=1, method=142484480; i= 2, TAttFill type= 0, offset= 40, len=1, method=142496992; i= 3, TAttMarker type= 0, offset= 48, len=1, method=142509704; ```. For example, the five data members beginning with `fEnties `and the; three data members beginning with `fMaximum`, are put into an array; called `fEntries` (`i = 9`) with the length 8. ``` {.cpp}; i= 9, fEntries type= 28, offset=452, len=8, method=0; ```. Only simple type data members are combined, object data members are not; combined. For example the three axis data members remain separate. The; ""method"" is a handle to the method that reads the object. ### Automatic Schema Evolution. When a class is defined in ROOT, it must include the `ClassDef` macro as; the ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/InputOutput.md:74891,optimiz,optimized,74891,documentation/users-guide/InputOutput.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/InputOutput.md,1,['optimiz'],['optimized']
Performance,"pe``, ``!absolute_symbol`` and ``!associated`` which can't be; unconditionally dropped unless the global is itself deleted. Metadata attached to a module using named metadata may not be dropped, with; the exception of debug metadata (named metadata with the name ``!llvm.dbg.*``). More information about specific metadata nodes recognized by the; optimizers and code generator is found below. .. _specialized-metadata:. Specialized Metadata Nodes; ^^^^^^^^^^^^^^^^^^^^^^^^^^. Specialized metadata nodes are custom data structures in metadata (as opposed; to generic tuples). Their fields are labelled, and can be specified in any; order. These aren't inherently debug info centric, but currently all the specialized; metadata nodes are related to debug info. .. _DICompileUnit:. DICompileUnit; """""""""""""""""""""""""". ``DICompileUnit`` nodes represent a compile unit. The ``enums:``,; ``retainedTypes:``, ``globals:``, ``imports:`` and ``macros:`` fields are tuples; containing the debug info to be emitted along with the compile unit, regardless; of code optimizations (some nodes are only emitted if there are references to; them from instructions). The ``debugInfoForProfiling:`` field is a boolean; indicating whether or not line-table discriminators are updated to provide; more-accurate debug info for profiling results. .. code-block:: text. !0 = !DICompileUnit(language: DW_LANG_C99, file: !1, producer: ""clang"",; isOptimized: true, flags: ""-O2"", runtimeVersion: 2,; splitDebugFilename: ""abc.debug"", emissionKind: FullDebug,; enums: !2, retainedTypes: !3, globals: !4, imports: !5,; macros: !6, dwoId: 0x0abcd). Compile unit descriptors provide the root scope for objects declared in a; specific compilation unit. File descriptors are defined using this scope. These; descriptors are collected by a named metadata node ``!llvm.dbg.cu``. They keep; track of global variables, type information, and imported entities (declarations; and namespaces). .. _DIFile:. DIFile; """""""""""". ``DIFile`` nodes represen",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:246397,optimiz,optimizations,246397,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['optimiz'],['optimizations']
Performance,"pecific build flags.; For example, the following CMake call will enabled '-fno-addrsig' only during; the stage2 build for C and C++. .. code-block:: console. $ cmake [..] -DBOOTSTRAP_CMAKE_CXX_FLAGS='-fno-addrsig' -DBOOTSTRAP_CMAKE_C_FLAGS='-fno-addrsig' [..]. The clang build system refers to builds as stages. A stage1 build is a standard; build using the compiler installed on the host, and a stage2 build is built; using the stage1 compiler. This nomenclature holds up to more stages too. In; general a stage*n* build is built using the output from stage*n-1*. Apple Clang Builds (A More Complex Bootstrap); =============================================. Apple's Clang builds are a slightly more complicated example of the simple; bootstrapping scenario. Apple Clang is built using a 2-stage build. The stage1 compiler is a host-only compiler with some options set. The stage1; compiler is a balance of optimization vs build time because it is a throwaway.; The stage2 compiler is the fully optimized compiler intended to ship to users. Setting up these compilers requires a lot of options. To simplify the; configuration the Apple Clang build settings are contained in CMake Cache files.; You can build an Apple Clang compiler using the following commands:. .. code-block:: console. $ cmake -G Ninja -C <path to source>/clang/cmake/caches/Apple-stage1.cmake <path to source>/llvm; $ ninja stage2-distribution. This CMake invocation configures the stage1 host compiler, and sets; CLANG_BOOTSTRAP_CMAKE_ARGS to pass the Apple-stage2.cmake cache script to the; stage2 configuration step. When you build the stage2-distribution target it builds the minimal stage1; compiler and required tools, then configures and builds the stage2 compiler; based on the settings in Apple-stage2.cmake. This pattern of using cache scripts to set complex settings, and specifically to; make later stage builds include cache scripts is common in our more advanced; build configurations. Multi-stage PGO; =============",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AdvancedBuilds.rst:3729,optimiz,optimized,3729,interpreter/llvm-project/llvm/docs/AdvancedBuilds.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AdvancedBuilds.rst,1,['optimiz'],['optimized']
Performance,"pecified `<architecture>` and ignore all; other architectures in the files. .. option:: -D. Use zero for timestamps and UIDs/GIDs. This is set by default. .. option:: -filelist <listfile[,dirname]>. Read input file names from `<listfile>`. File names are specified in `<listfile>`; one per line, separated only by newlines. Whitespace on a line is assumed; to be part of the filename. If the directory name, `dirname`, is also; specified then it is prepended to each file name in the `<listfile>`. .. option:: -h, -help. Show help and usage for this command. .. option:: -l <x>. Searches for the library libx.a in the library search path. If the string `<x>`; ends with '.o', then the library 'x' is searched for without prepending 'lib'; or appending '.a'. If the library is found, it is added to the list of input; files. Otherwise, an error is raised. .. option:: -L <dir>. Adds `<dir>` to the list of directories in which to search for libraries. The; directories are searched in the order in which they are specified with; :option:`-L` and before the default search path. The default search path; includes directories `/lib`, `/usr/lib` and `/usr/local/lib`. .. option:: -no_warning_for_no_symbols. Do not warn about files that have no symbols. .. option:: -warnings_as_errors. Produce a non-zero exit status if any warnings are emitted. .. option:: -o <filename>. Specify the output file name. Must be specified exactly once. .. option:: -static. Produces a static library from the input files. .. option:: -U. Use actual timestamps and UIDs/GIDs. .. option:: -V. Display the version of this program and perform any operation specified. .. option:: -version. Display the version of this program and exit immediately. EXIT STATUS; -----------. :program:`llvm-libtool-darwin` exits with a non-zero exit code if there is an error.; Otherwise, it exits with code 0. BUGS; ----. To report bugs, please visit <https://github.com/llvm/llvm-project/issues/>. SEE ALSO; --------. :manpage:`llvm-ar(1)`; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-libtool-darwin.rst:2227,perform,perform,2227,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-libtool-darwin.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-libtool-darwin.rst,1,['perform'],['perform']
Performance,"pen after; any preceding; global/generic load; atomic/; atomicrmw-with-return-value; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - s_waitcnt vscnt(0); must happen after; any preceding; global/generic; atomicrmw-no-return-value; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - Must happen before; the following; buffer_gl*_inv.; - Ensures that the; fence-paired atomic; has completed; before invalidating; the; caches. Therefore; any following; locations read must; be no older than; the value read by; the; fence-paired-atomic. 2. buffer_gl0_inv;; buffer_gl1_inv. - Must happen before any; following global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. **Release Atomic**; ------------------------------------------------------------------------------------; store atomic release - singlethread - global 1. buffer/global/ds/flat_store; - wavefront - local; - generic; store atomic release - workgroup - global 1. s_waitcnt lgkmcnt(0) &; - generic vmcnt(0) & vscnt(0). - If CU wavefront execution; mode, omit vmcnt(0) and; vscnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0), s_waitcnt; vscnt(0) and s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load/load; atomic/; atomicrmw-with-return-value.; - s_waitcnt vscnt(0); must happen after; any preceding; global/generic; store/store; atomic/; atomicrmw-no-return-value.; - s_waitcnt lgkmcnt(0); must happen after; any pr",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:354542,load,load,354542,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"pen after; any preceding; local/generic load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - Must happen before; the following; buffer_wbinvl1_vol.; - Ensures that the; fence-paired atomic; has completed; before invalidating; the; cache. Therefore; any following; locations read must; be no older than; the value read by; the; fence-paired-atomic. 2. buffer_wbinvl1_vol. - Must happen before any; following global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. **Release Atomic**; ------------------------------------------------------------------------------------; store atomic release - singlethread - global 1. buffer/global/ds/flat_store; - wavefront - local; - generic; store atomic release - workgroup - global 1. s_waitcnt lgkmcnt(0); - generic; - If OpenCL, omit.; - Must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; store.; - Ensures that all; memory operations; to local have; completed before; performing the; store that is being; released. 2. buffer/global/flat_store; store atomic release - workgroup - local 1. ds_store; store atomic release - agent - global 1. s_waitcnt lgkmcnt(0) &; - system - generic vmcnt(0). - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; store.; - Ensures that all; memory operations; to memory have; completed before; performing the; sto",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:219261,load,load,219261,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"pen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than the local load; atomic value being; acquired. atomicrmw acq_rel - workgroup - generic 1. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL, omit; lgkmcnt(0).; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load/store/; load atomic/store atomic/; atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; have; completed before; performing the; atomicrmw that is; being released. 2. flat_atomic; 3. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If not TgSplit execution; mode, omit vmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Must happen before; the following; buffer_inv and; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local load; atomic value being; acquired. 3. buffer_inv sc0=1. - If not TgSplit execution; mode, omit.; - Ensures that; following; loads will not see; stale data. atomicrmw acq_rel - agent - global 1. buffer_wbl2 sc1=1. - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at agent scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/stor",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:319254,load,load,319254,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"pen before; the following; store.; - Ensures that all; memory operations; to memory and the L2; writeback have; completed before; performing the; store that is being; released. 3. buffer/global/flat_store; sc0=1 sc1=1; atomicrmw release - singlethread - global 1. buffer/global/flat_atomic; - wavefront - generic; atomicrmw release - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; be used.*. 1. ds_atomic; atomicrmw release - workgroup - global 1. s_waitcnt lgkm/vmcnt(0); - generic; - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL, omit; lgkmcnt(0).; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load/store/; load atomic/store atomic/; atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; have; completed before; performing the; atomicrmw that is; being released. 2. buffer/global/flat_atomic sc0=1; atomicrmw release - workgroup - local *If TgSplit execution mode,; local address space cannot; be used.*. 1. ds_atomic; atomicrmw release - agent - global 1. buffer_wbl2 sc1=1; - generic; - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at agent scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.;",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:310570,perform,performing,310570,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['perform'],['performing']
Performance,"penCL; metadata along with other information. The IDs used to encode the OpenCL's logical address spaces in the argument info; metadata follows the SPIR address space mapping as defined in the SPIR; specification `section 2.2; <https://www.khronos.org/registry/spir/specs/spir_spec-2.0.pdf#18>`_. OpenCL Specific Options; -----------------------. In addition to the options described in :doc:`UsersManual` there are the; following options specific to the OpenCL frontend. All the options in this section are frontend-only and therefore if used; with regular clang driver they require frontend forwarding, e.g. ``-cc1``; or ``-Xclang``. .. _opencl_finclude_default_header:. .. option:: -finclude-default-header. Adds most of builtin types and function declarations during compilations. By; default the OpenCL headers are not loaded by the frontend and therefore certain; builtin types and most of builtin functions are not declared. To load them; automatically this flag can be passed to the frontend (see also :ref:`the; section on the OpenCL Header <opencl_header>`):. .. code-block:: console. $ clang -Xclang -finclude-default-header test.cl. Alternatively the internal header `opencl-c.h` containing the declarations; can be included manually using ``-include`` or ``-I`` followed by the path; to the header location. The header can be found in the clang source tree or; installation directory. .. code-block:: console. $ clang -I<path to clang sources>/lib/Headers/opencl-c.h test.cl; $ clang -I<path to clang installation>/lib/clang/<llvm version>/include/opencl-c.h/opencl-c.h test.cl. In this example it is assumed that the kernel code contains; ``#include <opencl-c.h>`` just as a regular C include. Because the header is very large and long to parse, PCH (:doc:`PCHInternals`); and modules (:doc:`Modules`) can be used internally to improve the compilation; speed. To enable modules for OpenCL:. .. code-block:: console. $ clang --target=spir-unknown-unknown -c -emit-llvm -Xclang -finclude-",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/OpenCLSupport.rst:3019,load,load,3019,interpreter/llvm-project/clang/docs/OpenCLSupport.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/OpenCLSupport.rst,1,['load'],['load']
Performance,"pendence, see the discussion above on; positive-definiteness. Possible other mathematical problems are the; following:. - Excessive numerical round off - be especially careful of; exponential and factorial functions which get big very quickly and; lose accuracy. - Starting too far from the solution - the function may have; unphysical local minima, especially at infinity in some variables. ## Minuit2 Package. `Minuit2` is a new object-oriented implementation, written in C++, of; the popular `MINUIT` minimization package. Compared with the; **`TMinuit`** class, which is a direct conversion from FORTRAN to C++,; `Minuit2` is a complete redesign and re-implementation of the package.; This new version provides all the functionality present in the old; FORTRAN version, with almost equivalent numerical accuracy and; computational performances.; Furthermore, it contains some fixes and small improvements and this new functionality:; * The possibility to set single side parameter limits; * the FUMILI algorithm (see the next paragraph ""FUMILI Minimization Package""),; which is an optimized method for least square and log; likelihood minimizations. Minuit2 has been originally developed by M.; Winkler and F. James in the SEAL project. More information can be found; on the [MINUIT Web Site](MINUIT Web Site) and in particular at the; following documentation page at; <http://www.cern.ch/minuit/doc/doc.html>. A detailed User Guide for Minuit2 exists, describing the API of the internal classes.; ROOT uses `Minuit2` for fitting via the `Minuit2Minimizer` class which implements; the `ROOT::Math::Minimizer` interface. `Minuit2` is also distributed as an independent package of ROOT and can be built; without any other dependency on the ROOT libraries. Examples on how to use the `Minuit2` and `Fumili2` plug-ins are provided; in the tutorials' directory `$ROOTSYS/tutorials/fit`:; `minuit2FitBench.C`, `minuit2FitBench2D.C` and `minuit2GausFit.C`.; More information on the classes and functions ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/FittingHistograms.md:65282,optimiz,optimized,65282,documentation/users-guide/FittingHistograms.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/FittingHistograms.md,1,['optimiz'],['optimized']
Performance,"pendent access is made to populate `value2`. ## High Level Mitigation Approach. While several approaches are being actively pursued to mitigate specific; branches and/or loads inside especially risky software (most notably various OS; kernels), these approaches require manual and/or static analysis aided auditing; of code and explicit source changes to apply the mitigation. They are unlikely; to scale well to large applications. We are proposing a comprehensive; mitigation approach that would apply automatically across an entire program; rather than through manual changes to the code. While this is likely to have a; high performance cost, some applications may be in a good position to take this; performance / security tradeoff. The specific technique we propose is to cause loads to be checked using; branchless code to ensure that they are executing along a valid control flow; path. Consider the following C-pseudo-code representing the core idea of a; predicate guarding potentially invalid loads:; ```; void leak(int data);; void example(int* pointer1, int* pointer2) {; if (condition) {; // ... lots of code ...; leak(*pointer1);; } else {; // ... more code ...; leak(*pointer2);; }; }; ```. This would get transformed into something resembling the following:; ```; uintptr_t all_ones_mask = std::numerical_limits<uintptr_t>::max();; uintptr_t all_zeros_mask = 0;; void leak(int data);; void example(int* pointer1, int* pointer2) {; uintptr_t predicate_state = all_ones_mask;; if (condition) {; // Assuming ?: is implemented using branchless logic...; predicate_state = !condition ? all_zeros_mask : predicate_state;; // ... lots of code ...; //; // Harden the pointer so it can't be loaded; pointer1 &= predicate_state;; leak(*pointer1);; } else {; predicate_state = condition ? all_zeros_mask : predicate_state;; // ... more code ...; //; // Alternative: Harden the loaded value; int value2 = *pointer2 & predicate_state;; leak(value2);; }; }; ```. The result should be that if the `i",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md:3284,load,loads,3284,interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,1,['load'],['loads']
Performance,"pending on which sometest you use). For example, the ``nightly`` test; explicitly outputs TEST-PASS or TEST-FAIL for every test after each; program. Though these lines are still drowned in the output, it's easy; to grep the output logs in the Output directories. Even better are the ``report`` and ``report.format`` targets (where; ``format`` is one of ``html``, ``csv``, ``text`` or ``graphs``). The; exact contents of the report are dependent on which ``TEST`` you are; running, but the text results are always shown at the end of the run and; the results are always stored in the ``report.<type>.format`` file (when; running with ``TEST=<type>``). The ``report`` also generate a file; called ``report.<type>.raw.out`` containing the output of the entire; test run. Writing Custom Tests for the test-suite; =======================================. Assuming you can run the test suite, (e.g.; ""``gmake TEST=nightly report``"" should work), it is really easy to run; optimizations or code generator components against every program in the; tree, collecting statistics or running custom checks for correctness. At; base, this is how the nightly tester works, it's just one example of a; general framework. Lets say that you have an LLVM optimization pass, and you want to see; how many times it triggers. First thing you should do is add an LLVM; `statistic <ProgrammersManual.html#Statistic>`_ to your pass, which will; tally counts of things you care about. Following this, you can set up a test and a report that collects these; and formats them for easy viewing. This consists of two files, a; ""``test-suite/TEST.XXX.Makefile``"" fragment (where XXX is the name of; your test) and a ""``test-suite/TEST.XXX.report``"" file that indicates; how to format the output into a table. There are many example reports of; various levels of sophistication included with the test suite, and the; framework is very general. If you are interested in testing an optimization pass, check out the; ""libcalls"" test as ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TestSuiteMakefileGuide.rst:5173,optimiz,optimizations,5173,interpreter/llvm-project/llvm/docs/TestSuiteMakefileGuide.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TestSuiteMakefileGuide.rst,1,['optimiz'],['optimizations']
Performance,"pens, these values are now in a 32-bit register, usually with; the top-bits known to be sign or zero extended. If spilled, we should be able; to spill these to a 8-bit or 16-bit stack slot, zero or sign extending as part; of the reload. Doing this reduces the size of the stack frame (important for thumb etc), and; also increases the likelihood that we will be able to reload multiple values; from the stack with a single load. //===---------------------------------------------------------------------===//. The constant island pass is in good shape. Some cleanups might be desirable,; but there is unlikely to be much improvement in the generated code. 1. There may be some advantage to trying to be smarter about the initial; placement, rather than putting everything at the end. 2. There might be some compile-time efficiency to be had by representing; consecutive islands as a single block rather than multiple blocks. 3. Use a priority queue to sort constant pool users in inverse order of; position so we always process the one closed to the end of functions; first. This may simply CreateNewWater. //===---------------------------------------------------------------------===//. Eliminate copysign custom expansion. We are still generating crappy code with; default expansion + if-conversion. //===---------------------------------------------------------------------===//. Eliminate one instruction from:. define i32 @_Z6slow4bii(i32 %x, i32 %y) {; %tmp = icmp sgt i32 %x, %y; %retval = select i1 %tmp, i32 %x, i32 %y; ret i32 %retval; }. __Z6slow4bii:; cmp r0, r1; movgt r1, r0; mov r0, r1; bx lr; =>. __Z6slow4bii:; cmp r0, r1; movle r0, r1; bx lr. //===---------------------------------------------------------------------===//. Implement long long ""X-3"" with instructions that fold the immediate in. These; were disabled due to badness with the ARM carry flag on subtracts. //===---------------------------------------------------------------------===//. More load / store optimizations:",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:1792,queue,queue,1792,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,2,['queue'],['queue']
Performance,"perations to; different; addresses will not; be reordered by; hardware. load *none* *none* - local 1. ds_load; store *none* *none* - global - !volatile & !nontemporal; - generic; - private 1. buffer/global/flat_store; - constant; - !volatile & nontemporal. 1. buffer/global/flat_store; glc=1 slc=1 dlc=1. - If GFX10, omit dlc=1. - volatile. 1. buffer/global/flat_store; dlc=1. - If GFX10, omit dlc=1. 2. s_waitcnt vscnt(0). - Must happen before; any following volatile; global/generic; load/store.; - Ensures that; volatile; operations to; different; addresses will not; be reordered by; hardware. store *none* *none* - local 1. ds_store; **Unordered Atomic**; ------------------------------------------------------------------------------------; load atomic unordered *any* *any* *Same as non-atomic*.; store atomic unordered *any* *any* *Same as non-atomic*.; atomicrmw unordered *any* *any* *Same as monotonic atomic*.; **Monotonic Atomic**; ------------------------------------------------------------------------------------; load atomic monotonic - singlethread - global 1. buffer/global/flat_load; - wavefront - generic; load atomic monotonic - workgroup - global 1. buffer/global/flat_load; - generic glc=1. - If CU wavefront execution; mode, omit glc=1. load atomic monotonic - singlethread - local 1. ds_load; - wavefront; - workgroup; load atomic monotonic - agent - global 1. buffer/global/flat_load; - system - generic glc=1 dlc=1. - If GFX11, omit dlc=1. store atomic monotonic - singlethread - global 1. buffer/global/flat_store; - wavefront - generic; - workgroup; - agent; - system; store atomic monotonic - singlethread - local 1. ds_store; - wavefront; - workgroup; atomicrmw monotonic - singlethread - global 1. buffer/global/flat_atomic; - wavefront - generic; - workgroup; - agent; - system; atomicrmw monotonic - singlethread - local 1. ds_atomic; - wavefront; - workgroup; **Acquire Atomic**; -----------------------------------------------------------------------------------",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:345132,load,load,345132,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"perations; to local have; completed before; performing the; atomicrmw that is; being released. 2. buffer/global_atomic. atomicrmw acq_rel - workgroup - local 1. ds_atomic; 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than the local load; atomic value being; acquired. atomicrmw acq_rel - workgroup - generic 1. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to local have; completed before; performing the; atomicrmw that is; being released. 2. flat_atomic; 3. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local load; atomic value being; acquired. atomicrmw acq_rel - agent - global 1. s_waitcnt lgkmcnt(0) &; - system vmcnt(0). - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global have; completed before; performing the; atomicrmw that is; being released. 2. buffer/global_atomic; 3. s_waitcnt vmcnt(0). - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the; atomicrmw has; completed before; invalidating the; cache. 4. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/l",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:225113,load,load,225113,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"perform loads and stores to; different address spaces via the x86 segment registers. A segment override; prefix byte on an instruction causes the instruction's memory access to go to; the specified segment. LLVM address space 0 is the default address space, which; includes the stack, and any unqualified memory accesses in a program. Address; spaces 1-255 are currently reserved for user-defined code. The GS-segment is; represented by address space 256, the FS-segment is represented by address space; 257, and the SS-segment is represented by address space 258. Other x86 segments; have yet to be allocated address space numbers. While these address spaces may seem similar to TLS via the ``thread_local``; keyword, and often use the same underlying hardware, there are some fundamental; differences. The ``thread_local`` keyword applies to global variables and specifies that they; are to be allocated in thread-local memory. There are no type qualifiers; involved, and these variables can be pointed to with normal pointers and; accessed with normal loads and stores. The ``thread_local`` keyword is; target-independent at the LLVM IR level (though LLVM doesn't yet have; implementations of it for some configurations). Special address spaces, in contrast, apply to static types. Every load and store; has a particular address space in its address operand type, and this is what; determines which address space is accessed. LLVM ignores these special address; space qualifiers on global variables, and does not provide a way to directly; allocate storage in them. At the LLVM IR level, the behavior of these special; address spaces depends in part on the underlying OS or runtime environment, and; they are specific to x86 (and LLVM doesn't yet handle them correctly in some; cases). Some operating systems and runtime environments use (or may in the future use); the FS/GS-segment registers for various low-level purposes, so care should be; taken when considering them. Instruction naming; ^^^^",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst:92832,load,loads,92832,interpreter/llvm-project/llvm/docs/CodeGenerator.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst,1,['load'],['loads']
Performance,"performance of the generated binaries. In addition to PGO profiling we also have limited support in-tree for generating; linker order files. These files provide the linker with a suggested ordering for; functions in the final binary layout. This can measurably speed up clang by; physically grouping functions that are called temporally close to each other.; The current tooling is only available on Darwin systems with ``dtrace(1)``. It; is worth noting that dtrace is non-deterministic, and so the order file; generation using dtrace is also non-deterministic. Options for Reducing Size; =========================. .. warning::; Any steps taken to reduce the binary size will come at a cost of runtime; performance in the generated binaries. The simplest and least significant way to reduce binary size is to set the; *CMAKE_BUILD_TYPE* variable to ``MinSizeRel``, which will set the compiler; optimization level to ``-Os`` which optimizes for binary size. This will have; both the least benefit to size and the least impact on performance. The most impactful way to reduce binary size is to dynamically link LLVM into; all the tools. This reduces code size by decreasing duplication of common code; between the LLVM-based tools. This can be done by setting the following two; CMake options to ``On``: *LLVM_BUILD_LLVM_DYLIB* and *LLVM_LINK_LLVM_DYLIB*. .. warning::; Distributions should never be built using the *BUILD_SHARED_LIBS* CMake; option. (:ref:`See the warning above for more explanation <shared_libs>`.). Relevant CMake Options; ======================. This section provides documentation of the CMake options that are intended to; help construct distributions. This is not an exhaustive list, and many; additional options are documented in the :doc:`CMake` page. Some key options; that are already documented include: *LLVM_TARGETS_TO_BUILD*, *LLVM_ENABLE_PROJECTS*,; *LLVM_ENABLE_RUNTIMES*, *LLVM_BUILD_LLVM_DYLIB*, and *LLVM_LINK_LLVM_DYLIB*. **LLVM_ENABLE_RUNTIMES**:STRING; When bu",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BuildingADistribution.rst:10313,perform,performance,10313,interpreter/llvm-project/llvm/docs/BuildingADistribution.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BuildingADistribution.rst,1,['perform'],['performance']
Performance,"pes must match. Pointer types of parameters; or return types may differ in pointee type, but not in address space. On the other hand, if the calling convention is `swifttailcc` or `swiftcc`:. - Only these ABI-impacting attributes attributes are allowed: sret, byval,; swiftself, and swiftasync.; - Prototypes are not required to match. Tail call optimization for calls marked ``tail`` is guaranteed to occur if; the following conditions are met:. - Caller and callee both have the calling convention ``fastcc`` or ``tailcc``.; - The call is in tail position (ret immediately follows call and ret; uses value of call or is void).; - Option ``-tailcallopt`` is enabled,; ``llvm::GuaranteedTailCallOpt`` is ``true``, or the calling convention; is ``tailcc``; - `Platform-specific constraints are; met. <CodeGenerator.html#tailcallopt>`_. #. The optional ``notail`` marker indicates that the optimizers should not add; ``tail`` or ``musttail`` markers to the call. It is used to prevent tail; call optimization from being performed on the call. #. The optional ``fast-math flags`` marker indicates that the call has one or more; :ref:`fast-math flags <fastmath>`, which are optimization hints to enable; otherwise unsafe floating-point optimizations. Fast-math flags are only valid; for calls that return a floating-point scalar or vector type, or an array; (nested to any depth) of floating-point scalar or vector types. #. The optional ""cconv"" marker indicates which :ref:`calling; convention <callingconv>` the call should use. If none is; specified, the call defaults to using C calling conventions. The; calling convention of the call must match the calling convention of; the target function, or else the behavior is undefined.; #. The optional :ref:`Parameter Attributes <paramattrs>` list for return; values. Only '``zeroext``', '``signext``', and '``inreg``' attributes; are valid here.; #. The optional addrspace attribute can be used to indicate the address space; of the called function. If i",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:475590,optimiz,optimization,475590,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,2,"['optimiz', 'perform']","['optimization', 'performed']"
Performance,"petitions`/`snippet size` times). The `loop` mode, especially with loop; unrolling tends to better hide the effects of the CPU frontend on architectures; that cache decoded instructions, but consumes a register for counting; iterations. If performing an analysis over many opcodes, it may be best to; instead use the `min` mode, which will run each other mode,; and produce the minimal measured result. .. option:: --num-repetitions=<Number of repetitions>. Specify the target number of executed instructions. Note that the actual; repetition count of the snippet will be `num-repetitions`/`snippet size`.; Higher values lead to more accurate measurements but lengthen the benchmark. .. option:: --loop-body-size=<Preferred loop body size>. Only effective for `-repetition-mode=[loop|min]`.; Instead of looping over the snippet directly, first duplicate it so that the; loop body contains at least this many instructions. This potentially results; in loop body being cached in the CPU Op Cache / Loop Cache, which allows to; which may have higher throughput than the CPU decoders. .. option:: --max-configs-per-opcode=<value>. Specify the maximum configurations that can be generated for each opcode.; By default this is `1`, meaning that we assume that a single measurement is; enough to characterize an opcode. This might not be true of all instructions:; for example, the performance characteristics of the LEA instruction on X86; depends on the value of assigned registers and immediates. Setting a value of; `-max-configs-per-opcode` larger than `1` allows `llvm-exegesis` to explore; more configurations to discover if some register or immediate assignments; lead to different performance characteristics. .. option:: --benchmarks-file=</path/to/file>. File to read (`analysis` mode) or write (`latency`/`uops`/`inverse_throughput`; modes) benchmark results. ""-"" uses stdin/stdout. .. option:: --analysis-clusters-output-file=</path/to/file>. If provided, write the analysis clusters as CSV to t",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-exegesis.rst:13725,cache,cached,13725,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-exegesis.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-exegesis.rst,2,"['cache', 'throughput']","['cached', 'throughput']"
Performance,"pically used with; control flow conditions such as in ``if`` and ``switch`` statements to help; branch prediction. It means that its first argument ``expr`` is expected to take; the value of its second argument ``val`` with probability ``p``. ``p`` must be; within ``[0.0 ; 1.0]`` bounds. This builtin always returns the value of ``expr``. Query for this feature with ``__has_builtin(__builtin_expect_with_probability)``. ``__builtin_prefetch``; ----------------------. ``__builtin_prefetch`` is used to communicate with the cache handler to bring; data into the cache before it gets used. **Syntax**:. .. code-block:: c++. void __builtin_prefetch(const void *addr, int rw=0, int locality=3). **Example of use**:. .. code-block:: c++. __builtin_prefetch(a + i);. **Description**:. The ``__builtin_prefetch(addr, rw, locality)`` builtin is expected to be used to; avoid cache misses when the developer has a good understanding of which data; are going to be used next. ``addr`` is the address that needs to be brought into; the cache. ``rw`` indicates the expected access mode: ``0`` for *read* and ``1``; for *write*. In case of *read write* access, ``1`` is to be used. ``locality``; indicates the expected persistence of data in cache, from ``0`` which means that; data can be discarded from cache after its next use to ``3`` which means that; data is going to be reused a lot once in cache. ``1`` and ``2`` provide; intermediate behavior between these two extremes. Query for this feature with ``__has_builtin(__builtin_prefetch)``. ``__sync_swap``; ---------------. ``__sync_swap`` is used to atomically swap integers or pointers in memory. **Syntax**:. .. code-block:: c++. type __sync_swap(type *ptr, type value, ...). **Example of Use**:. .. code-block:: c++. int old_value = __sync_swap(&value, new_value);. **Description**:. The ``__sync_swap()`` builtin extends the existing ``__sync_*()`` family of; atomic intrinsics to allow code to atomically swap the current value with the; new value.",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst:116065,cache,cache,116065,interpreter/llvm-project/clang/docs/LanguageExtensions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst,1,['cache'],['cache']
Performance,"picted in the `TOP_1` branch.; These are the nodes of the `physical` `tree` of positioned volumes; represented by **`TGeoNode`** objects. This hierarchy is a tree since a; node can have only one parent and several daughters. For a better; understanding of the hierarchy, have a look at; <https://root.cern.ch/doc/master/classTGeoManager.html>. Just close now the `X3D` window and focus at the wire frame picture; drawn in a pad. Activate Options/Event Status. Moving the mouse in the; pad, you will notice that objects are sometimes changing color to red.; Volumes are highlighted in this way whenever the mouse pointer is close; enough to one of its vertices. When this happens, the corresponding; volume is selected and you will see in the bottom right size of the ROOT; canvas its name, shape type and corresponding path in the physical tree.; Right clicking on the screen when a volume is selected will also open; its context menu (picking). Note that there are several actions that can; be performed both at view (no volume selected) and volume level. **`TView`** (mouse not selecting any volume):. - Click-and-drag rotates the view.; - Pressing some keys perform different actions:; - J/K - zoom / unzoom; - H, L, U, I - move the viewpoint; - Right click + `SetParallel` `()/SetPerspective` `()` - switch from; parallel to perspective view.; - Right click + `ShowAxis()` - show coordinate axes.; - Right click + `Centered/Left/Side/Top` - change view direction. **`TGeoVolume`** (mouse selecting a volume):. - Double click will focus the corresponding volume.; - Right click + `CheckOverlaps()` - run overlap checker on current; volume.; - Right click + `Draw` `()` - draw that volume according current; global visualization options; - Right click + `DrawOnly()`***` - `***draw only the selected volume.; - Right click + `InspectShape/Material()` - print info about shape or; material.; - Right click + `Raytrace()` - initiate a ray tracing algorithm on; current view.; - Right click + `RandomPo",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md:11492,perform,performed,11492,documentation/users-guide/Geometry.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md,1,['perform'],['performed']
Performance,"pile these BMIs(``*.pcm``) into object files(``*.o``) and add those object files to the archive instead. Consistency Requirement; ~~~~~~~~~~~~~~~~~~~~~~~. If we envision modules as a cache to speed up compilation, then - as with other caching techniques -; it is important to keep cache consistency.; So **currently** Clang will do very strict check for consistency. Options consistency; ^^^^^^^^^^^^^^^^^^^. The language option of module units and their non-module-unit users should be consistent.; The following example is not allowed:. .. code-block:: c++. // M.cppm; export module M;. // Use.cpp; import M;. .. code-block:: console. $ clang++ -std=c++20 M.cppm --precompile -o M.pcm; $ clang++ -std=c++23 Use.cpp -fprebuilt-module-path=. The compiler would reject the example due to the inconsistent language options.; Not all options are language options.; For example, the following example is allowed:. .. code-block:: console. $ clang++ -std=c++20 M.cppm --precompile -o M.pcm; # Inconsistent optimization level.; $ clang++ -std=c++20 -O3 Use.cpp -fprebuilt-module-path=.; # Inconsistent debugging level.; $ clang++ -std=c++20 -g Use.cpp -fprebuilt-module-path=. Although the two examples have inconsistent optimization and debugging level, both of them are accepted. Note that **currently** the compiler doesn't consider inconsistent macro definition a problem. For example:. .. code-block:: console. $ clang++ -std=c++20 M.cppm --precompile -o M.pcm; # Inconsistent optimization level.; $ clang++ -std=c++20 -O3 -DNDEBUG Use.cpp -fprebuilt-module-path=. Currently Clang would accept the above example. But it may produce surprising results if the; debugging code depends on consistent use of ``NDEBUG`` also in other translation units. Definitions consistency; ^^^^^^^^^^^^^^^^^^^^^^^. The C++ language defines that same declarations in different translation units should have; the same definition, as known as ODR (One Definition Rule). Prior to modules, the translation; units don't depend",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/StandardCPlusPlusModules.rst:16182,optimiz,optimization,16182,interpreter/llvm-project/clang/docs/StandardCPlusPlusModules.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/StandardCPlusPlusModules.rst,1,['optimiz'],['optimization']
Performance,"piler-level-spectre-fix-shows-how-hard-this-problem-will-be-to-solve/); [analysis](https://www.paulkocher.com/doc/MicrosoftCompilerSpectreMitigation.html); has shown that this approach is incomplete and only catches a small and limited; subset of attackable patterns which happen to resemble very closely the initial; proofs of concept. As such, while its performance is acceptable, it does not; appear to be an adequate systematic mitigation. ## Performance Overhead. The performance overhead of this style of comprehensive mitigation is very; high. However, it compares very favorably with previously recommended; approaches such as the `lfence` instruction. Just as users can restrict the; scope of `lfence` to control its performance impact, this mitigation technique; could be restricted in scope as well. However, it is important to understand what it would cost to get a fully; mitigated baseline. Here we assume targeting a Haswell (or newer) processor and; using all of the tricks to improve performance (so leaves the low 2gb; unprotected and +/- 2gb surrounding any PC in the program). We ran both; Google's microbenchmark suite and a large highly-tuned server built using; ThinLTO and PGO. All were built with `-march=haswell` to give access to BMI2; instructions, and benchmarks were run on large Haswell servers. We collected; data both with an `lfence`-based mitigation and load hardening as presented; here. The summary is that mitigating with load hardening is 1.77x faster than; mitigating with `lfence`, and the overhead of load hardening compared to a; normal program is likely between a 10% overhead and a 50% overhead with most; large applications seeing a 30% overhead or less. | Benchmark | `lfence` | Load Hardening | Mitigated Speedup |; | -------------------------------------- | -------: | -------------: | ----------------: |; | Google microbenchmark suite | -74.8% | -36.4% | **2.5x** |; | Large server QPS (using ThinLTO & PGO) | -62% | -29% | **1.8x** |. Below is a vi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md:47009,perform,performance,47009,interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,1,['perform'],['performance']
Performance,"pilers as well; as to improve functionality through Clang-specific features. The Clang; driver and language features are intentionally designed to be as; compatible with the GNU GCC compiler as reasonably possible, easing; migration from GCC to Clang. In most cases, code ""just works"".; Clang also provides an alternative driver, :ref:`clang-cl`, that is designed; to be compatible with the Visual C++ compiler, cl.exe. In addition to language specific features, Clang has a variety of; features that depend on what CPU architecture or operating system is; being compiled for. Please see the :ref:`Target-Specific Features and; Limitations <target_features>` section for more details. The rest of the introduction introduces some basic :ref:`compiler; terminology <terminology>` that is used throughout this manual and; contains a basic :ref:`introduction to using Clang <basicusage>` as a; command line compiler. .. _terminology:. Terminology; -----------. Front end, parser, backend, preprocessor, undefined behavior,; diagnostic, optimizer. .. _basicusage:. Basic Usage; -----------. Intro to how to use a C compiler for newbies. compile + link compile then link debug info enabling optimizations; picking a language to use, defaults to C17 by default. Autosenses based; on extension. using a makefile. Command Line Options; ====================. This section is generally an index into other sections. It does not go; into depth on the ones that are covered by other sections. However, the; first part introduces the language selection and other high level; options like :option:`-c`, :option:`-g`, etc. Options to Control Error and Warning Messages; ---------------------------------------------. .. option:: -Werror. Turn warnings into errors. .. This is in plain monospaced font because it generates the same label as; .. -Werror, and Sphinx complains. ``-Werror=foo``. Turn warning ""foo"" into an error. .. option:: -Wno-error=foo. Turn warning ""foo"" into a warning even if :option:`-Werror` i",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:3170,optimiz,optimizer,3170,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,1,['optimiz'],['optimizer']
Performance,"ping is too crude to; be useful as the typical usage in each standard library is too different.; Thus, for example, a thrown ``std::runtime_error`` instance will become a; ``cppyy.gbl.std.runtime_error`` instance on the Python side (with Python's; ``Exception`` as its base class), not a ``RuntimeError`` instance. The C++ code used for the examples below can be found; :doc:`here <cppyy_features_header>`, and it is assumed that that code is; loaded at the start of any session.; Download it, save it under the name ``features.h``, and load it:. .. code-block:: python. >>> import cppyy; >>> cppyy.include('features.h'); >>>. In addition, the examples require the ``throw`` to be in compiled code.; Save the following and build it into a shared library ``libfeatures.so`` (or; ``libfeatures.dll`` on MS Windows):. .. code-block:: C++. #include ""features.h"". void throw_an_error(int i) {; if (i) throw SomeError{""this is an error""};; throw SomeOtherError{""this is another error""};; }. And load the resulting library:. .. code-block:: python. >>> cppyy.load_library('libfeatures'); >>>. Then try it out:. .. code-block:: python. >>> cppyy.gbl.throw_an_error(1); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; cppyy.gbl.SomeError: void ::throw_an_error(int i) =>; SomeError: this is an error; >>> . Note how the full type is preserved and how the result of ``what()`` is used; for printing the exception.; By preserving the full C++ type, it is possible to call any other member; functions the exception may provide beyond ``what`` or access any additional; data it carries. To catch the exception, you can either use the full type, or any of its base; classes, including ``Exception`` and ``cppyy.gbl.std.exception``:. .. code-block:: python. >>> try:; ... cppyy.gbl.throw_an_error(0); ... except cppyy.gbl.SomeOtherError as e: # catch by exact type; ... print(""received:"", e); ... ; received: <cppyy.gbl.SomeOtherError object at 0x7f9e11d3db10>; >>> try:; ... cppyy.gbl.throw",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/exceptions.rst:2379,load,load,2379,bindings/pyroot/cppyy/cppyy/doc/source/exceptions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/exceptions.rst,1,['load'],['load']
Performance,"plains how to extend clang with the new functionality. **Parsing functionality**. If an extension modifies the standard parsing it needs to be added to; the clang frontend source code. This also means that the associated macro; indicating the presence of the extension should be added to clang. The default flow for adding a new extension into the frontend is to; modify `OpenCLExtensions.def; <https://github.com/llvm/llvm-project/blob/main/clang/include/clang/Basic/OpenCLExtensions.def>`__,; containing the list of all extensions and optional features supported by; the frontend. This will add the macro automatically and also add a field in the target; options ``clang::TargetOptions::OpenCLFeaturesMap`` to control the exposure; of the new extension during the compilation. Note that by default targets like `SPIR-V`, `SPIR` or `X86` expose all the OpenCL; extensions. For all other targets the configuration has to be made explicitly. Note that the target extension support performed by clang can be overridden; with :option:`-cl-ext` command-line flags. .. _opencl_ext_libs:. **Library functionality**. If an extension adds functionality that does not modify standard language; parsing it should not require modifying anything other than header files and; ``OpenCLBuiltins.td`` detailed in :ref:`OpenCL builtins <opencl_builtins>`.; Most commonly such extensions add functionality via libraries (by adding; non-native types or functions) parsed regularly. Similar to other languages this; is the most common way to add new functionality. Clang has standard headers where new types and functions are being added,; for more details refer to; :ref:`the section on the OpenCL Header <opencl_header>`. The macros indicating; the presence of such extensions can be added in the standard header files; conditioned on target specific predefined macros or/and language version; predefined macros (see `feature/extension preprocessor macros defined in; opencl-c-base.h; <https://github.com/llvm/llvm-pro",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/OpenCLSupport.rst:9267,perform,performed,9267,interpreter/llvm-project/clang/docs/OpenCLSupport.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/OpenCLSupport.rst,1,['perform'],['performed']
Performance,"plate that has all default template parameter without the \<\>.; With:. ``` {.cpp}; template <typename T = int> class templt {};; ```. With Cling (and any standard compliant compiler), using `*templt<>*` is; allowed (but `*templt*` is not). #### Namespace prefix of template parameters; Given `namespace N { class A; template <typename T> class B;}`, the name; `N::B<N::A>` is no longer ""shortened"" to `N::B<A>`. This affects the forward; and backward compatibility of files. #### Implicit dynamic up-casts; CINT would perform automatic upcasts to derived classes under certain contexts:. ``` {.cpp}; TH1* h1 = hpx; TH1F* h1f = h1;; ```. Cling does not allow this anymore. We might add this feature later if demand exists ([ROOT-4802](https://sft.its.cern.ch/jira/browse/ROOT-4802)). #### Using symbols that are only available at runtime: load libFoo; foo(); CINT was processing macros line by line; Cling compiles code.; When calling a function (or in general using a symbol) that is provided by a library loaded at runtime,; Cling will in some cases report an unresolved symbol:. ``` {.cpp}; #include ""Event.h""; void dynload() {; gSystem->Load(""libEvent"");; new Event();; }; ```. You will currently have to provide a rootmap file for libEvent (which also requires include; guards for Event.h). This might get fixed in a later version ([ROOT-4691](https://sft.its.cern.ch/jira/browse/ROOT-4691)). #### Using identifiers that are only available at runtime: gROOT->LoadMacro(""foo.h""); foo(); CINT was processing macros line by line; Cling compiles code.; During this compilation, Cling will not see identifiers provided by `gROOT->LoadMacro()`.; While this will covered by dynamic scopes, they are currently too limited to handle this.; Please `#include` the header instead. ### TInterpreter. ### TInterpreter. `TInterpreter::GetCurrentMacroName()` has a slightly different behavior in ROOT; 6 than in ROOT 5. In ROOT 5 it was a synonym for `__FILE__` (so please use; `__FILE__` to get the old value); ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/core/doc/v600/index.md:3914,load,loaded,3914,core/doc/v600/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/core/doc/v600/index.md,1,['load'],['loaded']
Performance,"ple anchor: the initial link to the location of the header and footer (cf. format specification); 3) A locator format: how are byte ranges addressed (e.g., through an offset in a file or an object ID). That means that new backends are likely to have implications on the RNTuple format specification. The page sources and sinks are ROOT internal classes.; They are not meant to be extended by users. Multi-Threading; ---------------. The following options exist in RNTuple for multithreaded data processing. ### Implicit Multi-Threading; When `ROOT::EnableImplicitMT()` is used, RNTuple uses ROOT's task arena to compress and decompress pages.; That requires writes to be buffered and reads uses the cluster pool resp.; The RNTuple data source for RDataFrame lets RDataFrame full control of the thread pool.; That means that RDataFrame uses a separate data source for every thread, each of the data sources runs in sequential mode. ### Concurrent Readers; Multiple readers can read the same RNTuple concurrently as long as access to every individual reader is sequential. ### Parallel REntry Preparation; Multiple `REntry` object can be concurrently prepared by multiple threads.; I.e., construction and binding of the objects can happen in parallel.; The actual reading and writing of entries (`RNTupleReader::LoadEntry()`, `RNTupleWriter::Fill()`) needs to be protected by a mutex.; This is considered ""mild scalability parallelization"" in RNTuple. ### RNTupleParallelWriter; The parallel writer offers the most scalable parallel writing interface.; Multiple _fill contexts_ can concurrently serialize and compress data.; Every fill context prepares a set of entire clusters in the final on-disk layout.; When a fill context flushes data,; a brief serialization point handles the RNTuple meta-data updates and the reservation of disk space to write into. Low precision float types; --------------------------; RNTuple supports encoding floating point types with a lower precision when writing them t",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/ntuple/v7/doc/Architecture.md:24969,concurren,concurrently,24969,tree/ntuple/v7/doc/Architecture.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/ntuple/v7/doc/Architecture.md,1,['concurren'],['concurrently']
Performance,"ple of ``element_size``; bytes wide and aligned at an ``element_size`` boundary. The order of the copy is unspecified. The same value may be read from the source; buffer many times, but only one write is issued to the destination buffer per; element. It is well defined to have concurrent reads and writes to both source; and destination provided those reads and writes are unordered atomic when; specified. This intrinsic does not provide any additional ordering guarantees over those; provided by a set of unordered loads from the source location and stores to the; destination. Lowering:; """""""""""""""""". In the most general case call to the; '``llvm.memmove.element.unordered.atomic.*``' is lowered to a call to the symbol; ``__llvm_memmove_element_unordered_atomic_*``. Where '*' is replaced with an; actual element size. See :ref:`RewriteStatepointsForGC intrinsic lowering; <RewriteStatepointsForGC_intrinsic_lowering>` for details on GC specific; lowering. The optimizer is allowed to inline the memory copy when it's profitable to do so. .. _int_memset_element_unordered_atomic:. '``llvm.memset.element.unordered.atomic``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". This is an overloaded intrinsic. You can use ``llvm.memset.element.unordered.atomic`` on; any integer bit width and for different address spaces. Not all targets; support all bit widths however. ::. declare void @llvm.memset.element.unordered.atomic.p0.i32(ptr <dest>,; i8 <value>,; i32 <len>,; i32 <element_size>); declare void @llvm.memset.element.unordered.atomic.p0.i64(ptr <dest>,; i8 <value>,; i64 <len>,; i32 <element_size>). Overview:; """""""""""""""""". The '``llvm.memset.element.unordered.atomic.*``' intrinsic is a specialization of the; '``llvm.memset.*``' intrinsic. It differs in that the ``dest`` is treated as an array; with elements that are exactly ``element_size`` bytes, and the assignment to that array; uses uses a sequence of :ref:`unordered atomic <ordering>` store operati",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:963153,optimiz,optimizer,963153,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['optimiz'],['optimizer']
Performance,"ple of configuring multiple distributions. Special Notes for Library-only Distributions; --------------------------------------------. One of the most powerful features of LLVM is its library-first design mentality; and the way you can compose a wide variety of tools using different portions of; LLVM. Even in this situation using *BUILD_SHARED_LIBS* is not supported. If you; want to distribute LLVM as a shared library for use in a tool, the recommended; method is using *LLVM_BUILD_LLVM_DYLIB*, and you can use *LLVM_DYLIB_COMPONENTS*; to configure which LLVM components are part of libLLVM.; Note: *LLVM_BUILD_LLVM_DYLIB* is not available on Windows. Options for Optimizing LLVM; ===========================. There are four main build optimizations that our CMake build system supports.; When performing a bootstrap build it is not beneficial to do anything other than; setting *CMAKE_BUILD_TYPE* to ``Release`` for the stage-1 compiler. This is; because the more intensive optimizations are expensive to perform and the; stage-1 compiler is thrown away. All of the further options described should be; set on the stage-2 compiler either using a CMake cache file, or by prefixing the; option with *BOOTSTRAP_*. The first and simplest to use is the compiler optimization level by setting the; *CMAKE_BUILD_TYPE* option. The main values of interest are ``Release`` or; ``RelWithDebInfo``. By default the ``Release`` option uses the ``-O3``; optimization level, and ``RelWithDebInfo`` uses ``-O2``. If you want to generate; debug information and use ``-O3`` you can override the; *CMAKE_<LANG>_FLAGS_RELWITHDEBINFO* option for C and CXX.; DistributionExample.cmake does this. Another easy to use option is Link-Time-Optimization. You can set the; *LLVM_ENABLE_LTO* option on your stage-2 build to ``Thin`` or ``Full`` to enable; building LLVM with LTO. These options will significantly increase link time of; the binaries in the distribution, but it will create much faster binaries. This; option sh",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BuildingADistribution.rst:7799,optimiz,optimizations,7799,interpreter/llvm-project/llvm/docs/BuildingADistribution.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BuildingADistribution.rst,2,"['optimiz', 'perform']","['optimizations', 'perform']"
Performance,"ple' in provided <div> element; // one also can specify ""grid2x2"" or ""flex"" or ""tabs""; h.setDisplay(""simple"", ""myMainDiv"");. // open file and display element; await h.openRootFile('../../files/hsimple.root');; await h.display('hpxpy;1"",""colz');; </script>; ```. After script loading one can configure different parameters in `gStyle` object.; It is instance of the `TStyle` object and behaves like `gStyle` variable in ROOT. For instance,; to change stat format using to display value in stats box:. ```javascript; import { gStyle } from 'https://root.cern/js/latest/modules/main.mjs';; gStyle.fStatFormat = '7.5g';; ```. There is also `settings` object which contains all other JSROOT settings. For instance,; one can configure custom format for different axes:. ```javascript; import { settings } from 'https://root.cern/js/latest/modules/main.mjs';; settings.XValuesFormat = '4.2g';; settings.YValuesFormat = '6.1f';; ```. One also can use `build/jsroot.js` bundle to load all functionality at one and access it via `JSROOT` global handle:. ```javascript; <script src=""https://root.cern/js/latest/build/jsroot.js""></script>; <script>; // getting json string from somewhere; let obj = JSROOT.parse(root_json);; JSROOT.draw('plain', obj, 'colz');; </script>; ```. ### Use of JSON. It is strongly recommended to use JSON when communicating with ROOT application.; THttpServer provides a JSON representation for every registered object with an url address like:. http://your_root_server:8080/Canvases/c1/root.json. Such JSON representation generated using the [TBufferJSON](https://root.cern/doc/master/classTBufferJSON.html) class. One could create JSON file for any ROOT object directly, just writing in the code:. ```cpp; obj->SaveAs(""file.json"");; ```. To access data from a remote web server, it is recommended to use the `httpRequest` method.; For instance to receive object from a THttpServer server one could do:. ```javascript; import { httpRequest } from 'https://root.cern/js/latest/modules",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/JSROOT/JSROOT.md:36282,load,load,36282,documentation/JSROOT/JSROOT.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/JSROOT/JSROOT.md,1,['load'],['load']
Performance,"ple, the linker recognizes that ``foo2()`` is an externally; visible symbol defined in LLVM bitcode file. The linker completes its usual; symbol resolution pass and finds that ``foo2()`` is not used; anywhere. This information is used by the LLVM optimizer and it; removes ``foo2()``. * As soon as ``foo2()`` is removed, the optimizer recognizes that condition ``i; < 0`` is always false, which means ``foo3()`` is never used. Hence, the; optimizer also removes ``foo3()``. * And this in turn, enables linker to remove ``foo4()``. This example illustrates the advantage of tight integration with the; linker. Here, the optimizer can not remove ``foo3()`` without the linker's; input. Alternative Approaches; ----------------------. **Compiler driver invokes link time optimizer separately.**; In this model the link time optimizer is not able to take advantage of; information collected during the linker's normal symbol resolution phase.; In the above example, the optimizer can not remove ``foo2()`` without the; linker's input because it is externally visible. This in turn prohibits the; optimizer from removing ``foo3()``. **Use separate tool to collect symbol information from all object files.**; In this model, a new, separate, tool or library replicates the linker's; capability to collect information for link time optimization. Not only is; this code duplication difficult to justify, but it also has several other; disadvantages. For example, the linking semantics and the features provided; by the linker on various platform are not unique. This means, this new tool; needs to support all such features and platforms in one super tool or a; separate tool per platform is required. This increases maintenance cost for; link time optimizer significantly, which is not necessary. This approach; also requires staying synchronized with linker developments on various; platforms, which is not the main focus of the link time optimizer. Finally,; this approach increases end user's build time d",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:3480,optimiz,optimizer,3480,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,1,['optimiz'],['optimizer']
Performance,"plemented, specific to 32 and 64 bit; architectures. It is configurable via compile time options. - the Secondary allocator: slower, it services larger allocation sizes via the; memory mapping primitives of the underlying operating system. Secondary backed; allocations are surrounded by Guard Pages. It is also configurable via compile; time options. - the thread specific data Registry: defines how local caches operate for each; thread. There are currently two models implemented: the exclusive model where; each thread holds its own caches (using the ELF TLS); or the shared model; where threads share a fixed size pool of caches. - the Quarantine: offers a way to delay the deallocation operations, preventing; blocks to be immediately available for reuse. Blocks held will be recycled; once certain size criteria are reached. This is essentially a delayed freelist; which can help mitigate some use-after-free situations. This feature is fairly; costly in terms of performance and memory footprint, is mostly controlled by; runtime options and is disabled by default. Allocations Header; ------------------; Every chunk of heap memory returned to an application by the allocator will be; preceded by a header. This has two purposes:. - being to store various information about the chunk, that can be leveraged to; ensure consistency of the heap operations;. - being able to detect potential corruption. For this purpose, the header is; checksummed and corruption of the header will be detected when said header is; accessed (note that if the corrupted header is not accessed, the corruption; will remain undetected). The following information is stored in the header:. - the class ID for that chunk, which identifies the region where the chunk; resides for Primary backed allocations, or 0 for Secondary backed allocations;. - the state of the chunk (available, allocated or quarantined);. - the allocation type (malloc, new, new[] or memalign), to detect potential; mismatches in the allocatio",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ScudoHardenedAllocator.rst:2379,perform,performance,2379,interpreter/llvm-project/llvm/docs/ScudoHardenedAllocator.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ScudoHardenedAllocator.rst,1,['perform'],['performance']
Performance,"plements an LLVM target. This will permit the target name to be used with; the :option:`-march` option so that code can be generated for that target. .. option:: -meabi=[default|gnu|4|5]. Specify which EABI version should conform to. Valid EABI versions are *gnu*,; *4* and *5*. Default value (*default*) depends on the triple. .. option:: -stack-size-section. Emit the .stack_sizes section which contains stack size metadata. The section; contains an array of pairs of function symbol values (pointer size) and stack; sizes (unsigned LEB128). The stack size values only include the space allocated; in the function prologue. Functions with dynamic stack allocations are not; included. .. option:: -remarks-section. Emit the __remarks (MachO) section which contains metadata about remark; diagnostics. Tuning/Configuration Options; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~. .. option:: --print-after-isel. Print generated machine code after instruction selection (useful for debugging). .. option:: --regalloc=<allocator>. Specify the register allocator to use.; Valid register allocators are:. *basic*. Basic register allocator. *fast*. Fast register allocator. It is the default for unoptimized code. *greedy*. Greedy register allocator. It is the default for optimized code. *pbqp*. Register allocator based on 'Partitioned Boolean Quadratic Programming'. .. option:: --spiller=<spiller>. Specify the spiller to use for register allocators that support it. Currently; this option is used only by the linear scan register allocator. The default; ``spiller`` is *local*. Valid spillers are:. *simple*. Simple spiller. *local*. Local spiller. Intel IA-32-specific Options; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~. .. option:: --x86-asm-syntax=[att|intel]. Specify whether to emit assembly code in AT&T syntax (the default) or Intel; syntax. EXIT STATUS; -----------. If :program:`llc` succeeds, it will exit with 0. Otherwise, if an error; occurs, it will exit with a non-zero value. SEE ALSO; --------. :manpage:`lli(1)`; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llc.rst:5663,optimiz,optimized,5663,interpreter/llvm-project/llvm/docs/CommandGuide/llc.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llc.rst,1,['optimiz'],['optimized']
Performance,"pleted before; performing the; following; fence-paired-atomic. fence release - agent *none* 1. buffer_wbl2 sc1=1. - If OpenCL and; address space is; local, omit.; - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at agent scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate. If; fence had an; address space then; set to address; space of OpenCL; fence flag, or to; generic if both; local and global; flags are; specified.; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; any following store; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - Ensures that all; memory operations; have; completed before; performing the; following; fence-paired-atomic. fence release - system *none* 1. buffer_wbl2 sc0=1 sc1=1. - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at system scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; a",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:314804,load,load,314804,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"pletely promote the IV to i64?). Beyond that, we have a partially redundant load in the loop: if ""winner"" (aka ; %i.01718) isn't updated, we reload Y[winner].y the next time through the loop.; Similarly, the addressing that feeds it (including the sext) is redundant. In; the end we get this generated assembly:. LBB0_2: ## %for.body; ## =>This Inner Loop Header: Depth=1; 	movsd	(%rdi), %xmm0; 	movslq	%edx, %r8; 	shlq	$4, %r8; 	ucomisd	(%rcx,%r8), %xmm0; 	jbe	LBB0_4; 	movl	%esi, %edx; LBB0_4: ## %for.inc; 	addq	$16, %rdi; 	incq	%rsi; 	cmpq	%rsi, %rax; 	jne	LBB0_2. All things considered this isn't too bad, but we shouldn't need the movslq or; the shlq instruction, or the load folded into ucomisd every time through the; loop. On an x86-specific topic, if the loop can't be restructure, the movl should be a; cmov. //===---------------------------------------------------------------------===//. [STORE SINKING]. GCC PR37810 is an interesting case where we should sink load/store reload; into the if block and outside the loop, so we don't reload/store it on the; non-call path. for () {; *P += 1;; if (); call();; else; ...; ->; tmp = *P; for () {; tmp += 1;; if () {; *P = tmp;; call();; tmp = *P;; } else ...; }; *P = tmp;. We now hoist the reload after the call (Transforms/GVN/lpre-call-wrap.ll), but; we don't sink the store. We need partially dead store sinking. //===---------------------------------------------------------------------===//. [LOAD PRE CRIT EDGE SPLITTING]. GCC PR37166: Sinking of loads prevents SROA'ing the ""g"" struct on the stack; leading to excess stack traffic. This could be handled by GVN with some crazy; symbolic phi translation. The code we get looks like (g is on the stack):. bb2:		; preds = %bb1; ..; 	%9 = getelementptr %struct.f* %g, i32 0, i32 0		; 	store i32 %8, i32* %9, align bel %bb3. bb3:		; preds = %bb1, %bb2, %bb; 	%c_addr.0 = phi %struct.f* [ %g, %bb2 ], [ %c, %bb ], [ %c, %bb1 ]; 	%b_addr.0 = phi %struct.f* [ %b, %bb2 ], [ %g, %bb ], [ %b, %",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:32092,load,load,32092,interpreter/llvm-project/llvm/lib/Target/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt,2,['load'],['load']
Performance,"plicated than we may expect; because LoopRotate ensures that the loop is in; :ref:`Loop Simplify Form <loop-terminology-loop-simplify>`; after rotation.; In this case, it inserted the %loop.preheader basic block so; that the loop has a preheader and it introduced the %loop.exit; basic block so that the loop has dedicated exits; (otherwise, %exit would be jumped from both %latch and %entry,; but %entry is not contained in the loop).; Note that a loop has to be in Loop Simplify Form beforehand; too for LoopRotate to be applied successfully. The main advantage of this form is that it allows hoisting; invariant instructions, especially loads, into the preheader.; That could be done in non-rotated loops as well but with; some disadvantages. Let's illustrate them with an example:. .. code-block:: C. for (int i = 0; i < n; ++i) {; auto v = *p;; use(v);; }. We assume that loading from p is invariant and use(v) is some; statement that uses v.; If we wanted to execute the load only once we could move it; ""out"" of the loop body, resulting in this:. .. code-block:: C. auto v = *p;; for (int i = 0; i < n; ++i) {; use(v);; }. However, now, in the case that n <= 0, in the initial form,; the loop body would never execute, and so, the load would; never execute. This is a problem mainly for semantic reasons.; Consider the case in which n <= 0 and loading from p is invalid.; In the initial program there would be no error. However, with this; transformation we would introduce one, effectively breaking; the initial semantics. To avoid both of these problems, we can insert a guard:. .. code-block:: C. if (n > 0) { // loop guard; auto v = *p;; for (int i = 0; i < n; ++i) {; use(v);; }; }. This is certainly better but it could be improved slightly. Notice; that the check for whether n is bigger than 0 is executed twice (and; n does not change in between). Once when we check the guard condition; and once in the first execution of the loop. To avoid that, we could; do an unconditional first ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:22594,load,load,22594,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst,1,['load'],['load']
Performance,"plication code. We call **`TGClient`** telling it the pixmap's; file name to create a **`TGPicture`** object and, in turn, it will; return a pointer to the created object. If the pixmap file cannot be; found the returned pointer will be `NULL`. As usual, the first parameter; of a **`TGIcon`** constructor is the parent frame. The second one is the; **`TGPicture`** object holding the pixmap we want to show. Last two; parameters define the width and height of pixmap in pixels. In the end; we add the created icon object to its parent. ``` {.cpp}; // icon widget; const TGPicture *ipic=(TGPicture *)gClient->GetPicture(""leaf.xpm"");; TGIcon *icon = new TGIcon(parent,ipic,40,40);; parent->AddFrame(icon,new TGLayoutHints(kLHintsLeft|kLHintsBottom,; 1, 15, 1, 1));; ```. The **`TGPicture`** objects are cached by **`TGClient`** in order to; keep the resource usage low and to improve the efficiency of the; client-server windowing systems. **`TGClient`** will check whether a; pixmap with the same name was already loaded before to register a new; picture object. If it finds it, it will return a pointer to the existing; object. Also, it will increase the usage counter for the object. All **`TGPicture`** objects are managed by the class; **`TGPicturePool`**. **`TGClient`** creates an object of this type upon; initialization. Normally your application program does not deal directly; with this class because all manipulations go through **`TGClient`**; class. Once you have finished with using of the **`TGPicture`** object, you; should call the method **`TGClient::FreePicture(const TGPicture *pic)`**; to free it. The usage counter of the picture object will be decreased; and when it reaches zero - the **`TGPicture`** object will be deleted. ### Status Bar. The status bar widget is used to display some information about the; current application state: what is being viewed in the window, a; descriptive message about selected objects, or other no interactive; information. It may also be use",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/WritingGUI.md:91192,load,loaded,91192,documentation/users-guide/WritingGUI.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/WritingGUI.md,1,['load'],['loaded']
Performance,"plit into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - Must happen before; the following; buffer_inv.; - Ensures that the; fence-paired atomic; has completed; before invalidating; the; cache. Therefore; any following; locations read must; be no older than; the value read by; the; fence-paired-atomic. 2. buffer_inv sc0=1 sc1=1. - Must happen before any; following global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. **Release Atomic**; ------------------------------------------------------------------------------------; store atomic release - singlethread - global 1. GFX940, GFX941; - wavefront - generic buffer/global/flat_store; sc0=1 sc1=1; GFX942; buffer/global/flat_store. store atomic release - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; be used.*. 1. ds_store; store atomic release - workgroup - global 1. s_waitcnt lgkm/vmcnt(0); - generic; - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL, omit lgkmcnt(0).; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load/store/; load atomic/store atomic/; atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; store.; - Ensures that all; memory operations; have; completed before; perform",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:306461,load,loads,306461,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['loads']
Performance,"plit level is 1).; Several run-time performance improvements.; In TTree::Fill use fZipBytes instead of fTotBytes for deciding when to flush or autosave.; Properly handle TTree aliases containing array indices.; Fix the default sorting order of baskets when the TTree is an older in-memory TTree.; Enhance the sort order to use the 'entry number' when the seek position are equal.; Consequently the default sort order for an older in-memory TTree is now; essentially kSortBasketsByEntry rather than kSortBasketsByBranch (old 'correct' sort; order) or 'random' (the 'broken' sort order prior to this release). IMPORTANT enhancement in TTree::Fill:; Slides from a recent seminar describing the main features of ROOT IO and Trees and the recent; improvements described below are available at; http://root.cern/files/brun_lcgapp09.pptx ; or; http://root.cern/files/brun_lcgapp09.pdf .; The baskets are flushed and the Tree header saved at regular intervals (See AutoFlush and OptimizeBaskets); When the amount of data written so far (fTotBytes) is greater than fAutoFlush (see SetAutoFlush) all the baskets are flushed to disk.; This makes future reading faster as it guarantees that baskets belonging to nearby entries will be on the same disk region.; When the first call to flush the baskets happens, we also take this opportunity to optimize the baskets buffers.; We also check if the number of bytes written is greater than fAutoSave (see SetAutoSave).; In this case we also write the Tree header. This makes the Tree recoverable up to this point in case the program writing the Tree crashes.; Note that the user can also decide to call FlushBaskets and AutoSave in her event loop on the base of the number of events written instead of the number of bytes written.; New function TTree::OptimizeBaskets. void TTree::OptimizeBaskets(Int_t maxMemory, Float_t minComp, Option_t *option). This function may be called after having filled some entries in a Tree; using the information in the existing branch ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/doc/v526/index.html:3504,Optimiz,OptimizeBaskets,3504,tree/doc/v526/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/doc/v526/index.html,1,['Optimiz'],['OptimizeBaskets']
Performance,"ply constant term optimization on pdf and dataset inside a likelihood; Applying the default `ConstantTermsOptimizer` optimization routines on the pdf and dataset inside a `RooAbsL` likelihood is as simple as:. ``` {.cpp}; likelihood.constOptimizeTestStatistic();; ```; This applies constant term optimization to the cloned pdf and dataset inside the likelihood object.; It will not modify anything outside of the likelihood. Optimization can also be activated through the minimizer, which may be more familiar to most users.; Given the `RooMinimizer` object `m` as defined in the example above, we can do:; ``` {.cpp}; m.optimizeConst(2);; ```. For the adventurous user, it is also possible to apply constant term optimization to a pdf and dataset directly without needing a likelihood object, e.g. given some `RooArgSet` set of observables `normSet`:; ``` {.cpp}; bool applyTrackingOpt = true;; ConstantTermsOptimizer::enableConstantTermsOptimization(&pdf, &normSet, dataset, applyTrackingOpt);; ```; We refer to RooFit documentation for more about ""tracking optimization"" which can be enabled or disabled using the final boolean parameter. ## Caveats; This package is still under development.; Some functionality that users of `RooAbsPdf::fitTo` or `RooAbsPdf::createNLL` were used to has not yet been ported to this namespace.; However, the functionality that is implemented has been tested thoroughly for a set of common usage patterns and should work as expected. The classes implemented here will give the exact same numerical results for most fits.; One notable exception is fitting _simultaneous_ pdfs with a _constrained_ term _when using offsetting_.; Because offsetting is handled differently in the `TestStatistics` classes compared to the way it was done in the object returned from `RooAbsPdf::createNLL` (a `RooAddition` of an offset `RooNLLVar` and a non-offset `RooConstraintSum`, whereas `RooSumL` applies the offset to the; total sum of its binned, unbinned and constraint componen",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/developers/test_statistics.md:11362,optimiz,optimization,11362,roofit/doc/developers/test_statistics.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/developers/test_statistics.md,1,['optimiz'],['optimization']
Performance,"plying; various optimization passes. Phase 4 : Symbol Resolution after optimization; ----------------------------------------------. In this phase, the linker reads optimized a native object file and updates the; internal global symbol table to reflect any changes. The linker also collects; information about any changes in use of external symbols by LLVM bitcode; files. In the example above, the linker notes that ``foo4()`` is not used any; more. If dead code stripping is enabled then the linker refreshes the live; symbol information appropriately and performs dead code stripping. After this phase, the linker continues linking as if it never saw LLVM bitcode; files. .. _libLTO:. ``libLTO``; ==========. ``libLTO`` is a shared object that is part of the LLVM tools, and is intended; for use by a linker. ``libLTO`` provides an abstract C interface to use the LLVM; interprocedural optimizer without exposing details of LLVM's internals. The; intention is to keep the interface as stable as possible even when the LLVM; optimizer continues to evolve. It should even be possible for a completely; different compilation technology to provide a different libLTO that works with; their object files and the standard linker tool. ``lto_module_t``; ----------------. A non-native object file is handled via an ``lto_module_t``. The following; functions allow the linker to check if a file (on disk or in a memory buffer) is; a file which libLTO can process:. .. code-block:: c. lto_module_is_object_file(const char*); lto_module_is_object_file_for_target(const char*, const char*); lto_module_is_object_file_in_memory(const void*, size_t); lto_module_is_object_file_in_memory_for_target(const void*, size_t, const char*). If the object file can be processed by ``libLTO``, the linker creates a; ``lto_module_t`` by using one of:. .. code-block:: c. lto_module_create(const char*); lto_module_create_from_memory(const void*, size_t). and when done, the handle is released via. .. code-block:: c. lto_m",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:8346,optimiz,optimizer,8346,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,1,['optimiz'],['optimizer']
Performance,"pointer is provided directly to the GEP; instruction as an operand without any need for accessing memory. It must,; therefore be indexed and requires an index operand. Consider this example:. .. code-block:: c++. struct munger_struct {; int f1;; int f2;; };; void munge(struct munger_struct *P) {; P[0].f1 = P[1].f1 + P[2].f2;; }; ...; struct munger_struct Array[3];; ...; munge(Array);. In this ""C"" example, the front end compiler (Clang) will generate three GEP; instructions for the three indices through ""P"" in the assignment statement. The; function argument ``P`` will be the second operand of each of these GEP; instructions. The third operand indexes through that pointer. The fourth; operand will be the field offset into the ``struct munger_struct`` type, for; either the ``f1`` or ``f2`` field. So, in LLVM assembly the ``munge`` function; looks like:. .. code-block:: llvm. define void @munge(ptr %P) {; entry:; %tmp = getelementptr %struct.munger_struct, ptr %P, i32 1, i32 0; %tmp1 = load i32, ptr %tmp; %tmp2 = getelementptr %struct.munger_struct, ptr %P, i32 2, i32 1; %tmp3 = load i32, ptr %tmp2; %tmp4 = add i32 %tmp3, %tmp1; %tmp5 = getelementptr %struct.munger_struct, ptr %P, i32 0, i32 0; store i32 %tmp4, ptr %tmp5; ret void; }. In each case the second operand is the pointer through which the GEP instruction; starts. The same is true whether the second operand is an argument, allocated; memory, or a global variable. To make this clear, let's consider a more obtuse example:. .. code-block:: text. @MyVar = external global i32; ...; %idx1 = getelementptr i32, ptr @MyVar, i64 0; %idx2 = getelementptr i32, ptr @MyVar, i64 1; %idx3 = getelementptr i32, ptr @MyVar, i64 2. These GEP instructions are simply making address computations from the base; address of ``MyVar``. They compute, as follows (using C syntax):. .. code-block:: c++. idx1 = (char*) &MyVar + 0; idx2 = (char*) &MyVar + 4; idx3 = (char*) &MyVar + 8. Since the type ``i32`` is known to be four bytes long, the ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GetElementPtr.rst:3248,load,load,3248,interpreter/llvm-project/llvm/docs/GetElementPtr.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GetElementPtr.rst,1,['load'],['load']
Performance,"pointer; from a function which is declared to never return null.; - ``-fsanitize=shift``: Shift operators where the amount shifted is; greater or equal to the promoted bit-width of the left hand side; or less than zero, or where the left hand side is negative. For a; signed left shift, also checks for signed overflow in C, and for; unsigned overflow in C++. You can use ``-fsanitize=shift-base`` or; ``-fsanitize=shift-exponent`` to check only left-hand side or; right-hand side of shift operation, respectively.; - ``-fsanitize=unsigned-shift-base``: check that an unsigned left-hand side of; a left shift operation doesn't overflow. Issues caught by this sanitizer are; not undefined behavior, but are often unintentional.; - ``-fsanitize=signed-integer-overflow``: Signed integer overflow, where the; result of a signed integer computation cannot be represented in its type.; This includes all the checks covered by ``-ftrapv``, as well as checks for; signed division overflow (``INT_MIN/-1``), but not checks for; lossy implicit conversions performed before the computation; (see ``-fsanitize=implicit-conversion``). Both of these two issues are; handled by ``-fsanitize=implicit-conversion`` group of checks.; - ``-fsanitize=unreachable``: If control flow reaches an unreachable; program point.; - ``-fsanitize=unsigned-integer-overflow``: Unsigned integer overflow, where; the result of an unsigned integer computation cannot be represented in its; type. Unlike signed integer overflow, this is not undefined behavior, but; it is often unintentional. This sanitizer does not check for lossy implicit; conversions performed before such a computation; (see ``-fsanitize=implicit-conversion``).; - ``-fsanitize=vla-bound``: A variable-length array whose bound; does not evaluate to a positive value.; - ``-fsanitize=vptr``: Use of an object whose vptr indicates that it is of; the wrong dynamic type, or that its lifetime has not begun or has ended.; Incompatible with ``-fno-rtti``. Link must be",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UndefinedBehaviorSanitizer.rst:8954,perform,performed,8954,interpreter/llvm-project/clang/docs/UndefinedBehaviorSanitizer.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UndefinedBehaviorSanitizer.rst,1,['perform'],['performed']
Performance,"pointers in CUDA are not explicitly; annotated with their address space, so it's up to LLVM to infer it where; possible. * `Bypassing 64-bit divides; <https://llvm.org/docs/doxygen/html/BypassSlowDivision_8cpp_source.html>`_ --; This was an existing optimization that we enabled for the PTX backend. 64-bit integer divides are much slower than 32-bit ones on NVIDIA GPUs.; Many of the 64-bit divides in our benchmarks have a divisor and dividend; which fit in 32-bits at runtime. This optimization provides a fast path for; this common case. * Aggressive loop unrolling and function inlining -- Loop unrolling and; function inlining need to be more aggressive for GPUs than for CPUs because; control flow transfer in GPU is more expensive. More aggressive unrolling and; inlining also promote other optimizations, such as constant propagation and; SROA, which sometimes speed up code by over 10x. (Programmers can force unrolling and inline using clang's `loop unrolling pragmas; <https://clang.llvm.org/docs/AttributeReference.html#pragma-unroll-pragma-nounroll>`_; and ``__attribute__((always_inline))``.). Publication; ===========. The team at Google published a paper in CGO 2016 detailing the optimizations; they'd made to clang/LLVM. Note that ""gpucc"" is no longer a meaningful name:; The relevant tools are now just vanilla clang/LLVM. | `gpucc: An Open-Source GPGPU Compiler <http://dl.acm.org/citation.cfm?id=2854041>`_; | Jingyue Wu, Artem Belevich, Eli Bendersky, Mark Heffernan, Chris Leary, Jacques Pienaar, Bjarke Roune, Rob Springer, Xuetian Weng, Robert Hundt; | *Proceedings of the 2016 International Symposium on Code Generation and Optimization (CGO 2016)*; |; | `Slides from the CGO talk <http://wujingyue.github.io/docs/gpucc-talk.pdf>`_; |; | `Tutorial given at CGO <http://wujingyue.github.io/docs/gpucc-tutorial.pdf>`_. Obtaining Help; ==============. To obtain help on LLVM in general and its CUDA support, see `the LLVM; community <https://llvm.org/docs/#mailing-lists>`_.; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CompileCudaWithLLVM.rst:20337,optimiz,optimizations,20337,interpreter/llvm-project/llvm/docs/CompileCudaWithLLVM.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CompileCudaWithLLVM.rst,1,['optimiz'],['optimizations']
Performance,"popup a dialog to open a file. - *Close Canvas*: close the canvas window. - *Save*: save the drawing of the current canvas in a format; selectable from the submenu. The current canvas name is used as a; file name for various formats such as PostScript, GIF, JPEG, C; macro file, root file. - *Save As...*: popup a dialog for saving the current canvas drawing; in a new filename. - *Print*: popup a dialog to print the current canvas drawing. - *Quit ROOT*: exit the ROOT session. ![](pictures/0300000B.png). #### Edit Menu. There is only one active menu entry in the Edit menu. The others menu; entries will be implemented and will become active in the near future. - *Clear:* delete all objects in the canvas; or in the selected pad according to the selected entry in the; submenu. #### View Menu. - *Editor*: toggles the view of the editor. If it is selected; activates and shows up the editor on the left side of the canvas; window. According to the selected object, the editor loads the; corresponding user interface for easy change of the object's; attributes. - *Toolbar*: toggles the view of the toolbar. If it is selected; activates and shows up the toolbar. It contains buttons for easy; and fast access to most frequently used commands and for graphics; primitive drawing. Tool tips are provided for helping users. - *Status Bar*: toggles the view of the status bar. If it is; selected, the status bar below the canvas window shows up. There; the identification of the objects is displayed when moving the; mouse (such as the object's name, the object's type, its; coordinates, etc.). - *Colors*: creates a new canvas showing the color palette. - *Markers*: creates a new canvas showing the various marker styles. - *Iconify*: create the canvas window icon, does not close the; canvas. - *View With...*: If the last selected pad contains a 3-d structure,; a new canvas is created with a 3-D picture according to the; selection made from the cascaded menu: X3D or OpenGL. The 3-D; image can ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/GettingStarted.md:8636,load,loads,8636,documentation/users-guide/GettingStarted.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/GettingStarted.md,1,['load'],['loads']
Performance,"port a; thread-local storage class. Variables with this storage class have distinct; values and addresses in distinct threads, much as automatic variables have; distinct values and addresses in each subprogram invocation. Typically,; there is a single block of storage containing all thread-local variables; declared in the main executable, and a separate block for the variables; declared in each shared library. Each thread-local variable can then be; accessed in its block using an identifier. This identifier is typically a; byte offset into the block and pushed onto the DWARF stack by one of the*; ``DW_OP_const*`` *operations prior to the* ``DW_OP_form_tls_address``; *operation. Computing the address of the appropriate block can be complex; (in some cases, the compiler emits a function call to do it), and difficult; to describe using ordinary DWARF location descriptions. Instead of forcing; complex thread-local storage calculations into the DWARF expressions, the*; ``DW_OP_form_tls_address`` *allows the consumer to perform the computation; based on the target architecture specific run-time environment.*. 5. ``DW_OP_call_frame_cfa``. ``DW_OP_call_frame_cfa`` pushes the location description L of the Canonical; Frame Address (CFA) of the current subprogram, obtained from the call frame; information on the stack. See :ref:`amdgpu-dwarf-call-frame-information`. *Although the value of the* ``DW_AT_frame_base`` *attribute of the debugger; information entry corresponding to the current subprogram can be computed; using a location list expression, in some cases this would require an; extensive location list because the values of the registers used in; computing the CFA change during a subprogram execution. If the call frame; information is present, then it already encodes such changes, and it is; space efficient to reference that using the* ``DW_OP_call_frame_cfa``; *operation.*. 6. ``DW_OP_fbreg``. ``DW_OP_fbreg`` has a single signed LEB128 integer operand that represents a;",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst:113838,perform,perform,113838,interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst,1,['perform'],['perform']
Performance,"pose of the ``llvm.arithmetic.fence`` intrinsic; is to prevent the optimizer from performing fast-math optimizations,; particularly reassociation,; between the argument and the expression that contains the argument.; It can be used to preserve the parentheses in the source language. Arguments:; """""""""""""""""""". The ``llvm.arithmetic.fence`` intrinsic takes only one argument.; The argument and the return value are floating-point numbers,; or vector floating-point numbers, of the same type. Semantics:; """""""""""""""""""". This intrinsic returns the value of its operand. The optimizer can optimize; the argument, but the optimizer cannot hoist any component of the operand; to the containing context, and the optimizer cannot move the calculation of; any expression in the containing context into the operand. '``llvm.donothing``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare void @llvm.donothing() nounwind memory(none). Overview:; """""""""""""""""". The ``llvm.donothing`` intrinsic doesn't perform any operation. It's one of only; three intrinsics (besides ``llvm.experimental.patchpoint`` and; ``llvm.experimental.gc.statepoint``) that can be called with an invoke; instruction. Arguments:; """""""""""""""""""". None. Semantics:; """""""""""""""""""". This intrinsic does nothing, and it's removed by optimizers and ignored; by codegen. '``llvm.experimental.deoptimize``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare type @llvm.experimental.deoptimize(...) [ ""deopt""(...) ]. Overview:; """""""""""""""""". This intrinsic, together with :ref:`deoptimization operand bundles; <deopt_opbundles>`, allow frontends to express transfer of control and; frame-local state from the currently executing (typically more specialized,; hence faster) version of a function into another (typically more generic, hence; slower) version. In languages with a fully integrated managed runtime like Java and JavaScript; this intrinsic can be used to implement ""uncommon trap"" or ""side exit"" ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:941653,perform,perform,941653,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['perform']
Performance,"positioning the; container itself. However, there are many practical cases when defining; such a container is not straightforward or even possible without; generating overlaps with the rest of the geometry. There are few ways; out of this:. - Defining the container for the structure as ""overlapping"" (see also; "" Overlapping Volumes **""**); - Representing the container as a composite shape - the Boolean union; of all components (see also "" Composite Shapes ""); - Using an assembly volume - this will be described in the following. The first two approaches have the disadvantage of penalizing the; navigation performance with a factor increasing more than linear of the; number of components in the structure. The best solution is the third; one because it uses all volume-related navigation optimizations. The; class **`TGeoVolumeAssembly`** represents an assembly volume. Its shape; is represented by **`TGeoShapeAssembly`** class that is the union of all; components. It uses volume voxelization to perform navigation tasks. An assembly volume creates a hierarchical level and it geometrically; insulates the structure from the rest (as a normal volume). Physically,; a point that is INSIDE a **`TGeoShapeAssembly`** is always inside one of; the components, so a **`TGeoVolumeAssembly`** does not need to have a; medium. Due to the self-containment of assemblies, they are very; practical to use when a container is hard to define due to possible; overlaps during positioning. For instance, it is very easy creating; honeycomb structures. A very useful example for creating and using; assemblies can be found at:; <http://root.cern.ch/root/html/examples/assembly.C.html>`.`. Creation of an assembly is very easy: one has just to create a; **`TGeoVolumeAssembly`** object and position the components inside as; for any volume:. ``` {.cpp}; TGeoVolume *vol = new TGeoVolumeAssembly(name);; vol->AddNode(vdaughter1, cpy1, matrix1);; vol->AddNode(vdaughter2, cpy2, matrix2);; ```. Note that component",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md:89436,perform,perform,89436,documentation/users-guide/Geometry.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md,1,['perform'],['perform']
Performance,"positive n; (before loop rotation) is 1, even though the loop body is not executed; at all. .. code-block:: C. for (int i = 0; i < n; ++i); body(i);. A better measure is the **backedge-taken count**, which is the number of; times any of the backedges is taken before the loop. It is one less than; the trip count for executions that enter the header. .. _loopinfo:. LoopInfo; ========. LoopInfo is the core analysis for obtaining information about loops.; There are few key implications of the definitions given above which; are important for working successfully with this interface. * LoopInfo does not contain information about non-loop cycles. As a; result, it is not suitable for any algorithm which requires complete; cycle detection for correctness. * LoopInfo provides an interface for enumerating all top level loops; (e.g. those not contained in any other loop). From there, you may; walk the tree of sub-loops rooted in that top level loop. * Loops which become statically unreachable during optimization *must*; be removed from LoopInfo. If this can not be done for some reason,; then the optimization is *required* to preserve the static; reachability of the loop. .. _loop-terminology-loop-simplify:. Loop Simplify Form; ==================. The Loop Simplify Form is a canonical form that makes; several analyses and transformations simpler and more effective.; It is ensured by the LoopSimplify; (:ref:`-loop-simplify <passes-loop-simplify>`) pass and is automatically; added by the pass managers when scheduling a LoopPass.; This pass is implemented in; `LoopSimplify.h <https://llvm.org/doxygen/LoopSimplify_8h_source.html>`_.; When it is successful, the loop has:. * A preheader.; * A single backedge (which implies that there is a single latch).; * Dedicated exits. That is, no exit block for the loop; has a predecessor that is outside the loop. This implies; that all exit blocks are dominated by the loop header. .. _loop-terminology-lcssa:. Loop Closed SSA (LCSSA); ===========",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:9727,optimiz,optimization,9727,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst,1,['optimiz'],['optimization']
Performance,"positive; integer and the type of vectorization can be specified with an optional; second parameter. The default for the second parameter is 'fixed' and; refers to fixed width vectorization, whereas 'scalable' indicates the; compiler should use scalable vectors instead. Another use of vectorize_width; is ``vectorize_width(fixed|scalable)`` where the user can hint at the type; of vectorization to use without specifying the exact width. In both variants; of the pragma the vectorizer may decide to fall back on fixed width; vectorization if the target does not support scalable vectors. The interleave count is specified by ``interleave_count(_value_)``, where; _value_ is a positive integer. This is useful for specifying the optimal; width/count of the set of target architectures supported by your application. .. code-block:: c++. #pragma clang loop vectorize_width(2); #pragma clang loop interleave_count(2); for(...) {; ...; }. Specifying a width/count of 1 disables the optimization, and is equivalent to; ``vectorize(disable)`` or ``interleave(disable)``. Vector predication is enabled by ``vectorize_predicate(enable)``, for example:. .. code-block:: c++. #pragma clang loop vectorize(enable); #pragma clang loop vectorize_predicate(enable); for(...) {; ...; }. This predicates (masks) all instructions in the loop, which allows the scalar; remainder loop (the tail) to be folded into the main vectorized loop. This; might be more efficient when vector predication is efficiently supported by the; target platform. Loop Unrolling; --------------. Unrolling a loop reduces the loop control overhead and exposes more; opportunities for ILP. Loops can be fully or partially unrolled. Full unrolling; eliminates the loop and replaces it with an enumerated sequence of loop; iterations. Full unrolling is only possible if the loop trip count is known at; compile time. Partial unrolling replicates the loop body within the loop and; reduces the trip count. If ``unroll(enable)`` is specified th",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst:165457,optimiz,optimization,165457,interpreter/llvm-project/clang/docs/LanguageExtensions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst,1,['optimiz'],['optimization']
Performance,"ppen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; MTYPE NC global data.; MTYPE RW and CC memory will; never be stale due to the; memory probes. fence acq_rel - singlethread *none* *none*; - wavefront; fence acq_rel - workgroup *none* 1. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0).; - However,; since LLVM; currently has no; address space on; the fence need to; conservatively; always generate; (see comment for; previous fence).; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/; load atomic/store atomic/; atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that all; memory operations; have; completed before; performing any; following global; memory operations.; - Ensures that the; preceding; local/generic load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; acquire-fence-paired-atomic); has completed; before following; global memory; operations. This; satisfies the; requirements of; acquire.; - Ensures that all; previous memory; operations have; completed before a; following; local/generic store; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; release-fence-paired-atomic).; This satisfies the; requirements of; release.; - Must happen before; the following; buffer_inv.; - Ensures that the; acquire-fence-paired; atomic has completed; before invalidating; the; cache. Therefore; any following; locations read must; be no older than; t",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:325672,load,load,325672,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"ppen before; following; buffer_inv.; - Ensures the load; has completed; before invalidating; the cache. 3. buffer_inv sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale global data. load atomic acquire - system - global 1. buffer/global/flat_load; sc0=1 sc1=1; 2. s_waitcnt vmcnt(0). - Must happen before; following; buffer_inv.; - Ensures the load; has completed; before invalidating; the cache. 3. buffer_inv sc0=1 sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale MTYPE NC global data.; MTYPE RW and CC memory will; never be stale due to the; memory probes. load atomic acquire - agent - generic 1. flat_load sc1=1; 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL omit; lgkmcnt(0).; - Must happen before; following; buffer_inv.; - Ensures the flat_load; has completed; before invalidating; the cache. 3. buffer_inv sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. load atomic acquire - system - generic 1. flat_load sc0=1 sc1=1; 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL omit; lgkmcnt(0).; - Must happen before; the following; buffer_inv.; - Ensures the flat_load; has completed; before invalidating; the caches. 3. buffer_inv sc0=1 sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale MTYPE NC global data.; MTYPE RW and CC memory will; never be stale due to the; memory probes. atomicrmw acquire - singlethread - global 1. buffer/global/flat_atomic; - wavefront - generic; atomicrmw acquire - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; be used.*. 1. ds_atomic; at",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:298138,cache,cache,298138,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['cache'],['cache']
Performance,"pport of `require.js` and `openui5` loaders was removed. * Global hierarchy painter `JSROOT.hpainter` no longer existing, one can use `getHPainter` function:. ```javascript; import { getHPainter } from 'https://root.cern/js/7.0.0/modules/main.mjs';; let hpainter = getHPainter();; ```. * All math functions previously available via `JSROOT.Math` should be imported from `base/math.mjs` module:. ```javascript; import * as math from 'https://root.cern/js/7.0.0/modules/base/math.mjs';; ```. * Indication of batch mode `JSROOT.batch_mode` should be accessed via functions:. ```javascript; import { isBatchMode, setBatchMode } from 'https://root.cern/js/7.0.0/modules/main.mjs';; let was_batch = isBatchMode();; if (!was_batch) setBatchMode(true);; ```. * `JSROOT.extend()` function was removed, use `Object.assign()` instead. ### Migration v5 -> v6. * Main script was renamed to `JSRoot.core.js`. Old `JSRootCore.js` was deprecated and removed in v6.2. All URL parameters for main script ignored now, to load JSROOT functionality one should use `JSROOT.require` function. To create standard GUI, `JSROOT.buildGUI` function has to be used. * Instead of `JSROOT.JSONR_unref()` one can use `JSROOT.parse()`. If object is provided to `JSROOT.parse()` it just replaces all references which were introduced by `TBufferJSON::ToJSON()` method. * Instead of `JSROOT.console()` one should use `console.log()`. Instead of `JSROOT.alert()` one should use `console.error()`. * Many settings were moved from `JSROOT.gStyle` to `JSROOT.settings` object. It was done to keep only TStyle-related members in `JSROOT.gStyle`. * Basic painter classes were renamed and made public:; - `JSROOT.TBasePainter` -> `JSROOT.BasePainter`; - `JSROOT.TObjectPainter` -> `JSROOT.ObjectPainter`. * Internal `ObjectPainter.DrawingReady` api was deprecated. Draw function has to return `Promise` if object drawing postponed. As argument of returned promise object painter has to be used. * Many function names where adjusted to naming co",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/JSROOT/JSROOT.md:48751,load,load,48751,documentation/JSROOT/JSROOT.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/JSROOT/JSROOT.md,1,['load'],['load']
Performance,"pported values for the CUDA language are:. | ``cuda``. NVIDIA CUDA(tm). .. option:: -stdlib=<library>. Specify the C++ standard library to use; supported options are libstdc++ and; libc++. If not specified, platform default will be used. .. option:: -rtlib=<library>. Specify the compiler runtime library to use; supported options are libgcc and; compiler-rt. If not specified, platform default will be used. .. option:: -ansi. Same as -std=c89. .. option:: -ObjC, -ObjC++. Treat source input files as Objective-C and Object-C++ inputs respectively. .. option:: -trigraphs. Enable trigraphs. .. option:: -ffreestanding. Indicate that the file should be compiled for a freestanding, not a hosted,; environment. Note that it is assumed that a freestanding environment will; additionally provide `memcpy`, `memmove`, `memset` and `memcmp`; implementations, as these are needed for efficient codegen for many programs. .. option:: -fno-builtin. Disable special handling and optimizations of well-known library functions,; like :c:func:`strlen` and :c:func:`malloc`. .. option:: -fno-builtin-<function>. Disable special handling and optimizations for the specific library function.; For example, ``-fno-builtin-strlen`` removes any special handling for the; :c:func:`strlen` library function. .. option:: -fno-builtin-std-<function>. Disable special handling and optimizations for the specific C++ standard; library function in namespace ``std``. For example,; ``-fno-builtin-std-move_if_noexcept`` removes any special handling for the; :cpp:func:`std::move_if_noexcept` library function. For C standard library functions that the C++ standard library also provides; in namespace ``std``, use :option:`-fno-builtin-\<function\>` instead. .. option:: -fmath-errno. Indicate that math functions should be treated as updating :c:data:`errno`. .. option:: -fpascal-strings. Enable support for Pascal-style strings with ""\\pfoo"". .. option:: -fms-extensions. Enable support for Microsoft extensions. .. option::",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/CommandGuide/clang.rst:6274,optimiz,optimizations,6274,interpreter/llvm-project/clang/docs/CommandGuide/clang.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/CommandGuide/clang.rst,1,['optimiz'],['optimizations']
Performance,"pports an empty optimization list, whereas MSVC supports the arguments, ``s``,; ``g``, ``t``, and ``y``. Currently, the implementation of ``pragma optimize`` behaves; the same as ``#pragma clang optimize``. All functions; between ``off`` and ``on`` will be decorated with the ``optnone`` attribute. .. code-block:: c++. #pragma optimize("""", off); // This function will be decorated with optnone.; void f1() {}. #pragma optimize("""", on); // This function will be optimized with whatever was specified on; // the commandline.; void f2() {}. // This will warn with Clang's current implementation.; #pragma optimize(""g"", on); void f3() {}. For MSVC, an empty optimization list and ``off`` parameter will turn off; all optimizations, ``s``, ``g``, ``t``, and ``y``. An empty optimization and; ``on`` parameter will reset the optimizations to the ones specified on the; commandline. .. list-table:: Parameters (unsupported by Clang). * - Parameter; - Type of optimization; * - g; - Deprecated; * - s or t; - Short or fast sequences of machine code; * - y; - Enable frame pointers. Extensions for loop hint optimizations; ======================================. The ``#pragma clang loop`` directive is used to specify hints for optimizing the; subsequent for, while, do-while, or c++11 range-based for loop. The directive; provides options for vectorization, interleaving, predication, unrolling and; distribution. Loop hints can be specified before any loop and will be ignored if; the optimization is not safe to apply. There are loop hints that control transformations (e.g. vectorization, loop; unrolling) and there are loop hints that set transformation options (e.g.; ``vectorize_width``, ``unroll_count``). Pragmas setting transformation options; imply the transformation is enabled, as if it was enabled via the corresponding; transformation pragma (e.g. ``vectorize(enable)``). If the transformation is; disabled (e.g. ``vectorize(disable)``), that takes precedence over; transformations option prag",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst:162092,optimiz,optimization,162092,interpreter/llvm-project/clang/docs/LanguageExtensions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst,1,['optimiz'],['optimization']
Performance,"ppyy.gbl.get_data(NDATA); >>> print(d); <cppyy.LowLevelView object at 0x1068cba30>; >>> d = cppyy.ll.cast['int*'](d); >>> d.reshape((NDATA,)); >>> print(list(d)); [0, 1, 2, 3]; >>>. * **C++-style casts**: Similar to the C-style cast, there are; ``ll.static_cast`` and ``ll.reinterpret_cast``.; There should never be a reason for a ``dynamic_cast``, since that only; applies to objects, for which auto-casting will work.; The syntax is ""template-style"", just like for the C-style cast above. .. _npcasts:. `NumPy casts`; -------------. The ``cppyy.LowLevelView`` type returned for pointers to basic types,; including for ``void*``, is a simple and light-weight view on memory, given a; pointer, type, and number of elements (or unchecked, if unknown).; It only supports basic operations such as indexing and iterations, but also; the buffer protocol for integration with full-fledged functional arrays such; as NumPy`s ``ndarray``. In addition, specifically when dealing with ``void*`` returns, you can use; NumPy's low-level ``frombuffer`` interface to perform the cast.; Example:. .. code-block:: python. >>> cppyy.cppdef(""""""; ... void* create_float_array(int sz) {; ... float* pf = (float*)malloc(sizeof(float)*sz);; ... for (int i = 0; i < sz; ++i) pf[i] = 2*i;; ... return pf;; ... }""""""); ...; >>> import numpy as np; >>> NDATA = 8; >>> arr = cppyy.gbl.create_float_array(NDATA); >>> print(arr); <cppyy.LowLevelView object at 0x109f15230>; >>> arr.reshape((NDATA,)) # adjust the llv's size; >>> v = np.frombuffer(arr, dtype=np.float32, count=NDATA) # cast to float; >>> print(len(v)); 8; >>> print(v); array([ 0., 2., 4., 6., 8., 10., 12., 14.], dtype=float32); >>>. Note that NumPy will internally check the total buffer size, so if the size; you are casting *to* is larger than the size you are casting *from*, then; the number of elements set in the ``reshape`` call needs to be adjusted; accordingly. `Capsules`; ----------. It is not possible to pass proxies from cppyy through function argu",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/lowlevel.rst:5015,perform,perform,5015,bindings/pyroot/cppyy/cppyy/doc/source/lowlevel.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/lowlevel.rst,1,['perform'],['perform']
Performance,"presenting a :doc:`type identifier; <TypeMetadata>`. Overview:; """""""""""""""""". The ``llvm.type.checked.load`` intrinsic safely loads a function pointer from a; virtual table pointer using type metadata. This intrinsic is used to implement; control flow integrity in conjunction with virtual call optimization. The; virtual call optimization pass will optimize away ``llvm.type.checked.load``; intrinsics associated with devirtualized calls, thereby removing the type; check in cases where it is not needed to enforce the control flow integrity; constraint. If the given pointer is associated with a type metadata identifier, this; function returns true as the second element of its return value. (Note that; the function may also return true if the given pointer is not associated; with a type metadata identifier.) If the function's return value's second; element is true, the following rules apply to the first element:. - If the given pointer is associated with the given type metadata identifier,; it is the function pointer loaded from the given byte offset from the given; pointer. - If the given pointer is not associated with the given type metadata; identifier, it is one of the following (the choice of which is unspecified):. 1. The function pointer that would have been loaded from an arbitrarily chosen; (through an unspecified mechanism) pointer associated with the type; metadata. 2. If the function has a non-void return type, a pointer to a function that; returns an unspecified value without causing side effects. If the function's return value's second element is false, the value of the; first element is undefined. .. _type.checked.load.relative:. '``llvm.type.checked.load.relative``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare {ptr, i1} @llvm.type.checked.load.relative(ptr %ptr, i32 %offset, metadata %type) argmemonly nounwind readonly. Overview:; """""""""""""""""". The ``llvm.type.checked.load.relative`` intrinsic loads a relative pointer",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:939144,load,loaded,939144,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['loaded']
Performance,"pressed bitstream as `LLVM's bitcode file format; <https://llvm.org/docs/BitCodeFormat.html>`_. Clang's AST files are loaded ""lazily"" from disk. When an AST file is initially; loaded, Clang reads only a small amount of data from the AST file to establish; where certain important data structures are stored. The amount of data read in; this initial load is independent of the size of the AST file, such that a; larger AST file does not lead to longer AST load times. The actual header data; in the AST file --- macros, functions, variables, types, etc. --- is loaded; only when it is referenced from the user's code, at which point only that; entity (and those entities it depends on) are deserialized from the AST file.; With this approach, the cost of using an AST file for a translation unit is; proportional to the amount of code actually used from the AST file, rather than; being proportional to the size of the AST file itself. When given the `-print-stats` option, Clang produces statistics; describing how much of the AST file was actually loaded from disk. For a; simple ""Hello, World!"" program that includes the Apple ``Cocoa.h`` header; (which is built as a precompiled header), this option illustrates how little of; the actual precompiled header is required:. .. code-block:: none. *** AST File Statistics:; 895/39981 source location entries read (2.238563%); 19/15315 types read (0.124061%); 20/82685 declarations read (0.024188%); 154/58070 identifiers read (0.265197%); 0/7260 selectors read (0.000000%); 0/30842 statements read (0.000000%); 4/8400 macros read (0.047619%); 1/4995 lexical declcontexts read (0.020020%); 0/4413 visible declcontexts read (0.000000%); 0/7230 method pool entries read (0.000000%); 0 method pool misses. For this small program, only a tiny fraction of the source locations, types,; declarations, identifiers, and macros were actually deserialized from the; precompiled header. These statistics can be useful to determine whether the; AST file implementat",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/PCHInternals.rst:4342,load,loaded,4342,interpreter/llvm-project/clang/docs/PCHInternals.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/PCHInternals.rst,1,['load'],['loaded']
Performance,"pressure and generated code size to; select the interleaving count. Vectorization is enabled by ``vectorize(enable)`` and interleaving is enabled; by ``interleave(enable)``. This is useful when compiling with ``-Os`` to; manually enable vectorization or interleaving. .. code-block:: c++. #pragma clang loop vectorize(enable); #pragma clang loop interleave(enable); for(...) {; ...; }. The vector width is specified by; ``vectorize_width(_value_[, fixed|scalable])``, where _value_ is a positive; integer and the type of vectorization can be specified with an optional; second parameter. The default for the second parameter is 'fixed' and; refers to fixed width vectorization, whereas 'scalable' indicates the; compiler should use scalable vectors instead. Another use of vectorize_width; is ``vectorize_width(fixed|scalable)`` where the user can hint at the type; of vectorization to use without specifying the exact width. In both variants; of the pragma the vectorizer may decide to fall back on fixed width; vectorization if the target does not support scalable vectors. The interleave count is specified by ``interleave_count(_value_)``, where; _value_ is a positive integer. This is useful for specifying the optimal; width/count of the set of target architectures supported by your application. .. code-block:: c++. #pragma clang loop vectorize_width(2); #pragma clang loop interleave_count(2); for(...) {; ...; }. Specifying a width/count of 1 disables the optimization, and is equivalent to; ``vectorize(disable)`` or ``interleave(disable)``. Vector predication is enabled by ``vectorize_predicate(enable)``, for example:. .. code-block:: c++. #pragma clang loop vectorize(enable); #pragma clang loop vectorize_predicate(enable); for(...) {; ...; }. This predicates (masks) all instructions in the loop, which allows the scalar; remainder loop (the tail) to be folded into the main vectorized loop. This; might be more efficient when vector predication is efficiently supported by the; targ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst:165049,scalab,scalable,165049,interpreter/llvm-project/clang/docs/LanguageExtensions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst,1,['scalab'],['scalable']
Performance,"preted as describing a sequence of pointers and their corresponding; base pointers. If the Location is of size N x sizeof(pointer), then; there will be N records of one pointer each contained within the Location.; Both Locations in a pair can be assumed to be of the same size. Note that the Locations used in each section may describe the same; physical location. e.g. A stack slot may appear as a deopt location,; a gc base pointer, and a gc derived pointer. The LiveOut section of the StkMapRecord will be empty for a statepoint; record. Safepoint Semantics & Verification; ==================================. The fundamental correctness property for the compiled code's; correctness w.r.t. the garbage collector is a dynamic one. It must be; the case that there is no dynamic trace such that an operation; involving a potentially relocated pointer is observably-after a; safepoint which could relocate it. 'observably-after' is this usage; means that an outside observer could observe this sequence of events; in a way which precludes the operation being performed before the; safepoint. To understand why this 'observable-after' property is required,; consider a null comparison performed on the original copy of a; relocated pointer. Assuming that control flow follows the safepoint,; there is no way to observe externally whether the null comparison is; performed before or after the safepoint. (Remember, the original; Value is unmodified by the safepoint.) The compiler is free to make; either scheduling choice. The actual correctness property implemented is slightly stronger than; this. We require that there be no *static path* on which a; potentially relocated pointer is 'observably-after' it may have been; relocated. This is slightly stronger than is strictly necessary (and; thus may disallow some otherwise valid programs), but greatly; simplifies reasoning about correctness of the compiled code. By construction, this property will be upheld by the optimizer if; correctly establ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Statepoints.rst:23781,perform,performed,23781,interpreter/llvm-project/llvm/docs/Statepoints.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Statepoints.rst,1,['perform'],['performed']
Performance,"previous pass to be executed (an analysis for example),; it can use one of these methods to arrange for it to be run before your pass.; LLVM has many different types of analyses and passes that can be required,; spanning the range from ``DominatorSet`` to ``BreakCriticalEdges``. Requiring; ``BreakCriticalEdges``, for example, guarantees that there will be no critical; edges in the CFG when your pass has been run. Some analyses chain to other analyses to do their job. For example, an; `AliasAnalysis <AliasAnalysis>` implementation is required to :ref:`chain; <aliasanalysis-chaining>` to other alias analysis passes. In cases where; analyses chain, the ``addRequiredTransitive`` method should be used instead of; the ``addRequired`` method. This informs the ``PassManager`` that the; transitively required pass should be alive as long as the requiring pass is. The ``AnalysisUsage::addPreserved<>`` method; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. One of the jobs of the ``PassManager`` is to optimize how and when analyses are; run. In particular, it attempts to avoid recomputing data unless it needs to.; For this reason, passes are allowed to declare that they preserve (i.e., they; don't invalidate) an existing analysis if it's available. For example, a; simple constant folding pass would not modify the CFG, so it can't possibly; affect the results of dominator analysis. By default, all passes are assumed; to invalidate all others. The ``AnalysisUsage`` class provides several methods which are useful in; certain circumstances that are related to ``addPreserved``. In particular, the; ``setPreservesAll`` method can be called to indicate that the pass does not; modify the LLVM program at all (which is true for analyses), and the; ``setPreservesCFG`` method can be used by transformations that change; instructions in the program but do not modify the CFG or terminator; instructions. ``addPreserved`` is particularly useful for transformations like; ``BreakCriticalEdges``. Thi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst:31265,optimiz,optimize,31265,interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst,1,['optimiz'],['optimize']
Performance,"printed if there are problems such as syntax errors. `Eval(const char* expr) `- the argument is a string of Python code that; is evaluated as an expression. The result of the expression is returned,; if it is either a builtin type (int, long, float, double, and; `const char*` are supported), a Python type that can cross, or a ROOT; type. If a ROOT type is returned, an explicit cast to void\* is needed; to assign the return value to a local pointer (which may have a; different type), whereas builtin types will be cast implicitly, if; possible, to the type of the local variable to which they are assigned. `Bind(TObject* obj,const char* label)` - transfer a ROOT object from the; Cling to the Python interpreter, where it will be referenced with a; variable called ""`label`"". `Prompt()` - Transfer the interactive prompt to Python. With the ROOT v4.00/06 and later, the **`TPython`** class will be loaded; automatically on use, for older editions, the `libPyROOT.so` needs to be; loaded first with `gSystem->Load()` before use. Refer back to the other; example of the use of **`TPython`** that was given in ""Access to Python; from ROOT"". To show in detail how Python access can be used, an example Python; module is needed, as follows:. ``` {.cpp}; print('creating class MyPyClass ... '); class MyPyClass:; def __init__(self):; print('in MyPyClass.__init__'); self._browser = None; def gime(self, what):; return what; ```. This module can now be loaded into a Cling session, the class used to; instantiate objects, and their member functions called for showing how; different types can cross:. ``` {.cpp}; root[] TPython::LoadMacro(""MyPyClass.py"");; creating class MyPyClass ...; root[] MyPyClass m;; in MyPyClass.__init__; root[] char* s = m.gime(""aap"");; root[] s; (char* 0x41ee7754)""aap""; ```. Note that the `LoadMacro()` call makes the class automatically; available, such that it can be used directly. Otherwise, a; `gROOT->GetClass()` call is required first. #### Callbacks. The simplest w",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/PythonRuby.md:17359,load,loaded,17359,documentation/users-guide/PythonRuby.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/PythonRuby.md,1,['load'],['loaded']
Performance,"processes and threads. ### Process Properties. A process in a multi-threaded system is the changeable entity. It must; be considered as an execution frame. It has all traditional process; attributes, such as:. - Process ID, process group ID, user ID, and group ID. - Environment. - Working directory. A process also provides a common address space and common system; resources:. - File descriptors. - Signal actions. - Shared libraries. - Inter-process communication tools (such as message queues, pipes,; semaphores, or shared memory). ### Thread Properties. A thread is the schedulable entity. It has only those properties that; are required to ensure its independent flow of control. These include; the following properties:. - Stack. - Scheduling properties (such as policy or priority). - Set of pending and blocked signals. - Some thread-specific data (TSD). An example of thread-specific data is the error indicator, `errno`. In; multi-threaded systems, `errno` is no longer a global variable, but; usually a subroutine returning a thread-specific `errno` value. Some; other systems may provide other implementations of `errno`. With respect; to ROOT, a thread specific data is for example the ***`gPad`*** pointer,; which is treated in a different way, whether it is accessed from any; thread or the main thread. Threads within a process must not be considered as a group of processes; (even though in Linux each thread receives an own process id, so that it; can be scheduled by the kernel scheduler). All threads share the same; address space. This means that two pointers having the same value in two; threads refer to the same data. Also, if any thread changes one of the; shared system resources, all threads within the process are affected.; For example, if a thread closes a file, the file is closed for all; threads. ### The Initial Thread. When a process is created, one thread is automatically created. This; thread is called the initial thread or the main thread. The initial; threa",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Threads.md:1508,multi-thread,multi-threaded,1508,documentation/users-guide/Threads.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Threads.md,1,['multi-thread'],['multi-threaded']
Performance,"profile-instrument-path=/path/to/file_pattern.profraw``. .. option:: -fcs-profile-generate[=<dirname>]. The ``-fcs-profile-generate`` and ``-fcs-profile-generate=`` flags will use; the same instrumentation method, and generate the same profile as in the; ``-fprofile-generate`` and ``-fprofile-generate=`` flags. The difference is; that the instrumentation is performed after inlining so that the resulted; profile has a better context sensitive information. They cannot be used; together with ``-fprofile-generate`` and ``-fprofile-generate=`` flags.; They are typically used in conjunction with ``-fprofile-use`` flag.; The profile generated by ``-fcs-profile-generate`` and ``-fprofile-generate``; can be merged by llvm-profdata. A use example:. .. code-block:: console. $ clang++ -O2 -fprofile-generate=yyy/zzz code.cc -o code; $ ./code; $ llvm-profdata merge -output=code.profdata yyy/zzz/. The first few steps are the same as that in ``-fprofile-generate``; compilation. Then perform a second round of instrumentation. .. code-block:: console. $ clang++ -O2 -fprofile-use=code.profdata -fcs-profile-generate=sss/ttt \; -o cs_code; $ ./cs_code; $ llvm-profdata merge -output=cs_code.profdata sss/ttt code.profdata. The resulted ``cs_code.prodata`` combines ``code.profdata`` and the profile; generated from binary ``cs_code``. Profile ``cs_code.profata`` can be used by; ``-fprofile-use`` compilation. .. code-block:: console. $ clang++ -O2 -fprofile-use=cs_code.profdata. The above command will read both profiles to the compiler at the identical; point of instrumentations. .. option:: -fprofile-use[=<pathname>]. Without any other arguments, ``-fprofile-use`` behaves identically to; ``-fprofile-instr-use``. Otherwise, if ``pathname`` is the full path to a; profile file, it reads from that file. If ``pathname`` is a directory name,; it reads from ``pathname/default.profdata``. .. option:: -fprofile-update[=<method>]. Unless ``-fsanitize=thread`` is specified, the default is ``single``, ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:109698,perform,perform,109698,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,1,['perform'],['perform']
Performance,"profiles for code coverage, sample-based profiles are too; coarse-grained for code coverage purposes; it would yield poor results. 4. Sampling profiles must be generated by an external tool. The profile; generated by that tool must then be converted into a format that can be read; by LLVM. The section on sampling profilers describes one of the supported; sampling profile formats. Using Sampling Profilers; ^^^^^^^^^^^^^^^^^^^^^^^^. Sampling profilers are used to collect runtime information, such as; hardware counters, while your application executes. They are typically; very efficient and do not incur a large runtime overhead. The; sample data collected by the profiler can be used during compilation; to determine what the most executed areas of the code are. Using the data from a sample profiler requires some changes in the way; a program is built. Before the compiler can use profiling information,; the code needs to execute under the profiler. The following is the; usual build cycle when using sample profilers for optimization:. 1. Build the code with source line table information. You can use all the; usual build flags that you always build your application with. The only; requirement is that you add ``-gline-tables-only`` or ``-g`` to the; command line. This is important for the profiler to be able to map; instructions back to source line locations. .. code-block:: console. $ clang++ -O2 -gline-tables-only code.cc -o code. 2. Run the executable under a sampling profiler. The specific profiler; you use does not really matter, as long as its output can be converted; into the format that the LLVM optimizer understands. Currently, there; exists a conversion tool for the Linux Perf profiler; (https://perf.wiki.kernel.org/), so these examples assume that you; are using Linux Perf to profile your code. .. code-block:: console. $ perf record -b ./code. Note the use of the ``-b`` flag. This tells Perf to use the Last Branch; Record (LBR) to record call chains. While this is",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:92677,optimiz,optimization,92677,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,1,['optimiz'],['optimization']
Performance,"profitable to emit these directly in the language frontend. This item; explicitly includes the use of the :ref:`overflow intrinsics <int_overflow>`. #. Avoid using the :ref:`assume intrinsic <int_assume>` until you've; established that a) there's no other way to express the given fact and b); that fact is critical for optimization purposes. Assumes are a great; prototyping mechanism, but they can have negative effects on both compile; time and optimization effectiveness. The former is fixable with enough; effort, but the later is fairly fundamental to their designed purpose. Describing Language Specific Properties; =======================================. When translating a source language to LLVM, finding ways to express concepts; and guarantees available in your source language which are not natively; provided by LLVM IR will greatly improve LLVM's ability to optimize your code.; As an example, C/C++'s ability to mark every add as ""no signed wrap (nsw)"" goes; a long way to assisting the optimizer in reasoning about loop induction; variables and thus generating more optimal code for loops. The LLVM LangRef includes a number of mechanisms for annotating the IR with; additional semantic information. It is *strongly* recommended that you become; highly familiar with this document. The list below is intended to highlight a; couple of items of particular interest, but is by no means exhaustive. Restricted Operation Semantics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; #. Add nsw/nuw flags as appropriate. Reasoning about overflow is; generally hard for an optimizer so providing these facts from the frontend; can be very impactful. #. Use fast-math flags on floating point operations if legal. If you don't; need strict IEEE floating point semantics, there are a number of additional; optimizations that can be performed. This can be highly impactful for; floating point intensive computations. Describing Aliasing Properties; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. #. Add noalias/align/dereferen",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Frontend/PerformanceTips.rst:10219,optimiz,optimizer,10219,interpreter/llvm-project/llvm/docs/Frontend/PerformanceTips.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Frontend/PerformanceTips.rst,1,['optimiz'],['optimizer']
Performance,"proto-fuzzer clang-proto-to-cxx. This directory also contains a Dockerfile which sets up all required; dependencies and builds the fuzzers. ============================; Running clang-proto-fuzzer; ============================; bin/clang-proto-fuzzer CORPUS_DIR. Arguments can be specified after -ignore_remaining_args=1 to modify the compiler; invocation. For example, the following command line will fuzz LLVM with a; custom optimization level and target triple:; bin/clang-proto-fuzzer CORPUS_DIR -ignore_remaining_args=1 -O3 -triple \; arm64apple-ios9. To translate a clang-proto-fuzzer corpus output to C++:; bin/clang-proto-to-cxx CORPUS_OUTPUT_FILE. ===================; llvm-proto-fuzzer; ===================; Like, clang-proto-fuzzer, llvm-proto-fuzzer is also a protobuf-mutator based; fuzzer. It receives as input a cxx_loop_proto which it then converts into a; string of valid LLVM IR: a function with either a single loop or two nested; loops. It then creates a new string of IR by running optimization passes over; the original IR. Currently, it only runs a loop-vectorize pass but more passes; can easily be added to the fuzzer. Once there are two versions of the input; function (optimized and not), llvm-proto-fuzzer uses LLVM's JIT Engine to; compile both functions. Lastly, it runs both functions on a suite of inputs and; checks that both functions behave the same on all inputs. In this way,; llvm-proto-fuzzer can find not only compiler crashes, but also miscompiles; originating from LLVM's optimization passes. llvm-proto-fuzzer is built very similarly to clang-proto-fuzzer. You can run the; fuzzer with the following command:; bin/clang-llvm-proto-fuzzer CORPUS_DIR. To translate a cxx_loop_proto file into LLVM IR do:; bin/clang-loop-proto-to-llvm CORPUS_OUTPUT_FILE; To translate a cxx_loop_proto file into C++ do:; bin/clang-loop-proto-to-cxx CORPUS_OUTPUT_FILE. Note: To get a higher number of executions per second with llvm-proto-fuzzer it; helps to build it without AS",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/clang-fuzzer/README.txt:4442,optimiz,optimization,4442,interpreter/llvm-project/clang/tools/clang-fuzzer/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/clang-fuzzer/README.txt,1,['optimiz'],['optimization']
Performance,"ps 2-5 above. ### Reading Case; The reverse process is performed on reading (e.g. `RNTupleReader::LoadEntry()`, `RNTupleView` call operator). By default, the page source uses an `RClusterPool` to asynchronously read-ahead data.; When a page of a certain cluster is required, the cluster pool reads pages of _active_ columns.; For instance, if only certain fields are used (e.g., through an imposed model), only the pages of columns connected to those fields are read.; Columns can be dynamically added (e.g. during event iteration, a new field view is created in a reader).; The cluster pool reads ahead a limited number of clusters given by the _cluster bunch size_ option (default = 1).; The read-ahead uses vector reads.; For the file backend, it additionally coalesces close read requests and uses uring reads when available. The page source can be restricted to a certain entry range.; This allows for optimizing the page lists that are being read.; Additionally, it allows for optimizing the cluster pool to not read-ahead beyond the limits. #### Late model extension; Reading an RNTuple with an extended model is transparent -- i.e., no additional interface calls are required.; Internally, columns that were created as part of late model extension will have synthesized zero-initialized column ranges for the clusters that were already written before the model was extended.; In addition, pages made up of 0x00 bytes are synthesized for deferred columns in the clusters that were already (partially) filled before the model was extended. Storage Backends; ----------------. Support for storage backends is implemented through derived classes of `RPageSink` and `RPageSource`.; The `RPage{Sink,Source}File` class provides a storage backend for RNTuple data in ROOT files, local or remote.; The `RPage{Sink,Source}Daos` class provides a storage backend for RNTuple data in the DAOS object store. Every new storage backend needs to define; 1) The RNTuple embedding: how are RNTuple data blobs st",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/ntuple/v7/doc/Architecture.md:22877,optimiz,optimizing,22877,tree/ntuple/v7/doc/Architecture.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/ntuple/v7/doc/Architecture.md,1,['optimiz'],['optimizing']
Performance,"ps developed; against each. Compile Flags; =============. LLVM runs much more quickly when it's optimized and assertions are removed.; However, such a build is currently incompatible with users who build without; defining ``NDEBUG``, and the lack of assertions makes it hard to debug problems; in user code. We recommend allowing users to install both optimized and debug; versions of LLVM in parallel. The following configure flags are relevant:. ``--disable-assertions``; Builds LLVM with ``NDEBUG`` defined. Changes the LLVM ABI. Also available; by setting ``DISABLE_ASSERTIONS=0|1`` in ``make``'s environment. This; defaults to enabled regardless of the optimization setting, but it slows; things down. ``--enable-debug-symbols``; Builds LLVM with ``-g``. Also available by setting ``DEBUG_SYMBOLS=0|1`` in; ``make``'s environment. This defaults to disabled when optimizing, so you; should turn it back on to let users debug their programs. ``--enable-optimized``; (For git checkouts) Builds LLVM with ``-O2`` and, by default, turns off; debug symbols. Also available by setting ``ENABLE_OPTIMIZED=0|1`` in; ``make``'s environment. This defaults to enabled when not in a; checkout. C++ Features; ============. RTTI; LLVM disables RTTI by default. Add ``REQUIRES_RTTI=1`` to your environment; while running ``make`` to re-enable it. This will allow users to build with; RTTI enabled and still inherit from LLVM classes. Shared Library; ==============. Configure with ``--enable-shared`` to build; ``libLLVM-<major>.<minor>.(so|dylib)`` and link the tools against it. This; saves lots of binary size at the cost of some startup time. Dependencies; ============. ``--enable-libffi``; Depend on `libffi <http://sources.redhat.com/libffi/>`_ to allow the LLVM; interpreter to call external functions. ``--with-oprofile``. Depend on `libopagent; <http://oprofile.sourceforge.net/doc/devel/index.html>`_ (>=version 0.9.4); to let the LLVM JIT tell oprofile about function addresses and line; numbers.; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Packaging.rst:1538,optimiz,optimized,1538,interpreter/llvm-project/llvm/docs/Packaging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Packaging.rst,1,['optimiz'],['optimized']
Performance,"ptimization levels used by; :program:`clang`. .. option:: -mtriple=<target triple>. Override the target triple specified in the input file with the specified; string. .. option:: -march=<arch>. Specify the architecture for which to generate assembly, overriding the target; encoded in the input file. See the output of ``llc -help`` for a list of; valid architectures. By default this is inferred from the target triple or; autodetected to the current architecture. .. option:: -mcpu=<cpuname>. Specify a specific chip in the current architecture to generate code for.; By default this is inferred from the target triple and autodetected to; the current architecture. For a list of available CPUs, use:. .. code-block:: none. llvm-as < /dev/null | llc -march=xyz -mcpu=help. .. option:: -filetype=<output file type>. Specify what kind of output ``llc`` should generated. Options are: ``asm``; for textual assembly ( ``'.s'``), ``obj`` for native object files (``'.o'``); and ``null`` for not emitting anything (for performance testing). Note that not all targets support all options. .. option:: -mattr=a1,+a2,-a3,... Override or control specific attributes of the target, such as whether SIMD; operations are enabled or not. The default set of attributes is set by the; current CPU. For a list of available attributes, use:. .. code-block:: none. llvm-as < /dev/null | llc -march=xyz -mattr=help. .. option:: --frame-pointer. Specify effect of frame pointer elimination optimization (all,non-leaf,none). .. option:: --disable-excess-fp-precision. Disable optimizations that may produce excess precision for floating point.; Note that this option can dramatically slow down code on some systems; (e.g. X86). .. option:: --enable-no-infs-fp-math. Enable optimizations that assume no Inf values. .. option:: --enable-no-nans-fp-math. Enable optimizations that assume no NAN values. .. option:: --enable-no-signed-zeros-fp-math. Enable FP math optimizations that assume the sign of 0 is insignificant. .",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llc.rst:2698,perform,performance,2698,interpreter/llvm-project/llvm/docs/CommandGuide/llc.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llc.rst,1,['perform'],['performance']
Performance,"ptimization-record <opt_fsave-optimization-record>`. On Darwin platforms, this is incompatible with passing multiple; ``-arch <arch>`` options. .. option:: -foptimization-record-passes. Only include passes which match a specified regular expression. When optimization reports are being output (see; :ref:`-fsave-optimization-record <opt_fsave-optimization-record>`), this; option controls the passes that will be included in the final report. If this option is not used, all the passes are included in the optimization; record. .. _opt_fdiagnostics-show-hotness:. .. option:: -f[no-]diagnostics-show-hotness. Enable profile hotness information in diagnostic line. This option controls whether Clang prints the profile hotness associated; with diagnostics in the presence of profile-guided optimization information.; This is currently supported with optimization remarks (see; :ref:`Options to Emit Optimization Reports <rpass>`). The hotness information; allows users to focus on the hot optimization remarks that are likely to be; more relevant for run-time performance. For example, in this output, the block containing the callsite of `foo` was; executed 3000 times according to the profile data:. ::. s.c:7:10: remark: foo inlined into bar (hotness: 3000) [-Rpass-analysis=inline]; sum += foo(x, x - 2);; ^. This option is implied when; :ref:`-fsave-optimization-record <opt_fsave-optimization-record>` is used.; Otherwise, it defaults to off. .. option:: -fdiagnostics-hotness-threshold. Prevent optimization remarks from being output if they do not have at least; this hotness value. This option, which defaults to zero, controls the minimum hotness an; optimization remark would need in order to be output by Clang. This is; currently supported with optimization remarks (see :ref:`Options to Emit; Optimization Reports <rpass>`) when profile hotness information in; diagnostics is enabled (see; :ref:`-fdiagnostics-show-hotness <opt_fdiagnostics-show-hotness>`). .. _opt_fdiagnostics-fixit-in",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:14191,optimiz,optimization,14191,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,2,"['optimiz', 'perform']","['optimization', 'performance']"
Performance,"ptimization:; *optimizeModule*. This function takes the module to be transformed as input (as; a ThreadSafeModule) along with a reference to a reference to a new class:; ``MaterializationResponsibility``. The MaterializationResponsibility argument; can be used to query JIT state for the module being transformed, such as the set; of definitions in the module that JIT'd code is actively trying to call/access.; For now we will ignore this argument and use a standard optimization; pipeline. To do this we set up a FunctionPassManager, add some passes to it, run; it over every function in the module, and then return the mutated module. The; specific optimizations are the same ones used in `Chapter 4 <LangImpl04.html>`_; of the ""Implementing a language with LLVM"" tutorial series. Readers may visit; that chapter for a more in-depth discussion of these, and of IR optimization in; general. And that's it in terms of changes to KaleidoscopeJIT: When a module is added via; addModule the OptimizeLayer will call our optimizeModule function before passing; the transformed module on to the CompileLayer below. Of course, we could have; called optimizeModule directly in our addModule function and not gone to the; bother of using the IRTransformLayer, but doing so gives us another opportunity; to see how layers compose. It also provides a neat entry point to the *layer*; concept itself, because IRTransformLayer is one of the simplest layers that; can be implemented. .. code-block:: c++. // From IRTransformLayer.h:; class IRTransformLayer : public IRLayer {; public:; using TransformFunction = std::function<Expected<ThreadSafeModule>(; ThreadSafeModule, const MaterializationResponsibility &R)>;. IRTransformLayer(ExecutionSession &ES, IRLayer &BaseLayer,; TransformFunction Transform = identityTransform);. void setTransform(TransformFunction Transform) {; this->Transform = std::move(Transform);; }. static ThreadSafeModule; identityTransform(ThreadSafeModule TSM,; const MaterializationRespon",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:6000,optimiz,optimizeModule,6000,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,1,['optimiz'],['optimizeModule']
Performance,"ptimizations are usually not interesting. Notes for code generation; This operation has Acquire and Release semantics; see the sections on Acquire; and Release. SequentiallyConsistent; ----------------------. SequentiallyConsistent (``seq_cst`` in IR) provides Acquire semantics for loads; and Release semantics for stores. Additionally, it guarantees that a total; ordering exists between all SequentiallyConsistent operations. Relevant standard; This corresponds to the C++/C ``memory_order_seq_cst``, Java volatile, and; the gcc-compatible ``__sync_*`` builtins which do not specify otherwise. Notes for frontends; If a frontend is exposing atomic operations, these are much easier to reason; about for the programmer than other kinds of operations, and using them is; generally a practical performance tradeoff. Notes for optimizers; Optimizers not aware of atomics can treat this like a nothrow call. For; SequentiallyConsistent loads and stores, the same reorderings are allowed as; for Acquire loads and Release stores, except that SequentiallyConsistent; operations may not be reordered. Notes for code generation; SequentiallyConsistent loads minimally require the same barriers as Acquire; operations and SequentiallyConsistent stores require Release; barriers. Additionally, the code generator must enforce ordering between; SequentiallyConsistent stores followed by SequentiallyConsistent loads. This; is usually done by emitting either a full fence before the loads or a full; fence after the stores; which is preferred varies by architecture. Atomics and IR optimization; ===========================. Predicates for optimizer writers to query:. * ``isSimple()``: A load or store which is not volatile or atomic. This is; what, for example, memcpyopt would check for operations it might transform. * ``isUnordered()``: A load or store which is not volatile and at most; Unordered. This would be checked, for example, by LICM before hoisting an; operation. * ``mayReadFromMemory()``/``may",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst:15147,load,loads,15147,interpreter/llvm-project/llvm/docs/Atomics.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst,2,['load'],['loads']
Performance,"ptimizeModule),; DL(std::move(DL)), Mangle(ES, this->DL),; Ctx(std::make_unique<LLVMContext>()) {; ES.getMainJITDylib().addGenerator(; cantFail(DynamicLibrarySearchGenerator::GetForCurrentProcess(DL.getGlobalPrefix())));; }. Our extended KaleidoscopeJIT class starts out the same as it did in Chapter 1,; but after the CompileLayer we introduce a new member, TransformLayer, which sits; on top of our CompileLayer. We initialize our OptimizeLayer with a reference to; the ExecutionSession and output layer (standard practice for layers), along with; a *transform function*. For our transform function we supply our classes; optimizeModule static method. .. code-block:: c++. // ...; return cantFail(OptimizeLayer.addModule(std::move(M),; std::move(Resolver)));; // ... Next we need to update our addModule method to replace the call to; ``CompileLayer::add`` with a call to ``OptimizeLayer::add`` instead. .. code-block:: c++. static Expected<ThreadSafeModule>; optimizeModule(ThreadSafeModule M, const MaterializationResponsibility &R) {; // Create a function pass manager.; auto FPM = std::make_unique<legacy::FunctionPassManager>(M.get());. // Add some optimizations.; FPM->add(createInstructionCombiningPass());; FPM->add(createReassociatePass());; FPM->add(createGVNPass());; FPM->add(createCFGSimplificationPass());; FPM->doInitialization();. // Run the optimizations over all functions in the module being added to; // the JIT.; for (auto &F : *M); FPM->run(F);. return M;; }. At the bottom of our JIT we add a private method to do the actual optimization:; *optimizeModule*. This function takes the module to be transformed as input (as; a ThreadSafeModule) along with a reference to a reference to a new class:; ``MaterializationResponsibility``. The MaterializationResponsibility argument; can be used to query JIT state for the module being transformed, such as the set; of definitions in the module that JIT'd code is actively trying to call/access.; For now we will ignore this argument ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:4394,optimiz,optimizeModule,4394,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,1,['optimiz'],['optimizeModule']
Performance,"ptimizer writers to query:. * ``isSimple()``: A load or store which is not volatile or atomic. This is; what, for example, memcpyopt would check for operations it might transform. * ``isUnordered()``: A load or store which is not volatile and at most; Unordered. This would be checked, for example, by LICM before hoisting an; operation. * ``mayReadFromMemory()``/``mayWriteToMemory()``: Existing predicate, but note; that they return true for any operation which is volatile or at least; Monotonic. * ``isStrongerThan`` / ``isAtLeastOrStrongerThan``: These are predicates on; orderings. They can be useful for passes that are aware of atomics, for; example to do DSE across a single atomic access, but not across a; release-acquire pair (see MemoryDependencyAnalysis for an example of this). * Alias analysis: Note that AA will return ModRef for anything Acquire or; Release, and for the address accessed by any Monotonic operation. To support optimizing around atomic operations, make sure you are using the; right predicates; everything should work if that is done. If your pass should; optimize some atomic operations (Unordered operations in particular), make sure; it doesn't replace an atomic load or store with a non-atomic operation. Some examples of how optimizations interact with various kinds of atomic; operations:. * ``memcpyopt``: An atomic operation cannot be optimized into part of a; memcpy/memset, including unordered loads/stores. It can pull operations; across some atomic operations. * LICM: Unordered loads/stores can be moved out of a loop. It just treats; monotonic operations like a read+write to a memory location, and anything; stricter than that like a nothrow call. * DSE: Unordered stores can be DSE'ed like normal stores. Monotonic stores can; be DSE'ed in some cases, but it's tricky to reason about, and not especially; important. It is possible in some case for DSE to operate across a stronger; atomic operation, but it is fairly tricky. DSE delegates this reasoni",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst:16789,optimiz,optimizing,16789,interpreter/llvm-project/llvm/docs/Atomics.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst,1,['optimiz'],['optimizing']
Performance,"ption BL. The DWARF expression is ill-formed if BS or BO are negative values. *rbss(L)* is the minimum remaining bit storage size of L which is defined as; follows. LS is the location storage and LO is the location bit offset; specified by a single location description SL of L. The remaining bit; storage size RBSS of SL is the bit size of LS minus LO. *rbss(L)* is the; minimum RBSS of each single location description SL of L. The DWARF expression is ill-formed if *rbss(BL)* is less than BO plus BS. If BS is 0, then the operation pushes BL. If BO is 0 and BS equals *rbss(BL)*, then the operation pushes OL. Otherwise, the operation is equivalent to performing the following steps to; push a composite location description. *The composite location description is conceptually the base location; description BL with the overlay location description OL positioned as an; overlay starting at the overlay offset BO and covering overlay bit size BS.*. 1. If BO is not 0 then push BL followed by performing the ``DW_OP_bit_piece; BO, 0`` operation.; 2. Push OL followed by performing the ``DW_OP_bit_piece BS, 0`` operation.; 3. If *rbss(BL)* is greater than BO plus BS, push BL followed by performing; the ``DW_OP_bit_piece (rbss(BL) - BO - BS), (BO + BS)`` operation.; 4. Perform the ``DW_OP_LLVM_piece_end`` operation. .. _amdgpu-dwarf-location-list-expressions:. A.2.5.5 DWARF Location List Expressions; +++++++++++++++++++++++++++++++++++++++. .. note::. This section replaces DWARF Version 5 section 2.6.2. *To meet the needs of recent computer architectures and optimization techniques,; debugging information must be able to describe the location of an object whose; location changes over the object’s lifetime, and may reside at multiple; locations during parts of an object's lifetime. Location list expressions are; used in place of operation expressions whenever the object whose location is; being described has these requirements.*. A location list expression consists of a series of loca",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst:141979,perform,performing,141979,interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst,1,['perform'],['performing']
Performance,"ption:: --analysis-clustering=[dbscan,naive]. Specify the clustering algorithm to use. By default DBSCAN will be used.; Naive clustering algorithm is better for doing further work on the; `-analysis-inconsistencies-output-file=` output, it will create one cluster; per opcode, and check that the cluster is stable (all points are neighbours). .. option:: --analysis-numpoints=<dbscan numPoints parameter>. Specify the numPoints parameters to be used for DBSCAN clustering; (`analysis` mode, DBSCAN only). .. option:: --analysis-clustering-epsilon=<dbscan epsilon parameter>. Specify the epsilon parameter used for clustering of benchmark points; (`analysis` mode). .. option:: --analysis-inconsistency-epsilon=<epsilon>. Specify the epsilon parameter used for detection of when the cluster; is different from the LLVM schedule profile values (`analysis` mode). .. option:: --analysis-display-unstable-clusters. If there is more than one benchmark for an opcode, said benchmarks may end up; not being clustered into the same cluster if the measured performance; characteristics are different. by default all such opcodes are filtered out.; This flag will instead show only such unstable opcodes. .. option:: --ignore-invalid-sched-class=false. If set, ignore instructions that do not have a sched class (class idx = 0). .. option:: --mtriple=<triple name>. Target triple. See `-version` for available targets. .. option:: --mcpu=<cpu name>. If set, measure the cpu characteristics using the counters for this CPU. This; is useful when creating new sched models (the host CPU is unknown to LLVM).; (`-mcpu=help` for details). .. option:: --analysis-override-benchmark-triple-and-cpu. By default, llvm-exegesis will analyze the benchmarks for the triple/CPU they; were measured for, but if you want to analyze them for some other combination; (specified via `-mtriple`/`-mcpu`), you can pass this flag. .. option:: --dump-object-to-disk=true. If set, llvm-exegesis will dump the generated code to a tempo",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-exegesis.rst:16506,perform,performance,16506,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-exegesis.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-exegesis.rst,1,['perform'],['performance']
Performance,"ptions over bindings exceptions; * Prevent infinite recursion when instantiating class with no constructors. 2021-11-14: 2.2.0; -----------------. * Migrated repos to github/wlav; * Properly resolve enum type of class enums; * Get proper shape of ``void*`` and enum arrays; * Fix access to (const) ref data members; * Fix sometimes PCH uninstall issue; * Fix argument passing of fixed arrays of pointers; * Include all gcc system paths (for asan); * Initial support for Apple M1. 2021-07-17: 2.1.0; -----------------. * Support for vector calls with CPython 3.8 and newer; * Support for typed C++ literals as defaults when mixing with keywords; * Enable reshaping of multi-dim LowLevelViews; * Refactored multi-dim arrays and support for multi-dim assignment; * Support tuple-based indexing for multi-dim arrays; * Direct support for C's _Complex (_Complex_double/_float on Windows); * sizeof() forwards to ctypes.sizeof() for ctypes' types; * Upgrade cmake fragments for Clang9; * Prevent clash with Julia's LLVM when loading cppyy into PyCall; * Upgrade to latest Cling patch release. 2021-05-14: 2.0.0; -----------------. * Upgrade to latest Cling based on Clang/LLVM 9; * Make C++17 the default standard on Windows. 2021-04-28: 1.9.6; -----------------. * Reverse operators for ``std::complex`` targeting Python's ``complex``; * Version the precompiled header with the ``cppyy-cling`` package version; * Cover more iterator protocol use cases; * Add missing cppyy/__pyinstaller pkg to sdist; * Single-inheritance support for cross-inherited templated constructors; * Disallow ``float`` -> ``const long long&`` conversion; * Capture python exception message string in PyException from callbacks; * Thread safety in enum lookups. 2021-03-22: 1.9.5; -----------------. * Do not regulate direct smart pointers (many to one can lead to double deletion); * Use pkg_resources of ``CPyCppyy``, if available, to find the API include path. 2021-03-17: 1.9.4; -----------------. * Fix for installing into a ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/changelog.rst:6053,load,loading,6053,bindings/pyroot/cppyy/cppyy/doc/source/changelog.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/changelog.rst,1,['load'],['loading']
Performance,"ptr %ptr, <vscale x 2 x i1> %mask, i32 %evl); declare <8 x float> @llvm.vp.load.v8f32.p1(ptr addrspace(1) %ptr, <8 x i1> %mask, i32 %evl); declare <vscale x 1 x i64> @llvm.vp.load.nxv1i64.p6(ptr addrspace(6) %ptr, <vscale x 1 x i1> %mask, i32 %evl). Overview:; """""""""""""""""". The '``llvm.vp.load.*``' intrinsic is the vector length predicated version of; the :ref:`llvm.masked.load <int_mload>` intrinsic. Arguments:; """""""""""""""""""". The first operand is the base pointer for the load. The second operand is a; vector of boolean values with the same number of elements as the return type.; The third is the explicit vector length of the operation. The return type and; underlying type of the base pointer are the same vector types. The :ref:`align <attr_align>` parameter attribute can be provided for the first; operand. Semantics:; """""""""""""""""""". The '``llvm.vp.load``' intrinsic reads a vector from memory in the same way as; the '``llvm.masked.load``' intrinsic, where the mask is taken from the; combination of the '``mask``' and '``evl``' operands in the usual VP way.; Certain '``llvm.masked.load``' operands do not have corresponding operands in; '``llvm.vp.load``': the '``passthru``' operand is implicitly ``poison``; the; '``alignment``' operand is taken as the ``align`` parameter attribute, if; provided. The default alignment is taken as the ABI alignment of the return; type as specified by the :ref:`datalayout string<langref_datalayout>`. Examples:; """""""""""""""""". .. code-block:: text. %r = call <8 x i8> @llvm.vp.load.v8i8.p0(ptr align 2 %ptr, <8 x i1> %mask, i32 %evl); ;; For all lanes below %evl, %r is lane-wise equivalent to %also.r. %also.r = call <8 x i8> @llvm.masked.load.v8i8.p0(ptr %ptr, i32 2, <8 x i1> %mask, <8 x i8> poison). .. _int_vp_store:. '``llvm.vp.store``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare void @llvm.vp.store.v4f32.p0(<4 x float> %val, ptr %ptr, <4 x i1> %mask, i32 %evl); declare",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:784097,load,load,784097,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,"ptr addrspace(N)``. This was proposed all the way back in; `2015 <https://lists.llvm.org/pipermail/llvm-dev/2015-February/081822.html>`_. Issues with explicit pointee types; ==================================. LLVM IR pointers can be cast back and forth between pointers with different; pointee types. The pointee type does not necessarily represent the actual; underlying type in memory. In other words, the pointee type carries no real; semantics. Historically LLVM was some sort of type-safe subset of C. Having pointee types; provided an extra layer of checks to make sure that the Clang frontend matched; its frontend values/operations with the corresponding LLVM IR. However, as other; languages like C++ adopted LLVM, the community realized that pointee types were; more of a hindrance for LLVM development and that the extra type checking with; some frontends wasn't worth it. LLVM's type system was `originally designed; <https://llvm.org/pubs/2003-05-01-GCCSummit2003.html>`_ to support high-level; optimization. However, years of LLVM implementation experience have demonstrated; that the pointee type system design does not effectively support; optimization. Memory optimization algorithms, such as SROA, GVN, and AA,; generally need to look through LLVM's struct types and reason about the; underlying memory offsets. The community realized that pointee types hinder LLVM; development, rather than helping it. Some of the initially proposed high-level; optimizations have evolved into `TBAA; <https://llvm.org/docs/LangRef.html#tbaa-metadata>`_ due to limitations with; representing higher-level language information directly via SSA values. Pointee types provide some value to frontends because the IR verifier uses types; to detect straightforward type confusion bugs. However, frontends also have to; deal with the complexity of inserting bitcasts everywhere that they might be; required. The community consensus is that the costs of pointee types; outweight the benefits, and that th",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:2458,optimiz,optimization,2458,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst,1,['optimiz'],['optimization']
Performance,"ptr nonnull %obj); to label %invoke.cont unwind label %lpad.catch. invoke.cont: ; preds = %entry; invoke void @""?may_throw@@YAXXZ""(); to label %invoke.cont.2 unwind label %lpad.cleanup. invoke.cont.2: ; preds = %invoke.cont; call void @""??_DCleanup@@QEAA@XZ""(ptr nonnull %obj) nounwind; br label %return. return: ; preds = %invoke.cont.3, %invoke.cont.2; %retval.0 = phi i32 [ 0, %invoke.cont.2 ], [ %3, %invoke.cont.3 ]; ret i32 %retval.0. lpad.cleanup: ; preds = %invoke.cont.2; %0 = cleanuppad within none []; call void @""??1Cleanup@@QEAA@XZ""(ptr nonnull %obj) nounwind; cleanupret from %0 unwind label %lpad.catch. lpad.catch: ; preds = %lpad.cleanup, %entry; %1 = catchswitch within none [label %catch.body] unwind label %lpad.terminate. catch.body: ; preds = %lpad.catch; %catch = catchpad within %1 [ptr @""??_R0H@8"", i32 0, ptr %e]; invoke void @""?may_throw@@YAXXZ""(); to label %invoke.cont.3 unwind label %lpad.terminate. invoke.cont.3: ; preds = %catch.body; %3 = load i32, ptr %e, align 4; catchret from %catch to label %return. lpad.terminate: ; preds = %catch.body, %lpad.catch; cleanuppad within none []; call void @""?terminate@@YAXXZ""(); unreachable; }. Funclet parent tokens; -----------------------. In order to produce tables for EH personalities that use funclets, it is; necessary to recover the nesting that was present in the source. This funclet; parent relationship is encoded in the IR using tokens produced by the new ""pad""; instructions. The token operand of a ""pad"" or ""ret"" instruction indicates which; funclet it is in, or ""none"" if it is not nested within another funclet. The ``catchpad`` and ``cleanuppad`` instructions establish new funclets, and; their tokens are consumed by other ""pad"" instructions to establish membership.; The ``catchswitch`` instruction does not create a funclet, but it produces a; token that is always consumed by its immediate successor ``catchpad``; instructions. This ensures that every catch handler modelled by a ``catchpad``; belongs to",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ExceptionHandling.rst:29056,load,load,29056,interpreter/llvm-project/llvm/docs/ExceptionHandling.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ExceptionHandling.rst,1,['load'],['load']
Performance,"pts more information for users has the label generally followed by; ellipsis (...). ![](pictures/0300020D.png). As we saw the hot strings ""&Draw"" and ""&Exit"" define the text labels; ""Draw"" and ""Exit"" and keyboard mnemonics `Alt+D`, `Alt+E` for their; selection. The letter D and E appear underlined on the screen. All text; buttons should have a unique shortcut key with the exception of OK and; Cancel. These buttons are usually placed within a window to provide fast access; to frequently used or critical commands. They help in situations where a; command is not available through the menu bar. You already know that a; command string can be passed in the text button via the constructor:. ``` {.cpp}; TGTextButton(const TGWindow *p, const char *s, const char *cmd,; Int_t id, GContext_t norm, FontStruct_t font,; UInt_t options);; ```. A button label can be changed by `SetText(new_label).` There are; important guidelines to be followed about a button label. The text has; to provide a meaningful description of the performed action. The; single-word label should be used whenever possible, only two-three words; for clarity, if necessary. Do not number labels. Always follow all; platform presentation and usage guidelines for standard button; functions. Let's remember a few standard names and definitions of well; known buttons:. ***`OK`*** - any changed information in a window is accepted and the; window is closed;. ***`Cancel`*** - closes window without implementing submitted changes;. ***`Reset `***- resets defaults and cancels any changed information that; has not be submitted;. ***`Apply`*** - any changed information is accepted and again displayed; in the window that remains open;. ***`Close`*** - closes the window;. ***`Help`*** - opens online Help. Below are examples of text buttons. Note the two placement methods. The; first example should be used when there are one to three command; buttons; the second one when there are more than three buttons. ![](pictures/0200020E.jpg",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/WritingGUI.md:56484,perform,performed,56484,documentation/users-guide/WritingGUI.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/WritingGUI.md,1,['perform'],['performed']
Performance,"pturing condition is; necessary in environments where the function might communicate the; pointer to another thread which then deallocates the memory. Alternatively,; ``nosync`` would ensure such communication cannot happen and even captured; pointers cannot be freed by the function. A ``nofree`` function is explicitly allowed to free memory which it; allocated or (if not ``nosync``) arrange for another thread to free; memory on it's behalf. As a result, perhaps surprisingly, a ``nofree``; function can return a pointer to a previously deallocated memory object.; ``noimplicitfloat``; Disallows implicit floating-point code. This inhibits optimizations that; use floating-point code and floating-point registers for operations that are; not nominally floating-point. LLVM instructions that perform floating-point; operations or require access to floating-point registers may still cause; floating-point code to be generated. Also inhibits optimizations that create SIMD/vector code and registers from; scalar code such as vectorization or memcpy/memset optimization. This; includes integer vectors. Vector instructions present in IR may still cause; vector code to be generated.; ``noinline``; This attribute indicates that the inliner should never inline this; function in any situation. This attribute may not be used together; with the ``alwaysinline`` attribute.; ``nomerge``; This attribute indicates that calls to this function should never be merged; during optimization. For example, it will prevent tail merging otherwise; identical code sequences that raise an exception or terminate the program.; Tail merging normally reduces the precision of source location information,; making stack traces less useful for debugging. This attribute gives the; user control over the tradeoff between code size and debug information; precision.; ``nonlazybind``; This attribute suppresses lazy symbol binding for the function. This; may make calls to the function faster, at the cost of extra progra",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:90463,optimiz,optimizations,90463,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,2,['optimiz'],"['optimization', 'optimizations']"
Performance,pu TestMethodDLSGDOptimizationCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-MethodDL-SGD-Optimization-Cpu COMMAND testMethodDLSGDOptimizationCpu). # DNN - MethodDL Adam Optimization CPU; ROOT_EXECUTABLE(testMethodDLAdamOptimizationCpu TestMethodDLAdamOptimizationCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-MethodDL-Adam-Optimization-Cpu COMMAND testMethodDLAdamOptimizationCpu TIMEOUT 1800). # DNN - MethodDL Adagrad Optimization CPU; ROOT_EXECUTABLE(testMethodDLAdagradOptimizationCpu TestMethodDLAdagradOptimizationCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-MethodDL-Adagrad-Optimization-Cpu COMMAND testMethodDLAdagradOptimizationCpu). # DNN - MethodDL RMSProp Optimization CPU; ROOT_EXECUTABLE(testMethodDLRMSPropOptimizationCpu TestMethodDLRMSPropOptimizationCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-MethodDL-RMSProp-Optimization-Cpu COMMAND testMethodDLRMSPropOptimizationCpu). # DNN - MethodDL Adadelta Optimization CPU; ROOT_EXECUTABLE(testMethodDLAdadeltaOptimizationCpu TestMethodDLAdadeltaOptimizationCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-MethodDL-Adadelta-Optimization-Cpu COMMAND testMethodDLAdadeltaOptimizationCpu). # DNN - Regression CPU; ROOT_EXECUTABLE(testRegressionCpu TestRegressionMethodDL.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-Regression-Cpu COMMAND testRegressionCpu). #( old-dnn-test ); # DNN - DataLoader CPU; ROOT_EXECUTABLE(testDataLoaderCpu TestDataLoaderCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-Data-Loader-Cpu COMMAND testDataLoaderCpu). # DNN - Minimization CPU; ROOT_EXECUTABLE(testMinimizationCpu TestMinimizationCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-Minimization-Cpu COMMAND testMinimizationCpu). # tests using TReference architecture; if ( reference-tests). # DNN - Activation Functions; ROOT_EXECUTABLE(testActivationFunctions TestActivationFunctions.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-Activation-Functions COMMAND testActivationFu,MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tmva/tmva/test/DNN/CMakeLists.txt:6592,Optimiz,Optimization,6592,tmva/tmva/test/DNN/CMakeLists.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tmva/tmva/test/DNN/CMakeLists.txt,1,['Optimiz'],['Optimization']
Performance,"purpose and single execution semantics seems desirable. It makes it easier; for the consumer that no longer have to track the context. It makes it; easier for the producer as it can rely on a single semantics for each; attribute. For that reason, limiting the ``DW_AT_location`` attribute to only; supporting evaluating the location description of an object, and using a; different attribute and encoding class for the evaluation of DWARF; expression *procedures* on the same operation expression stack seems; desirable. 2. ``DW_AT_const_value``. .. note::. Could deprecate using the ``DW_AT_const_value`` attribute for; ``DW_TAG_variable`` or ``DW_TAG_formal_parameter`` debugger information; entries that have been optimized to a constant. Instead,; ``DW_AT_location`` could be used with a DWARF expression that produces an; implicit location description now that any location description can be; used within a DWARF expression. This allows the ``DW_OP_call*`` operations; to be used to push the location description of any variable regardless of; how it is optimized. 3. ``DW_AT_LLVM_memory_space``. A ``DW_AT_memory_space`` attribute with a constant value representing a source; language specific DWARF memory space (see 2.14 ""Memory Spaces""). If omitted,; defaults to ``DW_MSPACE_none``. A.4.2 Common Block Entries; ~~~~~~~~~~~~~~~~~~~~~~~~~~. A common block entry also has a ``DW_AT_location`` attribute whose value is a; DWARF expression E that describes the location of the common block at run-time.; The result of the attribute is obtained by evaluating E with a context that has; a result kind of a location description, an unspecified object, the compilation; unit that contains E, an empty initial stack, and other context elements; corresponding to the source language thread of execution upon which the user is; focused, if any. The result of the evaluation is the location description of the; base of the common block. See :ref:`amdgpu-dwarf-control-flow-operations` for; special evalu",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst:174807,optimiz,optimized,174807,interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst,1,['optimiz'],['optimized']
Performance,"pyy.load_reflection_info('MyClassDict'); >>> cppyy.gbl.MyClass(42); <cppyy.gbl.MyClass object at 0x7ffb9f230950>; >>> print(_.get_int()); 42; >>>. and use the selected C++ entities as if the header was loaded. The dictionary shared library can be relocated, as long as it can be found; by the dynamic loader (e.g. through ``LD_LIBRARY_PATH``) and the header file; is fully embedded or still accessible (e.g. through a path added to; ``cppyy.add_include_path`` at run-time, or with ``-I`` to; ``rootcling``/``genreflex`` during build time).; When relocating the shared library, move the .pcm with it.; Once support for C++ modules is fully fleshed out, access to the header file; will no longer be needed. .. _`rootcling manual`: https://root.cern.ch/root/html/guides/users-guide/AddingaClass.html#the-linkdef.h-file; .. _`helper script`: https://github.com/wlav/cppyy/blob/master/test/make_dict_win32.py. Class loader; ^^^^^^^^^^^^. Explicitly loading dictionaries is fine if this is hidden under the hood of; a Python package and thus transparently done on ``import``.; Otherwise, the automatic class loader is more convenient, as it allows direct; use without having to manually find and load dictionaries (assuming these are; locatable by the dynamic loader). The class loader utilizes so-called rootmap files, which by convention should; live alongside the dictionary shared library (and C++ module file).; These are simple text files, which map C++ entities (such as classes) to the; dictionaries and other libraries that need to be loaded for their use. With ``genreflex``, the mapping file can be automatically created with; ``--rootmap-lib=MyClassDict``, where ""MyClassDict"" is the name of the shared; library (without the extension) build from the dictionary file.; With ``rootcling``, create the same mapping file with; ``-rmf MyClassDict.rootmap -rml MyClassDict``.; It is necessary to provide the final library name explicitly, since it is; only in the separate linking step where these na",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/utilities.rst:8192,load,loading,8192,bindings/pyroot/cppyy/cppyy/doc/source/utilities.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/utilities.rst,1,['load'],['loading']
Performance,"q	%rsi, %rax; 	jne	LBB0_2. All things considered this isn't too bad, but we shouldn't need the movslq or; the shlq instruction, or the load folded into ucomisd every time through the; loop. On an x86-specific topic, if the loop can't be restructure, the movl should be a; cmov. //===---------------------------------------------------------------------===//. [STORE SINKING]. GCC PR37810 is an interesting case where we should sink load/store reload; into the if block and outside the loop, so we don't reload/store it on the; non-call path. for () {; *P += 1;; if (); call();; else; ...; ->; tmp = *P; for () {; tmp += 1;; if () {; *P = tmp;; call();; tmp = *P;; } else ...; }; *P = tmp;. We now hoist the reload after the call (Transforms/GVN/lpre-call-wrap.ll), but; we don't sink the store. We need partially dead store sinking. //===---------------------------------------------------------------------===//. [LOAD PRE CRIT EDGE SPLITTING]. GCC PR37166: Sinking of loads prevents SROA'ing the ""g"" struct on the stack; leading to excess stack traffic. This could be handled by GVN with some crazy; symbolic phi translation. The code we get looks like (g is on the stack):. bb2:		; preds = %bb1; ..; 	%9 = getelementptr %struct.f* %g, i32 0, i32 0		; 	store i32 %8, i32* %9, align bel %bb3. bb3:		; preds = %bb1, %bb2, %bb; 	%c_addr.0 = phi %struct.f* [ %g, %bb2 ], [ %c, %bb ], [ %c, %bb1 ]; 	%b_addr.0 = phi %struct.f* [ %b, %bb2 ], [ %g, %bb ], [ %b, %bb1 ]; 	%10 = getelementptr %struct.f* %c_addr.0, i32 0, i32 0; 	%11 = load i32* %10, align 4. %11 is partially redundant, an in BB2 it should have the value %8. GCC PR33344 and PR35287 are similar cases. //===---------------------------------------------------------------------===//. [LOAD PRE]. There are many load PRE testcases in testsuite/gcc.dg/tree-ssa/loadpre* in the; GCC testsuite, ones we don't get yet are (checked through loadpre25):. [CRIT EDGE BREAKING]; predcom-4.c. [PRE OF READONLY CALL]; loadpre5.c. [TURN SELECT INTO BRAN",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:32630,load,loads,32630,interpreter/llvm-project/llvm/lib/Target/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt,2,['load'],['loads']
Performance,"q_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; vscnt(0) and so do; not need to be; considered.); - Ensures any; preceding; sequential; consistent global; memory instructions; have completed; before executing; this sequentially; consistent; instruction. This; prevents reordering; a seq_cst store; followed by a; seq_cst load. (Note; that seq_cst is; stronger than; acquire/release as; the reordering of; load acquire; followed by a store; release is; prevented by the; s_waitcnt of; the release, but; there is nothing; preventing a store; release followed by; load acquire from; completing out of; order. The s_waitcnt; could be placed after; seq_store or before; the seq_load. We; choose the load to; make the s_waitcnt be; as late as possible; so that the store; may have already; completed.). 2. *Following; instructions same as; corresponding load; atomic acquire,; except must generate; all instructions even; for OpenCL.*. load atomic seq_cst - agent - global 1. s_waitcnt lgkmcnt(0) &; - system - generic vmcnt(0) & vscnt(0). - Could be split into; separate s_waitcnt; vmcnt(0), s_waitcnt; vscnt(0) and s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt lgkmcnt(0); must happen after; preceding; local load; atomic/store; atomic/atomicrmw; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; lgkmcnt(0) and so do; not need to be; considered.); - s_waitcnt vmcnt(0); must happen after; preceding; global/generic load; atomic/; atomicrmw-with-return-value; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; vmcnt(0) and so do; not need to be; considered.); - s_waitcnt vscnt(0); Must happen after; preceding; global/generic store; atomic/; atomicrmw-no-return-value; with memory; ordering of seq_cst; and with equal or; wider sync ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:377552,load,load,377552,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"qual to 9. This means the full; address can only be guaranteed to be less than `(1 << 31) + 9`. The OS may wish; to protect an extra page of the low address space to account for this. ##### Optimizations. A very large portion of the cost for this approach comes from checking loads in; this way, so it is important to work to optimize this. However, beyond making; the instruction sequences to *apply* the checks efficient (for example by; avoiding `pushfq` and `popfq` sequences), the only significant optimization is; to check fewer loads without introducing a vulnerability. We apply several; techniques to accomplish that. ###### Don't check loads from compile-time constant stack offsets. We implement this optimization on x86 by skipping the checking of loads which; use a fixed frame pointer offset. The result of this optimization is that patterns like reloading a spilled; register or accessing a global field don't get checked. This is a very; significant performance win. ###### Don't check dependent loads. A core part of why this mitigation strategy works is that it establishes a; data-flow check on the loaded address. However, this means that if the address; itself was already loaded using a checked load, there is no need to check a; dependent load provided it is within the same basic block as the checked load,; and therefore has no additional predicates guarding it. Consider code like the; following:; ```; ... .LBB0_4: # %danger; movq (%rcx), %rdi; movl (%rdi), %edx; ```. This will get transformed into:; ```; ... .LBB0_4: # %danger; cmovneq %r8, %rax # Conditionally update predicate state.; orq %rax, %rcx # Mask the pointer if misspeculating.; movq (%rcx), %rdi # Hardened load.; movl (%rdi), %edx # Unhardened load due to dependent addr.; ```. This doesn't check the load through `%rdi` as that pointer is dependent on a; checked load already. ###### Protect large, load-heavy blocks with a single lfence. It may be worth using a single `lfence` instruction at the start o",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md:35311,load,loads,35311,interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,1,['load'],['loads']
Performance,"qual to ``NaN`` and; ``Inf``, and; * ``+0`` and ``-0`` are interchangeable. ``-ffast-math`` also defines the ``__FAST_MATH__`` preprocessor; macro. Some math libraries recognize this macro and change their behavior.; With the exception of ``-ffp-contract=fast``, using any of the options; below to disable any of the individual optimizations in ``-ffast-math``; will cause ``__FAST_MATH__`` to no longer be set.; ``-ffast-math`` enables ``-fcx-limited-range``. This option implies:. * ``-fno-honor-infinities``. * ``-fno-honor-nans``. * ``-fapprox-func``. * ``-fno-math-errno``. * ``-ffinite-math-only``. * ``-fassociative-math``. * ``-freciprocal-math``. * ``-fno-signed-zeros``. * ``-fno-trapping-math``. * ``-fno-rounding-math``. * ``-ffp-contract=fast``. Note: ``-ffast-math`` causes ``crtfastmath.o`` to be linked with code. See; :ref:`crtfastmath.o` for more details. .. option:: -fno-fast-math. Disable fast-math mode. This options disables unsafe floating-point; optimizations by preventing the compiler from making any transformations that; could affect the results. This option implies:. * ``-fhonor-infinities``. * ``-fhonor-nans``. * ``-fno-approx-func``. * ``-fno-finite-math-only``. * ``-fno-associative-math``. * ``-fno-reciprocal-math``. * ``-fsigned-zeros``. * ``-ffp-contract=on``. Also, this option resets following options to their target-dependent defaults. * ``-f[no-]math-errno``; * ``-fdenormal-fp-math=<value>``. There is ambiguity about how ``-ffp-contract``, ``-ffast-math``,; and ``-fno-fast-math`` behave when combined. To keep the value of; ``-ffp-contract`` consistent, we define this set of rules:. * ``-ffast-math`` sets ``ffp-contract`` to ``fast``. * ``-fno-fast-math`` sets ``-ffp-contract`` to ``on`` (``fast`` for CUDA and; HIP). * If ``-ffast-math`` and ``-ffp-contract`` are both seen, but; ``-ffast-math`` is not followed by ``-fno-fast-math``, ``ffp-contract``; will be given the value of whichever option was last seen. * If ``-fno-fast-math`` is seen and `",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:53133,optimiz,optimizations,53133,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,1,['optimiz'],['optimizations']
Performance,"quences to the above ones. However, these are still very marginally; slower, as there are fewer ports able to dispatch shift instructions in most; modern x86 processors than there are for `or` instructions. Fast, single register addressing mode:; ```; ... .LBB0_4: # %danger; cmovneq %r8, %rax # Conditionally update predicate state.; shrxq %rax, %rsi, %rsi # Shift away bits if misspeculating.; movl (%rsi), %edi; ```. This will collapse the register to zero or one, and everything but the offset; in the addressing mode to be less than or equal to 9. This means the full; address can only be guaranteed to be less than `(1 << 31) + 9`. The OS may wish; to protect an extra page of the low address space to account for this. ##### Optimizations. A very large portion of the cost for this approach comes from checking loads in; this way, so it is important to work to optimize this. However, beyond making; the instruction sequences to *apply* the checks efficient (for example by; avoiding `pushfq` and `popfq` sequences), the only significant optimization is; to check fewer loads without introducing a vulnerability. We apply several; techniques to accomplish that. ###### Don't check loads from compile-time constant stack offsets. We implement this optimization on x86 by skipping the checking of loads which; use a fixed frame pointer offset. The result of this optimization is that patterns like reloading a spilled; register or accessing a global field don't get checked. This is a very; significant performance win. ###### Don't check dependent loads. A core part of why this mitigation strategy works is that it establishes a; data-flow check on the loaded address. However, this means that if the address; itself was already loaded using a checked load, there is no need to check a; dependent load provided it is within the same basic block as the checked load,; and therefore has no additional predicates guarding it. Consider code like the; following:; ```; ... .LBB0_4: # %danger; movq (",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md:34802,optimiz,optimization,34802,interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,2,"['load', 'optimiz']","['loads', 'optimization']"
Performance,"quest will be closed and you will be notified by GitHub. Review expectations; -------------------. In order to make LLVM a long-term sustainable effort, code needs to be; maintainable and well tested. Code reviews help to achieve that goal.; Especially for new contributors, that often means many rounds of reviews; and push-back on design decisions that do not fit well within the; overall architecture of the project. For your first patches, this means:. - be kind, and expect reviewers to be kind in return - LLVM has a `Code; of Conduct <https://llvm.org/docs/CodeOfConduct.html>`__;. - be patient - understanding how a new feature fits into the; architecture of the project is often a time consuming effort, and; people have to juggle this with other responsibilities in their; lives; **ping the review once a week** when there is no response;. - if you can't agree, generally the best way is to do what the reviewer; asks; we optimize for readability of the code, which the reviewer is; in a better position to judge; if this feels like it's not the right; option, you can contact the cfe-dev mailing list to get more feedback; on the direction;. Commit access; =============. Once you've contributed a handful of patches to LLVM, start to think; about getting commit access yourself. It's probably a good idea if:. - you've landed 3-5 patches of larger scope than ""fix a typo"". - you'd be willing to review changes that are closely related to yours. - you'd like to keep contributing to LLVM. Getting commit access; ---------------------. LLVM uses Git for committing changes. The details are in the `developer; policy; document <https://llvm.org/docs/DeveloperPolicy.html#obtaining-commit-access>`__. With great power; ----------------. Actually, this would be a great time to read the rest of the `developer; policy <https://llvm.org/docs/DeveloperPolicy.html>`__, too. At minimum,; you need to be subscribed to the relevant commits list before landing; changes (e.g. llvm-commits@lists.llvm.",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:12426,optimiz,optimize,12426,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,1,['optimiz'],['optimize']
Performance,"quired. 3. buffer_gl0_inv. - If CU wavefront execution; mode, omit.; - Ensures that; following; loads will not see; stale data. load atomic acquire - agent - global 1. buffer/global_load; - system glc=1 dlc=1. - If GFX11, omit dlc=1. 2. s_waitcnt vmcnt(0). - Must happen before; following; buffer_gl*_inv.; - Ensures the load; has completed; before invalidating; the caches. 3. buffer_gl0_inv;; buffer_gl1_inv. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale global data. load atomic acquire - agent - generic 1. flat_load glc=1 dlc=1; - system; - If GFX11, omit dlc=1. 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If OpenCL omit; lgkmcnt(0).; - Must happen before; following; buffer_gl*_invl.; - Ensures the flat_load; has completed; before invalidating; the caches. 3. buffer_gl0_inv;; buffer_gl1_inv. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acquire - singlethread - global 1. buffer/global/ds/flat_atomic; - wavefront - local; - generic; atomicrmw acquire - workgroup - global 1. buffer/global_atomic; 2. s_waitcnt vm/vscnt(0). - If CU wavefront execution; mode, omit.; - Use vmcnt(0) if atomic with; return and vscnt(0) if; atomic with no-return.; - Must happen before; the following buffer_gl0_inv; and before any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw. 3. buffer_gl0_inv. - If CU wavefront execution; mode, omit.; - Ensures that; following; loads will not see; stale data. atomicrmw acquire - workgroup - local 1. ds_atomic; 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; the following; buffer_gl0_inv.; - Ensures any; following global; data read is no; older than the local; atomicrmw value; being acquired. 3. buffer_gl0_inv. - If OpenCL omit.; - Ensures that; following; loads will not see; stale data. atomicrmw acquire - workgroup - ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:348601,load,loads,348601,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['loads']
Performance,"quires a definition; ```. Line #4 forces cling to send ROOT a callback that TCanvas in unknown but; the GMI resolves it to module Gpad, loads it and returns the control to cling. ### Performance; This section compares ROOT PCH technology with C++ Modules which is important but; unfair comparison. As we noted earlier, PCH is very efficient, it cannot be; extended to the experiments’ software stacks because of its design constraints.; On the contrary, the C++ Modules can be used in third-party code where the PCH; is not available. The comparisons are to give a good metric when we are ready to switch ROOT to use; C++ Modules by default. However, since it is essentially the same technology,; optimizations of C++ Modules also affect the PCH. We have a few tricks up in; the sleeves to but they come with given trade-offs. #### Preloading of C++ Modules. The main focus for the technology preview was not in performance until recently.; We have invested some resources in optimizations and we would like to show you; (probably outdated) performance results:. * Memory footprint -- mostly due to importing all C++ Modules at startup; we see overhead which depends on the number of preloaded modules. For; ROOT it is between 40-60 MB depending on the concrete configuration.; When the workload increases we notice that the overall memory performance; decreases in number of cases.; * Execution times -- likewise we have an execution overhead. For ; workflows which take ms the slowdown can be 2x. Increasing of the work; to seconds shows 50-60% slowdowns. The performance is dependent on many factors such as configuration of ROOT and; workflow. You can read more at our Intel IPCC-ROOT Showcase presentation; here (pp 25-33)[[8]]. #### Loading C++ Modules on Demand. In long term, we should optimize the preloading of modules to be a no-op and; avoid recursive behavior based on identifier lookup callbacks. Unfortunately,; at the moment the loading of C++ modules on demand shows significantly bet",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/README.CXXMODULES.md:18089,optimiz,optimizations,18089,README/README.CXXMODULES.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/README.CXXMODULES.md,2,"['optimiz', 'perform']","['optimizations', 'performance']"
Performance,"quivalently, interpret) everything. On the far ; other side, traditional static compilers process everything statically and ; nothing dynamically. These approaches have typically been seen as a ; tradeoff between performance and portability. On a deeper level, however, ; there are two reasons that optimal system performance may be obtained by a; system somewhere in between these two extremes: Dynamic application ; behavior and social constraints. From a technical perspective, pure static compilation cannot ever give ; optimal performance in all cases, because applications have varying dynamic; behavior that the static compiler cannot take into consideration. Even ; compilers that support profile guided optimization generate poor code in ; the real world, because using such optimization tunes that application ; to one particular usage pattern, whereas real programs (as opposed to ; benchmarks) often have several different usage patterns. On a social level, static compilation is a very shortsighted solution to ; the performance problem. Instruction set architectures (ISAs) continuously ; evolve, and each implementation of an ISA (a processor) must choose a set ; of tradeoffs that make sense in the market context that it is designed for. ; With every new processor introduced, the vendor faces two fundamental ; problems: First, there is a lag time between when a processor is introduced ; to when compilers generate quality code for the architecture. Secondly, ; even when compilers catch up to the new architecture there is often a large ; body of legacy code that was compiled for previous generations and will ; not or can not be upgraded. Thus a large percentage of code running on a ; processor may be compiled quite sub-optimally for the current ; characteristics of the dynamic execution environment. For these reasons, LLVM has been designed from the beginning as a long-term ; solution to these problems. Its design allows the large body of platform ; independent, static, ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt:1725,perform,performance,1725,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt,1,['perform'],['performance']
Performance,"r (int i = 0; i < n; ++i); body(i);. A better measure is the **backedge-taken count**, which is the number of; times any of the backedges is taken before the loop. It is one less than; the trip count for executions that enter the header. .. _loopinfo:. LoopInfo; ========. LoopInfo is the core analysis for obtaining information about loops.; There are few key implications of the definitions given above which; are important for working successfully with this interface. * LoopInfo does not contain information about non-loop cycles. As a; result, it is not suitable for any algorithm which requires complete; cycle detection for correctness. * LoopInfo provides an interface for enumerating all top level loops; (e.g. those not contained in any other loop). From there, you may; walk the tree of sub-loops rooted in that top level loop. * Loops which become statically unreachable during optimization *must*; be removed from LoopInfo. If this can not be done for some reason,; then the optimization is *required* to preserve the static; reachability of the loop. .. _loop-terminology-loop-simplify:. Loop Simplify Form; ==================. The Loop Simplify Form is a canonical form that makes; several analyses and transformations simpler and more effective.; It is ensured by the LoopSimplify; (:ref:`-loop-simplify <passes-loop-simplify>`) pass and is automatically; added by the pass managers when scheduling a LoopPass.; This pass is implemented in; `LoopSimplify.h <https://llvm.org/doxygen/LoopSimplify_8h_source.html>`_.; When it is successful, the loop has:. * A preheader.; * A single backedge (which implies that there is a single latch).; * Dedicated exits. That is, no exit block for the loop; has a predecessor that is outside the loop. This implies; that all exit blocks are dominated by the loop header. .. _loop-terminology-lcssa:. Loop Closed SSA (LCSSA); =======================. A program is in Loop Closed SSA Form if it is in SSA form; and all values that are defined in a loo",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:9825,optimiz,optimization,9825,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst,1,['optimiz'],['optimization']
Performance,"r MachO; format, depending on the target) into the ObjectBufferStream object, which; is flushed to complete the process. If an ObjectCache is being used, the; image will be passed to the ObjectCache here. At this point, the ObjectBufferStream contains the raw object image.; Before the code can be executed, the code and data sections from this; image must be loaded into suitable memory, relocations must be applied and; memory permission and code cache invalidation (if required) must be completed. Object Loading; ==============. Once an object image has been obtained, either through code generation or; having been retrieved from an ObjectCache, it is passed to RuntimeDyld to; be loaded. The RuntimeDyld wrapper class examines the object to determine; its file format and creates an instance of either RuntimeDyldELF or; RuntimeDyldMachO (both of which derive from the RuntimeDyldImpl base; class) and calls the RuntimeDyldImpl::loadObject method to perform that; actual loading. .. image:: MCJIT-dyld-load.png. RuntimeDyldImpl::loadObject begins by creating an ObjectImage instance; from the ObjectBuffer it received. ObjectImage, which wraps the; ObjectFile class, is a helper class which parses the binary object image; and provides access to the information contained in the format-specific; headers, including section, symbol and relocation information. RuntimeDyldImpl::loadObject then iterates through the symbols in the; image. Information about common symbols is collected for later use. For; each function or data symbol, the associated section is loaded into memory; and the symbol is stored in a symbol table map data structure. When the; iteration is complete, a section is emitted for the common symbols. Next, RuntimeDyldImpl::loadObject iterates through the sections in the; object image and for each section iterates through the relocations for; that sections. For each relocation, it calls the format-specific; processRelocationRef method, which will examine the relocation and",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst:3994,load,load,3994,interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,1,['load'],['load']
Performance,"r TH2 class; 7. Support basic drawing of TPolyLine class; 8. Interactive axis zooming in 3D with mouse, very much like to 2D; 9. Zooming and tool buttons via keyboards. ## Changes in 4.6.0; 1. Improvements in TGeo drawings; - support of large (~10M volumes) models, only most significant volumes are shown; - one could activate several clip planes (only with WebGL); - interaction with object browser to change visibility flags or focus on selected volume; - support of floating browser for TGeo objects; - intensive use of HTML Worker to offload computation tasks and keep interactivity; - enable more details when changing camera position/zoom; - better and faster build of composite shapes; 2. Improvements in histograms 3D drawing; - all lego options: lego1..lego4, combined with 'fb', 'bb', '0' or 'z'; - support axis labels on lego plots; - support lego plots for TH1; 3. Improvements in all 3D graphics; - upgrade three.js to r79; - use of THREE.BufferGeometry for all components; - significant (up to factor 10) performance improvement; 4. Implement box and hbox draw options for TH1 class; 5. Implement drawing of axes ticks on opposite side (when fTickx/y specified); 6. Preliminary support of candle plot (many options to be implemented); 7. Update draw attributes (fill/line/position) when monitor objects. ## Changes in 4.5.3; 1. Fix - position of TFrame in canvas/pad; 2. Fix - use histogram fMinimum/fMaximum when creating color palette; 3. Fix - correctly draw empty th2 bins when zmin<0 is specified; 4. Fix - limit th2 text output size; 5. Fix - use histogram fMinimum/fMaximum when drawing z axis in lego plot; 6. Fix - error in TGeoCtub shape creation; 7. Fix - error in pcon/pgon shapes when Rmin===0. ## Changes in 4.5.1; 1. Fix - correctly handle ^2..^9 in TFormula equations; 2. Fix - support TMath::Gaus in TFormula; 3. Fix - correctly display ^2 and ^3 in SVG text output; 4. Fix - do not show tooltips for empty TProfile bins; 5. Fix - statbox toggling was not working on s",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/js/changes.md:53364,perform,performance,53364,js/changes.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/js/changes.md,1,['perform'],['performance']
Performance,"r Valls Pla, UJI, CERN/SFT,\; Vassil Vassilev, Princeton/CMS,\; Wouter Verkerke, NIKHEF/Atlas, RooFit,\; Stefan Wunsch, CERN/SFT, \; Zhe Zhang, UNL. ## Important Notice. The default compression algorithm used when writing ROOT files has been updated to use LZ4 in particular to improve read (decompression) performance. You can change this default for each file through (for example) the `TFile constructor` or `TFile::SetCompressionAlgorithm`. It should be noted that ROOT files written with LZ4 compression can not be read with older release of ROOT. Support for LZ4 was however back-ported to the patch branches of previous releases and the following tags (and later release in the same patch series) can read ROOT files written with LZ4 compression:. * v5.34/38; * v6.08/06 [not yet released]; * v6.10/08; * v6.12/02. ## Removed interfaces. ## Core Libraries; - Optimize away redundant deserialization of template specializations. This reduces the memory footprint for hsimple by around 30% while improving the runtime performance for various cases by around 15%.; - When ROOT is signaled with a SIGUSR2 (i.e. on Linux and MacOS X) it will now print a backtrace.; - Move RStringView.h to ROOT/RStringView.hxx and always include ROOT/RStringView.hxx instead of RStringView.h for backward compatibility; - In `TClingCallFunc`, support r-value reference parameters. This paves the way for the corresponding support in PyROOT (implemented now in the latest Cppyy).; - Included the new TSequentialExecutor in ROOT, sharing the interfaces of TExecutor.This should improve code economy when providing a fallback for TThreadExecutor/TProcessExecutor. ### Thread safety; - Resolved several race conditions, dead-locks, performance and order of initialization/destruction issues still lingering because of or despite the new read-write lock mechanism. ## Interpreter. - Enabled use of multi-threaded code from the interpreter.; - Previouslyl multi-threaded code could be run from the interpreter as long as ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v614/index.md:1898,perform,performance,1898,README/ReleaseNotes/v614/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v614/index.md,1,['perform'],['performance']
Performance,"r `RooAbsReal`. With all these inconsistencies, it was deemed safer to disable copy assignment; of RooAbsArgs from now on. ### RooBrowser: a graphical user interface for workspace exploration, visualization, and analysis. This experimental new feature utilises the technology from ROOT's familiar `TBrowser` in order to create an interface for graphically exploring and visualizing the content of a workspace, as well as perform basic fitting operations with the models and datasets. ![Demonstration of RooBrowser using json workspace from the roofit tutorials directory](RooBrowser.png). ### Removal of deprecated HistFactory functionality. #### Removal of HistoToWorkspaceFactory (non-Fast version). The original `HistoToWorkspaceFactory` produced models that consisted of a; Poisson term for each bin. In this ""number counting form"" the dataset has one; row and the collumns corresponded to the number of events for each bin. This; led to severe performance problems in statistical tools that generated; pseudo-experiments and evaluated likelihood ratio test statistics. Nowadays, everyone uses the faster `HistoToWorkspaceFactoryFast` implementation that; produces a model in the ""standard form"" where the dataset has one row for each; event, and the column corresponds to the value of the observable in the; histogram. Therefore, the original `HistoToWorkspaceFactory` is now removed to avoid; confusion and maintainance burden. #### Removing constant parameter flag from RooStats:HistFactory::NormFactor. As printed out by the HistFactory in a warning message for a long time already,; setting the `Const` attribute to the `<NormFactor>` tag is deprecated and it; will be ignored. Instead, add `<ParamSetting Const=""True""> myparam </ParamSetting>` to your top-level XML's `<Measurement>` entry. This deprecation implied that the constant parameter flag in the; `RooStats:HistFactory::NormFactor` class had no effect as well. To avoid; ambiguity in the future, the possibility to set and retrieve",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v628/index.md:22806,perform,performance,22806,README/ReleaseNotes/v628/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v628/index.md,1,['perform'],['performance']
Performance,"r a global variable. Here metadata; ``!22`` is attached to the ``f1`` and ``f2`` functions, and the globals ``g1``; and ``g2`` using the ``!dbg`` identifier:. .. code-block:: llvm. declare !dbg !22 void @f1(); define void @f2() !dbg !22 {; ret void; }. @g1 = global i32 0, !dbg !22; @g2 = external global i32, !dbg !22. Unlike instructions, global objects (functions and global variables) may have; multiple metadata attachments with the same identifier. A transformation is required to drop any metadata attachment that it; does not know or know it can't preserve. Currently there is an; exception for metadata attachment to globals for ``!func_sanitize``,; ``!type``, ``!absolute_symbol`` and ``!associated`` which can't be; unconditionally dropped unless the global is itself deleted. Metadata attached to a module using named metadata may not be dropped, with; the exception of debug metadata (named metadata with the name ``!llvm.dbg.*``). More information about specific metadata nodes recognized by the; optimizers and code generator is found below. .. _specialized-metadata:. Specialized Metadata Nodes; ^^^^^^^^^^^^^^^^^^^^^^^^^^. Specialized metadata nodes are custom data structures in metadata (as opposed; to generic tuples). Their fields are labelled, and can be specified in any; order. These aren't inherently debug info centric, but currently all the specialized; metadata nodes are related to debug info. .. _DICompileUnit:. DICompileUnit; """""""""""""""""""""""""". ``DICompileUnit`` nodes represent a compile unit. The ``enums:``,; ``retainedTypes:``, ``globals:``, ``imports:`` and ``macros:`` fields are tuples; containing the debug info to be emitted along with the compile unit, regardless; of code optimizations (some nodes are only emitted if there are references to; them from instructions). The ``debugInfoForProfiling:`` field is a boolean; indicating whether or not line-table discriminators are updated to provide; more-accurate debug info for profiling results. .. code-block:: te",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:245697,optimiz,optimizers,245697,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['optimiz'],['optimizers']
Performance,"r a single work-group are executed in the same; WGP. In CU wavefront execution mode the wavefronts may be executed by; different SIMDs in the same CU. In WGP wavefront execution mode the; wavefronts may be executed by different SIMDs in different CUs in the same; WGP.; * Each WGP has a single LDS memory shared by the wavefronts of the work-groups; executing on it.; * All LDS operations of a WGP are performed as wavefront wide operations in a; global order and involve no caching. Completion is reported to a wavefront in; execution order.; * The LDS memory has multiple request queues shared by the SIMDs of a; WGP. Therefore, the LDS operations performed by different wavefronts of a; work-group can be reordered relative to each other, which can result in; reordering the visibility of vector memory operations with respect to LDS; operations of other wavefronts in the same work-group. A ``s_waitcnt; lgkmcnt(0)`` is required to ensure synchronization between LDS operations and; vector memory operations between wavefronts of a work-group, but not between; operations performed by the same wavefront.; * The vector memory operations are performed as wavefront wide operations.; Completion of load/store/sample operations are reported to a wavefront in; execution order of other load/store/sample operations performed by that; wavefront.; * The vector memory operations access a vector L0 cache. There is a single L0; cache per CU. Each SIMD of a CU accesses the same L0 cache. Therefore, no; special action is required for coherence between the lanes of a single; wavefront. However, a ``buffer_gl0_inv`` is required for coherence between; wavefronts executing in the same work-group as they may be executing on SIMDs; of different CUs that access different L0s. A ``buffer_gl0_inv`` is also; required for coherence between wavefronts executing in different work-groups; as they may be executing on different WGPs.; * The scalar memory operations access a scalar L0 cache shared by all wavefr",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:336475,perform,performed,336475,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['perform'],['performed']
Performance,"r and easier.; As of `this revision <https://reviews.llvm.org/D121381>`_, the default was; changed to not optimize uses at build time, in order to provide the option to; reduce compile-time if the walking is not necessary in a pass. Most users call; the new API ``ensureOptimizedUses()`` to keep the previous behavior and do a; one-time optimization of ``MemoryUse``\ s, if this was not done before.; New pass users are recommended to call ``ensureOptimizedUses()``. Initially it was not possible to optimize ``MemoryDef``\ s in the same way, as we; restricted ``MemorySSA`` to one operand per access.; This was changed and ``MemoryDef``\ s now keep two operands.; The first one, the defining access, is; always the previous ``MemoryDef`` or ``MemoryPhi`` in the same basic block, or; the last one in a dominating predecessor if the current block doesn't have any; other accesses writing to memory. This is needed for walking Def chains.; The second operand is the optimized access, if there was a previous call on the; walker's ``getClobberingMemoryAccess(MA)``. This API will cache information; as part of ``MA``.; Optimizing all ``MemoryDef``\ s has quadratic time complexity and is not done; by default. A walk of the uses for any MemoryDef can find the accesses that were optimized; to it.; A code snippet for such a walk looks like this:. .. code-block:: c++. MemoryDef *Def; // find who's optimized or defining for this MemoryDef; for (auto& U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *DefUser = cast_of_null<MemoryDef>MA); if (DefUser->isOptimized() && DefUser->getOptimized() == Def) {; // User who is optimized to Def; } else {; // User who's defining access is Def; optimized to something else or not optimized.; }; }. When ``MemoryUse``\ s are optimized, for a given store, you can find all loads; clobbered by that store by walking the immediate and transitive uses of; the store. .. code-block:: c++. checkUses(MemoryAccess *Def) { // Def can be",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:12481,optimiz,optimized,12481,interpreter/llvm-project/llvm/docs/MemorySSA.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst,1,['optimiz'],['optimized']
Performance,"r constructs a Compilation object for each set of command line; arguments. The Driver itself is intended to be invariant during; construction of a Compilation; an IDE should be able to construct a; single long lived driver instance to use for an entire build, for; example. The Compilation object holds information that is particular to each; compilation sequence. For example, the list of used temporary files; (which must be removed once compilation is finished) and result files; (which should be removed if compilation fails). Unified Parsing & Pipelining; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Parsing and pipelining both occur without reference to a Compilation; instance. This is by design; the driver expects that both of these; phases are platform neutral, with a few very well defined exceptions; such as whether the platform uses a driver driver. ToolChain Argument Translation; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. In order to match gcc very closely, the clang driver currently allows; tool chains to perform their own translation of the argument list (into; a new ArgList data structure). Although this allows the clang driver to; match gcc easily, it also makes the driver operation much harder to; understand (since the Tools stop seeing some arguments the user; provided, and see new ones instead). For example, on Darwin ``-gfull`` gets translated into two separate; arguments, ``-g`` and ``-fno-eliminate-unused-debug-symbols``. Trying to; write Tool logic to do something with ``-gfull`` will not work, because; Tool argument translation is done after the arguments have been; translated. A long term goal is to remove this tool chain specific translation, and; instead force each tool to change its own logic to do the right thing on; the untranslated original arguments. Unused Argument Warnings; ^^^^^^^^^^^^^^^^^^^^^^^^. The driver operates by parsing all arguments but giving Tools the; opportunity to choose which arguments to pass on. One downside of this; infrastructure is that if the u",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/DriverInternals.rst:13131,perform,perform,13131,interpreter/llvm-project/clang/docs/DriverInternals.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/DriverInternals.rst,1,['perform'],['perform']
Performance,"r displayed objects; 10. Fix errors with scaling of axis labels.; 11. Support also Y axis with custom labels like: http://jsroot.gsi.de/dev/?nobrowser&file=../files/atlas.root&item=LEDShapeHeightCorr_Gain0;1&opt=col. ## Changes in 3.7; 1. Support of X axis with custom labels like: http://jsroot.gsi.de/dev/?nobrowser&json=../files/hist_xlabels.json; 2. Extend functionality of JSROOT.addDrawFunc() function. One could register type-specific; `make_request` and `after_request` functions; `icon`, `prereq`, `script`, `monitor` properties.; This let add more custom elements to the generic gui, implemented with JSROOT.HierarchyPainter; 3. Provide full support of require.js. One could load now JSRootCore.js script like:. <script type=""text/javascript"" src=""require.js"" data-main=""scripts/JSRootCore.js""></script>. After this several modules are defined and can be used with syntax like:. require(['JSRootPainter'], function(jsroot) { /*any user code*/});. Also inside JSROOT require.js used to load all dependencies. ## Changes in 3.6; 1. Try to provide workaround for websites where require.js already loaded.; This makes problem by direct loading of jquery and jquery-ui; 2. Provide workaround for older version of jquery-ui; 3. Prompt for input of command arguments; 4. After command execution one could automatically reload hierarchy (_hreload property) or; update view of displayed object (_update_item property); 5. Use HierarchyPainter for implementing draw.htm. This let us handle; all different kinds of extra attributes in central place; 6. Fix problem in tabs layout - new tab should be add to direct child; 7. When drawing several tabs, activate frame before drawing - only then; real frame size will be set; 8. Fix problem with GetBBox - it only can be used for visible elements in mozilla.; 9. Support drawing of fit parameters in stat box, use (as far as possible) stat and; fit format for statistic display; 10. Implement 'g' formatting kind for stat box output - one need to checks; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/js/changes.md:63810,load,load,63810,js/changes.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/js/changes.md,1,['load'],['load']
Performance,"r driver invokes link time optimizer separately.**; In this model the link time optimizer is not able to take advantage of; information collected during the linker's normal symbol resolution phase.; In the above example, the optimizer can not remove ``foo2()`` without the; linker's input because it is externally visible. This in turn prohibits the; optimizer from removing ``foo3()``. **Use separate tool to collect symbol information from all object files.**; In this model, a new, separate, tool or library replicates the linker's; capability to collect information for link time optimization. Not only is; this code duplication difficult to justify, but it also has several other; disadvantages. For example, the linking semantics and the features provided; by the linker on various platform are not unique. This means, this new tool; needs to support all such features and platforms in one super tool or a; separate tool per platform is required. This increases maintenance cost for; link time optimizer significantly, which is not necessary. This approach; also requires staying synchronized with linker developments on various; platforms, which is not the main focus of the link time optimizer. Finally,; this approach increases end user's build time due to the duplication of work; done by this separate tool and the linker itself. Multi-phase communication between ``libLTO`` and linker; =======================================================. The linker collects information about symbol definitions and uses in various; link objects which is more accurate than any information collected by other; tools during typical build cycles. The linker collects this information by; looking at the definitions and uses of symbols in native .o files and using; symbol visibility information. The linker also uses user-supplied information,; such as a list of exported symbols. LLVM optimizer collects control flow; information, data flow information and knows much more about program structure; fro",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:4255,optimiz,optimizer,4255,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,1,['optimiz'],['optimizer']
Performance,"r every event, whereas integrals are only evaluated when needed. The exception to the first type are pdfs; that only depend on constant parameters (or no parameters) - these are identified at the beginning, and precalculated once ; to avoid recalculating an expression with the same outcome for every iteration of the likelihood calculation. For composite pdfs a further optimization has been included: for a M(x,a,b) = f*F(x,a)+(1-f)G(x,b) ; it is e.g. not needed to recalculate G(x,b) if only parameter a has changed w.r.t to the previous likelihood; calculation. This optimization is now implemented by extending the value caching originally designed; for constant terms to be usable for non-constant terms, with a check executed at the beginning of each; likelihood evaluation if selected columns need to be updated because parameters have changed. The speed gain; of this optimization depends much on the structure of the pdf: in models with many free parameters most of the; likelihood evaluations are executed when MINUIT calculates numerical likelihood derivatives which vary ; one parameter at a time and the speedup is potentially larger. In models with few free parameters the; effect will be smaller. The new per-component caching strategy is enabled by default for all pdfs that are a component of; a RooAddPdf or a RooRealSumPdf, unless that component is a RooProdPdf or a RooProduct, in that; case the components of the product are cached instead of the product itself. You can disable this; new optimization by adding Optimize(1) to the RooAbsPdf::fitTo() command line (0 = no caching,; 1 = cache constant terms only, 2 = cache also variable terms according to above mentioned strategy (DEFAULT)). It is also possible to tune this 'cache-and-track' optimization to perform a more fine-grained caching; of components than Optimize(2) implements: to do so, call arg->setAttribute(""CacheAndTrack"") on each; pdf component that you'd like to be cache-and-tracked individually. New pdf/data",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v532/index.html:2849,optimiz,optimization,2849,roofit/doc/v532/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v532/index.html,2,['optimiz'],['optimization']
Performance,"r example in this section is compiled, result of the compilation will be:. .. code-block:: llvm. define i32 @main() {; entry:; tail call void @print(i32 4); tail call void @print(i32 5); tail call void @print(i32 6); ret i32 0; }. .. _final:; .. _final suspend:. Final Suspend; -------------. A coroutine author or a frontend may designate a particular suspend to be final,; by setting the second argument of the `coro.suspend`_ intrinsic to `true`.; Such a suspend point has two properties:. * it is possible to check whether a suspended coroutine is at the final suspend; point via `coro.done`_ intrinsic;. * a resumption of a coroutine stopped at the final suspend point leads to; undefined behavior. The only possible action for a coroutine at a final; suspend point is destroying it via `coro.destroy`_ intrinsic. From the user perspective, the final suspend point represents an idea of a; coroutine reaching the end. From the compiler perspective, it is an optimization; opportunity for reducing number of resume points (and therefore switch cases) in; the resume function. The following is an example of a function that keeps resuming the coroutine; until the final suspend point is reached after which point the coroutine is; destroyed:. .. code-block:: llvm. define i32 @main() {; entry:; %hdl = call ptr @f(i32 4); br label %while; while:; call void @llvm.coro.resume(ptr %hdl); %done = call i1 @llvm.coro.done(ptr %hdl); br i1 %done, label %end, label %while; end:; call void @llvm.coro.destroy(ptr %hdl); ret i32 0; }. Usually, final suspend point is a frontend injected suspend point that does not; correspond to any explicitly authored suspend point of the high level language.; For example, for a Python generator that has only one suspend point:. .. code-block:: python. def coroutine(n):; for i in range(n):; yield i. Python frontend would inject two more suspend points, so that the actual code; looks like this:. .. code-block:: c. void* coroutine(int n) {; int current_value;; <des",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Coroutines.rst:25179,optimiz,optimization,25179,interpreter/llvm-project/llvm/docs/Coroutines.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Coroutines.rst,1,['optimiz'],['optimization']
Performance,"r filePath[100];; SearchPath(NULL, ""file.dll"", NULL, 100, filePath, NULL);; return LoadLibrary(filePath); // warn; }. WinAPI.WideCharToMultiByte; (C); Buffer overrun while calling WideCharToMultiByte(). The size of; the input buffer equals the number of characters in the Unicode string, while; the size of the output buffer equals the number of bytes.; Source: ; MSDN: WideCharToMultiByte function. #include <windows.h>. void test() {; wchar_t ws[] = L""abc"";; char s[3];; WideCharToMultiByte(CP_UTF8, 0, ws, -1, s,; 3, NULL, NULL); // warn; }. optimization. Name, DescriptionExampleProgress. optimization.PassConstObjByValue; (C, C++); Optimization: It is more effective to pass constant parameter by reference to; avoid unnecessary object copying. struct A {};. void f(const struct A a); // warn. optimization.PostfixIncIter; (C++); Optimization: It is more effective to use prefix increment operator with; iterator.; Source: Scott Meyers ""More Effective C++"", item 6:; Distinguish between prefix and postfix forms of increment and decrement; operators. #include <vector>. void test() {; std::vector<int> v;; std::vector<int>::const_iterator it;; for(it = v.begin();; it != v.end(); it++) {}; // warn; }. optimization.MultipleCallsStrlen; (C); Optimization: multiple calls to strlen() for a string in an; expression. It is more effective to hold a value returned; from strlen() in a temporary variable. #include <string.h>. void test(const char* s) {; if (strlen(s) > 0 &&; strlen(s) < 7) {}; // warn; }. optimization.StrLengthCalculation; (C++); Optimization: it is more efficient to use string::length() to; calculate the length of an std::string. #include <string>; #include <string.h>. void test() {; std::string s;; if (strlen(s.c_str()) != 0) {}; // warn; }. optimization.EmptyContainerDetect; (C++); Optimization: It is more efficient to use containers empty(); method to identify an empty container. #include <list>. void test() {; std::list<int> l;; if (l.size() != 0) {}; // warn; }. ; . ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/analyzer/potential_checkers.html:27878,optimiz,optimization,27878,interpreter/llvm-project/clang/www/analyzer/potential_checkers.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/analyzer/potential_checkers.html,9,"['Optimiz', 'optimiz']","['Optimization', 'optimization']"
Performance,"r for the load. The second operand is the stride; value expressed in bytes. The third operand is a vector of boolean values; with the same number of elements as the return type. The fourth is the explicit; vector length of the operation. The base pointer underlying type matches the type of the scalar; elements of the return operand. The :ref:`align <attr_align>` parameter attribute can be provided for the first; operand. Semantics:; """""""""""""""""""". The '``llvm.experimental.vp.strided.load``' intrinsic loads, into a vector, multiple scalar; values from memory in the same way as the :ref:`llvm.vp.gather <int_vp_gather>` intrinsic,; where the vector of pointers is in the form:. ``%ptrs = <%ptr, %ptr + %stride, %ptr + 2 * %stride, ... >``,. with '``ptr``' previously casted to a pointer '``i8``', '``stride``' always interpreted as a signed; integer and all arithmetic occurring in the pointer type. Examples:; """""""""""""""""". .. code-block:: text. 	 %r = call <8 x i64> @llvm.experimental.vp.strided.load.v8i64.i64(i64* %ptr, i64 %stride, <8 x i64> %mask, i32 %evl); 	 ;; The operation can also be expressed like this:. 	 %addr = bitcast i64* %ptr to i8*; 	 ;; Create a vector of pointers %addrs in the form:; 	 ;; %addrs = <%addr, %addr + %stride, %addr + 2 * %stride, ...>; 	 %ptrs = bitcast <8 x i8* > %addrs to <8 x i64* >; 	 %also.r = call <8 x i64> @llvm.vp.gather.v8i64.v8p0i64(<8 x i64* > %ptrs, <8 x i64> %mask, i32 %evl). .. _int_experimental_vp_strided_store:. '``llvm.experimental.vp.strided.store``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare void @llvm.experimental.vp.strided.store.v4f32.i64(<4 x float> %val, ptr %ptr, i64 %stride, <4 x i1> %mask, i32 %evl); declare void @llvm.experimental.vp.strided.store.nxv2i16.i64(<vscale x 2 x i16> %val, ptr %ptr, i64 %stride, <vscale x 2 x i1> %mask, i32 %evl). Overview:; """""""""""""""""". The '``@llvm.experimental.vp.strided.store``' intrinsic stores the elements of",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:788713,load,load,788713,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,"r is dependent on the track; parameters, which is a highly undesirable effect. B) We will call ***`overlaps`*** only the regions in space contained by; more than one node inside the same container. The owner of such regions; cannot be determined based on hierarchical considerations; therefore; they will be considered as belonging to the node from which the current; track is coming from. When coming from their container, the ownership is totally; unpredictable. Again, the ownership of overlapping regions highly; depends on the current track parameters. We must say that even the overlaps of type A) and B) are allowed in case; the corresponding nodes are created using; **`TGeoVolume`**`::AddNodeOverlap()` method. Navigation is performed in such; cases by giving priority to the non-overlapping nodes. The modeller has; to perform an additional search through the overlapping candidates.; These are detected automatically during the geometry closing procedure; in order to optimize the algorithm, but we will stress that extensive; usage of this feature leads to a drastic deterioration of performance.; In the following we will focus on the non-declared overlaps of type A); and B) since this is the main source of errors during tracking. These; are generally non-intended overlaps due to coding mistakes or bad; geometry design. The checking package is loaded together with the; painter classes and contains an automated overlap checker.**. ![Overlap checking](pictures/030001DF.png). This can be activated both at volume level (checking for illegal; overlaps only one level inside a given volume) and from the geometry; manager level (checking full geometry):. ``` {.cpp}; myVolume->CheckOverlaps(precision, option);; gGeoManager->CheckOverlaps(precision);; myNode->CheckOverlaps(precision);; ```. Here precision represents the desired maximum accepted overlap value in; centimeters (default value is 0.1). This tool checks all possible; significant pairs of candidates inside a given volume ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md:131895,optimiz,optimize,131895,documentation/users-guide/Geometry.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md,2,"['optimiz', 'perform']","['optimize', 'performance']"
Performance,"r methods ``removeBranch`` and ``insertBranch`` to manage; subsequent operations. ``analyzeBranch`` should return false indicating success in most circumstances.; ``analyzeBranch`` should only return true when the method is stumped about what; to do, for example, if a block has three terminating branches.; ``analyzeBranch`` may return true if it encounters a terminator it cannot; handle, such as an indirect branch. .. _instruction-selector:. Instruction Selector; ====================. LLVM uses a ``SelectionDAG`` to represent LLVM IR instructions, and nodes of; the ``SelectionDAG`` ideally represent native target instructions. During code; generation, instruction selection passes are performed to convert non-native; DAG instructions into native target-specific instructions. The pass described; in ``XXXISelDAGToDAG.cpp`` is used to match patterns and perform DAG-to-DAG; instruction selection. Optionally, a pass may be defined (in; ``XXXBranchSelector.cpp``) to perform similar DAG-to-DAG operations for branch; instructions. Later, the code in ``XXXISelLowering.cpp`` replaces or removes; operations and data types not supported natively (legalizes) in a; ``SelectionDAG``. TableGen generates code for instruction selection using the following target; description input files:. * ``XXXInstrInfo.td`` --- Contains definitions of instructions in a; target-specific instruction set, generates ``XXXGenDAGISel.inc``, which is; included in ``XXXISelDAGToDAG.cpp``. * ``XXXCallingConv.td`` --- Contains the calling and return value conventions; for the target architecture, and it generates ``XXXGenCallingConv.inc``,; which is included in ``XXXISelLowering.cpp``. The implementation of an instruction selection pass must include a header that; declares the ``FunctionPass`` class or a subclass of ``FunctionPass``. In; ``XXXTargetMachine.cpp``, a Pass Manager (PM) should add each instruction; selection pass into the queue of passes to run. The LLVM static compiler (``llc``) is an excellent ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMBackend.rst:52543,perform,perform,52543,interpreter/llvm-project/llvm/docs/WritingAnLLVMBackend.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMBackend.rst,1,['perform'],['perform']
Performance,"r not the ``Function`` has a body defined. If the function is; ""external"", it does not have a body, and thus must be resolved by linking with; a function defined in a different translation unit. * | ``Function::iterator`` - Typedef for basic block list iterator; | ``Function::const_iterator`` - Typedef for const_iterator.; | ``begin()``, ``end()``, ``size()``, ``empty()``, ``insert()``,; ``splice()``, ``erase()``. These are forwarding methods that make it easy to access the contents of a; ``Function`` object's BasicBlock_ list. * | ``Function::arg_iterator`` - Typedef for the argument list iterator; | ``Function::const_arg_iterator`` - Typedef for const_iterator.; | ``arg_begin()``, ``arg_end()``, ``arg_size()``, ``arg_empty()``. These are forwarding methods that make it easy to access the contents of a; ``Function`` object's Argument_ list. * ``Function::ArgumentListType &getArgumentList()``. Returns the list of Argument_. This is necessary to use when you need to; update the list or perform a complex action that doesn't have a forwarding; method. * ``BasicBlock &getEntryBlock()``. Returns the entry ``BasicBlock`` for the function. Because the entry block; for the function is always the first block, this returns the first block of; the ``Function``. * | ``Type *getReturnType()``; | ``FunctionType *getFunctionType()``. This traverses the Type_ of the ``Function`` and returns the return type of; the function, or the FunctionType_ of the actual function. * ``SymbolTable *getSymbolTable()``. Return a pointer to the SymbolTable_ for this ``Function``. .. _GlobalVariable:. The ``GlobalVariable`` class; ----------------------------. ``#include ""llvm/IR/GlobalVariable.h""``. header source: `GlobalVariable.h; <https://llvm.org/doxygen/GlobalVariable_8h_source.html>`_. doxygen info: `GlobalVariable Class; <https://llvm.org/doxygen/classllvm_1_1GlobalVariable.html>`_. Superclasses: GlobalValue_, Constant_, User_, Value_. Global variables are represented with the (surprise surpr",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ProgrammersManual.rst:157717,perform,perform,157717,interpreter/llvm-project/llvm/docs/ProgrammersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ProgrammersManual.rst,1,['perform'],['perform']
Performance,"r own init-like methods that do not; follow the standard Cocoa naming conventions.; Example. #ifndef __has_feature; #define __has_feature(x) 0 // Compatibility with non-clang compilers.; #endif. #ifndef NS_CONSUMES_SELF; #if __has_feature((attribute_ns_consumes_self)); #define NS_CONSUMES_SELF __attribute__((ns_consumes_self)); #else; #define NS_CONSUMES_SELF; #endif; #endif. @interface MyClass : NSObject; - initWith:(MyClass *)x;; - nonstandardInitWith:(MyClass *)x NS_CONSUMES_SELF NS_RETURNS_RETAINED;; @end. In this example, -nonstandardInitWith: has the same ownership; semantics as the init method -initWith:. The static analyzer will; observe that the method consumes the receiver, and then returns an object with; a +1 retain count.; The Foundation framework defines a macro NS_REPLACES_RECEIVER; which is functionally equivalent to the combination of NS_CONSUMES_SELF; and NS_RETURNS_RETAINED shown above.; Libkern Memory Management Annotations; Libkern; requires developers to inherit all heap allocated objects from OSObject; and to perform manual reference counting.; The reference counting model is very similar to MRR (manual retain-release) mode in; Objective-C; or to CoreFoundation reference counting.; Freshly-allocated objects start with a reference count of 1,; and calls to retain increment it,; while calls to release decrement it.; The object is deallocated whenever its reference count reaches zero.; Manually incrementing and decrementing reference counts is error-prone:; over-retains lead to leaks, and over-releases lead to uses-after-free.; The analyzer can help the programmer to check for unbalanced; retain/release calls.; The reference count checking is based on the principle of; locality: it should be possible to establish correctness; (lack of leaks/uses after free) by looking at each function body,; and the declarations (not the definitions) of all the functions it interacts; with.; In order to support such reasoning, it should be possible to summarize; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/analyzer/annotations.html:13775,perform,perform,13775,interpreter/llvm-project/clang/www/analyzer/annotations.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/analyzer/annotations.html,2,['perform'],['perform']
Performance,"r possible. #. Tag function parameters with the noundef attribute whenever possible. Modeling Memory Effects; ^^^^^^^^^^^^^^^^^^^^^^^^. #. Mark functions as readnone/readonly/argmemonly or noreturn/nounwind when; known. The optimizer will try to infer these flags, but may not always be; able to. Manual annotations are particularly important for external; functions that the optimizer can not analyze. #. Use the lifetime.start/lifetime.end and invariant.start/invariant.end; intrinsics where possible. Common profitable uses are for stack like data; structures (thus allowing dead store elimination) and for describing; life times of allocas (thus allowing smaller stack sizes). #. Mark invariant locations using !invariant.load and TBAA's constant flags. Pass Ordering; ^^^^^^^^^^^^^. One of the most common mistakes made by new language frontend projects is to; use the existing -O2 or -O3 pass pipelines as is. These pass pipelines make a; good starting point for an optimizing compiler for any language, but they have; been carefully tuned for C and C++, not your target language. You will almost; certainly need to use a custom pass order to achieve optimal performance. A; couple specific suggestions:. #. For languages with numerous rarely executed guard conditions (e.g. null; checks, type checks, range checks) consider adding an extra execution or; two of LoopUnswitch and LICM to your pass order. The standard pass order,; which is tuned for C and C++ applications, may not be sufficient to remove; all dischargeable checks from loops. #. If your language uses range checks, consider using the IRCE pass. It is not; currently part of the standard pass order. #. A useful sanity check to run is to run your optimized IR back through the; -O2 pipeline again. If you see noticeable improvement in the resulting IR,; you likely need to adjust your pass order. I Still Can't Find What I'm Looking For; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. If you didn't find what you were looking for above",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Frontend/PerformanceTips.rst:12549,optimiz,optimizing,12549,interpreter/llvm-project/llvm/docs/Frontend/PerformanceTips.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Frontend/PerformanceTips.rst,2,"['optimiz', 'tune']","['optimizing', 'tuned']"
Performance,"r shared variables. Notes for frontends; This cannot be used for synchronization, but is useful for Java and other; ""safe"" languages which need to guarantee that the generated code never; exhibits undefined behavior. Note that this guarantee is cheap on common; platforms for loads of a native width, but can be expensive or unavailable for; wider loads, like a 64-bit store on ARM. (A frontend for Java or other ""safe""; languages would normally split a 64-bit store on ARM into two 32-bit unordered; stores.). Notes for optimizers; In terms of the optimizer, this prohibits any transformation that transforms a; single load into multiple loads, transforms a store into multiple stores,; narrows a store, or stores a value which would not be stored otherwise. Some; examples of unsafe optimizations are narrowing an assignment into a bitfield,; rematerializing a load, and turning loads and stores into a memcpy; call. Reordering unordered operations is safe, though, and optimizers should; take advantage of that because unordered operations are common in languages; that need them. Notes for code generation; These operations are required to be atomic in the sense that if you use; unordered loads and unordered stores, a load cannot see a value which was; never stored. A normal load or store instruction is usually sufficient, but; note that an unordered load or store cannot be split into multiple; instructions (or an instruction which does multiple memory operations, like; ``LDRD`` on ARM without LPAE, or not naturally-aligned ``LDRD`` on LPAE ARM). Monotonic; ---------. Monotonic is the weakest level of atomicity that can be used in synchronization; primitives, although it does not provide any general synchronization. It; essentially guarantees that if you take all the operations affecting a specific; address, a consistent ordering exists. Relevant standard; This corresponds to the C++/C ``memory_order_relaxed``; see those; standards for the exact definition. Notes for frontends; If",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst:9440,optimiz,optimizers,9440,interpreter/llvm-project/llvm/docs/Atomics.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst,1,['optimiz'],['optimizers']
Performance,"r the enabled sanitizers.; See :doc:`SanitizerStats` for more details. .. option:: -fsanitize-undefined-trap-on-error. Deprecated alias for ``-fsanitize-trap=undefined``. .. option:: -fsanitize-cfi-cross-dso. Enable cross-DSO control flow integrity checks. This flag modifies; the behavior of sanitizers in the ``cfi`` group to allow checking; of cross-DSO virtual and indirect calls. .. option:: -fsanitize-cfi-icall-generalize-pointers. Generalize pointers in return and argument types in function type signatures; checked by Control Flow Integrity indirect call checking. See; :doc:`ControlFlowIntegrity` for more details. .. option:: -fsanitize-cfi-icall-experimental-normalize-integers. Normalize integers in return and argument types in function type signatures; checked by Control Flow Integrity indirect call checking. See; :doc:`ControlFlowIntegrity` for more details. This option is currently experimental. .. option:: -fstrict-vtable-pointers. Enable optimizations based on the strict rules for overwriting polymorphic; C++ objects, i.e. the vptr is invariant during an object's lifetime.; This enables better devirtualization. Turned off by default, because it is; still experimental. .. option:: -fwhole-program-vtables. Enable whole-program vtable optimizations, such as single-implementation; devirtualization and virtual constant propagation, for classes with; :doc:`hidden LTO visibility <LTOVisibility>`. Requires ``-flto``. .. option:: -f[no]split-lto-unit. Controls splitting the :doc:`LTO unit <LTOVisibility>` into regular LTO and; :doc:`ThinLTO` portions, when compiling with -flto=thin. Defaults to false; unless ``-fsanitize=cfi`` or ``-fwhole-program-vtables`` are specified, in; which case it defaults to true. Splitting is required with ``fsanitize=cfi``,; and it is an error to disable via ``-fno-split-lto-unit``. Splitting is; optional with ``-fwhole-program-vtables``, however, it enables more; aggressive whole program vtable optimizations (specifically virtual const",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:80249,optimiz,optimizations,80249,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,1,['optimiz'],['optimizations']
Performance,"r the previous example, we have:. .. code-block:: llvm. !nvvm.annotations = !{!0}; !0 = !{void (float addrspace(1)*,; float addrspace(1)*,; float addrspace(1)*)* @kernel, !""kernel"", i32 1}. Here, we have a single metadata declaration in ``nvvm.annotations``. This; metadata annotates our ``@kernel`` function with the ``kernel`` attribute. Running the Kernel; ------------------. Generating PTX from LLVM IR is all well and good, but how do we execute it on; a real GPU device? The CUDA Driver API provides a convenient mechanism for; loading and JIT compiling PTX to a native GPU device, and launching a kernel.; The API is similar to OpenCL. A simple example showing how to load and; execute our vector addition code is shown below. Note that for brevity this; code does not perform much error checking!. .. note::. You can also use the ``ptxas`` tool provided by the CUDA Toolkit to offline; compile PTX to machine code (SASS) for a specific GPU architecture. Such; binaries can be loaded by the CUDA Driver API in the same way as PTX. This; can be useful for reducing startup time by precompiling the PTX kernels. .. code-block:: c++. #include <iostream>; #include <fstream>; #include <cassert>; #include ""cuda.h"". void checkCudaErrors(CUresult err) {; assert(err == CUDA_SUCCESS);; }. /// main - Program entry point; int main(int argc, char **argv) {; CUdevice device;; CUmodule cudaModule;; CUcontext context;; CUfunction function;; CUlinkState linker;; int devCount;. // CUDA initialization; checkCudaErrors(cuInit(0));; checkCudaErrors(cuDeviceGetCount(&devCount));; checkCudaErrors(cuDeviceGet(&device, 0));. char name[128];; checkCudaErrors(cuDeviceGetName(name, 128, device));; std::cout << ""Using CUDA Device [0]: "" << name << ""\n"";. int devMajor, devMinor;; checkCudaErrors(cuDeviceComputeCapability(&devMajor, &devMinor, device));; std::cout << ""Device Compute Capability: ""; << devMajor << ""."" << devMinor << ""\n"";; if (devMajor < 2) {; std::cerr << ""ERROR: Device 0 is not SM 2.0 or g",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:19420,load,loaded,19420,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,1,['load'],['loaded']
Performance,"r to find out this new state, one; has to propagate the point with a distance slightly bigger that the; computed step value (which is accurate within numerical precision). A; method that performs this task finding the next location is; TGeoManager::Step(), described in ""Making a Step"", but users may; implement more precise methods to insure post-step boundary crossing. \anchor GP08; ## Geometry Graphical User Interface. The geombuilder package allows you to create and edit geometries. The; package provides a library of all GUI classes related to geometry. Each; editable geometry class `TGeoXXX` have a correspondent editor; `TGeoXXXEditor` that provides a graphics user interface allowing to; edit some (or all) parameters of a geometry object. The editable objects; are geometry manager, volumes, nodes, shapes, media, materials and; matrices. The interfaces provide also access to specific functionality; of geometry objects. The editing mechanism is based on ROOT GED; (Graphics Editors) functionality and the library is loaded using the; plug-in mechanism. \anchor GP08a; ### Editing a Geometry. There are two different use cases having different ways of invoking the; geometry editors. The first one applies when starting with geometry from; scratch and using the builder functionality to create new geometry; objects. In this case, one should use the sequence:. ~~~{.cpp}; root[] TGeoManager *geom = new TGeoManager(""MyGeom"",; ""Test builder"");; root[] geom->Edit(Option_t *option="""");; ~~~. The lines above will create a new TGeoManager class, create an; empty canvas and start the editor in the left-sided editor frame; attached to the canvas. To open the editor in a separate frame one; should provide a non-empty string as option to the `Edit()` method. \image html geometry018.png ""The geometry manager editor"". \anchor GP08b; ### The Geometry Manager Editor. \image html geometry019.png ""Accessing/creating different categories of editable objects"" width=600px. The second use case a",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md:126371,load,loaded,126371,geom/geom/doc/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md,1,['load'],['loaded']
Performance,"r values and errors from; the Fit configuration class (ROOT::Fit::FitConfig).; This required adding a nw constructor of FitResult from FitConfig.; This originated from the Savannah request. ; Add also new methods Fitter::SetFCN.; Update the configuration (parameter values and errors) after a fit with the FitResult values; So next fit will use improved parameter values and errors. This update can be switched on/off; by using FitConfig::SetUpdateAfterFit(on/off). By default is on.; Add new method FitConfig::SetFromFitResult.; Add possibility to run Hesse (Fitter:::CalculateHessErrors) without having done the minimization. Add support for weighted likelihood fits. Add a new method Fitter::ApplyWeightCorrection(fcn2); which corrects covariance matrix for the weights using the likelihood function built using the weight square; Add the support for weights for the binned Poisson likelihood fits (in the; ROOT::Fit::PoissonLikelihoodFCN class). A new option (WL) has been added also in TH1::Fit; for performing weighted fits of histograms (see ).; . ROOT::Math::Minimizer; Add new methods Minimizer::GetHessianMatrix(double * mat) and Minimizer::GetCovMatrix(double * mat) to return the full; matrices by filling the passed C arrays, which must have a dimension of at least n x n, where n is the; total number of parameters. The elements for the fixed parameters will be filled with zeros.; These methods are currently implemented by only Minuit and Minuit2. Change default tolerance in ROOT::Math::MinimizerOptions to be 0.01 from 0.0001. MathMore. New class ROOT::Math::GSLMultiRootFinder for finding the root of system of functions.; The class is based on the GSL multi-root algorithm; (see the GSL online; manual) and it is used to solve a non-linear system of equations:; ; f1(x1,....xn) = 0; f2(x1,....xn) = 0; ..................; fn(x1,....xn) = 0. The available GSL algorithms require the derivatives of the supplied functions or not (they are; computed internally by GSL). In the first ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/math/doc/v530/index.html:2361,perform,performing,2361,math/doc/v530/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/math/doc/v530/index.html,2,['perform'],['performing']
Performance,"r vector stores). Implementation; ==============. There are 3 parts to the implementation:. 1. Predicate ``LDR`` and ``STR`` instructions so that they are never allowed to be selected to generate vector loads and stores. The exception is one-lane vectors [1]_ - these by definition cannot have lane ordering problems so are fine to use ``LDR``/``STR``. 2. Create code generation patterns for bitconverts that create ``REV`` instructions. 3. Make sure appropriate bitconverts are created so that vector values get passed over call boundaries as 1-element vectors (which is the same as if they were loaded with ``LDR``). Bitconverts; -----------. .. image:: ARM-BE-bitcastfail.png; :align: right. The main problem with the ``LD1`` solution is dealing with bitconverts (or bitcasts, or reinterpret casts). These are pseudo instructions that only change the compiler's interpretation of data, not the underlying data itself. A requirement is that if data is loaded and then saved again (called a ""round trip""), the memory contents should be the same after the store as before the load. If a vector is loaded and is then bitconverted to a different vector type before storing, the round trip will currently be broken. Take for example this code sequence::. %0 = load <4 x i32> %x; %1 = bitcast <4 x i32> %0 to <2 x i64>; store <2 x i64> %1, <2 x i64>* %y. This would produce a code sequence such as that in the figure on the right. The mismatched ``LD1`` and ``ST1`` cause the stored data to differ from the loaded data. .. container:: clearer. When we see a bitcast from type ``X`` to type ``Y``, what we need to do is to change the in-register representation of the data to be *as if* it had just been loaded by a ``LD1`` of type ``Y``. .. image:: ARM-BE-bitcastsuccess.png; :align: right. Conceptually this is simple - we can insert a ``REV`` undoing the ``LD1`` of type ``X`` (converting the in-register representation to the same as if it had been loaded by ``LDR``) and then insert another ``REV`` to",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BigEndianNEON.rst:10090,load,loaded,10090,interpreter/llvm-project/llvm/docs/BigEndianNEON.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BigEndianNEON.rst,2,['load'],"['load', 'loaded']"
Performance,"r(std::move(FPM)));. Generally you want to group CGSCC/function/loop passes together in a pass; manager, as opposed to adding adaptors for each pass to the containing upper; level pass manager. For example,. .. code-block:: c++. ModulePassManager MPM;; MPM.addPass(createModuleToFunctionPassAdaptor(FunctionPass1()));; MPM.addPass(createModuleToFunctionPassAdaptor(FunctionPass2()));; MPM.run();. will run ``FunctionPass1`` on each function in a module, then run; ``FunctionPass2`` on each function in the module. In contrast,. .. code-block:: c++. ModulePassManager MPM;. FunctionPassManager FPM;; FPM.addPass(FunctionPass1());; FPM.addPass(FunctionPass2());. MPM.addPass(createModuleToFunctionPassAdaptor(std::move(FPM)));. will run ``FunctionPass1`` and ``FunctionPass2`` on the first function in a; module, then run both passes on the second function in the module, and so on.; This is better for cache locality around LLVM data structures. This similarly; applies for the other IR types, and in some cases can even affect the quality; of optimization. For example, running all loop passes on a loop may cause a; later loop to be able to be optimized more than if each loop pass were run; separately. Inserting Passes into Default Pipelines; =======================================. Rather than manually adding passes to a pass manager, the typical way of; creating a pass manager is to use a ``PassBuilder`` and call something like; ``PassBuilder::buildPerModuleDefaultPipeline()`` which creates a typical; pipeline for a given optimization level. Sometimes either frontends or backends will want to inject passes into the; pipeline. For example, frontends may want to add instrumentation, and target; backends may want to add passes that lower custom intrinsics. For these; cases, ``PassBuilder`` exposes callbacks that allow injecting passes into; certain parts of the pipeline. For example,. .. code-block:: c++. PassBuilder PB;; PB.registerPipelineStartEPCallback([&](ModulePassManager &MPM,",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:4782,optimiz,optimization,4782,interpreter/llvm-project/llvm/docs/NewPassManager.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst,1,['optimiz'],['optimization']
Performance,"r, ``MachineFunctionPass``\ es; are not allowed to do any of the following:. #. Modify or create any LLVM IR ``Instruction``\ s, ``BasicBlock``\ s,; ``Argument``\ s, ``Function``\ s, ``GlobalVariable``\ s,; ``GlobalAlias``\ es, or ``Module``\ s.; #. Modify a ``MachineFunction`` other than the one currently being processed.; #. Maintain state across invocations of :ref:`runOnMachineFunction; <writing-an-llvm-pass-runOnMachineFunction>` (including global data). .. _writing-an-llvm-pass-runOnMachineFunction:. The ``runOnMachineFunction(MachineFunction &MF)`` method; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. .. code-block:: c++. virtual bool runOnMachineFunction(MachineFunction &MF) = 0;. ``runOnMachineFunction`` can be considered the main entry point of a; ``MachineFunctionPass``; that is, you should override this method to do the; work of your ``MachineFunctionPass``. The ``runOnMachineFunction`` method is called on every ``MachineFunction`` in a; ``Module``, so that the ``MachineFunctionPass`` may perform optimizations on; the machine-dependent representation of the function. If you want to get at; the LLVM ``Function`` for the ``MachineFunction`` you're working on, use; ``MachineFunction``'s ``getFunction()`` accessor method --- but remember, you; may not modify the LLVM ``Function`` or its contents from a; ``MachineFunctionPass``. .. _writing-an-llvm-pass-registration:. Pass registration; -----------------. In the :ref:`Hello World <writing-an-llvm-pass-basiccode>` example pass we; illustrated how pass registration works, and discussed some of the reasons that; it is used and what it does. Here we discuss how and why passes are; registered. As we saw above, passes are registered with the ``RegisterPass`` template. The; template parameter is the name of the pass that is to be used on the command; line to specify that the pass should be added to a program (for example, with; :program:`opt` or :program:`bugpoint`). The first argument is the name of the;",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst:26338,perform,perform,26338,interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst,2,"['optimiz', 'perform']","['optimizations', 'perform']"
Performance,"r, label %exit. loop.preheader:; br label %body. body:; %i2 = phi i32 [ 0, %loop.preheader ], [ %i.next, %latch ]; br label %latch. latch:; %i.next = add nsw i32 %i2, 1; %cond = icmp slt i32 %i.next, %n; br i1 %cond, label %body, label %loop.exit. loop.exit:; br label %exit. exit:; ret void; }. .. image:: ./loop-terminology-guarded-loop.png; :width: 500 px. The result is a little bit more complicated than we may expect; because LoopRotate ensures that the loop is in; :ref:`Loop Simplify Form <loop-terminology-loop-simplify>`; after rotation.; In this case, it inserted the %loop.preheader basic block so; that the loop has a preheader and it introduced the %loop.exit; basic block so that the loop has dedicated exits; (otherwise, %exit would be jumped from both %latch and %entry,; but %entry is not contained in the loop).; Note that a loop has to be in Loop Simplify Form beforehand; too for LoopRotate to be applied successfully. The main advantage of this form is that it allows hoisting; invariant instructions, especially loads, into the preheader.; That could be done in non-rotated loops as well but with; some disadvantages. Let's illustrate them with an example:. .. code-block:: C. for (int i = 0; i < n; ++i) {; auto v = *p;; use(v);; }. We assume that loading from p is invariant and use(v) is some; statement that uses v.; If we wanted to execute the load only once we could move it; ""out"" of the loop body, resulting in this:. .. code-block:: C. auto v = *p;; for (int i = 0; i < n; ++i) {; use(v);; }. However, now, in the case that n <= 0, in the initial form,; the loop body would never execute, and so, the load would; never execute. This is a problem mainly for semantic reasons.; Consider the case in which n <= 0 and loading from p is invalid.; In the initial program there would be no error. However, with this; transformation we would introduce one, effectively breaking; the initial semantics. To avoid both of these problems, we can insert a guard:. .. code-block:: C",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:22257,load,loads,22257,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst,1,['load'],['loads']
Performance,"r. If ``E1`` is a prvalue, the result is a prvalue with type; ``T`` and is the value of the element at the given row and column in the matrix.; Otherwise, the result is a glvalue with type ``cv T`` and with the same value; category as ``E1`` which refers to the element at the given row and column in; the matrix. Programs containing a single subscript expression into a matrix are ill-formed. **Note**: We considered providing an expression of the form; ``postfix-expression [expression]`` to access columns of a matrix. We think; that such an expression would be problematic once both column and row major; matrixes are supported: depending on the memory layout, either accessing columns; or rows can be done efficiently, but not both. Instead, we propose to provide; builtins to extract rows and columns from a matrix. This makes the operations; more explicit. Matrix Type Binary Operators; ----------------------------. Given two matrixes, the ``+`` and ``-`` operators perform element-wise addition; and subtraction, while the ``*`` operator performs matrix multiplication.; ``+``, ``-``, ``*``, and ``/`` can also be used with a matrix and a scalar; value, applying the operation to each element of the matrix. Earlier versions of this extension did not support division by a scalar.; You can test for the availability of this feature with; ``__has_extension(matrix_types_scalar_division)``. For the expression ``M1 BIN_OP M2`` where. * ``BIN_OP`` is one of ``+`` or ``-``, one of ``M1`` and ``M2`` is of matrix; type, and the other is of matrix type or real type; or; * ``BIN_OP`` is ``*``, one of ``M1`` and ``M2`` is of matrix type, and the; other is of a real type; or; * ``BIN_OP`` is ``/``, ``M1`` is of matrix type, and ``M2`` is of a real type:. * The usual arithmetic conversions are applied to ``M1`` and ``M2``. [ Note: if ``M1`` or; ``M2`` are of a real type, they are broadcast to matrices here. — end note ]; * ``M1`` and ``M2`` shall be of the same matrix type.; * The result is e",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/MatrixTypes.rst:5084,perform,perform,5084,interpreter/llvm-project/clang/docs/MatrixTypes.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/MatrixTypes.rst,2,['perform'],"['perform', 'performs']"
Performance,"r. This overrides the ``ssp`` and ``sspstrong`` function; attributes. Variables that are identified as requiring a protector will be arranged; on the stack such that they are adjacent to the stack protector guard.; The specific layout rules are:. #. Large arrays and structures containing large arrays; (``>= ssp-buffer-size``) are closest to the stack protector.; #. Small arrays and structures containing small arrays; (``< ssp-buffer-size``) are 2nd closest to the protector.; #. Variables that have had their address taken are 3rd closest to the; protector. If a function with an ``sspreq`` attribute is inlined into a calling; function which has an ``ssp`` or ``sspstrong`` attribute, the calling; function's attribute will be upgraded to ``sspreq``. ``strictfp``; This attribute indicates that the function was called from a scope that; requires strict floating-point semantics. LLVM will not attempt any; optimizations that require assumptions about the floating-point rounding; mode or that might alter the state of floating-point status flags that; might otherwise be set or cleared by calling this function. LLVM will; not introduce any new floating-point instructions that may trap. .. _denormal_fp_math:. ``""denormal-fp-math""``; This indicates the denormal (subnormal) handling that may be; assumed for the default floating-point environment. This is a; comma separated pair. The elements may be one of ``""ieee""``,; ``""preserve-sign""``, ``""positive-zero""``, or ``""dynamic""``. The; first entry indicates the flushing mode for the result of floating; point operations. The second indicates the handling of denormal inputs; to floating point instructions. For compatibility with older; bitcode, if the second value is omitted, both input and output; modes will assume the same mode. If this is attribute is not specified, the default is ``""ieee,ieee""``. If the output mode is ``""preserve-sign""``, or ``""positive-zero""``,; denormal outputs may be flushed to zero by standard floating-point; op",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:105749,optimiz,optimizations,105749,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['optimiz'],['optimizations']
Performance,"r/global/flat_atomic sc0=1; atomicrmw release - workgroup - local *If TgSplit execution mode,; local address space cannot; be used.*. 1. ds_atomic; atomicrmw release - agent - global 1. buffer_wbl2 sc1=1; - generic; - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at agent scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global and local; have completed; before performing; the atomicrmw that; is being released. 3. buffer/global/flat_atomic sc1=1; atomicrmw release - system - global 1. buffer_wbl2 sc0=1 sc1=1; - generic; - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at system scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to memory and the L2; writeback have; completed before; perfor",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:311647,perform,performing,311647,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['perform'],['performing']
Performance,"r/global/flat_store; - wavefront - generic; - workgroup; - agent; - system; store atomic monotonic - singlethread - local 1. ds_store; - wavefront; - workgroup; atomicrmw monotonic - singlethread - global 1. buffer/global/flat_atomic; - wavefront - generic; - workgroup; - agent; - system; atomicrmw monotonic - singlethread - local 1. ds_atomic; - wavefront; - workgroup; **Acquire Atomic**; ------------------------------------------------------------------------------------; load atomic acquire - singlethread - global 1. buffer/global/ds/flat_load; - wavefront - local; - generic; load atomic acquire - workgroup - global 1. buffer/global_load glc=1. - If CU wavefront execution; mode, omit glc=1. 2. s_waitcnt vmcnt(0). - If CU wavefront execution; mode, omit.; - Must happen before; the following buffer_gl0_inv; and before any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw. 3. buffer_gl0_inv. - If CU wavefront execution; mode, omit.; - Ensures that; following; loads will not see; stale data. load atomic acquire - workgroup - local 1. ds_load; 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; the following buffer_gl0_inv; and before any following; global/generic load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than the local load; atomic value being; acquired. 3. buffer_gl0_inv. - If CU wavefront execution; mode, omit.; - If OpenCL, omit.; - Ensures that; following; loads will not see; stale data. load atomic acquire - workgroup - generic 1. flat_load glc=1. - If CU wavefront execution; mode, omit glc=1. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If CU wavefront execution; mode, omit vmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Must happen before; the following; buffer_gl0_inv and any; following global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local load; atomic value being; acquired. 3. buffer_g",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:346624,load,loads,346624,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['loads']
Performance,"r/global_atomic; 4. s_waitcnt vmcnt(0). - Must happen before; following; buffer_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; cache. 5. buffer_inv sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acq_rel - system - global 1. buffer_wbl2 sc0=1 sc1=1. - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at system scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global and L2 writeback; have completed before; performing the; atomicrmw that is; being released. 3. buffer/global_atomic; sc1=1; 4. s_waitcnt vmcnt(0). - Must happen before; following; buffer_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; caches. 5. buffer_inv sc0=1 sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; MTYPE NC global data.; MTYPE RW and CC memory will; never be stale due to the; memory probes. atomicrmw acq_rel - agent - generic 1. buffer_wbl2 sc1=1. - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at agent scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgk",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:321452,load,load,321452,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"r1, r1, lsr #2; 	ldr r0, LCPI1_1; 	and r0, r1, r0; 	bx lr. it saves an instruction and a register. //===---------------------------------------------------------------------===//. It might be profitable to cse MOVi16 if there are lots of 32-bit immediates; with the same bottom half. //===---------------------------------------------------------------------===//. Robert Muth started working on an alternate jump table implementation that; does not put the tables in-line in the text. This is more like the llvm; default jump table implementation. This might be useful sometime. Several; revisions of patches are on the mailing list, beginning at:; http://lists.llvm.org/pipermail/llvm-dev/2009-June/022763.html. //===---------------------------------------------------------------------===//. Make use of the ""rbit"" instruction. //===---------------------------------------------------------------------===//. Take a look at test/CodeGen/Thumb2/machine-licm.ll. ARM should be taught how; to licm and cse the unnecessary load from cp#1. //===---------------------------------------------------------------------===//. The CMN instruction sets the flags like an ADD instruction, while CMP sets; them like a subtract. Therefore to be able to use CMN for comparisons other; than the Z bit, we'll need additional logic to reverse the conditionals; associated with the comparison. Perhaps a pseudo-instruction for the comparison,; with a post-codegen pass to clean up and handle the condition codes?; See PR5694 for testcase. //===---------------------------------------------------------------------===//. Given the following on armv5:; int test1(int A, int B) {; return (A&-8388481)|(B&8388480);; }. We currently generate:; 	ldr	r2, .LCPI0_0; 	and	r0, r0, r2; 	ldr	r2, .LCPI0_1; 	and	r1, r1, r2; 	orr	r0, r1, r0; 	bx	lr. We should be able to replace the second ldr+and with a bic (i.e. reuse the; constant which was already loaded). Not sure what's necessary to do that. //===--------------------------",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:17298,load,load,17298,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,2,['load'],['load']
Performance,"r8, %rax # Conditionally update predicate state.; addl (%rsi), %edi # Load and accumulate without leaking.; addl 4(%rsi), %edi # Continue without leaking.; addl 8(%rsi), %edi; orl %eax, %edi # Mask out bits from all three loads.; ```. ###### Preserving the flags while hardening loaded values on Haswell, Zen, and newer processors. Sadly, there are no useful instructions on x86 that apply a mask to all 64 bits; without touching the flag registers. However, we can harden loaded values that; are narrower than a word (fewer than 32-bits on 32-bit systems and fewer than; 64-bits on 64-bit systems) by zero-extending the value to the full word size; and then shifting right by at least the number of original bits using the BMI2; `shrx` instruction:; ```; ... .LBB0_4: # %danger; cmovneq %r8, %rax # Conditionally update predicate state.; addl (%rsi), %edi # Load and accumulate 32 bits of data.; shrxq %rax, %rdi, %rdi # Shift out all 32 bits loaded.; ```. Because on x86 the zero-extend is free, this can efficiently harden the loaded; value. ##### Hardening the address of the load. When hardening the loaded value is inapplicable, most often because the; instruction directly leaks information (like `cmp` or `jmpq`), we switch to; hardening the _address_ of the load instead of the loaded value. This avoids; increasing register pressure by unfolding the load or paying some other high; cost. To understand how this works in practice, we need to examine the exact; semantics of the x86 addressing modes which, in its fully general form, looks; like `(%base,%index,scale)offset`. Here `%base` and `%index` are 64-bit; registers that can potentially be any value, and may be attacker controlled,; and `scale` and `offset` are fixed immediate values. `scale` must be `1`, `2`,; `4`, or `8`, and `offset` can be any 32-bit sign extended value. The exact; computation performed to find the address is then: `%base + (scale * %index) +; offset` under 64-bit 2's complement modular arithmetic. One issue",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md:27476,load,loaded,27476,interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,1,['load'],['loaded']
Performance,"r:$src))]>;. . (set f64:$XT, (PPClfiwzx xoaddr:$src)). - Store as Integer Byte/Halfword Indexed: stxsibx stxsihx; . Similar to stxsiwx:; def STXSIWX : XX1Form<31, 140, (outs), (ins vsfrc:$XT, memrr:$dst),; ""stxsiwx $XT, $dst"", IIC_LdStSTFD,; [(PPCstfiwx f64:$XT, xoaddr:$dst)]>;. . (PPCstfiwx f64:$XT, xoaddr:$dst). - Load Vector Halfword*8/Byte*16 Indexed: lxvh8x lxvb16x; . Similar to lxvd2x/lxvw4x:; def LXVD2X : XX1Form<31, 844,; (outs vsrc:$XT), (ins memrr:$src),; ""lxvd2x $XT, $src"", IIC_LdStLFD,; [(set v2f64:$XT, (int_ppc_vsx_lxvd2x xoaddr:$src))]>;. . (set v8i16:$XT, (int_ppc_vsx_lxvh8x xoaddr:$src)); (set v16i8:$XT, (int_ppc_vsx_lxvb16x xoaddr:$src)). - Store Vector Halfword*8/Byte*16 Indexed: stxvh8x stxvb16x; . Similar to stxvd2x/stxvw4x:; def STXVD2X : XX1Form<31, 972,; (outs), (ins vsrc:$XT, memrr:$dst),; ""stxvd2x $XT, $dst"", IIC_LdStSTFD,; [(store v2f64:$XT, xoaddr:$dst)]>;. . (store v8i16:$XT, xoaddr:$dst); (store v16i8:$XT, xoaddr:$dst). - Load/Store Vector (Left-justified) with Length: lxvl lxvll stxvl stxvll; . Likely needs an intrinsic; . (set v?:$XT, (int_ppc_vsx_lxvl xoaddr:$src)); (set v?:$XT, (int_ppc_vsx_lxvll xoaddr:$src)). . (int_ppc_vsx_stxvl xoaddr:$dst)); (int_ppc_vsx_stxvll xoaddr:$dst)). - Load Vector Word & Splat Indexed: lxvwsx; . Likely needs an intrinsic; . (set v?:$XT, (int_ppc_vsx_lxvwsx xoaddr:$src)). Atomic operations (l[dw]at, st[dw]at):; - Provide custom lowering for common atomic operations to use these; instructions with the correct Function Code; - Ensure the operands are in the correct register (i.e. RT+1, RT+2); - Provide builtins since not all FC's necessarily have an existing LLVM; atomic operation. Move to CR from XER Extended (mcrxrx):; - Is there a use for this in LLVM?. Fixed Point Facility:. - Copy-Paste Facility: copy copy_first cp_abort paste paste. paste_last; . Use instrinstics:; (int_ppc_copy_first i32:$rA, i32:$rB); (int_ppc_copy i32:$rA, i32:$rB). (int_ppc_paste i32:$rA, i32:$rB); (int_ppc_paste_last i32:$rA, i32",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt:19433,Load,Load,19433,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt,1,['Load'],['Load']
Performance,"r::setMCJITMemoryManager; function. If the client does not explicitly create a memory manager at; this time, a default memory manager (specifically SectionMemoryManager); will be created when the MCJIT engine is instantiated. Once the options have been set, a client calls EngineBuilder::create to; create an instance of the MCJIT engine. If the client does not use the; form of this function that takes a TargetMachine as a parameter, a new; TargetMachine will be created based on the target triple associated with; the Module that was used to create the EngineBuilder. .. image:: MCJIT-engine-builder.png. EngineBuilder::create will call the static MCJIT::createJIT function,; passing in its pointers to the module, memory manager and target machine; objects, all of which will subsequently be owned by the MCJIT object. The MCJIT class has a member variable, Dyld, which contains an instance of; the RuntimeDyld wrapper class. This member will be used for; communications between MCJIT and the actual RuntimeDyldImpl object that; gets created when an object is loaded. .. image:: MCJIT-creation.png. Upon creation, MCJIT holds a pointer to the Module object that it received; from EngineBuilder but it does not immediately generate code for this; module. Code generation is deferred until either the; MCJIT::finalizeObject method is called explicitly or a function such as; MCJIT::getPointerToFunction is called which requires the code to have been; generated. Code Generation; ===============. When code generation is triggered, as described above, MCJIT will first; attempt to retrieve an object image from its ObjectCache member, if one; has been set. If a cached object image cannot be retrieved, MCJIT will; call its emitObject method. MCJIT::emitObject uses a local PassManager; instance and creates a new ObjectBufferStream instance, both of which it; passes to TargetMachine::addPassesToEmitMC before calling PassManager::run; on the Module with which it was created. .. image:: MCJIT-load",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst:1902,load,loaded,1902,interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,1,['load'],['loaded']
Performance,"r; fixed-width vectors is still to use a shufflevector, as that may allow for more; optimization opportunities. For example:. .. code-block:: text. {<2 x i64>, <2 x i64>} llvm.experimental.vector.deinterleave2.v4i64(<4 x i64> <i64 0, i64 1, i64 2, i64 3>); ==> {<2 x i64> <i64 0, i64 2>, <2 x i64> <i64 1, i64 3>}. Arguments:; """""""""""""""""""". The argument is a vector whose type corresponds to the logical concatenation of; the two result types. '``llvm.experimental.vector.interleave2``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <4 x double> @llvm.experimental.vector.interleave2.v4f64(<2 x double> %vec1, <2 x double> %vec2); declare <vscale x 8 x i32> @llvm.experimental.vector.interleave2.nxv8i32(<vscale x 4 x i32> %vec1, <vscale x 4 x i32> %vec2). Overview:; """""""""""""""""". The '``llvm.experimental.vector.interleave2``' intrinsic constructs a vector; by interleaving two input vectors. This intrinsic works for both fixed and scalable vectors. While this intrinsic; supports all vector types the recommended way to express this operation for; fixed-width vectors is still to use a shufflevector, as that may allow for more; optimization opportunities. For example:. .. code-block:: text. <4 x i64> llvm.experimental.vector.interleave2.v4i64(<2 x i64> <i64 0, i64 2>, <2 x i64> <i64 1, i64 3>); ==> <4 x i64> <i64 0, i64 1, i64 2, i64 3>. Arguments:; """"""""""""""""""""; Both arguments must be vectors of the same type whereby their logical; concatenation matches the result type. '``llvm.experimental.cttz.elts``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". This is an overloaded intrinsic. You can use ```llvm.experimental.cttz.elts```; on any vector of integer elements, both fixed width and scalable. ::. declare i8 @llvm.experimental.cttz.elts.i8.v8i1(<8 x i1> <src>, i1 <is_zero_poison>). Overview:; """""""""""""""""". The '``llvm.experimental.cttz.elts``' intrinsic counts the number of trailin",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:668538,scalab,scalable,668538,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['scalab'],['scalable']
Performance,"r; over the host linking job. It scans the input object files for the offloading; section ``.llvm.offloading``. The device files stored in this section are then; extracted and passed to the appropriate linking job. The linked device image is; then :ref:`wrapped <Device Binary Wrapping>` to create the symbols used to load; the device image and link it with the host. The linker wrapper tool supports linking bitcode files through link time; optimization (LTO). This is used whenever the object files embedded in the host; contain LLVM bitcode. Bitcode will be embedded for architectures that do not; support a relocatable object format, such as AMDGPU or SPIR-V, or if the user; requested it using the ``-foffload-lto`` flag. .. _Device Binary Wrapping:. Device Binary Wrapping; ----------------------. Various structures and functions are used to create the information necessary to; offload code on the device. We use the :ref:`linked device executable <Device; Linking>` with the corresponding offloading entries to create the symbols; necessary to load and execute the device image. Structure Types; ^^^^^^^^^^^^^^^. Several different structures are used to store offloading information. The; :ref:`device image structure <table-device_image_structure>` stores a single; linked device image and its associated offloading entries. The offloading; entries are stored using the ``__start_omp_offloading_entries`` and; ``__stop_omp_offloading_entries`` symbols generated by the linker using the; :ref:`table-tgt_offload_entry_structure`. .. table:: __tgt_device_image Structure; :name: table-device_image_structure. +----------------------+--------------+----------------------------------------+; | Type | Identifier | Description |; +======================+==============+========================================+; | void* | ImageStart | Pointer to the target code start |; +----------------------+--------------+----------------------------------------+; | void* | ImageEnd | Pointer to the targe",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/OffloadingDesign.rst:14876,load,load,14876,interpreter/llvm-project/clang/docs/OffloadingDesign.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/OffloadingDesign.rst,1,['load'],['load']
Performance,"r; preceding; local load; atomic/store; atomic/atomicrmw; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; lgkmcnt(0) and so do; not need to be; considered.); - s_waitcnt vmcnt(0); must happen after; preceding; global/generic load; atomic/; atomicrmw-with-return-value; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; vmcnt(0) and so do; not need to be; considered.); - s_waitcnt vscnt(0); Must happen after; preceding; global/generic store; atomic/; atomicrmw-no-return-value; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; vscnt(0) and so do; not need to be; considered.); - Ensures any; preceding; sequential; consistent global; memory instructions; have completed; before executing; this sequentially; consistent; instruction. This; prevents reordering; a seq_cst store; followed by a; seq_cst load. (Note; that seq_cst is; stronger than; acquire/release as; the reordering of; load acquire; followed by a store; release is; prevented by the; s_waitcnt of; the release, but; there is nothing; preventing a store; release followed by; load acquire from; completing out of; order. The s_waitcnt; could be placed after; seq_store or before; the seq_load. We; choose the load to; make the s_waitcnt be; as late as possible; so that the store; may have already; completed.). 2. *Following; instructions same as; corresponding load; atomic acquire,; except must generate; all instructions even; for OpenCL.*; store atomic seq_cst - singlethread - global *Same as corresponding; - wavefront - local store atomic release,; - workgroup - generic except must generate; - agent all instructions even; - system for OpenCL.*; atomicrmw seq_cst - singlethread - global *Same as corresponding; - wavefront - local atomicrmw acq_rel,; - workgroup - generic except must generate; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:378906,load,load,378906,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"r; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; release-fence-paired-atomic).; This satisfies the; requirements of; release. 2. buffer_inv sc0=1 sc1=1. - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; MTYPE NC global data.; MTYPE RW and CC memory will; never be stale due to the; memory probes. **Sequential Consistent Atomic**; ------------------------------------------------------------------------------------; load atomic seq_cst - singlethread - global *Same as corresponding; - wavefront - local load atomic acquire,; - generic except must generate; all instructions even; for OpenCL.*; load atomic seq_cst - workgroup - global 1. s_waitcnt lgkm/vmcnt(0); - generic; - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - s_waitcnt lgkmcnt(0) must; happen after; preceding; local/generic load; atomic/store; atomic/atomicrmw; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; lgkmcnt(0) and so do; not need to be; considered.); - s_waitcnt vmcnt(0); must happen after; preceding; global/generic load; atomic/store; atomic/atomicrmw; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; vmcnt(0) and so do; not need to be; considered.); - Ensures any; preceding; sequential; consistent global/local; memory instructions; have completed; before executing; this sequentially; consistent; instruction. This; prevents reordering; a seq_cst store; followed by a; seq_cst load. (Note; that seq_cst is; stronger than; acquire/release as; the reordering of; load acquire; followed by a store; release is; prevented by the; s_waitcnt of; the release, but; there is nothing; preventing a store; release followed by; load acquire from; completing out of; order. The s_wa",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:331144,load,load,331144,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"rDataStore which stores the information in STL arrays. Existing; datasets based on trees can be read in transparently, and are converted to vector form in the ; persistent-to-transient conversion (the datafile is not modified in this operation); ; The vector store has two important advantages: 1) faster data access (raw data access times are 70 times ; faster than for TTrees), 2) ability to rewrite columns on the fly. The first advantage is important; for the existing constant-term precalculation optimization in roofit likelihoods as these are now; also stored in vectors rather than trees. The faster access speed of vectors make that the constant; term optimization inside likelihoods results in a larger speed increase. This is particulatly noticeable in pdfs with; many constant expressions from pdfs that were moderately fast to begin with (e.g. RooHistPdf).; The second advantages allows new types of algorithmic likelihood optimization in RooFit detailed below. New algorithmic optimization in the caching of pdfs. So far - in the likelihood - two classes of; objects are identified: those that change with every event (i.e. the pdf) and those that change; only with the parameters (typically pdf normalization integrals). Pdfs are always recalculated; for every event, whereas integrals are only evaluated when needed. The exception to the first type are pdfs; that only depend on constant parameters (or no parameters) - these are identified at the beginning, and precalculated once ; to avoid recalculating an expression with the same outcome for every iteration of the likelihood calculation. For composite pdfs a further optimization has been included: for a M(x,a,b) = f*F(x,a)+(1-f)G(x,b) ; it is e.g. not needed to recalculate G(x,b) if only parameter a has changed w.r.t to the previous likelihood; calculation. This optimization is now implemented by extending the value caching originally designed; for constant terms to be usable for non-constant terms, with a check executed ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v532/index.html:1695,optimiz,optimization,1695,roofit/doc/v532/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v532/index.html,2,['optimiz'],['optimization']
Performance,"rEndOfToken()``. For; the rare cases where character-level source ranges information is needed we use; the ``CharSourceRange`` class. The Driver Library; ==================. The clang Driver and library are documented :doc:`here <DriverInternals>`. Precompiled Headers; ===================. Clang supports precompiled headers (:doc:`PCH <PCHInternals>`), which uses a; serialized representation of Clang's internal data structures, encoded with the; `LLVM bitstream format <https://llvm.org/docs/BitCodeFormat.html>`_. The Frontend Library; ====================. The Frontend library contains functionality useful for building tools on top of; the Clang libraries, for example several methods for outputting diagnostics. Compiler Invocation; -------------------. One of the classes provided by the Frontend library is ``CompilerInvocation``,; which holds information that describe current invocation of the Clang ``-cc1``; frontend. The information typically comes from the command line constructed by; the Clang driver or from clients performing custom initialization. The data; structure is split into logical units used by different parts of the compiler,; for example ``PreprocessorOptions``, ``LanguageOptions`` or ``CodeGenOptions``. Command Line Interface; ----------------------. The command line interface of the Clang ``-cc1`` frontend is defined alongside; the driver options in ``clang/Driver/Options.td``. The information making up an; option definition includes its prefix and name (for example ``-std=``), form and; position of the option value, help text, aliases and more. Each option may; belong to a certain group and can be marked with zero or more flags. Options; accepted by the ``-cc1`` frontend are marked with the ``CC1Option`` flag. Command Line Parsing; --------------------. Option definitions are processed by the ``-gen-opt-parser-defs`` tablegen; backend during early stages of the build. Options are then used for querying an; instance ``llvm::opt::ArgList``, a wrapper",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/InternalsManual.rst:26789,perform,performing,26789,interpreter/llvm-project/clang/docs/InternalsManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/InternalsManual.rst,1,['perform'],['performing']
Performance,"rStrongerThan``: These are predicates on; orderings. They can be useful for passes that are aware of atomics, for; example to do DSE across a single atomic access, but not across a; release-acquire pair (see MemoryDependencyAnalysis for an example of this). * Alias analysis: Note that AA will return ModRef for anything Acquire or; Release, and for the address accessed by any Monotonic operation. To support optimizing around atomic operations, make sure you are using the; right predicates; everything should work if that is done. If your pass should; optimize some atomic operations (Unordered operations in particular), make sure; it doesn't replace an atomic load or store with a non-atomic operation. Some examples of how optimizations interact with various kinds of atomic; operations:. * ``memcpyopt``: An atomic operation cannot be optimized into part of a; memcpy/memset, including unordered loads/stores. It can pull operations; across some atomic operations. * LICM: Unordered loads/stores can be moved out of a loop. It just treats; monotonic operations like a read+write to a memory location, and anything; stricter than that like a nothrow call. * DSE: Unordered stores can be DSE'ed like normal stores. Monotonic stores can; be DSE'ed in some cases, but it's tricky to reason about, and not especially; important. It is possible in some case for DSE to operate across a stronger; atomic operation, but it is fairly tricky. DSE delegates this reasoning to; MemoryDependencyAnalysis (which is also used by other passes like GVN). * Folding a load: Any atomic load from a constant global can be constant-folded,; because it cannot be observed. Similar reasoning allows sroa with; atomic loads and stores. Atomics and Codegen; ===================. Atomic operations are represented in the SelectionDAG with ``ATOMIC_*`` opcodes.; On architectures which use barrier instructions for all atomic ordering (like; ARM), appropriate fences can be emitted by the AtomicExpand Codegen pass if; ``",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst:17369,load,loads,17369,interpreter/llvm-project/llvm/docs/Atomics.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst,1,['load'],['loads']
Performance,"r_gl0_inv. - If OpenCL omit.; - Ensures that; following; loads will not see; stale data. atomicrmw acquire - workgroup - generic 1. flat_atomic; 2. s_waitcnt lgkmcnt(0) &; vm/vscnt(0). - If CU wavefront execution; mode, omit vm/vscnt(0).; - If OpenCL, omit lgkmcnt(0).; - Use vmcnt(0) if atomic with; return and vscnt(0) if; atomic with no-return.; - Must happen before; the following; buffer_gl0_inv.; - Ensures any; following global; data read is no; older than a local; atomicrmw value; being acquired. 3. buffer_gl0_inv. - If CU wavefront execution; mode, omit.; - Ensures that; following; loads will not see; stale data. atomicrmw acquire - agent - global 1. buffer/global_atomic; - system 2. s_waitcnt vm/vscnt(0). - Use vmcnt(0) if atomic with; return and vscnt(0) if; atomic with no-return.; - Must happen before; following; buffer_gl*_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; caches. 3. buffer_gl0_inv;; buffer_gl1_inv. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acquire - agent - generic 1. flat_atomic; - system 2. s_waitcnt vm/vscnt(0) &; lgkmcnt(0). - If OpenCL, omit; lgkmcnt(0).; - Use vmcnt(0) if atomic with; return and vscnt(0) if; atomic with no-return.; - Must happen before; following; buffer_gl*_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; caches. 3. buffer_gl0_inv;; buffer_gl1_inv. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. fence acquire - singlethread *none* *none*; - wavefront; fence acquire - workgroup *none* 1. s_waitcnt lgkmcnt(0) &; vmcnt(0) & vscnt(0). - If CU wavefront execution; mode, omit vmcnt(0) and; vscnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0) and vscnt(0).; - However, since LLVM; curre",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:350499,load,load,350499,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"r_gl0_inv.; - Ensures that the; fence-paired atomic; has completed; before invalidating; the; cache. Therefore; any following; locations read must; be no older than; the value read by; the; fence-paired-atomic. 3. buffer_gl0_inv. - If CU wavefront execution; mode, omit.; - Ensures that; following; loads will not see; stale data. fence acquire - agent *none* 1. s_waitcnt lgkmcnt(0) &; - system vmcnt(0) & vscnt(0). - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0) and vscnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate; (see comment for; previous fence).; - Could be split into; separate s_waitcnt; vmcnt(0), s_waitcnt; vscnt(0) and s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load; atomic/; atomicrmw-with-return-value; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - s_waitcnt vscnt(0); must happen after; any preceding; global/generic; atomicrmw-no-return-value; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - Must happen before; the following; buffer_gl*_inv.; - Ensures that the; fence-paired atomic; has completed; before invalidating; the; caches. Therefore; any following; locations read must; be no older than; the value read by; the; fence-paired-atomic. 2. buffer_gl0_inv;; buffer_gl1_inv. - Must happen before any; following global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that; following loads; wil",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:353580,load,load,353580,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"r`.; - The `RooFit::FitOptions(const char*)` command to steer [RooAbsPdf::fitTo()](https://root.cern.ch/doc/v628/classRooAbsPdf.html) with an option string was removed. This way of configuring the fit was deprecated since at least since ROOT 5.02.; Subsequently, the `RooMinimizer::fit(const char*)` function and the [RooMCStudy](https://root.cern.ch/doc/v628/classRooMCStudy.html) constructor that takes an option string were removed as well.; - The overload of `RooAbsData::createHistogram` that takes integer parameters for the bin numbers is now deprecated and will be removed in ROOT 6.30.; This was done to avoid confusion with inconsistent behavior when compared to other `createHistogram` overloads.; Please use the verson of `createHistogram` that takes RooFit command arguments.; - The `RooAbsData::valid()` method to cache valid entries in the variable range; was removed. It was not implemented in RooDataSet, so it never worked as; intended. Related to it was the `RooDataHist::cacheValidEntries()` function, which is removed as well.; The preferred way to reduce RooFit datasets to subranges is [RooAbsData::reduce()](https://root.cern.ch/doc/v628/classRooAbsData.html#acfa7b31e5cd751eec1bc4e95d2796390).; - The longtime-deprecated `RooStats::HistFactory::EstimateSummary` class is removed, including the functions that use it. The information that it was meant to store is managed by the `RooStats::HistFactory::Measurement` object since many years.; - The `RooSuperCategory::MakeIterator()` function that was deprecated since 6.22 is now removed. Please use range-based loops to iterate over the category states.; - The `HybridCalculatorOriginal` and `HypoTestInverterOriginal` classes in RooStats that were deprecated for a very long time aleady are removed. Please use `HybridCalculator` and `HypoTestInverter`.; - The `RooSimPdfBuilder` that was deprecated in ROOT 5.20 and replaced by the `RooSimWSTool` is removed.; - The RDataFrame factory functions `MakeNumpyDataFrame`, `MakeCs",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v628/index.md:3433,cache,cacheValidEntries,3433,README/ReleaseNotes/v628/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v628/index.md,1,['cache'],['cacheValidEntries']
Performance,"ra dyld stub load. If you try; vec_align.ll without -relocation-model=static, you'll see what I mean. //===---------------------------------------------------------------------===//. We should lower store(fneg(load p), q) into an integer load+xor+store, which; eliminates a constant pool load. For example, consider:. define i64 @ccosf(float %z.0, float %z.1) nounwind readonly {; entry:; %tmp6 = fsub float -0.000000e+00, %z.1		; <float> [#uses=1]; %tmp20 = tail call i64 @ccoshf( float %tmp6, float %z.0 ) nounwind readonly; ret i64 %tmp20; }; declare i64 @ccoshf(float %z.0, float %z.1) nounwind readonly. This currently compiles to:. LCPI1_0:					# <4 x float>; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; _ccosf:; 	subl	$12, %esp; 	movss	16(%esp), %xmm0; 	movss	%xmm0, 4(%esp); 	movss	20(%esp), %xmm0; 	xorps	LCPI1_0, %xmm0; 	movss	%xmm0, (%esp); 	call	L_ccoshf$stub; 	addl	$12, %esp; 	ret. Note the load into xmm0, then xor (to negate), then store. In PIC mode,; this code computes the pic base and does two loads to do the constant pool ; load, so the improvement is much bigger. The tricky part about this xform is that the argument load/store isn't exposed; until post-legalize, and at that point, the fneg has been custom expanded into ; an X86 fxor. This means that we need to handle this case in the x86 backend; instead of in target independent code. //===---------------------------------------------------------------------===//. Non-SSE4 insert into 16 x i8 is atrociously bad. //===---------------------------------------------------------------------===//. <2 x i64> extract is substantially worse than <2 x f64>, even if the destination; is memory. //===---------------------------------------------------------------------===//. INSERTPS can match any insert (extract, imm1), imm2 for 4 x float, and insert; any number of 0.0 simultaneously. Currently we only use it for simple; insertions. See comments i",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:13209,load,load,13209,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,2,['load'],['load']
Performance,ra/clang-tidy/objc/SuperSelfCheck.h; clang-tools-extra/clang-tidy/openmp/ExceptionEscapeCheck.cpp; clang-tools-extra/clang-tidy/openmp/ExceptionEscapeCheck.h; clang-tools-extra/clang-tidy/openmp/OpenMPTidyModule.cpp; clang-tools-extra/clang-tidy/openmp/UseDefaultNoneCheck.cpp; clang-tools-extra/clang-tidy/openmp/UseDefaultNoneCheck.h; clang-tools-extra/clang-tidy/performance/FasterStringFindCheck.cpp; clang-tools-extra/clang-tidy/performance/ForRangeCopyCheck.cpp; clang-tools-extra/clang-tidy/performance/InefficientAlgorithmCheck.cpp; clang-tools-extra/clang-tidy/performance/InefficientAlgorithmCheck.h; clang-tools-extra/clang-tidy/performance/InefficientStringConcatenationCheck.cpp; clang-tools-extra/clang-tidy/performance/InefficientStringConcatenationCheck.h; clang-tools-extra/clang-tidy/performance/MoveConstArgCheck.cpp; clang-tools-extra/clang-tidy/performance/MoveConstArgCheck.h; clang-tools-extra/clang-tidy/performance/MoveConstructorInitCheck.cpp; clang-tools-extra/clang-tidy/performance/MoveConstructorInitCheck.h; clang-tools-extra/clang-tidy/performance/NoAutomaticMoveCheck.cpp; clang-tools-extra/clang-tidy/performance/NoAutomaticMoveCheck.h; clang-tools-extra/clang-tidy/performance/NoexceptMoveConstructorCheck.cpp; clang-tools-extra/clang-tidy/performance/NoexceptMoveConstructorCheck.h; clang-tools-extra/clang-tidy/performance/NoIntToPtrCheck.cpp; clang-tools-extra/clang-tidy/performance/NoIntToPtrCheck.h; clang-tools-extra/clang-tidy/performance/PerformanceTidyModule.cpp; clang-tools-extra/clang-tidy/performance/TriviallyDestructibleCheck.cpp; clang-tools-extra/clang-tidy/performance/TriviallyDestructibleCheck.h; clang-tools-extra/clang-tidy/performance/TypePromotionInMathFnCheck.cpp; clang-tools-extra/clang-tidy/performance/TypePromotionInMathFnCheck.h; clang-tools-extra/clang-tidy/performance/UnnecessaryCopyInitialization.cpp; clang-tools-extra/clang-tidy/performance/UnnecessaryValueParamCheck.cpp; clang-tools-extra/clang-tidy/performance/UnnecessaryVal,MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/tools/clang-formatted-files.txt:65338,perform,performance,65338,interpreter/llvm-project/clang/docs/tools/clang-formatted-files.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/tools/clang-formatted-files.txt,1,['perform'],['performance']
Performance,"racks is less than 585, the full event is; read in memory. This test is obviously not possible in non-split mode.; In non-split mode, the full event must be read in memory. The times; reported in the table correspond to complete I/O operations necessary to; deal with **machine independent binary files**. On **Linux**, this also; includes byte-swapping operations. The ROOT file allows for direct; access to any event in the file and direct access to any part of an; event when split=1. Note also that the uncompressed file generated with split=0 is 48.7; Mbytes and only 47.17 Mbytes for the option split=1. The difference in; size is due to the object identification mechanism overhead when the; event is written to a single buffer. This overhead does not exist in; split mode because the branch buffers are optimized for homogeneous data; types. You can run the test programs on your architecture. The program; `Event` will report the write performance. You can measure the read; performance by executing the scripts `eventa` and `eventb`. The; performance depends not only of the processor type, but also of the disk; devices (local, NFS, AFS, etc.). ## Chains; \index{tree!chains}. A **`TChain`** object is a list of ROOT files containing the same tree.; As an example, assume we have three files called; `file1.root, file2.root, file3.root`. Each file contains one tree called; ""`T`"". We can create a chain with the following statements:. ``` {.cpp}; TChain chain(""T""); // name of the tree is the argument; chain.Add(""file1.root"");; chain.Add(""file2.root"");; chain.Add(""file3.root"");; ```. The name of the **`TChain`** will be the same as the name of the tree;; in this case it will be `""T"". Note that two `objects can have the same; name as long as they are not histograms in the same directory, because; there, the histogram names are used to build a hash table. The class; **`TChain`** is derived from the class **`TTree`**. For example, to; generate a histogram corresponding to the attrib",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Trees.md:139977,perform,performance,139977,documentation/users-guide/Trees.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Trees.md,1,['perform'],['performance']
Performance,"raction of events in a splittable node. nTrees No 20 − Number of trees in forest. ForestType No AdaBoost AdaBoost, Random Method to use for forest generation (AdaBoost or RandomForest). RuleMinDist No 0.001 − Minimum distance between rules. MinImp No 0.01 − Minimum rule importance accepted. Model No ModRuleLinear ModRule, ModRuleLinear, ModLinear Model to be used. RuleFitModule No RFTMVA RFTMVA, RFFriedman Which RuleFit module to use. RFWorkDir No ./rulefit − Friedman's RuleFit module (RFF): working dir. RFNrules No 2000 − RFF: Mximum number of rules. RFNendnodes No 4 − RFF: Average number of end nodes. Configuration options for MVA method :. Configuration options reference for MVA method: Likelihood. Option Array Default value Predefined values Description. V No False − Verbose output (short form of VerbosityLevel below - overrides the latter one). VerbosityLevel No Default Default, Debug, Verbose, Info, Warning, Error, Fatal Verbosity level. VarTransform No None − List of variable transformations performed before training, e.g., D_Background,P_Signal,G,N_AllClasses for: Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed). H No False − Print method-specific help message. CreateMVAPdfs No False − Create PDFs for classifier outputs (signal and background). IgnoreNegWeightsInTraining No False − Events with negative weights are ignored in the training (but are included for testing and performance evaluation). TransformOutput No False − Transform likelihood output by inverse sigmoid function. Configuration options for MVA method :. Configuration options reference for MVA method: MLP. Option Array Default value Predefined values Description. NCycles No 500 − Number of training cycles. HiddenLayers No N,N-1 − Specification of hidden layer architecture. NeuronType No sigmoid − Neuron activation function type. RandomSeed No 1 ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/tmva/UsersGuide/optionRef.html:20133,perform,performed,20133,documentation/tmva/UsersGuide/optionRef.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/tmva/UsersGuide/optionRef.html,2,['perform'],['performed']
Performance,"ral options when creating http server. They could be add as additional URL parameters to the constructor arguments like:. ```cpp; auto serv = new THttpServer(""http:8080?loopback&thrds=2"");; ```. Following URL parameters are supported:. | Name | Description |; | :-------------------- | :---------------- |; | thrds=N | number of threads used by the civetweb (default is 10) |; | top=name | configure top name, visible in the web browser |; | auth_file=filename | authentication file name, created with htdigets utility |; | auth_domain=domain | authentication domain |; | loopback | bind specified port to loopback 127.0.0.1 address |; | debug | enable debug mode, server returns html page with request info |; | websocket_timeout=tm | set web sockets timeout in seconds (default 300) |; | websocket_disable | disable web sockets handling (default enabled) |; | cors=domain | define value for CORS header ""Access-Control-Allow-Origin"" in server response |; | log=filename | configure civetweb log file |; | max_age=value | configures ""Cache-Control: max_age=value"" http header for all file-related requests, default 3600 |; | nocache | try to fully disable cache control for file requests |; | winsymlinks=no | do not resolve symbolic links on file system (Windows only), default true |; | dirlisting=no | enable/disable directory listing for browsing filesystem (default no) |. If necessary, one could bind http server to specific IP address like:. ```cpp; new THttpServer(""http:192.168.1.17:8080""); ```. One also can provide extra arguments for THttpServer itself:. | Name | Description |; | :------------- | :---------------- |; | readonly, ro | use server in read-only mode (default) |; | readwrite, rw | use server in read-write mode |; | global | let scan global directories for canvases and files (default) |; | noglobal | disable scan of global directories |; | basic_sniffer | use basic `TRootSniffer` without support of hist, gpad, graph, tree classes |. Example:. ```cpp; new THttpServer(""h",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/HttpServer/HttpServer.md:2480,cache,cache,2480,documentation/HttpServer/HttpServer.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/HttpServer/HttpServer.md,1,['cache'],['cache']
Performance,"ral, or a function. Spaces are accepted before, after and between any of; these elements. Numeric operands have 64-bit precision. Overflow and underflow; are rejected. There is no support for operator precedence, but parentheses; can be used to change the evaluation order. The supported operators are:. * ``+`` - Returns the sum of its two operands.; * ``-`` - Returns the difference of its two operands. The syntax of a function call is ``<name>(<arguments>)`` where:. * ``name`` is a predefined string literal. Accepted values are:. * add - Returns the sum of its two operands.; * div - Returns the quotient of its two operands.; * max - Returns the largest of its two operands.; * min - Returns the smallest of its two operands.; * mul - Returns the product of its two operands.; * sub - Returns the difference of its two operands. * ``<arguments>`` is a comma separated list of expressions. For example:. .. code-block:: llvm. ; CHECK: load r[[#REG:]], [r0]; ; CHECK: load r[[#REG+1]], [r1]; ; CHECK: Loading from 0x[[#%x,ADDR:]]; ; CHECK-SAME: to 0x[[#ADDR + 7]]. The above example would match the text:. .. code-block:: gas. load r5, [r0]; load r6, [r1]; Loading from 0xa0463440 to 0xa0463447. but would not match the text:. .. code-block:: gas. load r5, [r0]; load r7, [r1]; Loading from 0xa0463440 to 0xa0463443. Due to ``7`` being unequal to ``5 + 1`` and ``a0463443`` being unequal to; ``a0463440 + 7``. A numeric variable can also be defined to the result of a numeric expression,; in which case the numeric expression constraint is checked and if verified the; variable is assigned to the value. The unified syntax for both checking a; numeric expression and capturing its value into a numeric variable is thus; ``[[#%<fmtspec>,<NUMVAR>: <constraint> <expr>]]`` with each element as; described previously. One can use this syntax to make a testcase more; self-describing by using variables instead of values:. .. code-block:: gas. ; CHECK: mov r[[#REG_OFFSET:]], 0x[[#%X,FIELD_OFFSET:12]",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/FileCheck.rst:32125,load,load,32125,interpreter/llvm-project/llvm/docs/CommandGuide/FileCheck.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/FileCheck.rst,2,['load'],['load']
Performance,"rally optional if the target GC does not; require the corresponding barrier. The GC strategy used with such a collector; should replace the intrinsic calls with the corresponding ``load`` or; ``store`` instruction if they are used. One known deficiency with the current design is that the barrier intrinsics do; not include the size or alignment of the underlying operation performed. It is; currently assumed that the operation is of pointer size and the alignment is; assumed to be the target machine's default alignment. Write barrier: ``llvm.gcwrite``; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. .. code-block:: llvm. void @llvm.gcwrite(i8* %value, i8* %object, i8** %derived). For write barriers, LLVM provides the ``llvm.gcwrite`` intrinsic function. It; has exactly the same semantics as a non-volatile ``store`` to the derived; pointer (the third argument). The exact code generated is specified by the; Function's selected :ref:`GC strategy <plugin>`. Many important algorithms require write barriers, including generational and; concurrent collectors. Additionally, write barriers could be used to implement; reference counting. Read barrier: ``llvm.gcread``; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. .. code-block:: llvm. i8* @llvm.gcread(i8* %object, i8** %derived). For read barriers, LLVM provides the ``llvm.gcread`` intrinsic function. It has; exactly the same semantics as a non-volatile ``load`` from the derived pointer; (the second argument). The exact code generated is specified by the Function's; selected :ref:`GC strategy <plugin>`. Read barriers are needed by fewer algorithms than write barriers, and may have a; greater performance impact since pointer reads are more frequent than writes. .. _plugin:. .. _builtin-gc-strategies:. Built In GC Strategies; ======================. LLVM includes built in support for several varieties of garbage collectors. The Shadow Stack GC; ----------------------. To use this collector strategy, mark your functions with:. .. code-block:: c++. F.setGC(""shad",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GarbageCollection.rst:15138,concurren,concurrent,15138,interpreter/llvm-project/llvm/docs/GarbageCollection.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GarbageCollection.rst,1,['concurren'],['concurrent']
Performance,"ram as a unit,; referring to function bodies in no predictable order, or adding and removing; functions. Because nothing is known about the behavior of ``ModulePass``; subclasses, no optimization can be done for their execution. A module pass can use function level passes (e.g. dominators) using the; ``getAnalysis`` interface ``getAnalysis<DominatorTree>(llvm::Function *)`` to; provide the function to retrieve analysis result for, if the function pass does; not require any module or immutable passes. Note that this can only be done; for functions for which the analysis ran, e.g. in the case of dominators you; should only ask for the ``DominatorTree`` for function definitions, not; declarations. To write a correct ``ModulePass`` subclass, derive from ``ModulePass`` and; override the ``runOnModule`` method with the following signature:. The ``runOnModule`` method; ^^^^^^^^^^^^^^^^^^^^^^^^^^. .. code-block:: c++. virtual bool runOnModule(Module &M) = 0;. The ``runOnModule`` method performs the interesting work of the pass. It; should return ``true`` if the module was modified by the transformation and; ``false`` otherwise. .. _writing-an-llvm-pass-CallGraphSCCPass:. The ``CallGraphSCCPass`` class; ------------------------------. The `CallGraphSCCPass; <https://llvm.org/doxygen/classllvm_1_1CallGraphSCCPass.html>`_ is used by; passes that need to traverse the program bottom-up on the call graph (callees; before callers). Deriving from ``CallGraphSCCPass`` provides some mechanics; for building and traversing the ``CallGraph``, but also allows the system to; optimize execution of ``CallGraphSCCPass``\ es. If your pass meets the; requirements outlined below, and doesn't meet the requirements of a; :ref:`FunctionPass <writing-an-llvm-pass-FunctionPass>`, you should derive from; ``CallGraphSCCPass``. ``TODO``: explain briefly what SCC, Tarjan's algo, and B-U mean. To be explicit, CallGraphSCCPass subclasses are:. #. ... *not allowed* to inspect or modify any ``Function``\ s ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst:13917,perform,performs,13917,interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst,1,['perform'],['performs']
Performance,"ram representations, e.g. Object Files) are no longer added; directly to JIT classes or layers. Instead, they are added to ``JITDylib``; instances *by* layers. The ``JITDylib`` determines *where* the definitions; reside, the layers determine *how* the definitions will be compiled.; Linkage relationships between ``JITDylibs`` determine how inter-module; references are resolved, and symbol resolvers are no longer used. See the; section `Design Overview`_ for more details. Unless multiple JITDylibs are needed to model linkage relationships, ORCv1; clients should place all code in a single JITDylib.; MCJIT clients should use LLJIT (see `LLJIT and LLLazyJIT`_), and can place; code in LLJIT's default created main JITDylib (See; ``LLJIT::getMainJITDylib()``). 2. All JIT stacks now need an ``ExecutionSession`` instance. ExecutionSession; manages the string pool, error reporting, synchronization, and symbol; lookup. 3. ORCv2 uses uniqued strings (``SymbolStringPtr`` instances) rather than; string values in order to reduce memory overhead and improve lookup; performance. See the subsection `How to manage symbol strings`_. 4. IR layers require ThreadSafeModule instances, rather than; std::unique_ptr<Module>s. ThreadSafeModule is a wrapper that ensures that; Modules that use the same LLVMContext are not accessed concurrently.; See `How to use ThreadSafeModule and ThreadSafeContext`_. 5. Symbol lookup is no longer handled by layers. Instead, there is a; ``lookup`` method on JITDylib that takes a list of JITDylibs to scan. .. code-block:: c++. ExecutionSession ES;; JITDylib &JD1 = ...;; JITDylib &JD2 = ...;. auto Sym = ES.lookup({&JD1, &JD2}, ES.intern(""_main""));. 6. The removeModule/removeObject methods are replaced by; ``ResourceTracker::remove``.; See the subsection `How to remove code`_. For code examples and suggestions of how to use the ORCv2 APIs, please see; the section `How-tos`_. How-tos; =======. How to manage symbol strings; ----------------------------. Symbol string",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:21112,perform,performance,21112,interpreter/llvm-project/llvm/docs/ORCv2.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst,1,['perform'],['performance']
Performance,"ranch containing the number of tracks is; read. In case the number of tracks is less than 585, the full event is; read in memory. This test is obviously not possible in non-split mode.; In non-split mode, the full event must be read in memory. The times; reported in the table correspond to complete I/O operations necessary to; deal with **machine independent binary files**. On **Linux**, this also; includes byte-swapping operations. The ROOT file allows for direct; access to any event in the file and direct access to any part of an; event when split=1. Note also that the uncompressed file generated with split=0 is 48.7; Mbytes and only 47.17 Mbytes for the option split=1. The difference in; size is due to the object identification mechanism overhead when the; event is written to a single buffer. This overhead does not exist in; split mode because the branch buffers are optimized for homogeneous data; types. You can run the test programs on your architecture. The program; `Event` will report the write performance. You can measure the read; performance by executing the scripts `eventa` and `eventb`. The; performance depends not only of the processor type, but also of the disk; devices (local, NFS, AFS, etc.). ## Chains; \index{tree!chains}. A **`TChain`** object is a list of ROOT files containing the same tree.; As an example, assume we have three files called; `file1.root, file2.root, file3.root`. Each file contains one tree called; ""`T`"". We can create a chain with the following statements:. ``` {.cpp}; TChain chain(""T""); // name of the tree is the argument; chain.Add(""file1.root"");; chain.Add(""file2.root"");; chain.Add(""file3.root"");; ```. The name of the **`TChain`** will be the same as the name of the tree;; in this case it will be `""T"". Note that two `objects can have the same; name as long as they are not histograms in the same directory, because; there, the histogram names are used to build a hash table. The class; **`TChain`** is derived from the class **`TTre",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Trees.md:139938,perform,performance,139938,documentation/users-guide/Trees.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Trees.md,1,['perform'],['performance']
Performance,"ranch('myints',mystruct,'MyInt1/I:MyInt2:MyInt3'); tree.Branch('mycode',AddressOf(mystruct,'fMyCode'),'MyCode/C'); for i in range(0,10):; mystruct.fMyInt1 = i; mystruct.fMyInt2 = i*i; mystruct.fMyInt3 = i*i*i; mystruct.fMyCode = ""%03d"" % i # note string assignment. tree.Fill(). f.Write(); f.Close(); ```. The C++ class is defined through the `gROOT.ProcessLine()` call, and; note how the `AddressOf()` function is used for data members of built-in; type. Most of the above is for ROOT version 5.02 and later only. For; older releases, and without further support, here is an example as to; how you can get hold of a pointer-to-pointer to a ROOT object:. ``` {.cpp}; h = TH1F(); addressofobject = array('i',[h.IsA().DynamicCast(h.IsA(),h)]); ```. ### Using Your Own Classes. A user's own classes can be accessed after loading, either directly or; indirectly, the library that contains the dictionary. One easy way of; obtaining such a library, is by using ACLiC:. ``` {.cpp}; $ cat MyClass.C; class MyClass {; public:. MyClass(int value = 0) {; m_value = value;; }. void SetValue(int value) {; m_value = value;; }. int GetValue() {; return m_value;; }. private:; int m_value;; };. $ echo .L MyClass.C+ | root.exe -b; [...]; Info in <TUnixSystem::ACLiC>: creating shared library [..]/./MyClass_C.so; $; ```. Then you can use it, for example, like so:. ``` {.cpp}; from ROOT import gSystem. # load library with MyClass dictionary; gSystem.Load('MyClass_C'). # get MyClass from ROOT; from ROOT import MyClass; # use MyClass; m = MyClass(42); print(m.GetValue()); ```. You can also load a macro directly, but if you do not use ACLiC, you; will be restricted to use the default constructor of your class, which; is otherwise fully functional. For example:. ``` {.cpp}; from ROOT import gROOT. # load MyClass definition macro (append '+' to use ACLiC); gROOT.LoadMacro('MyClass.C'). # get MyClass from ROOT; from ROOT import MyClass. # use MyClass; m = MyClass(); m.SetValue(42); print(m.GetValue()); ```; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/PythonRuby.md:33142,load,load,33142,documentation/users-guide/PythonRuby.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/PythonRuby.md,3,['load'],['load']
Performance,"ranch). Preliminary tests indicate between 30%; and a factor 2.5 in memory decrease. This improvement is transparent for `RDataFrame` users. ## Graphics backends; The ROOT release 6.32 brings a lot of impressive enhancements to the Web Graphics package, greatly surpassing the features and capabilities of version 6.30. ; This update provides users with a more robust Web Graphics. * The JSROOT version has been updated to v7.7. ## 2D Graphics Libraries. - TMultiGraph: Add the objects from the list of functions in legend produce by TLegend.; - Implement the IsInside method for TEllipse, TCrown and TDiamond. Also, a new graphics example `inside.C` has been added.; - Two new methods in TColor: `ListColors()` and `GetColorByname()`.; - Make sure the option `L` draws closed polygon for `TH2Poly`.; - Use Tex Gyre fonts for sans serif (similar to Helvetica) .; - The new method `TPad::ModifiedUpdate` is short cut to call `Modified()` and `Update()` in a single call. On Mac with Cocoa, it performs an additional ProcessEvents().; - Improve `SetTextSize` error: show code and values.; - Very long text string generated a wrong SVG file.; - Fix the option `SAME` works for `TGraph2D`.; - Implement the title for the palette of a `TH3`.; - Fix typo in `TLegend::PaintPrimitives()` and improve the exclusion graphs legend.; - `SetParameters(…)` or `SetParameter(…)` on a TF1 reset the properties of the axis that have been previously defined.; This was due to the `Update()` that was done after the parameters definition.; - Update fonts' documentation (CMS request).; - Delaunay triangles were computed by the package `triangle.c` included in the ROOT code.; This package had several problems:; - It was not maintained anymore.; - Its license was not compatible with LGPL.; It is now replaced by the CDT package which is properly maintained and has a license (MLP) compatible with LGPL. ## 3D Graphics Libraries. ### REve; * Introduce lightweight visualization of instanced shapes on the level of 100",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v632/index.md:17133,perform,performs,17133,README/ReleaseNotes/v632/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v632/index.md,1,['perform'],['performs']
Performance,"rands ``A`` and ``B`` and for; any ``N`` larger than the operands' width, ``ext(A op B) to iN`` is; not equal to ``(ext(A) to iN) op (ext(B) to iN)`` where ``ext`` is; ``sext`` for signed overflow and ``zext`` for unsigned overflow, and; ``op`` is the underlying arithmetic operation. The behavior of these intrinsics is well-defined for all argument; values. '``llvm.sadd.with.overflow.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". This is an overloaded intrinsic. You can use ``llvm.sadd.with.overflow``; on any integer bit width or vectors of integers. ::. declare {i16, i1} @llvm.sadd.with.overflow.i16(i16 %a, i16 %b); declare {i32, i1} @llvm.sadd.with.overflow.i32(i32 %a, i32 %b); declare {i64, i1} @llvm.sadd.with.overflow.i64(i64 %a, i64 %b); declare {<4 x i32>, <4 x i1>} @llvm.sadd.with.overflow.v4i32(<4 x i32> %a, <4 x i32> %b). Overview:; """""""""""""""""". The '``llvm.sadd.with.overflow``' family of intrinsic functions perform; a signed addition of the two arguments, and indicate whether an overflow; occurred during the signed summation. Arguments:; """""""""""""""""""". The arguments (%a and %b) and the first element of the result structure; may be of integer types of any bit width, but they must have the same; bit width. The second element of the result structure must be of type; ``i1``. ``%a`` and ``%b`` are the two values that will undergo signed; addition. Semantics:; """""""""""""""""""". The '``llvm.sadd.with.overflow``' family of intrinsic functions perform; a signed addition of the two variables. They return a structure --- the; first element of which is the signed summation, and the second element; of which is a bit specifying if the signed summation resulted in an; overflow. Examples:; """""""""""""""""". .. code-block:: llvm. %res = call {i32, i1} @llvm.sadd.with.overflow.i32(i32 %a, i32 %b); %sum = extractvalue {i32, i1} %res, 0; %obit = extractvalue {i32, i1} %res, 1; br i1 %obit, label %overflow, label %normal. '``llvm.uadd.with.overflow.*``' Intrinsic",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:601495,perform,perform,601495,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['perform']
Performance,"ransform No None − List of variable transformations performed before training, e.g., D_Background,P_Signal,G,N_AllClasses for: Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed). H No False − Print method-specific help message. CreateMVAPdfs No False − Create PDFs for classifier outputs (signal and background). IgnoreNegWeightsInTraining No False − Events with negative weights are ignored in the training (but are included for testing and performance evaluation). Configuration options for MVA method :. Configuration options reference for MVA method: SVM. Option Array Default value Predefined values Description. V No False − Verbose output (short form of VerbosityLevel below - overrides the latter one). VerbosityLevel No Default Default, Debug, Verbose, Info, Warning, Error, Fatal Verbosity level. VarTransform No None − List of variable transformations performed before training, e.g., D_Background,P_Signal,G,N_AllClasses for: Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed). H No False − Print method-specific help message. CreateMVAPdfs No False − Create PDFs for classifier outputs (signal and background). IgnoreNegWeightsInTraining No False − Events with negative weights are ignored in the training (but are included for testing and performance evaluation). Gamma No 1 − RBF kernel parameter: Gamma (size of the Kernel). C No 1 − Cost parameter. Tol No 0.01 − Tolerance parameter. MaxIter No 1000 − Maximum number of training loops. Configuration options for MVA method :. Configuration options reference for MVA method: CFMlpANN. Option Array Default value Predefined values Description. V No False − Verbose output (short form of VerbosityLevel below - overrides the latter on",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/tmva/UsersGuide/optionRef.html:7594,perform,performed,7594,documentation/tmva/UsersGuide/optionRef.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/tmva/UsersGuide/optionRef.html,2,['perform'],['performed']
Performance,"ranularity**.; * For every such object a random `TS`-bit tag `T` is chosen (`TS`, or tag size, is e.g. 4 or 8); * The pointer to the object is tagged with `T`.; * The memory for the object is also tagged with `T` (using a `TG=>1` shadow memory); * Every load and store is instrumented to read the memory tag and compare it; with the pointer tag, exception is raised on tag mismatch. For a more detailed discussion of this approach see https://arxiv.org/pdf/1802.09517.pdf. Short granules; --------------. A short granule is a granule of size between 1 and `TG-1` bytes. The size; of a short granule is stored at the location in shadow memory where the; granule's tag is normally stored, while the granule's actual tag is stored; in the last byte of the granule. This means that in order to verify that a; pointer tag matches a memory tag, HWASAN must check for two possibilities:. * the pointer tag is equal to the memory tag in shadow memory, or; * the shadow memory tag is actually a short granule size, the value being loaded; is in bounds of the granule and the pointer tag is equal to the last byte of; the granule. Pointer tags between 1 to `TG-1` are possible and are as likely as any other; tag. This means that these tags in memory have two interpretations: the full; tag interpretation (where the pointer tag is between 1 and `TG-1` and the; last byte of the granule is ordinary data) and the short tag interpretation; (where the pointer tag is stored in the granule). When HWASAN detects an error near a memory tag between 1 and `TG-1`, it; will show both the memory tag and the last byte of the granule. Currently,; it is up to the user to disambiguate the two possibilities. Instrumentation; ===============. Memory Accesses; ---------------; In the majority of cases, memory accesses are prefixed with a call to; an outlined instruction sequence that verifies the tags. The code size; and performance overhead of the call is reduced by using a custom calling; convention that. * preserv",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/HardwareAssistedAddressSanitizerDesign.rst:2491,load,loaded,2491,interpreter/llvm-project/clang/docs/HardwareAssistedAddressSanitizerDesign.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/HardwareAssistedAddressSanitizerDesign.rst,1,['load'],['loaded']
Performance,"rarchies of Volumes. The first thing one would like to do after building some geometry is to; visualize the volume tree. This provides the fastest validation check; for most common coding or design mistakes. As soon as the geometry is; successfully closed, one should draw it starting from the top-level; volume:. ~~~{.cpp}; //... code for geometry building; root[] gGeoManager->CloseGeometry();; root[] gGeoManager->GetMasterVolume()->Draw();; ~~~. Doing this ensures that the original top-level volume of the geometry is; drawn, even if another volume is currently the geometry `root`. OK, I; suppose you already did that with your simple geometry and immediately; noticed a new ROOT canvas popping-up and having some more or less; strange picture inside. Here are few questions that might come:. **Q:** ""The picture is strangely rotated; where are the coordinate axes?"". **A:** If drawn in a new canvas, any view has some default; viewpoint, center of view and size. One can then perform mouse/keyboard; actions to change them:. - Mouse left-click and drag will rotate the view;. - Some keys can be pressed when the view canvas is selected: J/K; zoom/un-zoom, U/I move up/down, L/H move left/right. The coordinate axes; display as well as changing top or side viewpoints can be activated from; the **`TView`** context menu: right-click on the picture when no object; is selected;. **Q:** ""Every line is black! I cannot figure out what is what..."". **A:** Volumes can have different colors (those known by %ROOT of; course). Think at using them after each volume creation:; `myvolume->SetLineColor(Int_t color);` otherwise everything is by; default black. **Q:** ""The top volume of my geometry is a box but I see only its content."". **A:** By default the drawn volume is not displayed just because we; do not want to hide its content when changing the view to HLR or solid; mode. In order to see it in the default wire frame picture one has to; call TGeoManager::SetTopVisible(). **Q:** ""I do not s",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md:98885,perform,perform,98885,geom/geom/doc/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md,1,['perform'],['perform']
Performance,"rashes while generating the memory; plots.; In TProofMgr::GetSessionLogs:. add the possibility to postpone the retrieval of the; logs files when the TProofLog object is created. This improved; functionality is exploited in the log window.; add decoding of the session starting time and full; information about the master URL. Enable new xrootd configuration options, including the; possibility to set the compiler and linker; Cleanup of the TProofMgr functions DetachSession and; ShutdownSession, and better handling of the internal list registration,; to fix potential segvs when reopening a PROOF session inside the same; ROOT session.; Optimize the way results are transferred and merged:. Output objects are added to the same TMessage until a; HWM is reached (default 1MB; controlled by 'ProofServ.MsgSizeHWM');; this limits the number of transfers in the case of large numbers of; small objects.; Reasonably small histograms (GetSize() <; MsgSizeHWM) are merged in one-go at the end instead of one-by-one to; exploit, for example, the better performance of TH1::Merge on the full; list of histos.; Add possibility to compress the messages; this is; controlled by ProofServ.CompressMessage; <compression_level>; The default is still 'no compression' but this will allow to study the; impact of compression. Add sort of 'progress' counter for merging is now shown; on the client:;  ;     root [n] p->Process(...);       ... ;       Mst-0: merging output objects ... / (4; workers still sending). This asserts socket activity and fixes the timeout; problems during long merging phases reported in a few cases.; In TFileMerger, create directly the output file at the; final destination do not make a local copy in the temp directory first; (if needed, one can always set the temporary destination to temp; followed by a TFile::Cp to the final destination); this allows to avoid; reported problems with small temp partitions (see Forum).; In XrdProofConn, enable cycling through the; authentication pr",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/proof/doc/v524/index.html:7033,perform,performance,7033,proof/doc/v524/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/proof/doc/v524/index.html,2,['perform'],['performance']
Performance,"rationale, and buy-in from the LLVM; community as for any RFC. In some cases, parts of the codebase could be handled; as security-sensitive but need significant work to get to the stage where that's; manageable. The LLVM community will need to decide whether it wants to invest in; making these parts of the code securable, and maintain these security; properties over time. In all cases the LLVM Security Group should be consulted,; since they'll be responding to security issues filed against these parts of the; codebase. If you're not sure whether an issue is in-scope for this security process or; not, err towards assuming that it is. The Security Group might agree or disagree; and will explain its rationale in the report, as well as update this document; through the above process. The security-sensitive parts of the LLVM Project currently are the following.; Note that this list can change over time. * None are currently defined. Please don't let this stop you from reporting; issues to the security group that you believe are security-sensitive. The parts of the LLVM Project which are currently treated as non-security; sensitive are the following. Note that this list can change over time. * Language front-ends, such as clang, for which a malicious input file can cause; undesirable behavior. For example, a maliciously crafted C or Rust source file; can cause arbitrary code to execute in LLVM. These parts of LLVM haven't been; hardened, and compiling untrusted code usually also includes running utilities; such as `make` which can more readily perform malicious things. .. _CVE process: https://cve.mitre.org; .. _open a new issue: https://bugs.chromium.org/p/llvm/issues/entry; .. _chromium issue tracker: https://crbug.com; .. _GitHub security: https://help.github.com/en/articles/about-maintainer-security-advisories; .. _Discourse forums: https://discourse.llvm.org; .. _MITRE: https://cve.mitre.org; .. _example nomination is available here: https://reviews.llvm.org/D99232; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Security.rst:16235,perform,perform,16235,interpreter/llvm-project/llvm/docs/Security.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Security.rst,1,['perform'],['perform']
Performance,"rator, along with the enumeration and; description of opcodes, can be found in ``Opcodes.td``. The opcodes are; implemented as generic template methods in ``Interp.h`` and instantiated; with the relevant primitive types by the interpreter loop or by the; evaluating emitter. Primitive Types; ---------------. * ``PT_{U|S}int{8|16|32|64}``. Signed or unsigned integers of a specific bit width, implemented using; the ```Integral``` type. * ``PT_{U|S}intFP``. Signed or unsigned integers of an arbitrary, but fixed width used to; implement integral types which are required by the target, but are not; supported by the host. Under the hood, they rely on APValue. The; ``Integral`` specialisation for these types is required by opcodes to; share an implementation with fixed integrals. * ``PT_Bool``. Representation for boolean types, essentially a 1-bit unsigned; ``Integral``. * ``PT_RealFP``. Arbitrary, but fixed precision floating point numbers. Could be; specialised in the future similarly to integers in order to improve; floating point performance. * ``PT_Ptr``. Pointer type, defined in ``""Pointer.h""``. A pointer can be either null,; reference interpreter-allocated memory (``BlockPointer``) or point to an; address which can be derived, but not accessed (``ExternPointer``). * ``PT_FnPtr``. Function pointer type, can also be a null function pointer. Defined; in ``""FnPointer.h""``. * ``PT_MemPtr``. Member pointer type, can also be a null member pointer. Defined; in ``""MemberPointer.h""``. * ``PT_VoidPtr``. Void pointer type, can be used for round-trip casts. Represented as; the union of all pointers which can be cast to void.; Defined in ``""VoidPointer.h""``. * ``PT_ObjCBlockPtr``. Pointer type for ObjC blocks. Defined in ``""ObjCBlockPointer.h""``. Composite types; ---------------. The interpreter distinguishes two kinds of composite types: arrays and; records (structs and classes). Unions are represented as records, except; at most a single field can be marked as active. The content",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ConstantInterpreter.rst:2336,perform,performance,2336,interpreter/llvm-project/clang/docs/ConstantInterpreter.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ConstantInterpreter.rst,1,['perform'],['performance']
Performance,"rb	$5, %al; 	movsbl	%al, %eax. while it could get this:. 	movsbl	(%eax), %eax; 	sarl	$5, %eax. //===---------------------------------------------------------------------===//. GCC PR31029:. int test(int x) { return 1-x == x; } // --> return false; int test2(int x) { return 2-x == x; } // --> return x == 1 ?. Always foldable for odd constants, what is the rule for even?. //===---------------------------------------------------------------------===//. PR 3381: GEP to field of size 0 inside a struct could be turned into GEP; for next field in struct (which is at same address). For example: store of float into { {{}}, float } could be turned into a store to; the float directly. //===---------------------------------------------------------------------===//. The arg promotion pass should make use of nocapture to make its alias analysis; stuff much more precise. //===---------------------------------------------------------------------===//. The following functions should be optimized to use a select instead of a; branch (from gcc PR40072):. char char_int(int m) {if(m>7) return 0; return m;}; int int_char(char m) {if(m>7) return 0; return m;}. //===---------------------------------------------------------------------===//. int func(int a, int b) { if (a & 0x80) b |= 0x80; else b &= ~0x80; return b; }. Generates this:. define i32 @func(i32 %a, i32 %b) nounwind readnone ssp {; entry:; %0 = and i32 %a, 128 ; <i32> [#uses=1]; %1 = icmp eq i32 %0, 0 ; <i1> [#uses=1]; %2 = or i32 %b, 128 ; <i32> [#uses=1]; %3 = and i32 %b, -129 ; <i32> [#uses=1]; %b_addr.0 = select i1 %1, i32 %3, i32 %2 ; <i32> [#uses=1]; ret i32 %b_addr.0; }. However, it's functionally equivalent to:. b = (b & ~0x80) | (a & 0x80);. Which generates this:. define i32 @func(i32 %a, i32 %b) nounwind readnone ssp {; entry:; %0 = and i32 %b, -129 ; <i32> [#uses=1]; %1 = and i32 %a, 128 ; <i32> [#uses=1]; %2 = or i32 %0, %1 ; <i32> [#uses=1]; ret i32 %2; }. This can be generalized for other forms:. b = (b & ~0x80) | ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:43597,optimiz,optimized,43597,interpreter/llvm-project/llvm/lib/Target/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt,2,['optimiz'],['optimized']
Performance,rc/GiniIndex.cxx; src/GiniIndexWithLaplace.cxx; src/HyperParameterOptimisation.cxx; src/IFitterTarget.cxx; src/IMethod.cxx; src/Interval.cxx; src/KDEKernel.cxx; src/LDA.cxx; src/LogInterval.cxx; src/LossFunction.cxx; src/MCFitter.cxx; src/MethodANNBase.cxx; src/MethodBase.cxx; src/MethodBayesClassifier.cxx; src/MethodBDT.cxx; src/MethodBoost.cxx; src/MethodCategory.cxx; src/MethodCFMlpANN.cxx; src/MethodCFMlpANN_Utils.cxx; src/MethodCompositeBase.cxx; src/MethodCrossValidation.cxx; src/MethodCuts.cxx; src/MethodDL.cxx; src/MethodDNN.cxx; src/MethodDT.cxx; src/MethodFDA.cxx; src/MethodFisher.cxx; src/MethodHMatrix.cxx; src/MethodKNN.cxx; src/MethodLD.cxx; src/MethodLikelihood.cxx; src/MethodMLP.cxx; src/MethodPDEFoam.cxx; src/MethodPDERS.cxx; src/MethodPlugins.cxx; src/MethodRuleFit.cxx; src/MethodSVM.cxx; src/MethodTMlpANN.cxx; src/MinuitFitter.cxx; src/MinuitWrapper.cxx; src/MisClassificationError.cxx; src/ModulekNN.cxx; src/MsgLogger.cxx; src/NeuralNet.cxx; src/Node.cxx; src/OptimizeConfigParameters.cxx; src/Option.cxx; src/OptionMap.cxx; src/PDEFoamCell.cxx; src/PDEFoam.cxx; src/PDEFoamDecisionTree.cxx; src/PDEFoamDecisionTreeDensity.cxx; src/PDEFoamDensityBase.cxx; src/PDEFoamDiscriminant.cxx; src/PDEFoamDiscriminantDensity.cxx; src/PDEFoamEvent.cxx; src/PDEFoamEventDensity.cxx; src/PDEFoamKernelBase.cxx; src/PDEFoamKernelGauss.cxx; src/PDEFoamKernelLinN.cxx; src/PDEFoamKernelTrivial.cxx; src/PDEFoamMultiTarget.cxx; src/PDEFoamTarget.cxx; src/PDEFoamTargetDensity.cxx; src/PDEFoamVect.cxx; src/PDF.cxx; src/QuickMVAProbEstimator.cxx; src/Ranking.cxx; src/Reader.cxx; src/RegressionVariance.cxx; src/ResultsClassification.cxx; src/Results.cxx; src/ResultsMulticlass.cxx; src/ResultsRegression.cxx; src/ROCCalc.cxx; src/ROCCurve.cxx; src/RootFinder.cxx; src/RuleCut.cxx; src/Rule.cxx; src/RuleEnsemble.cxx; src/RuleFitAPI.cxx; src/RuleFit.cxx; src/RuleFitParams.cxx; src/SdivSqrtSplusB.cxx; src/SeparationBase.cxx; src/SimulatedAnnealing.cxx; src/SimulatedAnnealingFitter.cx,MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tmva/tmva/CMakeLists.txt:6938,Optimiz,OptimizeConfigParameters,6938,tmva/tmva/CMakeLists.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tmva/tmva/CMakeLists.txt,1,['Optimiz'],['OptimizeConfigParameters']
Performance,"rces in optimizations and we would like to show you; (probably outdated) performance results:. * Memory footprint -- mostly due to importing all C++ Modules at startup; we see overhead which depends on the number of preloaded modules. For; ROOT it is between 40-60 MB depending on the concrete configuration.; When the workload increases we notice that the overall memory performance; decreases in number of cases.; * Execution times -- likewise we have an execution overhead. For ; workflows which take ms the slowdown can be 2x. Increasing of the work; to seconds shows 50-60% slowdowns. The performance is dependent on many factors such as configuration of ROOT and; workflow. You can read more at our Intel IPCC-ROOT Showcase presentation; here (pp 25-33)[[8]]. #### Loading C++ Modules on Demand. In long term, we should optimize the preloading of modules to be a no-op and; avoid recursive behavior based on identifier lookup callbacks. Unfortunately,; at the moment the loading of C++ modules on demand shows significantly better; performance results. You can visit our continuous performance monitoring tool where we compare; the performance of ROOT against ROOT with a PCH [[9]].; *Note: if you get error 400, clean your cache or open a private browser session.*. ## How to use; C++ Modules in ROOT are default since v6.20 (Unix) and v6.22 (OSX). Enjoy. To disable C++ Modules in ROOT use `-Druntime_cxxmodules=Off`. ## Citing ROOT's C++ Modules; ```latex; % Peer-Reviewed Publication; %; % 22nd International Conference on Computing in High Energy and Nuclear Physics (CHEP); % 8-14 October, 2016, San Francisco, USA; %; @inproceedings{Vassilev_ROOTModules,; author = {Vassilev,V.},; title = {{Optimizing ROOT's Performance Using C++ Modules}},; journal = {Journal of Physics: Conference Series},; year = 2017,; month = {oct},; volume = {898},; number = {7},; pages = {072023},; doi = {10.1088/1742-6596/898/7/072023},; url = {https://iopscience.iop.org/article/10.1088/1742-6596/898/7/0720",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/README.CXXMODULES.md:19058,load,loading,19058,README/README.CXXMODULES.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/README.CXXMODULES.md,2,"['load', 'perform']","['loading', 'performance']"
Performance,"rchPath() function is used to retrieve a path to a DLL for; a subsequent LoadLibrary() call.; Source: ; MSDN: LoadLibrary function, Security Remarks. #include <windows.h>. HINSTANCE test() {; char filePath[100];; SearchPath(NULL, ""file.dll"", NULL, 100, filePath, NULL);; return LoadLibrary(filePath); // warn; }. WinAPI.WideCharToMultiByte; (C); Buffer overrun while calling WideCharToMultiByte(). The size of; the input buffer equals the number of characters in the Unicode string, while; the size of the output buffer equals the number of bytes.; Source: ; MSDN: WideCharToMultiByte function. #include <windows.h>. void test() {; wchar_t ws[] = L""abc"";; char s[3];; WideCharToMultiByte(CP_UTF8, 0, ws, -1, s,; 3, NULL, NULL); // warn; }. optimization. Name, DescriptionExampleProgress. optimization.PassConstObjByValue; (C, C++); Optimization: It is more effective to pass constant parameter by reference to; avoid unnecessary object copying. struct A {};. void f(const struct A a); // warn. optimization.PostfixIncIter; (C++); Optimization: It is more effective to use prefix increment operator with; iterator.; Source: Scott Meyers ""More Effective C++"", item 6:; Distinguish between prefix and postfix forms of increment and decrement; operators. #include <vector>. void test() {; std::vector<int> v;; std::vector<int>::const_iterator it;; for(it = v.begin();; it != v.end(); it++) {}; // warn; }. optimization.MultipleCallsStrlen; (C); Optimization: multiple calls to strlen() for a string in an; expression. It is more effective to hold a value returned; from strlen() in a temporary variable. #include <string.h>. void test(const char* s) {; if (strlen(s) > 0 &&; strlen(s) < 7) {}; // warn; }. optimization.StrLengthCalculation; (C++); Optimization: it is more efficient to use string::length() to; calculate the length of an std::string. #include <string>; #include <string.h>. void test() {; std::string s;; if (strlen(s.c_str()) != 0) {}; // warn; }. optimization.EmptyContainerDetect; (C+",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/analyzer/potential_checkers.html:27470,optimiz,optimization,27470,interpreter/llvm-project/clang/www/analyzer/potential_checkers.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/analyzer/potential_checkers.html,2,['optimiz'],['optimization']
Performance,"rd defines what an ""integer constant; expression"" (i-c-e) is with very precise and specific requirements. The; language then requires i-c-e's in a lot of places (for example, the size of a; bitfield, the value for a case statement, etc). For these, we have to be able; to constant fold the constants, to do semantic checks (e.g., verify bitfield; size is non-negative and that case statements aren't duplicated). We aim for; Clang to be very pedantic about this, diagnosing cases when the code does not; use an i-c-e where one is required, but accepting the code unless running with; ``-pedantic-errors``. Things get a little bit more tricky when it comes to compatibility with; real-world source code. Specifically, GCC has historically accepted a huge; superset of expressions as i-c-e's, and a lot of real world code depends on; this unfortunate accident of history (including, e.g., the glibc system; headers). GCC accepts anything its ""fold"" optimizer is capable of reducing to; an integer constant, which means that the definition of what it accepts changes; as its optimizer does. One example is that GCC accepts things like ""``case; X-X:``"" even when ``X`` is a variable, because it can fold this to 0. Another issue are how constants interact with the extensions we support, such; as ``__builtin_constant_p``, ``__builtin_inf``, ``__extension__`` and many; others. C99 obviously does not specify the semantics of any of these; extensions, and the definition of i-c-e does not include them. However, these; extensions are often used in real code, and we have to have a way to reason; about them. Finally, this is not just a problem for semantic analysis. The code generator; and other clients have to be able to fold constants (e.g., to initialize global; variables) and have to handle a superset of what C99 allows. Further, these; clients can benefit from extended information. For example, we know that; ""``foo() || 1``"" always evaluates to ``true``, but we can't replace the; expression wi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/InternalsManual.rst:119734,optimiz,optimizer,119734,interpreter/llvm-project/clang/docs/InternalsManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/InternalsManual.rst,2,['optimiz'],['optimizer']
Performance,"rd widening looks like replacement of. .. code-block:: text. %widenable_cond = call i1 @llvm.experimental.widenable.condition(); %guard_cond = and i1 %cond, %widenable_cond; br i1 %guard_cond, label %guarded, label %deopt. with. .. code-block:: text. %widenable_cond = call i1 @llvm.experimental.widenable.condition(); %new_cond = and i1 %any_other_cond, %widenable_cond; %new_guard_cond = and i1 %cond, %new_cond; br i1 %new_guard_cond, label %guarded, label %deopt. for this branch. Here `%any_other_cond` is an arbitrarily chosen; well-defined `i1` value. By making guard widening, we may; impose stricter conditions on `guarded` block and bail to the; deopt when the new condition is not met. Lowering:; """""""""""""""""". Default lowering strategy is replacing the result of; call of ``@llvm.experimental.widenable.condition`` with; constant `true`. However it is always correct to replace; it with any other `i1` value. Any pass can; freely do it if it can benefit from non-default lowering. '``llvm.load.relative``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare ptr @llvm.load.relative.iN(ptr %ptr, iN %offset) nounwind memory(argmem: read). Overview:; """""""""""""""""". This intrinsic loads a 32-bit value from the address ``%ptr + %offset``,; adds ``%ptr`` to that value and returns it. The constant folder specifically; recognizes the form of this intrinsic and the constant initializers it may; load from; if a loaded constant initializer is known to have the form; ``i32 trunc(x - %ptr)``, the intrinsic call is folded to ``x``. LLVM provides that the calculation of such a constant initializer will; not overflow at link time under the medium code model if ``x`` is an; ``unnamed_addr`` function. However, it does not provide this guarantee for; a constant initializer folded into a function body. This intrinsic can be; used to avoid the possibility of overflows when loading from such a constant. .. _llvm_sideeffect:. '``llvm.sideeffect``' Intrinsic; ^^^^^^^^^^^^^^^^^^",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:950723,load,load,950723,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,"rd32`` (G + GOT + A - P) >> 32; ``R_AMDGPU_REL32_LO`` Static 10 ``word32`` (S + A - P) & 0xFFFFFFFF; ``R_AMDGPU_REL32_HI`` Static 11 ``word32`` (S + A - P) >> 32; *reserved* 12; ``R_AMDGPU_RELATIVE64`` Dynamic 13 ``word64`` B + A; ``R_AMDGPU_REL16`` Static 14 ``word16`` ((S + A - P) - 4) / 4; ========================== ======= ===== ========== ==============================. ``R_AMDGPU_ABS32_LO`` and ``R_AMDGPU_ABS32_HI`` are only supported by; the ``mesa3d`` OS, which does not support ``R_AMDGPU_ABS64``. There is no current OS loader support for 32-bit programs and so; ``R_AMDGPU_ABS32`` is not used. .. _amdgpu-loaded-code-object-path-uniform-resource-identifier:. Loaded Code Object Path Uniform Resource Identifier (URI); ---------------------------------------------------------. The AMD GPU code object loader represents the path of the ELF shared object from; which the code object was loaded as a textual Uniform Resource Identifier (URI).; Note that the code object is the in memory loaded relocated form of the ELF; shared object. Multiple code objects may be loaded at different memory; addresses in the same process from the same ELF shared object. The loaded code object path URI syntax is defined by the following BNF syntax:. .. code::. code_object_uri ::== file_uri | memory_uri; file_uri ::== ""file://"" file_path [ range_specifier ]; memory_uri ::== ""memory://"" process_id range_specifier; range_specifier ::== [ ""#"" | ""?"" ] ""offset="" number ""&"" ""size="" number; file_path ::== URI_ENCODED_OS_FILE_PATH; process_id ::== DECIMAL_NUMBER; number ::== HEX_NUMBER | DECIMAL_NUMBER | OCTAL_NUMBER. **number**; Is a C integral literal where hexadecimal values are prefixed by ""0x"" or ""0X"",; and octal values by ""0"". **file_path**; Is the file's path specified as a URI encoded UTF-8 string. In URI encoding,; every character that is not in the regular expression ``[a-zA-Z0-9/_.~-]`` is; encoded as two uppercase hexadecimal digits proceeded by ""%"". Directories in; the path are separa",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:83039,load,loaded,83039,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['loaded']
Performance,"rd[=<format>]. Enable optimization remarks during compilation and write them to a separate; file. This option, which defaults to off, controls whether Clang writes; optimization reports to a separate file. By recording diagnostics in a file,; users can parse or sort the remarks in a convenient way. By default, the serialization format is YAML. The supported serialization formats are:. - .. _opt_fsave_optimization_record_yaml:. ``-fsave-optimization-record=yaml``: A structured YAML format. - .. _opt_fsave_optimization_record_bitstream:. ``-fsave-optimization-record=bitstream``: A binary format based on LLVM; Bitstream. The output file is controlled by :option:`-foptimization-record-file`. In the absence of an explicit output file, the file is chosen using the; following scheme:. ``<base>.opt.<format>``. where ``<base>`` is based on the output file of the compilation (whether; it's explicitly specified through `-o` or not) when used with `-c` or `-S`.; For example:. * ``clang -fsave-optimization-record -c in.c -o out.o`` will generate; ``out.opt.yaml``. * ``clang -fsave-optimization-record -c in.c`` will generate; ``in.opt.yaml``. When targeting (Thin)LTO, the base is derived from the output filename, and; the extension is not dropped. When targeting ThinLTO, the following scheme is used:. ``<base>.opt.<format>.thin.<num>.<format>``. Darwin-only: when used for generating a linked binary from a source file; (through an intermediate object file), the driver will invoke `cc1` to; generate a temporary object file. The temporary remark file will be emitted; next to the object file, which will then be picked up by `dsymutil` and; emitted in the .dSYM bundle. This is available for all formats except YAML. For example:. ``clang -fsave-optimization-record=bitstream in.c -o out`` will generate. * ``/var/folders/43/9y164hh52tv_2nrdxrj31nyw0000gn/T/a-9be59b.o``. * ``/var/folders/43/9y164hh52tv_2nrdxrj31nyw0000gn/T/a-9be59b.opt.bitstream``. * ``out``. * ``out.dSYM/Contents/Resourc",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:11844,optimiz,optimization-record,11844,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,1,['optimiz'],['optimization-record']
Performance,"rder of 9 seconds on alpha), this should only be used for small; timings. Semantics:; """""""""""""""""""". When directly supported, reading the cycle counter should not modify any; memory. Implementations are allowed to either return an application; specific value or a system wide value. On backends without support, this; is lowered to a constant 0. Note that runtime support may be conditional on the privilege-level code is; running at and the host platform. '``llvm.clear_cache``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare void @llvm.clear_cache(ptr, ptr). Overview:; """""""""""""""""". The '``llvm.clear_cache``' intrinsic ensures visibility of modifications; in the specified range to the execution unit of the processor. On; targets with non-unified instruction and data cache, the implementation; flushes the instruction cache. Semantics:; """""""""""""""""""". On platforms with coherent instruction and data caches (e.g. x86), this; intrinsic is a nop. On platforms with non-coherent instruction and data; cache (e.g. ARM, MIPS), the intrinsic is lowered either to appropriate; instructions or a system call, if cache flushing requires special; privileges. The default behavior is to emit a call to ``__clear_cache`` from the run; time library. This intrinsic does *not* empty the instruction pipeline. Modifications; of the current function are outside the scope of the intrinsic. '``llvm.instrprof.increment``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare void @llvm.instrprof.increment(ptr <name>, i64 <hash>,; i32 <num-counters>, i32 <index>). Overview:; """""""""""""""""". The '``llvm.instrprof.increment``' intrinsic can be emitted by a; frontend for use with instrumentation based profiling. These will be; lowered by the ``-instrprof`` pass to generate execution counts of a; program at runtime. Arguments:; """""""""""""""""""". The first argument is a pointer to a global variable containing the; name of the entity being instrumented. This should genera",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:526410,cache,cache,526410,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['cache'],['cache']
Performance,"rdered and less than; #. ``ole``: ordered and less than or equal; #. ``one``: ordered and not equal; #. ``ord``: ordered (no nans); #. ``ueq``: unordered or equal; #. ``ugt``: unordered or greater than; #. ``uge``: unordered or greater than or equal; #. ``ult``: unordered or less than; #. ``ule``: unordered or less than or equal; #. ``une``: unordered or not equal; #. ``uno``: unordered (either nans); #. ``true``: no comparison, always returns true. *Ordered* means that neither operand is a QNAN while *unordered* means; that either operand may be a QNAN. Each of ``val1`` and ``val2`` arguments must be either a :ref:`floating-point; <t_floating>` type or a :ref:`vector <t_vector>` of floating-point type.; They must have identical types. Semantics:; """""""""""""""""""". The '``fcmp``' instruction compares ``op1`` and ``op2`` according to the; condition code given as ``cond``. If the operands are vectors, then the; vectors are compared element by element. Each comparison performed; always yields an :ref:`i1 <t_integer>` result, as follows:. #. ``false``: always yields ``false``, regardless of operands.; #. ``oeq``: yields ``true`` if both operands are not a QNAN and ``op1``; is equal to ``op2``.; #. ``ogt``: yields ``true`` if both operands are not a QNAN and ``op1``; is greater than ``op2``.; #. ``oge``: yields ``true`` if both operands are not a QNAN and ``op1``; is greater than or equal to ``op2``.; #. ``olt``: yields ``true`` if both operands are not a QNAN and ``op1``; is less than ``op2``.; #. ``ole``: yields ``true`` if both operands are not a QNAN and ``op1``; is less than or equal to ``op2``.; #. ``one``: yields ``true`` if both operands are not a QNAN and ``op1``; is not equal to ``op2``.; #. ``ord``: yields ``true`` if both operands are not a QNAN.; #. ``ueq``: yields ``true`` if either operand is a QNAN or ``op1`` is; equal to ``op2``.; #. ``ugt``: yields ``true`` if either operand is a QNAN or ``op1`` is; greater than ``op2``.; #. ``uge``: yields ``true`` if either ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:464633,perform,performed,464633,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performed']
Performance,"rdering problems so are fine to use ``LDR``/``STR``. 2. Create code generation patterns for bitconverts that create ``REV`` instructions. 3. Make sure appropriate bitconverts are created so that vector values get passed over call boundaries as 1-element vectors (which is the same as if they were loaded with ``LDR``). Bitconverts; -----------. .. image:: ARM-BE-bitcastfail.png; :align: right. The main problem with the ``LD1`` solution is dealing with bitconverts (or bitcasts, or reinterpret casts). These are pseudo instructions that only change the compiler's interpretation of data, not the underlying data itself. A requirement is that if data is loaded and then saved again (called a ""round trip""), the memory contents should be the same after the store as before the load. If a vector is loaded and is then bitconverted to a different vector type before storing, the round trip will currently be broken. Take for example this code sequence::. %0 = load <4 x i32> %x; %1 = bitcast <4 x i32> %0 to <2 x i64>; store <2 x i64> %1, <2 x i64>* %y. This would produce a code sequence such as that in the figure on the right. The mismatched ``LD1`` and ``ST1`` cause the stored data to differ from the loaded data. .. container:: clearer. When we see a bitcast from type ``X`` to type ``Y``, what we need to do is to change the in-register representation of the data to be *as if* it had just been loaded by a ``LD1`` of type ``Y``. .. image:: ARM-BE-bitcastsuccess.png; :align: right. Conceptually this is simple - we can insert a ``REV`` undoing the ``LD1`` of type ``X`` (converting the in-register representation to the same as if it had been loaded by ``LDR``) and then insert another ``REV`` to change the representation to be as if it had been loaded by an ``LD1`` of type ``Y``. For the previous example, this would be::. LD1 v0.4s, [x]. REV64 v0.4s, v0.4s // There is no REV128 instruction, so it must be synthesizedcd; EXT v0.16b, v0.16b, v0.16b, #8 // with a REV64 then an EXT to swap the ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BigEndianNEON.rst:10393,load,load,10393,interpreter/llvm-project/llvm/docs/BigEndianNEON.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BigEndianNEON.rst,1,['load'],['load']
Performance,"rding to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/load atomic; atomicrmw-with-return-value.; - s_waitcnt vscnt(0); must happen after; any preceding; global/generic; store/store atomic/; atomicrmw-no-return-value.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; have; completed before; performing the; atomicrmw that is; being released. 2. flat_atomic; 3. s_waitcnt vm/vscnt(0) &; lgkmcnt(0). - If OpenCL, omit; lgkmcnt(0).; - Use vmcnt(0) if atomic with; return and vscnt(0) if; atomic with no-return.; - Must happen before; following; buffer_gl*_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; caches. 4. buffer_gl0_inv;; buffer_gl1_inv. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. fence acq_rel - singlethread *none* *none*; - wavefront; fence acq_rel - workgroup *none* 1. s_waitcnt lgkmcnt(0) &; vmcnt(0) & vscnt(0). - If CU wavefront execution; mode, omit vmcnt(0) and; vscnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0) and vscnt(0).; - However,; since LLVM; currently has no; address space on; the fence need to; conservatively; always generate; (see comment for; previous fence).; - Could be split into; separate s_waitcnt; vmcnt(0), s_waitcnt; vscnt(0) and s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/load; atomic/; atomicrmw-with-return-value.; - s_waitcnt vscnt(0); must happen after; any preceding; global/generic; store/store atomic/; atomicrmw-no-return-value.; - s_waitcnt lgkmcnt(0); must happen after; any precedin",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:369395,load,loads,369395,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['loads']
Performance,"rds** compatibility with the format of the; coverage mappings emitted into instrumented binaries. These formats are not; forwards-compatible. * The JSON coverage export format has a (major, minor, patch) version triple.; Only a major version increment indicates a backwards-incompatible change. A; minor version increment is for added functionality, and patch version; increments are for bugfixes. Impact of llvm optimizations on coverage reports; ================================================. llvm optimizations (such as inlining or CFG simplification) should have no; impact on coverage report quality. This is due to the fact that the mapping; from source regions to profile counters is immutable, and is generated before; the llvm optimizer kicks in. The optimizer can't prove that profile counter; instrumentation is safe to delete (because it's not: it affects the profile the; program emits), and so leaves it alone. Note that this coverage feature does not rely on information that can degrade; during the course of optimization, such as debug info line tables. Using the profiling runtime without static initializers; =======================================================. By default the compiler runtime uses a static initializer to determine the; profile output path and to register a writer function. To collect profiles; without using static initializers, do this manually:. * Export a ``int __llvm_profile_runtime`` symbol from each instrumented shared; library and executable. When the linker finds a definition of this symbol, it; knows to skip loading the object which contains the profiling runtime's; static initializer. * Forward-declare ``void __llvm_profile_initialize_file(void)`` and call it; once from each instrumented executable. This function parses; ``LLVM_PROFILE_FILE``, sets the output path, and truncates any existing files; at that path. To get the same behavior without truncating existing files,; pass a filename pattern string to ``void __llvm_profile_set_f",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/SourceBasedCodeCoverage.rst:15242,optimiz,optimization,15242,interpreter/llvm-project/clang/docs/SourceBasedCodeCoverage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/SourceBasedCodeCoverage.rst,1,['optimiz'],['optimization']
Performance,"re (AArch64), 4.1.2 Short Vectors. The use of ``LDR`` and ``STR`` as the ABI defines has at least one advantage over ``LD1`` and ``ST1``. ``LDR`` and ``STR`` are oblivious to the size of the individual lanes of a vector. ``LD1`` and ``ST1`` are not - the lane size is encoded within them. This is important across an ABI boundary, because it would become necessary to know the lane width the callee expects. Consider the following code:. .. code-block:: c. <callee.c>; void callee(uint32x2_t v) {; ...; }. <caller.c>; extern void callee(uint32x2_t);; void caller() {; callee(...);; }. If ``callee`` changed its signature to ``uint16x4_t``, which is equivalent in register content, if we passed as ``LD1`` we'd break this code until ``caller`` was updated and recompiled. There is an argument that if the signatures of the two functions are different then the behaviour should be undefined. But there may be functions that are agnostic to the lane layout of the vector, and treating the vector as an opaque value (just loading it and storing it) would be impossible without a common format across ABI boundaries. So to preserve ABI compatibility, we need to use the ``LDR`` lane layout across function calls. Alignment; ---------. In strict alignment mode, ``LDR qX`` requires its address to be 128-bit aligned, whereas ``LD1`` only requires it to be as aligned as the lane size. If we canonicalised on using ``LDR``, we'd still need to use ``LD1`` in some places to avoid alignment faults (the result of the ``LD1`` would then need to be reversed with ``REV``). Most operating systems however do not run with alignment faults enabled, so this is often not an issue. Summary; -------. The following table summarises the instructions that are required to be emitted for each property mentioned above for each of the two solutions. +-------------------------------+-------------------------------+---------------------+; | | ``LDR`` layout | ``LD1`` layout |; +===============================+==========",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BigEndianNEON.rst:7238,load,loading,7238,interpreter/llvm-project/llvm/docs/BigEndianNEON.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BigEndianNEON.rst,1,['load'],['loading']
Performance,"re Libraries. ### New command line flag ""--version"" for root. `root --version` now displays ROOT version and build info and quits:. ```; ROOT Version: 6.15/01; Built for linuxx8664gcc on Sep 20 2018, 11:04:35; From heads/master@v6-13-04-1273-gea3f4333a2; ```. ### Fish support for thisroot script. `. bin/thisroot.fish` sets up the needed ROOT environment variables for one of the ROOT team's favorite shells, the [fish shell](https://fishshell.com/). ### Change of setting the compression algorithm in `rootrc`. The previous setting called `ROOT.ZipMode` is now unused and ignored.; Instead, use `Root.CompressionAlgorithm` which sets the compression algorithm according to the values of [ECompression](https://root.cern/doc/master/Compression_8h.html#a0a7df9754a3b7be2b437f357254a771c):. * 0: use the default value of `R__ZipMode` (currently selecting ZLIB); * 1: use ZLIB (the default until 6.12 and from 6.16); * 2: use LZMA; * 3: legacy, please don't use; * 4: LZ4. ### TRef. * Improve thread scalability of `TRef`. Creating and looking up a lot of `TRef` from the same `processID` now has practically perfect weak scaling. ### Parallelism; * Upgrade the built-in TBB version to 2019_U1. ### Type System; * Upgrade the `TClass::GetMissingDictionaries` method to support `std::unique_ptr`, `std::array` and `std::tuple` without getting trapped in the internal STL implementation details. ## I/O Libraries. * To allow for increase run-time performance and increase thread scalability the override ability of `TFile::GetStreamerInfoList` is replaced by an override of `TFile::GetStreamerInfoListImp` with updated return type and arguments. If a class override `TFile::GetStreamerInfoList` you will now see a compilation error like:. ```; /opt/build/root_builds/rootcling.cmake/include/TSQLFile.h:225:19: error: declaration of 'GetStreamerInfoList' overrides a 'final' function; virtual TList *GetStreamerInfoList();; ^; /opt/build/root_builds/rootcling.cmake/include/TFile.h:231:24: note: overridde",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v616/index.md:3524,scalab,scalability,3524,README/ReleaseNotes/v616/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v616/index.md,1,['scalab'],['scalability']
Performance,"re alignment-independent. For; MIPS, it can make a big difference. Note that if your loads and stores are atomic, the backend will be unable to; lower an under aligned access into a sequence of natively aligned accesses.; As a result, alignment is mandatory for atomic loads and stores. Other Things to Consider; ^^^^^^^^^^^^^^^^^^^^^^^^. #. Use ptrtoint/inttoptr sparingly (they interfere with pointer aliasing; analysis), prefer GEPs. #. Prefer globals over inttoptr of a constant address - this gives you; dereferencability information. In MCJIT, use getSymbolAddress to provide; actual address. #. Be wary of ordered and atomic memory operations. They are hard to optimize; and may not be well optimized by the current optimizer. Depending on your; source language, you may consider using fences instead. #. If calling a function which is known to throw an exception (unwind), use; an invoke with a normal destination which contains an unreachable; instruction. This form conveys to the optimizer that the call returns; abnormally. For an invoke which neither returns normally or requires unwind; code in the current function, you can use a noreturn call instruction if; desired. This is generally not required because the optimizer will convert; an invoke with an unreachable unwind destination to a call instruction. #. Use profile metadata to indicate statically known cold paths, even if; dynamic profiling information is not available. This can make a large; difference in code placement and thus the performance of tight loops. #. When generating code for loops, try to avoid terminating the header block of; the loop earlier than necessary. If the terminator of the loop header; block is a loop exiting conditional branch, the effectiveness of LICM will; be limited for loads not in the header. (This is due to the fact that LLVM; may not know such a load is safe to speculatively execute and thus can't; lift an otherwise loop invariant load unless it can prove the exiting; condition is ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Frontend/PerformanceTips.rst:6680,optimiz,optimizer,6680,interpreter/llvm-project/llvm/docs/Frontend/PerformanceTips.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Frontend/PerformanceTips.rst,1,['optimiz'],['optimizer']
Performance,"re are two types of lines in the function body. - Sampled line represents the profile information of a source location.; ``offsetN[.discriminator]: number_of_samples [fn5:num fn6:num ... ]``. - Callsite line represents the profile information of an inlined callsite.; ``offsetA[.discriminator]: fnA:num_of_total_samples``. Each sampled line may contain several items. Some are optional (marked; below):. a. Source line offset. This number represents the line number; in the function where the sample was collected. The line number is; always relative to the line where symbol of the function is; defined. So, if the function has its header at line 280, the offset; 13 is at line 293 in the file. Note that this offset should never be a negative number. This could; happen in cases like macros. The debug machinery will register the; line number at the point of macro expansion. So, if the macro was; expanded in a line before the start of the function, the profile; converter should emit a 0 as the offset (this means that the optimizers; will not be able to associate a meaningful weight to the instructions; in the macro). b. [OPTIONAL] Discriminator. This is used if the sampled program; was compiled with DWARF discriminator support; (http://wiki.dwarfstd.org/index.php?title=Path_Discriminators).; DWARF discriminators are unsigned integer values that allow the; compiler to distinguish between multiple execution paths on the; same source line location. For example, consider the line of code ``if (cond) foo(); else bar();``.; If the predicate ``cond`` is true 80% of the time, then the edge; into function ``foo`` should be considered to be taken most of the; time. But both calls to ``foo`` and ``bar`` are at the same source; line, so a sample count at that line is not sufficient. The; compiler needs to know which part of that line is taken more; frequently. This is what discriminators provide. In this case, the calls to; ``foo`` and ``bar`` will be at the same line, but will have; diff",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:99793,optimiz,optimizers,99793,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,1,['optimiz'],['optimizers']
Performance,"re both called UploadFiles and differ in the first argument,; which is used to pass the files to be uploaded. These can be given as a; list (of TFileInfo or TObjString), a directory or specified in a text; file.; Add support for paralell dataset verification. This is; implemented via a dedicated TSelector (TSelVerifyDataSet) which is run; over the list of files in the dataset via TPacketizerFile. The file; order is preserved using the recently introduced index in TFileInfo.; In TProofOutputFile, add switch to control the way histograms; are merged by TFileMerger, i.e. one-by-one or all-in-one-go. The; default is one-by-one which requires much less memory. Merging in; one-go (the previous default) can be activated by passing 'H' in the; constructor options.; In ProofBench, add possibility to change the location of the; generated files via the third argument of TProofBench::MakeDataSet.; Several optimizations in the low level PROOF event loop; (TProofPlayer::Process),  allowing to reduce dramatically the; overhead introduced by the operations PROOF needs to perform during the; event loop. A measurement of the overhead can be obtained from a very; light computational task, for example, generating one random number and; filling one histogram; executing this task within a PROOF-Lite session; with 1 worker now takes only 1.8 times the time required by a straight; loop in the parent ROOT session; the same number before was about 13. ; In TDrawFeedback::Feedback, call method Draw() of objects not; identified as TH1 derivation. This allows user-defined objects; implementing Draw to be displayed via this utility class.; In TProof::LoadPackageOnClient, do not create a symlink; 'pack_name' to the package dir, but add directly the package dir to the; include path. This solves the longstanding annoying problem of failure; when a directory or file with the name of the package did already exist; in the local working directory. . Fixes; ; Fix merging issue affecting automatic datase",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/proof/doc/v534/index.html:3437,optimiz,optimizations,3437,proof/doc/v534/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/proof/doc/v534/index.html,4,"['optimiz', 'perform']","['optimizations', 'perform']"
Performance,"re branch weights are; created and assigned based on profiling information or the use of the; ``llvm.expect`` intrinsic, and our implementation focuses on these; places to perform the verification. We calculate the threshold for emitting MisExpect related diagnostics; based on the values the compiler assigns to ``llvm.expect`` intrinsics,; which can be set through the ``-likely-branch-weight`` and; ``-unlikely-branch-weight`` LLVM options. During verification, if the; profile weights mismatch the calculated threshold, then we will emit a; remark or warning detailing a potential performance regression. The; diagnostic also reports the percentage of the time the annotation was; correct during profiling to help developers reason about how to proceed. The diagnostics are also available in the form of optimization remarks,; which can be serialized and processed through the ``opt-viewer.py``; scripts in LLVM. .. option:: -pass-remarks=misexpect. Enables optimization remarks for misexpect when profiling data conflicts with; use of ``llvm.expect`` intrinsics. .. option:: -pgo-warn-misexpect. Enables misexpect warnings when profiling data conflicts with use of; ``llvm.expect`` intrinsics. LLVM supports 4 types of profile formats: Frontend, IR, CS-IR, and; Sampling. MisExpect Diagnostics are compatible with all Profiling formats. +----------------+--------------------------------------------------------------------------------------+; | Profile Type | Description |; +================+======================================================================================+; | Frontend | Profiling instrumentation added during compilation by the frontend, i.e. ``clang`` |; +----------------+--------------------------------------------------------------------------------------+; | IR | Profiling instrumentation added during by the LLVM backend |; +----------------+--------------------------------------------------------------------------------------+; | CS-IR | Context Sensitive IR",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MisExpect.rst:2336,optimiz,optimization,2336,interpreter/llvm-project/llvm/docs/MisExpect.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MisExpect.rst,1,['optimiz'],['optimization']
Performance,"re byte ranges addressed (e.g., through an offset in a file or an object ID). That means that new backends are likely to have implications on the RNTuple format specification. The page sources and sinks are ROOT internal classes.; They are not meant to be extended by users. Multi-Threading; ---------------. The following options exist in RNTuple for multithreaded data processing. ### Implicit Multi-Threading; When `ROOT::EnableImplicitMT()` is used, RNTuple uses ROOT's task arena to compress and decompress pages.; That requires writes to be buffered and reads uses the cluster pool resp.; The RNTuple data source for RDataFrame lets RDataFrame full control of the thread pool.; That means that RDataFrame uses a separate data source for every thread, each of the data sources runs in sequential mode. ### Concurrent Readers; Multiple readers can read the same RNTuple concurrently as long as access to every individual reader is sequential. ### Parallel REntry Preparation; Multiple `REntry` object can be concurrently prepared by multiple threads.; I.e., construction and binding of the objects can happen in parallel.; The actual reading and writing of entries (`RNTupleReader::LoadEntry()`, `RNTupleWriter::Fill()`) needs to be protected by a mutex.; This is considered ""mild scalability parallelization"" in RNTuple. ### RNTupleParallelWriter; The parallel writer offers the most scalable parallel writing interface.; Multiple _fill contexts_ can concurrently serialize and compress data.; Every fill context prepares a set of entire clusters in the final on-disk layout.; When a fill context flushes data,; a brief serialization point handles the RNTuple meta-data updates and the reservation of disk space to write into. Low precision float types; --------------------------; RNTuple supports encoding floating point types with a lower precision when writing them to disk. This encoding is specified by the; user per field and it is independent on the in-memory type used for that field (m",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/ntuple/v7/doc/Architecture.md:25107,concurren,concurrently,25107,tree/ntuple/v7/doc/Architecture.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/ntuple/v7/doc/Architecture.md,1,['concurren'],['concurrently']
Performance,"re dozens of passes inside the compiler, each of these flags; take a regular expression that identifies the name of the pass which should; emit the associated diagnostic. For example, to get a report from the inliner,; compile the code with:. .. code-block:: console. $ clang -O2 -Rpass=inline code.cc -o code; code.cc:4:25: remark: foo inlined into bar [-Rpass=inline]; int bar(int j) { return foo(j, j - 2); }; ^. Note that remarks from the inliner are identified with `[-Rpass=inline]`.; To request a report from every optimization pass, you should use; `-Rpass=.*` (in fact, you can use any valid POSIX regular; expression). However, do not expect a report from every transformation; made by the compiler. Optimization remarks do not really make sense; outside of the major transformations (e.g., inlining, vectorization,; loop optimizations) and not every optimization pass supports this; feature. Note that when using profile-guided optimization information, profile hotness; information can be included in the remarks (see; :ref:`-fdiagnostics-show-hotness <opt_fdiagnostics-show-hotness>`). Current limitations; ^^^^^^^^^^^^^^^^^^^. 1. Optimization remarks that refer to function names will display the; mangled name of the function. Since these remarks are emitted by the; back end of the compiler, it does not know anything about the input; language, nor its mangling rules. 2. Some source locations are not displayed correctly. The front end has; a more detailed source location tracking than the locations included; in the debug info (e.g., the front end can locate code inside macro; expansions). However, the locations used by `-Rpass` are; translated from debug annotations. That translation can be lossy,; which results in some remarks having no location information. Options to Emit Resource Consumption Reports; --------------------------------------------. These are options that report execution time and consumed memory of different; compilations steps. .. option:: -fproc-stat-r",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:25258,optimiz,optimization,25258,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,1,['optimiz'],['optimization']
Performance,"re filename, which; is the convention used by NMake and Jom. .. option:: -femit-dwarf-unwind=<value>. When to emit DWARF unwind (EH frame) info. This is a Mach-O-specific option. Valid values are:. * ``no-compact-unwind`` - Only emit DWARF unwind when compact unwind encodings; aren't available. This is the default for arm64.; * ``always`` - Always emit DWARF unwind regardless.; * ``default`` - Use the platform-specific default (``always`` for all; non-arm64-platforms). ``no-compact-unwind`` is a performance optimization -- Clang will emit smaller; object files that are more quickly processed by the linker. This may cause; binary compatibility issues on older x86_64 targets, however, so use it with; caution. .. _configuration-files:. Configuration files; -------------------. Configuration files group command-line options and allow all of them to be; specified just by referencing the configuration file. They may be used, for; example, to collect options required to tune compilation for particular; target, such as ``-L``, ``-I``, ``-l``, ``--sysroot``, codegen options, etc. Configuration files can be either specified on the command line or loaded; from default locations. If both variants are present, the default configuration; files are loaded first. The command line option ``--config=`` can be used to specify explicit; configuration files in a Clang invocation. If the option is used multiple times,; all specified files are loaded, in order. For example:. ::. clang --config=/home/user/cfgs/testing.txt; clang --config=debug.cfg --config=runtimes.cfg. If the provided argument contains a directory separator, it is considered as; a file path, and options are read from that file. Otherwise the argument is; treated as a file name and is searched for sequentially in the directories:. - user directory,; - system directory,; - the directory where Clang executable resides. Both user and system directories for configuration files are specified during; clang build using CMake param",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:30746,tune,tune,30746,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,1,['tune'],['tune']
Performance,"re three other base classes: ``F3_1`` for register/register; operations, ``F3_2`` for register/immediate operations, and ``F3_3`` for; floating-point operations. ``SparcInstrInfo.td`` also adds the base class; ``Pseudo`` for synthetic SPARC instructions. ``SparcInstrInfo.td`` largely consists of operand and instruction definitions; for the SPARC target. In ``SparcInstrInfo.td``, the following target; description file entry, ``LDrr``, defines the Load Integer instruction for a; Word (the ``LD`` SPARC opcode) from a memory address to a register. The first; parameter, the value 3 (``11``\ :sub:`2`), is the operation value for this; category of operation. The second parameter (``000000``\ :sub:`2`) is the; specific operation value for ``LD``/Load Word. The third parameter is the; output destination, which is a register operand and defined in the ``Register``; target description file (``IntRegs``). .. code-block:: text. def LDrr : F3_1 <3, 0b000000, (outs IntRegs:$rd), (ins (MEMrr $rs1, $rs2):$addr),; ""ld [$addr], $dst"",; [(set i32:$dst, (load ADDRrr:$addr))]>;. The fourth parameter is the input source, which uses the address operand; ``MEMrr`` that is defined earlier in ``SparcInstrInfo.td``:. .. code-block:: text. def MEMrr : Operand<i32> {; let PrintMethod = ""printMemOperand"";; let MIOperandInfo = (ops IntRegs, IntRegs);; }. The fifth parameter is a string that is used by the assembly printer and can be; left as an empty string until the assembly printer interface is implemented.; The sixth and final parameter is the pattern used to match the instruction; during the SelectionDAG Select Phase described in :doc:`CodeGenerator`.; This parameter is detailed in the next section, :ref:`instruction-selector`. Instruction class definitions are not overloaded for different operand types,; so separate versions of instructions are needed for register, memory, or; immediate value operands. For example, to perform a Load Integer instruction; for a Word from an immediate operand to ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMBackend.rst:33084,load,load,33084,interpreter/llvm-project/llvm/docs/WritingAnLLVMBackend.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMBackend.rst,1,['load'],['load']
Performance,"re under the control of ARC:. * objects of static, automatic, and temporary storage duration; * instance variables of Objective-C objects; * elements of arrays where the array object's initialization and; deinitialization are under the control of ARC; * fields of Objective-C struct types where the struct object's; initialization and deinitialization are under the control of ARC; * non-static data members of Objective-C++ non-union class types; * Objective-C++ objects and arrays of dynamic storage duration created; with the ``new`` or ``new[]`` operators and destroyed with the; corresponding ``delete`` or ``delete[]`` operator. They are not followed automatically for these objects:. * objects of dynamic storage duration created in other memory, such as; that returned by ``malloc``; * union members. .. admonition:: Rationale. ARC must perform special operations when initializing an object and; when destroying it. In many common situations, ARC knows when an; object is created and when it is destroyed and can ensure that these; operations are performed correctly. Otherwise, however, ARC requires; programmer cooperation to establish its initialization invariants; because it is infeasible for ARC to dynamically infer whether they; are intact. For example, there is no syntactic difference in C between; an assignment that is intended by the programmer to initialize a variable; and one that is intended to replace the existing value stored there,; but ARC must perform one operation or the other. ARC chooses to always; assume that objects are initialized (except when it is in charge of; initializing them) because the only workable alternative would be to; ban all code patterns that could potentially be used to access; uninitialized memory, and that would be too limiting. In practice,; this is rarely a problem because programmers do not generally need to; work with objects for which the requirements are not handled; automatically. Note that dynamically-allocated Objective-C++ ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst:47149,perform,performed,47149,interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,1,['perform'],['performed']
Performance,"re you obtained LLVM (if not from our Git; repository). Thanks for helping us make LLVM better!. .. _crashes the compiler:. Crashing Bugs; =============. More often than not, bugs in the compiler cause it to crash---often due to; an assertion failure of some sort. The most important piece of the puzzle; is to figure out if it is crashing in the Clang front-end or if it is one of; the LLVM libraries (e.g. the optimizer or code generator) that has; problems. To figure out which component is crashing (the front-end, middle-end; optimizer, or backend code generator), run the ``clang`` command line as you; were when the crash occurred, but with the following extra command line; options:. * ``-emit-llvm -Xclang -disable-llvm-passes``: If ``clang`` still crashes when; passed these options (which disable the optimizer and code generator), then; the crash is in the front-end. Jump ahead to :ref:`front-end bugs; <frontend-crash>`. * ``-emit-llvm``: If ``clang`` crashes with this option (which disables; the code generator), you found a middle-end optimizer bug. Jump ahead to; :ref:`middle-end bugs <middleend-crash>`. * Otherwise, you have a backend code generator crash. Jump ahead to :ref:`code; generator bugs <backend-crash>`. .. _frontend-crash:. Front-end bugs; --------------. On a ``clang`` crash, the compiler will dump a preprocessed file and a script; to replay the ``clang`` command. For example, you should see something like. .. code-block:: text. PLEASE ATTACH THE FOLLOWING FILES TO THE BUG REPORT:; Preprocessed source(s) and associated run script(s) are located at:; clang: note: diagnostic msg: /tmp/foo-xxxxxx.c; clang: note: diagnostic msg: /tmp/foo-xxxxxx.sh. The `creduce <https://github.com/csmith-project/creduce>`_ tool helps to; reduce the preprocessed file down to the smallest amount of code that still; replicates the problem. You're encouraged to use creduce to reduce the code; to make the developers' lives easier. The; ``clang/utils/creduce-clang-crash.py`` sc",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToSubmitABug.rst:2332,optimiz,optimizer,2332,interpreter/llvm-project/llvm/docs/HowToSubmitABug.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToSubmitABug.rst,1,['optimiz'],['optimizer']
Performance,"re(cxx_user_literals)`` to determine if support for; user-defined literals is enabled. C++11 variadic templates; ^^^^^^^^^^^^^^^^^^^^^^^^. Use ``__has_feature(cxx_variadic_templates)`` or; ``__has_extension(cxx_variadic_templates)`` to determine if support for; variadic templates is enabled. C++14; -----. The features listed below are part of the C++14 standard. As a result, all; these features are enabled with the ``-std=C++14`` or ``-std=gnu++14`` option; when compiling C++ code. C++14 binary literals; ^^^^^^^^^^^^^^^^^^^^^. Use ``__has_feature(cxx_binary_literals)`` or; ``__has_extension(cxx_binary_literals)`` to determine whether; binary literals (for instance, ``0b10010``) are recognized. Clang supports this; feature as an extension in all language modes. C++14 contextual conversions; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Use ``__has_feature(cxx_contextual_conversions)`` or; ``__has_extension(cxx_contextual_conversions)`` to determine if the C++14 rules; are used when performing an implicit conversion for an array bound in a; *new-expression*, the operand of a *delete-expression*, an integral constant; expression, or a condition in a ``switch`` statement. C++14 decltype(auto); ^^^^^^^^^^^^^^^^^^^^. Use ``__has_feature(cxx_decltype_auto)`` or; ``__has_extension(cxx_decltype_auto)`` to determine if support; for the ``decltype(auto)`` placeholder type is enabled. C++14 default initializers for aggregates; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Use ``__has_feature(cxx_aggregate_nsdmi)`` or; ``__has_extension(cxx_aggregate_nsdmi)`` to determine if support; for default initializers in aggregate members is enabled. C++14 digit separators; ^^^^^^^^^^^^^^^^^^^^^^. Use ``__cpp_digit_separators`` to determine if support for digit separators; using single quotes (for instance, ``10'000``) is enabled. At this time, there; is no corresponding ``__has_feature`` name. C++14 generalized lambda capture; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Use ``__has_feature(cxx_init_captures)`` or; `",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst:48763,perform,performing,48763,interpreter/llvm-project/clang/docs/LanguageExtensions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst,1,['perform'],['performing']
Performance,"re, e.g. ``libomptarget.rtl.cuda``. This section; describes the steps necessary to create a functioning device image that can be; loaded by the OpenMP runtime. More information on the OpenMP runtimes can be; found at the `OpenMP documentation page <https://openmp.llvm.org>`__. .. _Offloading Overview:. Offloading Overview; -------------------. The goal of offloading compilation is to create an executable device image that; can be run on the target device. OpenMP offloading creates executable images by; compiling the input file for both the host and the target device. The output; from the device phase then needs to be embedded into the host to create a fat; object. A special tool then needs to extract the device code from the fat; objects, run the device linking step, and embed the final image in a symbol the; host runtime library can use to register the library and access the symbols on; the device. Compilation Process; ^^^^^^^^^^^^^^^^^^^. The compiler performs the following high-level actions to generate OpenMP; offloading code:. * Compile the input file for the host to produce a bitcode file. Lower ``#pragma; omp target`` declarations to :ref:`offloading entries <Generating Offloading; Entries>` and create metadata to indicate which entries are on the device.; * Compile the input file for the target :ref:`device <Device Compilation>` using; the :ref:`offloading entry <Generating Offloading Entries>` metadata created; by the host.; * Link the OpenMP device runtime library and run the backend to create a device; object file.; * Run the backend on the host bitcode file and create a :ref:`fat object file; <Creating Fat Objects>` using the device object file.; * Pass the fat object file to the :ref:`linker wrapper tool <Device Linking>`; and extract the device objects. Run the device linking action on the extracted; objects.; * :ref:`Wrap <Device Binary Wrapping>` the :ref:`device images <Device linking>`; and :ref:`offload entries <Generating Offloading Entries>` in a",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/OffloadingDesign.rst:1748,perform,performs,1748,interpreter/llvm-project/clang/docs/OffloadingDesign.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/OffloadingDesign.rst,1,['perform'],['performs']
Performance,"re, the size has to; be known at compile time. Since the release 5.10, SMatrix supports symmetric matrices using; a storage class (ROOT::Math::MatRepSym) which contains only the N*(N+1)/2 independent element; of a NxN symmetric matrix.; It is not in the mandate of this package to provide a complete linear algebra functionality; for these classes. What is provided are basic \ref MatrixFunctions and \ref VectFunction,; such as the matrix-matrix, matrix-vector, vector-vector operations, plus some extra; functionality for square matrices, like inversion, which is based on the optimized Cramer; method for squared matrices of size up to 6x6, and determinant calculation.; For a more detailed descriptions and usage examples see:. * \ref SVectorDoc; * \ref SMatrixDoc; * \ref MatVecFunctions. The SMatrix package contains only header files. Normally one does not need to build any library.; In the %ROOT distribution a library, _libSmatrix_ is produced with the C++ dictionary information; for vectors, symmetric and squared matrices for double, float types up to dimension 7.; The current version of SMatrix can be downloaded from [here](../SMatrix.tar.gz). If you want; to install the header files or run the test _configure_ script and then _make install_ or; _make check_ to build the tests. No dictionary library is built in this case. ## References. 1. T. Veldhuizen, [_Expression Templates_](http://osl.iu.edu/~tveldhui/papers/Expression-Templates/exprtmpl.html),; C++ Report, 1995.; 2. T. Glebe, _SMatrix - A high performance library for Vector/Matrix calculation and Vertexing_,; HERA-B Software Note 01-134, December 2, 2003 ([pdf](http://seal.web.cern.ch/seal/documents/mathlib/smatrix_herab.pdf)); 3. L. Moneta, %ROOT Math proposal for Linear Algebra, [presentation](http://seal.cern.ch/documents/mathlib/aa_matrix_nov05.pdf); at the LCG Application Area meeting, November 23, 2005. * * *. @authors the %ROOT Math Library Team, T. Glebe (original SMatrix author) and J. Palacios (LHCb); ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/math/smatrix/doc/index.md:2875,perform,performance,2875,math/smatrix/doc/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/math/smatrix/doc/index.md,1,['perform'],['performance']
Performance,"re.; - Ensures that; volatile; operations to; different; addresses will not; be reordered by; hardware. store *none* *none* - local 1. ds_store; **Unordered Atomic**; ------------------------------------------------------------------------------------; load atomic unordered *any* *any* *Same as non-atomic*.; store atomic unordered *any* *any* *Same as non-atomic*.; atomicrmw unordered *any* *any* *Same as monotonic atomic*.; **Monotonic Atomic**; ------------------------------------------------------------------------------------; load atomic monotonic - singlethread - global 1. buffer/global/flat_load; - wavefront - generic; load atomic monotonic - workgroup - global 1. buffer/global/flat_load; - generic glc=1. - If not TgSplit execution; mode, omit glc=1. load atomic monotonic - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; - workgroup be used.*. 1. ds_load; load atomic monotonic - agent - global 1. buffer/global/flat_load; - generic glc=1; load atomic monotonic - system - global 1. buffer/global/flat_load; - generic glc=1; store atomic monotonic - singlethread - global 1. buffer/global/flat_store; - wavefront - generic; - workgroup; - agent; store atomic monotonic - system - global 1. buffer/global/flat_store; - generic; store atomic monotonic - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; - workgroup be used.*. 1. ds_store; atomicrmw monotonic - singlethread - global 1. buffer/global/flat_atomic; - wavefront - generic; - workgroup; - agent; atomicrmw monotonic - system - global 1. buffer/global/flat_atomic; - generic; atomicrmw monotonic - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; - workgroup be used.*. 1. ds_atomic; **Acquire Atomic**; ------------------------------------------------------------------------------------; load atomic acquire - singlethread - global 1. buffer/global/ds/flat_load; - wavefront - local; - generic; lo",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:244297,load,load,244297,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"re/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; have; completed before; performing the; atomicrmw that is; being released. 2. flat_atomic; 3. s_waitcnt lgkmcnt(0) &; vmcnt(0) & vscnt(0). - If CU wavefront execution; mode, omit vmcnt(0) and; vscnt(0).; - If OpenCL, omit lgkmcnt(0).; - Must happen before; the following; buffer_gl0_inv.; - Ensures any; following global; data read is no; older than the load; atomic value being; acquired. 3. buffer_gl0_inv. - If CU wavefront execution; mode, omit.; - Ensures that; following; loads will not see; stale data. atomicrmw acq_rel - agent - global 1. s_waitcnt lgkmcnt(0) &; - system vmcnt(0) & vscnt(0). - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0), s_waitcnt; vscnt(0) and s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/load atomic/; atomicrmw-with-return-value.; - s_waitcnt vscnt(0); must happen after; any preceding; global/generic; store/store atomic/; atomicrmw-no-return-value.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global have; completed before; performing the; atomicrmw that is; being released. 2. buffer/global_atomic; 3. s_waitcnt vm/vscnt(0). - Use vmcnt(0) if atomic with; return and vscnt(0) if; atomic with no-return.; - Must happen before; following; buffer_gl*_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; caches. 4. buffer_gl0_inv;; buffer_gl1_inv. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acq_rel - agent - generic 1. s_waitcnt lgkmcnt(0) &; - system vmcnt(0) &",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:367223,load,load,367223,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"re/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; buffer_inv.; - Ensures that the; preceding; global/local/generic; load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; acquire-fence-paired-atomic); has completed; before invalidating; the cache. This; satisfies the; requirements of; acquire.; - Ensures that all; previous memory; operations have; completed before a; following; global/local/generic; store; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; release-fence-paired-atomic).; This satisfies the; requirements of; release. 2. buffer_inv sc0=1 sc1=1. - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; MTYPE NC global data.; MTYPE RW and CC memory will; never be stale due to the; memory probes. **Sequential Consistent Atomic**; ------------------------------------------------------------------------------------; load atomic seq_cst - singlethread - global *Same as corresponding; - wavefront - local load atomic acquire,; - generic except must generate; all instructions even; for OpenCL.*; load atomic seq_cst - workgroup - global 1. s_waitcnt lgkm/vmcnt(0); - generic; - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - s_waitcnt lgkmcnt(0) must; happen after; preceding; local/generic load; atomic/store; atomic/atomicrmw; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; lgkmcnt(0) and so do; not need to be; considered.); - s_waitcnt vmcnt(0); must happen after; preceding; global/generic load; atomic/store; atomic/atomicrmw; with memory; ordering of seq_c",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:330485,load,loads,330485,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['loads']
Performance,"re; Collections. Improve the output of the call to Print for ROOT Collections. In TCollection replace Print() methods:; virtual void Print(Option_t *wildcard="""") const;; virtual void Print(Option_t *wildcard, Option_t *option) const;; with; virtual void Print(Option_t *option="""") const;; virtual void Print(Option_t *option, Int_t recurse) const;; virtual void Print(Option_t *option, const char* wildcard,; Int_t recurse=1) const;; virtual void Print(Option_t *option, TPRegexp& regexp,; Int_t recurse=1) const;. Introduce three new protected methods used to print out collection details.; These are called from the Print() methods and make overriding of what; is printed for a collection / entries easier.; virtual void PrintCollectionHeader(Option_t* option) const;; virtual const char* GetCollectionEntryName(TObject* entry) const;; virtual void PrintCollectionEntry(TObject* entry, Option_t* option,; Int_t recurse) const;. Improve performance of reading TExMap object (50 times faster) using an updated,; slightly larger (16%) on file format for the TExMap object. ACLiC; Fix problem finding rootcint in the --prefix configuration.; Change the naming convention for the temporary files created by ACLiC, instead of randomly named ACLiC now uses:scriptname_scriptextension_ACLiC_function.extension; When ACLiC need to revert to the temp directory for storing the library, it now create the directory; structure under a directory named after the userid. Allow white space in the name of the directory where a script to be compiled by ACLiC resides.; Add optional 5th argument to CompileMacro to allow set the file bit; mode of the directory created.; Avoid looking for Microsoft's link.exe when we already have it (and hence avoid complaining about cygpath when cygwin is not even installed. Meta. Insure that the TClass list of methods is refreshed when new functions are added to the dictionary. TStyle. In TStyle::Reset, the Frame; Fill Color default value did not match the TFrame; Fill Colo",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/core/doc/v522/index.html:942,perform,performance,942,core/doc/v522/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/core/doc/v522/index.html,2,['perform'],['performance']
Performance,"re; can be multiple inner remainder loops. See; :ref:`Transformation Metadata <transformation-metadata>` for details. '``llvm.loop.unroll_and_jam.followup_all``' Metadata; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Attributes specified in the metadata is added to all; ``llvm.loop.unroll_and_jam.*`` loops. See; :ref:`Transformation Metadata <transformation-metadata>` for details. '``llvm.loop.licm_versioning.disable``' Metadata; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. This metadata indicates that the loop should not be versioned for the purpose; of enabling loop-invariant code motion (LICM). The metadata has a single operand; which is the string ``llvm.loop.licm_versioning.disable``. For example:. .. code-block:: llvm. !0 = !{!""llvm.loop.licm_versioning.disable""}. '``llvm.loop.distribute.enable``' Metadata; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Loop distribution allows splitting a loop into multiple loops. Currently,; this is only performed if the entire loop cannot be vectorized due to unsafe; memory dependencies. The transformation will attempt to isolate the unsafe; dependencies into their own loop. This metadata can be used to selectively enable or disable distribution of the; loop. The first operand is the string ``llvm.loop.distribute.enable`` and the; second operand is a bit. If the bit operand value is 1 distribution is; enabled. A value of 0 disables distribution:. .. code-block:: llvm. !0 = !{!""llvm.loop.distribute.enable"", i1 0}; !1 = !{!""llvm.loop.distribute.enable"", i1 1}. This metadata should be used in conjunction with ``llvm.loop`` loop; identification metadata. '``llvm.loop.distribute.followup_coincident``' Metadata; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. This metadata defines which attributes extracted loops with no cyclic; dependencies will have (i.e. can be vectorized). See; :ref:`Transformation Metadata <transformation-metadata>` for details. '``llvm.loop.distribute.followup_sequential``' Metadata; ^^^^",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:307666,perform,performed,307666,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performed']
Performance,"re; the following; buffer_inv.; - Ensures any; following global; data read is no; older than the; atomicrmw value; being acquired. 4. buffer_inv sc0=1. - If not TgSplit execution; mode, omit.; - Ensures that; following; loads will not see; stale data. atomicrmw acq_rel - workgroup - local *If TgSplit execution mode,; local address space cannot; be used.*. 1. ds_atomic; 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than the local load; atomic value being; acquired. atomicrmw acq_rel - workgroup - generic 1. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL, omit; lgkmcnt(0).; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load/store/; load atomic/store atomic/; atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; have; completed before; performing the; atomicrmw that is; being released. 2. flat_atomic; 3. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If not TgSplit execution; mode, omit vmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Must happen before; the following; buffer_inv and; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local load; atomic value being; acquired. 3. buffer_inv sc0=1. - If not TgSplit execution; mode, omit.; - Ensures that; following; loads will not see; stale data. atomicrmw acq_rel - agent - global 1. buffer_wbl2 sc1=1. - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at agent scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:318822,load,load,318822,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"reachable code; 3 . Compute live ranges for CSE; 4 . [t] Jump threading (jumps to jumps with identical or inverse conditions); 5 . [t] CSE; 6 . *** Conversion to SSA ; 7 . [t] SSA Based DCE; 8 . *** Conversion to LLVM; 9 . UnSSA; 10. GCSE; 11. LICM; 12. Strength Reduction; 13. Loop unrolling; 14. [t] CSE; 15. [t] DCE; 16. Instruction combination, register movement, scheduling... etc. I've marked optimizations with a [t] to indicate things that I believe to; be relatively trivial to implement in LLVM itself. The time consuming; things to reimplement would be SSA based PRE, Strength reduction & loop; unrolling... these would be the major things we would miss out on if we; did LLVM creation from tree code [inlining and other high level; optimizations are done on the tree representation]. Given the lack of ""strong"" optimizations that would take a long time to; reimplement, I am leaning a bit more towards creating LLVM from the tree; code. Especially given that SGI has GPL'd their compiler, including many; SSA based optimizations that could be adapted (besides the fact that their; code looks MUCH nicer than GCC :). Even if we choose to do LLVM code emission from RTL, we will almost; certainly want to move LLVM emission from step 8 down until at least CSE; has been rerun... which causes me to wonder if the SSA generation code; will still work (due to global variable dependencies and stuff). I assume; that it can be made to work, but might be a little more involved than we; would like. I'm continuing to look at the Tree -> RTL code. It is pretty gross; because they do some of the translation a statement at a time, and some; of it a function at a time... I'm not quite clear why and how the; distinction is drawn, but it does not appear that there is a wonderful; place to attach extra info. Anyways, I'm proceeding with the RTL -> LLVM conversion phase for now. We; can talk about this more on Monday. Wouldn't it be nice if there were a obvious decision to be made? :). -Chris. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations.txt:1455,optimiz,optimizations,1455,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations.txt,1,['optimiz'],['optimizations']
Performance,"read.ptx.sreg.ntid.{x,y,z}`` blockDim.{x,y,z}; ``i32 @llvm.nvvm.read.ptx.sreg.nctaid.{x,y,z}`` gridDim.{x,y,z}; ``void @llvm.nvvm.barrier0()`` __syncthreads(); ================================================ ====================. Address Spaces; ^^^^^^^^^^^^^^. You may have noticed that all of the pointer types in the LLVM IR example had; an explicit address space specifier. What is address space 1? NVIDIA GPU; devices (generally) have four types of memory:. - Global: Large, off-chip memory; - Shared: Small, on-chip memory shared among all threads in a CTA; - Local: Per-thread, private memory; - Constant: Read-only memory shared across all threads. These different types of memory are represented in LLVM IR as address spaces.; There is also a fifth address space used by the NVPTX code generator that; corresponds to the ""generic"" address space. This address space can represent; addresses in any other address space (with a few exceptions). This allows; users to write IR functions that can load/store memory using the same; instructions. Intrinsics are provided to convert pointers between the generic; and non-generic address spaces. See :ref:`address_spaces` and :ref:`nvptx_intrinsics` for more information. Kernel Metadata; ^^^^^^^^^^^^^^^. In PTX, a function can be either a `kernel` function (callable from the host; program), or a `device` function (callable only from GPU code). You can think; of `kernel` functions as entry-points in the GPU program. To mark an LLVM IR; function as a `kernel` function, we make use of special LLVM metadata. The; NVPTX back-end will look for a named metadata node called; ``nvvm.annotations``. This named metadata must contain a list of metadata that; describe the IR. For our purposes, we need to declare a metadata node that; assigns the ""kernel"" attribute to the LLVM IR function that should be emitted; as a PTX `kernel` function. These metadata nodes take the form:. .. code-block:: text. !{<function ref>, metadata !""kernel"", i32 1}. For t",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:17440,load,load,17440,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,1,['load'],['load']
Performance,"reads). Incremental; -----------; .. _incremental:. ThinLTO supports fast incremental builds through the use of a cache,; which currently must be enabled through a linker option. - gold (as of LLVM 4.0):; ``-Wl,-plugin-opt,cache-dir=/path/to/cache``; - ld64 (supported since clang 3.9 and Xcode 8) and Mach-O ld64.lld (as of LLVM; 15.0):; ``-Wl,-cache_path_lto,/path/to/cache``; - ELF ld.lld (as of LLVM 5.0):; ``-Wl,--thinlto-cache-dir=/path/to/cache``; - COFF lld-link (as of LLVM 6.0):; ``/lldltocache:/path/to/cache``. Cache Pruning; -------------. To help keep the size of the cache under control, ThinLTO supports cache; pruning. Cache pruning is supported with gold, ld64, and lld, but currently only; gold and lld allow you to control the policy with a policy string. The cache; policy must be specified with a linker option. - gold (as of LLVM 6.0):; ``-Wl,-plugin-opt,cache-policy=POLICY``; - ELF ld.lld (as of LLVM 5.0), Mach-O ld64.lld (as of LLVM 15.0):; ``-Wl,--thinlto-cache-policy=POLICY``; - COFF lld-link (as of LLVM 6.0):; ``/lldltocachepolicy:POLICY``. A policy string is a series of key-value pairs separated by ``:`` characters.; Possible key-value pairs are:. - ``cache_size=X%``: The maximum size for the cache directory is ``X`` percent; of the available space on the disk. Set to 100 to indicate no limit,; 50 to indicate that the cache size will not be left over half the available; disk space. A value over 100 is invalid. A value of 0 disables the percentage; size-based pruning. The default is 75%. - ``cache_size_bytes=X``, ``cache_size_bytes=Xk``, ``cache_size_bytes=Xm``,; ``cache_size_bytes=Xg``:; Sets the maximum size for the cache directory to ``X`` bytes (or KB, MB,; GB respectively). A value over the amount of available space on the disk; will be reduced to the amount of available space. A value of 0 disables; the byte size-based pruning. The default is no byte size-based pruning. Note that ThinLTO will apply both size-based pruning policies simultaneousl",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ThinLTO.rst:5504,cache,cache-policy,5504,interpreter/llvm-project/clang/docs/ThinLTO.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ThinLTO.rst,1,['cache'],['cache-policy']
Performance,"realize that it is finite (if it were infinite, it would be undefined). Not; having this blocks Loop Idiom from matching strlen and friends. . void foo(char *C) {; int x = 0;; while (*C); ++x,++C;; }. //===---------------------------------------------------------------------===//. [LOOP RECOGNITION]. These idioms should be recognized as popcount (see PR1488):. unsigned countbits_slow(unsigned v) {; unsigned c;; for (c = 0; v; v >>= 1); c += v & 1;; return c;; }. unsigned int popcount(unsigned int input) {; unsigned int count = 0;; for (unsigned int i = 0; i < 4 * 8; i++); count += (input >> i) & i;; return count;; }. This should be recognized as CLZ: https://github.com/llvm/llvm-project/issues/64167. unsigned clz_a(unsigned a) {; int i;; for (i=0;i<32;i++); if (a & (1<<(31-i))); return i;; return 32;; }. This sort of thing should be added to the loop idiom pass. //===---------------------------------------------------------------------===//. These should turn into single 16-bit (unaligned?) loads on little/big endian; processors. unsigned short read_16_le(const unsigned char *adr) {; return adr[0] | (adr[1] << 8);; }; unsigned short read_16_be(const unsigned char *adr) {; return (adr[0] << 8) | adr[1];; }. //===---------------------------------------------------------------------===//. -instcombine should handle this transform:; icmp pred (sdiv X / C1 ), C2; when X, C1, and C2 are unsigned. Similarly for udiv and signed operands. . Currently InstCombine avoids this transform but will do it when the signs of; the operands and the sign of the divide match. See the FIXME in ; InstructionCombining.cpp in the visitSetCondInst method after the switch case ; for Instruction::UDiv (around line 4447) for more details. The SingleSource/Benchmarks/Shootout-C++/hash and hash2 tests have examples of; this construct. . //===---------------------------------------------------------------------===//. [LOOP OPTIMIZATION]. SingleSource/Benchmarks/Misc/dt.c shows several interesting o",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:7332,load,loads,7332,interpreter/llvm-project/llvm/lib/Target/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt,2,['load'],['loads']
Performance,"reasonable starting point for someone interested; in writing an ``gcroot`` compatible GC plugin. In particular, these are the; only in tree examples of how to produce a custom binary stack map format using; a ``gcroot`` strategy. As there names imply, the binary format produced is intended to model that; used by the Erlang and OCaml compilers respectively. .. _statepoint_example_gc:. The Statepoint Example GC; -------------------------. .. code-block:: c++. F.setGC(""statepoint-example"");. This GC provides an example of how one might use the infrastructure provided; by ``gc.statepoint``. This example GC is compatible with the; :ref:`PlaceSafepoints` and :ref:`RewriteStatepointsForGC` utility passes; which simplify ``gc.statepoint`` sequence insertion. If you need to build a; custom GC strategy around the ``gc.statepoints`` mechanisms, it is recommended; that you use this one as a starting point. This GC strategy does not support read or write barriers. As a result, these; intrinsics are lowered to normal loads and stores. The stack map format generated by this GC strategy can be found in the; :ref:`stackmap-section` using a format documented :ref:`here; <statepoint-stackmap-format>`. This format is intended to be the standard; format supported by LLVM going forward. The CoreCLR GC; -------------------------. .. code-block:: c++. F.setGC(""coreclr"");. This GC leverages the ``gc.statepoint`` mechanism to support the; `CoreCLR <https://github.com/dotnet/coreclr>`__ runtime. Support for this GC strategy is a work in progress. This strategy will; differ from; :ref:`statepoint-example GC<statepoint_example_gc>` strategy in; certain aspects like:. * Base-pointers of interior pointers are not explicitly; tracked and reported. * A different format is used for encoding stack maps. * Safe-point polls are only needed before loop-back edges; and before tail-calls (not needed at function-entry). Custom GC Strategies; ====================. If none of the built in GC strategy descript",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GarbageCollection.rst:20617,load,loads,20617,interpreter/llvm-project/llvm/docs/GarbageCollection.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GarbageCollection.rst,1,['load'],['loads']
Performance,"reateReassociatePass());; FPM->add(createGVNPass());; FPM->add(createCFGSimplificationPass());; FPM->doInitialization();. // Run the optimizations over all functions in the module being added to; // the JIT.; for (auto &F : *M); FPM->run(F);. return M;; }. At the bottom of our JIT we add a private method to do the actual optimization:; *optimizeModule*. This function takes the module to be transformed as input (as; a ThreadSafeModule) along with a reference to a reference to a new class:; ``MaterializationResponsibility``. The MaterializationResponsibility argument; can be used to query JIT state for the module being transformed, such as the set; of definitions in the module that JIT'd code is actively trying to call/access.; For now we will ignore this argument and use a standard optimization; pipeline. To do this we set up a FunctionPassManager, add some passes to it, run; it over every function in the module, and then return the mutated module. The; specific optimizations are the same ones used in `Chapter 4 <LangImpl04.html>`_; of the ""Implementing a language with LLVM"" tutorial series. Readers may visit; that chapter for a more in-depth discussion of these, and of IR optimization in; general. And that's it in terms of changes to KaleidoscopeJIT: When a module is added via; addModule the OptimizeLayer will call our optimizeModule function before passing; the transformed module on to the CompileLayer below. Of course, we could have; called optimizeModule directly in our addModule function and not gone to the; bother of using the IRTransformLayer, but doing so gives us another opportunity; to see how layers compose. It also provides a neat entry point to the *layer*; concept itself, because IRTransformLayer is one of the simplest layers that; can be implemented. .. code-block:: c++. // From IRTransformLayer.h:; class IRTransformLayer : public IRLayer {; public:; using TransformFunction = std::function<Expected<ThreadSafeModule>(; ThreadSafeModule, const Materializ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:5635,optimiz,optimizations,5635,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,1,['optimiz'],['optimizations']
Performance,"reated. * For example, the following are equivalent. .. code-block:: shell. $ opt -passes='no-op-function,no-op-loop' /tmp/a.ll -S; $ opt -passes='no-op-function,loop(no-op-loop)' /tmp/a.ll -S. For a list of available passes and analyses, including the IR unit (module,; CGSCC, function, loop) they operate on, run. .. code-block:: shell. $ opt --print-passes. or take a look at ``PassRegistry.def``. To make sure an analysis named ``foo`` is available before a pass, add; ``require<foo>`` to the pass pipeline. This adds a pass that simply requests; that the analysis is run. This pass is also subject to proper nesting. For; example, to make sure some function analysis is already computed for all; functions before a module pass:. .. code-block:: shell. $ opt -passes='function(require<my-function-analysis>),my-module-pass' /tmp/a.ll -S. Status of the New and Legacy Pass Managers; ==========================================. LLVM currently contains two pass managers, the legacy PM and the new PM. The; optimization pipeline (aka the middle-end) uses the new PM, whereas the backend; target-dependent code generation uses the legacy PM. The legacy PM somewhat works with the optimization pipeline, but this is; deprecated and there are ongoing efforts to remove its usage. Some IR passes are considered part of the backend codegen pipeline even if; they are LLVM IR passes (whereas all MIR passes are codegen passes). This; includes anything added via ``TargetPassConfig`` hooks, e.g.; ``TargetPassConfig::addCodeGenPrepare()``. The ``TargetMachine::adjustPassManager()`` function that was used to extend a; legacy PM with passes on a per target basis has been removed. It was mainly; used from opt, but since support for using the default pipelines has been; removed in opt the function isn't needed any longer. In the new PM such; adjustments are done by using ``TargetMachine::registerPassBuilderCallbacks()``. Currently there are efforts to make the codegen pipeline work with the new; PM.; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:20749,optimiz,optimization,20749,interpreter/llvm-project/llvm/docs/NewPassManager.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst,2,['optimiz'],['optimization']
Performance,"reates a frame at the same address, respectively. There is no need for a; ``s_dcache_inv`` as all scalar writes are write-before-read in the same thread. For kernarg backing memory:. * CP invalidates the L1 cache at the start of each kernel dispatch.; * On dGPU over XGMI or PCIe the kernarg backing memory is allocated in host; memory accessed as MTYPE UC (uncached) to avoid needing to invalidate the L2; cache. This also causes it to be treated as non-volatile and so is not; invalidated by ``*_vol``.; * On APU the kernarg backing memory is accessed as MTYPE CC (cache coherent) and; so the L2 cache will be coherent with the CPU and other agents. Scratch backing memory (which is used for the private address space) is accessed; with MTYPE NC_NV (non-coherent non-volatile). Since the private address space is; only accessed by a single thread, and is always write-before-read, there is; never a need to invalidate these entries from the L1 cache. Hence all cache; invalidates are done as ``*_vol`` to only invalidate the volatile cache lines. The code sequences used to implement the memory model for GFX90A are defined; in table :ref:`amdgpu-amdhsa-memory-model-code-sequences-gfx90a-table`. .. table:: AMDHSA Memory Model Code Sequences GFX90A; :name: amdgpu-amdhsa-memory-model-code-sequences-gfx90a-table. ============ ============ ============== ========== ================================; LLVM Instr LLVM Memory LLVM Memory AMDGPU AMDGPU Machine Code; Ordering Sync Scope Address GFX90A; Space; ============ ============ ============== ========== ================================; **Non-Atomic**; ------------------------------------------------------------------------------------; load *none* *none* - global - !volatile & !nontemporal; - generic; - private 1. buffer/global/flat_load; - constant; - !volatile & nontemporal. 1. buffer/global/flat_load; glc=1 slc=1. - volatile. 1. buffer/global/flat_load; glc=1; 2. s_waitcnt vmcnt(0). - Must happen before; any following volatile; glo",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:241773,cache,cache,241773,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['cache'],['cache']
Performance,"reates a frame at the same address, respectively. There is no need for a; ``s_dcache_inv`` as all scalar writes are write-before-read in the same thread. For kernarg backing memory:. * CP invalidates the L1 cache at the start of each kernel dispatch.; * On dGPU over XGMI or PCIe the kernarg backing memory is allocated in host; memory accessed as MTYPE UC (uncached) to avoid needing to invalidate the L2; cache. This also causes it to be treated as non-volatile and so is not; invalidated by ``*_vol``.; * On APU the kernarg backing memory is accessed as MTYPE CC (cache coherent) and; so the L2 cache will be coherent with the CPU and other agents. Scratch backing memory (which is used for the private address space) is accessed; with MTYPE NC_NV (non-coherent non-volatile). Since the private address space is; only accessed by a single thread, and is always write-before-read, there is; never a need to invalidate these entries from the L1 cache. Hence all cache; invalidates are done as ``*_vol`` to only invalidate the volatile cache lines. The code sequences used to implement the memory model for GFX940, GFX941, GFX942; are defined in table :ref:`amdgpu-amdhsa-memory-model-code-sequences-gfx940-gfx941-gfx942-table`. .. table:: AMDHSA Memory Model Code Sequences GFX940, GFX941, GFX942; :name: amdgpu-amdhsa-memory-model-code-sequences-gfx940-gfx941-gfx942-table. ============ ============ ============== ========== ================================; LLVM Instr LLVM Memory LLVM Memory AMDGPU AMDGPU Machine Code; Ordering Sync Scope Address GFX940, GFX941, GFX942; Space; ============ ============ ============== ========== ================================; **Non-Atomic**; ------------------------------------------------------------------------------------; load *none* *none* - global - !volatile & !nontemporal; - generic; - private 1. buffer/global/flat_load; - constant; - !volatile & nontemporal. 1. buffer/global/flat_load; nt=1. - volatile. 1. buffer/global/flat_load; sc0=1 sc1=",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:291791,cache,cache,291791,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['cache'],['cache']
Performance,"recommended; method is using *LLVM_BUILD_LLVM_DYLIB*, and you can use *LLVM_DYLIB_COMPONENTS*; to configure which LLVM components are part of libLLVM.; Note: *LLVM_BUILD_LLVM_DYLIB* is not available on Windows. Options for Optimizing LLVM; ===========================. There are four main build optimizations that our CMake build system supports.; When performing a bootstrap build it is not beneficial to do anything other than; setting *CMAKE_BUILD_TYPE* to ``Release`` for the stage-1 compiler. This is; because the more intensive optimizations are expensive to perform and the; stage-1 compiler is thrown away. All of the further options described should be; set on the stage-2 compiler either using a CMake cache file, or by prefixing the; option with *BOOTSTRAP_*. The first and simplest to use is the compiler optimization level by setting the; *CMAKE_BUILD_TYPE* option. The main values of interest are ``Release`` or; ``RelWithDebInfo``. By default the ``Release`` option uses the ``-O3``; optimization level, and ``RelWithDebInfo`` uses ``-O2``. If you want to generate; debug information and use ``-O3`` you can override the; *CMAKE_<LANG>_FLAGS_RELWITHDEBINFO* option for C and CXX.; DistributionExample.cmake does this. Another easy to use option is Link-Time-Optimization. You can set the; *LLVM_ENABLE_LTO* option on your stage-2 build to ``Thin`` or ``Full`` to enable; building LLVM with LTO. These options will significantly increase link time of; the binaries in the distribution, but it will create much faster binaries. This; option should not be used if your distribution includes static archives, as the; objects inside the archive will be LLVM bitcode, which is not portable. The :doc:`AdvancedBuilds` documentation describes the built-in tooling for; generating LLVM profiling information to drive Profile-Guided-Optimization. The; in-tree profiling tests are very limited, and generating the profile takes a; significant amount of time, but it can result in a significant imp",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BuildingADistribution.rst:8264,optimiz,optimization,8264,interpreter/llvm-project/llvm/docs/BuildingADistribution.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BuildingADistribution.rst,1,['optimiz'],['optimization']
Performance,"recompiling TMVA. ; - Support inference (network evaluation) in batch mode in addition to single event. Batch mode evaluation is now the default when used within the `TMVA::Factory` class (i.e. when calling; `Factory::TestAllMethod()` or `Factory::EvaluateAllMethods()`; - Support splitting the overall training data in Train and Validation data. The train data is used for finding the optimal network weight and the validation data is used to monitor the validation; error. The weights which are giving a minimal validation error will be stored. For the splitting a new option, *ValidationSize* has been added to the global options for `MethodDL`.; The same option is also available in the `PyKeras` method of `PyMVA`; - The fast tanh implementation from VDT is now used as activation function when training the network on CPU.; - Using `Cblas` from the GSL library is supported for CPU training when no other Blas libraries are found. However, it is strongly recommended, to use an optimized Blas implementation such as `libopenblas`, that is; available in cvmfs.; - Add several performance optimizations for both CPU and GPU versions of `MethodDL`. . ### Other New TMVA Features. - Add a new option to the `DataLoader` to switch off computation of correlation matrix. The new option is called *CalcCorrelations* and it should be used when a large number of input variables are; provided, otherwise TMVA will spend a long time in setting up the data set before training. ; ; - Build configuration:; - Add new cmake flags, `tmva-cpu` and `tmva-gpu`, which can be used to swicth on/off the CPU and GPU (based on CUDA) implementations of the TMVA Deep Learning module. `tmva-cpu` is enabled by; default if a Blas or CBlas library is found in the system. `tmva-gpu` is enabled when the cmake flag `cuda` is enabled and a compatible Cuda library is found. ; enabled if the corre; - Add possibility to independently configure building of optional pymva part of tmva with flag `-Dpymva=ON|OFF`. - New Cross",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v616/index.md:14947,optimiz,optimized,14947,README/ReleaseNotes/v616/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v616/index.md,1,['optimiz'],['optimized']
Performance,"record. In addition, objects that; are referenced by such a pointer have an additional field in the base TObject.; See \ref tobject. A description of how these pointers work is given under; the \ref ptpo ""Pointers to persistent objects"" heading below. ### ""application"" layer record types. These are either user defined record types, or record types supplied; by ROOT that are not needed by ROOTIO. The format of such an object that; uses the default streamer is shown in \ref dobject. ## Data compression. The user can set the data compression level for new or modified data records; when creating or opening a file. When an existing file is opened for update,; the compression level selected need not match that used previously. The; compression level of existing records is not modified unless the record itself; is modified. There are ten compression levels, 0-9, ranging from 0 (no compression) to 9; (maximum compression), with level 1 being the default. The level chosen is; a tradeoff between disk space and compression performance. The decompression; speed is independent of level. Currently, in release 3.2.6, level 2 is not used.; If level 2 is selected, level 1 is used with no notification to the user. The chosen compression level is not applied to the entire file. The following; portions of the file are not compressed, regardless of the compression level; selected:. 1. the file header; 2. the KeysList data record; 3. the FreeSegments data record; 4. any data record (outside of a TTree) where the uncompressed size of; the data portion is 256 bytes or less.; 5. the key portion of any data record. Furthermore, the data portion of the StreamerInfo data record is always; compressed at level 1 (if over 256 bytes uncompressed), regardless of the; compression level selected (even if no compression is selected). The compression algorithm used is an in memory ZIP compression written for the; DELPHI collaboration at CERN. Its author is E. Chernyaev (IHEP/Protvino).; The source code",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/io/doc/TFile/README.md:5055,perform,performance,5055,io/doc/TFile/README.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/io/doc/TFile/README.md,1,['perform'],['performance']
Performance,"rectives can be embedded using strings so you still have access to; the convenience available in llvm-lit. Debugging; ---------. One debugging technique that's proven particularly valuable is to use the; BlockExtractor to extract basic blocks into new functions. This can be used; to track down correctness bugs and can also be used to track down performance; regressions. It can also be coupled with function attributes to disable; GlobalISel for one or more of the extracted functions. .. image:: block-extract.png. The command to do the extraction is:. .. code-block:: shell. ./bin/llvm-extract -o - -S -b ‘foo:bb1;bb4’ <input> > extracted.ll. This particular example extracts two basic blocks from a function named ``foo``.; The new LLVM-IR can then be modified to add the ``failedISel`` attribute to the; extracted function containing bb4 to make that function use SelectionDAG. This can prevent some optimizations as GlobalISel is generally able to work on a; single function at a time. This technique can be repeated for different; combinations of basic blocks until you have identified the critical blocks; involved in a bug. Once the critical blocks have been identified, you can further increase the; resolution to the critical instructions by splitting the blocks like from:. .. code-block:: none. bb1:; ... instructions group 1 ...; ... instructions group 2 ... into:. .. code-block:: none. bb1:; ... instructions group 1 ...; br %bb2. bb2:; ... instructions group 2 ... and then repeating the process for the new blocks. It's also possible to use this technique in a mode where the main function; is compiled with GlobalISel and the extracted basic blocks are compiled with; SelectionDAG (or the other way around) to leverage the existing quality of; another code generator to track down bugs. This technique can also be used to; improve the similarity between fast and slow code when tracking down performance; regressions and help you zero in on a particular cause of the regression.; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GlobalISel/Pipeline.rst:6890,perform,performance,6890,interpreter/llvm-project/llvm/docs/GlobalISel/Pipeline.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GlobalISel/Pipeline.rst,1,['perform'],['performance']
Performance,"rectly use svn. You will get the message:. $ svn up; svn: Failed to add directory 'xrootd/src/xrootd': object of the same name already exists; $ rm -rf xrootd/src/xrootd; $ svn up; svn: Failed to add directory 'asimage/src/libAfterImage': object of the same name already exists; $ rm -rf asimage/src/libAfterImage; $ svn up. Port to gcc 4.3.1. This version of gcc is much stricker with respect to; implicit header files so in many source files <stdlib.h> and <string.h>; had to be added. TPRegexp. Modularized Match() and Substitute() functions so that the low-level work; is done by MatchInternal() and SubstituteInternal(). Added function TString GetModifiers() that translates from bit-flag; options back to perl-style option characters. For all functions that do actual matching replaced the default value; of 'Int_t nMatchMax' argument from 30 to 10. PCRE internals require; the index buffer to be 3-times the number of allowed matches. This; multiplication is now performed in individual functions and nMatchMax; is really the number of allowed matches. Two function calls in; TString passing this number explicitly have been updated. TPMERegexp. New sub-class of TPRegexp with API closer to PERL.; Supports main Perl operations using regular expressions (Match,; Substitute and Split). To retrieve the results one can simply use; operator[] returning a TString. New tutorial regexp_pme.C. Meta. Add a new TClass interface to be able to trigger the ""auto addition of object; to a TDirectory object"". If a class has a member function:; DirectoryAutoAdd(TDirectory*); it will now be accessible (when the dictionary is generated via rootcint; for now) via TClass::GetDirectoryAutoAdd which return a wrapper with the; signature:; void (*)(void *this_obj,TDirectory *where_to_add). Extend #pragma create TClass; to namespaces.; Enable autoloading of the cintdlls.; rlibmap now handles #pragma create TClass; statements. Cont. Support for std algorithms and iterators for ROOT collection classes by A",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/core/doc/v520/index.html:2594,perform,performed,2594,core/doc/v520/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/core/doc/v520/index.html,2,['perform'],['performed']
Performance,"rectly. This character is the dot at the beginning of; a line:. ``` {.cpp}; root [1] .<command>; ```. This is a selection of the most common commands. - **quit root**, simply type `.q` or `.quit` or `.exit`. - obtain the full **list of commands**, use `.?` or `.help`. - **access the shell** of the operating system, type `.!<OS_command>`;; try, e.g. `.!ls` or `.!pwd`. - **execute a macro**, enter `.x <file_name>`; in the above example,; you might have used `.x slits.C` at the ROOT prompt. - **load a macro**, type `.L <file_name>`; in the above example, you; might instead have used the command `.L slits.C` followed by the; function call `slits();`. Note that after loading a macro all; functions and procedures defined therein are available at the ROOT; prompt. - **compile a macro**, type `.L <file_name>+`; ROOT is able to manage; for you the `C++` compiler behind the scenes and to produce machine; code starting from your macro. One could decide to compile a macro; in order to obtain better performance or to get nearer to the; production environment. ## Plotting Measurements ##. To display measurements in ROOT, including errors, there exists a; powerful class `TGraphErrors` with different types of constructors. In; the example here, we use data from the file `ExampleData.txt` in text; format:. ``` {.cpp}; root [0] TGraphErrors gr(""ExampleData.txt"");; root [1] gr.Draw(""AP"");; ```. You should see the output shown in Figure [2.2](#f22). [f22]: figures/TGraphErrors_Example.png ""f22""; <a name=""f22""></a>. ![Visualisation of data points with errors using the class TGraphErrors. \label{f22}][f22]. Make sure the file `ExampleData.txt` is available in the directory from; which you started ROOT. Inspect this file now with your favourite; editor, or use the command `less ExampleData.txt` to inspect the file,; you will see that the format is very simple and easy to understand.; Lines beginning with `#` are ignored. It is very convenient to add some; comments about the type of data. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/primer/ROOT_as_calculator.md:8731,perform,performance,8731,documentation/primer/ROOT_as_calculator.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/primer/ROOT_as_calculator.md,1,['perform'],['performance']
Performance,"rectories does not need the ``lit.local.cfg`` file. Read the :doc:`Lit; documentation <CommandGuide/lit>` for more information. Each test file must contain lines starting with ""RUN:"" that tell :program:`lit`; how to run it. If there are no RUN lines, :program:`lit` will issue an error; while running a test. RUN lines are specified in the comments of the test program using the; keyword ``RUN`` followed by a colon, and lastly the command (pipeline); to execute. Together, these lines form the ""script"" that :program:`lit`; executes to run the test case. The syntax of the RUN lines is similar to a; shell's syntax for pipelines including I/O redirection and variable; substitution. However, even though these lines may *look* like a shell; script, they are not. RUN lines are interpreted by :program:`lit`.; Consequently, the syntax differs from shell in a few ways. You can specify; as many RUN lines as needed. :program:`lit` performs substitution on each RUN line to replace LLVM tool names; with the full paths to the executable built for each tool (in; ``$(LLVM_OBJ_ROOT)/bin``). This ensures that :program:`lit` does; not invoke any stray LLVM tools in the user's path during testing. Each RUN line is executed on its own, distinct from other lines unless; its last character is ``\``. This continuation character causes the RUN; line to be concatenated with the next one. In this way you can build up; long pipelines of commands without making huge line lengths. The lines; ending in ``\`` are concatenated until a RUN line that doesn't end in; ``\`` is found. This concatenated set of RUN lines then constitutes one; execution. :program:`lit` will substitute variables and arrange for the pipeline; to be executed. If any process in the pipeline fails, the entire line (and; test case) fails too. Below is an example of legal RUN lines in a ``.ll`` file:. .. code-block:: llvm. ; RUN: llvm-as < %s | llvm-dis > %t1; ; RUN: llvm-dis < %s.bc-13 > %t2; ; RUN: diff %t1 %t2. As with a Unix shel",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TestingGuide.rst:9069,perform,performs,9069,interpreter/llvm-project/llvm/docs/TestingGuide.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TestingGuide.rst,1,['perform'],['performs']
Performance,"rectories. Even better are the ``report`` and ``report.format`` targets (where; ``format`` is one of ``html``, ``csv``, ``text`` or ``graphs``). The; exact contents of the report are dependent on which ``TEST`` you are; running, but the text results are always shown at the end of the run and; the results are always stored in the ``report.<type>.format`` file (when; running with ``TEST=<type>``). The ``report`` also generate a file; called ``report.<type>.raw.out`` containing the output of the entire; test run. Writing Custom Tests for the test-suite; =======================================. Assuming you can run the test suite, (e.g.; ""``gmake TEST=nightly report``"" should work), it is really easy to run; optimizations or code generator components against every program in the; tree, collecting statistics or running custom checks for correctness. At; base, this is how the nightly tester works, it's just one example of a; general framework. Lets say that you have an LLVM optimization pass, and you want to see; how many times it triggers. First thing you should do is add an LLVM; `statistic <ProgrammersManual.html#Statistic>`_ to your pass, which will; tally counts of things you care about. Following this, you can set up a test and a report that collects these; and formats them for easy viewing. This consists of two files, a; ""``test-suite/TEST.XXX.Makefile``"" fragment (where XXX is the name of; your test) and a ""``test-suite/TEST.XXX.report``"" file that indicates; how to format the output into a table. There are many example reports of; various levels of sophistication included with the test suite, and the; framework is very general. If you are interested in testing an optimization pass, check out the; ""libcalls"" test as an example. It can be run like this:. .. code-block:: bash. % cd llvm/projects/test-suite/MultiSource/Benchmarks # or some other level; % make TEST=libcalls report. This will do a bunch of stuff, then eventually print a table like this:. ::. Name | tota",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TestSuiteMakefileGuide.rst:5442,optimiz,optimization,5442,interpreter/llvm-project/llvm/docs/TestSuiteMakefileGuide.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TestSuiteMakefileGuide.rst,1,['optimiz'],['optimization']
Performance,"ree in half by refactoring the code reading and writing the TBasket data;; A single transient buffer holding the compressed data is now managed by TTree (and could be made thread local); rather than having one per TBranch. Updated TBranchElement::Unroll to no longer split a base class; that can not be split (i.e. respect the information returned; by TStreamerElement::CannotSplit (and thus TClass::CanSplit). This disabling is currently _not_ done automatically for backward compatibility reasons and because; ; Without TClass::SetCanSplit there was no way to; force the splitting (short of setting the split level lower); Some classes still requires a custom streamer solely to; read older data files (for example for file written before; the advent of StreamerInfo) and are such not necessary to; be used when writting (and schema evolution rules can not; yet be used in this case). Allowed removing branches when cloning a TNtuple. Added an option value (""cachedbranches"") to the Print() function of TTreeCache to be able to print the list of cached branches. Made the ownership of the TBranch by fBranch clearer (and thus allow the 'reuse' of TTree object without memory leak). Introduced GetLeaf(branchname,leafname) used in TTreeFormula to avoid ambiguity in the syntax introduced by too many slashes. Improved performance of TTree::GetEntry. With this changes the 'overhead'; compare to protobuf goes from 48% to 24%. (This does not include the; cost of the file opening which can be comparatively large for small; files. For the example used in the comparison the cost TFile::Open is 8% of the cost; of 100000 calls to TTree::GetEntry). Prevented the use of non-existent memory when reading in an object that is part of an STL collection and which used; to contains an embedded object (and this data member has been removed). Now properly recognize a TClonesArray data member even if the requested type was a typedef (to TClonesArray) that is in a namespace (for example edm::Event::Contaie",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/doc/v532/index.html:3287,cache,cachedbranches,3287,tree/doc/v532/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/doc/v532/index.html,4,['cache'],"['cached', 'cachedbranches']"
Performance,"reeDataStore). The default storage type; has now been changed to class RooVectorDataStore which stores the information in STL arrays. Existing; datasets based on trees can be read in transparently, and are converted to vector form in the ; persistent-to-transient conversion (the datafile is not modified in this operation); ; The vector store has two important advantages: 1) faster data access (raw data access times are 70 times ; faster than for TTrees), 2) ability to rewrite columns on the fly. The first advantage is important; for the existing constant-term precalculation optimization in roofit likelihoods as these are now; also stored in vectors rather than trees. The faster access speed of vectors make that the constant; term optimization inside likelihoods results in a larger speed increase. This is particulatly noticeable in pdfs with; many constant expressions from pdfs that were moderately fast to begin with (e.g. RooHistPdf).; The second advantages allows new types of algorithmic likelihood optimization in RooFit detailed below. New algorithmic optimization in the caching of pdfs. So far - in the likelihood - two classes of; objects are identified: those that change with every event (i.e. the pdf) and those that change; only with the parameters (typically pdf normalization integrals). Pdfs are always recalculated; for every event, whereas integrals are only evaluated when needed. The exception to the first type are pdfs; that only depend on constant parameters (or no parameters) - these are identified at the beginning, and precalculated once ; to avoid recalculating an expression with the same outcome for every iteration of the likelihood calculation. For composite pdfs a further optimization has been included: for a M(x,a,b) = f*F(x,a)+(1-f)G(x,b) ; it is e.g. not needed to recalculate G(x,b) if only parameter a has changed w.r.t to the previous likelihood; calculation. This optimization is now implemented by extending the value caching originally designed;",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v532/index.html:1640,optimiz,optimization,1640,roofit/doc/v532/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v532/index.html,2,['optimiz'],['optimization']
Performance,"reeze ty <val> ; yields ty:result. Overview:; """""""""""""""""". The '``freeze``' instruction is used to stop propagation of; :ref:`undef <undefvalues>` and :ref:`poison <poisonvalues>` values. Arguments:; """""""""""""""""""". The '``freeze``' instruction takes a single argument. Semantics:; """""""""""""""""""". If the argument is ``undef`` or ``poison``, '``freeze``' returns an; arbitrary, but fixed, value of type '``ty``'.; Otherwise, this instruction is a no-op and returns the input argument.; All uses of a value returned by the same '``freeze``' instruction are; guaranteed to always observe the same value, while different '``freeze``'; instructions may yield different values. While ``undef`` and ``poison`` pointers can be frozen, the result is a; non-dereferenceable pointer. See the; :ref:`Pointer Aliasing Rules <pointeraliasing>` section for more information.; If an aggregate value or vector is frozen, the operand is frozen element-wise.; The padding of an aggregate isn't considered, since it isn't visible; without storing it into memory and loading it with a different type. Example:; """""""""""""""". .. code-block:: text. %w = i32 undef; %x = freeze i32 %w; %y = add i32 %w, %w ; undef; %z = add i32 %x, %x ; even number because all uses of %x observe; ; the same value; %x2 = freeze i32 %w; %cmp = icmp eq i32 %x, %x2 ; can be true or false. ; example with vectors; %v = <2 x i32> <i32 undef, i32 poison>; %a = extractelement <2 x i32> %v, i32 0 ; undef; %b = extractelement <2 x i32> %v, i32 1 ; poison; %add = add i32 %a, %a ; undef. %v.fr = freeze <2 x i32> %v ; element-wise freeze; %d = extractelement <2 x i32> %v.fr, i32 0 ; not undef; %add.f = add i32 %d, %d ; even number. ; branching on frozen value; %poison = add nsw i1 %k, undef ; poison; %c = freeze i1 %poison; br i1 %c, label %foo, label %bar ; non-deterministic branch to %foo or %bar. .. _i_call:. '``call``' Instruction; ^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. <result> = [tail | musttail | notail ] call [fast-math flags] [cconv] [r",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:471331,load,loading,471331,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['loading']
Performance,"region where the chunk; resides for Primary backed allocations, or 0 for Secondary backed allocations;. - the state of the chunk (available, allocated or quarantined);. - the allocation type (malloc, new, new[] or memalign), to detect potential; mismatches in the allocation APIs used;. - the size (Primary) or unused bytes amount (Secondary) for that chunk, which is; necessary for reallocation or sized-deallocation operations;. - the offset of the chunk, which is the distance in bytes from the beginning of; the returned chunk to the beginning of the backend allocation (the ""block"");. - the 16-bit checksum;. This header fits within 8 bytes on all platforms supported, and contributes to a; small overhead for each allocation. The checksum is computed using a CRC32 (made faster with hardware support); of the global secret, the chunk pointer itself, and the 8 bytes of header with; the checksum field zeroed out. It is not intended to be cryptographically; strong. The header is atomically loaded and stored to prevent races. This is important; as two consecutive chunks could belong to different threads. We work on local; copies and use compare-exchange primitives to update the headers in the heap; memory, and avoid any type of double-fetching. Randomness; ----------; Randomness is a critical factor to the additional security provided by the; allocator. The allocator trusts the memory mapping primitives of the OS to; provide pages at (mostly) non-predictable locations in memory, as well as the; binaries to be compiled with ASLR. In the event one of those assumptions is; incorrect, the security will be greatly reduced. Scudo further randomizes how; blocks are allocated in the Primary, can randomize how caches are assigned to; threads. Memory reclaiming; -----------------; Primary and Secondary allocators have different behaviors with regard to; reclaiming. While Secondary mapped allocations can be unmapped on deallocation,; it isn't the case for the Primary, which could lead to",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ScudoHardenedAllocator.rst:4131,load,loaded,4131,interpreter/llvm-project/llvm/docs/ScudoHardenedAllocator.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ScudoHardenedAllocator.rst,1,['load'],['loaded']
Performance,"region, J. Comput. Appl. Math. 6 (1980) 295-302*.; It is part of the *MathCore* library.; The user can control the relative and absolute tolerance and the maximum allowed number of function evaluation. #### `ROOT::Math::GSLMCIntegrator`. It is a class for performing numerical integration of a multidimensional function. It uses the numerical integration algorithms of GSL, which reimplements the algorithms used; in the QUADPACK, a numerical integration package written in Fortran. Plain MC, MISER and VEGAS integration algorithms are supported for integration over finite (hypercubic) ranges.; For a detail description of the GSL methods visit the GSL users guide.; Specific configuration options (documented in the GSL user guide) for the `ROOT::Math::GSLMCIntegration` can be set directly in the class, or when using it via the `ROOT::Math::IntegratorMultiDim`; interface, can be defined using the `ROOT::Math::IntegratorMultiDimOptions`. ## Function Derivation. There are in ROOT only two classes to perform numerical derivation. One of them is in the MathCore library while the other is in the MathMore wrapping an integration function from the GSL library.; * RichardsonDerivator: Implements the Richardson method for numerical integration. It can calculate up to the third derivative of a function.; * GSLDerivator of *MathMore* based on GSL. ## Numerical Minimization. The algorithms provided by ROOT for numerical integration are implemented following the hierarchy shown in the next image. The left branch of classes are used for one dimensional minimization, while; the right one is used for multidimensional minimization. In the case of multidimensional minimization we have also the classes `TMinuitMinimizer` implemented using `TMinuit`, `TFumiliMinimizer`; implemented using `TFumili` for least square or likelihood minimizations.; We encourage the use of the GSL algorithms for one dimensional minimization and `Minuit2` (or the old version`Minuit`) for multi dimensional minimizatio",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/MathLibraries.md:62417,perform,perform,62417,documentation/users-guide/MathLibraries.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/MathLibraries.md,1,['perform'],['perform']
Performance,"register pressure. E.g. ""Minimum Register; Instruction Sequence Problem: Revisiting Optimal Code Generation for DAGs""; and other related papers.; http://citeseer.ist.psu.edu/govindarajan01minimum.html. //===---------------------------------------------------------------------===//. Should we promote i16 to i32 to avoid partial register update stalls?. //===---------------------------------------------------------------------===//. Leave any_extend as pseudo instruction and hint to register; allocator. Delay codegen until post register allocation.; Note. any_extend is now turned into an INSERT_SUBREG. We still need to teach; the coalescer how to deal with it though. //===---------------------------------------------------------------------===//. It appears icc use push for parameter passing. Need to investigate. //===---------------------------------------------------------------------===//. The instruction selector sometimes misses folding a load into a compare. The; pattern is written as (cmp reg, (load p)). Because the compare isn't; commutative, it is not matched with the load on both sides. The dag combiner; should be made smart enough to canonicalize the load into the RHS of a compare; when it can invert the result of the compare for free. //===---------------------------------------------------------------------===//. In many cases, LLVM generates code like this:. _test:; movl 8(%esp), %eax; cmpl %eax, 4(%esp); setl %al; movzbl %al, %eax; ret. on some processors (which ones?), it is more efficient to do this:. _test:; movl 8(%esp), %ebx; xor %eax, %eax; cmpl %ebx, 4(%esp); setl %al; ret. Doing this correctly is tricky though, as the xor clobbers the flags. //===---------------------------------------------------------------------===//. We should generate bts/btr/etc instructions on targets where they are cheap or; when codesize is important. e.g., for:. void setbit(int *target, int bit) {; *target |= (1 << bit);; }; void clearbit(int *target, int bit) {; *targ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:2739,load,load,2739,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt,2,['load'],['load']
Performance,"register scavenger) to assign it a free register to allow; reuse:; ldr r3, [sp, #+4]; add r3, r3, #3; ldr r2, [sp, #+8]; add r2, r2, #2; ldr r1, [sp, #+4] <==; add r1, r1, #1; ldr r0, [sp, #+4]; add r0, r0, #2. //===---------------------------------------------------------------------===//. LLVM aggressively lift CSE out of loop. Sometimes this can be negative side-; effects:. R1 = X + 4; R2 = X + 7; R3 = X + 15. loop:; load [i + R1]; ...; load [i + R2]; ...; load [i + R3]. Suppose there is high register pressure, R1, R2, R3, can be spilled. We need; to implement proper re-materialization to handle this:. R1 = X + 4; R2 = X + 7; R3 = X + 15. loop:; R1 = X + 4 @ re-materialized; load [i + R1]; ...; R2 = X + 7 @ re-materialized; load [i + R2]; ...; R3 = X + 15 @ re-materialized; load [i + R3]. Furthermore, with re-association, we can enable sharing:. R1 = X + 4; R2 = X + 7; R3 = X + 15. loop:; T = i + X; load [T + 4]; ...; load [T + 7]; ...; load [T + 15]; //===---------------------------------------------------------------------===//. It's not always a good idea to choose rematerialization over spilling. If all; the load / store instructions would be folded then spilling is cheaper because; it won't require new live intervals / registers. See 2003-05-31-LongShifts for; an example. //===---------------------------------------------------------------------===//. With a copying garbage collector, derived pointers must not be retained across; collector safe points; the collector could move the objects and invalidate the; derived pointer. This is bad enough in the first place, but safe points can; crop up unpredictably. Consider:. %array = load { i32, [0 x %obj] }** %array_addr; %nth_el = getelementptr { i32, [0 x %obj] }* %array, i32 0, i32 %n; %old = load %obj** %nth_el; %z = div i64 %x, %y; store %obj* %new, %obj** %nth_el. If the i64 division is lowered to a libcall, then a safe point will (must); appear for the call site. If a collection occurs, %array and %nth_el no ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/README.txt:2278,load,load,2278,interpreter/llvm-project/llvm/lib/CodeGen/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/README.txt,1,['load'],['load']
Performance,"registers, going to extremes to achieve this; by disabling callee save registers. This calling convention should; not be used lightly but only for specific situations such as an; alternative to the *register pinning* performance technique often; used when implementing functional programming languages. At the; moment only X86, AArch64, and RISCV support this convention. The ; following limitations exist:. - On *X86-32* only up to 4 bit type parameters are supported. No; floating-point types are supported.; - On *X86-64* only up to 10 bit type parameters and 6; floating-point parameters are supported.; - On *AArch64* only up to 4 32-bit floating-point parameters,; 4 64-bit floating-point parameters, and 10 bit type parameters; are supported.; - *RISCV64* only supports up to 11 bit type parameters, 4; 32-bit floating-point parameters, and 4 64-bit floating-point; parameters. This calling convention supports `tail call; optimization <CodeGenerator.html#tail-call-optimization>`_ but requires; both the caller and callee are using it.; ""``cc 11``"" - The HiPE calling convention; This calling convention has been implemented specifically for use by; the `High-Performance Erlang; (HiPE) <http://www.it.uu.se/research/group/hipe/>`_ compiler, *the*; native code compiler of the `Ericsson's Open Source Erlang/OTP; system <http://www.erlang.org/download.shtml>`_. It uses more; registers for argument passing than the ordinary C calling; convention and defines no callee-saved registers. The calling; convention properly supports `tail call; optimization <CodeGenerator.html#tail-call-optimization>`_ but requires; that both the caller and the callee use it. It uses a *register pinning*; mechanism, similar to GHC's convention, for keeping frequently; accessed runtime components pinned to specific hardware registers.; At the moment only X86 supports this convention (both 32 and 64; bit).; ""``anyregcc``"" - Dynamic calling convention for code patching; This is a special convention that suppo",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:14783,optimiz,optimization,14783,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['optimiz'],['optimization']
Performance,"release unless we could prove that the message send could not; modify ``_ivar`` (or deallocate ``self``). Since message sends are; opaque to the optimizer, this is not possible, and so ARC's hands; would be almost completely tied. ARC makes no guarantees about the execution of a computation history; which contains undefined behavior. In particular, ARC makes no; guarantees in the presence of race conditions. ARC may assume that any retainable object pointers it receives or; generates are instantaneously valid from that point until a point; which, by the concurrency model of the host language, happens-after; the generation of the pointer and happens-before a release of that; object (possibly via an aliasing pointer or indirectly due to; destruction of a different object). .. admonition:: Rationale. There is very little point in trying to guarantee correctness in the; presence of race conditions. ARC does not have a stack-scanning; garbage collector, and guaranteeing the atomicity of every load and; store operation would be prohibitive and preclude a vast amount of; optimization. ARC may assume that non-ARC code engages in sensible balancing; behavior and does not rely on exact or minimum retain count values; except as guaranteed by ``__strong`` object invariants or +1 transfer; conventions. For example, if an object is provably double-retained; and double-released, ARC may eliminate the inner retain and release;; it does not need to guard against code which performs an unbalanced; release followed by a ""balancing"" retain. .. _arc.optimization.liveness:. Object liveness; ---------------. ARC may not allow a retainable object ``X`` to be deallocated at a; time ``T`` in a computation history if:. * ``X`` is the value stored in a ``__strong`` object ``S`` with; :ref:`precise lifetime semantics <arc.optimization.precise>`, or. * ``X`` is the value stored in a ``__strong`` object ``S`` with; imprecise lifetime semantics and, at some point after ``T`` but; before the next st",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst:78089,load,load,78089,interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,2,"['load', 'optimiz']","['load', 'optimization']"
Performance,"rementioned free function. **Sample Steps**: To add Code Generation support to an existing RooFit class,; following is a sample set of steps (using the aforementioned approach of; extracting free functions in a separate file.). **1. Extract logic into a separate file** Implement what your class is; supposed to do as a free function in [MathFuncs].; This implementation must be compatible with the syntax supported by Clad. **2. Refactor evaluate():** Refactor the existing `RooAbsReal::evaluate()`; function to use the `MathFuncs.h` implementation. This is optional, but; can reduce code duplication and potential for bugs. This may require some; effort if an extensive caching infrastructure is used in your model. **3. Add translate():** RooFit classes are extended using a (typically) simple; `translate()` function that extracts the mathematically differentiable; properties out of the RooFit classes that make up the statistical model. The `translate()` function helps implement the Code Squashing logic that is; used to optimize numerical evaluations. It accomplishes this by using a small; subset of helper functions that are available in the; `RooFit::Detail::CodeSquashContext` and `RooFuncWrapper` classes; (see Appendix B). It converts a RooFit expression into a form that can be; efficiently evaluated by Clad. The `translate()` function returns an `std::string` representing the; underlying mathematical notation of the class as code, that can later be; concatenated into a single string representing the entire model. This string; of code is then just-in-time compiled by Cling (a C++ interpreter for Root). **4. analyticalIntegral() Use Case:** If your class includes (or should; include) the `analyticalIntegral()` function, then a simple; `buildCallToAnalyticIntegral()` function needs to be created to help call the; `analyticalIntegral()` function. # Example for adding Code Generation support to RooFit classes. Let us take the `RooPoisson.cxx` class as an example. > [roofit/roo",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/developers/roofit_ad.md:8420,optimiz,optimize,8420,roofit/doc/developers/roofit_ad.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/developers/roofit_ad.md,1,['optimiz'],['optimize']
Performance,"rent particle position can be set using; SetCurrentPosition(x,y,z) method of the manager class, in which; case FindNode() can be called without arguments. The method; returns a pointer to the ""deepest node"" that geometrically contains *P*; (in our case let us suppose it is `B\_3`). Since a node is just a; positioned volume, we can then get a pointer to the volume, medium or; material objects related to it. ""Deepest"" means that `B\_3` still; contains point `P` (as well as `A\_1` and `TOP\_1`), but none of the; daughters of volume `B` does. After finding out the node containing; the particle, one can check if the geometry state is different compared; to the last located point:. ~~~{.cpp}; Bool_t *TGeoManager::IsSameLocation(); ~~~. The algorithm for finding where a point is located in geometry is; presented in the figure 17-36. It always starts by checking if the last computed modeller state is the; answer. This optimizes the search when continuously tracking a particle.; The main actions performed are:. - moving up and down in the logical node tree while updating the; current node and its global matrix; - converting the global position into the local frame of the current; node/volume; - checking whether the local position lies within the geometrical; shape of the current volume - if this is the case continue the; search downwards for the daughters of the current node, otherwise; search upwards its containers until the top level is reached.; - the number of candidate nodes to be checked at a given level is; minimized by an additional optimization structure: voxels. This is; effective even in case there is only one daughter of the current; volume.; - in case the current node is declared as possibly overlapping, the; method FindInCluster() is invoked. This method checks all different; possibilities within the cluster of overlapping candidates. One of; the candidates is prioritized if one of the following conditions id; fulfilled (in order):; - Is declared as non-overlap",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md:117746,perform,performed,117746,geom/geom/doc/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md,1,['perform'],['performed']
Performance,"res any; following global; data read is no; older than the local; atomicrmw value; being acquired. atomicrmw acquire - workgroup - generic 1. flat_atomic; 2. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL, omit lgkmcnt(0).; - Must happen before; the following; buffer_inv and; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local; atomicrmw value; being acquired. 3. buffer_inv sc0=1. - If not TgSplit execution; mode, omit.; - Ensures that; following; loads will not see; stale data. atomicrmw acquire - agent - global 1. buffer/global_atomic; 2. s_waitcnt vmcnt(0). - Must happen before; following; buffer_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; cache. 3. buffer_inv sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acquire - system - global 1. buffer/global_atomic; sc1=1; 2. s_waitcnt vmcnt(0). - Must happen before; following; buffer_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; caches. 3. buffer_inv sc0=1 sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale MTYPE NC global data.; MTYPE RW and CC memory will; never be stale due to the; memory probes. atomicrmw acquire - agent - generic 1. flat_atomic; 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Must happen before; following; buffer_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; cache. 3. buffer_inv sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acquire - system - generic 1. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:300835,load,loads,300835,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['loads']
Performance,"res that every volatile load and store happens and is performed in the; stated order. A couple examples: if a SequentiallyConsistent store is; immediately followed by another SequentiallyConsistent store to the same; address, the first store can be erased. This transformation is not allowed for a; pair of volatile stores. On the other hand, a non-volatile non-atomic load can; be moved across a volatile load freely, but not an Acquire load. This document is intended to provide a guide to anyone either writing a frontend; for LLVM or working on optimization passes for LLVM with a guide for how to deal; with instructions with special semantics in the presence of concurrency. This; is not intended to be a precise guide to the semantics; the details can get; extremely complicated and unreadable, and are not usually necessary. .. _Optimization outside atomic:. Optimization outside atomic; ===========================. The basic ``'load'`` and ``'store'`` allow a variety of optimizations, but can; lead to undefined results in a concurrent environment; see `NotAtomic`_. This; section specifically goes into the one optimizer restriction which applies in; concurrent environments, which gets a bit more of an extended description; because any optimization dealing with stores needs to be aware of it. From the optimizer's point of view, the rule is that if there are not any; instructions with atomic ordering involved, concurrency does not matter, with; one exception: if a variable might be visible to another thread or signal; handler, a store cannot be inserted along a path where it might not execute; otherwise. Take the following example:. .. code-block:: c. /* C code, for readability; run through clang -O2 -S -emit-llvm to get; equivalent IR */; int x;; void f(int* a) {; for (int i = 0; i < 100; i++) {; if (a[i]); x += 1;; }; }. The following is equivalent in non-concurrent situations:. .. code-block:: c. int x;; void f(int* a) {; int xtemp = x;; for (int i = 0; i < 100; i++) {; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst:2475,load,load,2475,interpreter/llvm-project/llvm/docs/Atomics.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst,3,"['concurren', 'load', 'optimiz']","['concurrent', 'load', 'optimizations']"
Performance,"responding leaflist letters are 'G' and 'g'.; - when looping over a `TTree` with a friend with a larger number of entries, `TTreeReader` now ends the event loop when the entries in the _main_ `TTree` are exhausted, consistently with other interfaces. See [#6518](https://github.com/root-project/root/issues/6518) for more details.; - `TTreeProcessorMT::SetMaxTasksPerFilePerWorker` is now deprecated in favor of the more flexible and newly introduced `TTreeProcessorMT::SetTasksPerWorkerHint`. See the relevant entries in our reference guide for more information.; - The name of the sub-branches of a split collection no longer have 2 consecutive dots if the top level branche name has a trailing dot. The name of the collection's index leaf also no longer include the dot. For example for ""t."" the names where ""t._"" and ""t..fValue"" and are now ""t_"" and ""t.fValue"". . ## RDataFrame. ### New features. - Introduce `ROOT::RDF::RunGraphs`, which allows to compute the results of multiple `RDataFrame`s (or better, multiple independent computation graphs) concurrently while sharing the same thread pool. The computation may be more efficient than running the `RDataFrame`s sequentially if an analysis consists of several computation graphs that individually do not fully utilize the available resources. See e.g. [this tutorial](https://root.cern/doc/master/df104__HiggsToTwoPhotons_8py.html) for an example usage.; - `RDataFrame` now supports reading friend `TTree`s with a `TTreeIndex`, aka ""indexed friends"". More details at [ROOT-9559](https://sft.its.cern.ch/jira/browse/ROOT-9559).; - Experimental logging capabilities have been added to `RDataFrame`. To activate logging, define the following variable before creating the `RDataFrame` object: `auto verbosity = ROOT::Experimental::RLogScopedVerbosity(ROOT::Detail::RDF::RDFLogChannel(), ROOT::Experimental::ELogLevel.kInfo);`.; - With [ROOT-10023](https://sft.its.cern.ch/jira/browse/ROOT-10023) fixed, `RDataFrame` can now read and write certain",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v624/index.md:5872,concurren,concurrently,5872,README/ReleaseNotes/v624/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v624/index.md,1,['concurren'],['concurrently']
Performance,"ress space on; the fence need to; conservatively; always generate. If; fence had an; address space then; set to address; space of OpenCL; fence flag, or to; generic if both; local and global; flags are; specified.; - Could be split into; separate s_waitcnt; vmcnt(0), s_waitcnt; vscnt(0) and s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/load atomic/; atomicrmw-with-return-value.; - s_waitcnt vscnt(0); must happen after; any preceding; global/generic; store/store atomic/; atomicrmw-no-return-value.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; any following store; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - Ensures that all; memory operations; have; completed before; performing the; following; fence-paired-atomic. **Acquire-Release Atomic**; ------------------------------------------------------------------------------------; atomicrmw acq_rel - singlethread - global 1. buffer/global/ds/flat_atomic; - wavefront - local; - generic; atomicrmw acq_rel - workgroup - global 1. s_waitcnt lgkmcnt(0) &; vmcnt(0) & vscnt(0). - If CU wavefront execution; mode, omit vmcnt(0) and; vscnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Could be split into; separate s_waitcnt; vmcnt(0), s_waitcnt; vscnt(0), and s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load/load; atomic/; atomicrmw-with-return-value.; - s_waitcnt vscnt(0); must happen after; any preceding; global/generic; store/store; atomic/; atomicrmw-no-return-value.; - s_waitcnt lgk",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:362764,perform,performing,362764,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['perform'],['performing']
Performance,"ress* of a source variable. .. code-block:: text. %i.addr = alloca i32, align 4; call void @llvm.dbg.declare(metadata i32* %i.addr, metadata !1,; metadata !DIExpression()), !dbg !2; !1 = !DILocalVariable(name: ""i"", ...) ; int i; !2 = !DILocation(...); ...; %buffer = alloca [256 x i8], align 8; ; The address of i is buffer+64.; call void @llvm.dbg.declare(metadata [256 x i8]* %buffer, metadata !3,; metadata !DIExpression(DW_OP_plus, 64)), !dbg !4; !3 = !DILocalVariable(name: ""i"", ...) ; int i; !4 = !DILocation(...). A frontend should generate exactly one call to ``llvm.dbg.declare`` at the point; of declaration of a source variable. Optimization passes that fully promote the; variable from memory to SSA values will replace this call with possibly multiple; calls to `llvm.dbg.value`. Passes that delete stores are effectively partial; promotion, and they will insert a mix of calls to ``llvm.dbg.value`` to track; the source variable value when it is available. After optimization, there may be; multiple calls to ``llvm.dbg.declare`` describing the program points where the; variables lives in memory. All calls for the same concrete source variable must; agree on the memory location. ``llvm.dbg.value``; ^^^^^^^^^^^^^^^^^^. .. code-block:: llvm. void @llvm.dbg.value(metadata, metadata, metadata). This intrinsic provides information when a user source variable is set to a new; value. The first argument is the new value (wrapped as metadata). The second; argument is a `local variable <LangRef.html#dilocalvariable>`_ containing a; description of the variable. The third argument is a `complex expression; <LangRef.html#diexpression>`_. An `llvm.dbg.value` intrinsic describes the *value* of a source variable; directly, not its address. Note that the value operand of this intrinsic may; be indirect (i.e, a pointer to the source variable), provided that interpreting; the complex expression derives the direct value. ``llvm.dbg.assign``; ^^^^^^^^^^^^^^^^^^^; .. toctree::; :hidden:. A",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst:9933,optimiz,optimization,9933,interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,1,['optimiz'],['optimization']
Performance,"ress; uint32 : NumFaultingPCs; uint32 : Reserved (expected to be 0); FunctionFaultInfo[NumFaultingPCs] {; uint32 : FaultKind; uint32 : FaultingPCOffset; uint32 : HandlerPCOffset; }; }. FailtKind describes the reason of expected fault. Currently three kind; of faults are supported:. 1. ``FaultMaps::FaultingLoad`` - fault due to load from memory.; 2. ``FaultMaps::FaultingLoadStore`` - fault due to instruction load and store.; 3. ``FaultMaps::FaultingStore`` - fault due to store to memory. The ``ImplicitNullChecks`` pass; ===============================. The ``ImplicitNullChecks`` pass transforms explicit control flow for; checking if a pointer is ``null``, like:. .. code-block:: llvm. %ptr = call i32* @get_ptr(); %ptr_is_null = icmp i32* %ptr, null; br i1 %ptr_is_null, label %is_null, label %not_null, !make.implicit !0. not_null:; %t = load i32, i32* %ptr; br label %do_something_with_t. is_null:; call void @HFC(); unreachable. !0 = !{}. to control flow implicit in the instruction loading or storing through; the pointer being null checked:. .. code-block:: llvm. %ptr = call i32* @get_ptr(); %t = load i32, i32* %ptr ;; handler-pc = label %is_null; br label %do_something_with_t. is_null:; call void @HFC(); unreachable. This transform happens at the ``MachineInstr`` level, not the LLVM IR; level (so the above example is only representative, not literal). The; ``ImplicitNullChecks`` pass runs during codegen, if; ``-enable-implicit-null-checks`` is passed to ``llc``. The ``ImplicitNullChecks`` pass adds entries to the; ``__llvm_faultmaps`` section described above as needed. ``make.implicit`` metadata; --------------------------. Making null checks implicit is an aggressive optimization, and it can; be a net performance pessimization if too many memory operations end; up faulting because of it. A language runtime typically needs to; ensure that only a negligible number of implicit null checks actually; fault once the application has reached a steady state. A standard way; of ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/FaultMaps.rst:2497,load,loading,2497,interpreter/llvm-project/llvm/docs/FaultMaps.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/FaultMaps.rst,1,['load'],['loading']
Performance,"resses.; * Each CU has a separate request queue per channel for its associated L2.; Therefore, the vector and scalar memory operations performed by wavefronts; executing with different L1 caches and the same L2 cache can be reordered; relative to each other.; * A ``s_waitcnt vmcnt(0)`` is required to ensure synchronization between; vector memory operations of different CUs. It ensures a previous vector; memory operation has completed before executing a subsequent vector memory; or LDS operation and so can be used to meet the requirements of acquire and; release.; * An L2 cache can be kept coherent with other L2 caches by using the MTYPE RW; (read-write) for memory local to the L2, and MTYPE NC (non-coherent) with; the PTE C-bit set for memory not local to the L2. * Any local memory cache lines will be automatically invalidated by writes; from CUs associated with other L2 caches, or writes from the CPU, due to; the cache probe caused by the PTE C-bit.; * XGMI accesses from the CPU to local memory may be cached on the CPU.; Subsequent access from the GPU will automatically invalidate or writeback; the CPU cache due to the L2 probe filter.; * To ensure coherence of local memory writes of CUs with different L1 caches; in the same agent a ``buffer_wbl2`` is required. It does nothing if the; agent is configured to have a single L2, or will writeback dirty L2 cache; lines if configured to have multiple L2 caches.; * To ensure coherence of local memory writes of CUs in different agents a; ``buffer_wbl2 sc1`` is required. It will writeback dirty L2 cache lines.; * To ensure coherence of local memory reads of CUs with different L1 caches; in the same agent a ``buffer_inv sc1`` is required. It does nothing if the; agent is configured to have a single L2, or will invalidate non-local L2; cache lines if configured to have multiple L2 caches.; * To ensure coherence of local memory reads of CUs in different agents a; ``buffer_inv sc0 sc1`` is required. It will invalidate non-local",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:288635,cache,cached,288635,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['cache'],['cached']
Performance,"ression:. * v5.34/38; * v6.08/06 [not yet released]; * v6.10/08; * v6.12/02. ## Removed interfaces. ## Core Libraries; - Optimize away redundant deserialization of template specializations. This reduces the memory footprint for hsimple by around 30% while improving the runtime performance for various cases by around 15%.; - When ROOT is signaled with a SIGUSR2 (i.e. on Linux and MacOS X) it will now print a backtrace.; - Move RStringView.h to ROOT/RStringView.hxx and always include ROOT/RStringView.hxx instead of RStringView.h for backward compatibility; - In `TClingCallFunc`, support r-value reference parameters. This paves the way for the corresponding support in PyROOT (implemented now in the latest Cppyy).; - Included the new TSequentialExecutor in ROOT, sharing the interfaces of TExecutor.This should improve code economy when providing a fallback for TThreadExecutor/TProcessExecutor. ### Thread safety; - Resolved several race conditions, dead-locks, performance and order of initialization/destruction issues still lingering because of or despite the new read-write lock mechanism. ## Interpreter. - Enabled use of multi-threaded code from the interpreter.; - Previouslyl multi-threaded code could be run from the interpreter as long as the call starting the threada was the same code that initialized the ROOT global lock, any other uses, including attempting to run the same code a second time in the same session would lead to a dead lock (if any other thread attempted to take on the ROOT lock).; - The interpreter now suspend the ROOT lock (which is taken to protect the interpreter global state) during user code execution. ## I/O Libraries; - LZ4 (with compression level 4) is now the default compression algorithm for new ROOT files (LZ4 is lossless data compression algorithm that is focused on compression and decompression speed, while in ROOT case providing benefit in faster decompression at the price of a bit worse compression ratio comparing to ZLIB); - If two or mo",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v614/index.md:2560,race condition,race conditions,2560,README/ReleaseNotes/v614/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v614/index.md,2,"['perform', 'race condition']","['performance', 'race conditions']"
Performance,"rested readers may read that chapter for details, but; in short: to optimize a Module we create an llvm::FunctionPassManager; instance, configure it with a set of optimizations, then run the PassManager on; a Module to mutate it into a (hopefully) more optimized but semantically; equivalent form. In the original tutorial series the FunctionPassManager was; created outside the KaleidoscopeJIT and modules were optimized before being; added to it. In this Chapter we will make optimization a phase of our JIT; instead. For now this will provide us a motivation to learn more about ORC; layers, but in the long term making optimization part of our JIT will yield an; important benefit: When we begin lazily compiling code (i.e. deferring; compilation of each function until the first time it's run) having; optimization managed by our JIT will allow us to optimize lazily too, rather; than having to do all our optimization up-front. To add optimization support to our JIT we will take the KaleidoscopeJIT from; Chapter 1 and compose an ORC *IRTransformLayer* on top. We will look at how the; IRTransformLayer works in more detail below, but the interface is simple: the; constructor for this layer takes a reference to the execution session and the; layer below (as all layers do) plus an *IR optimization function* that it will; apply to each Module that is added via addModule:. .. code-block:: c++. class KaleidoscopeJIT {; private:; ExecutionSession ES;; RTDyldObjectLinkingLayer ObjectLayer;; IRCompileLayer CompileLayer;; IRTransformLayer TransformLayer;. DataLayout DL;; MangleAndInterner Mangle;; ThreadSafeContext Ctx;. public:. KaleidoscopeJIT(JITTargetMachineBuilder JTMB, DataLayout DL); : ObjectLayer(ES,; []() { return std::make_unique<SectionMemoryManager>(); }),; CompileLayer(ES, ObjectLayer, ConcurrentIRCompiler(std::move(JTMB))),; TransformLayer(ES, CompileLayer, optimizeModule),; DL(std::move(DL)), Mangle(ES, this->DL),; Ctx(std::make_unique<LLVMContext>()) {; ES.getMainJITDy",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:2487,optimiz,optimization,2487,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,1,['optimiz'],['optimization']
Performance,"result value.; t4: ch = CopyToReg t3, Register:f64 %vreg, t2; t5: res,ch = CopyFromReg t4, Register:f64 %vreg; t6: res = FADD t5, t9. We also need this for locally streaming functions, where an ``SMSTART`` needs to; be inserted into the DAG at the start of the function. Functions with __attribute__((arm_locally_streaming)); -----------------------------------------------------. If a function is marked as ``arm_locally_streaming``, then the runtime SVE; vector length in the prologue/epilogue may be different from the vector length; in the function's body. This happens because we invoke smstart after setting up; the stack-frame and similarly invoke smstop before deallocating the stack-frame. To ensure we use the correct SVE vector length to allocate the locals with, we; can use the streaming vector-length to allocate the stack-slots through the; ``ADDSVL`` instruction, even when the CPU is not yet in streaming mode. This only works for locals and not callee-save slots, since LLVM doesn't support; mixing two different scalable vector lengths in one stack frame. That means that the; case where a function is marked ``arm_locally_streaming`` and needs to spill SVE; callee-saves in the prologue is currently unsupported. However, it is unlikely; for this to happen without user intervention, because ``arm_locally_streaming``; functions cannot take or return vector-length-dependent values. This would otherwise; require forcing both the SVE PCS using '``aarch64_sve_pcs``' combined with using; ``arm_locally_streaming`` in order to encounter this problem. This combination; can be prevented in Clang through emitting a diagnostic. An example of how the prologue/epilogue would look for a function that is; attributed with ``arm_locally_streaming``:. .. code-block:: c++. #define N 64. void __attribute__((arm_streaming_compatible)) some_use(svfloat32_t *);. // Use a float argument type, to check the value isn't clobbered by smstart.; // Use a float return type to check the value isn't ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AArch64SME.rst:11729,scalab,scalable,11729,interpreter/llvm-project/llvm/docs/AArch64SME.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AArch64SME.rst,1,['scalab'],['scalable']
Performance,"result> = insertelement <vscale x n x <ty>> <val>, <ty> <elt>, <ty2> <idx> ; yields <vscale x n x <ty>>. Overview:; """""""""""""""""". The '``insertelement``' instruction inserts a scalar element into a; vector at a specified index. Arguments:; """""""""""""""""""". The first operand of an '``insertelement``' instruction is a value of; :ref:`vector <t_vector>` type. The second operand is a scalar value whose; type must equal the element type of the first operand. The third operand; is an index indicating the position at which to insert the value. The; index may be a variable of any integer type, and will be treated as an; unsigned integer. Semantics:; """""""""""""""""""". The result is a vector of the same type as ``val``. Its element values; are those of ``val`` except at position ``idx``, where it gets the value; ``elt``. If ``idx`` exceeds the length of ``val`` for a fixed-length vector,; the result is a :ref:`poison value <poisonvalues>`. For a scalable vector,; if the value of ``idx`` exceeds the runtime length of the vector, the result; is a :ref:`poison value <poisonvalues>`. Example:; """""""""""""""". .. code-block:: text. <result> = insertelement <4 x i32> %vec, i32 1, i32 0 ; yields <4 x i32>. .. _i_shufflevector:. '``shufflevector``' Instruction; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. <result> = shufflevector <n x <ty>> <v1>, <n x <ty>> <v2>, <m x i32> <mask> ; yields <m x <ty>>; <result> = shufflevector <vscale x n x <ty>> <v1>, <vscale x n x <ty>> v2, <vscale x m x i32> <mask> ; yields <vscale x m x <ty>>. Overview:; """""""""""""""""". The '``shufflevector``' instruction constructs a permutation of elements; from two input vectors, returning a vector with the same element type as; the input and length that is the same as the shuffle mask. Arguments:; """""""""""""""""""". The first two operands of a '``shufflevector``' instruction are vectors; with the same type. The third argument is a shuffle mask vector constant; whose element type is ``i32``. The mask vector elements must be constant; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:403223,scalab,scalable,403223,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['scalab'],['scalable']
Performance,"returned; pointer may not be unique. The order in which memory is allocated (ie.,; which way the stack grows) is not specified. Note that '``alloca``' outside of the alloca address space from the; :ref:`datalayout string<langref_datalayout>` is meaningful only if the; target has assigned it a semantics. If the returned pointer is used by :ref:`llvm.lifetime.start <int_lifestart>`,; the returned object is initially dead.; See :ref:`llvm.lifetime.start <int_lifestart>` and; :ref:`llvm.lifetime.end <int_lifeend>` for the precise semantics of; lifetime-manipulating intrinsics. Example:; """""""""""""""". .. code-block:: llvm. %ptr = alloca i32 ; yields ptr; %ptr = alloca i32, i32 4 ; yields ptr; %ptr = alloca i32, i32 4, align 1024 ; yields ptr; %ptr = alloca i32, align 1024 ; yields ptr. .. _i_load:. '``load``' Instruction; ^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. <result> = load [volatile] <ty>, ptr <pointer>[, align <alignment>][, !nontemporal !<nontemp_node>][, !invariant.load !<empty_node>][, !invariant.group !<empty_node>][, !nonnull !<empty_node>][, !dereferenceable !<deref_bytes_node>][, !dereferenceable_or_null !<deref_bytes_node>][, !align !<align_node>][, !noundef !<empty_node>]; <result> = load atomic [volatile] <ty>, ptr <pointer> [syncscope(""<target-scope>"")] <ordering>, align <alignment> [, !invariant.group !<empty_node>]; !<nontemp_node> = !{ i32 1 }; !<empty_node> = !{}; !<deref_bytes_node> = !{ i64 <dereferenceable_bytes> }; !<align_node> = !{ i64 <value_alignment> }. Overview:; """""""""""""""""". The '``load``' instruction is used to read from memory. Arguments:; """""""""""""""""""". The argument to the ``load`` instruction specifies the memory address from which; to load. The type specified must be a :ref:`first class <t_firstclass>` type of; known size (i.e. not containing an :ref:`opaque structural type <t_opaque>`). If; the ``load`` is marked as ``volatile``, then the optimizer is not allowed to; modify the number or order of execution of this ``load`` with other; :re",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:412006,load,load,412006,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,"returned; value is not appropriately aligned at runtime, a poison value is returned; instead. The optional ``!noundef`` metadata must reference a single metadata name; ``<empty_node>`` corresponding to a node with no entries. The existence of; ``!noundef`` metadata on the instruction tells the optimizer that the value; loaded is known to be :ref:`well defined <welldefinedvalues>`.; If the value isn't well defined, the behavior is undefined. If the ``!noundef``; metadata is combined with poison-generating metadata like ``!nonnull``,; violation of that metadata constraint will also result in undefined behavior. Semantics:; """""""""""""""""""". The location of memory pointed to is loaded. If the value being loaded; is of scalar type then the number of bytes read does not exceed the; minimum number of bytes needed to hold all bits of the type. For; example, loading an ``i24`` reads at most three bytes. When loading a; value of a type like ``i20`` with a size that is not an integral number; of bytes, the result is undefined if the value was not originally; written using a store of the same type.; If the value being loaded is of aggregate type, the bytes that correspond to; padding may be accessed but are ignored, because it is impossible to observe; padding from the loaded aggregate value.; If ``<pointer>`` is not a well-defined value, the behavior is undefined. Examples:; """""""""""""""""". .. code-block:: llvm. %ptr = alloca i32 ; yields ptr; store i32 3, ptr %ptr ; yields void; %val = load i32, ptr %ptr ; yields i32:val = i32 3. .. _i_store:. '``store``' Instruction; ^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. store [volatile] <ty> <value>, ptr <pointer>[, align <alignment>][, !nontemporal !<nontemp_node>][, !invariant.group !<empty_node>] ; yields void; store atomic [volatile] <ty> <value>, ptr <pointer> [syncscope(""<target-scope>"")] <ordering>, align <alignment> [, !invariant.group !<empty_node>] ; yields void; !<nontemp_node> = !{ i32 1 }; !<empty_node> = !{}. Overview:; """"""""""""""",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:418382,load,loading,418382,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['loading']
Performance,"rformance evaluation). TransformOutput No False − Transform likelihood output by inverse sigmoid function. Configuration options for MVA method :. Configuration options reference for MVA method: MLP. Option Array Default value Predefined values Description. NCycles No 500 − Number of training cycles. HiddenLayers No N,N-1 − Specification of hidden layer architecture. NeuronType No sigmoid − Neuron activation function type. RandomSeed No 1 − Random seed for initial synapse weights (0 means unique seed for each run; default value '1'). EstimatorType No MSE MSE, CE, linear, sigmoid, tanh, radial MSE (Mean Square Estimator) for Gaussian Likelihood or CE(Cross-Entropy) for Bernoulli Likelihood. NeuronInputType No sum sum, sqsum, abssum Neuron input function type. V No False − Verbose output (short form of VerbosityLevel below - overrides the latter one). VerbosityLevel No Default Default, Debug, Verbose, Info, Warning, Error, Fatal Verbosity level. VarTransform No None − List of variable transformations performed before training, e.g., D_Background,P_Signal,G,N_AllClasses for: Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed). H No False − Print method-specific help message. CreateMVAPdfs No False − Create PDFs for classifier outputs (signal and background). IgnoreNegWeightsInTraining No False − Events with negative weights are ignored in the training (but are included for testing and performance evaluation). TrainingMethod No BP BP, GA, BFGS Train with Back-Propagation (BP), BFGS Algorithm (BFGS), or Genetic Algorithm (GA - slower and worse). LearningRate No 0.02 − ANN learning rate parameter. DecayRate No 0.01 − Decay rate for learning parameter. TestRate No 10 − Test for overtraining performed at each #th epochs. EpochMonitoring No False − Provide epoch-wise monitoring plots according to TestRate (caution: causes big R",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/tmva/UsersGuide/optionRef.html:21690,perform,performed,21690,documentation/tmva/UsersGuide/optionRef.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/tmva/UsersGuide/optionRef.html,2,['perform'],['performed']
Performance,"rgence regions must be satisfied by; valid programs:. If a convergence region R for a token T1 contains a use of a convergence; token T2, then R must also contain the definition of T2. (In other words,; convergence regions must be reasonably nested.). .. note::. For brevity, this document uses the term ""convergence region of a token; definition ``D``"" to actually refer to the convergence region of the token; ``T`` defined by ``D``. .. _inferring_noconvergent:. Inferring non-convergence; =========================. When the target or the environment guarantees that threads do not; communicate using convergent operations or that threads never diverge,; the dynamic instances in the program are irrelevant and an optimizer; may remove any occurrence of the ``convergent`` attribute on a; call-site or a function and any explicit ``convergencectrl`` operand; bundle at a call-site. An optimizer may remove the ``convergent`` attribute and any explicit; ``convergencectrl`` operand bundle from a call-site if it can prove; that the execution of this call-site always results in a call to a; non-convergent function. An optimizer may remove the ``convergent`` attribute on a function if it can; prove that the function does not contain a call to; :ref:`llvm.experimental.convergence.entry; <llvm.experimental.convergence.entry>`, or any uncontrolled convergent; operations. Memory Model Non-Interaction; ============================. The fact that an operation is convergent has no effect on how it is treated for; memory model purposes. In particular, an operation that is ``convergent`` and; ``readnone`` does not introduce additional ordering constraints as far as the; memory model is concerned. There is no implied barrier, neither in the memory; barrier sense nor in the control barrier sense of synchronizing the execution; of threads. Informational note: Threads that execute converged dynamic instances do not; necessarily do so at the same time. Other Interactions; ==================. A fu",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ConvergentOperations.rst:33643,optimiz,optimizer,33643,interpreter/llvm-project/llvm/docs/ConvergentOperations.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ConvergentOperations.rst,1,['optimiz'],['optimizer']
Performance,"rger and requests the runtime to increase the queue's scratch; size if necessary. CP directly loads from the kernel dispatch packet Private Segment Byte Size; field and rounds up to a multiple of DWORD. Having CP load it once avoids; loading it at the beginning of every wavefront. The kernel prolog code must move it to FLAT_SCRATCH_LO which is SGPRn-3 on; GFX7 and SGPRn-5 on GFX8. FLAT_SCRATCH_LO is used as the FLAT SCRATCH SIZE; in flat memory instructions. * If the *Target Properties* column of :ref:`amdgpu-processor-table`; specifies *Absolute flat scratch*:. If the kernel or any function it calls may use flat operations to access; scratch memory, the prolog code must set up the FLAT_SCRATCH register pair; (FLAT_SCRATCH_LO/FLAT_SCRATCH_HI which are in SGPRn-4/SGPRn-3). Initialization; uses Flat Scratch Init and Scratch Wavefront Offset SGPR registers (see; :ref:`amdgpu-amdhsa-initial-kernel-execution-state`):. The Flat Scratch Init is the 64-bit address of the base of scratch backing; memory being managed by SPI for the queue executing the kernel dispatch. CP obtains this from the runtime. The kernel prolog must add the value of the wave's Scratch Wavefront Offset; and move the result as a 64-bit value to the FLAT_SCRATCH SGPR register pair; which is SGPRn-6 and SGPRn-5. It is used as the FLAT SCRATCH BASE in flat; memory instructions. The Scratch Wavefront Offset must also be used as an offset with Private; segment address when using the Scratch Segment Buffer (see; :ref:`amdgpu-amdhsa-kernel-prolog-private-segment-buffer`). * If the *Target Properties* column of :ref:`amdgpu-processor-table`; specifies *Architected flat scratch*:. If ENABLE_PRIVATE_SEGMENT is enabled in; :ref:`amdgpu-amdhsa-compute_pgm_rsrc2-gfx6-gfx12-table` then the FLAT_SCRATCH; register pair will be initialized to the 64-bit address of the base of scratch; backing memory being managed by SPI for the queue executing the kernel; dispatch plus the value of the wave's Scratch Wavefront Offset fo",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:196993,queue,queue,196993,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['queue'],['queue']
Performance,"rget; specific restriction are not met and a hardware-loop can't be generated. These intrinsics may be modified in the future and are not intended to be used; outside the backend. Thus, front-end and mid-level optimizations should not be; generating these intrinsics. '``llvm.set.loop.iterations.*``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". This is an overloaded intrinsic. ::. declare void @llvm.set.loop.iterations.i32(i32); declare void @llvm.set.loop.iterations.i64(i64). Overview:; """""""""""""""""". The '``llvm.set.loop.iterations.*``' intrinsics are used to specify the; hardware-loop trip count. They are placed in the loop preheader basic block and; are marked as ``IntrNoDuplicate`` to avoid optimizers duplicating these; instructions. Arguments:; """""""""""""""""""". The integer operand is the loop trip count of the hardware-loop, and thus; not e.g. the loop back-edge taken count. Semantics:; """""""""""""""""""". The '``llvm.set.loop.iterations.*``' intrinsics do not perform any arithmetic; on their operand. It's a hint to the backend that can use this to set up the; hardware-loop count with a target specific instruction, usually a move of this; value to a special register or a hardware-loop instruction. '``llvm.start.loop.iterations.*``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". This is an overloaded intrinsic. ::. declare i32 @llvm.start.loop.iterations.i32(i32); declare i64 @llvm.start.loop.iterations.i64(i64). Overview:; """""""""""""""""". The '``llvm.start.loop.iterations.*``' intrinsics are similar to the; '``llvm.set.loop.iterations.*``' intrinsics, used to specify the; hardware-loop trip count but also produce a value identical to the input; that can be used as the input to the loop. They are placed in the loop; preheader basic block and the output is expected to be the input to the; phi for the induction variable of the loop, decremented by the; '``llvm.loop.decrement.reg.*``'. Arguments:; """""""""""""""""""". The integer operand is",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:643810,perform,perform,643810,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['perform']
Performance,"rgument ``vec`` are vectors with the same type.; The second argument ``mask`` is a vector mask and has the same number of; elements as the result. The third argument is the explicit vector length of; the operation. Semantics:; """""""""""""""""""". This intrinsic reverses the order of the first ``evl`` elements in a vector.; The lanes in the result vector disabled by ``mask`` are ``poison``. The; elements past ``evl`` are poison. .. _int_vp_load:. '``llvm.vp.load``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <4 x float> @llvm.vp.load.v4f32.p0(ptr %ptr, <4 x i1> %mask, i32 %evl); declare <vscale x 2 x i16> @llvm.vp.load.nxv2i16.p0(ptr %ptr, <vscale x 2 x i1> %mask, i32 %evl); declare <8 x float> @llvm.vp.load.v8f32.p1(ptr addrspace(1) %ptr, <8 x i1> %mask, i32 %evl); declare <vscale x 1 x i64> @llvm.vp.load.nxv1i64.p6(ptr addrspace(6) %ptr, <vscale x 1 x i1> %mask, i32 %evl). Overview:; """""""""""""""""". The '``llvm.vp.load.*``' intrinsic is the vector length predicated version of; the :ref:`llvm.masked.load <int_mload>` intrinsic. Arguments:; """""""""""""""""""". The first operand is the base pointer for the load. The second operand is a; vector of boolean values with the same number of elements as the return type.; The third is the explicit vector length of the operation. The return type and; underlying type of the base pointer are the same vector types. The :ref:`align <attr_align>` parameter attribute can be provided for the first; operand. Semantics:; """""""""""""""""""". The '``llvm.vp.load``' intrinsic reads a vector from memory in the same way as; the '``llvm.masked.load``' intrinsic, where the mask is taken from the; combination of the '``mask``' and '``evl``' operands in the usual VP way.; Certain '``llvm.masked.load``' operands do not have corresponding operands in; '``llvm.vp.load``': the '``passthru``' operand is implicitly ``poison``; the; '``alignment``' operand is taken as the ``align`` parameter attribut",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:783447,load,load,783447,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,"rguments. The design of the ROOTMAP infrastructure; requires the default arguments to be attached to more than one declaration which; is not allowed by standard C++. The behavior of line #1 is equivalent to:; ```cpp; // ROOT prompt; root [] namespace foo { };struct S;; root [] S *s;; ```. Line #2 does not require a definition, however, the second identifier lookup; fails. The implementation knows that `foo::bar` is in *libFoo*. It `dlopen`s; libFoo which in turn, during its static initialization, inserts annotated forward; declaration as shown in `G__Foo.cxx`. In turn, this resolves `foo::bar` and; parsing of `Foo.h` is again avoided at relatively small overhead. However, this; is very hard to measure because the dictionary of each library can have different; amount of content. In the case where the library is big and the annotated; forward declarations are many, and we want to include a relatively small header; file it may not pay off. Moreover, the loading of the annotated forward; declarations can happen at any time during parsing. This is nick-named; ""recursive parsing"" and is a code path that exists only in ROOT, never exercised; by clang itself and is thus not well tested. The behavior of line #2 is; equivalent to:; ```cpp; // ROOT prompt; root [] namespace foo { };struct S;; root [] foo::bar/*store parsing state*/; gSystem->Load(""Foo"");; // More scaffolding.; extern int __Cling_AutoLoading_Map;; namespace foo{struct __attribute__((annotate(""$clingAutoload$Foo.h""))) bar;}; struct __attribute__((annotate(""$clingAutoload$Foo.h""))) S;; // More initialization scaffolding.; /*restore parsing state*/ *baz1;; ```. Line #3 requires a definition and the implementation behaves exactly as in #2.; Then it is informed that a definition is required, it reads the information in; the annotation and parses `Foo.h`. The recursive parsing happens at two places; making this code path error prone.; ```cpp; // ROOT prompt; root [] namespace foo { };struct S;; root [] foo::bar/*stor",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/README.CXXMODULES.md:9391,load,loading,9391,README/README.CXXMODULES.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/README.CXXMODULES.md,1,['load'],['loading']
Performance,"ribution of LLVM; ===============================. .. contents::; :local:. Introduction; ============. This document is geared toward people who want to build and package LLVM and any; combination of LLVM sub-project tools for distribution. This document covers; useful features of the LLVM build system as well as best practices and general; information about packaging LLVM. If you are new to CMake you may find the :doc:`CMake` or :doc:`CMakePrimer`; documentation useful. Some of the things covered in this document are the inner; workings of the builds described in the :doc:`AdvancedBuilds` document. General Distribution Guidance; =============================. When building a distribution of a compiler it is generally advised to perform a; bootstrap build of the compiler. That means building a ""stage 1"" compiler with; your host toolchain, then building the ""stage 2"" compiler using the ""stage 1""; compiler. This is done so that the compiler you distribute benefits from all the; bug fixes, performance optimizations and general improvements provided by the; new compiler. In deciding how to build your distribution there are a few trade-offs that you; will need to evaluate. The big two are:. #. Compile time of the distribution against performance of the built compiler. #. Binary size of the distribution against performance of the built compiler. The guidance for maximizing performance of the generated compiler is to use LTO,; PGO, and statically link everything. This will result in an overall larger; distribution, and it will take longer to generate, but it provides the most; opportunity for the compiler to optimize. The guidance for minimizing distribution size is to dynamically link LLVM and; Clang libraries into the tools to reduce code duplication. This will come at a; substantial performance penalty to the generated binary both because it reduces; optimization opportunity, and because dynamic linking requires resolving symbols; at process launch time, which can be ver",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BuildingADistribution.rst:1050,perform,performance,1050,interpreter/llvm-project/llvm/docs/BuildingADistribution.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BuildingADistribution.rst,2,"['optimiz', 'perform']","['optimizations', 'performance']"
Performance,"ric glc=1; load atomic monotonic - system - global 1. buffer/global/flat_load; - generic glc=1; store atomic monotonic - singlethread - global 1. buffer/global/flat_store; - wavefront - generic; - workgroup; - agent; store atomic monotonic - system - global 1. buffer/global/flat_store; - generic; store atomic monotonic - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; - workgroup be used.*. 1. ds_store; atomicrmw monotonic - singlethread - global 1. buffer/global/flat_atomic; - wavefront - generic; - workgroup; - agent; atomicrmw monotonic - system - global 1. buffer/global/flat_atomic; - generic; atomicrmw monotonic - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; - workgroup be used.*. 1. ds_atomic; **Acquire Atomic**; ------------------------------------------------------------------------------------; load atomic acquire - singlethread - global 1. buffer/global/ds/flat_load; - wavefront - local; - generic; load atomic acquire - workgroup - global 1. buffer/global_load glc=1. - If not TgSplit execution; mode, omit glc=1. 2. s_waitcnt vmcnt(0). - If not TgSplit execution; mode, omit.; - Must happen before the; following buffer_wbinvl1_vol. 3. buffer_wbinvl1_vol. - If not TgSplit execution; mode, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale data. load atomic acquire - workgroup - local *If TgSplit execution mode,; local address space cannot; be used.*. 1. ds_load; 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than the local load; atomic value being; acquired. load atomic acquire - workgroup - generic 1. flat_load glc=1. - If not TgSplit execution; mode, omit glc=1. 2. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) i",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:245295,load,load,245295,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"ric; - workgroup; - agent; - system; atomicrmw monotonic - singlethread - local 1. ds_atomic; - wavefront; - workgroup; **Acquire Atomic**; ------------------------------------------------------------------------------------; load atomic acquire - singlethread - global 1. buffer/global/ds/flat_load; - wavefront - local; - generic; load atomic acquire - workgroup - global 1. buffer/global_load; load atomic acquire - workgroup - local 1. ds/flat_load; - generic 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local load; atomic value being; acquired. load atomic acquire - agent - global 1. buffer/global_load; - system glc=1; 2. s_waitcnt vmcnt(0). - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the load; has completed; before invalidating; the cache. 3. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale global data. load atomic acquire - agent - generic 1. flat_load glc=1; - system 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If OpenCL omit; lgkmcnt(0).; - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the flat_load; has completed; before invalidating; the cache. 3. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acquire - singlethread - global 1. buffer/global/ds/flat_atomic; - wavefront - local; - generic; atomicrmw acquire - workgroup - global 1. buffer/global_atomic; atomicrmw acquire - workgroup - local 1. ds/flat_atomic; - generic 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local; at",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:214880,load,load,214880,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"ric; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale global data. load atomic acquire - system - global 1. buffer/global/flat_load; sc0=1 sc1=1; 2. s_waitcnt vmcnt(0). - Must happen before; following; buffer_inv.; - Ensures the load; has completed; before invalidating; the cache. 3. buffer_inv sc0=1 sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale MTYPE NC global data.; MTYPE RW and CC memory will; never be stale due to the; memory probes. load atomic acquire - agent - generic 1. flat_load sc1=1; 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL omit; lgkmcnt(0).; - Must happen before; following; buffer_inv.; - Ensures the flat_load; has completed; before invalidating; the cache. 3. buffer_inv sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. load atomic acquire - system - generic 1. flat_load sc0=1 sc1=1; 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL omit; lgkmcnt(0).; - Must happen before; the following; buffer_inv.; - Ensures the flat_load; has completed; before invalidating; the caches. 3. buffer_inv sc0=1 sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale MTYPE NC global data.; MTYPE RW and CC memory will; never be stale due to the; memory probes. atomicrmw acquire - singlethread - global 1. buffer/global/flat_atomic; - wavefront - generic; atomicrmw acquire - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; be used.*. 1. ds_atomic; atomicrmw acquire - workgroup - global 1. buffer/global_atomic; 2. s_waitcnt vmcnt(0). - If not TgSplit execution; mode, omit.; - Must happen before the; following buffer_inv.",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:298275,load,loads,298275,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['loads']
Performance,"ric; store/atomicrmw are; visible at system scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global and L2 writeback; have completed before; performing the; atomicrmw that is; being released. 3. flat_atomic; 4. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Must happen before; following buffer_invl2 and; buffer_wbinvl1_vol.; - Ensures the; atomicrmw has; completed before; invalidating the; caches. 5. buffer_invl2;; buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale L1 global data,; nor see stale L2 MTYPE; NC global data.; MTYPE RW and CC memory will; never be stale in L2 due to; the memory probes. fence acq_rel - singlethread *none* *none*; - wavefront; fence acq_rel - workgroup *none* 1. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0).; - However,; since LLVM; currently has no; address space on; the fence need to; conservatively; always generate; (see comment for; previous fence).; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/; load atomic/store atomic/; atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:273862,cache,caches,273862,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['cache'],['caches']
Performance,"rical types, (; see issue in ROOT forum). When reading more than one TStreamerInfo for the same versioned; class, we now use the highest possible class version as the current; version of the class. Practically, we update the class version; when reading new (higher versioned) StreamerInfo until the Class; is actually used (i.e. TClass::GetClassVersion is call directly; or indirectly). In particular, if a file has several StreamerInfos for the same; versioned class, we will use the highest version number as the; 'current' class version (as opposed to the lowest until now). For backward compatibility TStreamerInfo::BuildCheck compares the checksum of; the on-file StreamerInfo not only to the current value of the class checksum; but also to the checksum calculated using the older algorithms. This patch extends this test to also be done when comparing 2 on-file StreamerInfos. This removes spurrious warning message when loading 2 older files which; were written with 2 different version of the TClass CheckSum algorithm; (and the in-memory class's version is greater than both TStreamerInfos'; class version). Extend support of TStreamerInfo::ReadValueAux to 'converted' numerical types, hence solving TTree::Draw's schema evolution problem (see http://root.cern/phpBB2/viewtopic.php?t=6225). DirectoryAutoAdd; Use the new DirectoryAutoAdd facility for the classes:; TTree, TH1, TEventList, TEntryList, TGraph2D; (and hence their derived classes). The instances of those classes are now added automatically; to the current directory only when Constructe'd with arguments or Clone'd; and to the directory they are read from when their are stored; directly in a TKey. [Note: the default constructor never adds; the object to the current directory]. The directory auto add can still be disabled for instance; of TH1 and TGraph2D by setting TH1::AddDirectory. Additionally one can disable the directory auto add for; a specific class by doing:. TClass::GetClass(""myclass"")->SetDirectoryAutoAdd(0)",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/io/doc/v520/index.html:2036,load,loading,2036,io/doc/v520/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/io/doc/v520/index.html,2,['load'],['loading']
Performance,"ries, implementing all the new and old minimization interface, include:. - \ref MinuitOld ""Minuit"": library providing via a class TMinuit an implementation of the popular MINUIT minimization package. In addition the library contains also an implementation of the linear fitter (class TLinearFitter), for solving linear least square fits.; - \ref Minuit2Page ""Minuit2"": new object-oriented implementation of MINUIT, with the same minimization algorithms (such as Migrad or Simplex). In addition it provides a new implementation of the Fumili algorithm, a specialized method for finding the minimum of a standard least square or likelihood functions.; - **Fumili**: library providing the implementation of the original Fumili fitting algorithm (class TFumili). - **Linear algebra**. Two libraries are contained in %ROOT for describing linear algebra matrices and vector classes:. - Matrix: general matrix package providing matrix classes (TMatrixD and TMatrixF) and vector classes (TVectorD and TVectorF) and the complete environment to perform linear algebra calculations, like equation solving and eigenvalue decompositions.; - \ref SMatrixPage ""SMatrix"": package optimized for high performances matrix and vector computations of small and fixed size. It is based on expression templates to achieve an high level optimization. - **Physics Vectors**: Classes for describing vectors in 2, 3 and 4 dimensions (relativistic vectors) and their rotation and transformation algorithms. Two package exist in %ROOT:. - Physics: library with the TVector3 and TLorentzVector classes.; - GenVector: new library providing generic class templates for modeling the vectors. See the \ref GenVector ""GenVector"" page. - \ref Unuran ""UNURAN"": Package with universal algorithms for generating non-uniform pseudo-random numbers, from a large classes of continuous or discrete distributions in one or multi-dimensions. - **Foam** Multi-dimensional general purpose Monte Carlo event generator (and integrator). It generates ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/math/doc/index.md:2738,perform,perform,2738,math/doc/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/math/doc/index.md,1,['perform'],['perform']
Performance,"rieties of garbage collectors. The Shadow Stack GC; ----------------------. To use this collector strategy, mark your functions with:. .. code-block:: c++. F.setGC(""shadow-stack"");. Unlike many GC algorithms which rely on a cooperative code generator to compile; stack maps, this algorithm carefully maintains a linked list of stack roots; [:ref:`Henderson2002 <henderson02>`]. This so-called ""shadow stack"" mirrors the; machine stack. Maintaining this data structure is slower than using a stack map; compiled into the executable as constant data, but has a significant portability; advantage because it requires no special support from the target code generator,; and does not require tricky platform-specific code to crawl the machine stack. The tradeoff for this simplicity and portability is:. * High overhead per function call. * Not thread-safe. Still, it's an easy way to get started. After your compiler and runtime are up; and running, writing a :ref:`plugin <plugin>` will allow you to take advantage; of :ref:`more advanced GC features <collector-algos>` of LLVM in order to; improve performance. The shadow stack doesn't imply a memory allocation algorithm. A semispace; collector or building atop ``malloc`` are great places to start, and can be; implemented with very little code. When it comes time to collect, however, your runtime needs to traverse the stack; roots, and for this it needs to integrate with the shadow stack. Luckily, doing; so is very simple. (This code is heavily commented to help you understand the; data structure, but there are only 20 lines of meaningful code.). .. code-block:: c++. /// The map for a single function's stack frame. One of these is; /// compiled as constant data into the executable for each function.; ///; /// Storage of metadata values is elided if the %metadata parameter to; /// @llvm.gcroot is null.; struct FrameMap {; int32_t NumRoots; //< Number of roots in stack frame.; int32_t NumMeta; //< Number of metadata entries. May be < Num",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GarbageCollection.rst:17037,perform,performance,17037,interpreter/llvm-project/llvm/docs/GarbageCollection.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GarbageCollection.rst,1,['perform'],['performance']
Performance,"ring about two questions. * When should something belong to a checker and when should something belong to the engine?; Sometimes we model library aspects in the engine and model language constructs in checkers. * What is the checker programming model that we are aiming for? Maximum freedom or more easy checker development?. I think if we aim for maximum freedom, we do not need to worry about the; potential stress on checkers, and we can introduce abstractions to mitigate that; later on.; If we want to simplify the API, then maybe it makes more sense to move language; construct modeling to the engine when the checker API is not sufficient instead; of complicating the API. Right now I have no preference or objections between the alternatives but there; are some random thoughts:. * Maybe it would be great to have a guideline how to evolve the analyzer and; follow it, so it can help us to decide in similar situations. * I do care about performance in this case. The reason is that we have a; limited performance budget. And I think we should not expect most of the checker; writers to add modeling of language constructs. So, in my opinion, it is ok to; have less nice/more verbose API for language modeling if we can have better; performance this way, since it only needs to be done once, and is done by the; framework developers. **Artem:** These are some great questions, i guess it'd be better to discuss; them more openly. As a quick dump of my current mood:. * To me it seems obvious that we need to aim for a checker API that is both; simple and powerful. This can probably by keeping the API as powerful as; necessary while providing a layer of simple ready-made solutions on top of it.; Probably a few reusable components for assembling checkers. And this layer; should ideally be pleasant enough to work with, so that people would prefer to; extend it when something is lacking, instead of falling back to the complex; omnipotent API. I'm thinking of AST matchers vs. AST visitors",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/analyzer/developer-docs/InitializerLists.rst:5685,perform,performance,5685,interpreter/llvm-project/clang/docs/analyzer/developer-docs/InitializerLists.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/analyzer/developer-docs/InitializerLists.rst,1,['perform'],['performance']
Performance,"ring compiler-compile time. For example, the integer ""``(not; x)``"" operation is actually defined as a pattern fragment that expands as; ""``(xor x, -1)``"", since the SelectionDAG does not have a native '``not``'; operation. Targets can define their own short-hand fragments as they see fit.; See the definition of '``not``' and '``ineg``' for examples. * In addition to instructions, targets can specify arbitrary patterns that map; to one or more instructions using the 'Pat' class. For example, the PowerPC; has no way to load an arbitrary integer immediate into a register in one; instruction. To tell tblgen how to do this, it defines:. ::. // Arbitrary immediate support. Implement in terms of LIS/ORI.; def : Pat<(i32 imm:$imm),; (ORI (LIS (HI16 imm:$imm)), (LO16 imm:$imm))>;. If none of the single-instruction patterns for loading an immediate into a; register match, this will be used. This rule says ""match an arbitrary i32; immediate, turning it into an ``ORI`` ('or a 16-bit immediate') and an ``LIS``; ('load 16-bit immediate, where the immediate is shifted to the left 16 bits'); instruction"". To make this work, the ``LO16``/``HI16`` node transformations; are used to manipulate the input immediate (in this case, take the high or low; 16-bits of the immediate). * When using the 'Pat' class to map a pattern to an instruction that has one; or more complex operands (like e.g. `X86 addressing mode`_), the pattern may; either specify the operand as a whole using a ``ComplexPattern``, or else it; may specify the components of the complex operand separately. The latter is; done e.g. for pre-increment instructions by the PowerPC back end:. ::. def STWU : DForm_1<37, (outs ptr_rc:$ea_res), (ins GPRC:$rS, memri:$dst),; ""stwu $rS, $dst"", LdStStoreUpd, []>,; RegConstraint<""$dst.reg = $ea_res"">, NoEncode<""$ea_res"">;. def : Pat<(pre_store GPRC:$rS, ptr_rc:$ptrreg, iaddroff:$ptroff),; (STWU GPRC:$rS, iaddroff:$ptroff, ptr_rc:$ptrreg)>;. Here, the pair of ``ptroff`` and ``ptrreg`` opera",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst:50887,load,load,50887,interpreter/llvm-project/llvm/docs/CodeGenerator.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst,1,['load'],['load']
Performance,"ring created by a ``seq_cst``; fence must be compatible with the individual total orders of; ``monotonic`` (or stronger) memory accesses occurring before and after; such a fence. The exact semantics of this interaction are somewhat; complicated, see the C++ standard's `[atomics.order]; <https://wg21.link/atomics.order>`_ section for more details. A ``fence`` instruction can also take an optional; "":ref:`syncscope <syncscope>`"" argument. Example:; """""""""""""""". .. code-block:: text. fence acquire ; yields void; fence syncscope(""singlethread"") seq_cst ; yields void; fence syncscope(""agent"") seq_cst ; yields void. .. _i_cmpxchg:. '``cmpxchg``' Instruction; ^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. cmpxchg [weak] [volatile] ptr <pointer>, <ty> <cmp>, <ty> <new> [syncscope(""<target-scope>"")] <success ordering> <failure ordering>[, align <alignment>] ; yields { ty, i1 }. Overview:; """""""""""""""""". The '``cmpxchg``' instruction is used to atomically modify memory. It; loads a value in memory and compares it to a given value. If they are; equal, it tries to store a new value into the memory. Arguments:; """""""""""""""""""". There are three arguments to the '``cmpxchg``' instruction: an address; to operate on, a value to compare to the value currently be at that; address, and a new value to place at that address if the compared values; are equal. The type of '<cmp>' must be an integer or pointer type whose; bit width is a power of two greater than or equal to eight and less; than or equal to a target-specific size limit. '<cmp>' and '<new>' must; have the same type, and the type of '<pointer>' must be a pointer to; that type. If the ``cmpxchg`` is marked as ``volatile``, then the; optimizer is not allowed to modify the number or order of execution of; this ``cmpxchg`` with other :ref:`volatile operations <volatile>`. The success and failure :ref:`ordering <ordering>` arguments specify how this; ``cmpxchg`` synchronizes with other atomic operations. Both ordering parameters; must be at ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:425874,load,loads,425874,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['loads']
Performance,"ring. The; ``module`` element defining a module ID must always be emitted before any; other elements that refer to that module ID, so that a filter never needs to; keep track of dangling references. The second ``%s`` is the module type and it; determines what the remaining fields are. The following module types are; supported:. * ``elf:%x``. Here ``%x`` encodes an ELF Build ID. The Build ID should refer to a single; linked binary. The Build ID string is the sole way to identify the binary from; which this module was loaded. Example::. {{{module:1:libc.so:elf:83238ab56ba10497}}}. ``{{{mmap:%p:%i:...}}}``. This contextual element is used to give information about a particular region; in memory. ``%p`` is the starting address and ``%i`` gives the size in hex of the; region of memory. The ``...`` part can take different forms to give different; information about the specified region of memory. The allowed forms are the; following:. * ``load:%i:%s:%p``. This subelement informs the filter that a segment was loaded from a module.; The module is identified by its module ID ``%i``. The ``%s`` is one or more of; the letters 'r', 'w', and 'x' (in that order and in either upper or lower; case) to indicate this segment of memory is readable, writable, and/or; executable. The symbolizing filter can use this information to guess whether; an address is a likely code address or a likely data address in the given; module. The remaining ``%p`` gives the module relative address. For ELF files; the module relative address will be the ``p_vaddr`` of the associated program; header. For example if your module's executable segment has; ``p_vaddr=0x1000``, ``p_memsz=0x1234``, and was loaded at ``0x7acba69d5000``; then you need to subtract ``0x7acba69d4000`` from any address between; ``0x7acba69d5000`` and ``0x7acba69d6234`` to get the module relative address.; The starting address will usually have been rounded down to the active page; size, and the size rounded up. Example::. {{{mmap:0x7acb",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SymbolizerMarkupFormat.rst:20523,load,loaded,20523,interpreter/llvm-project/llvm/docs/SymbolizerMarkupFormat.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SymbolizerMarkupFormat.rst,1,['load'],['loaded']
Performance,"rinsic returns the; address of the given thread local global in the calling thread. .. _int_vscale:. '``llvm.vscale``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare i32 llvm.vscale.i32(); declare i64 llvm.vscale.i64(). Overview:; """""""""""""""""". The ``llvm.vscale`` intrinsic returns the value for ``vscale`` in scalable; vectors such as ``<vscale x 16 x i8>``. Semantics:; """""""""""""""""""". ``vscale`` is a positive value that is constant throughout program; execution, but is unknown at compile time.; If the result value does not fit in the result type, then the result is; a :ref:`poison value <poisonvalues>`. Stack Map Intrinsics; --------------------. LLVM provides experimental intrinsics to support runtime patching; mechanisms commonly desired in dynamic language JITs. These intrinsics; are described in :doc:`StackMaps`. Element Wise Atomic Memory Intrinsics; -------------------------------------. These intrinsics are similar to the standard library memory intrinsics except; that they perform memory transfer as a sequence of atomic memory accesses. .. _int_memcpy_element_unordered_atomic:. '``llvm.memcpy.element.unordered.atomic``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". This is an overloaded intrinsic. You can use ``llvm.memcpy.element.unordered.atomic`` on; any integer bit width and for different address spaces. Not all targets; support all bit widths however. ::. declare void @llvm.memcpy.element.unordered.atomic.p0.p0.i32(ptr <dest>,; ptr <src>,; i32 <len>,; i32 <element_size>); declare void @llvm.memcpy.element.unordered.atomic.p0.p0.i64(ptr <dest>,; ptr <src>,; i64 <len>,; i32 <element_size>). Overview:; """""""""""""""""". The '``llvm.memcpy.element.unordered.atomic.*``' intrinsic is a specialization of the; '``llvm.memcpy.*``' intrinsic. It differs in that the ``dest`` and ``src`` are treated; as arrays with elements that are exactly ``element_size`` bytes, and the copy between; buffers uses a sequence of :ref:`",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:956931,perform,perform,956931,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['perform']
Performance,"ripped. In the; following, we have some parameters, like tick marks length and; characters height (in percentage of the length of the axis, in user; coordinates). The default values are as follows:. - Primary tick marks: 3.0 %. - Secondary tick marks: 1.5 %. - Third order tick marks: .75 %. - Characters height for labels: 4%. - Labels offset: 1.0 %. #### Stripping Decimals. Use the `TStyle::SetStripDecimals` to strip decimals when drawing axis; labels. By default, the option is set to true, and `TGaxis::PaintAxis`; removes trailing zeros after the dot in the axis labels, e.g. {0, 0.5,; 1, 1.5, 2, 2.5, etc.}. ``` {.cpp}; TStyle::SetStripDecimals (Bool_t strip=kTRUE); ```. If this function is called with `strip=kFALSE`, `TGaxis::PaintAxis()`; will draw labels with the same number of digits after the dot, e.g.; {0.0, 0.5, 1.0, 1.5, 2.0, 2.5, etc.}. #### Optional Grid. `chopt = 'W'`: cross-Wire. #### Axis Binning Optimization. By default, the axis binning is optimized. - `chopt = 'N'`: No binning optimization. - `chopt = 'I'`: Integer labeling. ### Axis with Time Units. Histograms' axis can be defined as ""time axis"". To do that it is enough; to activate the `SetTimeDisplay` attribute on a given axis. If `h` is a; histogram, it is done the following way:. ``` {.cpp}; h->GetXaxis()->SetTimeDisplay(1); // X axis is a time axis; ```. Two parameters can be adjusted in order to define time axis: the time; format and the time offset. #### Time Format. It defines the format of the labels along the time axis. It can be; changed using the **`TAxis`** method `SetTimeFormat`. The time format is; the one used by the C function `strftime()`. It is a string containing; the following formatting characters:. +-----------------+----------------------------------------------------------+; | For the date: | %a abbreviated weekday name |; | | |; | | %b abbreviated month name |; | | |; | | %d day of the month (01-31) |; | | |; | | %m month (01-12) |; | | |; | | %y year without century |; | |",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Graphics.md:58215,optimiz,optimization,58215,documentation/users-guide/Graphics.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Graphics.md,1,['optimiz'],['optimization']
Performance,"ripts to set complex settings, and specifically to; make later stage builds include cache scripts is common in our more advanced; build configurations. Multi-stage PGO; ===============. Profile-Guided Optimizations (PGO) is a really great way to optimize the code; clang generates. Our multi-stage PGO builds are a workflow for generating PGO; profiles that can be used to optimize clang. At a high level, the way PGO works is that you build an instrumented compiler,; then you run the instrumented compiler against sample source files. While the; instrumented compiler runs it will output a bunch of files containing; performance counters (.profraw files). After generating all the profraw files; you use llvm-profdata to merge the files into a single profdata file that you; can feed into the LLVM_PROFDATA_FILE option. Our PGO.cmake cache automates that whole process. You can use it for; configuration with CMake with the following command:. .. code-block:: console. $ cmake -G Ninja -C <path to source>/clang/cmake/caches/PGO.cmake \; <path to source>/llvm. There are several additional options that the cache file also accepts to modify; the build, particularly the PGO_INSTRUMENT_LTO option. Setting this option to; Thin or Full will enable ThinLTO or full LTO respectively, further enhancing; the performance gains from a PGO build by enabling interprocedural; optimizations. For example, to run a CMake configuration for a PGO build; that also enables ThinTLO, use the following command:. .. code-block:: console. $ cmake -G Ninja -C <path to source>/clang/cmake/caches/PGO.cmake \; -DPGO_INSTRUMENT_LTO=Thin \; <path to source>/llvm. By default, clang will generate profile data by compiling a simple; hello world program. You can also tell clang use an external; project for generating profile data that may be a better fit for your; use case. The project you specify must either be a lit test suite; (use the CLANG_PGO_TRAINING_DATA option) or a CMake project (use the; CLANG_PERF_TRAINING",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AdvancedBuilds.rst:5572,cache,caches,5572,interpreter/llvm-project/llvm/docs/AdvancedBuilds.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AdvancedBuilds.rst,1,['cache'],['caches']
Performance,"rite code for a subclass of ``AsmPrinter`` that performs the; LLVM-to-assembly conversion and a trivial subclass of ``TargetAsmInfo``. * Optionally, add support for subtargets (i.e., variants with different; capabilities). You should also write code for a subclass of the; ``TargetSubtarget`` class, which allows you to use the ``-mcpu=`` and; ``-mattr=`` command-line options. * Optionally, add JIT support and create a machine code emitter (subclass of; ``TargetJITInfo``) that is used to emit binary code directly into memory. In the ``.cpp`` and ``.h``. files, initially stub up these methods and then; implement them later. Initially, you may not know which private members that; the class will need and which components will need to be subclassed. Preliminaries; -------------. To actually create your compiler backend, you need to create and modify a few; files. The absolute minimum is discussed here. But to actually use the LLVM; target-independent code generator, you must perform the steps described in the; :doc:`LLVM Target-Independent Code Generator <CodeGenerator>` document. First, you should create a subdirectory under ``lib/Target`` to hold all the; files related to your target. If your target is called ""Dummy"", create the; directory ``lib/Target/Dummy``. In this new directory, create a ``CMakeLists.txt``. It is easiest to copy a; ``CMakeLists.txt`` of another target and modify it. It should at least contain; the ``LLVM_TARGET_DEFINITIONS`` variable. The library can be named ``LLVMDummy``; (for example, see the MIPS target). Alternatively, you can split the library; into ``LLVMDummyCodeGen`` and ``LLVMDummyAsmPrinter``, the latter of which; should be implemented in a subdirectory below ``lib/Target/Dummy`` (for example,; see the PowerPC target). Note that these two naming schemes are hardcoded into ``llvm-config``. Using; any other naming scheme will confuse ``llvm-config`` and produce a lot of; (seemingly unrelated) linker errors when linking ``llc``. To make you",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMBackend.rst:6092,perform,perform,6092,interpreter/llvm-project/llvm/docs/WritingAnLLVMBackend.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMBackend.rst,1,['perform'],['perform']
Performance,"ritten by vmulps is immediately used by the first; vhaddps, and register %xmm3 written by the first vhaddps is used by the second; vhaddps. Long data dependencies negatively impact the ILP (Instruction Level; Parallelism). In the dot-product example, there are anti-dependencies introduced by; instructions from different iterations. However, those dependencies can be; removed at register renaming stage (at the cost of allocating register aliases,; and therefore consuming physical registers). Table *Average Wait times* helps diagnose performance issues that are caused by; the presence of long latency instructions and potentially long data dependencies; which may limit the ILP. Last row, ``<total>``, shows a global average over all; instructions measured. Note that :program:`llvm-mca`, by default, assumes at; least 1cy between the dispatch event and the issue event. When the performance is limited by data dependencies and/or long latency; instructions, the number of cycles spent while in the *ready* state is expected; to be very small when compared with the total number of cycles spent in the; scheduler's queue. The difference between the two counters is a good indicator; of how large of an impact data dependencies had on the execution of the; instructions. When performance is mostly limited by the lack of hardware; resources, the delta between the two counters is small. However, the number of; cycles spent in the queue tends to be larger (i.e., more than 1-3cy),; especially when compared to other low latency instructions. Bottleneck Analysis; ^^^^^^^^^^^^^^^^^^^; The ``-bottleneck-analysis`` command line option enables the analysis of; performance bottlenecks. This analysis is potentially expensive. It attempts to correlate increases in; backend pressure (caused by pipeline resource pressure and data dependencies) to; dynamic dispatch stalls. Below is an example of ``-bottleneck-analysis`` output generated by; :program:`llvm-mca` for 500 iterations of the dot-product e",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst:25829,perform,performance,25829,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,3,"['latency', 'perform', 'queue']","['latency', 'performance', 'queue']"
Performance,"rk written entirely by you; rather, the intent is to; exercise the right to control the distribution of derivative or; collective works based on the Program. In addition, mere aggregation of another work not based on the Program; with the Program (or with a work based on the Program) on a volume of; a storage or distribution medium does not bring the other work under; the scope of this License. 3. You may copy and distribute the Program (or a work based on it,; under Section 2) in object code or executable form under the terms of; Sections 1 and 2 above provided that you also do one of the following:. a) Accompany it with the complete corresponding machine-readable; source code, which must be distributed under the terms of Sections; 1 and 2 above on a medium customarily used for software interchange; or,. b) Accompany it with a written offer, valid for at least three; years, to give any third party, for a charge no more than your; cost of physically performing source distribution, a complete; machine-readable copy of the corresponding source code, to be; distributed under the terms of Sections 1 and 2 above on a medium; customarily used for software interchange; or,. c) Accompany it with the information you received as to the offer; to distribute corresponding source code. (This alternative is; allowed only for noncommercial distribution and only if you; received the program in object code or executable form with such; an offer, in accord with Subsection b above.). The source code for a work means the preferred form of the work for; making modifications to it. For an executable work, complete source; code means all the source code for all modules it contains, plus any; associated interface definition files, plus the scripts used to; control compilation and installation of the executable. However, as a; special exception, the source code distributed need not include; anything that is normally distributed (in either source or binary; form) with the major components (co",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/misc/rootql/LICENSE.txt:7438,perform,performing,7438,misc/rootql/LICENSE.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/misc/rootql/LICENSE.txt,4,['perform'],['performing']
Performance,"rk` (OFF by default). ## Histogram Libraries. ## Math Libraries. - Update the definitions of the physical constants using the recommended 2018 values from NIST.; - Use also the new SI definition of base units from 2019, where the Planck constant, the Boltzmann constant, the elementary electric charge and the Avogadro constant are exact numerical values. See <https://en.wikipedia.org/wiki/2019_redefinition_of_the_SI_base_units>. Note that with this new definition the functions `TMath::HUncertainty()`, `TMath::KUncertainty()`, `TMath::QeUncertainty()` and `TMath::NaUncertainty()` all return a `0.0` value.; - Due to some planned major improvements to `RVec`, the layout of `RVec` objects will change in a backward-incompatible way between v6.24 and v6.26.; Because of this, we now print a warning if an application is reading or writing a `ROOT::RVec` object from/to a ROOT file. We assume this is an; exceedingly rare case, as the ROOT interface typically used to manipulate `RVec`s is `RDataFrame`, and `RDataFrame` performs an on-the-fly; `RVec <-> std::vector` conversion rather than writing `RVec`s to disk. Note that, currently, `RVecs` written e.g. in a `TTree` cannot be read back; using certain ROOT interfaces (e.g. `TTreeReaderArray`, `RDataFrame` and the experimental `RNTuple`). All these limitations will be lifted in v6.26.; - Portable implementation of the RANLUX++ generator, see [RanluxppEngine](https://root.cern/doc/master/classROOT_1_1Math_1_1RanluxppEngine.html) and [our blog post](https://root.cern/blog/ranluxpp/). ## TMVA. - Introducing TMVA PyTorch Interface, a method to use PyTorch internally with TMVA for deep learning. This can be used as an alternative to PyKeras Interface for complex models providing more flexibility and power. ## RooFit Libraries. - Extension / updates of the doxygen reference guide.; - Allow for removing RooPlot from global directory management, see [RooPlot::AddDirectory](https://root.cern/doc/v624/classRooPlot.html#a47f7ba71dcaca30ad9e",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v624/index.md:13095,perform,performs,13095,README/ReleaseNotes/v624/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v624/index.md,1,['perform'],['performs']
Performance,"rld:. ~~~{.cpp}; root[] top->SetLineColor(kMagenta);; root[] gGeoManager->SetTopVisible(); // the TOP is invisible; root[] top->Draw();; ~~~. \anchor GP00b; ### Example 2: A Geometrical Hierarchy Look and Feel. Before going further, let us get a look and feel of interacting with the; modeller. For this, we will use one of the examples illustrating the; geometry package. To get an idea on the geometry structure created in; this example, just look at rootgeom.C. You will; notice that this is a bit more complex that just creating the ""world""; since several other volumes are created and put together in a hierarchy.; The purpose here is just to learn how to interact with a geometry that; is already built, but just few hints on the building steps in this; example might be useful. The geometry here represents the word %ROOT that; is replicated in some symmetric manner. You might for instance ask some; questions after having a first look:. **Q:** ""OK, I understand the first lines that load the libGeom library and create; a geometry manager object. I also recognize from the previous example the following; lines creating some materials and media, but what about the geometrical transformations below?"". **A:** As explained before, the model that we are trying to create; is a hierarchy of volumes based on ""containment"". This is; accomplished by ""positioning"" some volumes ""inside"" others.; Any volume is an un-positioned object in the sense that it defines only; a ""local frame"" (matching the one of its ""shape""). In order; to fully define the mother-daughter relationship between two volumes one; has to specify how the daughter will be positioned inside. This is; accomplished by defining a ""local geometrical transformation"" of; the daughter with respect to the mother coordinate system. These; transformations will be subsequently used in the example. **Q:** ""I see the lines defining the top level volume as in the previous example,; but what about the other volumes named REPLICA and R",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md:7845,load,load,7845,geom/geom/doc/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md,1,['load'],['load']
Performance,"rm default will be used. .. option:: -ansi. Same as -std=c89. .. option:: -ObjC, -ObjC++. Treat source input files as Objective-C and Object-C++ inputs respectively. .. option:: -trigraphs. Enable trigraphs. .. option:: -ffreestanding. Indicate that the file should be compiled for a freestanding, not a hosted,; environment. Note that it is assumed that a freestanding environment will; additionally provide `memcpy`, `memmove`, `memset` and `memcmp`; implementations, as these are needed for efficient codegen for many programs. .. option:: -fno-builtin. Disable special handling and optimizations of well-known library functions,; like :c:func:`strlen` and :c:func:`malloc`. .. option:: -fno-builtin-<function>. Disable special handling and optimizations for the specific library function.; For example, ``-fno-builtin-strlen`` removes any special handling for the; :c:func:`strlen` library function. .. option:: -fno-builtin-std-<function>. Disable special handling and optimizations for the specific C++ standard; library function in namespace ``std``. For example,; ``-fno-builtin-std-move_if_noexcept`` removes any special handling for the; :cpp:func:`std::move_if_noexcept` library function. For C standard library functions that the C++ standard library also provides; in namespace ``std``, use :option:`-fno-builtin-\<function\>` instead. .. option:: -fmath-errno. Indicate that math functions should be treated as updating :c:data:`errno`. .. option:: -fpascal-strings. Enable support for Pascal-style strings with ""\\pfoo"". .. option:: -fms-extensions. Enable support for Microsoft extensions. .. option:: -fmsc-version=. Set ``_MSC_VER``. When on Windows, this defaults to either the same value as; the currently installed version of cl.exe, or ``1933``. Not set otherwise. .. option:: -fborland-extensions. Enable support for Borland extensions. .. option:: -fwritable-strings. Make all string literals default to writable. This disables uniquing of; strings and other optimizations. .. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/CommandGuide/clang.rst:6662,optimiz,optimizations,6662,interpreter/llvm-project/clang/docs/CommandGuide/clang.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/CommandGuide/clang.rst,1,['optimiz'],['optimizations']
Performance,"rm v16i8:$XA, v16i8:$XB)); (set v16i8:$XT, (int_ppc_vsx_xxpermr v16i8:$XA, v16i8:$XB)). - Vector Splat Immediate Byte: xxspltib; . Similar to XXSPLTW:; def XXSPLTW : XX2Form_2<60, 164,; (outs vsrc:$XT), (ins vsrc:$XB, u2imm:$UIM),; ""xxspltw $XT, $XB, $UIM"", IIC_VecPerm, []>;. . No SDAG, intrinsic, builtin are required?. - Load/Store Vector: lxv stxv; . Has likely SDAG match:; (set v?:$XT, (load ix16addr:$src)); (set v?:$XT, (store ix16addr:$dst)). . Need define ix16addr in PPCInstrInfo.td; ix16addr: 16-byte aligned, see ""def memrix16"" in PPCInstrInfo.td. - Load/Store Vector Indexed: lxvx stxvx; . Has likely SDAG match:; (set v?:$XT, (load xoaddr:$src)); (set v?:$XT, (store xoaddr:$dst)). - Load/Store DWord: lxsd stxsd; . Similar to lxsdx/stxsdx:; def LXSDX : XX1Form<31, 588,; (outs vsfrc:$XT), (ins memrr:$src),; ""lxsdx $XT, $src"", IIC_LdStLFD,; [(set f64:$XT, (load xoaddr:$src))]>;. . (set f64:$XT, (load iaddrX4:$src)); (set f64:$XT, (store iaddrX4:$dst)). - Load/Store SP, with conversion from/to DP: lxssp stxssp; . Similar to lxsspx/stxsspx:; def LXSSPX : XX1Form<31, 524, (outs vssrc:$XT), (ins memrr:$src),; ""lxsspx $XT, $src"", IIC_LdStLFD,; [(set f32:$XT, (load xoaddr:$src))]>;. . (set f32:$XT, (load iaddrX4:$src)); (set f32:$XT, (store iaddrX4:$dst)). - Load as Integer Byte/Halfword & Zero Indexed: lxsibzx lxsihzx; . Similar to lxsiwzx:; def LXSIWZX : XX1Form<31, 12, (outs vsfrc:$XT), (ins memrr:$src),; ""lxsiwzx $XT, $src"", IIC_LdStLFD,; [(set f64:$XT, (PPClfiwzx xoaddr:$src))]>;. . (set f64:$XT, (PPClfiwzx xoaddr:$src)). - Store as Integer Byte/Halfword Indexed: stxsibx stxsihx; . Similar to stxsiwx:; def STXSIWX : XX1Form<31, 140, (outs), (ins vsfrc:$XT, memrr:$dst),; ""stxsiwx $XT, $dst"", IIC_LdStSTFD,; [(PPCstfiwx f64:$XT, xoaddr:$dst)]>;. . (PPCstfiwx f64:$XT, xoaddr:$dst). - Load Vector Halfword*8/Byte*16 Indexed: lxvh8x lxvb16x; . Similar to lxvd2x/lxvw4x:; def LXVD2X : XX1Form<31, 844,; (outs vsrc:$XT), (ins memrr:$src),; ""lxvd2x $XT, $src"", IIC_LdStLFD,; [",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt:17945,Load,Load,17945,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt,1,['Load'],['Load']
Performance,"rmat of the generated file can be generated in one of three ways:. .. option:: --binary (default). Emit the profile using a binary encoding. For instrumentation-based profile; the output format is the indexed binary format. .. option:: --extbinary. Emit the profile using an extensible binary encoding. This option can only; be used with sample-based profile. The extensible binary encoding can be; more compact with compression enabled and can be loaded faster than the; default binary encoding. .. option:: --text. Emit the profile in text mode. This option can also be used with both; sample-based and instrumentation-based profile. When this option is used; the profile will be dumped in the text format that is parsable by the profile; reader. .. option:: --gcc. Emit the profile using GCC's gcov format (Not yet supported). .. option:: --sparse[=true|false]. Do not emit function records with 0 execution count. Can only be used in; conjunction with -instr. Defaults to false, since it can inhibit compiler; optimization during PGO. .. option:: --num-threads=<N>, -j. Use N threads to perform profile merging. When N=0, llvm-profdata auto-detects; an appropriate number of threads to use. This is the default. .. option:: --failure-mode=[any|all]. Set the failure mode. There are two options: 'any' causes the merge command to; fail if any profiles are invalid, and 'all' causes the merge command to fail; only if all profiles are invalid. If 'all' is set, information from any; invalid profiles is excluded from the final merged product. The default; failure mode is 'any'. .. option:: --prof-sym-list=<path>. Specify a file which contains a list of symbols to generate profile symbol; list in the profile. This option can only be used with sample-based profile; in extbinary format. The entries in this file are newline-separated. .. option:: --compress-all-sections=[true|false]. Compress all sections when writing the profile. This option can only be used; with sample-based profile in extbi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-profdata.rst:3830,optimiz,optimization,3830,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-profdata.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-profdata.rst,1,['optimiz'],['optimization']
Performance,"rmation is condensed in one single; plot. These lines provide a skeleton to perform this operation. ## Two-dimensional Histograms ##. Two-dimensional histograms are a very useful tool, for example to; inspect correlations between variables. You can exploit the; bi-dimensional histogram classes provided by ROOT in a simple way.; Let's see how in this macro:. ``` {.cpp}; @ROOT_INCLUDE_FILE macros/macro7.C; ```. Two kinds of plots are provided within the code, the first one; containing three-dimensional representations (Figure [5.4](#f54)) and the second one; projections and profiles (Figure [5.5](#f55)) of the bi-dimensional histogram. [f54]: figures/th2f.png ""f54""; <a name=""f54""></a>. ![Different ways of representing bi-dimensional; histograms.\label{f54}][f54]. [f55]: figures/proj_and_prof.png ""f55""; <a name=""f55""></a>. ![The projections and profiles of bi-dimensional; histograms.\label{f55}][f55]. When a projection is performed along the x (y) direction, for every bin; along the x (y) axis, all bin contents along the y (x) axis are summed; up (upper the plots of Figure [5.5](#f55)). When a profile is performed along the x (y); direction, for every bin along the x (y) axis, the average of all the; bin contents along the y (x) is calculated together with their RMS and; displayed as a symbol with error bar (lower two plots of Figure [5.5](#f55)). Correlations between the variables are quantified by the methods; `Double_t GetCovariance()` and `Double_t GetCorrelationFactor()`. \newpage. ## Multiple histograms ##. The class `THStack` allows to manipulate a set of histograms as a single entity.; It is a collection of `TH1` (or derived) objects. When drawn, the X and Y axis; ranges are automatically computed such as all the histograms will be visible.; Several drawing option are available for both 1D and 2D histograms. The next; macros shows how it looks for 2D histograms:. ``` {.cpp .numberLines}; @ROOT_INCLUDE_FILE macros/hstack.C; ```. - Line *4*: creates the stack. - L",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/primer/histograms.md:3983,perform,performed,3983,documentation/primer/histograms.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/primer/histograms.md,1,['perform'],['performed']
Performance,"rmation to the result of TTree::Print in the case of; TBranchElement:; *Br 17 :fH : TH1F* *; *Entries : 20 : Total Size= 19334 bytes File Size = 1671 *; *Baskets : 2 : Basket Size= 16000 bytes Compression= 11.29 *; *............................................................................*; *Br 18 :fTriggerBits : TBits *; *Entries : 20 : Total Size= 1398 bytes File Size = 400 *; *Baskets : 1 : Basket Size= 16000 bytes Compression= 2.23 *; *............................................................................*; *Br 19 :fIsValid : Bool_t *; *Entries : 20 : Total Size= 582 bytes File Size = 92 *; *Baskets : 1 : Basket Size= 16000 bytes Compression= 1.00 *. Add a new function TBranch::SetStatus It is much faster to call this function in case of a Tree with many branches; instead of calling TTree::SetBranchStatus.; Implement TTreeCache::Print that shows information like:; // ******TreeCache statistics for file: cms2.root ******; // Number of branches in the cache ...: 1093; // Cache Efficiency ..................: 0.997372; // Cache Efficiency Rel...............: 1.000000; // Learn entries......................: 100; // Reading............................: 72761843 bytes in 7 transactions; // Readahead..........................: 256000 bytes with overhead = 0 bytes; // Average transaction................: 10394.549000 Kbytes; // Number of blocks in current cache..: 210, total size: 6280352; This function can be called directly from TTree: T->PrintCacheStats();. Add support for variable size array of object in a TTree (when the owner of the array is split.); And many other bug fixes, security fixes, thread safety and performance improvements ; see the svn log for details. TTree Scan and Draw. Insured that the generated histogram as an integral bin width when plotting a string or integer.; Improved the output of TTree::Scan by inserting a blank space whenever a value is not available because there is no proper row in a friend.; (Previously it was re-printing the pr",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/doc/v528/index.html:5807,Cache,Cache,5807,tree/doc/v528/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/doc/v528/index.html,1,['Cache'],['Cache']
Performance,"rmation. To provide basic functionality, the LLVM debugger does have to make some; assumptions about the source-level language being debugged, though it keeps; these to a minimum. The only common features that the LLVM debugger assumes; exist are `source files <LangRef.html#difile>`_, and `program objects; <LangRef.html#diglobalvariable>`_. These abstract objects are used by a; debugger to form stack traces, show information about local variables, etc. This section of the documentation first describes the representation aspects; common to any source-language. :ref:`ccxx_frontend` describes the data layout; conventions used by the C and C++ front-ends. Debug information descriptors are `specialized metadata nodes; <LangRef.html#specialized-metadata>`_, first-class subclasses of ``Metadata``. .. _format_common_intrinsics:. Debugger intrinsic functions; ----------------------------. LLVM uses several intrinsic functions (name prefixed with ""``llvm.dbg``"") to; track source local variables through optimization and code generation. ``llvm.dbg.declare``; ^^^^^^^^^^^^^^^^^^^^. .. code-block:: llvm. void @llvm.dbg.declare(metadata, metadata, metadata). This intrinsic provides information about a local element (e.g., variable).; The first argument is metadata holding the address of variable, typically a; static alloca in the function entry block. The second argument is a; `local variable <LangRef.html#dilocalvariable>`_ containing a description of; the variable. The third argument is a `complex expression; <LangRef.html#diexpression>`_. An `llvm.dbg.declare` intrinsic describes the; *address* of a source variable. .. code-block:: text. %i.addr = alloca i32, align 4; call void @llvm.dbg.declare(metadata i32* %i.addr, metadata !1,; metadata !DIExpression()), !dbg !2; !1 = !DILocalVariable(name: ""i"", ...) ; int i; !2 = !DILocation(...); ...; %buffer = alloca [256 x i8], align 8; ; The address of i is buffer+64.; call void @llvm.dbg.declare(metadata [256 x i8]* %buffer, metadata ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst:8360,optimiz,optimization,8360,interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,1,['optimiz'],['optimization']
Performance,"rminology section for the LCSSA form <loop-terminology-lcssa>`. .. _passes-licm:. ``licm``: Loop Invariant Code Motion; ------------------------------------. This pass performs loop invariant code motion, attempting to remove as much; code from the body of a loop as possible. It does this by either hoisting code; into the preheader block, or by sinking code to the exit blocks if it is safe.; This pass also promotes must-aliased memory locations in the loop to live in; registers, thus hoisting and sinking ""invariant"" loads and stores. Hoisting operations out of loops is a canonicalization transform. It enables; and simplifies subsequent optimizations in the middle-end. Rematerialization; of hoisted instructions to reduce register pressure is the responsibility of; the back-end, which has more accurate information about register pressure and; also handles other optimizations than LICM that increase live-ranges. This pass uses alias analysis for two purposes:. #. Moving loop invariant loads and calls out of loops. If we can determine; that a load or call inside of a loop never aliases anything stored to, we; can hoist it or sink it like any other instruction. #. Scalar Promotion of Memory. If there is a store instruction inside of the; loop, we try to move the store to happen AFTER the loop instead of inside of; the loop. This can only happen if a few conditions are true:. #. The pointer stored through is loop invariant.; #. There are no stores or loads in the loop which *may* alias the pointer.; There are no calls in the loop which mod/ref the pointer. If these conditions are true, we can promote the loads and stores in the; loop of the pointer to use a temporary alloca'd variable. We then use the; :ref:`mem2reg <passes-mem2reg>` functionality to construct the appropriate; SSA form for the variable. ``loop-deletion``: Delete dead loops; ------------------------------------. This file implements the Dead Loop Deletion Pass. This pass is responsible for; eliminating loo",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:24330,load,loads,24330,interpreter/llvm-project/llvm/docs/Passes.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst,1,['load'],['loads']
Performance,"rmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; have; completed before; performing the; atomicrmw that is; being released. 2. buffer/global/flat_atomic sc0=1; atomicrmw release - workgroup - local *If TgSplit execution mode,; local address space cannot; be used.*. 1. ds_atomic; atomicrmw release - agent - global 1. buffer_wbl2 sc1=1; - generic; - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at agent scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global and local; have completed; before performing; the atomicrmw that; is being released. 3. buffer/global/flat_atomic sc1=1; atomicrmw release - system - global 1. buffer_wbl2 sc0=1 sc1=1; - generic; - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at system scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:311342,load,load,311342,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"rn a matrix, return a TMatrixTLazy object instead. The; conversion is completely transparent to the end user, e.g.; ""TMatrixT m = THaarMatrixT(5);"" and _is_ efficient. Since TMatrixT et al. are fully integrated in %ROOT, they of course; can be stored in a %ROOT database. ### How to efficiently use this package. #### 1. Never return complex objects (matrices or vectors); Danger: For example, when the following snippet:. ~~~ {.cpp}; TMatrixD foo(int n); {; TMatrixD foom(n,n); fill_in(foom); return foom;; }; TMatrixD m = foo(5);; ~~~. runs, it constructs matrix foo:foom, copies it onto stack as a; return value and destroys foo:foom. Return value (a matrix); from foo() is then copied over to m (via a copy constructor),; and the return value is destroyed. So, the matrix constructor is; called 3 times and the destructor 2 times. For big matrices,; the cost of multiple constructing/copying/destroying of objects; may be very large. *Some* optimized compilers can cut down on 1; copying/destroying, but still it leaves at least two calls to; the constructor. Note, TMatrixDLazy (see below) can construct; TMatrixD m ""inplace"", with only a _single_ call to the; constructor. #### 2. Use ""two-address instructions"". ~~~ {.cpp}; void TMatrixD::operator += (const TMatrixD &B);; ~~~. as much as possible.; That is, to add two matrices, it's much more efficient to write. ~~~ {.cpp}; A += B;; ~~~. than. ~~~ {.cpp}; TMatrixD C = A + B;; ~~~. (if both operand should be preserved, `TMatrixD C = A; C += B;`; is still better). #### 3. Use glorified constructors when returning of an object seems inevitable:. ~~~ {.cpp}; TMatrixD A(TMatrixD::kTransposed,B);; TMatrixD C(A,TMatrixD::kTransposeMult,B);; ~~~. like in the following snippet (from `$ROOTSYS/test/vmatrix.cxx`); that verifies that for an orthogonal matrix T, T'T = TT' = E. ~~~ {.cpp}; TMatrixD haar = THaarMatrixD(5);; TMatrixD unit(TMatrixD::kUnit,haar);; TMatrixD haar_t(TMatrixD::kTransposed,haar);; TMatrixD hth(haar,TMatrixD::kTranspos",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/math/matrix/doc/index.md:15443,optimiz,optimized,15443,math/matrix/doc/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/math/matrix/doc/index.md,1,['optimiz'],['optimized']
Performance,"rn values; ^^^^^^^^^^^^^^^^^^^^^^^^. A method or function which returns a retainable object type but does not return; a retained value must ensure that the object is still valid across the return; boundary. When returning from such a function or method, ARC retains the value at the; point of evaluation of the return statement, then leaves all local scopes, and; then balances out the retain while ensuring that the value lives across the; call boundary. In the worst case, this may involve an ``autorelease``, but; callers must not assume that the value is actually in the autorelease pool. ARC performs no extra mandatory work on the caller side, although it may elect; to do something to shorten the lifetime of the returned value. .. admonition:: Rationale. It is common in non-ARC code to not return an autoreleased value; therefore; the convention does not force either path. It is convenient to not be; required to do unnecessary retains and autoreleases; this permits; optimizations such as eliding retain/autoreleases when it can be shown that; the original pointer will still be valid at the point of return. A method or function may be marked with; ``__attribute__((ns_returns_autoreleased))`` to indicate that it returns a; pointer which is guaranteed to be valid at least as long as the innermost; autorelease pool. There are no additional semantics enforced in the definition; of such a method; it merely enables optimizations in callers. .. _arc.objects.operands.casts:. Bridged casts; ^^^^^^^^^^^^^. A :arc-term:`bridged cast` is a C-style cast annotated with one of three; keywords:. * ``(__bridge T) op`` casts the operand to the destination type ``T``. If; ``T`` is a retainable object pointer type, then ``op`` must have a; non-retainable pointer type. If ``T`` is a non-retainable pointer type,; then ``op`` must have a retainable object pointer type. Otherwise the cast; is ill-formed. There is no transfer of ownership, and ARC inserts no retain; operations.; * ``(__bridge_ret",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst:22280,optimiz,optimizations,22280,interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,1,['optimiz'],['optimizations']
Performance,"rn-value.; - Must happen before; the following; store.; - Ensures that all; global memory; operations have; completed before; performing the; store that is being; released. 2. ds_store; store atomic release - agent - global 1. s_waitcnt lgkmcnt(0) &; - system - generic vmcnt(0) & vscnt(0). - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0), s_waitcnt vscnt(0); and s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/load; atomic/; atomicrmw-with-return-value.; - s_waitcnt vscnt(0); must happen after; any preceding; global/generic; store/store atomic/; atomicrmw-no-return-value.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; store.; - Ensures that all; memory operations; have; completed before; performing the; store that is being; released. 2. buffer/global/flat_store; atomicrmw release - singlethread - global 1. buffer/global/ds/flat_atomic; - wavefront - local; - generic; atomicrmw release - workgroup - global 1. s_waitcnt lgkmcnt(0) &; - generic vmcnt(0) & vscnt(0). - If CU wavefront execution; mode, omit vmcnt(0) and; vscnt(0).; - If OpenCL, omit lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0), s_waitcnt; vscnt(0) and s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load/load; atomic/; atomicrmw-with-return-value.; - s_waitcnt vscnt(0); must happen after; any preceding; global/generic; store/store; atomic/; atomicrmw-no-return-value.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; me",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:357331,perform,performing,357331,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['perform'],['performing']
Performance,"rnVM image and contextualization; ---------------------------------------------. ### Download the CernVM bare image. The Virtual Analysis Facility currently works with *CernVM Batch 2.7.1; 64-bit*. This means that you need to have this CernVM image available; either on your local hard disk (in case of a desktop deployment) or in; your cloud's image repository. > For convenience we provide the direct link for the working versions:; >; > - [CernVM 2.7.1 batch 64-bit for; > **KVM**](https://cernvm.cern.ch/releases/19/cernvm-batch-node-2.7.1-2-3-x86_64.hdd.gz); >; > - [CernVM 2.7.1 batch 64-bit for; > **Xen**](https://cernvm.cern.ch/releases/19/cernvm-batch-node-2.7.1-2-3-x86_64.ext3.gz); >; > Images are gzipped. In most cases you'll need to gunzip them before; > registering to your image repository. ### Create VM configuration profiles. CernVM images are base images supporting boot-time customization via; configuration profiles called ""contexts"". Context creation can be; performed through the [CernVM Online](https://cernvm-online.cern.ch/); website. The site is immediately accessible if you have a CERN account. Go to your [CernVM Online; Dashboard](https://cernvm-online.cern.ch/dashboard), click on the; **Create new context...** dropdown and select **Virtual Analysis Facility; node**. There's only a few parameters to configure. Context name; : A name for your context (such as *VAF Master for ATLAS*). Any name; will work. Role; : Use this to configure either a *master* or a *slave*. VAF master (only available when configuring a slave); : IP address or FQDN of the Virtual Analysis Facility master. Auth method; : Choose between *ALICE LDAP* (useful only for ALICE users) or *Pool; accounts* (good for authenticating all the other Grid users). Num. pool accounts (only available when using pool accounts auth); : Number of pool accounts to create. Proxy for CVMFS; : An URL specifying the proxy server for CernVM-FS, such as; `http://ca-proxy.cern.ch:3128/`. If you leave it empt",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/proof/doc/confman/DeployVirtualAnalysisFacility.md:1792,perform,performed,1792,proof/doc/confman/DeployVirtualAnalysisFacility.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/proof/doc/confman/DeployVirtualAnalysisFacility.md,1,['perform'],['performed']
Performance,"rned vector all have the same float or; integer element type. '``llvm.matrix.column.major.load.*``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare vectorty @llvm.matrix.column.major.load.*(; ptrty %Ptr, i64 %Stride, i1 <IsVolatile>, i32 <Rows>, i32 <Cols>). Overview:; """""""""""""""""". The '``llvm.matrix.column.major.load.*``' intrinsics load a ``<Rows> x <Cols>``; matrix using a stride of ``%Stride`` to compute the start address of the; different columns. The offset is computed using ``%Stride``'s bitwidth. This; allows for convenient loading of sub matrixes. If ``<IsVolatile>`` is true, the; intrinsic is considered a :ref:`volatile memory access <volatile>`. The result; matrix is returned in the result vector. If the ``%Ptr`` argument is known to; be aligned to some boundary, this can be specified as an attribute on the; argument. Arguments:; """""""""""""""""""". The first argument ``%Ptr`` is a pointer type to the returned vector type, and; corresponds to the start address to load from. The second argument ``%Stride``; is a positive, constant integer with ``%Stride >= <Rows>``. ``%Stride`` is used; to compute the column memory addresses. I.e., for a column ``C``, its start; memory addresses is calculated with ``%Ptr + C * %Stride``. The third Argument; ``<IsVolatile>`` is a boolean value. The fourth and fifth arguments,; ``<Rows>`` and ``<Cols>``, correspond to the number of rows and columns,; respectively, and must be positive, constant integers. The returned vector must; have ``<Rows> * <Cols>`` elements. The :ref:`align <attr_align>` parameter attribute can be provided for the; ``%Ptr`` arguments. '``llvm.matrix.column.major.store.*``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare void @llvm.matrix.column.major.store.*(; vectorty %In, ptrty %Ptr, i64 %Stride, i1 <IsVolatile>, i32 <Rows>, i32 <Cols>). Overview:; """""""""""""""""". The '``llvm.matrix.column.major.stor",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:678569,load,load,678569,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,"rnel-descriptor`) of the kernel to execute is obtained.; It must be for a kernel that is contained in a code object that was loaded; by an HSA compatible runtime on the kernel agent with which the AQL queue is; associated.; 3. Space is allocated for the kernel arguments using the HSA compatible runtime; allocator for a memory region with the kernarg property for the kernel agent; that will execute the kernel. It must be at least 16-byte aligned.; 4. Kernel argument values are assigned to the kernel argument memory; allocation. The layout is defined in the *HSA Programmer's Language; Reference* [HSA]_. For AMDGPU the kernel execution directly accesses the; kernel argument memory in the same way constant memory is accessed. (Note; that the HSA specification allows an implementation to copy the kernel; argument contents to another location that is accessed by the kernel.); 5. An AQL kernel dispatch packet is created on the AQL queue. The HSA compatible; runtime api uses 64-bit atomic operations to reserve space in the AQL queue; for the packet. The packet must be set up, and the final write must use an; atomic store release to set the packet kind to ensure the packet contents are; visible to the kernel agent. AQL defines a doorbell signal mechanism to; notify the kernel agent that the AQL queue has been updated. These rules, and; the layout of the AQL queue and kernel dispatch packet is defined in the *HSA; System Architecture Specification* [HSA]_.; 6. A kernel dispatch packet includes information about the actual dispatch,; such as grid and work-group size, together with information from the code; object about the kernel, such as segment sizes. The HSA compatible runtime; queries on the kernel symbol can be used to obtain the code object values; which are recorded in the :ref:`amdgpu-amdhsa-code-object-metadata`.; 7. CP executes micro-code and is responsible for detecting and setting up the; GPU to execute the wavefronts of a kernel dispatch.; 8. CP ensures that when ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:151121,queue,queue,151121,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['queue'],['queue']
Performance,"rnk (Phabricator), rnk (GitHub). Incremental compilation, REPLs, clang-repl; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~; | Vassil Vassilev; | Vassil.Vassilev\@cern.ch (email), v.g.vassilev (Phabricator), vgvassilev (GitHub). Standards Conformance; ---------------------; The following people are responsible for validating that changes are conforming; to a relevant standard. Contact them for questions about how to interpret a; standard, when fixing standards bugs, or when implementing a new standard feature. C conformance; ~~~~~~~~~~~~~; | Aaron Ballman; | aaron\@aaronballman.com (email), aaron.ballman (Phabricator), AaronBallman (GitHub), AaronBallman (Discourse), aaronballman (Discord), AaronBallman (IRC). C++ conformance; ~~~~~~~~~~~~~~~; | Hubert Tong; | hubert.reinterpretcast\@gmail.com (email), hubert.reinterpretcast (Phabricator), hubert-reinterpretcast (GitHub). Objective-C/C++ conformance; ~~~~~~~~~~~~~~~~~~~~~~~~~~~; | John McCall; | rjmccall\@apple.com (email), rjmccall (Phabricator), rjmccall (GitHub). OpenMP conformance; ~~~~~~~~~~~~~~~~~~; | Alexey Bataev; | a.bataev\@hotmail.com (email), ABataev (Phabricator), alexey-bataev (GitHub). OpenCL conformance; ~~~~~~~~~~~~~~~~~~; | Anastasia Stulova; | anastasia\@compiler-experts.com (email), Anastasia (Phabricator), AnastasiaStulova (GitHub). SYCL conformance; ~~~~~~~~~~~~~~~~; | Alexey Bader; | alexey.bader\@intel.com (email), bader (Phabricator), bader (GitHub). Former Code Owners; ==================; The following people have graciously spent time performing code ownership; responsibilities but are no longer active in that role. Thank you for all your; help with the success of the project!. Emeritus owners; ---------------; | Doug Gregor (dgregor\@apple.com); | Richard Smith (richard\@metafoo.co.uk). Former component owners; -----------------------; | Chandler Carruth (chandlerc\@gmail.com, chandlerc\@google.com) -- CMake, library layering; | Devin Coughlin (dcoughlin\@apple.com) -- Clang static analyzer; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/CodeOwners.rst:6667,perform,performing,6667,interpreter/llvm-project/clang/CodeOwners.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/CodeOwners.rst,1,['perform'],['performing']
Performance,"rocess the one closed to the end of functions; first. This may simply CreateNewWater. //===---------------------------------------------------------------------===//. Eliminate copysign custom expansion. We are still generating crappy code with; default expansion + if-conversion. //===---------------------------------------------------------------------===//. Eliminate one instruction from:. define i32 @_Z6slow4bii(i32 %x, i32 %y) {; %tmp = icmp sgt i32 %x, %y; %retval = select i1 %tmp, i32 %x, i32 %y; ret i32 %retval; }. __Z6slow4bii:; cmp r0, r1; movgt r1, r0; mov r0, r1; bx lr; =>. __Z6slow4bii:; cmp r0, r1; movle r0, r1; bx lr. //===---------------------------------------------------------------------===//. Implement long long ""X-3"" with instructions that fold the immediate in. These; were disabled due to badness with the ARM carry flag on subtracts. //===---------------------------------------------------------------------===//. More load / store optimizations:; 1) Better representation for block transfer? This is from Olden/power:. 	fldd d0, [r4]; 	fstd d0, [r4, #+32]; 	fldd d0, [r4, #+8]; 	fstd d0, [r4, #+40]; 	fldd d0, [r4, #+16]; 	fstd d0, [r4, #+48]; 	fldd d0, [r4, #+24]; 	fstd d0, [r4, #+56]. If we can spare the registers, it would be better to use fldm and fstm here.; Need major register allocator enhancement though. 2) Can we recognize the relative position of constantpool entries? i.e. Treat. 	ldr r0, LCPI17_3; 	ldr r1, LCPI17_4; 	ldr r2, LCPI17_5. as; 	ldr r0, LCPI17; 	ldr r1, LCPI17+4; 	ldr r2, LCPI17+8. Then the ldr's can be combined into a single ldm. See Olden/power. Note for ARM v4 gcc uses ldmia to load a pair of 32-bit values to represent a; double 64-bit FP constant:. 	adr	r0, L6; 	ldmia	r0, {r0-r1}. 	.align 2; L6:; 	.long	-858993459; 	.long	1074318540. 3) struct copies appear to be done field by field; instead of by words, at least sometimes:. struct foo { int x; short s; char c1; char c2; };; void cpy(struct foo*a, struct foo*b) { *a = *b; }",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:2823,load,load,2823,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,4,"['load', 'optimiz']","['load', 'optimizations']"
Performance,"rocessing time:. Instantaneous processing rate, which is now better; estimated by a better estimation of the normalizing times; Average read chunck size, defined as; TFile::GetFileBytesRead() / TFile::GetFileReadCalls() during the last; unit of time; this allows to monitor the usage of the cache; this plot; is present only if some I/O is done, i.e. not for pure CPU tasks.; The number of active workers; The number of total and effecive sessions running; concurrently on the cluster (started by the same daemon); this plot is; present only is the number is at least onec different from 1. If enabled, send monitoring information from the master; at each GetNextPacket (at each call of TPerfStat::PacketEvent) to allow; extrnal real-time progress monitoring.; Save the status of a 'proofserv' session into a new file; in the 'activesessions' area. The full path of the new file is;          ; <admin_path>/.xproofd.<port>/activesessions/<user>.<group>.<pid>.status. The status indicates whether the session is idle, running or queued.; The status is updated every 'checkfq' secs (see xpd.proofservmgr;; default 30 s). The status is dumped by the reader thread of TXProofServ; and therefore its r/w access is protected. Enable the use of the tree cache also for local files,; adapting the default settings for the cache to the recent changes; In the XrdProofd plug-in. Improve synchronization between parent and child during; fork; Optimize loops over directory entries; Improve error and notification messages. Improved handling of Ctrl-C; this follows from a fix in; TMonitor and an improved handling of non-finished query state in the; workers (results are not send to master if the query was aborted) . Fixes. TFileMerger. Fix a problem preventing correct transmission of all; non-mergeable objects (fixes bug #52886); Remove the argument isdir from the function; MergeRecursive; Do not remove the first file in the list when returning; from MergeRecursive (fixes bug #54591); Fix a major leak whe",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/proof/doc/v526/index.html:5795,queue,queued,5795,proof/doc/v526/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/proof/doc/v526/index.html,2,['queue'],['queued']
Performance,"rojects <workflow-cross-repo-commit>`.; * :ref:`Branching/Stashing/Updating for Local Development or Experiments <workflow-mono-branching>`.; * :ref:`Bisecting <workflow-mono-bisecting>`. Workflow Before/After; =====================. This section goes through a few examples of workflows, intended to illustrate; how end-users or developers would interact with the repository for; various use-cases. .. _workflow-checkout-commit:. Checkout/Clone a Single Project, with Commit Access; ---------------------------------------------------. Currently; ^^^^^^^^^. ::. # direct SVN checkout; svn co https://user@llvm.org/svn/llvm-project/llvm/trunk llvm; # or using the read-only Git view, with git-svn; git clone https://llvm.org/git/llvm.git; cd llvm; git svn init https://llvm.org/svn/llvm-project/llvm/trunk --username=<username>; git config svn-remote.svn.fetch :refs/remotes/origin/main; git svn rebase -l # -l avoids fetching ahead of the git mirror. Commits are performed using `svn commit` or with the sequence `git commit` and; `git svn dcommit`. .. _workflow-multicheckout-nocommit:. Monorepo Variant; ^^^^^^^^^^^^^^^^. With the monorepo variant, there are a few options, depending on your; constraints. First, you could just clone the full repository:. git clone https://github.com/llvm/llvm-project.git. At this point you have every sub-project (llvm, clang, lld, lldb, ...), which; :ref:`doesn't imply you have to build all of them <build_single_project>`. You; can still build only compiler-rt for instance. In this way it's not different; from someone who would check out all the projects with SVN today. If you want to avoid checking out all the sources, you can hide the other; directories using a Git sparse checkout::. git config core.sparseCheckout true; echo /compiler-rt > .git/info/sparse-checkout; git read-tree -mu HEAD. The data for all sub-projects is still in your `.git` directory, but in your; checkout, you only see `compiler-rt`.; Before you push, you'll need to fetch and r",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/GitHubMove.rst:13304,perform,performed,13304,interpreter/llvm-project/llvm/docs/Proposals/GitHubMove.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/GitHubMove.rst,1,['perform'],['performed']
Performance,"ront in execution order. The exception is; that ``flat_load/store/atomic`` instructions can report out of vector memory; order if they access LDS memory, and out of LDS operation order if they access; global memory.; * The vector memory operations access a single vector L1 cache shared by all; SIMDs a CU. Therefore:. * No special action is required for coherence between the lanes of a single; wavefront. * No special action is required for coherence between wavefronts in the same; work-group since they execute on the same CU. The exception is when in; tgsplit execution mode as wavefronts of the same work-group can be in; different CUs and so a ``buffer_wbinvl1_vol`` is required as described in; the following item. * A ``buffer_wbinvl1_vol`` is required for coherence between wavefronts; executing in different work-groups as they may be executing on different; CUs. * The scalar memory operations access a scalar L1 cache shared by all wavefronts; on a group of CUs. The scalar and vector L1 caches are not coherent. However,; scalar operations are used in a restricted way so do not impact the memory; model. See :ref:`amdgpu-amdhsa-memory-spaces`.; * The vector and scalar memory operations use an L2 cache shared by all CUs on; the same agent. * The L2 cache has independent channels to service disjoint ranges of virtual; addresses.; * Each CU has a separate request queue per channel. Therefore, the vector and; scalar memory operations performed by wavefronts executing in different; work-groups (which may be executing on different CUs), or the same; work-group if executing in tgsplit mode, of an agent can be reordered; relative to each other. A ``s_waitcnt vmcnt(0)`` is required to ensure; synchronization between vector memory operations of different CUs. It; ensures a previous vector memory operation has completed before executing a; subsequent vector memory or LDS operation and so can be used to meet the; requirements of acquire and release.; * The L2 cache of one agent ca",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:236631,cache,caches,236631,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['cache'],['caches']
Performance,"ront wide operations in a; global order and involve no caching. Completion is reported to a wavefront in; execution order.; * The LDS memory has multiple request queues shared by the SIMDs of a; WGP. Therefore, the LDS operations performed by different wavefronts of a; work-group can be reordered relative to each other, which can result in; reordering the visibility of vector memory operations with respect to LDS; operations of other wavefronts in the same work-group. A ``s_waitcnt; lgkmcnt(0)`` is required to ensure synchronization between LDS operations and; vector memory operations between wavefronts of a work-group, but not between; operations performed by the same wavefront.; * The vector memory operations are performed as wavefront wide operations.; Completion of load/store/sample operations are reported to a wavefront in; execution order of other load/store/sample operations performed by that; wavefront.; * The vector memory operations access a vector L0 cache. There is a single L0; cache per CU. Each SIMD of a CU accesses the same L0 cache. Therefore, no; special action is required for coherence between the lanes of a single; wavefront. However, a ``buffer_gl0_inv`` is required for coherence between; wavefronts executing in the same work-group as they may be executing on SIMDs; of different CUs that access different L0s. A ``buffer_gl0_inv`` is also; required for coherence between wavefronts executing in different work-groups; as they may be executing on different WGPs.; * The scalar memory operations access a scalar L0 cache shared by all wavefronts; on a WGP. The scalar and vector L0 caches are not coherent. However, scalar; operations are used in a restricted way so do not impact the memory model. See; :ref:`amdgpu-amdhsa-memory-spaces`.; * The vector and scalar memory L0 caches use an L1 cache shared by all WGPs on; the same SA. Therefore, no special action is required for coherence between; the wavefronts of a single work-group. However, a ``buffer_gl1_",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:336824,cache,cache,336824,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['cache'],['cache']
Performance,"root canvas; `Options` menu, you will see exactly which is the selected node in the; bottom right. Right-clicking when a volume is selected will open its; context menu where several actions can be performed (e.g. drawing it). **Q:** ""OK, but now I do not want to see all the geometry, but just a particular volume and its content. How can I do this?"". **A:** Once you have set a convenient global visualization option; and level, what you need is just call the `Draw()` method of your; interesting volume. You can do this either by interacting with the; expanded tree of volumes in a ROOT browser (where the context menu of; any volume is available), either by getting a pointer to it (e.g. by; name): `gGeoManager->GetVolume(""vol_name"")->Draw();`. \anchor GP04b; ### Visualization Settings and Attributes. Supposing you now understand the basic things to do for drawing the; geometry or parts of it, you still might be not happy and wishing to; have more control on it. We will describe below how you can fine-tune some; settings. Since the corresponding attributes are flags belonging to; volume and node objects, you can change them at any time (even when the; picture is already drawn) and see immediately the result. \anchor GP04ba; #### Colors and Line Styles. We have already described how to change the line colors for volumes. In; fact, volume objects inherit from TAttLine class so the line style or; width can also be changed:. ~~~{.cpp}; myVolume->SetLineColor(kRed);; myVolume->SetLineWith(2);; myVolume->SetLineStyle(kDotted);; ~~~. When drawing in solid mode, the color of the drawn volume corresponds to; the line color. \anchor GP04bb; #### Visibility Settings. The way geometry is build forces the definition of several volumes that; does not represent real objects, but just virtual containers used for; grouping and positioning volumes together. One would not want to see; them in the picture. Since every volume is by default visible, one has; to do this sort of tuning by its own",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md:101770,tune,tune,101770,geom/geom/doc/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md,1,['tune'],['tune']
Performance,"root"");; chain.Add(""file2.root"");; chain.Add(""file3.root"");; ```. The name of the **`TChain`** will be the same as the name of the tree;; in this case it will be `""T"". Note that two `objects can have the same; name as long as they are not histograms in the same directory, because; there, the histogram names are used to build a hash table. The class; **`TChain`** is derived from the class **`TTree`**. For example, to; generate a histogram corresponding to the attribute ""`x`"" in tree ""`T`""; by processing sequentially the three files of this chain, we can use the; `TChain::Draw` method. ``` {.cpp}; chain.Draw(""x"");; ```. When using a **`TChain`**, the branch address(es) must be set with:. ``` {.cpp}; chain.SetBranchAdress(branchname,...) // use this for TChain; ```. rather than:. ``` {.cpp}; branch->SetAddress(...); // this will not work; ```. The second form returns the pointer to the branch of the current; **`TTree`** in the chain, typically the first one. The information is; lost when the next **`TTree`** is loaded. The following statements; illustrate how to set the address of the object to be read and how to; loop on all events of all files of the chain. ``` {.cpp}; {; TChain chain(""T""); // create the chain with tree ""T""; chain.Add(""file1.root""); // add the files; chain.Add(""file2.root"");; chain.Add(""file3.root"");; TH1F *hnseg = new TH1F(""hnseg"",; ""Number of segments for selected tracks"",; 5000,0,5000);; // create an object before setting the branch address; Event *event = new Event();; // Specify the address where to read the event object; chain.SetBranchAddress(""event"", &event);. // Start main loop on all events In case you want to read only a few; // branches, use TChain::SetBranchStatus to activate a branch.; Int_t nevent = chain.GetEntries();; for (Int_t i=0;i<nevent;i++) {; // read complete accepted event in memory; chain.GetEvent(i);; // Fill histogram with number of segments; hnseg->Fill(event->GetNseg());; }; // Draw the histogram; hnseg->Draw();; }; ```.",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Trees.md:141548,load,loaded,141548,documentation/users-guide/Trees.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Trees.md,1,['load'],['loaded']
Performance,"rotate it. ![](pictures/03000025.png) **A Marker**: Click with the left button; where to place the marker. The marker can be modified by using the; method `SetMarkerStyle()` of **`TSystem`**. ![](pictures/03000026.png) **A Graphical Cut**: Click with the left; button on each point of a polygon delimiting the selected area. Close; the cut by double clicking on the last point. A **`TCutG`** object is; created. It can be used as a selection for a **`TTree`**`::Draw`. You; can get a pointer to this object with:. ``` {.cpp}; TCutG cut = (TCutG*)gPad->GetPrimitive(""CUTG""); ```. Once you are happy with your picture, you can select the; `Save as canvas.C` item in the canvas File menu. This will; automatically generate a script with the C++ statements corresponding; to the picture. This facility also works if you have other objects not; drawn with the graphics editor (histograms for example). ### The Editor Frame. The ROOT graphics editor loads the corresponding object editor; `objEditor` according to the selected object `obj` in the canvas; respecting the class inheritance. An object in the canvas is selected; after the left mouse click on it. For example, if the selected object; is **`TAxis`**, the **`TAxisEditor`** will shows up in the editor; frame giving the possibility for changing different axis attributes.; The graphics editor can be:. Embedded - connected only with the canvas in the application window; that appears on the left of the canvas window after been activated via; View menu / Editor. It appears on the left side if the canvas window; allowing users to edit the attributes of the selected object via; provided user interface. The name of the selected object is displayed; on the top of the editor frame in red color. If the user interface; needs more space then the height of the canvas window, a vertical; scroll bar appears for easer navigation. ![](pictures/03000027.png). Global - has own application window and can be connected to any; created canvas in a ROOT s",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/GettingStarted.md:16752,load,loads,16752,documentation/users-guide/GettingStarted.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/GettingStarted.md,1,['load'],['loads']
Performance,"roup; - agent; - system; store atomic monotonic - singlethread - local 1. ds_store; - wavefront; - workgroup; atomicrmw monotonic - singlethread - global 1. buffer/global/flat_atomic; - wavefront - generic; - workgroup; - agent; - system; atomicrmw monotonic - singlethread - local 1. ds_atomic; - wavefront; - workgroup; **Acquire Atomic**; ------------------------------------------------------------------------------------; load atomic acquire - singlethread - global 1. buffer/global/ds/flat_load; - wavefront - local; - generic; load atomic acquire - workgroup - global 1. buffer/global_load glc=1. - If CU wavefront execution; mode, omit glc=1. 2. s_waitcnt vmcnt(0). - If CU wavefront execution; mode, omit.; - Must happen before; the following buffer_gl0_inv; and before any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw. 3. buffer_gl0_inv. - If CU wavefront execution; mode, omit.; - Ensures that; following; loads will not see; stale data. load atomic acquire - workgroup - local 1. ds_load; 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; the following buffer_gl0_inv; and before any following; global/generic load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than the local load; atomic value being; acquired. 3. buffer_gl0_inv. - If CU wavefront execution; mode, omit.; - If OpenCL, omit.; - Ensures that; following; loads will not see; stale data. load atomic acquire - workgroup - generic 1. flat_load glc=1. - If CU wavefront execution; mode, omit glc=1. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If CU wavefront execution; mode, omit vmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Must happen before; the following; buffer_gl0_inv and any; following global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local load; atomic value being; acquired. 3. buffer_gl0_inv. - If CU wavefront execution; mode, omit.; - ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:346656,load,load,346656,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"roup; atomicrmw monotonic - singlethread - global 1. buffer/global/flat_atomic; - wavefront - generic; - workgroup; - agent; - system; atomicrmw monotonic - singlethread - local 1. ds_atomic; - wavefront; - workgroup; **Acquire Atomic**; ------------------------------------------------------------------------------------; load atomic acquire - singlethread - global 1. buffer/global/ds/flat_load; - wavefront - local; - generic; load atomic acquire - workgroup - global 1. buffer/global_load; load atomic acquire - workgroup - local 1. ds/flat_load; - generic 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local load; atomic value being; acquired. load atomic acquire - agent - global 1. buffer/global_load; - system glc=1; 2. s_waitcnt vmcnt(0). - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the load; has completed; before invalidating; the cache. 3. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale global data. load atomic acquire - agent - generic 1. flat_load glc=1; - system 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If OpenCL omit; lgkmcnt(0).; - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the flat_load; has completed; before invalidating; the cache. 3. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acquire - singlethread - global 1. buffer/global/ds/flat_atomic; - wavefront - local; - generic; atomicrmw acquire - workgroup - global 1. buffer/global_atomic; atomicrmw acquire - workgroup - local 1. ds/flat_atomic; - generic 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:214751,load,load,214751,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,"['cache', 'load']","['cache', 'load']"
Performance,"routine returning a thread-specific `errno` value. Some; other systems may provide other implementations of `errno`. With respect; to ROOT, a thread specific data is for example the ***`gPad`*** pointer,; which is treated in a different way, whether it is accessed from any; thread or the main thread. Threads within a process must not be considered as a group of processes; (even though in Linux each thread receives an own process id, so that it; can be scheduled by the kernel scheduler). All threads share the same; address space. This means that two pointers having the same value in two; threads refer to the same data. Also, if any thread changes one of the; shared system resources, all threads within the process are affected.; For example, if a thread closes a file, the file is closed for all; threads. ### The Initial Thread. When a process is created, one thread is automatically created. This; thread is called the initial thread or the main thread. The initial; thread executes the main routine in multi-threaded programs. Note: At the end of this chapter is a glossary of thread specific terms. ## Implementation of Threads in ROOT. The **`TThread`** class has been developed to provide a platform; independent interface to threads for ROOT. ### Installation. For the time being, it is still necessary to compile a threaded version; of ROOT to enable some very special treatments of the canvas operations.; We hope that this will become the default later. To compile ROOT, just do (for example on a debian Linux):. ```; ./configure linuxdeb2 --with-thread=/usr/lib/libpthread.so; gmake depend; gmake; ```. This configures and builds ROOT using `/usr/lib/libpthread.so` as the; `Pthread` library, and defines `R__THREAD`. This enables the thread specific treatment of *`gPad`*, and creates; `$ROOTSYS/lib/libThread.so.`. Note: The parameter linuxdeb2 has to be replaced with the appropriate; ROOT keyword for your platform. ### Classes. **`TThread`** class implements threads . The pla",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Threads.md:2603,multi-thread,multi-threaded,2603,documentation/users-guide/Threads.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Threads.md,1,['multi-thread'],['multi-threaded']
Performance,"rowse/ROOT-8478)] - Prompt error when building streamer info and a data member is a vector<T> w/o dictionary; - Fix ROOT-8686 and ROOT-8595 which led to error when persistifying classes which featured std::arrays as data members in TTrees.; - TDavixFile: Added support for bucket name in path; - Fix error sometimes prompted when trying to write std::array column-wise. ## Database Libraries. ## Networking Libraries. ## GUI Libraries. ## Montecarlo Libraries. ## Parallelism and PROOF; - Add ROOT::GetImplicitMTPoolSize function to get the size of the pool used to enable implicit multi threading; - Add the TThreadExecutor::Foreach method for parallelising functions featuring void return type; - Add TBufferMerger and TBufferMergerFile classes. TBufferMerger is a class to facilitate writing data in; parallel from multiple threads, while writing to a single; output file. Its purpose is similar to TParallelMergingFile,; but instead of using processes that connect to a network; socket, TBufferMerger uses threads that each write to a; TBufferMergerFile, which in turn push data into a queue; managed by the TBufferMerger. An excerpt of the; [tutorial](https://github.com/root-project/root/blob/master/tutorials/multicore/mt103_fillNtuples.C); is shown below.; ```{.cpp}; // Create the TBufferMerger; TBufferMerger merger(""mp103_fillNtuple.root"");. // Define what each worker will do; auto work_function = [&]() {; auto f = merger.GetFile();; TNtuple ntrand(""ntrand"", ""Random Numbers"", ""r"");; fill(ntrand, nEventsPerWorker);; ntrand.Write();; f->Write();; };; ```. ## Language Bindings. - Add in PyROOT the converter for std::string_view; - Fix ROOT-8811: pickling of ROOT.Long now works; - Fix ROOT-8809: push_back on a vector of pointers; - Fix ROOT-8805: itemsize was not set on buffers returned by PyROOT functions. ## JavaScript ROOT. ## Tutorials. ## Class Reference Guide. ## Build, Configuration and Testing Infrastructure. - rlibmap has been removed; it was deprecated for three years.; -",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v610/index.md:16656,queue,queue,16656,README/ReleaseNotes/v610/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v610/index.md,1,['queue'],['queue']
Performance,"rowser&file=../files/atlas.root&item=LEDShapeHeightCorr_Gain0;1&opt=col. ## Changes in 3.7; 1. Support of X axis with custom labels like: http://jsroot.gsi.de/dev/?nobrowser&json=../files/hist_xlabels.json; 2. Extend functionality of JSROOT.addDrawFunc() function. One could register type-specific; `make_request` and `after_request` functions; `icon`, `prereq`, `script`, `monitor` properties.; This let add more custom elements to the generic gui, implemented with JSROOT.HierarchyPainter; 3. Provide full support of require.js. One could load now JSRootCore.js script like:. <script type=""text/javascript"" src=""require.js"" data-main=""scripts/JSRootCore.js""></script>. After this several modules are defined and can be used with syntax like:. require(['JSRootPainter'], function(jsroot) { /*any user code*/});. Also inside JSROOT require.js used to load all dependencies. ## Changes in 3.6; 1. Try to provide workaround for websites where require.js already loaded.; This makes problem by direct loading of jquery and jquery-ui; 2. Provide workaround for older version of jquery-ui; 3. Prompt for input of command arguments; 4. After command execution one could automatically reload hierarchy (_hreload property) or; update view of displayed object (_update_item property); 5. Use HierarchyPainter for implementing draw.htm. This let us handle; all different kinds of extra attributes in central place; 6. Fix problem in tabs layout - new tab should be add to direct child; 7. When drawing several tabs, activate frame before drawing - only then; real frame size will be set; 8. Fix problem with GetBBox - it only can be used for visible elements in mozilla.; 9. Support drawing of fit parameters in stat box, use (as far as possible) stat and; fit format for statistic display; 10. Implement 'g' formatting kind for stat box output - one need to checks; significant digits when producing output.; 11. Support new draw options for TGraph: 'C', 'B1', '0', '2', '3', '4', '[]'; 12. Primary support fo",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/js/changes.md:63957,load,loading,63957,js/changes.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/js/changes.md,1,['load'],['loading']
Performance,"roxy, the TWebFile ctor (or via TFile::Open()) supports the; option ""NOPROXY"".; Add support for streaming std::bitset STL containers; Extend the checks done in case of a StreamerInfo checksum mismatch to; avoid spurrious failures (for example because of the various possible; type names for STL containers) and to report details on the nature of; the mismatch: explicit list missing base classese, missing data members; or the actual differences in type or comments.; For example:. Warning in : The following data member of the on-file layout version 2 of class 'Tdata' differs from the in-memory layout version 2:; double mydouble; //; vs; double mydouble_two; //; Warning in : The following data member of the in-memory layout version 2 of class 'Tdata' is missing from the on-file layout version 2:; int more; //; Warning in : The following data member of the in-memory layout version 2 of class 'Tdata' is missing from the on-file layout version 2:; int three; //. Upgrade MakeProject to be able to handle ROOT files created by for ATLAS.; Allow user to provide a custom reallocator when the TBuffer is being passed; memory. If the TBuffer does not own the memory __and__ no custom memory; reallocator has been set, a Fatal error will be issued:; Fatal in : Failed to expand the data buffer because TBuffer does not own it and no custom memory reallocator was provided.; Re-allow reading empty vector< long double >, however long double is still not supported.; Upgrade TSQLFile to properly work with MySQL on MacOS.; Update to the CollectionProxyInfo interface to insure the proper creation of iterator over std containers on all platforms.; In XML and SQL output, use %e format to write float and double:; ; Conversion from float/double to string per default performed with ""%e"" (exponential) format.; Format can be configured with SetFloatFormat methods that one can specify precision, width arguments of printf call.; sscanf works as before - ""%f"" accpet both exponential and decimal format. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/io/doc/v524/index.html:2006,perform,performed,2006,io/doc/v524/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/io/doc/v524/index.html,2,['perform'],['performed']
Performance,"royed. When such a message is sent to an; object, the object's lifetime will be extended until at least the earliest of:. * the last use of the returned pointer, or any pointer derived from it, in the; calling function or; * the autorelease pool is restored to a previous state. .. admonition:: Rationale. Rationale: not all memory and resources are managed with reference counts; it; is common for objects to manage private resources in their own, private way.; Typically these resources are completely encapsulated within the object, but; some classes offer their users direct access for efficiency. If ARC is not; aware of methods that return such ""interior"" pointers, its optimizations can; cause the owning object to be reclaimed too soon. This attribute informs ARC; that it must tread lightly. The extension rules are somewhat intentionally vague. The autorelease pool; limit is there to permit a simple implementation to simply retain and; autorelease the receiver. The other limit permits some amount of; optimization. The phrase ""derived from"" is intended to encompass the results; both of pointer transformations, such as casts and arithmetic, and of loading; from such derived pointers; furthermore, it applies whether or not such; derivations are applied directly in the calling code or by other utility code; (for example, the C library routine ``strchr``). However, the implementation; never need account for uses after a return from the code which calls the; method returning an interior pointer. As an exception, no extension is required if the receiver is loaded directly; from a ``__strong`` object with :ref:`precise lifetime semantics; <arc.optimization.precise>`. .. admonition:: Rationale. Implicit autoreleases carry the risk of significantly inflating memory use,; so it's important to provide users a way of avoiding these autoreleases.; Tying this to precise lifetime semantics is ideal, as for local variables; this requires a very explicit annotation, which allows ARC to",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst:100875,optimiz,optimization,100875,interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,1,['optimiz'],['optimization']
Performance,"rr`` that is defined earlier in ``SparcInstrInfo.td``:. .. code-block:: text. def MEMrr : Operand<i32> {; let PrintMethod = ""printMemOperand"";; let MIOperandInfo = (ops IntRegs, IntRegs);; }. The fifth parameter is a string that is used by the assembly printer and can be; left as an empty string until the assembly printer interface is implemented.; The sixth and final parameter is the pattern used to match the instruction; during the SelectionDAG Select Phase described in :doc:`CodeGenerator`.; This parameter is detailed in the next section, :ref:`instruction-selector`. Instruction class definitions are not overloaded for different operand types,; so separate versions of instructions are needed for register, memory, or; immediate value operands. For example, to perform a Load Integer instruction; for a Word from an immediate operand to a register, the following instruction; class is defined:. .. code-block:: text. def LDri : F3_2 <3, 0b000000, (outs IntRegs:$rd), (ins (MEMri $rs1, $simm13):$addr),; ""ld [$addr], $dst"",; [(set i32:$rd, (load ADDRri:$addr))]>;. Writing these definitions for so many similar instructions can involve a lot of; cut and paste. In ``.td`` files, the ``multiclass`` directive enables the; creation of templates to define several instruction classes at once (using the; ``defm`` directive). For example in ``SparcInstrInfo.td``, the ``multiclass``; pattern ``F3_12`` is defined to create 2 instruction classes each time; ``F3_12`` is invoked:. .. code-block:: text. multiclass F3_12 <string OpcStr, bits<6> Op3Val, SDNode OpNode> {; def rr : F3_1 <2, Op3Val,; (outs IntRegs:$rd), (ins IntRegs:$rs1, IntRegs:$rs1),; !strconcat(OpcStr, "" $rs1, $rs2, $rd""),; [(set i32:$rd, (OpNode i32:$rs1, i32:$rs2))]>;; def ri : F3_2 <2, Op3Val,; (outs IntRegs:$rd), (ins IntRegs:$rs1, i32imm:$simm13),; !strconcat(OpcStr, "" $rs1, $simm13, $rd""),; [(set i32:$rd, (OpNode i32:$rs1, simm13:$simm13))]>;; }. So when the ``defm`` directive is used for the ``XOR`` and ``ADD``; ins",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMBackend.rst:34238,load,load,34238,interpreter/llvm-project/llvm/docs/WritingAnLLVMBackend.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMBackend.rst,1,['load'],['load']
Performance,"rranty; and distribute a copy of this License along with the; Library. You may charge a fee for the physical act of transferring a copy,; and you may at your option offer warranty protection in exchange for a; fee.; ; 2. You may modify your copy or copies of the Library or any portion; of it, thus forming a work based on the Library, and copy and; distribute such modifications or work under the terms of Section 1; above, provided that you also meet all of these conditions:. a) The modified work must itself be a software library. b) You must cause the files modified to carry prominent notices; stating that you changed the files and the date of any change. c) You must cause the whole of the work to be licensed at no; charge to all third parties under the terms of this License. d) If a facility in the modified Library refers to a function or a; table of data to be supplied by an application program that uses; the facility, other than as an argument passed when the facility; is invoked, then you must make a good faith effort to ensure that,; in the event an application does not supply such function or; table, the facility still operates, and performs whatever part of; its purpose remains meaningful. (For example, a function in a library to compute square roots has; a purpose that is entirely well-defined independent of the; application. Therefore, Subsection 2d requires that any; application-supplied function or table used by this function must; be optional: if the application does not supply it, the square; root function must still compute square roots.). These requirements apply to the modified work as a whole. If; identifiable sections of that work are not derived from the Library,; and can be reasonably considered independent and separate works in; themselves, then this License, and its terms, do not apply to those; sections when you distribute them as separate works. But when you; distribute the same sections as part of a whole which is a work based; on the Library",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/LGPL2_1.txt:9301,perform,performs,9301,LGPL2_1.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/LGPL2_1.txt,1,['perform'],['performs']
Performance,"rray[2]``. The data in the referenced stream is a; debug data directory of type ``IMAGE_DEBUG_TYPE_FIXUP``. **Omap To Src Data** - ``DbgStreamArray[3]``. The data in the referenced stream; is a debug data directory of type ``IMAGE_DEBUG_TYPE_OMAP_TO_SRC``. This; is used for mapping addresses between instrumented and uninstrumented code. **Omap From Src Data** - ``DbgStreamArray[4]``. The data in the referenced stream; is a debug data directory of type ``IMAGE_DEBUG_TYPE_OMAP_FROM_SRC``. This; is used for mapping addresses between instrumented and uninstrumented code. **Section Header Data** - ``DbgStreamArray[5]``. A dump of all section headers from; the original executable. **Token / RID Map** - ``DbgStreamArray[6]``. The layout of this stream is not; understood, but it is assumed to be a mapping from ``CLR Token`` to; ``CLR Record ID``. Refer to `ECMA 335 <http://www.ecma-international.org/publications/standards/Ecma-335.htm>`__; for more information. **Xdata** - ``DbgStreamArray[7]``. A copy of the ``.xdata`` section from the; executable. **Pdata** - ``DbgStreamArray[8]``. This is assumed to be a copy of the ``.pdata``; section from the executable, but that would make it identical to; ``DbgStreamArray[1]``. The difference between these two indices is not well; understood. **New FPO Data** - ``DbgStreamArray[9]``. The data in the referenced stream is a; debug data directory of type ``IMAGE_DEBUG_TYPE_FPO``. Note that this is different; from ``DbgStreamArray[0]`` in that ``.debug$F`` sections are only emitted by MASM.; Thus, it is possible for both to appear in the same PDB if both MASM object files; and cl object files are linked into the same program. **Original Section Header Data** - ``DbgStreamArray[10]``. Similar to; ``DbgStreamArray[5]``, but contains the section headers before any binary translation; has been performed. This can be used in conjunction with ``DebugStreamArray[3]``; and ``DbgStreamArray[4]`` to map instrumented and uninstrumented addresses.; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/PDB/DbiStream.rst:18574,perform,performed,18574,interpreter/llvm-project/llvm/docs/PDB/DbiStream.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/PDB/DbiStream.rst,1,['perform'],['performed']
Performance,"rrible and we suffer as a result:. e.g.; struct s {; double d1;; int s1;; };. void foo(struct s S) {; printf(""%g, %d\n"", S.d1, S.s1);; }. 'S' is passed via registers r0, r1, r2. But gcc stores them to the stack, and; then reload them to r1, r2, and r3 before issuing the call (r0 contains the; address of the format string):. 	stmfd	sp!, {r7, lr}; 	add	r7, sp, #0; 	sub	sp, sp, #12; 	stmia	sp, {r0, r1, r2}; 	ldmia	sp, {r1-r2}; 	ldr	r0, L5; 	ldr	r3, [sp, #8]; L2:; 	add	r0, pc, r0; 	bl	L_printf$stub. Instead of a stmia, ldmia, and a ldr, wouldn't it be better to do three moves?. * Return an aggregate type is even worse:. e.g.; struct s foo(void) {; struct s S = {1.1, 2};; return S;; }. 	mov	ip, r0; 	ldr	r0, L5; 	sub	sp, sp, #12; L2:; 	add	r0, pc, r0; 	@ lr needed for prologue; 	ldmia	r0, {r0, r1, r2}; 	stmia	sp, {r0, r1, r2}; 	stmia	ip, {r0, r1, r2}; 	mov	r0, ip; 	add	sp, sp, #12; 	bx	lr. r0 (and later ip) is the hidden parameter from caller to store the value in. The; first ldmia loads the constants into r0, r1, r2. The last stmia stores r0, r1,; r2 into the address passed in. However, there is one additional stmia that; stores r0, r1, and r2 to some stack location. The store is dead. The llvm-gcc generated code looks like this:. csretcc void %foo(%struct.s* %agg.result) {; entry:; 	%S = alloca %struct.s, align 4		; <%struct.s*> [#uses=1]; 	%memtmp = alloca %struct.s		; <%struct.s*> [#uses=1]; 	cast %struct.s* %S to sbyte*		; <sbyte*>:0 [#uses=2]; 	call void %llvm.memcpy.i32( sbyte* %0, sbyte* cast ({ double, int }* %C.0.904 to sbyte*), uint 12, uint 4 ); 	cast %struct.s* %agg.result to sbyte*		; <sbyte*>:1 [#uses=2]; 	call void %llvm.memcpy.i32( sbyte* %1, sbyte* %0, uint 12, uint 0 ); 	cast %struct.s* %memtmp to sbyte*		; <sbyte*>:2 [#uses=1]; 	call void %llvm.memcpy.i32( sbyte* %2, sbyte* %1, uint 12, uint 0 ); 	ret void; }. llc ends up issuing two memcpy's (the first memcpy becomes 3 loads from; constantpool). Perhaps we should 1) fix llvm-gcc so the memcpy is trans",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:6184,load,loads,6184,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,2,['load'],['loads']
Performance,rs and arguments; Unknown. 1771; CD6; Restricted lookup in nested-name-specifier; Unknown. 1772; C++14; __func__ in a lambda body; Clang 14. 1773; C++14; Out-of-lifetime lvalue-to-rvalue conversion; Unknown. 1774; CD4; Discrepancy between subobject destruction and stack unwinding; Unknown. 1775; C++14; Undefined behavior of line splice in raw string literal; Unknown. 1776; CD4; Replacement of class objects containing reference members; Unknown. 1777; CD4; Empty pack expansion in dynamic-exception-specification; Unknown. 1778; C++14; exception-specification in explicitly-defaulted functions; Clang 9. 1779; CD4; Type dependency of __func__; Clang 14. 1780; CD4; Explicit instantiation/specialization of generic lambda operator(); Unknown. 1781; CD5; Converting from nullptr_t to bool in overload resolution; Unknown. 1782; CD4; Form of initialization for nullptr_t to bool conversion; Unknown. 1783; NAD; Why are virtual destructors non-trivial?; Unknown. 1784; C++17; Concurrent execution during static local initialization; Unknown. 1785; NAD; Conflicting diagnostic requirements for template definitions; Unknown. 1786; C++14; Effect of merging allocations on memory leakage; Unknown. 1787; C++14; Uninitialized unsigned char values; Unknown. 1788; CD4; Sized deallocation of array of non-class type; Unknown. 1789; open; Array reference vs array decay in overload resolution; Not resolved. 1790; open; Ellipsis following function parameter pack; Not resolved. 1791; CD4; Incorrect restrictions on cv-qualifier-seq and ref-qualifier; Unknown. 1792; NAD; Incorrect example of explicit specialization of member enumeration; Unknown. 1793; CD4; thread_local in explicit specializations; Unknown. 1794; C++17; template keyword and alias templates; Yes. 1795; CD4; Disambiguating original-namespace-definition and extension-namespace-definition; Unknown. 1796; CD4; Is all-bits-zero for null characters a meaningful requirement?; Unknown. 1797; CD4; Are all bit patterns of unsigned char distinct ,MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/cxx_dr_status.html:121027,Concurren,Concurrent,121027,interpreter/llvm-project/clang/www/cxx_dr_status.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/cxx_dr_status.html,1,['Concurren'],['Concurrent']
Performance,"rs or so, it will place a profdata file in your; build directory. This takes a really long time because it builds clang twice,; and you *must* have compiler-rt in your build tree. This process uses any source files under the perf-training directory as training; data as long as the source files are marked up with LIT-style RUN lines. After it finishes you can use :code:`find . -name clang.profdata` to find it, but it; should be at a path something like:. .. code-block:: console. <build dir>/tools/clang/stage2-instrumented-bins/utils/perf-training/clang.profdata. You can feed that file into the LLVM_PROFDATA_FILE option when you build your; optimized compiler. It may be necessary to build additional targets before running perf training, such as; builtins and runtime libraries. You can use the :code:`CLANG_PGO_TRAINING_DEPS` CMake; variable for that purpose:. .. code-block:: cmake. set(CLANG_PGO_TRAINING_DEPS builtins runtimes CACHE STRING """"). The PGO cache has a slightly different stage naming scheme than other; multi-stage builds. It generates three stages: stage1, stage2-instrumented, and; stage2. Both of the stage2 builds are built using the stage1 compiler. The PGO cache generates the following additional targets:. **stage2-instrumented**; Builds a stage1 compiler, runtime, and required tools (llvm-config,; llvm-profdata) then uses that compiler to build an instrumented stage2 compiler. **stage2-instrumented-generate-profdata**; Depends on stage2-instrumented and will use the instrumented compiler to; generate profdata based on the training files in clang/utils/perf-training. **stage2**; Depends on stage2-instrumented-generate-profdata and will use the stage1; compiler with the stage2 profdata to build a PGO-optimized compiler. **stage2-check-llvm**; Depends on stage2 and runs check-llvm using the stage2 compiler. **stage2-check-clang**; Depends on stage2 and runs check-clang using the stage2 compiler. **stage2-check-all**; Depends on stage2 and runs check-all usi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AdvancedBuilds.rst:8627,cache,cache,8627,interpreter/llvm-project/llvm/docs/AdvancedBuilds.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AdvancedBuilds.rst,1,['cache'],['cache']
Performance,"rs to the module, memory manager and target machine; objects, all of which will subsequently be owned by the MCJIT object. The MCJIT class has a member variable, Dyld, which contains an instance of; the RuntimeDyld wrapper class. This member will be used for; communications between MCJIT and the actual RuntimeDyldImpl object that; gets created when an object is loaded. .. image:: MCJIT-creation.png. Upon creation, MCJIT holds a pointer to the Module object that it received; from EngineBuilder but it does not immediately generate code for this; module. Code generation is deferred until either the; MCJIT::finalizeObject method is called explicitly or a function such as; MCJIT::getPointerToFunction is called which requires the code to have been; generated. Code Generation; ===============. When code generation is triggered, as described above, MCJIT will first; attempt to retrieve an object image from its ObjectCache member, if one; has been set. If a cached object image cannot be retrieved, MCJIT will; call its emitObject method. MCJIT::emitObject uses a local PassManager; instance and creates a new ObjectBufferStream instance, both of which it; passes to TargetMachine::addPassesToEmitMC before calling PassManager::run; on the Module with which it was created. .. image:: MCJIT-load.png. The PassManager::run call causes the MC code generation mechanisms to emit; a complete relocatable binary object image (either in either ELF or MachO; format, depending on the target) into the ObjectBufferStream object, which; is flushed to complete the process. If an ObjectCache is being used, the; image will be passed to the ObjectCache here. At this point, the ObjectBufferStream contains the raw object image.; Before the code can be executed, the code and data sections from this; image must be loaded into suitable memory, relocations must be applied and; memory permission and code cache invalidation (if required) must be completed. Object Loading; ==============. Once an object imag",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst:2501,cache,cached,2501,interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,1,['cache'],['cached']
Performance,"rs' arguments are simply the arguments which need to; be passed to the call target. They will be lowered according to the; specified calling convention and otherwise handled like a normal call; instruction. The number of arguments must exactly match what is; specified in '# call args'. The types must match the signature of; 'target'. The 'call parameter' attributes must be followed by two 'i64 0' constants.; These were originally the length prefixes for 'gc transition parameter' and; 'deopt parameter' arguments, but the role of these parameter sets have been; entirely replaced with the corresponding operand bundles. In a future; revision, these now redundant arguments will be removed. Semantics:; """""""""""""""""""". A statepoint is assumed to read and write all memory. As a result,; memory operations can not be reordered past a statepoint. It is; illegal to mark a statepoint as being either 'readonly' or 'readnone'. Note that legal IR can not perform any memory operation on a 'gc; pointer' argument of the statepoint in a location statically reachable; from the statepoint. Instead, the explicitly relocated value (from a; ``gc.relocate``) must be used. '``llvm.experimental.gc.result``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare type; @llvm.experimental.gc.result(token %statepoint_token). Overview:; """""""""""""""""". ``gc.result`` extracts the result of the original call instruction; which was replaced by the ``gc.statepoint``. The ``gc.result``; intrinsic is actually a family of three intrinsics due to an; implementation limitation. Other than the type of the return value,; the semantics are the same. Operands:; """""""""""""""""". The first and only argument is the ``gc.statepoint`` which starts; the safepoint sequence of which this ``gc.result`` is a part.; Despite the typing of this as a generic token, *only* the value defined; by a ``gc.statepoint`` is legal here. Semantics:; """""""""""""""""""". The ``gc.result`` represents the return value of the call t",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:503476,perform,perform,503476,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['perform']
Performance,"rs, its optimizations can; cause the owning object to be reclaimed too soon. This attribute informs ARC; that it must tread lightly. The extension rules are somewhat intentionally vague. The autorelease pool; limit is there to permit a simple implementation to simply retain and; autorelease the receiver. The other limit permits some amount of; optimization. The phrase ""derived from"" is intended to encompass the results; both of pointer transformations, such as casts and arithmetic, and of loading; from such derived pointers; furthermore, it applies whether or not such; derivations are applied directly in the calling code or by other utility code; (for example, the C library routine ``strchr``). However, the implementation; never need account for uses after a return from the code which calls the; method returning an interior pointer. As an exception, no extension is required if the receiver is loaded directly; from a ``__strong`` object with :ref:`precise lifetime semantics; <arc.optimization.precise>`. .. admonition:: Rationale. Implicit autoreleases carry the risk of significantly inflating memory use,; so it's important to provide users a way of avoiding these autoreleases.; Tying this to precise lifetime semantics is ideal, as for local variables; this requires a very explicit annotation, which allows ARC to trust the user; with good cheer. .. _arc.misc.c-retainable:. C retainable pointer types; --------------------------. A type is a :arc-term:`C retainable pointer type` if it is a pointer to; (possibly qualified) ``void`` or a pointer to a (possibly qualifier) ``struct``; or ``class`` type. .. admonition:: Rationale. ARC does not manage pointers of CoreFoundation type (or any of the related; families of retainable C pointers which interoperate with Objective-C for; retain/release operation). In fact, ARC does not even know how to; distinguish these types from arbitrary C pointer types. The intent of this; concept is to filter out some obviously non-object types ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst:101523,optimiz,optimization,101523,interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,1,['optimiz'],['optimization']
Performance,"rst vector argument ``%A`` corresponds to a matrix with ``<OuterRows> *; <Inner>`` elements, and the second argument ``%B`` to a matrix with; ``<Inner> * <OuterColumns>`` elements. Arguments ``<OuterRows>``,; ``<Inner>`` and ``<OuterColumns>`` must be positive, constant integers. The; returned vector must have ``<OuterRows> * <OuterColumns>`` elements.; Vectors ``%A``, ``%B``, and the returned vector all have the same float or; integer element type. '``llvm.matrix.column.major.load.*``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare vectorty @llvm.matrix.column.major.load.*(; ptrty %Ptr, i64 %Stride, i1 <IsVolatile>, i32 <Rows>, i32 <Cols>). Overview:; """""""""""""""""". The '``llvm.matrix.column.major.load.*``' intrinsics load a ``<Rows> x <Cols>``; matrix using a stride of ``%Stride`` to compute the start address of the; different columns. The offset is computed using ``%Stride``'s bitwidth. This; allows for convenient loading of sub matrixes. If ``<IsVolatile>`` is true, the; intrinsic is considered a :ref:`volatile memory access <volatile>`. The result; matrix is returned in the result vector. If the ``%Ptr`` argument is known to; be aligned to some boundary, this can be specified as an attribute on the; argument. Arguments:; """""""""""""""""""". The first argument ``%Ptr`` is a pointer type to the returned vector type, and; corresponds to the start address to load from. The second argument ``%Stride``; is a positive, constant integer with ``%Stride >= <Rows>``. ``%Stride`` is used; to compute the column memory addresses. I.e., for a column ``C``, its start; memory addresses is calculated with ``%Ptr + C * %Stride``. The third Argument; ``<IsVolatile>`` is a boolean value. The fourth and fifth arguments,; ``<Rows>`` and ``<Cols>``, correspond to the number of rows and columns,; respectively, and must be positive, constant integers. The returned vector must; have ``<Rows> * <Cols>`` elements. The :ref:`ali",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:678126,load,loading,678126,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['loading']
Performance,rt/PostfixOperatorCheck.h; clang-tools-extra/clang-tidy/cert/ProperlySeededRandomGeneratorCheck.cpp; clang-tools-extra/clang-tidy/cert/ProperlySeededRandomGeneratorCheck.h; clang-tools-extra/clang-tidy/cert/SetLongJmpCheck.cpp; clang-tools-extra/clang-tidy/cert/SetLongJmpCheck.h; clang-tools-extra/clang-tidy/cert/StaticObjectExceptionCheck.cpp; clang-tools-extra/clang-tidy/cert/StaticObjectExceptionCheck.h; clang-tools-extra/clang-tidy/cert/StrToNumCheck.cpp; clang-tools-extra/clang-tidy/cert/StrToNumCheck.h; clang-tools-extra/clang-tidy/cert/ThrownExceptionTypeCheck.cpp; clang-tools-extra/clang-tidy/cert/ThrownExceptionTypeCheck.h; clang-tools-extra/clang-tidy/cert/VariadicFunctionDefCheck.cpp; clang-tools-extra/clang-tidy/cert/VariadicFunctionDefCheck.h; clang-tools-extra/clang-tidy/concurrency/MtUnsafeCheck.cpp; clang-tools-extra/clang-tidy/concurrency/MtUnsafeCheck.h; clang-tools-extra/clang-tidy/concurrency/ThreadCanceltypeAsynchronousCheck.cpp; clang-tools-extra/clang-tidy/concurrency/ThreadCanceltypeAsynchronousCheck.h; clang-tools-extra/clang-tidy/cppcoreguidelines/AvoidGotoCheck.cpp; clang-tools-extra/clang-tidy/cppcoreguidelines/AvoidGotoCheck.h; clang-tools-extra/clang-tidy/cppcoreguidelines/AvoidNonConstGlobalVariablesCheck.cpp; clang-tools-extra/clang-tidy/cppcoreguidelines/AvoidNonConstGlobalVariablesCheck.h; clang-tools-extra/clang-tidy/cppcoreguidelines/CppCoreGuidelinesTidyModule.cpp; clang-tools-extra/clang-tidy/cppcoreguidelines/InitVariablesCheck.cpp; clang-tools-extra/clang-tidy/cppcoreguidelines/InitVariablesCheck.h; clang-tools-extra/clang-tidy/cppcoreguidelines/InterfacesGlobalInitCheck.cpp; clang-tools-extra/clang-tidy/cppcoreguidelines/InterfacesGlobalInitCheck.h; clang-tools-extra/clang-tidy/cppcoreguidelines/MacroUsageCheck.cpp; clang-tools-extra/clang-tidy/cppcoreguidelines/MacroUsageCheck.h; clang-tools-extra/clang-tidy/cppcoreguidelines/NarrowingConversionsCheck.cpp; clang-tools-extra/clang-tidy/cppcoreguidelines/NarrowingConversionsCh,MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/tools/clang-formatted-files.txt:51403,concurren,concurrency,51403,interpreter/llvm-project/clang/docs/tools/clang-formatted-files.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/tools/clang-formatted-files.txt,1,['concurren'],['concurrency']
Performance,"rtain kinds of code transformations can inadvertently result in a loss of; debug info, or worse, make debug info misrepresent the state of a program. This document specifies how to correctly update debug info in various kinds of; code transformations, and offers suggestions for how to create targeted debug; info tests for arbitrary transformations. For more on the philosophy behind LLVM debugging information, see; :doc:`SourceLevelDebugging`. Rules for updating debug locations; ==================================. .. _WhenToPreserveLocation:. When to preserve an instruction location; ----------------------------------------. A transformation should preserve the debug location of an instruction if the; instruction either remains in its basic block, or if its basic block is folded; into a predecessor that branches unconditionally. The APIs to use are; ``IRBuilder``, or ``Instruction::setDebugLoc``. The purpose of this rule is to ensure that common block-local optimizations; preserve the ability to set breakpoints on source locations corresponding to; the instructions they touch. Debugging, crash logs, and SamplePGO accuracy; would be severely impacted if that ability were lost. Examples of transformations that should follow this rule include:. * Instruction scheduling. Block-local instruction reordering should not drop; source locations, even though this may lead to jumpy single-stepping; behavior. * Simple jump threading. For example, if block ``B1`` unconditionally jumps to; ``B2``, *and* is its unique predecessor, instructions from ``B2`` can be; hoisted into ``B1``. Source locations from ``B2`` should be preserved. * Peephole optimizations that replace or expand an instruction, like ``(add X; X) => (shl X 1)``. The location of the ``shl`` instruction should be the same; as the location of the ``add`` instruction. * Tail duplication. For example, if blocks ``B1`` and ``B2`` both; unconditionally branch to ``B3`` and ``B3`` can be folded into its; predecessors, sourc",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToUpdateDebugInfo.rst:1196,optimiz,optimizations,1196,interpreter/llvm-project/llvm/docs/HowToUpdateDebugInfo.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToUpdateDebugInfo.rst,1,['optimiz'],['optimizations']
Performance,"rth between pointers with different; pointee types. The pointee type does not necessarily represent the actual; underlying type in memory. In other words, the pointee type carries no real; semantics. Historically LLVM was some sort of type-safe subset of C. Having pointee types; provided an extra layer of checks to make sure that the Clang frontend matched; its frontend values/operations with the corresponding LLVM IR. However, as other; languages like C++ adopted LLVM, the community realized that pointee types were; more of a hindrance for LLVM development and that the extra type checking with; some frontends wasn't worth it. LLVM's type system was `originally designed; <https://llvm.org/pubs/2003-05-01-GCCSummit2003.html>`_ to support high-level; optimization. However, years of LLVM implementation experience have demonstrated; that the pointee type system design does not effectively support; optimization. Memory optimization algorithms, such as SROA, GVN, and AA,; generally need to look through LLVM's struct types and reason about the; underlying memory offsets. The community realized that pointee types hinder LLVM; development, rather than helping it. Some of the initially proposed high-level; optimizations have evolved into `TBAA; <https://llvm.org/docs/LangRef.html#tbaa-metadata>`_ due to limitations with; representing higher-level language information directly via SSA values. Pointee types provide some value to frontends because the IR verifier uses types; to detect straightforward type confusion bugs. However, frontends also have to; deal with the complexity of inserting bitcasts everywhere that they might be; required. The community consensus is that the costs of pointee types; outweight the benefits, and that they should be removed. Many operations do not actually care about the underlying type. These; operations, typically intrinsics, usually end up taking an arbitrary pointer; type ``i8*`` and sometimes a size. This causes lots of redundant no-op bitcasts",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:2627,optimiz,optimization,2627,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst,1,['optimiz'],['optimization']
Performance,"rtual call optimization pass will optimize away ``llvm.type.checked.load``; intrinsics associated with devirtualized calls, thereby removing the type; check in cases where it is not needed to enforce the control flow integrity; constraint. If the given pointer is associated with a type metadata identifier, this; function returns true as the second element of its return value. (Note that; the function may also return true if the given pointer is not associated; with a type metadata identifier.) If the function's return value's second; element is true, the following rules apply to the first element:. - If the given pointer is associated with the given type metadata identifier,; it is the function pointer loaded from the given byte offset from the given; pointer. - If the given pointer is not associated with the given type metadata; identifier, it is one of the following (the choice of which is unspecified):. 1. The function pointer that would have been loaded from an arbitrarily chosen; (through an unspecified mechanism) pointer associated with the type; metadata. 2. If the function has a non-void return type, a pointer to a function that; returns an unspecified value without causing side effects. If the function's return value's second element is false, the value of the; first element is undefined. .. _type.checked.load.relative:. '``llvm.type.checked.load.relative``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare {ptr, i1} @llvm.type.checked.load.relative(ptr %ptr, i32 %offset, metadata %type) argmemonly nounwind readonly. Overview:; """""""""""""""""". The ``llvm.type.checked.load.relative`` intrinsic loads a relative pointer to a; function from a virtual table pointer using metadata. Otherwise, its semantic is; identical to the ``llvm.type.checked.load`` intrinsic. A relative pointer is a pointer to an offset to the pointed to value. The; address of the underlying pointer of the relative pointer is obtained by adding; the offset t",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:939397,load,loaded,939397,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['loaded']
Performance,"ructions, or by flat instructions. Multi-dword; access is not supported except by flat and scratch instructions in; GFX9-GFX11. Code that manipulates the stack values in other lanes of a wavefront,; such as by ``addrspacecast``-ing stack pointers to generic ones and taking offsets; that reach other lanes or by explicitly constructing the scratch buffer descriptor,; triggers undefined behavior when it modifies the scratch values of other lanes.; The compiler may assume that such modifications do not occur.; When using code object V5 ``LIBOMPTARGET_STACK_SIZE`` may be used to provide the; private segment size in bytes, for cases where a dynamic stack is used. **Constant 32-bit**; *TODO*. **Buffer Fat Pointer**; The buffer fat pointer is an experimental address space that is currently; unsupported in the backend. It exposes a non-integral pointer that is in; the future intended to support the modelling of 128-bit buffer descriptors; plus a 32-bit offset into the buffer (in total encapsulating a 160-bit; *pointer*), allowing normal LLVM load/store/atomic operations to be used to; model the buffer descriptors used heavily in graphics workloads targeting; the backend. The buffer descriptor used to construct a buffer fat pointer must be *raw*:; the stride must be 0, the ""add tid"" flag must be 0, the swizzle enable bits; must be off, and the extent must be measured in bytes. (On subtargets where; bounds checking may be disabled, buffer fat pointers may choose to enable; it or not). **Buffer Resource**; The buffer resource pointer, in address space 8, is the newer form; for representing buffer descriptors in AMDGPU IR, replacing their; previous representation as `<4 x i32>`. It is a non-integral pointer; that represents a 128-bit buffer descriptor resource (`V#`). Since, in general, a buffer resource supports complex addressing modes that cannot; be easily represented in LLVM (such as implicit swizzled access to structured; buffers), it is **illegal** to perform non-trivial a",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:28857,load,load,28857,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"rumented compiler runs it will output a bunch of files containing; performance counters (.profraw files). After generating all the profraw files; you use llvm-profdata to merge the files into a single profdata file that you; can feed into the LLVM_PROFDATA_FILE option. Our PGO.cmake cache automates that whole process. You can use it for; configuration with CMake with the following command:. .. code-block:: console. $ cmake -G Ninja -C <path to source>/clang/cmake/caches/PGO.cmake \; <path to source>/llvm. There are several additional options that the cache file also accepts to modify; the build, particularly the PGO_INSTRUMENT_LTO option. Setting this option to; Thin or Full will enable ThinLTO or full LTO respectively, further enhancing; the performance gains from a PGO build by enabling interprocedural; optimizations. For example, to run a CMake configuration for a PGO build; that also enables ThinTLO, use the following command:. .. code-block:: console. $ cmake -G Ninja -C <path to source>/clang/cmake/caches/PGO.cmake \; -DPGO_INSTRUMENT_LTO=Thin \; <path to source>/llvm. By default, clang will generate profile data by compiling a simple; hello world program. You can also tell clang use an external; project for generating profile data that may be a better fit for your; use case. The project you specify must either be a lit test suite; (use the CLANG_PGO_TRAINING_DATA option) or a CMake project (use the; CLANG_PERF_TRAINING_DATA_SOURCE_DIR option). For example, If you wanted to use the; `LLVM Test Suite <https://github.com/llvm/llvm-test-suite/>`_ to generate; profile data you would use the following command:. .. code-block:: console. $ cmake -G Ninja -C <path to source>/clang/cmake/caches/PGO.cmake \; -DBOOTSTRAP_CLANG_PGO_TRAINING_DATA_SOURCE_DIR=<path to llvm-test-suite> \; -DBOOTSTRAP_CLANG_PGO_TRAINING_DEPS=runtimes. The BOOTSTRAP\_ prefixes tells CMake to pass the variables on to the instrumented; stage two build. And the CLANG_PGO_TRAINING_DEPS option let's ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AdvancedBuilds.rst:6124,cache,caches,6124,interpreter/llvm-project/llvm/docs/AdvancedBuilds.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AdvancedBuilds.rst,1,['cache'],['caches']
Performance,"run through clang -O2 -S -emit-llvm to get; equivalent IR */; int x;; void f(int* a) {; for (int i = 0; i < 100; i++) {; if (a[i]); x += 1;; }; }. The following is equivalent in non-concurrent situations:. .. code-block:: c. int x;; void f(int* a) {; int xtemp = x;; for (int i = 0; i < 100; i++) {; if (a[i]); xtemp += 1;; }; x = xtemp;; }. However, LLVM is not allowed to transform the former to the latter: it could; indirectly introduce undefined behavior if another thread can access ``x`` at; the same time. That thread would read `undef` instead of the value it was; expecting, which can lead to undefined behavior down the line. (This example is; particularly of interest because before the concurrency model was implemented,; LLVM would perform this transformation.). Note that speculative loads are allowed; a load which is part of a race returns; ``undef``, but does not have undefined behavior. Atomic instructions; ===================. For cases where simple loads and stores are not sufficient, LLVM provides; various atomic instructions. The exact guarantees provided depend on the; ordering; see `Atomic orderings`_. ``load atomic`` and ``store atomic`` provide the same basic functionality as; non-atomic loads and stores, but provide additional guarantees in situations; where threads and signals are involved. ``cmpxchg`` and ``atomicrmw`` are essentially like an atomic load followed by an; atomic store (where the store is conditional for ``cmpxchg``), but no other; memory operation can happen on any thread between the load and store. A ``fence`` provides Acquire and/or Release ordering which is not part; of another operation; it is normally used along with Monotonic memory; operations. A Monotonic load followed by an Acquire fence is roughly; equivalent to an Acquire load, and a Monotonic store following a; Release fence is roughly equivalent to a Release; store. SequentiallyConsistent fences behave as both an Acquire and a; Release fence, and additionally provide a t",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst:4210,load,loads,4210,interpreter/llvm-project/llvm/docs/Atomics.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst,1,['load'],['loads']
Performance,"run-time requires compiler support to; locate all places that hold live pointer variables at run-time, including the; :ref:`processor stack and registers <gcroot>`. Conservative garbage collection is attractive because it does not require any; special compiler support, but it does have problems. In particular, because the; conservative garbage collector cannot *know* that a particular word in the; machine is a pointer, it cannot move live objects in the heap (preventing the; use of compacting and generational GC algorithms) and it can occasionally suffer; from memory leaks due to integer values that happen to point to objects in the; program. In addition, some aggressive compiler transformations can break; conservative garbage collectors (though these seem rare in practice). Accurate garbage collectors do not suffer from any of these problems, but they; can suffer from degraded scalar optimization of the program. In particular,; because the runtime must be able to identify and update all pointers active in; the program, some optimizations are less effective. In practice, however, the; locality and performance benefits of using aggressive garbage collection; techniques dominates any low-level losses. This document describes the mechanisms and interfaces provided by LLVM to; support accurate garbage collection. Goals and non-goals; -------------------. LLVM's intermediate representation provides :ref:`garbage collection intrinsics; <gc_intrinsics>` that offer support for a broad class of collector models. For; instance, the intrinsics permit:. * semi-space collectors. * mark-sweep collectors. * generational collectors. * incremental collectors. * concurrent collectors. * cooperative collectors. * reference counting. We hope that the support built into the LLVM IR is sufficient to support a; broad class of garbage collected languages including Scheme, ML, Java, C#,; Perl, Python, Lua, Ruby, other scripting languages, and more. Note that LLVM **does not itself provide a",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GarbageCollection.rst:4702,optimiz,optimizations,4702,interpreter/llvm-project/llvm/docs/GarbageCollection.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GarbageCollection.rst,1,['optimiz'],['optimizations']
Performance,"runcStoreAction`` --- Truncating store.; * ``setIndexedLoadAction`` --- Indexed load.; * ``setIndexedStoreAction`` --- Indexed store.; * ``setConvertAction`` --- Type conversion.; * ``setCondCodeAction`` --- Support for a given condition code. Note: on older releases, ``setLoadXAction`` is used instead of; ``setLoadExtAction``. Also, on older releases, ``setCondCodeAction`` may not; be supported. Examine your release to see what methods are specifically; supported. These callbacks are used to determine that an operation does or does not work; with a specified type (or types). And in all cases, the third parameter is a; ``LegalAction`` type enum value: ``Promote``, ``Expand``, ``Custom``, or; ``Legal``. ``SparcISelLowering.cpp`` contains examples of all four; ``LegalAction`` values. Promote; ^^^^^^^. For an operation without native support for a given type, the specified type; may be promoted to a larger type that is supported. For example, SPARC does; not support a sign-extending load for Boolean values (``i1`` type), so in; ``SparcISelLowering.cpp`` the third parameter below, ``Promote``, changes; ``i1`` type values to a large type before loading. .. code-block:: c++. setLoadExtAction(ISD::SEXTLOAD, MVT::i1, Promote);. Expand; ^^^^^^. For a type without native support, a value may need to be broken down further,; rather than promoted. For an operation without native support, a combination; of other operations may be used to similar effect. In SPARC, the; floating-point sine and cosine trig operations are supported by expansion to; other operations, as indicated by the third parameter, ``Expand``, to; ``setOperationAction``:. .. code-block:: c++. setOperationAction(ISD::FSIN, MVT::f32, Expand);; setOperationAction(ISD::FCOS, MVT::f32, Expand);. Custom; ^^^^^^. For some operations, simple type promotion or operation expansion may be; insufficient. In some cases, a special intrinsic function must be implemented. For example, a constant value may require special treatme",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMBackend.rst:58786,load,load,58786,interpreter/llvm-project/llvm/docs/WritingAnLLVMBackend.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMBackend.rst,1,['load'],['load']
Performance,"runtime stack, returning a pointer of the; appropriate type to the program. If ""NumElements"" is specified, it is; the number of elements allocated, otherwise ""NumElements"" is defaulted; to be one. If a constant alignment is specified, the value result of the; allocation is guaranteed to be aligned to at least that boundary. The; alignment may not be greater than ``1 << 32``. The alignment is only optional when parsing textual IR; for in-memory IR,; it is always present. If not specified, the target can choose to align the; allocation on any convenient boundary compatible with the type. '``type``' may be any sized type. Structs containing scalable vectors cannot be used in allocas unless all; fields are the same scalable vector type (e.g. ``{<vscale x 2 x i32>,; <vscale x 2 x i32>}`` contains the same type while ``{<vscale x 2 x i32>,; <vscale x 2 x i64>}`` doesn't). Semantics:; """""""""""""""""""". Memory is allocated; a pointer is returned. The allocated memory is; uninitialized, and loading from uninitialized memory produces an undefined; value. The operation itself is undefined if there is insufficient stack; space for the allocation.'``alloca``'d memory is automatically released; when the function returns. The '``alloca``' instruction is commonly used; to represent automatic variables that must have an address available. When; the function returns (either with the ``ret`` or ``resume`` instructions),; the memory is reclaimed. Allocating zero bytes is legal, but the returned; pointer may not be unique. The order in which memory is allocated (ie.,; which way the stack grows) is not specified. Note that '``alloca``' outside of the alloca address space from the; :ref:`datalayout string<langref_datalayout>` is meaningful only if the; target has assigned it a semantics. If the returned pointer is used by :ref:`llvm.lifetime.start <int_lifestart>`,; the returned object is initially dead.; See :ref:`llvm.lifetime.start <int_lifestart>` and; :ref:`llvm.lifetime.end <int_lifeend>`",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:410528,load,loading,410528,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['loading']
Performance,"rve the condition code which is important if the spill; is between a cmp and a bcc instruction. However, we can use the (potentially); cheaper sequence if we know it's ok to clobber the condition register. add r2, sp, #255 * 4; add r2, #132; ldr r2, [r2, #7 * 4]. This is especially bad when dynamic alloca is used. The all fixed size stack; objects are referenced off the frame pointer with negative offsets. See; oggenc for an example. //===---------------------------------------------------------------------===//. Poor codegen test/CodeGen/ARM/select.ll f7:. 	ldr r5, LCPI1_0; LPC0:; 	add r5, pc; 	ldr r6, LCPI1_1; 	ldr r2, LCPI1_2; 	mov r3, r6; 	mov lr, pc; 	bx r5. //===---------------------------------------------------------------------===//. Make register allocator / spiller smarter so we can re-materialize ""mov r, imm"",; etc. Almost all Thumb instructions clobber condition code. //===---------------------------------------------------------------------===//. Thumb load / store address mode offsets are scaled. The values kept in the; instruction operands are pre-scale values. This probably ought to be changed; to avoid extra work when we convert Thumb2 instructions to Thumb1 instructions. //===---------------------------------------------------------------------===//. We need to make (some of the) Thumb1 instructions predicable. That will allow; shrinking of predicated Thumb2 instructions. To allow this, we need to be able; to toggle the 's' bit since they do not set CPSR when they are inside IT blocks. //===---------------------------------------------------------------------===//. Make use of hi register variants of cmp: tCMPhir / tCMPZhir. //===---------------------------------------------------------------------===//. Thumb1 immediate field sometimes keep pre-scaled values. See; ThumbRegisterInfo::eliminateFrameIndex. This is inconsistent from ARM and; Thumb2. //===---------------------------------------------------------------------===//. Rather than having tB",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt:5203,load,load,5203,interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt,2,['load'],['load']
Performance,"rver QPS (using ThinLTO & PGO) | -62% | -29% | **1.8x** |. Below is a visualization of the microbenchmark suite results which helps show; the distribution of results that is somewhat lost in the summary. The y-axis is; a log-scale speedup ratio of load hardening relative to `lfence` (up -> faster; -> better). Each box-and-whiskers represents one microbenchmark which may have; many different metrics measured. The red line marks the median, the box marks; the first and third quartiles, and the whiskers mark the min and max. ![Microbenchmark result visualization](speculative_load_hardening_microbenchmarks.png). We don't yet have benchmark data on SPEC or the LLVM test suite, but we can; work on getting that. Still, the above should give a pretty clear; characterization of the performance, and specific benchmarks are unlikely to; reveal especially interesting properties. ### Future Work: Fine Grained Control and API-Integration. The performance overhead of this technique is likely to be very significant and; something users wish to control or reduce. There are interesting options here; that impact the implementation strategy used. One particularly appealing option is to allow both opt-in and opt-out of this; mitigation at reasonably fine granularity such as on a per-function basis,; including intelligent handling of inlining decisions -- protected code can be; prevented from inlining into unprotected code, and unprotected code will become; protected when inlined into protected code. For systems where only a limited; set of code is reachable by externally controlled inputs, it may be possible to; limit the scope of mitigation through such mechanisms without compromising the; application's overall security. The performance impact may also be focused in a; few key functions that can be hand-mitigated in ways that have lower; performance overhead while the remainder of the application receives automatic; protection. For both limiting the scope of mitigation or manually miti",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md:48879,perform,performance,48879,interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,1,['perform'],['performance']
Performance,"rves. Note that this is quite different from the goal of debug information,; which is a best-effort attempt to track the location of named; variables at every instruction. An important motivation for this design is to allow a runtime to; commandeer a stack frame when execution reaches an instruction address; associated with a stack map. The runtime must be able to rebuild a; stack frame and resume program execution using the information; provided by the stack map. For example, execution may resume in an; interpreter or a recompiled version of the same function. This usage restricts LLVM optimization. Clearly, LLVM must not move; stores across a stack map. However, loads must also be handled; conservatively. If the load may trigger an exception, hoisting it; above a stack map could be invalid. For example, the runtime may; determine that a load is safe to execute without a type check given; the current state of the type system. If the type system changes while; some activation of the load's function exists on the stack, the load; becomes unsafe. The runtime can prevent subsequent execution of that; load by immediately patching any stack map location that lies between; the current call site and the load (typically, the runtime would; simply patch all stack map locations to invalidate the function). If; the compiler had hoisted the load above the stack map, then the; program could crash before the runtime could take back control. To enforce these semantics, stackmap and patchpoint intrinsics are; considered to potentially read and write all memory. This may limit; optimization more than some clients desire. This limitation may be; avoided by marking the call site as ""readonly"". In the future we may; also allow meta-data to be added to the intrinsic call to express; aliasing, thereby allowing optimizations to hoist certain loads above; stack maps. Direct Stack Map Entries; ^^^^^^^^^^^^^^^^^^^^^^^^. As shown in :ref:`stackmap-section`, a Direct stack map location; record",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/StackMaps.rst:18297,load,load,18297,interpreter/llvm-project/llvm/docs/StackMaps.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/StackMaps.rst,2,['load'],['load']
Performance,"rview:; """""""""""""""""". The '``llvm.vector.reduce.fminimum.*``' intrinsics do a floating-point; ``MIN`` reduction of a vector, returning the result as a scalar. The return type; matches the element-type of the vector input. This instruction has the same comparison semantics as the '``llvm.minimum.*``'; intrinsic. That is, this intrinsic propagates NaNs and -0.0 is considered less; than +0.0. If any element of the vector is a NaN, the result is NaN. Arguments:; """"""""""""""""""""; The argument to this intrinsic must be a vector of floating-point values. '``llvm.vector.insert``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. ; Insert fixed type into scalable type; declare <vscale x 4 x float> @llvm.vector.insert.nxv4f32.v4f32(<vscale x 4 x float> %vec, <4 x float> %subvec, i64 <idx>); declare <vscale x 2 x double> @llvm.vector.insert.nxv2f64.v2f64(<vscale x 2 x double> %vec, <2 x double> %subvec, i64 <idx>). ; Insert scalable type into scalable type; declare <vscale x 4 x float> @llvm.vector.insert.nxv4f64.nxv2f64(<vscale x 4 x float> %vec, <vscale x 2 x float> %subvec, i64 <idx>). ; Insert fixed type into fixed type; declare <4 x double> @llvm.vector.insert.v4f64.v2f64(<4 x double> %vec, <2 x double> %subvec, i64 <idx>). Overview:; """""""""""""""""". The '``llvm.vector.insert.*``' intrinsics insert a vector into another vector; starting from a given index. The return type matches the type of the vector we; insert into. Conceptually, this can be used to build a scalable vector out of; non-scalable vectors, however this intrinsic can also be used on purely fixed; types. Scalable vectors can only be inserted into other scalable vectors. Arguments:; """""""""""""""""""". The ``vec`` is the vector which ``subvec`` will be inserted into.; The ``subvec`` is the vector that will be inserted. ``idx`` represents the starting element number at which ``subvec`` will be; inserted. ``idx`` must be a constant multiple of ``subvec``'s known minimum; vector len",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:662676,scalab,scalable,662676,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,2,['scalab'],['scalable']
Performance,"rwise it is; inactive. The result of the attribute is the value V. *Some targets may update the target architecture execution mask for regions; of code that must execute with different sets of lanes than the current; active lanes. For example, some code must execute with all lanes made; temporarily active.* ``DW_AT_LLVM_active_lane`` *allows the compiler to; provide the means to determine the source language active lanes at any; program location. Typically, this attribute will use a loclist to express; different locations of the active lane mask at different program locations.*. If not present and ``DW_AT_LLVM_lanes`` is greater than 1, then the target; architecture execution mask is used. 7. A ``DW_TAG_subprogram``, ``DW_TAG_inlined_subroutine``, or; ``DW_TAG_entry_point`` debugger information entry may have a; ``DW_AT_LLVM_iterations`` attribute whose value is an integer constant or a; DWARF expression E. Its value is the number of source language loop; iterations executing concurrently by the target architecture for a single; source language thread of execution. *A compiler may generate code that executes more than one iteration of a; source language loop concurrently using optimization techniques such as; software pipelining or SIMD vectorization. The number of concurrent; iterations may vary for different loop nests in the same subprogram.; Typically, this attribute will use a loclist to express different values at; different program locations.*. If the attribute is an integer constant, then the value is the constant. The; DWARF is ill-formed if the constant is less than or equal to 0. Otherwise, E is evaluated with a context that has a result kind of a; location description, an unspecified object, the compilation unit that; contains E, an empty initial stack, and other context elements corresponding; to the source language thread of execution upon which the user is focused,; if any. The DWARF is ill-formed if the result is not a location description; comprised ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst:165802,concurren,concurrently,165802,interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst,1,['concurren'],['concurrently']
Performance,"rwise we fail.; if(DEFINED CMAKE_CXX_STANDARD AND CMAKE_CXX_STANDARD LESS ${LLVM_REQUIRED_CXX_STANDARD}); message(FATAL_ERROR ""Requested CMAKE_CXX_STANDARD=${CMAKE_CXX_STANDARD} which is less than the required ${LLVM_REQUIRED_CXX_STANDARD}.""); endif(). set(CMAKE_CXX_STANDARD ${LLVM_REQUIRED_CXX_STANDARD} CACHE STRING ""C++ standard to conform to""); set(CMAKE_CXX_STANDARD_REQUIRED YES). if (CYGWIN); # Cygwin is a bit stricter and lack things like 'strdup', 'stricmp', etc in; # c++xx mode.; set(CMAKE_CXX_EXTENSIONS YES); else(); set(CMAKE_CXX_EXTENSIONS NO); endif(). if (NOT CMAKE_BUILD_TYPE AND NOT CMAKE_CONFIGURATION_TYPES); message(FATAL_ERROR ""; No build type selected. You need to pass -DCMAKE_BUILD_TYPE=<type> in order to configure LLVM.; Available options are:; * -DCMAKE_BUILD_TYPE=Release - For an optimized build with no assertions or debug info.; * -DCMAKE_BUILD_TYPE=Debug - For an unoptimized build with assertions and debug info.; * -DCMAKE_BUILD_TYPE=RelWithDebInfo - For an optimized build with no assertions but with debug info.; * -DCMAKE_BUILD_TYPE=MinSizeRel - For a build optimized for size instead of speed.; Learn more about these options in our documentation at https://llvm.org/docs/CMake.html#cmake-build-type; ""); endif(). # Set default build type for cmake's try_compile module.; # CMake 3.17 or newer sets CMAKE_DEFAULT_BUILD_TYPE to one of the; # items from CMAKE_CONFIGURATION_TYPES. Logic below can be further; # simplified once LLVM's minimum CMake version is updated to 3.17.; if(CMAKE_DEFAULT_BUILD_TYPE); set(CMAKE_TRY_COMPILE_CONFIGURATION ${CMAKE_DEFAULT_BUILD_TYPE}); else(); if(CMAKE_CONFIGURATION_TYPES); list(GET CMAKE_CONFIGURATION_TYPES 0 CMAKE_TRY_COMPILE_CONFIGURATION); elseif(CMAKE_BUILD_TYPE); set(CMAKE_TRY_COMPILE_CONFIGURATION ${CMAKE_BUILD_TYPE}); endif(); endif(). # Side-by-side subprojects layout: automatically set the; # LLVM_EXTERNAL_${project}_SOURCE_DIR using LLVM_ALL_PROJECTS; # This allows an easy way of setting up a build direct",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/CMakeLists.txt:4204,optimiz,optimized,4204,interpreter/llvm-project/llvm/CMakeLists.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/CMakeLists.txt,1,['optimiz'],['optimized']
Performance,"ry among other optimizations performed; at the binary level. There are also CMake caches available to build; LLVM/Clang with BOLT. To configure a single-stage build that builds LLVM/Clang and then optimizes; it with BOLT, use the following CMake configuration:. .. code-block:: console. $ cmake <path to source>/llvm -C <path to source>/clang/cmake/caches/BOLT.cmake. Then, build the BOLT-optimized binary by running the following ninja command:. .. code-block:: console. $ ninja clang-bolt. If you're seeing errors in the build process, try building with a recent; version of Clang/LLVM by setting the CMAKE_C_COMPILER and; CMAKE_CXX_COMPILER flags to the appropriate values. It is also possible to use BOLT on top of PGO and (Thin)LTO for an even more; significant runtime speedup. To configure a three stage PGO build with ThinLTO; that optimizes the resulting binary with BOLT, use the following CMake; configuration command:. .. code-block:: console. $ cmake -G Ninja <path to source>/llvm \; -C <path to source>/clang/cmake/caches/BOLT-PGO.cmake \; -DBOOTSTRAP_LLVM_ENABLE_LLD=ON \; -DBOOTSTRAP_BOOTSTRAP_LLVM_ENABLE_LLD=ON \; -DPGO_INSTRUMENT_LTO=Thin. Then, to build the final optimized binary, build the stage2-clang-bolt target:. .. code-block:: console. $ ninja stage2-clang-bolt. 3-Stage Non-Determinism; =======================. In the ancient lore of compilers non-determinism is like the multi-headed hydra.; Whenever its head pops up, terror and chaos ensue. Historically one of the tests to verify that a compiler was deterministic would; be a three stage build. The idea of a three stage build is you take your sources; and build a compiler (stage1), then use that compiler to rebuild the sources; (stage2), then you use that compiler to rebuild the sources a third time; (stage3) with an identical configuration to the stage2 build. At the end of; this, you have a stage2 and stage3 compiler that should be bit-for-bit; identical. You can perform one of these 3-stage builds with LL",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AdvancedBuilds.rst:11111,cache,caches,11111,interpreter/llvm-project/llvm/docs/AdvancedBuilds.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AdvancedBuilds.rst,1,['cache'],['caches']
Performance,"ry boosted method linear, log, step. Boost_RandomSeed No 0 − Seed for random number generator used for bagging. Configuration options for MVA method :. Configuration options reference for MVA method: RuleFit. Option Array Default value Predefined values Description. V No False − Verbose output (short form of VerbosityLevel below - overrides the latter one). VerbosityLevel No Default Default, Debug, Verbose, Info, Warning, Error, Fatal Verbosity level. VarTransform No None − List of variable transformations performed before training, e.g., D_Background,P_Signal,G,N_AllClasses for: Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed). H No False − Print method-specific help message. CreateMVAPdfs No False − Create PDFs for classifier outputs (signal and background). IgnoreNegWeightsInTraining No False − Events with negative weights are ignored in the training (but are included for testing and performance evaluation). GDTau No -1 − Gradient-directed (GD) path: default fit cut-off. GDTauPrec No 0.01 − GD path: precision of tau. GDStep No 0.01 − GD path: step size. GDNSteps No 10000 − GD path: number of steps. GDErrScale No 1.1 − Stop scan when error > scale*errmin. LinQuantile No 0.025 − Quantile of linear terms (removes outliers). GDPathEveFrac No 0.5 − Fraction of events used for the path search. GDValidEveFrac No 0.5 − Fraction of events used for the validation. fEventsMin No 0.1 − Minimum fraction of events in a splittable node. fEventsMax No 0.9 − Maximum fraction of events in a splittable node. nTrees No 20 − Number of trees in forest. ForestType No AdaBoost AdaBoost, Random Method to use for forest generation (AdaBoost or RandomForest). RuleMinDist No 0.001 − Minimum distance between rules. MinImp No 0.01 − Minimum rule importance accepted. Model No ModRuleLinear ModRule, ModRuleLinear, ModLinear Model to be used. Ru",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/tmva/UsersGuide/optionRef.html:18541,perform,performance,18541,documentation/tmva/UsersGuide/optionRef.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/tmva/UsersGuide/optionRef.html,2,['perform'],['performance']
Performance,"ry general and powerful way of processing a `TChain` is; provided via the method `TChain::Process()`. This method takes as; arguments an instance of a -- user-implemented-- class of type; `TSelector`, and -- optionally -- the number of entries and the first; entry to be processed. A template for the class `TSelector` is provided; by the method `TTree::MakeSelector`, as is shown in the little macro; `makeSelector.C` below. It opens the n-tuple `conductivity_experiment.root` from the example; above and creates from it the header file `MySelector.h` and a template; to insert your own analysis code, `MySelector.C`.; \newpage. ``` {.cpp}; @ROOT_INCLUDE_FILE macros/makeMySelector.C; ```. The template contains the entry points `Begin()` and `SlaveBegin()`; called before processing of the `TChain` starts, `Process()` called for; every entry of the chain, and `SlaveTerminate()` and `Terminate()`; called after the last entry has been processed. Typically,; initialization like booking of histograms is performed in; `SlaveBegin()`, the analysis, i.e. the selection of entries,; calculations and filling of histograms, is done in `Process()`, and; final operations like plotting and storing of results happen in; `SlaveTerminate()` or `Terminate()`. The entry points `SlaveBegin()` and `SlaveTerminate()` are called on; so-called slave nodes only if parallel processing via `PROOF` or; `PROOF lite` is enabled, as will be explained below. A simple example of a selector class is shown in the macro; `MySelector.C`. The example is executed with the following sequence of; commands:. ``` {.cpp}; > TChain *ch=new TChain(""cond_data"", ""Chain for Example N-Tuple"");; > ch->Add(""conductivity_experiment*.root"");; > ch->Process(""MySelector.C+"");; ```. As usual, the ""`+`"" appended to the name of the macro to be executed; initiates the compilation of the `MySelector.C` with the system compiler; in order to improve performance. The code in `MySelector.C`, shown in the listing below, books some; histogr",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/primer/filio.md:7383,perform,performed,7383,documentation/primer/filio.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/primer/filio.md,1,['perform'],['performed']
Performance,"ry generator.; * Fix handling of template parameter pack in the forward declaration printer. [ROOT-8096]; * Do not autoparse headers for classes in the pch.; * Avoid autoparse on IsForeign() if possible.; * Check for new-style empty pcm with key named ""EMPTY"" created since commit 90047b0cba6fd295f5c5722749a0d043fbc11ea5.; * Do not insert macro definition of `__ROOTCLING__` into the pch. ### Interpreter Library. * llvm / clang have been updated to r274612.; * The GCC5 ABI is now supported [ROOT-7947].; * Exceptions are now caught in the interactive ROOT session, instead of terminating ROOT.; * A ValuePrinter for tuple and pair has been added to visualise the content of these entities at the prompt.; * When interpreting dereferences of invalid pointers, cling will now complain (throw, actually) instead of crash.; * Resolve memory hoarding in some case of looking up functions [ROOT-8145]. ## Parallelism. * Three methods have been added to manage implicit multi-threading in ROOT: `ROOT::EnableImplicitMT(numthreads)`, `ROOT::DisableImplicitMT` and `ROOT::IsImplicitMTEnabled`. They can be used to enable, disable and check the status of the global implicit multi-threading in ROOT, respectively.; * Even if the default reduce function specified in the invocation of the `MapReduce` method of `TProcessExecutor` returns a pointer to a `TObject`, the return value of `MapReduce` is properly casted to the type returned by the map function.; * Add a new class named `TThreadExecutor` implementing a MapReduce framework sharing `TProcessExecutor` interface and based in tbb.; * Add a new class named `TExecutor` defining the MapReduce interface for `TProcessExecutor` and `TThreadExecutor`, who inherit from it.; * Remove all `TPool` signatures accepting collections as an argument with the exception of std::vector and initializer_lists. ; * Extend `TThreadExecutor` functionality offering parallel reduction given a binary operator as a reduction function.; * Add a new class named `TThreaded",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v608/index.md:5689,multi-thread,multi-threading,5689,README/ReleaseNotes/v608/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v608/index.md,1,['multi-thread'],['multi-threading']
Performance,"ry have; completed before; performing the; store that is being; released. 2. buffer/global/flat_store; atomicrmw release - singlethread - global 1. buffer/global/ds/flat_atomic; - wavefront - local; - generic; atomicrmw release - workgroup - global 1. s_waitcnt lgkmcnt(0); - generic; - If OpenCL, omit.; - Must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to local have; completed before; performing the; atomicrmw that is; being released. 2. buffer/global/flat_atomic; atomicrmw release - workgroup - local 1. ds_atomic; atomicrmw release - agent - global 1. s_waitcnt lgkmcnt(0) &; - system - generic vmcnt(0). - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global and local; have completed; before performing; the atomicrmw that; is being released. 2. buffer/global/flat_atomic; fence release - singlethread *none* *none*; - wavefront; fence release - workgroup *none* 1. s_waitcnt lgkmcnt(0). - If OpenCL and; address space is; not generic, omit.; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate. If; fence had an; address space then; set to address; space of OpenCL; fence flag, or to; generic if both; local and global; flags are; specified.; - Must happen after; any preceding; local/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Must happen before; any following store; atomic/atomicrmw; with an eq",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:221226,load,load,221226,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"ry on it's behalf. As a result, perhaps surprisingly, a ``nofree``; function can return a pointer to a previously deallocated memory object.; ``noimplicitfloat``; Disallows implicit floating-point code. This inhibits optimizations that; use floating-point code and floating-point registers for operations that are; not nominally floating-point. LLVM instructions that perform floating-point; operations or require access to floating-point registers may still cause; floating-point code to be generated. Also inhibits optimizations that create SIMD/vector code and registers from; scalar code such as vectorization or memcpy/memset optimization. This; includes integer vectors. Vector instructions present in IR may still cause; vector code to be generated.; ``noinline``; This attribute indicates that the inliner should never inline this; function in any situation. This attribute may not be used together; with the ``alwaysinline`` attribute.; ``nomerge``; This attribute indicates that calls to this function should never be merged; during optimization. For example, it will prevent tail merging otherwise; identical code sequences that raise an exception or terminate the program.; Tail merging normally reduces the precision of source location information,; making stack traces less useful for debugging. This attribute gives the; user control over the tradeoff between code size and debug information; precision.; ``nonlazybind``; This attribute suppresses lazy symbol binding for the function. This; may make calls to the function faster, at the cost of extra program; startup time if the function is not called during program startup.; ``noprofile``; This function attribute prevents instrumentation based profiling, used for; coverage or profile based optimization, from being added to a function. It; also blocks inlining if the caller and callee have different values of this; attribute.; ``skipprofile``; This function attribute prevents instrumentation based profiling, used for; coverag",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:90989,optimiz,optimization,90989,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['optimiz'],['optimization']
Performance,"rySearchGenerator::GetForCurrentProcess(DL.getGlobalPrefix())));; }. Our extended KaleidoscopeJIT class starts out the same as it did in Chapter 1,; but after the CompileLayer we introduce a new member, TransformLayer, which sits; on top of our CompileLayer. We initialize our OptimizeLayer with a reference to; the ExecutionSession and output layer (standard practice for layers), along with; a *transform function*. For our transform function we supply our classes; optimizeModule static method. .. code-block:: c++. // ...; return cantFail(OptimizeLayer.addModule(std::move(M),; std::move(Resolver)));; // ... Next we need to update our addModule method to replace the call to; ``CompileLayer::add`` with a call to ``OptimizeLayer::add`` instead. .. code-block:: c++. static Expected<ThreadSafeModule>; optimizeModule(ThreadSafeModule M, const MaterializationResponsibility &R) {; // Create a function pass manager.; auto FPM = std::make_unique<legacy::FunctionPassManager>(M.get());. // Add some optimizations.; FPM->add(createInstructionCombiningPass());; FPM->add(createReassociatePass());; FPM->add(createGVNPass());; FPM->add(createCFGSimplificationPass());; FPM->doInitialization();. // Run the optimizations over all functions in the module being added to; // the JIT.; for (auto &F : *M); FPM->run(F);. return M;; }. At the bottom of our JIT we add a private method to do the actual optimization:; *optimizeModule*. This function takes the module to be transformed as input (as; a ThreadSafeModule) along with a reference to a reference to a new class:; ``MaterializationResponsibility``. The MaterializationResponsibility argument; can be used to query JIT state for the module being transformed, such as the set; of definitions in the module that JIT'd code is actively trying to call/access.; For now we will ignore this argument and use a standard optimization; pipeline. To do this we set up a FunctionPassManager, add some passes to it, run; it over every function in the module, and ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:4588,optimiz,optimizations,4588,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,1,['optimiz'],['optimizations']
Performance,"s (data vs data ) and; one sample (data vs distribution); For the data vs distribution test, the user can compare using a; predefined distributions (Gaussian, LogNormal or Exponential) or; by passing a user defined PDF or CDF.; Example 1: perform a 2 sample GoF test from two arrays,; sample1[n1] and sample2[n2] containing the data; ; ROOT::Math::GoFTest goftest(n1, sample1, n2, sample2);; double pValueAD = goftest.AndersonDarling2SamplesTest();; double pValueKS = goftest.KolmogorovSmirnov2SamplesTest();; ; The class can return optionally also the test statistics instead of; the p value.; Example 2: perform a 1 sample test with a pre-defined; distribution starting from a data set sample[n]. ROOT::Math::GoFTest goftest(n, sample, ROOT::Math::GoFTest::kGaussian);; double pValueAD = goftest.AndersonDarlingTest();; double pValueKS = goftest.KolmogorovSmirnovTest();; . Example 3: perform a 1 sample test with a user-defined; distribution provided as cdf; ; ROOT::Math::Functor1D cdf_func(&ROOT::Math::landau_cdf);; ROOT::Math::GofTest goftest(n, sample, cdf_func, ROOT::Math::GoFTest::kCDF);; double pValueAD = goftest.AndersonDarlingTest();; . Example 4: perform a 1 sample test with a user-defined; distribution provided as pdf. Note that in this case to avoid; integration problems is sometimes recommended to give some; reasonable xmin and xmax values. xmin (and xmax) should however be; smaller (larger) than the minimum (maximum) data value.; ; ROOT::Math::Functor1D pdf_func(&ROOT::Math::landau_pdf);; double xmin = 5*TMath::Min_Element(n,sample);; double xmax = 5*TMath::Max_Element(n,sample);; ROOT::Math::GofTest goftest(n, sample, pdf_func, ROOT::Math::GoFTest::kPDF,xmin,xmax);; double pValueAD = goftest.AndersonDarlingTest();; . The tutorial math/goftest.C is an example on; how to use the ROOT::Math::GofTest class. New class TKDTreeBinning for binning multidimensional data.; ; The class implements multidimensional binning by constructing a; TKDTree inner structure form the da",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/math/doc/v528/index.html:1799,perform,perform,1799,math/doc/v528/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/math/doc/v528/index.html,2,['perform'],['perform']
Performance,"s -S -o - load.c; [...]; foo:; stp x30, x20, [sp, #-16]!; adrp x20, :got:__hwasan_shadow // load shadow address from GOT into x20; ldr x20, [x20, :got_lo12:__hwasan_shadow]; bl __hwasan_check_x0_2_short_v2 // call outlined tag check; // (arguments: x0 = address, x20 = shadow base;; // ""2"" encodes the access type and size); ldr w0, [x0] // inline load; ldp x30, x20, [sp], #16; ret. [...]; __hwasan_check_x0_2_short_v2:; sbfx x16, x0, #4, #52 // shadow offset; ldrb w16, [x20, x16] // load shadow tag; cmp x16, x0, lsr #56 // extract address tag, compare with shadow tag; b.ne .Ltmp0 // jump to short tag handler on mismatch; .Ltmp1:; ret; .Ltmp0:; cmp w16, #15 // is this a short tag?; b.hi .Ltmp2 // if not, error; and x17, x0, #0xf // find the address's position in the short granule; add x17, x17, #3 // adjust to the position of the last byte loaded; cmp w16, w17 // check that position is in bounds; b.ls .Ltmp2 // if not, error; orr x16, x0, #0xf // compute address of last byte of granule; ldrb w16, [x16] // load tag from it; cmp x16, x0, lsr #56 // compare with pointer tag; b.eq .Ltmp1 // if matches, continue; .Ltmp2:; stp x0, x1, [sp, #-256]! // save original x0, x1 on stack (they will be overwritten); stp x29, x30, [sp, #232] // create frame record; mov x1, #2 // set x1 to a constant indicating the type of failure; adrp x16, :got:__hwasan_tag_mismatch_v2 // call runtime function to save remaining registers and report error; ldr x16, [x16, :got_lo12:__hwasan_tag_mismatch_v2] // (load address from GOT to avoid potential register clobbers in delay load handler); br x16. Heap; ----. Tagging the heap memory/pointers is done by `malloc`.; This can be based on any malloc that forces all objects to be TG-aligned.; `free` tags the memory with a different tag. Stack; -----. Stack frames are instrumented by aligning all non-promotable allocas; by `TG` and tagging stack memory in function prologue and epilogue. Tags for different allocas in one function are **not** generated; indep",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/HardwareAssistedAddressSanitizerDesign.rst:4779,load,load,4779,interpreter/llvm-project/clang/docs/HardwareAssistedAddressSanitizerDesign.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/HardwareAssistedAddressSanitizerDesign.rst,1,['load'],['load']
Performance,"s ``__seg_fs`` and ``__seg_gs`` for; the same purpose. The preprocessor symbols ``__SEG_FS`` and ``__SEG_GS``; indicate their support. PowerPC Language Extensions; ---------------------------. Set the Floating Point Rounding Mode; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; PowerPC64/PowerPC64le supports the builtin function ``__builtin_setrnd`` to set; the floating point rounding mode. This function will use the least significant; two bits of integer argument to set the floating point rounding mode. .. code-block:: c++. double __builtin_setrnd(int mode);. The effective values for mode are:. - 0 - round to nearest; - 1 - round to zero; - 2 - round to +infinity; - 3 - round to -infinity. Note that the mode argument will modulo 4, so if the integer argument is greater; than 3, it will only use the least significant two bits of the mode.; Namely, ``__builtin_setrnd(102))`` is equal to ``__builtin_setrnd(2)``. PowerPC cache builtins; ^^^^^^^^^^^^^^^^^^^^^^. The PowerPC architecture specifies instructions implementing cache operations.; Clang provides builtins that give direct programmer access to these cache; instructions. Currently the following builtins are implemented in clang:. ``__builtin_dcbf`` copies the contents of a modified block from the data cache; to main memory and flushes the copy from the data cache. **Syntax**:. .. code-block:: c. void __dcbf(const void* addr); /* Data Cache Block Flush */. **Example of Use**:. .. code-block:: c. int a = 1;; __builtin_dcbf (&a);. Extensions for Static Analysis; ==============================. Clang supports additional attributes that are useful for documenting program; invariants and rules for static analysis tools, such as the `Clang Static; Analyzer <https://clang-analyzer.llvm.org/>`_. These attributes are documented; in the analyzer's `list of source-level annotations; <https://clang-analyzer.llvm.org/annotations.html>`_. Extensions for Dynamic Analysis; ===============================. Use ``__has_feature(address_sanitize",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst:156988,cache,cache,156988,interpreter/llvm-project/clang/docs/LanguageExtensions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst,1,['cache'],['cache']
Performance,"s a :ref:`poison value <poisonvalues>` if; unsigned and/or signed overflow, respectively, occurs. Example:; """""""""""""""". .. code-block:: text. <result> = sub i32 4, %var ; yields i32:result = 4 - %var; <result> = sub i32 0, %val ; yields i32:result = -%var. .. _i_fsub:. '``fsub``' Instruction; ^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. <result> = fsub [fast-math flags]* <ty> <op1>, <op2> ; yields ty:result. Overview:; """""""""""""""""". The '``fsub``' instruction returns the difference of its two operands. Arguments:; """""""""""""""""""". The two arguments to the '``fsub``' instruction must be; :ref:`floating-point <t_floating>` or :ref:`vector <t_vector>` of; floating-point values. Both arguments must have identical types. Semantics:; """""""""""""""""""". The value produced is the floating-point difference of the two operands.; This instruction is assumed to execute in the default :ref:`floating-point; environment <floatenv>`.; This instruction can also take any number of :ref:`fast-math; flags <fastmath>`, which are optimization hints to enable otherwise; unsafe floating-point optimizations:. Example:; """""""""""""""". .. code-block:: text. <result> = fsub float 4.0, %var ; yields float:result = 4.0 - %var; <result> = fsub float -0.0, %val ; yields float:result = -%var. .. _i_mul:. '``mul``' Instruction; ^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. <result> = mul <ty> <op1>, <op2> ; yields ty:result; <result> = mul nuw <ty> <op1>, <op2> ; yields ty:result; <result> = mul nsw <ty> <op1>, <op2> ; yields ty:result; <result> = mul nuw nsw <ty> <op1>, <op2> ; yields ty:result. Overview:; """""""""""""""""". The '``mul``' instruction returns the product of its two operands. Arguments:; """""""""""""""""""". The two arguments to the '``mul``' instruction must be; :ref:`integer <t_integer>` or :ref:`vector <t_vector>` of integer values. Both; arguments must have identical types. Semantics:; """""""""""""""""""". The value produced is the integer product of the two operands. If the result of the multiplication has unsigned overflow, ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:381647,optimiz,optimization,381647,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,2,['optimiz'],"['optimization', 'optimizations']"
Performance,"s a normal state, but not all fields are; proven to be overwritten we can't perform the refactoring. ```c++; void target(bool b, Customer* c) {; // Overwritten: {}; if (b) {; c->account_id = 42; // Overwritten: {c->account_id}; } else {; c->name = ""Konrad""; // Overwritten: {c->name}; }; // Overwritten: {}; }; ```. Similarly, when the data flow algorithm computes a failure state, we also can't; perform the refactoring. ```c++; Customer* kGlobalCustomer;. void GetCustomer(Customer* c) {; // Overwritten: {}; c->account_id = ...; // Overwritten: {c->account_id}; if (...) {; print(c->name); // Unsafe read; } else {; kGlobalCustomer = c; // Pointer escape; }; // Unsafe read, Pointer escape; }; ```. ## Example: finding dead stores. Let's say we want to find redundant stores, because they indicate potential; bugs. ```c++; x = GetX();; x = GetY();; ```. The first store to `x` is never read, probably there is a bug. The implementation of dead store analysis is very similar to output parameter; analysis: we need to track stores and loads, and find stores that were never; read. [Liveness analysis](https://en.wikipedia.org/wiki/Live_variable_analysis) is a; generalization of this idea, which is often used to answer many related; questions, for example:. * finding dead stores,; * finding uninitialized variables,; * finding a good point to deallocate memory,; * finding out if it would be safe to move an object. ## Example: definitive initialization. Definitive initialization proves that variables are known to be initialized when; read. If we find a variable which is read when not initialized then we generate; a warning. ```c++; void Init() {; int x; // x is uninitialized; if (cond()) {; x = 10; // x is initialized; } else {; x = 20; // x is initialized; }; print(x); // x is initialized; }; ```. ```c++; void Uninit() {; int x; // x is uninitialized; if (cond()) {; x = 10; // x is initialized; }; print(x); // x is maybe uninitialized, x is being read, report a bug.; }; ```. For this ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/DataFlowAnalysisIntro.md:19960,load,loads,19960,interpreter/llvm-project/clang/docs/DataFlowAnalysisIntro.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/DataFlowAnalysisIntro.md,1,['load'],['loads']
Performance,"s a value of; type traits::pos_type result in undefined behavior.; Source: C++03 27.4.3.2p3; C++11 27.5.4.2p3. #include <fstream>. class my_streambuf : public std::streambuf {; void f() {; seekpos(-1); // warn; }; };. #include <fstream>. void test() {; std::filebuf fb;; std::istream in(&fb);; std::filebuf::off_type pos(-1);; in.seekg(pos); // warn; }. different. Name, DescriptionExampleProgress. different.SuccessiveAssign; (C); Successive assign to a variable. int test() {; int i;; i=1;; i=2; // warn; return i;; }. different.NullDerefStmtOrder; (C); Dereferencing of the null pointer might take place. Checking the pointer for; null should be performed first.; Note: possibly an enhancement to ; core.NullDereference. struct S {; int x;; };. struct S* f();. void test() {; struct S *p1 = f();; int x1 = p1->x; // warn; if (p1) {};. struct S *p2 = f();; int x2 = p2->x; // ok; }. different.NullDerefCondOrder; (C); Dereferencing of the null pointer might take place. Checking the pointer for; null should be performed first.; Note: possibly an enhancement to ; core.NullDereference. struct S {int i;};. struct S* f();. void test() {; struct S *p = f();; if (p->i && p) {}; // warn; }. different.MultipleAccessors; (C++); Identical accessor bodies. Possibly a misprint. class A {; int i;; int j;; public:; int getI() { return i; }; int getJ() { return i; } // warn; };. class A {; int i;; int j;; public:; void setI(int& ii) { i = ii; }; void setJ(int& jj) { i = jj; } // warn; };. different.AccessorsForPublic; (C++); Accessors exist for a public class field. Should this field really be; public?. class A {; public:; int i; // warn; int getI() { return i; }; void setI(int& ii) { i = ii; }; };. different.LibFuncResultUnised; (C, C++); Calling a function ignoring its return value is of no use (create the list of; known system/library/API functions falling into this category). #include <vector>. void test() {; std::vector<int> v;; v.empty(); // warn; }. different.WrongVarForStmt; (C, C++); ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/analyzer/potential_checkers.html:20287,perform,performed,20287,interpreter/llvm-project/clang/www/analyzer/potential_checkers.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/analyzer/potential_checkers.html,2,['perform'],['performed']
Performance,"s a; parameter, and first checks to see if this is a case of reading or; writing the buffer. ``` {.cpp}; void Event::Streamer(TBuffer &R__b) {; if (R__b.IsReading()) {; Event::Class()->ReadBuffer(R__b, this);; fTransient = gDirectory; //save current directory; fPt= TMath::Sqrt(fPx*fPx + fPy*fPy + fPz*fPz);; } else {; Event::Class()->WriteBuffer(R__b, this);; }; }; ```. ### Writing Objects. The `Streamer` decomposes the objects into data members and writes them; to a buffer. It does not write the buffer to a file, it simply populates; a buffer with bytes representing the object. This allows us to write the; buffer to a file or do anything else we could do with the buffer. For; example, we can write it to a socket to send it over the network. This; is beyond the scope of this chapter, but it is worthwhile to emphasize; the need and advantage of separating the creation of the buffer from its; use. Let us look how a buffer is written to a file. The dictionary for a; class needs to be loaded before any object of that type can be saved. The `TObject::Write` method does the following:. - Creates a **`TKey`** object in the current directory. - Creates a **`TBuffer`** object which is part of the newly created; **`TKey`**. - Fills the **`TBuffer`** with a call to the `class::Streamer` method. - Creates a second buffer for compression, if needed. - Reserves space by scanning the **`TFree`** list. At this point, the; size of the buffer is known. - Writes the buffer to the file. - Releases the **`TBuffer`** part of the key. In other words, the `TObject::Write` calls the `Streamer` method of the; class to build the buffer. The buffer is in the key and the key is; written to disk. Once written to disk the memory consumed by the buffer; part is released. The key part of the **`TKey`** is kept. ![A diagram of a streamed TH1F in the buffer](pictures/020000EB.jpg). The key consumes about 60 bytes, whereas the buffer, since it contains; the object data, can be very large. ### Ignore Ob",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/InputOutput.md:51753,load,loaded,51753,documentation/users-guide/InputOutput.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/InputOutput.md,1,['load'],['loaded']
Performance,"s allows; the LLVM LTO code to be updated independently of the linker tool. On platforms; that support it, the shared object is lazily loaded. Phase 2 : Symbol Resolution; ---------------------------. In this stage, the linker resolves symbols using global symbol table. It may; report undefined symbol errors, read archive members, replace weak symbols, etc.; The linker is able to do this seamlessly even though it does not know the exact; content of input LLVM bitcode files. If dead code stripping is enabled then the; linker collects the list of live symbols. Phase 3 : Optimize Bitcode Files; --------------------------------. After symbol resolution, the linker tells the LTO shared object which symbols; are needed by native object files. In the example above, the linker reports; that only ``foo1()`` is used by native object files using; ``lto_codegen_add_must_preserve_symbol()``. Next the linker invokes the LLVM; optimizer and code generators using ``lto_codegen_compile()`` which returns a; native object file creating by merging the LLVM bitcode files and applying; various optimization passes. Phase 4 : Symbol Resolution after optimization; ----------------------------------------------. In this phase, the linker reads optimized a native object file and updates the; internal global symbol table to reflect any changes. The linker also collects; information about any changes in use of external symbols by LLVM bitcode; files. In the example above, the linker notes that ``foo4()`` is not used any; more. If dead code stripping is enabled then the linker refreshes the live; symbol information appropriately and performs dead code stripping. After this phase, the linker continues linking as if it never saw LLVM bitcode; files. .. _libLTO:. ``libLTO``; ==========. ``libLTO`` is a shared object that is part of the LLVM tools, and is intended; for use by a linker. ``libLTO`` provides an abstract C interface to use the LLVM; interprocedural optimizer without exposing details of ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:7172,optimiz,optimizer,7172,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,2,['optimiz'],"['optimization', 'optimizer']"
Performance,"s any; following global; data read is no; older than the local load; atomic value being; acquired. load atomic acquire - workgroup - generic 1. flat_load glc=1. - If not TgSplit execution; mode, omit glc=1. 2. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL, omit lgkmcnt(0).; - Must happen before; the following; buffer_wbinvl1_vol and any; following global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local load; atomic value being; acquired. 3. buffer_wbinvl1_vol. - If not TgSplit execution; mode, omit.; - Ensures that; following; loads will not see; stale data. load atomic acquire - agent - global 1. buffer/global_load; glc=1; 2. s_waitcnt vmcnt(0). - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the load; has completed; before invalidating; the cache. 3. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale global data. load atomic acquire - system - global 1. buffer/global/flat_load; glc=1; 2. s_waitcnt vmcnt(0). - Must happen before; following buffer_invl2 and; buffer_wbinvl1_vol.; - Ensures the load; has completed; before invalidating; the cache. 3. buffer_invl2;; buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale L1 global data,; nor see stale L2 MTYPE; NC global data.; MTYPE RW and CC memory will; never be stale in L2 due to; the memory probes. load atomic acquire - agent - generic 1. flat_load glc=1; 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL omit; lgkmcnt(0).; - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the flat_load; has completed; before invalidating; the cache. 3. buffer_wbinvl1_vol. - Must happen before; any",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:247046,load,load,247046,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"s are not forwards-compatible: i.e, a tool which uses format; version X will not be able to understand format version (X+k). * Tools must also retain **backwards** compatibility with the format of the; coverage mappings emitted into instrumented binaries. These formats are not; forwards-compatible. * The JSON coverage export format has a (major, minor, patch) version triple.; Only a major version increment indicates a backwards-incompatible change. A; minor version increment is for added functionality, and patch version; increments are for bugfixes. Impact of llvm optimizations on coverage reports; ================================================. llvm optimizations (such as inlining or CFG simplification) should have no; impact on coverage report quality. This is due to the fact that the mapping; from source regions to profile counters is immutable, and is generated before; the llvm optimizer kicks in. The optimizer can't prove that profile counter; instrumentation is safe to delete (because it's not: it affects the profile the; program emits), and so leaves it alone. Note that this coverage feature does not rely on information that can degrade; during the course of optimization, such as debug info line tables. Using the profiling runtime without static initializers; =======================================================. By default the compiler runtime uses a static initializer to determine the; profile output path and to register a writer function. To collect profiles; without using static initializers, do this manually:. * Export a ``int __llvm_profile_runtime`` symbol from each instrumented shared; library and executable. When the linker finds a definition of this symbol, it; knows to skip loading the object which contains the profiling runtime's; static initializer. * Forward-declare ``void __llvm_profile_initialize_file(void)`` and call it; once from each instrumented executable. This function parses; ``LLVM_PROFILE_FILE``, sets the output path, and truncates",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/SourceBasedCodeCoverage.rst:14977,optimiz,optimizer,14977,interpreter/llvm-project/clang/docs/SourceBasedCodeCoverage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/SourceBasedCodeCoverage.rst,1,['optimiz'],['optimizer']
Performance,"s at one and cache those in a histogram that is returned as p.d.f shape; (with optional interpolation), has been expanded. This infrastructure comprises RooAbsCached; the base class for all caching p.d.fs, RooAbsSelfCachedPdf a base class for end-user; caching p.d.f implementations that simply cache the result of evaluate() and RooCachedPdf; that can wrap and cache any input p.d.f specified in its constructor. . By default a p.d.f is sampled and cached in all observables in any; given use context, with no need to specify what those are in advance.; The internal code has also been changed such that all cache; histograms now store pre-normalized p.d.f, which is more efficient; than 'raw' p.d.f histograms that are explicitly post-normalized; through integration. Multiple different use cases (e.g. definitions; of what are observables vs parameters) can be cached; simultaneously. Now it is also possible to specify that p.d.f.s; should be sampled and cached in one or more parameter dimensions; in addition to the automatically determined set of observables.; as well. Also a complete new line of classes with similar functionality has been added inheriting from RooAbsReal.; These are RooAbsCachedReal,RooAbsSelfCachedReal and RooCachedReal. A newly; added class RooHistFunc presents these shapes and is capable of handling negative entries. New PDF error handling structure. New infrastructure has been put into place to propagate and process p.d.f evaluation errors during fitting.; Previously evaluation errors were marked with a zero p.d.f value and propagated as a special condition; in RooAddPdf, RooProdPdf etc to result in a zero top-level p.d.f value that was caught by the RooFit minuit; interface as a special condition. Summary information on the value of the parameters and the observables; was printed for the first 10 occurrences of such conditions. . Now, each p.d.f component that generates an error; in its evaluation logs the error into a separate facility during fitting ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v520/index.html:14588,cache,cached,14588,roofit/doc/v520/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v520/index.html,2,['cache'],['cached']
Performance,"s being stored in a SVN server, these developers are already using Git; through the Git-SVN integration. Git allows you to:. * Commit, squash, merge, and fork locally without touching the remote server.; * Maintain local branches, enabling multiple threads of development.; * Collaborate on these branches (e.g. through your own fork of llvm on GitHub).; * Inspect the repository history (blame, log, bisect) without Internet access.; * Maintain remote forks and branches on Git hosting services and; integrate back to the main repository. In addition, because Git seems to be replacing many OSS projects' version; control systems, there are many tools that are built over Git.; Future tooling may support Git first (if not only). Why GitHub?; -----------. GitHub, like GitLab and BitBucket, provides free code hosting for open source; projects. Any of these could replace the code-hosting infrastructure that we; have today. These services also have a dedicated team to monitor, migrate, improve and; distribute the contents of the repositories depending on region and load. GitHub has one important advantage over GitLab and; BitBucket: it offers read-write **SVN** access to the repository; (https://github.com/blog/626-announcing-svn-support).; This would enable people to continue working post-migration as though our code; were still canonically in an SVN repository. In addition, there are already multiple LLVM mirrors on GitHub, indicating that; part of our community has already settled there. On Managing Revision Numbers with Git; -------------------------------------. The current SVN repository hosts all the LLVM sub-projects alongside each other.; A single revision number (e.g. r123456) thus identifies a consistent version of; all LLVM sub-projects. Git does not use sequential integer revision number but instead uses a hash to; identify each commit. The loss of a sequential integer revision number has been a sticking point in; past discussions about Git:. - ""The 'branch' I most",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/GitHubMove.rst:3470,load,load,3470,interpreter/llvm-project/llvm/docs/Proposals/GitHubMove.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/GitHubMove.rst,1,['load'],['load']
Performance,"s byref, sret, inalloca and preallocated.; * Some intrinsics require an ``elementtype`` attribute, which can be retrieved; using ``getParamElementType()``. This attribute is required in cases where; the intrinsic does not naturally encode a needed element type. This is also; used for inline assembly. Note that some of the methods mentioned above only exist to support both typed; and opaque pointers at the same time, and will be dropped once the migration; has completed. For example, ``isOpaqueOrPointeeTypeEquals()`` becomes; meaningless once all pointers are opaque. While direct usage of pointer element types is immediately apparent in code,; there is a more subtle issue that opaque pointers need to contend with: A lot; of code assumes that pointer equality also implies that the used load/store; type or GEP source element type is the same. Consider the following examples; with typed and opaque pointers:. .. code-block:: llvm. define i32 @test(i32* %p) {; store i32 0, i32* %p; %bc = bitcast i32* %p to i64*; %v = load i64, i64* %bc; ret i64 %v; }. define i32 @test(ptr %p) {; store i32 0, ptr %p; %v = load i64, ptr %p; ret i64 %v; }. Without opaque pointers, a check that the pointer operand of the load and; store are the same also ensures that the accessed type is the same. Using a; different type requires a bitcast, which will result in distinct pointer; operands. With opaque pointers, the bitcast is not present, and this check is no longer; sufficient. In the above example, it could result in store to load forwarding; of an incorrect type. Code making such assumptions needs to be adjusted to; check the accessed type explicitly:; ``LI->getType() == SI->getValueOperand()->getType()``. Frontends; ---------. Frontends need to be adjusted to track pointee types independently of LLVM,; insofar as they are necessary for lowering. For example, clang now tracks the; pointee type in the ``Address`` structure. Frontends using the C API through an FFI interface should be aware t",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:9008,load,load,9008,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst,1,['load'],['load']
Performance,"s case the; architecture will be determined by inspection of the object header. #. ``createLinkGraph`` can be used when neither the object format nor; the architecture are known ahead of time. In this case the object header; will be inspected to determine both the format and architecture. .. _jit_linking:. JIT Linking; ===========. The JIT linker concept was introduced in LLVM's earlier generation of JIT APIs,; MCJIT. In MCJIT the *RuntimeDyld* component enabled re-use of LLVM as an; in-memory compiler by adding an in-memory link step to the end of the usual; compiler pipeline. Rather than dumping relocatable objects to disk as a compiler; usually would, MCJIT passed them to RuntimeDyld to be linked into a target; process. This approach to linking differs from standard *static* or *dynamic* linking:. A *static linker* takes one or more relocatable object files as input and links; them into an executable or dynamic library on disk. A *dynamic linker* applies relocations to executables and dynamic libraries that; have been loaded into memory. A *JIT linker* takes a single relocatable object file at a time and links it; into a target process, usually using a context object to allow the linked code; to resolve symbols in the target. RuntimeDyld; -----------. In order to keep RuntimeDyld's implementation simple MCJIT imposed some; restrictions on compiled code:. #. It had to use the Large code model, and often restricted available relocation; models in order to limit the kinds of relocations that had to be supported. #. It required strong linkage and default visibility on all symbols -- behavior; for other linkages/visibilities was not well defined. #. It constrained and/or prohibited the use of features requiring runtime; support, e.g. static initializers or thread local storage. As a result of these restrictions not all language features supported by LLVM; worked under MCJIT, and objects to be loaded under the JIT had to be compiled to; target it (precluding the use of",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/JITLink.rst:35475,load,loaded,35475,interpreter/llvm-project/llvm/docs/JITLink.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/JITLink.rst,1,['load'],['loaded']
Performance,"s clarity; > and readability. I agree to some extent, but it also comes at the; > cost of verbosity. And when the types are obvious from people's; > experience (e.g., in the br instruction), it doesn't seem to help as; > much. Very true. We should discuss this more, but my reasoning is more of a; consistency argument. There are VERY few instructions that can have all; of the types eliminated, and doing so when available unnecessarily makes; the language more difficult to handle. Especially when you see 'int; %this' and 'bool %that' all over the place, I think it would be; disorienting to see:. br %predicate, %iftrue, %iffalse. for branches. Even just typing that once gives me the creeps. ;) Like I; said, we should probably discuss this further in person... > On reflection, I really like your idea of having the two different; > switch types (even though they encode implementation techniques rather; > than semantics). It should simplify building the CFG and my guess is it; > could enable some significant optimizations, though we should think; > about which. Great. I added a note to the switch section commenting on how the VM; should just use the instruction type as a hint, and that the; implementation may choose altermate representations (such as predicated; branches). > In the lookup-indirect form of the switch, is there a reason not to; > make the val-type uint?. No. This was something I was debating for a while, and didn't really feel; strongly about either way. It is common to switch on other types in HLL's; (for example signed int's are particularly common), but in this case, all; that will be added is an additional 'cast' instruction. I removed that; from the spec. > I agree with your comment that we don't need 'neg'. Removed. > There's a trade-off with the cast instruction:; > + it avoids having to define all the upcasts and downcasts that are; > valid for the operands of each instruction (you probably have; > thought of other benefits also); > - it could make ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt:1611,optimiz,optimizations,1611,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt,1,['optimiz'],['optimizations']
Performance,"s code coverage mapping. :doc:`CFIVerify`; A description of the verification tool for Control Flow Integrity. LLVM Builds and Distributions; -----------------------------. :doc:`BuildingADistribution`; A best-practices guide for using LLVM's CMake build system to package and; distribute LLVM-based tools. :doc:`CMake`; An addendum to the main Getting Started guide for those using the `CMake; build system <http://www.cmake.org>`_. :doc:`Docker`; A reference for using Dockerfiles provided with LLVM. :doc:`Support Library <SupportLibrary>`; This document describes the LLVM Support Library (``lib/Support``) and; how to keep LLVM source code portable. :doc:`AdvancedBuilds`; This document describes more advanced build configurations. Optimizations; -------------. :doc:`WritingAnLLVMPass`; Information on how to write LLVM transformations and analyses. :doc:`WritingAnLLVMNewPMPass`; Information on how to write LLVM transformations under the new pass; manager. :doc:`Passes`; A list of optimizations and analyses implemented in LLVM. :doc:`StackSafetyAnalysis`; This document describes the design of the stack safety analysis of local; variables. :doc:`MergeFunctions`; Describes functions merging optimization. :doc:`AliasAnalysis`; Information on how to write a new alias analysis implementation or how to; use existing analyses. :doc:`MemorySSA`; Information about the MemorySSA utility in LLVM, as well as how to use it. :doc:`LoopTerminology`; A document describing Loops and associated terms as used in LLVM. :doc:`CycleTerminology`; A document describing cycles as a generalization of loops. :doc:`Vectorizers`; This document describes the current status of vectorization in LLVM. :doc:`LinkTimeOptimization`; This document describes the interface between LLVM intermodular optimizer; and the linker and its design. :doc:`GoldPlugin`; How to build your programs with link-time optimization on Linux. :doc:`Remarks`; A reference on the implementation of remarks in LLVM. :doc:`Source Level D",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/UserGuides.rst:2853,optimiz,optimizations,2853,interpreter/llvm-project/llvm/docs/UserGuides.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/UserGuides.rst,1,['optimiz'],['optimizations']
Performance,"s constructed (in; ``FunctionAST::codegen()``), but before it is returned to the client:. .. code-block:: c++. if (Value *RetVal = Body->codegen()) {; // Finish off the function.; Builder.CreateRet(RetVal);. // Validate the generated code, checking for consistency.; verifyFunction(*TheFunction);. // Optimize the function.; TheFPM->run(*TheFunction, *TheFAM);. return TheFunction;; }. As you can see, this is pretty straightforward. The; ``FunctionPassManager`` optimizes and updates the LLVM Function\* in; place, improving (hopefully) its body. With this in place, we can try; our test above again:. ::. ready> def test(x) (1+2+x)*(x+(1+2));; ready> Read function definition:; define double @test(double %x) {; entry:; %addtmp = fadd double %x, 3.000000e+00; %multmp = fmul double %addtmp, %addtmp; ret double %multmp; }. As expected, we now get our nicely optimized code, saving a floating; point add instruction from every execution of this function. LLVM provides a wide variety of optimizations that can be used in; certain circumstances. Some `documentation about the various; passes <../../Passes.html>`_ is available, but it isn't very complete.; Another good source of ideas can come from looking at the passes that; ``Clang`` runs to get started. The ""``opt``"" tool allows you to; experiment with passes from the command line, so you can see if they do; anything. Now that we have reasonable code coming out of our front-end, let's talk; about executing it!. Adding a JIT Compiler; =====================. Code that is available in LLVM IR can have a wide variety of tools; applied to it. For example, you can run optimizations on it (as we did; above), you can dump it out in textual or binary forms, you can compile; the code to an assembly file (.s) for some target, or you can JIT; compile it. The nice thing about the LLVM IR representation is that it; is the ""common currency"" between many different parts of the compiler. In this section, we'll add JIT compiler support to our inter",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:9074,optimiz,optimizations,9074,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,1,['optimiz'],['optimizations']
Performance,"s control into or out of ARC. However, the following exceptions apply. .. _arc.objects.restrictions.conversion.with.known.semantics:. Conversion to retainable object pointer type of expressions with known semantics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. :when-revised:`[beginning Apple 4.0, LLVM 3.1]`; :revision:`These exceptions have been greatly expanded; they previously applied; only to a much-reduced subset which is difficult to categorize but which; included null pointers, message sends (under the given rules), and the various; global constants.`. An unbridged conversion to a retainable object pointer type from a type other; than a retainable object pointer type is ill-formed, as discussed above, unless; the operand of the cast has a syntactic form which is known retained, known; unretained, or known retain-agnostic. An expression is :arc-term:`known retain-agnostic` if it is:. * an Objective-C string literal,; * a load from a ``const`` system global variable of :ref:`C retainable pointer; type <arc.misc.c-retainable>`, or; * a null pointer constant. An expression is :arc-term:`known unretained` if it is an rvalue of :ref:`C; retainable pointer type <arc.misc.c-retainable>` and it is:. * a direct call to a function, and either that function has the; ``cf_returns_not_retained`` attribute or it is an :ref:`audited; <arc.misc.c-retainable.audit>` function that does not have the; ``cf_returns_retained`` attribute and does not follow the create/copy naming; convention,; * a message send, and the declared method either has the; ``cf_returns_not_retained`` attribute or it has neither the; ``cf_returns_retained`` attribute nor a :ref:`selector family; <arc.method-families>` that implies a retained result, or; * :when-revised:`[beginning LLVM 3.6]` :revision:`a load from a` ``const``; :revision:`non-system global variable.`. An expression is :arc-term:`known retained` if it is an rvalue of :ref:`C; retainable pointer type <arc.m",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst:26133,load,load,26133,interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,1,['load'],['load']
Performance,"s described above, MCJIT will first; attempt to retrieve an object image from its ObjectCache member, if one; has been set. If a cached object image cannot be retrieved, MCJIT will; call its emitObject method. MCJIT::emitObject uses a local PassManager; instance and creates a new ObjectBufferStream instance, both of which it; passes to TargetMachine::addPassesToEmitMC before calling PassManager::run; on the Module with which it was created. .. image:: MCJIT-load.png. The PassManager::run call causes the MC code generation mechanisms to emit; a complete relocatable binary object image (either in either ELF or MachO; format, depending on the target) into the ObjectBufferStream object, which; is flushed to complete the process. If an ObjectCache is being used, the; image will be passed to the ObjectCache here. At this point, the ObjectBufferStream contains the raw object image.; Before the code can be executed, the code and data sections from this; image must be loaded into suitable memory, relocations must be applied and; memory permission and code cache invalidation (if required) must be completed. Object Loading; ==============. Once an object image has been obtained, either through code generation or; having been retrieved from an ObjectCache, it is passed to RuntimeDyld to; be loaded. The RuntimeDyld wrapper class examines the object to determine; its file format and creates an instance of either RuntimeDyldELF or; RuntimeDyldMachO (both of which derive from the RuntimeDyldImpl base; class) and calls the RuntimeDyldImpl::loadObject method to perform that; actual loading. .. image:: MCJIT-dyld-load.png. RuntimeDyldImpl::loadObject begins by creating an ObjectImage instance; from the ObjectBuffer it received. ObjectImage, which wraps the; ObjectFile class, is a helper class which parses the binary object image; and provides access to the information contained in the format-specific; headers, including section, symbol and relocation information. RuntimeDyldImpl::loadO",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst:3346,load,loaded,3346,interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,2,"['cache', 'load']","['cache', 'loaded']"
Performance,"s done in the Clang driver to construct individual jobs based on the; driver arguments and also in the ``CompilerInvocation::CreateFromArgs`` function; that parses the ``-cc1`` frontend arguments. Command Line Generation; -----------------------. Any valid ``CompilerInvocation`` created from a ``-cc1`` command line can be; also serialized back into semantically equivalent command line in a; deterministic manner. This enables features such as implicitly discovered,; explicitly built modules. ..; TODO: Create and link corresponding section in Modules.rst. Adding new Command Line Option; ------------------------------. When adding a new command line option, the first place of interest is the header; file declaring the corresponding options class (e.g. ``CodeGenOptions.h`` for; command line option that affects the code generation). Create new member; variable for the option value:. .. code-block:: diff. class CodeGenOptions : public CodeGenOptionsBase {. + /// List of dynamic shared object files to be loaded as pass plugins.; + std::vector<std::string> PassPlugins;. }. Next, declare the command line interface of the option in the tablegen file; ``clang/include/clang/Driver/Options.td``. This is done by instantiating the; ``Option`` class (defined in ``llvm/include/llvm/Option/OptParser.td``). The; instance is typically created through one of the helper classes that encode the; acceptable ways to specify the option value on the command line:. * ``Flag`` - the option does not accept any value,; * ``Joined`` - the value must immediately follow the option name within the same; argument,; * ``Separate`` - the value must follow the option name in the next command line; argument,; * ``JoinedOrSeparate`` - the value can be specified either as ``Joined`` or; ``Separate``,; * ``CommaJoined`` - the values are comma-separated and must immediately follow; the option name within the same argument (see ``Wl,`` for an example). The helper classes take a list of acceptable prefixes of t",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/InternalsManual.rst:28810,load,loaded,28810,interpreter/llvm-project/clang/docs/InternalsManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/InternalsManual.rst,1,['load'],['loaded']
Performance,"s encoded in the template name.; =qualified: The element type include parents in its name.; =reference: Element declaration and definition references.; =subrange: Subrange encoding information for arrays.; =typename: Template parameters.; =underlying: Underlying type for type definitions. The following attributes describe the debug location information for; a symbol or scope. It includes the symbol percentage coverage and any; gaps within the location layout; ranges determining the code sections; attached to a function. When descriptors are used, the target processor; registers are displayed. .. code-block:: text. =coverage: Symbol location coverage.; =gaps: Missing debug location (gaps).; =location: Symbol debug location.; =range: Debug location ranges.; =register: Processor register names. The following attributes are associated with low level details, such; as: offsets in the binary file; discriminators added to the lines of; inlined functions in order to distinguish specific instances; debug; lines state machine registers; elements discarded by the compiler; (inlining) or by the linker optimizations (dead-stripping); system; compile units generated by the MS toolchain in PDBs. .. code-block:: text. =discarded: Discarded elements by the linker.; =discriminator: Discriminators for inlined function instances.; =inserted: Generated inlined abstract references.; =linkage: Object file linkage name.; =offset: Debug information offset.; =qualifier: Line qualifiers (Newstatement, BasicBlock, etc).; =zero: Zero line numbers. The following attribute described specific information for the **PE/COFF**; file format. It includes MS runtime types. .. code-block:: text. =system: Display PDB's MS system elements. The above attributes are grouped into *standard* and *extended*; categories that can be enabled. The *standard* group, contains those attributes that add sufficient; information to describe a logical element and that can cover the; normal situations while dealing with deb",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-debuginfo-analyzer.rst:7595,optimiz,optimizations,7595,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-debuginfo-analyzer.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-debuginfo-analyzer.rst,1,['optimiz'],['optimizations']
Performance,"s explicit to help people who; are transitioning from them to ORC. JIT API Basics; ==============. The purpose of a JIT compiler is to compile code ""on-the-fly"" as it is needed,; rather than compiling whole programs to disk ahead of time as a traditional; compiler does. To support that aim our initial, bare-bones JIT API will have; just two functions:. 1. ``Error addModule(std::unique_ptr<Module> M)``: Make the given IR module; available for execution.; 2. ``Expected<ExecutorSymbolDef> lookup()``: Search for pointers to; symbols (functions or variables) that have been added to the JIT. A basic use-case for this API, executing the 'main' function from a module,; will look like:. .. code-block:: c++. JIT J;; J.addModule(buildModule());; auto *Main = J.lookup(""main"").getAddress().toPtr<int(*)(int, char *[])>();; int Result = Main();. The APIs that we build in these tutorials will all be variations on this simple; theme. Behind this API we will refine the implementation of the JIT to add; support for concurrent compilation, optimization and lazy compilation.; Eventually we will extend the API itself to allow higher-level program; representations (e.g. ASTs) to be added to the JIT. KaleidoscopeJIT; ===============. In the previous section we described our API, now we examine a simple; implementation of it: The KaleidoscopeJIT class [1]_ that was used in the; `Implementing a language with LLVM <LangImpl01.html>`_ tutorials. We will use; the REPL code from `Chapter 7 <LangImpl07.html>`_ of that tutorial to supply the; input for our JIT: Each time the user enters an expression the REPL will add a; new IR module containing the code for that expression to the JIT. If the; expression is a top-level expression like '1+1' or 'sin(x)', the REPL will also; use the lookup method of our JIT class find and execute the code for the; expression. In later chapters of this tutorial we will modify the REPL to enable; new interactions with our JIT class, but for now we will take this setup",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst:3516,concurren,concurrent,3516,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst,2,"['concurren', 'optimiz']","['concurrent', 'optimization']"
Performance,"s for a single work-group are executed in the same CU but may be; executed by different SIMDs.; * Each CU has a single LDS memory shared by the wavefronts of the work-groups; executing on it.; * All LDS operations of a CU are performed as wavefront wide operations in a; global order and involve no caching. Completion is reported to a wavefront in; execution order.; * The LDS memory has multiple request queues shared by the SIMDs of a; CU. Therefore, the LDS operations performed by different wavefronts of a; work-group can be reordered relative to each other, which can result in; reordering the visibility of vector memory operations with respect to LDS; operations of other wavefronts in the same work-group. A ``s_waitcnt; lgkmcnt(0)`` is required to ensure synchronization between LDS operations and; vector memory operations between wavefronts of a work-group, but not between; operations performed by the same wavefront.; * The vector memory operations are performed as wavefront wide operations and; completion is reported to a wavefront in execution order. The exception is; that for GFX7-GFX9 ``flat_load/store/atomic`` instructions can report out of; vector memory order if they access LDS memory, and out of LDS operation order; if they access global memory.; * The vector memory operations access a single vector L1 cache shared by all; SIMDs a CU. Therefore, no special action is required for coherence between the; lanes of a single wavefront, or for coherence between wavefronts in the same; work-group. A ``buffer_wbinvl1_vol`` is required for coherence between; wavefronts executing in different work-groups as they may be executing on; different CUs.; * The scalar memory operations access a scalar L1 cache shared by all wavefronts; on a group of CUs. The scalar and vector L1 caches are not coherent. However,; scalar operations are used in a restricted way so do not impact the memory; model. See :ref:`amdgpu-amdhsa-memory-spaces`.; * The vector and scalar memory operation",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:207380,perform,performed,207380,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['perform'],['performed']
Performance,"s found can be drawn in 3D. This software cannot; be guaranteed to work under all circumstances. It was originally; written to work with a few hundred points in an` XY` space with; similar `X` and `Y` ranges. ![Graph2D drawn with option ""surf1"" and ""tri1 p0""](pictures/0300005A.png). ``` {.cpp}; {; TCanvas *c = new TCanvas(""c"",""Graph2D example"",0,0,700,600);; Double_t x, y, z, P = 6.;; Int_t np = 200;; TGraph2D *dt = new TGraph2D();; TRandom *r = new TRandom();. for (Int_t N=0; N<np; N++) {; x = 2*P*(r->Rndm(N))-P;; y = 2*P*(r->Rndm(N))-P;; z = (sin(x)/x)*(sin(y)/y)+0.2;; dt->SetPoint(N,x,y,z);; }; gStyle->SetPalette(55);; dt->Draw(""surf1""); // use ""surf1"" to generate the left picture; } // use ""tri1 p0"" to generate the right one; ```. A more complete example is `$ROOTSYS/tutorials/fit/graph2dfit.C` that; produces the next figure. ![Output of macro graph2dfit.C](pictures/0300005C.png). ## TGraph2DErrors. A **`TGraph2DErrors`** is a **`TGraph2D`** with errors. It is useful to; perform fits with errors on a 2D graph. An example is the macro; `$ROOTSYS/tutorials/graphs/graph2derrorsfit.C`. ## Fitting a Graph. The graph `Fit` method in general works the same way as the `TH1::Fit`.; See ""Fitting Histograms"". ## Setting the Graph's Axis Title. To give the axis of a graph a title you need to draw the graph first,; only then does it actually have an axis object. Once drawn, you set the; title by getting the axis and calling the `TAxis::SetTitle` method, and; if you want to center it, you can call the `TAxis::CenterTitle` method. Assuming that `n, x,` and `y` are defined. Next code sets the titles of; the `x` and `y` axes. ``` {.cpp}; root[] gr5 = new TGraph(n,x,y); root[] gr5->Draw(); <TCanvas::MakeDefCanvas>: created default TCanvas with name c1; root[] gr5->Draw(""ALP""); root[] gr5->GetXaxis()->SetTitle(""X-Axis""); root[] gr5->GetYaxis()->SetTitle(""Y-Axis""); root[] gr5->GetXaxis()->CenterTitle(); root[] gr5->GetYaxis()->CenterTitle(); root[] gr5->Draw(""ALP""); ```. For more gr",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Graphs.md:20660,perform,perform,20660,documentation/users-guide/Graphs.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Graphs.md,1,['perform'],['perform']
Performance,"s from checking loads in; this way, so it is important to work to optimize this. However, beyond making; the instruction sequences to *apply* the checks efficient (for example by; avoiding `pushfq` and `popfq` sequences), the only significant optimization is; to check fewer loads without introducing a vulnerability. We apply several; techniques to accomplish that. ###### Don't check loads from compile-time constant stack offsets. We implement this optimization on x86 by skipping the checking of loads which; use a fixed frame pointer offset. The result of this optimization is that patterns like reloading a spilled; register or accessing a global field don't get checked. This is a very; significant performance win. ###### Don't check dependent loads. A core part of why this mitigation strategy works is that it establishes a; data-flow check on the loaded address. However, this means that if the address; itself was already loaded using a checked load, there is no need to check a; dependent load provided it is within the same basic block as the checked load,; and therefore has no additional predicates guarding it. Consider code like the; following:; ```; ... .LBB0_4: # %danger; movq (%rcx), %rdi; movl (%rdi), %edx; ```. This will get transformed into:; ```; ... .LBB0_4: # %danger; cmovneq %r8, %rax # Conditionally update predicate state.; orq %rax, %rcx # Mask the pointer if misspeculating.; movq (%rcx), %rdi # Hardened load.; movl (%rdi), %edx # Unhardened load due to dependent addr.; ```. This doesn't check the load through `%rdi` as that pointer is dependent on a; checked load already. ###### Protect large, load-heavy blocks with a single lfence. It may be worth using a single `lfence` instruction at the start of a block; which begins with a (very) large number of loads that require independent; protection *and* which require hardening the address of the load. However, this; is unlikely to be profitable in practice. The latency hit of the hardening; would need to exc",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md:35493,load,loaded,35493,interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,4,['load'],"['load', 'loaded']"
Performance,"s guaranteed to be the first PHI node in; the loop header block.; * Any pointer arithmetic recurrences are raised to use array subscripts. If the trip count of a loop is computable, this pass also makes the following; changes:. * The exit condition for the loop is canonicalized to compare the induction; value against the exit value. This turns loops like:. .. code-block:: c++. for (i = 7; i*i < 1000; ++i). into. .. code-block:: c++. for (i = 0; i != 25; ++i). * Any use outside of the loop of an expression derived from the indvar is; changed to compute the derived value outside of the loop, eliminating the; dependence on the exit value of the induction variable. If the only purpose; of the loop is to compute the exit value of some derived expression, this; transformation will make the loop dead. This transformation should be followed by strength reduction after all of the; desired loop transformations have been performed. Additionally, on targets; where it is profitable, the loop could be transformed to count down to zero; (the ""do loop"" optimization). ``inline``: Function Integration/Inlining; -----------------------------------------. Bottom-up inlining of functions into callees. .. _passes-instcombine:. ``instcombine``: Combine redundant instructions; -----------------------------------------------. Combine instructions to form fewer, simple instructions. This pass does not; modify the CFG. This pass is where algebraic simplification happens. This pass combines things like:. .. code-block:: llvm. %Y = add i32 %X, 1; %Z = add i32 %Y, 1. into:. .. code-block:: llvm. %Z = add i32 %X, 2. This is a simple worklist driven algorithm. This pass guarantees that the following canonicalizations are performed on the; program:. #. If a binary operator has a constant operand, it is moved to the right-hand; side.; #. Bitwise operators with constant operands are always grouped so that shifts; are performed first, then ``or``\ s, then ``and``\ s, then ``xor``\ s.; #. Compare instru",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:19120,optimiz,optimization,19120,interpreter/llvm-project/llvm/docs/Passes.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst,1,['optimiz'],['optimization']
Performance,"s have their; own s_waitcnt; lgkmcnt(0) and so do; not need to be; considered.); - s_waitcnt vmcnt(0); must happen after; preceding; global/generic load; atomic/store; atomic/atomicrmw; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; vmcnt(0) and so do; not need to be; considered.); - Ensures any; preceding; sequential; consistent global; memory instructions; have completed; before executing; this sequentially; consistent; instruction. This; prevents reordering; a seq_cst store; followed by a; seq_cst load. (Note; that seq_cst is; stronger than; acquire/release as; the reordering of; load acquire; followed by a store; release is; prevented by the; s_waitcnt of; the release, but; there is nothing; preventing a store; release followed by; load acquire from; completing out of; order. The s_waitcnt; could be placed after; seq_store or before; the seq_load. We; choose the load to; make the s_waitcnt be; as late as possible; so that the store; may have already; completed.). 2. *Following; instructions same as; corresponding load; atomic acquire,; except must generate; all instructions even; for OpenCL.*; store atomic seq_cst - singlethread - global *Same as corresponding; - wavefront - local store atomic release,; - workgroup - generic except must generate; - agent all instructions even; - system for OpenCL.*; atomicrmw seq_cst - singlethread - global *Same as corresponding; - wavefront - local atomicrmw acq_rel,; - workgroup - generic except must generate; - agent all instructions even; - system for OpenCL.*; fence seq_cst - singlethread *none* *Same as corresponding; - wavefront fence acq_rel,; - workgroup except must generate; - agent all instructions even; - system for OpenCL.*; ============ ============ ============== ========== ================================. .. _amdgpu-amdhsa-memory-model-gfx10-gfx11:. Memory Model GFX10-GFX11; ++++++++++++++++++++++++. For GFX10-GFX11:. * Each agent ha",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:334153,load,load,334153,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"s have their; own s_waitcnt; lgkmcnt(0) and so do; not need to be; considered.); - s_waitcnt vmcnt(0); must happen after; preceding; global/generic load; atomic/store; atomic/atomicrmw; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; vmcnt(0) and so do; not need to be; considered.); - Ensures any; preceding; sequential; consistent global; memory instructions; have completed; before executing; this sequentially; consistent; instruction. This; prevents reordering; a seq_cst store; followed by a; seq_cst load. (Note; that seq_cst is; stronger than; acquire/release as; the reordering of; load acquire; followed by a store; release is; prevented by the; s_waitcnt of; the release, but; there is nothing; preventing a store; release followed by; load acquire from; completing out of; order. The s_waitcnt; could be placed after; seq_store or before; the seq_load. We; choose the load to; make the s_waitcnt be; as late as possible; so that the store; may have already; completed.). 2. *Following; instructions same as; corresponding load; atomic acquire,; except must generate; all instructions even; for OpenCL.*; store atomic seq_cst - singlethread - global *Same as corresponding; - wavefront - local store atomic release,; - workgroup - generic except must generate; - agent all instructions even; - system for OpenCL.*; atomicrmw seq_cst - singlethread - global *Same as corresponding; - wavefront - local atomicrmw acq_rel,; - workgroup - generic except must generate; - agent all instructions even; - system for OpenCL.*; fence seq_cst - singlethread *none* *Same as corresponding; - wavefront fence acq_rel,; - workgroup except must generate; - agent all instructions even; - system for OpenCL.*; ============ ============ ============== ========== ================================. .. _amdgpu-amdhsa-memory-model-gfx90a:. Memory Model GFX90A; +++++++++++++++++++. For GFX90A:. * Each agent has multiple shader ar",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:233159,load,load,233159,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"s have their; own s_waitcnt; lgkmcnt(0) and so do; not need to be; considered.); - s_waitcnt vmcnt(0); must happen after; preceding; global/generic load; atomic/store; atomic/atomicrmw; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; vmcnt(0) and so do; not need to be; considered.); - Ensures any; preceding; sequential; consistent global; memory instructions; have completed; before executing; this sequentially; consistent; instruction. This; prevents reordering; a seq_cst store; followed by a; seq_cst load. (Note; that seq_cst is; stronger than; acquire/release as; the reordering of; load acquire; followed by a store; release is; prevented by the; s_waitcnt of; the release, but; there is nothing; preventing a store; release followed by; load acquire from; completing out of; order. The s_waitcnt; could be placed after; seq_store or before; the seq_load. We; choose the load to; make the s_waitcnt be; as late as possible; so that the store; may have already; completed.). 2. *Following; instructions same as; corresponding load; atomic acquire,; except must generate; all instructions even; for OpenCL.*; store atomic seq_cst - singlethread - global *Same as corresponding; - wavefront - local store atomic release,; - workgroup - generic except must generate; - agent all instructions even; - system for OpenCL.*; atomicrmw seq_cst - singlethread - global *Same as corresponding; - wavefront - local atomicrmw acq_rel,; - workgroup - generic except must generate; - agent all instructions even; - system for OpenCL.*; fence seq_cst - singlethread *none* *Same as corresponding; - wavefront fence acq_rel,; - workgroup except must generate; - agent all instructions even; - system for OpenCL.*; ============ ============ ============== ========== ================================. .. _amdgpu-amdhsa-memory-model-gfx942:. Memory Model GFX942; +++++++++++++++++++. For GFX942:. * Each agent has multiple shader ar",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:283307,load,load,283307,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"s i32*.; br i1 %Condition, label %cond_true, label %cond_false. cond_true:; %X.0 = load i32, i32* @G; store i32 %X.0, i32* %X ; Update X; br label %cond_next. cond_false:; %X.1 = load i32, i32* @H; store i32 %X.1, i32* %X ; Update X; br label %cond_next. cond_next:; %X.2 = load i32, i32* %X ; Read X; ret i32 %X.2; }. With this, we have discovered a way to handle arbitrary mutable; variables without the need to create Phi nodes at all:. #. Each mutable variable becomes a stack allocation.; #. Each read of the variable becomes a load from the stack.; #. Each update of the variable becomes a store to the stack.; #. Taking the address of a variable just uses the stack address; directly. While this solution has solved our immediate problem, it introduced; another one: we have now apparently introduced a lot of stack traffic; for very simple and common operations, a major performance problem.; Fortunately for us, the LLVM optimizer has a highly-tuned optimization; pass named ""mem2reg"" that handles this case, promoting allocas like this; into SSA registers, inserting Phi nodes as appropriate. If you run this; example through the pass, for example, you'll get:. .. code-block:: bash. $ llvm-as < example.ll | opt -passes=mem2reg | llvm-dis; @G = weak global i32 0; @H = weak global i32 0. define i32 @test(i1 %Condition) {; entry:; br i1 %Condition, label %cond_true, label %cond_false. cond_true:; %X.0 = load i32, i32* @G; br label %cond_next. cond_false:; %X.1 = load i32, i32* @H; br label %cond_next. cond_next:; %X.01 = phi i32 [ %X.1, %cond_false ], [ %X.0, %cond_true ]; ret i32 %X.01; }. The mem2reg pass implements the standard ""iterated dominance frontier""; algorithm for constructing SSA form and has a number of optimizations; that speed up (very common) degenerate cases. The mem2reg optimization; pass is the answer to dealing with mutable variables, and we highly; recommend that you depend on it. Note that mem2reg only works on; variables in certain circumstances:. #. mem2",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:6690,optimiz,optimizer,6690,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,3,"['optimiz', 'tune']","['optimization', 'optimizer', 'tuned']"
Performance,"s implemented by replacing each reference to a native ABI; function with a reference to a function which uses the instrumented ABI.; Such functions are automatically-generated wrappers for the native functions.; For example, given the ABI list example provided in the user manual, the; following wrappers will be generated under the args ABI:. .. code-block:: llvm. define linkonce_odr { i8*, i16 } @""dfsw$malloc""(i64 %0, i16 %1) {; entry:; %2 = call i8* @malloc(i64 %0); %3 = insertvalue { i8*, i16 } undef, i8* %2, 0; %4 = insertvalue { i8*, i16 } %3, i16 0, 1; ret { i8*, i16 } %4; }. define linkonce_odr { i32, i16 } @""dfsw$tolower""(i32 %0, i16 %1) {; entry:; %2 = call i32 @tolower(i32 %0); %3 = insertvalue { i32, i16 } undef, i32 %2, 0; %4 = insertvalue { i32, i16 } %3, i16 %1, 1; ret { i32, i16 } %4; }. define linkonce_odr { i8*, i16 } @""dfsw$memcpy""(i8* %0, i8* %1, i64 %2, i16 %3, i16 %4, i16 %5) {; entry:; %labelreturn = alloca i16; %6 = call i8* @__dfsw_memcpy(i8* %0, i8* %1, i64 %2, i16 %3, i16 %4, i16 %5, i16* %labelreturn); %7 = load i16* %labelreturn; %8 = insertvalue { i8*, i16 } undef, i8* %6, 0; %9 = insertvalue { i8*, i16 } %8, i16 %7, 1; ret { i8*, i16 } %9; }. As an optimization, direct calls to native ABI functions will call the; native ABI function directly and the pass will compute the appropriate label; internally. This has the advantage of reducing the number of union operations; required when the return value label is known to be zero (i.e. ``discard``; functions, or ``functional`` functions with known unlabelled arguments). Checking ABI Consistency; ------------------------. DFSan changes the ABI of each function in the module. This makes it possible; for a function with the native ABI to be called with the instrumented ABI,; or vice versa, thus possibly invoking undefined behavior. A simple way; of statically detecting instances of this problem is to append the suffix; "".dfsan"" to the name of each instrumented-ABI function. This will not catch ever",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/DataFlowSanitizerDesign.rst:12101,load,load,12101,interpreter/llvm-project/clang/docs/DataFlowSanitizerDesign.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/DataFlowSanitizerDesign.rst,1,['load'],['load']
Performance,"s implements various Loop Invariant Code Motion related; transformations. It uses the ``AliasAnalysis`` interface for several different; transformations:. * It uses mod/ref information to hoist or sink load instructions out of loops if; there are no instructions in the loop that modifies the memory loaded. * It uses mod/ref information to hoist function calls out of loops that do not; write to memory and are loop-invariant. * It uses alias information to promote memory objects that are loaded and stored; to in loops to live in a register instead. It can do this if there are no may; aliases to the loaded/stored memory location. The ``-argpromotion`` pass; ^^^^^^^^^^^^^^^^^^^^^^^^^^. The ``-argpromotion`` pass promotes by-reference arguments to be passed in; by-value instead. In particular, if pointer arguments are only loaded from it; passes in the value loaded instead of the address to the function. This pass; uses alias information to make sure that the value loaded from the argument; pointer is not modified between the entry of the function and any load of the; pointer. The ``-gvn``, ``-memcpyopt``, and ``-dse`` passes; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. These passes use AliasAnalysis information to reason about loads and stores. .. _the clients:. Clients for debugging and evaluation of implementations; -------------------------------------------------------. These passes are useful for evaluating the various alias analysis; implementations. You can use them with commands like:. .. code-block:: bash. % opt -ds-aa -aa-eval foo.bc -disable-output -stats. The ``-print-alias-sets`` pass; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. The ``-print-alias-sets`` pass is exposed as part of the ``opt`` tool to print; out the Alias Sets formed by the `AliasSetTracker`_ class. This is useful if; you're using the ``AliasSetTracker`` class. To use it, use something like:. .. code-block:: bash. % opt -ds-aa -print-alias-sets -disable-output. The ``-aa-eval`` pass; ^^^^^^^^^^^^",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AliasAnalysis.rst:28545,load,loaded,28545,interpreter/llvm-project/llvm/docs/AliasAnalysis.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AliasAnalysis.rst,2,['load'],"['load', 'loaded']"
Performance,"s in C++. For ``__strong`` lvalues, moving is equivalent; to loading the lvalue with primitive semantics, writing a null pointer to it; with primitive semantics, and then releasing the result of the load at the end; of the current full-expression. For all other lvalues, moving is equivalent to; reading the object. .. _arc.ownership.restrictions:. Restrictions; ------------. .. _arc.ownership.restrictions.weak:. Weak-unavailable types; ^^^^^^^^^^^^^^^^^^^^^^. It is explicitly permitted for Objective-C classes to not support ``__weak``; references. It is undefined behavior to perform an operation with weak; assignment semantics with a pointer to an Objective-C object whose class does; not support ``__weak`` references. .. admonition:: Rationale. Historically, it has been possible for a class to provide its own; reference-count implementation by overriding ``retain``, ``release``, etc.; However, weak references to an object require coordination with its class's; reference-count implementation because, among other things, weak loads and; stores must be atomic with respect to the final release. Therefore, existing; custom reference-count implementations will generally not support weak; references without additional effort. This is unavoidable without breaking; binary compatibility. A class may indicate that it does not support weak references by providing the; ``objc_arc_weak_reference_unavailable`` attribute on the class's interface declaration. A; retainable object pointer type is **weak-unavailable** if; is a pointer to an (optionally protocol-qualified) Objective-C class ``T`` where; ``T`` or one of its superclasses has the ``objc_arc_weak_reference_unavailable``; attribute. A program is ill-formed if it applies the ``__weak`` ownership; qualifier to a weak-unavailable type or if the value operand of a weak; assignment operation has a weak-unavailable type. .. _arc.ownership.restrictions.autoreleasing:. Storage duration of ``__autoreleasing`` objects; ^^^^^^^^^^^^^^^^",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst:41611,load,loads,41611,interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,1,['load'],['loads']
Performance,"s in the set"". The; choice of constraint is made independently for each constraint in the; constraint list. 2) Use ""``|``"" between constraint code sets, creating alternatives. Every; constraint in the constraint list must have the same number of alternative; sets. With this syntax, the same alternative in *all* of the items in the; constraint list will be chosen together. Putting those together, you might have a two operand constraint string like; ``""rm|r,ri|rm""``. This indicates that if operand 0 is ``r`` or ``m``, then; operand 1 may be one of ``r`` or ``i``. If operand 0 is ``r``, then operand 1; may be one of ``r`` or ``m``. But, operand 0 and 1 cannot both be of type m. However, the use of either of the alternatives features is *NOT* recommended, as; LLVM is not able to make an intelligent choice about which one to use. (At the; point it currently needs to choose, not enough information is available to do so; in a smart way.) Thus, it simply tries to make a choice that's most likely to; compile, not one that will be optimal performance. (e.g., given ""``rm``"", it'll; always choose to use memory, not registers). And, if given multiple registers,; or multiple register classes, it will simply choose the first one. (In fact, it; doesn't currently even ensure explicitly specified physical registers are; unique, so specifying multiple physical registers as alternatives, like; ``{r11}{r12},{r11}{r12}``, will assign r11 to both operands, not at all what was; intended.). Supported Constraint Code List; """""""""""""""""""""""""""""""""""""""""""""""""""""""""""". The constraint codes are, in general, expected to behave the same way they do in; GCC. LLVM's support is often implemented on an 'as-needed' basis, to support C; inline asm code which was supported by GCC. A mismatch in behavior between LLVM; and GCC likely indicates a bug in LLVM. Some constraint codes are typically supported by all targets:. - ``r``: A register in the target's general purpose register class.; - ``m``: A memory address oper",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:219934,perform,performance,219934,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performance']
Performance,"s in; each `FUNCTION_BLOCK`_. In version 0, each value defined by an instruction is assigned an ID; unique to the function. Function-level value IDs are assigned starting from; ``NumModuleValues`` since they share the same namespace as module-level; values. The value enumerator resets after each function. When a value is; an operand of an instruction, the value ID is used to represent the operand.; For large functions or large modules, these operand values can be large. The encoding in version 1 attempts to avoid large operand values; in common cases. Instead of using the value ID directly, operands are; encoded as relative to the current instruction. Thus, if an operand; is the value defined by the previous instruction, the operand; will be encoded as 1. For example, instead of. .. code-block:: none. #n = load #n-1; #n+1 = icmp eq #n, #const0; br #n+1, label #(bb1), label #(bb2). version 1 will encode the instructions as. .. code-block:: none. #n = load #1; #n+1 = icmp eq #1, (#n+1)-#const0; br #1, label #(bb1), label #(bb2). Note in the example that operands which are constants also use; the relative encoding, while operands like basic block labels; do not use the relative encoding. Forward references will result in a negative value.; This can be inefficient, as operands are normally encoded; as unsigned VBRs. However, forward references are rare, except in the; case of phi instructions. For phi instructions, operands are encoded as; `Signed VBRs`_ to deal with forward references. In version 2, the meaning of module records ``FUNCTION``, ``GLOBALVAR``,; ``ALIAS``, ``IFUNC`` and ``COMDAT`` change such that the first two operands; specify an offset and size of a string in a string table (see `STRTAB_BLOCK; Contents`_), the function name is removed from the ``FNENTRY`` record in the; value symbol table, and the top-level ``VALUE_SYMTAB_BLOCK`` may only contain; ``FNENTRY`` records. MODULE_CODE_TRIPLE Record; ^^^^^^^^^^^^^^^^^^^^^^^^^. ``[TRIPLE, ...string...]``. The ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BitCodeFormat.rst:24362,load,load,24362,interpreter/llvm-project/llvm/docs/BitCodeFormat.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BitCodeFormat.rst,1,['load'],['load']
Performance,"s information about; the range of the index, you may wish to manually extend indices to machine; register width using a zext instruction. When to specify alignment; ^^^^^^^^^^^^^^^^^^^^^^^^^^; LLVM will always generate correct code if you don’t specify alignment, but may; generate inefficient code. For example, if you are targeting MIPS (or older; ARM ISAs) then the hardware does not handle unaligned loads and stores, and; so you will enter a trap-and-emulate path if you do a load or store with; lower-than-natural alignment. To avoid this, LLVM will emit a slower; sequence of loads, shifts and masks (or load-right + load-left on MIPS) for; all cases where the load / store does not have a sufficiently high alignment; in the IR. The alignment is used to guarantee the alignment on allocas and globals,; though in most cases this is unnecessary (most targets have a sufficiently; high default alignment that they’ll be fine). It is also used to provide a; contract to the back end saying ‘either this load/store has this alignment, or; it is undefined behavior’. This means that the back end is free to emit; instructions that rely on that alignment (and mid-level optimizers are free to; perform transforms that require that alignment). For x86, it doesn’t make; much difference, as almost all instructions are alignment-independent. For; MIPS, it can make a big difference. Note that if your loads and stores are atomic, the backend will be unable to; lower an under aligned access into a sequence of natively aligned accesses.; As a result, alignment is mandatory for atomic loads and stores. Other Things to Consider; ^^^^^^^^^^^^^^^^^^^^^^^^. #. Use ptrtoint/inttoptr sparingly (they interfere with pointer aliasing; analysis), prefer GEPs. #. Prefer globals over inttoptr of a constant address - this gives you; dereferencability information. In MCJIT, use getSymbolAddress to provide; actual address. #. Be wary of ordered and atomic memory operations. They are hard to optimize; and may",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Frontend/PerformanceTips.rst:5381,load,load,5381,interpreter/llvm-project/llvm/docs/Frontend/PerformanceTips.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Frontend/PerformanceTips.rst,1,['load'],['load']
Performance,"s inline constant; registers ``SRC_SHARED_BASE/LIMIT`` and ``SRC_PRIVATE_BASE/LIMIT``. In 64 bit; address mode the aperture sizes are 2^32 bytes and the base is aligned to 2^32; which makes it easier to convert from flat to segment or segment to flat. Image and Samplers; ~~~~~~~~~~~~~~~~~~. Image and sample handles created by an HSA compatible runtime (see; :ref:`amdgpu-os`) are 64-bit addresses of a hardware 32-byte V# and 48 byte S#; object respectively. In order to support the HSA ``query_sampler`` operations; two extra dwords are used to store the HSA BRIG enumeration values for the; queries that are not trivially deducible from the S# representation. HSA Signals; ~~~~~~~~~~~. HSA signal handles created by an HSA compatible runtime (see :ref:`amdgpu-os`); are 64-bit addresses of a structure allocated in memory accessible from both the; CPU and GPU. The structure is defined by the runtime and subject to change; between releases. For example, see [AMD-ROCm-github]_. .. _amdgpu-amdhsa-hsa-aql-queue:. HSA AQL Queue; ~~~~~~~~~~~~~. The HSA AQL queue structure is defined by an HSA compatible runtime (see; :ref:`amdgpu-os`) and subject to change between releases. For example, see; [AMD-ROCm-github]_. For some processors it contains fields needed to implement; certain language features such as the flat address aperture bases. It also; contains fields used by CP such as managing the allocation of scratch memory. .. _amdgpu-amdhsa-kernel-descriptor:. Kernel Descriptor; ~~~~~~~~~~~~~~~~~. A kernel descriptor consists of the information needed by CP to initiate the; execution of a kernel, including the entry point address of the machine code; that implements the kernel. Code Object V3 Kernel Descriptor; ++++++++++++++++++++++++++++++++. CP microcode requires the Kernel descriptor to be allocated on 64-byte; alignment. The fields used by CP for code objects before V3 also match those specified in; :ref:`amdgpu-amdhsa-kernel-descriptor-v3-table`. .. table:: Code Object V3 Kern",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:157478,queue,queue,157478,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['queue'],['queue']
Performance,"s integration you can press the bound key and clang-format will; format the current line in NORMAL and INSERT mode or the selected region in; VISUAL mode. The line or region is extended to the next bigger syntactic; entity. It operates on the current, potentially unsaved buffer and does not create; or save any files. To revert a formatting, just undo. An alternative option is to format changes when saving a file and thus to; have a zero-effort integration into the coding workflow. To do this, add this to; your `.vimrc`:. .. code-block:: vim. function! Formatonsave(); let l:formatdiff = 1; pyf <path-to-this-file>/clang-format.py; endfunction; autocmd BufWritePre *.h,*.cc,*.cpp call Formatonsave(). Emacs Integration; =================. Similar to the integration for :program:`vim`, there is an integration for; :program:`emacs`. It can be found at `clang/tools/clang-format/clang-format.el`; and used by adding this to your `.emacs`:. .. code-block:: common-lisp. (load ""<path-to-clang>/tools/clang-format/clang-format.el""); (global-set-key [C-M-tab] 'clang-format-region). This binds the function `clang-format-region` to C-M-tab, which then formats the; current line or selected region. BBEdit Integration; ==================. :program:`clang-format` cannot be used as a text filter with BBEdit, but works; well via a script. The AppleScript to do this integration can be found at; `clang/tools/clang-format/clang-format-bbedit.applescript`; place a copy in; `~/Library/Application Support/BBEdit/Scripts`, and edit the path within it to; point to your local copy of :program:`clang-format`. With this integration you can select the script from the Script menu and; :program:`clang-format` will format the selection. Note that you can rename the; menu item by renaming the script, and can assign the menu item a keyboard; shortcut in the BBEdit preferences, under Menus & Shortcuts. CLion Integration; =================. :program:`clang-format` is integrated into `CLion <https://www.jetbr",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ClangFormat.rst:8230,load,load,8230,interpreter/llvm-project/clang/docs/ClangFormat.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ClangFormat.rst,1,['load'],['load']
Performance,"s into SSA values, which can; eventually be placed in physical registers or memory locations. To track SSA; values through compilation, when objects are promoted to SSA values an; ``llvm.dbg.value`` intrinsic is created for each assignment, recording the; variable's new location. Compared with the ``llvm.dbg.declare`` intrinsic:. * A dbg.value terminates the effect of any preceding dbg.values for (any; overlapping fragments of) the specified variable.; * The dbg.value's position in the IR defines where in the instruction stream; the variable's value changes.; * Operands can be constants, indicating the variable is assigned a; constant value. Care must be taken to update ``llvm.dbg.value`` intrinsics when optimization; passes alter or move instructions and blocks -- the developer could observe such; changes reflected in the value of variables when debugging the program. For any; execution of the optimized program, the set of variable values presented to the; developer by the debugger should not show a state that would never have existed; in the execution of the unoptimized program, given the same input. Doing so; risks misleading the developer by reporting a state that does not exist,; damaging their understanding of the optimized program and undermining their; trust in the debugger. Sometimes perfectly preserving variable locations is not possible, often when a; redundant calculation is optimized out. In such cases, a ``llvm.dbg.value``; with operand ``poison`` should be used, to terminate earlier variable locations; and let the debugger present ``optimized out`` to the developer. Withholding; these potentially stale variable values from the developer diminishes the; amount of available debug information, but increases the reliability of the; remaining information. To illustrate some potential issues, consider the following example:. .. code-block:: llvm. define i32 @foo(i32 %bar, i1 %cond) {; entry:; call @llvm.dbg.value(metadata i32 0, metadata !1, metadata !2); b",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst:18962,optimiz,optimized,18962,interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,1,['optimiz'],['optimized']
Performance,"s intrinsic is lowered to the ``val``. .. _int_assume:. '``llvm.assume``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare void @llvm.assume(i1 %cond). Overview:; """""""""""""""""". The ``llvm.assume`` allows the optimizer to assume that the provided; condition is true. This information can then be used in simplifying other parts; of the code. More complex assumptions can be encoded as; :ref:`assume operand bundles <assume_opbundles>`. Arguments:; """""""""""""""""""". The argument of the call is the condition which the optimizer may assume is; always true. Semantics:; """""""""""""""""""". The intrinsic allows the optimizer to assume that the provided condition is; always true whenever the control flow reaches the intrinsic call. No code is; generated for this intrinsic, and instructions that contribute only to the; provided condition are not used for code generation. If the condition is; violated during execution, the behavior is undefined. Note that the optimizer might limit the transformations performed on values; used by the ``llvm.assume`` intrinsic in order to preserve the instructions; only used to form the intrinsic's input argument. This might prove undesirable; if the extra information provided by the ``llvm.assume`` intrinsic does not cause; sufficient overall improvement in code quality. For this reason,; ``llvm.assume`` should not be used to document basic mathematical invariants; that the optimizer can otherwise deduce or facts that are of little use to the; optimizer. .. _int_ssa_copy:. '``llvm.ssa.copy``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare type @llvm.ssa.copy(type returned %operand) memory(none). Arguments:; """""""""""""""""""". The first argument is an operand which is used as the returned value. Overview:; """""""""""""""""""". The ``llvm.ssa.copy`` intrinsic can be used to attach information to; operations by copying them and giving them new names. For example,; the PredicateInfo utility uses it to build Extended SSA form, and;",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:935947,optimiz,optimizer,935947,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,2,"['optimiz', 'perform']","['optimizer', 'performed']"
Performance,"s is not strictly required,; it provides better call information, which improves the accuracy of; the profile data. 3. Convert the collected profile data to LLVM's sample profile format.; This is currently supported via the AutoFDO converter ``create_llvm_prof``.; It is available at https://github.com/google/autofdo. Once built and; installed, you can convert the ``perf.data`` file to LLVM using; the command:. .. code-block:: console. $ create_llvm_prof --binary=./code --out=code.prof. This will read ``perf.data`` and the binary file ``./code`` and emit; the profile data in ``code.prof``. Note that if you ran ``perf``; without the ``-b`` flag, you need to use ``--use_lbr=false`` when; calling ``create_llvm_prof``. Alternatively, the LLVM tool ``llvm-profgen`` can also be used to generate; the LLVM sample profile:. .. code-block:: console. $ llvm-profgen --binary=./code --output=code.prof--perfdata=perf.data. 4. Build the code again using the collected profile. This step feeds; the profile back to the optimizers. This should result in a binary; that executes faster than the original one. Note that you are not; required to build the code with the exact same arguments that you; used in the first step. The only requirement is that you build the code; with ``-gline-tables-only`` and ``-fprofile-sample-use``. .. code-block:: console. $ clang++ -O2 -gline-tables-only -fprofile-sample-use=code.prof code.cc -o code. [OPTIONAL] Sampling-based profiles can have inaccuracies or missing block/; edge counters. The profile inference algorithm (profi) can be used to infer; missing blocks and edge counts, and improve the quality of profile data.; Enable it with ``-fsample-profile-use-profi``. .. code-block:: console. $ clang++ -O2 -gline-tables-only -fprofile-sample-use=code.prof \; -fsample-profile-use-profi code.cc -o code. Sample Profile Formats; """""""""""""""""""""""""""""""""""""""""""". Since external profilers generate profile data in a variety of custom formats,; the data generated by the profil",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:94660,optimiz,optimizers,94660,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,1,['optimiz'],['optimizers']
Performance,"s may benefit if the node is created even if there; is a name conflict. During the CTU analysis of certain projects, we recognized; that there are global declarations which collide with declarations from other; translation units, but they are not referenced outside from their translation; unit. These declarations should be in an unnamed namespace ideally. If we treat; these collisions liberally then CTU analysis can find more results. Note, the; feature be able to choose between name conflict handling strategies is still an; ongoing work. .. _CFG:. The ``CFG`` class; -----------------. The ``CFG`` class is designed to represent a source-level control-flow graph; for a single statement (``Stmt*``). Typically instances of ``CFG`` are; constructed for function bodies (usually an instance of ``CompoundStmt``), but; can also be instantiated to represent the control-flow of any class that; subclasses ``Stmt``, which includes simple expressions. Control-flow graphs; are especially useful for performing `flow- or path-sensitive; <https://en.wikipedia.org/wiki/Data_flow_analysis#Sensitivities>`_ program; analyses on a given function. Basic Blocks; ^^^^^^^^^^^^. Concretely, an instance of ``CFG`` is a collection of basic blocks. Each basic; block is an instance of ``CFGBlock``, which simply contains an ordered sequence; of ``Stmt*`` (each referring to statements in the AST). The ordering of; statements within a block indicates unconditional flow of control from one; statement to the next. :ref:`Conditional control-flow; <ConditionalControlFlow>` is represented using edges between basic blocks. The; statements within a given ``CFGBlock`` can be traversed using the; ``CFGBlock::*iterator`` interface. A ``CFG`` object owns the instances of ``CFGBlock`` within the control-flow; graph it represents. Each ``CFGBlock`` within a CFG is also uniquely numbered; (accessible via ``CFGBlock::getBlockID()``). Currently the number is based on; the ordering the blocks were created, but no ass",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/InternalsManual.rst:112282,perform,performing,112282,interpreter/llvm-project/clang/docs/InternalsManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/InternalsManual.rst,1,['perform'],['performing']
Performance,"s nice. **Gabor:**. At this point, I am a bit wondering about two questions. * When should something belong to a checker and when should something belong to the engine?; Sometimes we model library aspects in the engine and model language constructs in checkers. * What is the checker programming model that we are aiming for? Maximum freedom or more easy checker development?. I think if we aim for maximum freedom, we do not need to worry about the; potential stress on checkers, and we can introduce abstractions to mitigate that; later on.; If we want to simplify the API, then maybe it makes more sense to move language; construct modeling to the engine when the checker API is not sufficient instead; of complicating the API. Right now I have no preference or objections between the alternatives but there; are some random thoughts:. * Maybe it would be great to have a guideline how to evolve the analyzer and; follow it, so it can help us to decide in similar situations. * I do care about performance in this case. The reason is that we have a; limited performance budget. And I think we should not expect most of the checker; writers to add modeling of language constructs. So, in my opinion, it is ok to; have less nice/more verbose API for language modeling if we can have better; performance this way, since it only needs to be done once, and is done by the; framework developers. **Artem:** These are some great questions, i guess it'd be better to discuss; them more openly. As a quick dump of my current mood:. * To me it seems obvious that we need to aim for a checker API that is both; simple and powerful. This can probably by keeping the API as powerful as; necessary while providing a layer of simple ready-made solutions on top of it.; Probably a few reusable components for assembling checkers. And this layer; should ideally be pleasant enough to work with, so that people would prefer to; extend it when something is lacking, instead of falling back to the complex; omnipotent",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/analyzer/developer-docs/InitializerLists.rst:5621,perform,performance,5621,interpreter/llvm-project/clang/docs/analyzer/developer-docs/InitializerLists.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/analyzer/developer-docs/InitializerLists.rst,1,['perform'],['performance']
Performance,"s no; older than the; value read by the; fence-paired-atomic. 3. buffer_inv sc0=1. - If not TgSplit execution; mode, omit.; - Ensures that; following; loads will not see; stale data. fence acquire - agent *none* 1. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate; (see comment for; previous fence).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - Must happen before; the following; buffer_inv.; - Ensures that the; fence-paired atomic; has completed; before invalidating; the; cache. Therefore; any following; locations read must; be no older than; the value read by; the; fence-paired-atomic. 2. buffer_inv sc1=1. - Must happen before any; following global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. fence acquire - system *none* 1. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate; (see comment for; previous fence).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:304536,load,load,304536,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"s not contain an observable and shares one or more parameters; with another pdf term in the same product that does contain an observable is automatically picked up as a constraint term.; For example given a dataset D(x) which defines variable x as observable, the default logic works out as follows. F(x,a,b)*G(a,a0,a1) --> G is constraint term (a also appears in F(x)); F(x,a,b)*G(y,c,d) --> G is dropped (factorizing term). A Constrain(y) term in the above example will still force term G(y,c,d) to be interpreted as constraint term; Automatic caching of numeric integral calculations; Integrals that require numeric integrations in two of more dimensions are now automatically cached in the expensive object store.; The expensive object store allows to cache such values between difference instance of integral objects that represent the; same configuration. If integrals are created from an object (function or pdf) that live in a RooWorkspace the ; expensive object cache of the workspace will be used instead of the global store instance, and values stored in the workspace; store will also be persisted if the workspace is persisted. The global caching behavior of integral objects can be ; controlled through RooRealIntegral::setCacheAllNumeric(Int_t nDimNumMin). Miscellaneous improvements data classes. The RooAbsData::tree() method has been restored. It will only return a TTree* pointer for datasets; that are based on a RooTreeDataStore implementation, i.e. not for the composite datasets mentioned below; A new composite data storage class RooCompositeDataStore has been added that allows to construct composite; RooDataSet objects without copying the input data. . // Make 2 input datasets and an index category; RooWorkspace w(""w"",true) ;; w->factory(""Gaussian::g(x[-10,10],m[-10,10],s[3,0.1,10])""); w->factory(""Uniform::u(x)""); w->factory(""index[S,B]""); RooDataSet* d1 = w::g.generate(w::x,1000); RooDataSet* d2 = w::u.generate(w::x,1000). // Make monolithic composite dataset (copies",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v526/index.html:8110,cache,cache,8110,roofit/doc/v526/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v526/index.html,2,['cache'],['cache']
Performance,"s of a single work-group. However, a ``buffer_gl1_inv`` is; required for coherence between wavefronts executing in different work-groups; as they may be executing on different SAs that access different L1s.; * The L1 caches have independent quadrants to service disjoint ranges of virtual; addresses.; * Each L0 cache has a separate request queue per L1 quadrant. Therefore, the; vector and scalar memory operations performed by different wavefronts, whether; executing in the same or different work-groups (which may be executing on; different CUs accessing different L0s), can be reordered relative to each; other. A ``s_waitcnt vmcnt(0) & vscnt(0)`` is required to ensure; synchronization between vector memory operations of different wavefronts. It; ensures a previous vector memory operation has completed before executing a; subsequent vector memory or LDS operation and so can be used to meet the; requirements of acquire, release and sequential consistency.; * The L1 caches use an L2 cache shared by all SAs on the same agent.; * The L2 cache has independent channels to service disjoint ranges of virtual; addresses.; * Each L1 quadrant of a single SA accesses a different L2 channel. Each L1; quadrant has a separate request queue per L2 channel. Therefore, the vector; and scalar memory operations performed by wavefronts executing in different; work-groups (which may be executing on different SAs) of an agent can be; reordered relative to each other. A ``s_waitcnt vmcnt(0) & vscnt(0)`` is; required to ensure synchronization between vector memory operations of; different SAs. It ensures a previous vector memory operation has completed; before executing a subsequent vector memory and so can be used to meet the; requirements of acquire, release and sequential consistency.; * The L2 cache can be kept coherent with other agents on some targets, or ranges; of virtual addresses can be set up to bypass it to ensure system coherence.; * On GFX10.3 and GFX11 a memory attached last lev",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:338745,cache,caches,338745,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['cache'],"['cache', 'caches']"
Performance,"s of other instructions to create new; branches or perform other operations. An implementation of ``analyzeBranch``; requires the helper methods ``removeBranch`` and ``insertBranch`` to manage; subsequent operations. ``analyzeBranch`` should return false indicating success in most circumstances.; ``analyzeBranch`` should only return true when the method is stumped about what; to do, for example, if a block has three terminating branches.; ``analyzeBranch`` may return true if it encounters a terminator it cannot; handle, such as an indirect branch. .. _instruction-selector:. Instruction Selector; ====================. LLVM uses a ``SelectionDAG`` to represent LLVM IR instructions, and nodes of; the ``SelectionDAG`` ideally represent native target instructions. During code; generation, instruction selection passes are performed to convert non-native; DAG instructions into native target-specific instructions. The pass described; in ``XXXISelDAGToDAG.cpp`` is used to match patterns and perform DAG-to-DAG; instruction selection. Optionally, a pass may be defined (in; ``XXXBranchSelector.cpp``) to perform similar DAG-to-DAG operations for branch; instructions. Later, the code in ``XXXISelLowering.cpp`` replaces or removes; operations and data types not supported natively (legalizes) in a; ``SelectionDAG``. TableGen generates code for instruction selection using the following target; description input files:. * ``XXXInstrInfo.td`` --- Contains definitions of instructions in a; target-specific instruction set, generates ``XXXGenDAGISel.inc``, which is; included in ``XXXISelDAGToDAG.cpp``. * ``XXXCallingConv.td`` --- Contains the calling and return value conventions; for the target architecture, and it generates ``XXXGenCallingConv.inc``,; which is included in ``XXXISelLowering.cpp``. The implementation of an instruction selection pass must include a header that; declares the ``FunctionPass`` class or a subclass of ``FunctionPass``. In; ``XXXTargetMachine.cpp``, a Pass Manag",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMBackend.rst:52431,perform,perform,52431,interpreter/llvm-project/llvm/docs/WritingAnLLVMBackend.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMBackend.rst,1,['perform'],['perform']
Performance,"s of the object or a (non-static and non-transient) data member; of the object. If the base class or data member is itself a class, then there will; also be a streamerinfo object in the record for that class. In this way, each; class is recursively decomposed into its atomic elements, each of which is a simple; type (e.g. ""int""). A ""long"" or ""unsigned long"" member is always written; as an 8 byte quantity, even if it occupies only 4 bytes in memory. A data member of a class is marked transient on the line of its declaration by a; comment beginning with ""//!"". Such members are not written to disk, nor is there; any streamerinfo for such a member. A data member that is a C++ pointer (not to be confused with ""pointers to persistent; objects"" described below) is never written to disk as a pointer value. If it is a; pointer to an object, the object itself (or 0 (4 bytes) if the pointer value is NULL); is written. If the declaration line has a comment beginning with ""//->"", this indicates; that the pointer value will never be null, which allows a performance optimization.; Another optimization is that if two or more pointers pointing to the same object are; streamed in the same I/O operation, the object is written only once. The remaining; pointers reference the object through a unique object identifier. This saves space; and avoids the infinite loop that might otherwise arise if the directed graph of object; instance pointer references contains a cycle. If a data member is a pointer to a simple type, the Streamer presumes it is an array,; with the dimension defined in a comment of the form ""//[<length>]"", where length is; either an integer constant or a variable that is an integer data member of the class.; If a variable is used, it must be defined ahead of its use or in a base class. The above describes the function of the StreamerInfo record in decomposing a; self-identifying object if the user uses the streamer generated by ""rootcint"".; There are two reasons why a user ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/io/doc/TFile/README.md:8060,perform,performance,8060,io/doc/TFile/README.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/io/doc/TFile/README.md,2,"['optimiz', 'perform']","['optimization', 'performance']"
Performance,"s omitted, the location description of the data is the same as; the location description of the object. The result of the attribute is obtained by evaluating E with a context that; has a result kind of a location description, an object that is the location; description of the data descriptor, the compilation unit that contains E, an; empty initial stack, and other context elements corresponding to the source; language thread of execution upon which the user is focused, if any. The; result of the evaluation is the location description of the base of the; member entry. *E will typically involve an operation expression that begins with a*; ``DW_OP_push_object_address`` *operation which loads the location; description of the object which can then serve as a descriptor in subsequent; calculation.*. .. note::. Since ``DW_AT_data_member_location``, ``DW_AT_use_location``, and; ``DW_AT_vtable_elem_location`` allow both operation expressions and; location list expressions, why does ``DW_AT_data_location`` not allow; both? In all cases they apply to data objects so less likely that; optimization would cause different operation expressions for different; program location ranges. But if supporting for some then should be for; all. It seems odd this attribute is not the same as; ``DW_AT_data_member_location`` in having an initial stack with the; location description of the object since the expression has to need it. A.6 Other Debugging Information; -------------------------------. .. note::. This section provides changes to existing debugger information entry; attributes. These would be incorporated into the corresponding DWARF Version 5; chapter 6 sections. A.6.1 Accelerated Access; ~~~~~~~~~~~~~~~~~~~~~~~~. .. _amdgpu-dwarf-lookup-by-name:. A.6.1.1 Lookup By Name; ++++++++++++++++++++++. A.6.1.1.1 Contents of the Name Index; ####################################. .. note::. The following provides changes to DWARF Version 5 section 6.1.1.1. The rule for debugger information entri",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst:185130,optimiz,optimization,185130,interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst,1,['optimiz'],['optimization']
Performance,"s only for use by :doc:`bugpoint <Bugpoint>`. ``extract-blocks``: Extract Basic Blocks From Module (for bugpoint use); -----------------------------------------------------------------------. This pass is used by bugpoint to extract all blocks from the module into their; own functions. ``instnamer``: Assign names to anonymous instructions; -----------------------------------------------------. This is a little utility pass that gives instructions names, this is mostly; useful when diffing the effect of an optimization because deleting an unnamed; instruction can change all other instruction numbering, making the diff very; noisy. .. _passes-verify:. ``verify``: Module Verifier; ---------------------------. Verifies an LLVM IR code. This is useful to run after an optimization which is; undergoing testing. Note that llvm-as verifies its input before emitting; bitcode, and also that malformed bitcode is likely to make LLVM crash. All; language front-ends are therefore encouraged to verify their output before; performing optimizing transformations. #. Both of a binary operator's parameters are of the same type.; #. Verify that the indices of mem access instructions match other operands.; #. Verify that arithmetic and other things are only performed on first-class; types. Verify that shifts and logicals only happen on integrals f.e.; #. All of the constants in a switch statement are of the correct type.; #. The code is in valid SSA form.; #. It is illegal to put a label into any other type (like a structure) or to; return one.; #. Only phi nodes can be self referential: ``%x = add i32 %x``, ``%x`` is; invalid.; #. PHI nodes must have an entry for each predecessor, with no extras.; #. PHI nodes must be the first thing in a basic block, all grouped together.; #. PHI nodes must have at least one entry.; #. All basic blocks should only end with terminator insts, not contain them.; #. The entry node to a function must not have predecessors.; #. All Instructions must be embedd",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:41274,perform,performing,41274,interpreter/llvm-project/llvm/docs/Passes.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst,2,"['optimiz', 'perform']","['optimizing', 'performing']"
Performance,"s reachable objects starting; from the roots, then deallocates unreachable objects in a sweep phase. Copying; As reachability analysis proceeds, the collector copies objects from one heap; area to another, compacting them in the process. Copying collectors enable; highly efficient ""bump pointer"" allocation and can improve locality of; reference. Incremental; (Including generational collectors.) Incremental collectors generally have all; the properties of a copying collector (regardless of whether the mature heap; is compacting), but bring the added complexity of requiring write barriers. Threaded; Denotes a multithreaded mutator; the collector must still stop the mutator; (""stop the world"") before beginning reachability analysis. Stopping a; multithreaded mutator is a complicated problem. It generally requires highly; platform-specific code in the runtime, and the production of carefully; designed machine code at safe points. Concurrent; In this technique, the mutator and the collector run concurrently, with the; goal of eliminating pause times. In a *cooperative* collector, the mutator; further aids with collection should a pause occur, allowing collection to take; advantage of multiprocessor hosts. The ""stop the world"" problem of threaded; collectors is generally still present to a limited extent. Sophisticated; marking algorithms are necessary. Read barriers may be necessary. As the matrix indicates, LLVM's garbage collection infrastructure is already; suitable for a wide variety of collectors, but does not currently extend to; multithreaded programs. This will be added in the future as there is; interest. .. _stack-map:. Computing stack maps; --------------------. LLVM automatically computes a stack map. One of the most important features; of a ``GCStrategy`` is to compile this information into the executable in; the binary representation expected by the runtime library. The stack map consists of the location and identity of each GC root in the; each function in",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GarbageCollection.rst:31324,concurren,concurrently,31324,interpreter/llvm-project/llvm/docs/GarbageCollection.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GarbageCollection.rst,1,['concurren'],['concurrently']
Performance,"s section we only mention loads. Stores have exactly the same problems as their associated loads, so have been skipped for brevity. Considerations; ==============. LLVM IR Lane ordering; ---------------------. LLVM IR has first class vector types. In LLVM IR, the zero'th element of a vector resides at the lowest memory address. The optimizer relies on this property in certain areas, for example when concatenating vectors together. The intention is for arrays and vectors to have identical memory layouts - ``[4 x i8]`` and ``<4 x i8>`` should be represented the same in memory. Without this property there would be many special cases that the optimizer would have to cleverly handle. Use of ``LDR`` would break this lane ordering property. This doesn't preclude the use of ``LDR``, but we would have to do one of two things:. 1. Insert a ``REV`` instruction to reverse the lane order after every ``LDR``.; 2. Disable all optimizations that rely on lane layout, and for every access to an individual lane (``insertelement``/``extractelement``/``shufflevector``) reverse the lane index. AAPCS; -----. The ARM procedure call standard (AAPCS) defines the ABI for passing vectors between functions in registers. It states:. When a short vector is transferred between registers and memory it is treated as an opaque object. That is a short vector is stored in memory as if it were stored with a single ``STR`` of the entire register; a short vector is loaded from memory using the corresponding ``LDR`` instruction. On a little-endian system this means that element 0 will always contain the lowest addressed element of a short vector; on a big-endian system element 0 will contain the highest-addressed element of a short vector. -- Procedure Call Standard for the ARM 64-bit Architecture (AArch64), 4.1.2 Short Vectors. The use of ``LDR`` and ``STR`` as the ABI defines has at least one advantage over ``LD1`` and ``ST1``. ``LDR`` and ``STR`` are oblivious to the size of the individual lanes of a ve",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BigEndianNEON.rst:5360,optimiz,optimizations,5360,interpreter/llvm-project/llvm/docs/BigEndianNEON.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BigEndianNEON.rst,1,['optimiz'],['optimizations']
Performance,"s should be consistent.; The following example is not allowed:. .. code-block:: c++. // M.cppm; export module M;. // Use.cpp; import M;. .. code-block:: console. $ clang++ -std=c++20 M.cppm --precompile -o M.pcm; $ clang++ -std=c++23 Use.cpp -fprebuilt-module-path=. The compiler would reject the example due to the inconsistent language options.; Not all options are language options.; For example, the following example is allowed:. .. code-block:: console. $ clang++ -std=c++20 M.cppm --precompile -o M.pcm; # Inconsistent optimization level.; $ clang++ -std=c++20 -O3 Use.cpp -fprebuilt-module-path=.; # Inconsistent debugging level.; $ clang++ -std=c++20 -g Use.cpp -fprebuilt-module-path=. Although the two examples have inconsistent optimization and debugging level, both of them are accepted. Note that **currently** the compiler doesn't consider inconsistent macro definition a problem. For example:. .. code-block:: console. $ clang++ -std=c++20 M.cppm --precompile -o M.pcm; # Inconsistent optimization level.; $ clang++ -std=c++20 -O3 -DNDEBUG Use.cpp -fprebuilt-module-path=. Currently Clang would accept the above example. But it may produce surprising results if the; debugging code depends on consistent use of ``NDEBUG`` also in other translation units. Definitions consistency; ^^^^^^^^^^^^^^^^^^^^^^^. The C++ language defines that same declarations in different translation units should have; the same definition, as known as ODR (One Definition Rule). Prior to modules, the translation; units don't dependent on each other and the compiler itself can't perform a strong; ODR violation check. With the introduction of modules, now the compiler have; the chance to perform ODR violations with language semantics across translation units. However, in the practice, we found the existing ODR checking mechanism is not stable; enough. Many people suffers from the false positive ODR violation diagnostics, AKA,; the compiler are complaining two identical declarations have different de",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/StandardCPlusPlusModules.rst:16657,optimiz,optimization,16657,interpreter/llvm-project/clang/docs/StandardCPlusPlusModules.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/StandardCPlusPlusModules.rst,1,['optimiz'],['optimization']
Performance,"s some sort of type-safe subset of C. Having pointee types; provided an extra layer of checks to make sure that the Clang frontend matched; its frontend values/operations with the corresponding LLVM IR. However, as other; languages like C++ adopted LLVM, the community realized that pointee types were; more of a hindrance for LLVM development and that the extra type checking with; some frontends wasn't worth it. LLVM's type system was `originally designed; <https://llvm.org/pubs/2003-05-01-GCCSummit2003.html>`_ to support high-level; optimization. However, years of LLVM implementation experience have demonstrated; that the pointee type system design does not effectively support; optimization. Memory optimization algorithms, such as SROA, GVN, and AA,; generally need to look through LLVM's struct types and reason about the; underlying memory offsets. The community realized that pointee types hinder LLVM; development, rather than helping it. Some of the initially proposed high-level; optimizations have evolved into `TBAA; <https://llvm.org/docs/LangRef.html#tbaa-metadata>`_ due to limitations with; representing higher-level language information directly via SSA values. Pointee types provide some value to frontends because the IR verifier uses types; to detect straightforward type confusion bugs. However, frontends also have to; deal with the complexity of inserting bitcasts everywhere that they might be; required. The community consensus is that the costs of pointee types; outweight the benefits, and that they should be removed. Many operations do not actually care about the underlying type. These; operations, typically intrinsics, usually end up taking an arbitrary pointer; type ``i8*`` and sometimes a size. This causes lots of redundant no-op bitcasts; in the IR to and from a pointer with a different pointee type. No-op bitcasts take up memory/disk space and also take up compile time to look; through. However, perhaps the biggest issue is the code complexity required ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:2915,optimiz,optimizations,2915,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst,1,['optimiz'],['optimizations']
Performance,"s space of a; volatile operation may not be changed. Different address spaces may; have different trapping behavior when dereferencing an invalid; pointer. The compiler may assume execution will continue after a volatile operation,; so operations which modify memory or may have undefined behavior can be; hoisted past a volatile operation. As an exception to the preceding rule, the compiler may not assume execution; will continue after a volatile store operation. This restriction is necessary; to support the somewhat common pattern in C of intentionally storing to an; invalid pointer to crash the program. In the future, it might make sense to; allow frontends to control this behavior. IR-level volatile loads and stores cannot safely be optimized into llvm.memcpy; or llvm.memmove intrinsics even when those intrinsics are flagged volatile.; Likewise, the backend should never split or merge target-legal volatile; load/store instructions. Similarly, IR-level volatile loads and stores cannot; change from integer to floating-point or vice versa. .. admonition:: Rationale. Platforms may rely on volatile loads and stores of natively supported; data width to be executed as single instruction. For example, in C; this holds for an l-value of volatile primitive type with native; hardware support, but not necessarily for aggregate types. The; frontend upholds these expectations, which are intentionally; unspecified in the IR. The rules above ensure that IR transformations; do not violate the frontend's contract with the language. .. _memmodel:. Memory Model for Concurrent Operations; --------------------------------------. The LLVM IR does not define any way to start parallel threads of; execution or to register signal handlers. Nonetheless, there are; platform-specific ways to create them, and we define LLVM IR's behavior; in their presence. This model is inspired by the C++ memory model. For a more informal introduction to this model, see the :doc:`Atomics`. We define a *happens",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:148142,load,loads,148142,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['loads']
Performance,"s specified on its command line and; never revisits a library. In this way, no circular dependencies between; libraries can exist. This doesn't fully enforce all inter-library dependencies, and importantly; doesn't enforce header file circular dependencies created by inline functions.; A good way to answer the ""is this layered correctly"" would be to consider; whether a Unix linker would succeed at linking the program if all inline; functions were defined out-of-line. (& for all valid orderings of dependencies; - since linking resolution is linear, it's possible that some implicit; dependencies can sneak through: A depends on B and C, so valid orderings are; ""C B A"" or ""B C A"", in both cases the explicit dependencies come before their; use. But in the first case, B could still link successfully if it implicitly; depended on C, or the opposite in the second case). .. _minimal list of #includes:. ``#include`` as Little as Possible; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. ``#include`` hurts compile time performance. Don't do it unless you have to,; especially in header files. But wait! Sometimes you need to have the definition of a class to use it, or to; inherit from it. In these cases go ahead and ``#include`` that header file. Be; aware however that there are many cases where you don't need to have the full; definition of a class. If you are using a pointer or reference to a class, you; don't need the header file. If you are simply returning a class instance from a; prototyped function or method, you don't need it. In fact, for most cases, you; simply don't need the definition of a class. And not ``#include``\ing speeds up; compilation. It is easy to try to go too overboard on this recommendation, however. You; **must** include all of the header files that you are using --- you can include; them either directly or indirectly through another header file. To make sure; that you don't accidentally forget to include a header file in your module; header, make sure to include y",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodingStandards.rst:31895,perform,performance,31895,interpreter/llvm-project/llvm/docs/CodingStandards.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodingStandards.rst,1,['perform'],['performance']
Performance,"s stored member-wise, add support for the schema evolution of the class 'Data'. This requires a change in the on file format used to store this type; of data members (i.e. by adding inline the version number of the class; 'Data'). To read file containing this construct and written with this revision; using an older version of ROOT you will need the following patches:; For v5.22/00, you will need the patch r33174; or v5.22/00k; For v5.26/00, you will need patch r33176; or v5.26/00c. Additionally, we no longer allow the member wise streaming of a class which; has a custom streamer nor of any data members marked with //||. Run time performance. We introduced an optimized infrastructure for reading objects using a StreamerInfo. Rather than driving the streaming using a switch statement inside TStreamerInfo::ReadBuffer,; the streaming is now driven using a simple loop over a sequence of configured StreamerInfo actions. This improves run-time performance by allowing a dramatic reduction in function calls and code; branches at the expense of some code duplication. There are 3 versions of this loop implemented in TBufferFile and overloaded in TBufferXML and TBufferSQL:. virtual Int_t ReadSequence(const TStreamerInfoActions::TActionSequence &sequence, void *object);; virtual Int_t ReadSequence(const TStreamerInfoActions::TActionSequence &sequence,; void *start_collection, void *end_collection);; virtual Int_t ReadSequence(const TStreamerInfoActions::TActionSequence &sequence,; void *start_collection, void *end_collection);. The 1st version is optimized to read a single object. The 2nd version is optimized to read the content of TClonesArrays and vectors of pointers to objects. The 3rd version is used to streamed any collections. TBufferXML and TBufferSQL overload the loops to introduce extra code to help the buffer keep track of which streamer element is being streamed (this functionality is not used by TBufferFile.). A TStreamerInfoActions::TActionSequence is an ordered seq",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/io/doc/v528/index.html:3139,perform,performance,3139,io/doc/v528/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/io/doc/v528/index.html,2,['perform'],['performance']
Performance,"s stored within that declaration context. Therefore, Clang will; deserialize the translation unit declaration without deserializing the; declarations within that translation unit. When required, the declarations; stored within a declaration context will be deserialized. There are two; representations of the declarations within a declaration context, which; correspond to the name-lookup and iteration behavior described above:. * When the front end performs name lookup to find a name ``x`` within a given; declaration context (for example, during semantic analysis of the expression; ``p->x``, where ``p``'s type is defined in the precompiled header), Clang; refers to an on-disk hash table that maps from the names within that; declaration context to the declaration IDs that represent each visible; declaration with that name. The actual declarations will then be; deserialized to provide the results of name lookup.; * When the front end performs iteration over all of the declarations within a; declaration context, all of those declarations are immediately; de-serialized. For large declaration contexts (e.g., the translation unit),; this operation is expensive; however, large declaration contexts are not; traversed in normal compilation, since such a traversal is unnecessary.; However, it is common for the code generator and semantic analysis to; traverse declaration contexts for structs, classes, unions, and; enumerations, although those contexts contain relatively few declarations in; the common case. Statements and Expressions; ^^^^^^^^^^^^^^^^^^^^^^^^^^. Statements and expressions are stored in the AST file in both the :ref:`types; <pchinternals-types>` and the :ref:`declarations <pchinternals-decls>` blocks,; because every statement or expression will be associated with either a type or; declaration. The actual statement and expression records are stored; immediately following the declaration or type that owns the statement or; expression. For example, the statement rep",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/PCHInternals.rst:15681,perform,performs,15681,interpreter/llvm-project/clang/docs/PCHInternals.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/PCHInternals.rst,1,['perform'],['performs']
Performance,"s tend to dominate critical path of the build, and are; thus worth optimizing. Use CCache and NOT incremental builds; Using ccache materially improves average build times. Incremental builds; can be slightly faster, but introduce the risk of build corruption due to; e.g. state changes, etc... At this point, the recommendation is not to; use incremental builds and instead use ccache as the latter captures the; majority of the benefit with less risk of false positives. One of the non-obvious benefits of using ccache is that it makes the; builder less sensitive to which projects are being monitored vs built.; If a change triggers a build request, but doesn't change the build output; (e.g. doc changes, python utility changes, etc..), the build will entirely; hit in cache and the build request will complete in just the testing time. With multiple workers, it is tempting to try to configure a shared cache; between the workers. Experience to date indicates this is difficult to; well, and that having local per-worker caches gets most of the benefit; anyways. We don't currently recommend shared caches. CCache does depend on the builder hardware having sufficient IO to access; the cache with reasonable access times - i.e. a fast disk, or enough memory; for a RAM cache, etc.. For builders without, incremental may be your best; option, but is likely to require higher ongoing involvement from the; sponsor. Enable batch builds; As a last resort, you can configure your builder to batch build requests.; This makes the build failure notifications markedly less actionable, and; should only be done once all other reasonable measures have been taken. Leave it on the staging buildmaster; While most of this section has been biased towards builders intended for; the main buildmaster, it is worth highlighting that builders can run; indefinitely on the staging buildmaster. Such a builder may still be; useful for the sponsoring organization, without concern of negatively; impacting the broad",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToAddABuilder.rst:11992,cache,caches,11992,interpreter/llvm-project/llvm/docs/HowToAddABuilder.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToAddABuilder.rst,1,['cache'],['caches']
Performance,"s that all; memory operations; to global have; completed before; performing the; atomicrmw that is; being released. 2. buffer/global_atomic; 3. s_waitcnt vmcnt(0). - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the; atomicrmw has; completed before; invalidating the; cache. 4. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acq_rel - system - global 1. buffer_wbl2. - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at system scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global and L2 writeback; have completed before; performing the; atomicrmw that is; being released. 3. buffer/global_atomic; 4. s_waitcnt vmcnt(0). - Must happen before; following buffer_invl2 and; buffer_wbinvl1_vol.; - Ensures the; atomicrmw has; completed before; invalidating the; caches. 5. buffer_invl2;; buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale L1 global data,; nor see stale L2 MTYPE; NC global data.; MTYPE RW and CC memory will; never be stale in L2 due to; the memory probes. atomicrmw acq_rel - agent - generic 1. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; -",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:270652,load,load,270652,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"s the `kSigInterrupt` signal. It causes the printing of the; message: ***`*** Break *** keyboard interrupt `***and makes a long jump; back to the ROOT command prompt. If no **`TRint`** object is created,; there will be no `kSigInterrupt` handling. All signals can be reset to; their default UNIX behavior via the call of; **`TSytem`**`::ResetSignal()`. All signals can be ignored via; `TSytem::IgnoreSignal()`. The **`TSytem::IgnoreInterrupt()`** is a method; to toggle the handling of the interrupt signal. Typically it is called; to prevent a `SIGINT` to interrupt some important call (like writing to; a ROOT file). If **`TRint`** is used and the default ROOT interrupt handler is not; desired, you should use `GetSignalHandler()` of **`TApplication`** to; get the interrupt handler and to remove it by `RemoveSignalHandler()`of; **`TSystem`** . ## Glossary. The following glossary is adapted from the description of the Rogue Wave; `Threads.h`++ package. A **`process`** is a program that is loaded into memory and prepared for; execution. Each process has a private address space. Processes begin; with a single thread. A **`thread`** is a sequence of instructions being executed in a; program. A thread has a program counter and a private stack to keep; track of local variables and return addresses. A multithreaded process; is associated with one or more threads. Threads execute independently.; All threads in a given process share the private address space of that; process. **`Concurrency`** exists when at least two threads are in progress at; the same time. A system with only a single processor can support; concurrency by switching execution contexts among multiple threads. **`Parallelism`** arises when at least two threads are executing; simultaneously. This requires a system with multiple processors.; Parallelism implies concurrency, but not vice-versa. A function is **`reentrant`** if it will behave correctly even if a; thread of execution enters the function while one or more",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Threads.md:19678,load,loaded,19678,documentation/users-guide/Threads.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Threads.md,1,['load'],['loaded']
Performance,"s the converted number. Examples:; """""""""""""""""". .. code-block:: llvm. %res = call i16 @llvm.convert.to.fp16.f32(float %a); store i16 %res, i16* @x, align 2. .. _int_convert_from_fp16:. '``llvm.convert.from.fp16``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare float @llvm.convert.from.fp16.f32(i16 %a); declare double @llvm.convert.from.fp16.f64(i16 %a). Overview:; """""""""""""""""". The '``llvm.convert.from.fp16``' intrinsic function performs a; conversion from half precision floating-point format to single precision; floating-point format. Arguments:; """""""""""""""""""". The intrinsic function contains single argument - the value to be; converted. Semantics:; """""""""""""""""""". The '``llvm.convert.from.fp16``' intrinsic function performs a; conversion from half single precision floating-point format to single; precision floating-point format. The input half-float value is; represented by an ``i16`` value. Examples:; """""""""""""""""". .. code-block:: llvm. %a = load i16, ptr @x, align 2; %res = call float @llvm.convert.from.fp16(i16 %a). Saturating floating-point to integer conversions; ------------------------------------------------. The ``fptoui`` and ``fptosi`` instructions return a; :ref:`poison value <poisonvalues>` if the rounded-towards-zero value is not; representable by the result type. These intrinsics provide an alternative; conversion, which will saturate towards the smallest and largest representable; integer values instead. '``llvm.fptoui.sat.*``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". This is an overloaded intrinsic. You can use ``llvm.fptoui.sat`` on any; floating-point argument type and any integer result type, or vectors thereof.; Not all targets may support all types, however. ::. declare i32 @llvm.fptoui.sat.i32.f32(float %f); declare i19 @llvm.fptoui.sat.i19.f64(double %f); declare <4 x i100> @llvm.fptoui.sat.v4i100.v4f128(<4 x fp128> %f). Overview:; """""""""""""""""". This intrinsic converts the argument into an unsigned inte",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:683153,load,load,683153,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,"s the intended value in the source code. For example:. .. code-block:: c++. static task coro_task(int v) {; int a = v;; co_await await_counter{};; a++; // __int_32_0 is 43 here; std::cout << a << ""\n"";; a++; // __int_32_0 is still 43 here; std::cout << a << ""\n"";; a++; // __int_32_0 is still 43 here!; std::cout << a << ""\n"";; co_await await_counter{};; a++; // __int_32_0 is still 43 here!!; std::cout << a << ""\n"";; a++; // Why is __int_32_0 still 43 here?; std::cout << a << ""\n"";; }. When debugging step-by-step, the value of `__int_32_0` seemingly does not; change, despite being frequently incremented, and instead is always `43`.; While this might be surprising, this is a result of the optimizer recognizing; that it can eliminate most of the load/store operations. The above code gets; optimized to the equivalent of:. .. code-block:: c++. static task coro_task(int v) {; store v to __int_32_0 in the frame; co_await await_counter{};; a = load __int_32_0; std::cout << a+1 << ""\n"";; std::cout << a+2 << ""\n"";; std::cout << a+3 << ""\n"";; co_await await_counter{};; a = load __int_32_0; std::cout << a+4 << ""\n"";; std::cout << a+5 << ""\n"";; }. It should now be obvious why the value of `__int_32_0` remains unchanged; throughout the function. It is important to recognize that `__int_32_0`; does not directly correspond to `a`, but is instead a variable generated; to assist the compiler in code generation. The variables in an optimized; coroutine frame should not be thought of as directly representing the; variables in the C++ source. Get the suspended points; ========================. An important requirement for debugging coroutines is to understand suspended; points, which are where the coroutine is currently suspended and awaiting. For simple cases like the above, inspecting the value of the `__coro_index`; variable in the coroutine frame works well. However, it is not quite so simple in really complex situations. In these; cases, it is necessary to use the coroutine librarie",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/DebuggingCoroutines.rst:9853,load,load,9853,interpreter/llvm-project/clang/docs/DebuggingCoroutines.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/DebuggingCoroutines.rst,2,['load'],['load']
Performance,"s through instance; >>> Concrete.s_int; 123. .. _sec-operators-label:. `Structs/Unions`; ----------------. Structs and unions are both supported, named or anonymous.; If the latter, the field are accessible through the parent scope by their; declared name.; For example:. .. code-block:: python. >>> cppyy.cppdef(""""""\; ... struct PointXYZ {; ... PointXYZI() : intensity(5.) {}; ... double x, y, z;; ... union {; ... int offset1;; ... struct {; ... int offset2;; ... float intensity;; ... };; ... float data_c[4];; ... };; ... };""""""); True; >>> p = cppyy.gbl.PointXYZI(); >>> type(p.x); <class 'float'>; >>> p.intensity; 5.0; >>> type(p.data_c[1]); <class 'float'>; >>> p.data_c[1] = 3.0; >>> p.intensity; 3.0; >>>. `Operators`; -----------. Many C++ operators can be mapped to their Python equivalent.; When the operators are part of the C++ class definition, this is done; directly.; If they are defined globally, the lookup is done lazily (ie. can resolve; after the class definition by loading the global definition or by defining; them interactively).; Some operators have no Python equivalent and are instead made available by; mapping them onto the following conventional functions:. =================== ===================; C++ Python; =================== ===================; ``operator=`` ``__assign__``; ``operator++(int)`` ``__postinc__``; ``operator++()`` ``__preinc__``; ``operator--(int)`` ``__postdec__``; ``operator--()`` ``__predec__``; ``unary operator*`` ``__deref__``; ``operator->`` ``__follow__``; ``operator&&`` ``__dand__``; ``operator||`` ``__dor__``; ``operator,`` ``__comma__``; =================== ===================. Here is an example of operator usage, using STL iterators directly (note that; this is not necessary in practice as STL and STL-like containers work; transparently in Python for-loops):. .. code-block:: python. >>> v = cppyy.gbl.std.vector[int](range(3)); >>> i = v.begin(); >>> while (i != v.end()):; ... print(i.__deref__()); ... _ = i.__preinc__(); .",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/classes.rst:9897,load,loading,9897,bindings/pyroot/cppyy/cppyy/doc/source/classes.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/classes.rst,1,['load'],['loading']
Performance,"s up. ### Printing the Canvas. The Print command in the canvas File menu pops-up a print dialog where; the user can specify a preferred print command and the printer name. ![](pictures/0300002D.png). Both print parameters can be set via the new Print.Command and; Print.Printer rootrc resources as follows:. ```; # Printer settings.; WinNT.*.Print.Command: AcroRd32.exe; Unix.*.Print.Command: xprint -P%p %f; Print.Printer: 32-rb205-hp; Print.Directory: .; ```. If the `%p` and `%f` are specified as a part of the print command,; they will be replaced by the specified printer name and the file name.; All other parameters will be kept as they are written. A print button; is available in the canvas toolbar (activated via View menu/Toolbar). ## The ROOT Command Line. We have briefly touched on how to use the command line. There are; different types of commands. 1. Cling commands start with ""`.`"". ``` {.cpp}; root[] .? //this command will list all the Cling commands; root[] .L <filename> //load [filename]; root[] .x <filename> //load and execute [filename]; ```. 2. SHELL commands start with ""`.!`"" for example:. ``` {.cpp}; root[] .! ls; ```. 3. C++ commands follow C++ syntax (almost). ``` {.cpp}; root[] TBrowser *b = new TBrowser(); ```. ### Multi-line Commands. You can use the command line to execute multi-line commands. To begin; a multi-line command you must type a single left curly bracket `{`,; and to end it you must type a single right curly bracket `}`.; For example:. ``` {.cpp}; root[] {; end with '}'> Int_t j = 0;; end with '}'> for (Int_t i = 0; i < 3; i++); end with '}'> {; end with '}'> j= j + i;; end with '}'> cout << ""i = "" << i << "", j = "" << j << endl;; end with '}'> }; end with '}'> }; i = 0, j = 0; i = 1, j = 1; i = 2, j = 3; ```. It is more convenient to edit a script than the command line, and if; your multi line commands are getting unmanageable, you may want to; start with a script instead. ### Cling Extensions. We should say that some things are not sta",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/GettingStarted.md:25534,load,load,25534,documentation/users-guide/GettingStarted.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/GettingStarted.md,1,['load'],['load']
Performance,"s wait until they reach the write-back stage. At that point, they; get removed from the queue and the retire control unit is notified. When instructions are executed, the retire control unit flags the instruction as; ""ready to retire."". Instructions are retired in program order. The register file is notified of the; retirement so that it can free the physical registers that were allocated for; the instruction during the register renaming stage. Load/Store Unit and Memory Consistency Model; """"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""; To simulate an out-of-order execution of memory operations, :program:`llvm-mca`; utilizes a simulated load/store unit (LSUnit) to simulate the speculative; execution of loads and stores. Each load (or store) consumes an entry in the load (or store) queue. Users can; specify flags ``-lqueue`` and ``-squeue`` to limit the number of entries in the; load and store queues respectively. The queues are unbounded by default. The LSUnit implements a relaxed consistency model for memory loads and stores.; The rules are:. 1. A younger load is allowed to pass an older load only if there are no; intervening stores or barriers between the two loads.; 2. A younger load is allowed to pass an older store provided that the load does; not alias with the store.; 3. A younger store is not allowed to pass an older store.; 4. A younger store is not allowed to pass an older load. By default, the LSUnit optimistically assumes that loads do not alias; (`-noalias=true`) store operations. Under this assumption, younger loads are; always allowed to pass older stores. Essentially, the LSUnit does not attempt; to run any alias analysis to predict when loads and stores do not alias with; each other. Note that, in the case of write-combining memory, rule 3 could be relaxed to; allow reordering of non-aliasing store operations. That being said, at the; moment, there is no way to further relax the memory model (``-noalias`` is the; only option). Essentially, there is no ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst:39759,load,loads,39759,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,1,['load'],['loads']
Performance,"s whether or not Clang; prints the category associated with a diagnostic when emitting it.; Each diagnostic may or many not have an associated category, if it; has one, it is listed in the diagnostic categorization field of the; diagnostic line (in the []'s). For example, a format string warning will produce these three; renditions based on the setting of this option:. ::. t.c:3:11: warning: conversion specifies type 'char *' but the argument has type 'int' [-Wformat]; t.c:3:11: warning: conversion specifies type 'char *' but the argument has type 'int' [-Wformat,1]; t.c:3:11: warning: conversion specifies type 'char *' but the argument has type 'int' [-Wformat,Format String]. This category can be used by clients that want to group diagnostics; by category, so it should be a high level category. We want dozens; of these, not hundreds or thousands of them. .. _opt_fsave-optimization-record:. .. option:: -f[no-]save-optimization-record[=<format>]. Enable optimization remarks during compilation and write them to a separate; file. This option, which defaults to off, controls whether Clang writes; optimization reports to a separate file. By recording diagnostics in a file,; users can parse or sort the remarks in a convenient way. By default, the serialization format is YAML. The supported serialization formats are:. - .. _opt_fsave_optimization_record_yaml:. ``-fsave-optimization-record=yaml``: A structured YAML format. - .. _opt_fsave_optimization_record_bitstream:. ``-fsave-optimization-record=bitstream``: A binary format based on LLVM; Bitstream. The output file is controlled by :option:`-foptimization-record-file`. In the absence of an explicit output file, the file is chosen using the; following scheme:. ``<base>.opt.<format>``. where ``<base>`` is based on the output file of the compilation (whether; it's explicitly specified through `-o` or not) when used with `-c` or `-S`.; For example:. * ``clang -fsave-optimization-record -c in.c -o out.o`` will generate; ``out.",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:10870,optimiz,optimization,10870,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,1,['optimiz'],['optimization']
Performance,"s with other caching techniques -; it is important to keep cache consistency.; So **currently** Clang will do very strict check for consistency. Options consistency; ^^^^^^^^^^^^^^^^^^^. The language option of module units and their non-module-unit users should be consistent.; The following example is not allowed:. .. code-block:: c++. // M.cppm; export module M;. // Use.cpp; import M;. .. code-block:: console. $ clang++ -std=c++20 M.cppm --precompile -o M.pcm; $ clang++ -std=c++23 Use.cpp -fprebuilt-module-path=. The compiler would reject the example due to the inconsistent language options.; Not all options are language options.; For example, the following example is allowed:. .. code-block:: console. $ clang++ -std=c++20 M.cppm --precompile -o M.pcm; # Inconsistent optimization level.; $ clang++ -std=c++20 -O3 Use.cpp -fprebuilt-module-path=.; # Inconsistent debugging level.; $ clang++ -std=c++20 -g Use.cpp -fprebuilt-module-path=. Although the two examples have inconsistent optimization and debugging level, both of them are accepted. Note that **currently** the compiler doesn't consider inconsistent macro definition a problem. For example:. .. code-block:: console. $ clang++ -std=c++20 M.cppm --precompile -o M.pcm; # Inconsistent optimization level.; $ clang++ -std=c++20 -O3 -DNDEBUG Use.cpp -fprebuilt-module-path=. Currently Clang would accept the above example. But it may produce surprising results if the; debugging code depends on consistent use of ``NDEBUG`` also in other translation units. Definitions consistency; ^^^^^^^^^^^^^^^^^^^^^^^. The C++ language defines that same declarations in different translation units should have; the same definition, as known as ODR (One Definition Rule). Prior to modules, the translation; units don't dependent on each other and the compiler itself can't perform a strong; ODR violation check. With the introduction of modules, now the compiler have; the chance to perform ODR violations with language semantics across translatio",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/StandardCPlusPlusModules.rst:16396,optimiz,optimization,16396,interpreter/llvm-project/clang/docs/StandardCPlusPlusModules.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/StandardCPlusPlusModules.rst,1,['optimiz'],['optimization']
Performance,"s"" \; source=""fXbins"" \; targetClass=""TAxis"" \; target=""fXbins"" \; version=""[-5]"" \; include=""TAxis.h"" \; code=""\; {\; Float_t * xbins=0; \; Int_t n = buffer.ReadArray( xbins ); \; fXbins.Set( xbins ); \; }""; ```. - For REFLEX dictionaries:. ``` {.cpp}; <ioread sourceClass=""ClassA""; source=""double m_a; double m_b; double m_c""; version=""[4-5,7,9,12-]""; checksum=""[12345,123456]""; targetClass=""ClassB""; target=""m_x""; embed=""true""; include=""iostream,cstdlib"">; <![CDATA[; m_x = onfile.m_a * onfile.m_b * onfile.m_c;; ]] >; </ioread>. <ioreadraw sourceClass=""TAxis""; source=""fXbins""; targetClass=""TAxis""; target=""fXbins""; version=""[-5]""; include=""TAxis.h"">; <![CDATA[; Float_t *xbins = 0;; Int_t n = buffer.ReadArray( xbins ) ;; fXbins.Set( xbins );; ]] >; </ioreadraw>; ```. The variables in the rules have the following meaning:. * sourceClass; - The field defines the on-disk class that is the input for the rule.; * source; - A semicolon-separated list of values defining the source class data members; that need to be cached and accessible via object proxy when the rule is; executed. The values are either the names of the data members or the type-name; pairs (separated by a space). If types are specified then the ondisk structure; can be generated and used in the code snippet defined by the user.; * version; - A list of versions of the source class that can be an input for this rule.; The list has to be enclosed in a square bracket and be a comma-separated; list of versions or version ranges. The version is an integer number, whereas; the version range is one of the following:; - ""a-b"": a and b are integers and the expression means all the numbers between; and including a and b; - ""-a"": a is an integer and the expression means all the version numbers smaller; than or equal to a; - ""a-"": a is an integer and the expression means all the version numbers greater; than or equal to a; * checksum; - A list of checksums of the source class that can be an input for this; rule. The list ha",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/InputOutput.md:79649,cache,cached,79649,documentation/users-guide/InputOutput.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/InputOutput.md,1,['cache'],['cached']
Performance,"s' counter for merging is now shown; on the client:;  ;     root [n] p->Process(...);       ... ;       Mst-0: merging output objects ... / (4; workers still sending). This asserts socket activity and fixes the timeout; problems during long merging phases reported in a few cases.; In TFileMerger, create directly the output file at the; final destination do not make a local copy in the temp directory first; (if needed, one can always set the temporary destination to temp; followed by a TFile::Cp to the final destination); this allows to avoid; reported problems with small temp partitions (see Forum).; In XrdProofConn, enable cycling through the; authentication protocol presented by the server. This only holds for; the choice of the protocol, because the server currently supports only; one full handshake.; In test/stressProof.cxx, avoid interferences between the; settings used for the PROOF tutorial and possible local settings; (daemon, dataset manager).; Add possibility to control the automatic re-loading of; the <proof.conf> file via the keyword; 'reload:1'/'reload:0'; in the xpd.resource directive.; Move the validation of <proof.conf> at the; moment of use; this allows to specify a file path and to dynamically; create/modify/destroy the file; used by PoD.; Improve displaying speed of large log files. Fixes. Fix two severe; bugs in the way TTreeCache; was used in PROOF: one bug was de facto disactivating the cache; the; other was causing a std::bad_alloc exception to be thrown on workers; when opening a remote file after a local one.    ; Fix several problems in TChain::Draw including. drawing into an existing histogram, i.e.; chain->Draw(""var>>myhist"");. treatment of histogram merging in case of small; statistics, i.e. when; the autobinning is not or only partially active;. usage of existing canvases when different histogram; names are specified;. Fix a problem causing a duplication of the final feedback; object. Fix problem with determining the subdir name in; TFil",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/proof/doc/v524/index.html:8329,load,loading,8329,proof/doc/v524/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/proof/doc/v524/index.html,2,['load'],['loading']
Performance,"s'; version with the script in ``llvm/utils/release/bump-version.py``. Tagging the LLVM Release Candidates; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Tag release candidates:. ::. $ git tag -sa llvmorg-X.Y.Z-rcN. The Release Manager must supply pre-packaged source tarballs for users. This can; be done with the export.sh script in utils/release. Tarballs, release binaries, or any other release artifacts must be uploaded to; GitHub. This can be done using the github-upload-release.py script in utils/release. ::. $ github-upload-release.py upload --token <github-token> --release X.Y.Z-rcN --files <release_files>. ::. $ ./export.sh -release X.Y.Z -rc $RC. This will generate source tarballs for each LLVM project being validated, which; can be uploaded to github for further testing. Build The Binary Distribution; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Creating the binary distribution requires following the instructions; :doc:`here <ReleaseProcess>`. That process will perform both Release+Asserts and Release builds but only; pack the Release build for upload. You should use the Release+Asserts sysroot,; normally under ``final/Phase3/Release+Asserts/llvmCore-3.8.1-RCn.install/``,; for test-suite and run-time benchmarks, to make sure nothing serious has; passed through the net. For compile-time benchmarks, use the Release version. The minimum required version of the tools you'll need are :doc:`here <GettingStarted>`. Release Qualification Criteria; ------------------------------. There are no official release qualification criteria. It is up to the; the release manager to determine when a release is ready. The release manager; should pay attention to the results of community testing, the number of outstanding; bugs, and then number of regressions when determining whether or not to make a; release. The community values time based releases, so releases should not be delayed for; too long unless there are critical issues remaining. In most cases, the only; kind of bugs that are critical eno",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToReleaseLLVM.rst:5664,perform,perform,5664,interpreter/llvm-project/llvm/docs/HowToReleaseLLVM.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToReleaseLLVM.rst,1,['perform'],['perform']
Performance,"s's returned ``PreservedAnalyses``. This can be also done; manually within the pass:. .. code-block:: c++. FooModulePass::run(Module& M, ModuleAnalysisManager& AM) {; auto &FAM = AM.getResult<FunctionAnalysisManagerModuleProxy>(M).getManager();. // Invalidate all analysis results for function F1.; FAM.invalidate(F1, PreservedAnalyses::none());. // Invalidate all analysis results across the entire module.; AM.invalidate(M, PreservedAnalyses::none());. // Clear the entry in the analysis manager for function F2 if we've completely removed it from the module.; FAM.clear(F2);. ...; }. One thing to note when accessing inner level IR analyses is cached results for; deleted IR. If a function is deleted in a module pass, its address is still used; as the key for cached analyses. Take care in the pass to either clear the; results for that function or not use inner analyses at all. ``AM.invalidate(M, PreservedAnalyses::none());`` will invalidate the inner; analysis manager proxy which will clear all cached analyses, conservatively; assuming that there are invalid addresses used as keys for cached analyses.; However, if you'd like to be more selective about which analyses are; cached/invalidated, you can mark the analysis manager proxy as preserved,; essentially saying that all deleted entries have been taken care of manually.; This should only be done with measurable compile time gains as it can be tricky; to make sure all the right analyses are invalidated. Implementing Analysis Invalidation; ==================================. By default, an analysis is invalidated if ``PreservedAnalyses`` says that; analyses on the IR unit it runs on are not preserved (see; ``AnalysisResultModel::invalidate()``). An analysis can implement; ``invalidate()`` to be more conservative when it comes to invalidation. For; example,. .. code-block:: c++. bool FooAnalysisResult::invalidate(Function &F, const PreservedAnalyses &PA,; FunctionAnalysisManager::Invalidator &) {; auto PAC = PA.getChecker<Fo",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:14438,cache,cached,14438,interpreter/llvm-project/llvm/docs/NewPassManager.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst,2,['cache'],['cached']
Performance,"s, and framework; stability. The changes with respect to ROOT 5.27 / TMVA 4.0.7 are; in detail:. Framework. Multi-class support. The support of multiple; output classes (i.e., more than a single background and signal; class) has been enabled for these methods: MLP (NN), BDTG,; FDA.; The multiclass; functionality can be enabled with the Factory option; ""AnalysisType=multiclass"". Training data is; specified with an additional classname, e.g. via; factory->AddTree(tree,""classname"");. After the; training a genetic algorithm is invoked to determine the best; cuts for selecting a specific class, based on the figure of; merit: purity*efficiency. TMVA comes with two examples in; $ROOTSYS/tmva/test: TMVAMulticlass.C; and TMVAMulticlassApplication.C. New TMVA event vector building. The code; for splitting the input data into training and test samples for; all classes and the mixing of those samples to one training and; one test sample has been rewritten completely. The new code is; more performant and has a clearer structure. This fixes several; bugs which have been reported by some users of TMVA.; Code and performance test framework: A unit; test framework for daily software and method performance; validation has been implemented.; . Methods. BDT Automatic parameter optimisation for building the; tree architecture: The optimisation procedure uses the; performance of the trained classifier on the ""test sample"" for; finding the set of optimal parameters. Two different methods to; traverse the parameter space are available (scanning, genetic; algorithm). Currently parameter optimization is implemented only; for these three parameters that influence the tree architectur:; the maximum depth of a tree, MaxDepth, the minimum; number of events in each node, NodeMinEvents, and; the number of tress, NTrees. Optimization can; is invoked by calling; factory->OptimizeAllMethods(); prior to the call; factory->TrainAllMethods();. Automated and configurable parameter optimization is soon to",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tmva/doc/v528/index.html:1295,perform,performant,1295,tmva/doc/v528/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tmva/doc/v528/index.html,2,['perform'],['performant']
Performance,"s, and macros. This depends on libbasic. libparse - C (for now) parsing and local semantic analysis. This library; invokes coarse-grained 'Actions' provided by the client to do; stuff (e.g. libsema builds ASTs). This depends on liblex. libsema - Provides a set of parser actions to build a standardized AST; for programs. AST's are 'streamed' out a top-level declaration; at a time, allowing clients to use decl-at-a-time processing,; build up entire translation units, or even build 'whole; program' ASTs depending on how they use the APIs. This depends; on libast and libparse. librewrite - Fast, scalable rewriting of source code. This operates on; the raw syntactic text of source code, allowing a client; to insert and delete text in very large source files using; the same source location information embedded in ASTs. This; is intended to be a low-level API that is useful for; higher-level clients and libraries such as code refactoring. libanalysis - Source-level dataflow analysis useful for performing analyses; such as computing live variables. It also includes a; path-sensitive ""graph-reachability"" engine for writing; analyses that reason about different possible paths of; execution through source code. This is currently being; employed to write a set of checks for finding bugs in software. libcodegen - Lower the AST to LLVM IR for optimization & codegen. Depends; on libast.; ; clang - An example driver, client of the libraries at various levels.; This depends on all these libraries, and on LLVM VMCore. This front-end has been intentionally built as a DAG of libraries, making it; easy to reuse individual parts or replace pieces if desired. For example, to; build a preprocessor, you take the Basic and Lexer libraries. If you want an; indexer, you take those plus the Parser library and provide some actions for; indexing. If you want a refactoring, static analysis, or source-to-source; compiler tool, it makes sense to take those plus the AST building and semantic; analyzer",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2007-OriginalClangReadme.txt:2648,perform,performing,2648,interpreter/llvm-project/llvm/docs/HistoricalNotes/2007-OriginalClangReadme.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2007-OriginalClangReadme.txt,1,['perform'],['performing']
Performance,"s, as the assertion condition usually; implies a safety condition (e.g., a pointer is not null) prior to performing; some action that depends on that condition (e.g., dereferencing a pointer).; The analyzer knows about several well-known assertion handlers, but can; automatically infer if a function should be treated as an assertion handler if; it is annotated with the 'noreturn' attribute or the (Clang-specific); 'analyzer_noreturn' attribute. Note that, currently, clang does not support; these attributes on Objective-C methods and C++ methods.; Attribute 'noreturn'; The 'noreturn' attribute is a GCC-attribute that can be placed on the; declarations of functions. It means exactly what its name implies: a function; with a 'noreturn' attribute should never return.; Specific details of the syntax of using the 'noreturn' attribute can be found; in GCC's; documentation.; Not only does the analyzer exploit this information when pruning false paths,; but the compiler also takes it seriously and will generate different code (and; possibly better optimized) under the assumption that the function does not; return.; Example; On Mac OS X, the function prototype for __assert_rtn (declared in; assert.h) is specifically annotated with the 'noreturn' attribute:. void __assert_rtn(const char *, const char *, int, const char *) __attribute__((__noreturn__));. Attribute 'analyzer_noreturn' (Clang-specific); The Clang-specific 'analyzer_noreturn' attribute is almost identical to; 'noreturn' except that it is ignored by the compiler for the purposes of code; generation.; This attribute is useful for annotating assertion handlers that actually; can return, but for the purpose of using the analyzer we want to; pretend that such functions do not return.; Because this attribute is Clang-specific, its use should be conditioned with; the use of preprocessor macros.; Example. #ifndef CLANG_ANALYZER_NORETURN; #if __has_feature(attribute_analyzer_noreturn); #define CLANG_ANALYZER_NORETURN __att",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/analyzer/annotations.html:22344,optimiz,optimized,22344,interpreter/llvm-project/clang/www/analyzer/annotations.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/analyzer/annotations.html,2,['optimiz'],['optimized']
Performance,"s, buffers, and; leafs, are explained a little later in this chapter, but for now, it is; important to realize that each object is not written individually, but; rather collected and written a bunch at a time. This is where the **`TTree`** takes advantage of compression and will; produce a much smaller file than if the objects were written; individually. Since the unit to be compressed is a buffer, and the; **`TTree`** contains many same-class objects, the header of the objects; can be compressed. The **`TTree`** reduces the header of each object, but it still contains; the class name. Using compression, the class name of each same-class; object has a good chance of being compressed, since the compression; algorithm recognizes the bit pattern representing the class name. Using; a **`TTree`** and compression the header is reduced to about 4 bytes; compared to the original 60 bytes. However, if compression is turned; off, you will not see these large savings. The **`TTree`** is also used to optimize the data access. A tree uses a; hierarchy of branches, and each branch can be read independently from; any other branch. Now, assume that `Px` and `Py` are data members of the; event, and we would like to compute `Px2 + Py2` for every event; and histogram the result. If we had saved the million events without a **`TTree`** we would have; to:. - read each event in its entirety into memory; - extract the `Px` and `Py` from the event; - compute the sum of the squares; - fill a histogram. We would have to do that a million times! This is very time consuming,; and we really do not need to read the entire event, every time. All we; need are two little data members (`Px` and `Py`). On the other hand, if; we use a tree with one branch containing `Px` and another branch; containing `Py`, we can read all values of `Px` and `Py` by only reading; the `Px` and `Py` branches. This makes the use of the **`TTree`** very; attractive. ## A Simple TTree. This script builds a **`TTree`** from",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Trees.md:1705,optimiz,optimize,1705,documentation/users-guide/Trees.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Trees.md,1,['optimiz'],['optimize']
Performance,"s, like `GCC; <https://gcc.gnu.org/onlinedocs/gccint/LTO-Overview.html>`_, for some time. Under FatLTO the compiler can emit standard object files which contain both the; machine code in the ``.text`` section and LLVM bitcode in the ``.llvm.lto``; section. Overview; ========. Within LLVM, FatLTO is supported by choosing the ``FatLTODefaultPipeline``.; This pipeline will:. #) Run the pre-link (Thin)LTO pipeline on the current module.; #) Embed the pre-link bitcode in a special ``.llvm.lto`` section.; #) Finish optimizing the module using the ModuleOptimization pipeline.; #) Emit the object file, including the new ``.llvm.lto`` section. .. NOTE. Previously, we conservatively ran independent pipelines on separate copies; of the LLVM module to generate the bitcode section and the object code,; which happen to be identical to those used outside of FatLTO. While that; resulted in compiled artifacts that were identical to those produced by the; default and (Thin)LTO pipelines, module cloning led to some cases of; miscompilation, and we have moved away from trying to keep bitcode; generation and optimization completely disjoint. Bit-for-bit compatibility is not (and never was) a guarantee, and we reserve; the right to change this at any time. Explicitly, users should not rely on; the produced bitcode or object code to match their non-LTO counterparts; precisely. They will exhibit similar performance characteristics, but may; not be bit-for-bit the same. Internally, the ``.llvm.lto`` section is created by running the; ``EmbedBitcodePass`` after the ``ThinLTOPreLinkDefaultPipeline``. This pass is; responsible for emitting the ``.llvm.lto`` section. Afterwards, the; ``ThinLTODefaultPipeline`` runs and the compiler can emit the fat object file. Limitations; ===========. Linkers; -------. Currently, using LTO with LLVM fat lto objects is supported by LLD and by the; GNU linkers via :doc:`GoldPlugin`. This may change in the future, but; extending support to other linkers isn't pl",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/FatLTO.rst:1619,optimiz,optimization,1619,interpreter/llvm-project/llvm/docs/FatLTO.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/FatLTO.rst,1,['optimiz'],['optimization']
Performance,"s, the integral representation may be target dependent or; unstable (not backed by a fixed integer). ``inttoptr`` and ``ptrtoint`` instructions have the same semantics as for; integral (i.e. normal) pointers in that they convert integers to and from; corresponding pointer types, but there are additional implications to be; aware of. Because the bit-representation of a non-integral pointer may; not be stable, two identical casts of the same operand may or may not; return the same value. Said differently, the conversion to or from the; non-integral type depends on environmental state in an implementation; defined manner. If the frontend wishes to observe a *particular* value following a cast, the; generated IR must fence with the underlying environment in an implementation; defined manner. (In practice, this tends to require ``noinline`` routines for; such operations.). From the perspective of the optimizer, ``inttoptr`` and ``ptrtoint`` for; non-integral types are analogous to ones on integral types with one; key exception: the optimizer may not, in general, insert new dynamic; occurrences of such casts. If a new cast is inserted, the optimizer would; need to either ensure that a) all possible values are valid, or b); appropriate fencing is inserted. Since the appropriate fencing is; implementation defined, the optimizer can't do the latter. The former is; challenging as many commonly expected properties, such as; ``ptrtoint(v)-ptrtoint(v) == 0``, don't hold for non-integral types.; Similar restrictions apply to intrinsics that might examine the pointer bits,; such as :ref:`llvm.ptrmask<int_ptrmask>`. . The alignment information provided by the frontend for a non-integral pointer; (typically using attributes or metadata) must be valid for every possible ; representation of the pointer. .. _globalvars:. Global Variables; ----------------. Global variables define regions of memory allocated at compilation time; instead of run-time. Global variable definitions must be i",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:29427,optimiz,optimizer,29427,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,2,['optimiz'],['optimizer']
Performance,"s, thus hoisting and sinking ""invariant"" loads and stores. Hoisting operations out of loops is a canonicalization transform. It enables; and simplifies subsequent optimizations in the middle-end. Rematerialization; of hoisted instructions to reduce register pressure is the responsibility of; the back-end, which has more accurate information about register pressure and; also handles other optimizations than LICM that increase live-ranges. This pass uses alias analysis for two purposes:. #. Moving loop invariant loads and calls out of loops. If we can determine; that a load or call inside of a loop never aliases anything stored to, we; can hoist it or sink it like any other instruction. #. Scalar Promotion of Memory. If there is a store instruction inside of the; loop, we try to move the store to happen AFTER the loop instead of inside of; the loop. This can only happen if a few conditions are true:. #. The pointer stored through is loop invariant.; #. There are no stores or loads in the loop which *may* alias the pointer.; There are no calls in the loop which mod/ref the pointer. If these conditions are true, we can promote the loads and stores in the; loop of the pointer to use a temporary alloca'd variable. We then use the; :ref:`mem2reg <passes-mem2reg>` functionality to construct the appropriate; SSA form for the variable. ``loop-deletion``: Delete dead loops; ------------------------------------. This file implements the Dead Loop Deletion Pass. This pass is responsible for; eliminating loops with non-infinite computable trip counts that have no side; effects or volatile instructions, and do not contribute to the computation of; the function's return value. .. _passes-loop-extract:. ``loop-extract``: Extract loops into new functions; --------------------------------------------------. A pass wrapper around the ``ExtractLoop()`` scalar transformation to extract; each top-level loop into its own new function. If the loop is the *only* loop; in a given function, it",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:24802,load,loads,24802,interpreter/llvm-project/llvm/docs/Passes.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst,1,['load'],['loads']
Performance,"s. ### RBrowser improvements. - central factory methods to handle browsing, editing and drawing of different classes; - simple possibility to extend RBrowser on user-defined classes; - support of web-based geometry viewer; - better support of TTree drawing; - server-side handling of code editor and image viewer widgets; - rbrowser content is fully recovered when web-browser is reloaded; - load of widgets code only when really required (shorter startup time for RBrowser). ## Montecarlo Libraries. ## PROOF Libraries. ## Language Bindings. ## JavaScript ROOT. ### Major JSROOT update to version 6. - update all used libraries `d3.js`, `three.js`, `MathJax.js`, openui5; - change to Promise based interface for all async methods, remove call-back arguments; - change scripts names, core scripts name now `JSRoot.core.js`; - unify function/methods naming conventions, many changes in method names; - provide central code loader via `JSROOT.require`, supporting 4 different loading engines; - many nice features and many bug fixes; see JSROOT v6 release notes. ## Tutorials. ## Class Reference Guide. ## Build, Configuration and Testing Infrastructure. - a new cmake variable, `CMAKE_INSTALL_PYTHONDIR`, has been added: it allows customization of the installation directory of ROOT's python modules; - the developer build option `asserts` is introduced to enable/disable asserts via the `NDEBUG` C/CXX flag. Asserts are always enabled for `CMAKE_BUILD_TYPE=Debug` and `dev=ON`. The previous behavior of the builds set via the `CMAKE_BUILD_TYPE` variable has not changed.; - `CMAKE_CXX_STANDARD`, i.e. the C++ standard ROOT is built with, now defaults to the compiler default (or C++11 if the compiler default is older than that) rather than always defaulting to C++11. In turn this means that v6.24 is the first ROOT release for which ROOT's pre-compiled binaries are not compiled with C++11 but with the default standard in use by the default system compiler. On Ubuntu 20.04, for example, the v6.24 ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v624/index.md:27764,load,loading,27764,README/ReleaseNotes/v624/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v624/index.md,1,['load'],['loading']
Performance,"s. %masked.a = select <4 x i1> %mask, <4 x i32> %a, <4 x i32> <i32 0, i32 0, i32 0, i32 0>; %reduction = call i32 @llvm.vector.reduce.umax.v4i32(<4 x i32> %masked.a); %also.r = call i32 @llvm.umax.i32(i32 %reduction, i32 %start). .. _int_vp_reduce_umin:. '``llvm.vp.reduce.umin.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare i32 @llvm.vp.reduce.umin.v4i32(i32 <start_value>, <4 x i32> <val>, <4 x i1> <mask>, i32 <vector_length>); declare i16 @llvm.vp.reduce.umin.nxv8i16(i16 <start_value>, <vscale x 8 x i16> <val>, <vscale x 8 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". Predicated unsigned-integer ``MIN`` reduction of a vector and a scalar starting; value, returning the result as a scalar. Arguments:; """""""""""""""""""". The first operand is the start value of the reduction, which must be a scalar; integer type equal to the result type. The second operand is the vector on; which the reduction is performed and must be a vector of integer values whose; element type is the result/start type. The third operand is the vector mask and; is a vector of boolean values with the same number of elements as the vector; operand. The fourth operand is the explicit vector length of the operation. Semantics:; """""""""""""""""""". The '``llvm.vp.reduce.umin``' intrinsic performs the unsigned-integer ``MIN``; reduction (:ref:`llvm.vector.reduce.umin <int_vector_reduce_umin>`) of the; vector operand ``val`` on each enabled lane, taking the minimum of that and the; scalar ``start_value``. Disabled lanes are treated as containing the neutral; value ``UINT_MAX``, or ``-1`` (i.e. having no effect on the reduction; operation). If the vector length is zero, the result is the start value. To ignore the start value, the neutral value can be used. Examples:; """""""""""""""""". .. code-block:: llvm. %r = call i32 @llvm.vp.reduce.umin.v4i32(i32 %start, <4 x i32> %a, <4 x i1> %mask, i32 %evl); ; %r is equivalent to %also.r, where lanes great",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:769652,perform,performed,769652,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performed']
Performance,"s. These constraints are; not exhaustive of the optimization opportunities: values held in local; variables are subject to additional restrictions, described later in this; document. It is undefined behavior if a computation history featuring a send of; ``retain`` followed by a send of ``release`` to the same object, with no; intervening ``release`` on that object, is not equivalent under the high-level; semantics to a computation history in which these sends are removed. Note that; this implies that these methods may not raise exceptions. It is undefined behavior if a computation history features any use whatsoever; of an object following the completion of a send of ``release`` that is not; preceded by a send of ``retain`` to the same object. The behavior of ``autorelease`` must be equivalent to sending ``release`` when; one of the autorelease pools currently in scope is popped. It may not throw an; exception. When the semantics call for performing one of these operations on a retainable; object pointer, if that pointer is ``null`` then the effect is a no-op. All of the semantics described in this document are subject to additional; :ref:`optimization rules <arc.optimization>` which permit the removal or; optimization of operations based on local knowledge of data flow. The; semantics describe the high-level behaviors that the compiler implements, not; an exact sequence of operations that a program will be compiled into. .. _arc.objects.operands:. Retainable object pointers as operands and arguments; ----------------------------------------------------. In general, ARC does not perform retain or release operations when simply using; a retainable object pointer as an operand within an expression. This includes:. * loading a retainable pointer from an object with non-weak :ref:`ownership; <arc.ownership>`,; * passing a retainable pointer as an argument to a function or method, and; * receiving a retainable pointer as the result of a function or method call. .. admoni",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst:14861,perform,performing,14861,interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,1,['perform'],['performing']
Performance,"s. You can also extend LLVM -; possibly through a loadable :ref:`code generation plugins <plugin>` - to; generate code and data structures which conforms to the *binary interface*; specified by the *runtime library*. This is similar to the relationship between; LLVM and DWARF debugging info, for example. The difference primarily lies in; the lack of an established standard in the domain of garbage collection --- thus; the need for a flexible extension mechanism. The aspects of the binary interface with which LLVM's GC support is; concerned are:. * Creation of GC safepoints within code where collection is allowed to execute; safely. * Computation of the stack map. For each safe point in the code, object; references within the stack frame must be identified so that the collector may; traverse and perhaps update them. * Write barriers when storing object references to the heap. These are commonly; used to optimize incremental scans in generational collectors. * Emission of read barriers when loading object references. These are useful; for interoperating with concurrent collectors. There are additional areas that LLVM does not directly address:. * Registration of global roots with the runtime. * Registration of stack map entries with the runtime. * The functions used by the program to allocate memory, trigger a collection,; etc. * Computation or compilation of type maps, or registration of them with the; runtime. These are used to crawl the heap for object references. In general, LLVM's support for GC does not include features which can be; adequately addressed with other features of the IR and does not specify a; particular binary interface. On the plus side, this means that you should be; able to integrate LLVM with an existing runtime. On the other hand, it can; have the effect of leaving a lot of work for the developer of a novel; language. We try to mitigate this by providing built in collector strategy; descriptions that can work with many common collector design",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GarbageCollection.rst:6982,load,loading,6982,interpreter/llvm-project/llvm/docs/GarbageCollection.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GarbageCollection.rst,1,['load'],['loading']
Performance,"s/Utils/helloworld.ll; ; RUN: opt -disable-output -passes=helloworld %s 2>&1 | FileCheck %s. ; CHECK: {{^}}foo{{$}}; define i32 @foo() {; %a = add i32 2, 3; ret i32 %a; }. ; CHECK-NEXT: {{^}}bar{{$}}; define void @bar() {; ret void; }. $ ninja -C build check-llvm; # runs our new test alongside all other llvm lit tests. FAQs; ====. Required passes; ---------------. A pass that defines a static ``isRequired()`` method that returns true is a required pass. For example:. .. code-block:: c++. class HelloWorldPass : public PassInfoMixin<HelloWorldPass> {; public:; PreservedAnalyses run(Function &F, FunctionAnalysisManager &AM);. static bool isRequired() { return true; }; };. A required pass is a pass that may not be skipped. An example of a required; pass is ``AlwaysInlinerPass``, which must always be run to preserve; ``alwaysinline`` semantics. Pass managers are required since they may contain; other required passes. An example of how a pass can be skipped is the ``optnone`` function; attribute, which specifies that optimizations should not be run on the; function. Required passes will still be run on ``optnone`` functions. For more implementation details, see; ``PassInstrumentation::runBeforePass()``. Registering passes as plugins; -----------------------------. LLVM provides a mechanism to register pass plugins within various tools like; ``clang`` or ``opt``. A pass plugin can add passes to default optimization; pipelines or to be manually run via tools like ``opt``. For more information,; see :doc:`NewPassManager`. Create a CMake project at the root of the repo alongside; other projects. This project must contain the following minimal; ``CMakeLists.txt``:. .. code-block:: cmake. add_llvm_pass_plugin(MyPassName source.cpp). See the definition of ``add_llvm_pass_plugin`` for more CMake details. The pass must provide at least one of two entry points for the new pass manager,; one for static registration and one for dynamically loaded plugins:. - ``llvm::PassPluginLibraryI",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMNewPMPass.rst:7113,optimiz,optimizations,7113,interpreter/llvm-project/llvm/docs/WritingAnLLVMNewPMPass.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMNewPMPass.rst,1,['optimiz'],['optimizations']
Performance,"s1`` on each function in a module, then run; ``FunctionPass2`` on each function in the module. In contrast,. .. code-block:: c++. ModulePassManager MPM;. FunctionPassManager FPM;; FPM.addPass(FunctionPass1());; FPM.addPass(FunctionPass2());. MPM.addPass(createModuleToFunctionPassAdaptor(std::move(FPM)));. will run ``FunctionPass1`` and ``FunctionPass2`` on the first function in a; module, then run both passes on the second function in the module, and so on.; This is better for cache locality around LLVM data structures. This similarly; applies for the other IR types, and in some cases can even affect the quality; of optimization. For example, running all loop passes on a loop may cause a; later loop to be able to be optimized more than if each loop pass were run; separately. Inserting Passes into Default Pipelines; =======================================. Rather than manually adding passes to a pass manager, the typical way of; creating a pass manager is to use a ``PassBuilder`` and call something like; ``PassBuilder::buildPerModuleDefaultPipeline()`` which creates a typical; pipeline for a given optimization level. Sometimes either frontends or backends will want to inject passes into the; pipeline. For example, frontends may want to add instrumentation, and target; backends may want to add passes that lower custom intrinsics. For these; cases, ``PassBuilder`` exposes callbacks that allow injecting passes into; certain parts of the pipeline. For example,. .. code-block:: c++. PassBuilder PB;; PB.registerPipelineStartEPCallback([&](ModulePassManager &MPM,; PassBuilder::OptimizationLevel Level) {; MPM.addPass(FooPass());; };. will add ``FooPass`` near the very beginning of the pipeline for pass; managers created by that ``PassBuilder``. See the documentation for; ``PassBuilder`` for the various places that passes can be added. If a ``PassBuilder`` has a corresponding ``TargetMachine`` for a backend, it; will call ``TargetMachine::registerPassBuilderCallbacks()`` to a",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:5272,optimiz,optimization,5272,interpreter/llvm-project/llvm/docs/NewPassManager.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst,1,['optimiz'],['optimization']
Performance,"s:. .. code-block:: c++. char bits[] = { 0, 0, 1, 0, 0, 0, 3, 0, 0, 0, 0, 5, 0, 0 };. To emit a virtual call, the compiler will assemble code that checks that; the object's virtual table pointer is in-bounds and aligned and that the; relevant bit is set in the bit vector. For example on x86 a typical virtual call may look like this:. .. code-block:: none. ca7fbb: 48 8b 0f mov (%rdi),%rcx; ca7fbe: 48 8d 15 c3 42 fb 07 lea 0x7fb42c3(%rip),%rdx; ca7fc5: 48 89 c8 mov %rcx,%rax; ca7fc8: 48 29 d0 sub %rdx,%rax; ca7fcb: 48 c1 c0 3d rol $0x3d,%rax; ca7fcf: 48 3d 7f 01 00 00 cmp $0x17f,%rax; ca7fd5: 0f 87 36 05 00 00 ja ca8511; ca7fdb: 48 8d 15 c0 0b f7 06 lea 0x6f70bc0(%rip),%rdx; ca7fe2: f6 04 10 10 testb $0x10,(%rax,%rdx,1); ca7fe6: 0f 84 25 05 00 00 je ca8511; ca7fec: ff 91 98 00 00 00 callq *0x98(%rcx); [...]; ca8511: 0f 0b ud2. The compiler relies on co-operation from the linker in order to assemble; the bit vectors for the whole program. It currently does this using LLVM's; `type metadata`_ mechanism together with link-time optimization. .. _address point: https://itanium-cxx-abi.github.io/cxx-abi/abi.html#vtable-general; .. _type metadata: https://llvm.org/docs/TypeMetadata.html; .. _ByteArrayBuilder: https://llvm.org/docs/doxygen/html/structllvm_1_1ByteArrayBuilder.html. Optimizations; -------------. The scheme as described above is the fully general variant of the scheme.; Most of the time we are able to apply one or more of the following; optimizations to improve binary size or performance. In fact, if you try the above example with the current version of the; compiler, you will probably find that it will not use the described virtual; table layout or machine instructions. Some of the optimizations we are about; to introduce cause the compiler to use a different layout or a different; sequence of machine instructions. Stripping Leading/Trailing Zeros in Bit Vectors; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. If a bit vector contains leading or trailing zeros",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ControlFlowIntegrityDesign.rst:3197,optimiz,optimization,3197,interpreter/llvm-project/clang/docs/ControlFlowIntegrityDesign.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ControlFlowIntegrityDesign.rst,1,['optimiz'],['optimization']
Performance,"s://jsroot.gsi.de/>. When required, there are following alternatives to install JSROOT on other web servers:. - download and unpack [provided](https://github.com/root-project/jsroot/releases) packages (recommended); - use [npm](https://npmjs.com/package/jsroot) package manager and invoke `npm install jsroot`; - clone master branch from [repository](https://github.com/root-project/jsroot/). ## Drawing objects in JSROOT. [The main page](https://root.cern/js/latest/) of the JSROOT project provides the possibility to interactively open ROOT files and draw objects like histogram or canvas. To automate files loading and objects drawing, one can provide number of URL parameters in address string like:. - file - name of the file, which will be automatically open with page loading; - files - array of file names for loading; - json - name of JSON file with stored ROOT object like histogram or canvas; - item - item name to be displayed; - opt - drawing option for the item; - items - array of items name to be displayed; - opts - array of drawing options for the items; - expand - item name(s) to be expanded in the hierarchy browser; - focus - item name to be focused on in the hierarchy browser; - title - set browser title; - dir - list files in directory on http server, see https://github.com/root-project/jsroot/issues/283; - layout - can be 'simple', 'flex', 'tabs', 'gridNxM', 'horizNMK', 'vertNMK'; - browser - layout of the browser 'fix' (default), 'float', 'no' (hidden), 'off' (fully disabled); - nobrowser - do not display file browser (same as browser=no); - float - display floating browser (same as browser=float); - status - configure status line 'no' (default), 'off' (completely disable), 'size'; - inject - name of extra JavaScript to load, see several examples in demo/ subdir; - optimize - drawing optimization 0:off, 1:only large histograms (default), 2:always; - palette - id of default color palette, 51..121 - new ROOT6 palette (default 57); - interactive - enable/disable",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/JSROOT/JSROOT.md:1153,load,loading,1153,documentation/JSROOT/JSROOT.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/JSROOT/JSROOT.md,2,['load'],['loading']
Performance,"s://llvm.org/docs/GoldPlugin.html>`_.; - **ld64**:; Starting with `Xcode 8 <https://developer.apple.com/xcode/>`_.; - **lld**:; Starting with r284050 for ELF, r298942 for COFF. Usage; =====. Basic; -----. To utilize ThinLTO, simply add the -flto=thin option to compile and link. E.g. .. code-block:: console. % clang -flto=thin -O2 file1.c file2.c -c; % clang -flto=thin -O2 file1.o file2.o -o a.out. When using lld-link, the -flto option need only be added to the compile step:. .. code-block:: console. % clang-cl -flto=thin -O2 -c file1.c file2.c; % lld-link /out:a.exe file1.obj file2.obj. As mentioned earlier, by default the linkers will launch the ThinLTO backend; threads in parallel, passing the resulting native object files back to the; linker for the final native link. As such, the usage model is the same as; non-LTO. With gold, if you see an error during the link of the form:. .. code-block:: console. /usr/bin/ld: error: /path/to/clang/bin/../lib/LLVMgold.so: could not load plugin library: /path/to/clang/bin/../lib/LLVMgold.so: cannot open shared object file: No such file or directory. Then either gold was not configured with plugins enabled, or clang; was not built with ``-DLLVM_BINUTILS_INCDIR`` set properly. See; the instructions for the; `LLVM gold plugin <https://llvm.org/docs/GoldPlugin.html#how-to-build-it>`_. Controlling Backend Parallelism; -------------------------------; .. _parallelism:. By default, the ThinLTO link step will launch as many; threads in parallel as there are cores. If the number of; cores can't be computed for the architecture, then it will launch; ``std::thread::hardware_concurrency`` number of threads in parallel.; For machines with hyper-threading, this is the total number of; virtual cores. For some applications and machine configurations this; may be too aggressive, in which case the amount of parallelism can; be reduced to ``N`` via:. - gold:; ``-Wl,-plugin-opt,jobs=N``; - ld64:; ``-Wl,-mllvm,-threads=N``; - ld.lld, ld64.lld:; ``-",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ThinLTO.rst:3243,load,load,3243,interpreter/llvm-project/clang/docs/ThinLTO.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ThinLTO.rst,1,['load'],['load']
Performance,"s:; """""""""""""""""""". The first argument is a pointer to be tested. The second argument is a; metadata object representing a :doc:`type identifier <TypeMetadata>`. Overview:; """""""""""""""""". The ``llvm.type.test`` intrinsic tests whether the given pointer is associated; with the given type identifier. .. _type.checked.load:. '``llvm.type.checked.load``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare {ptr, i1} @llvm.type.checked.load(ptr %ptr, i32 %offset, metadata %type) nounwind memory(argmem: read). Arguments:; """""""""""""""""""". The first argument is a pointer from which to load a function pointer. The; second argument is the byte offset from which to load the function pointer. The; third argument is a metadata object representing a :doc:`type identifier; <TypeMetadata>`. Overview:; """""""""""""""""". The ``llvm.type.checked.load`` intrinsic safely loads a function pointer from a; virtual table pointer using type metadata. This intrinsic is used to implement; control flow integrity in conjunction with virtual call optimization. The; virtual call optimization pass will optimize away ``llvm.type.checked.load``; intrinsics associated with devirtualized calls, thereby removing the type; check in cases where it is not needed to enforce the control flow integrity; constraint. If the given pointer is associated with a type metadata identifier, this; function returns true as the second element of its return value. (Note that; the function may also return true if the given pointer is not associated; with a type metadata identifier.) If the function's return value's second; element is true, the following rules apply to the first element:. - If the given pointer is associated with the given type metadata identifier,; it is the function pointer loaded from the given byte offset from the given; pointer. - If the given pointer is not associated with the given type metadata; identifier, it is one of the following (the choice of which is unspecified):. 1. The function poi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:938411,optimiz,optimization,938411,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['optimiz'],['optimization']
Performance,"s; (e.g. `etas[etas > k]` returns a new `RVec` with all elements greater than `k`); - helper functions such as `InvariantMass`, `DeltaR`, `Argsort` are also provided. The current implementation of `RVec` is based on LLVM's SmallVector, extracted; from the head of LLVM's repo around December 2020.; We are not tracking the upstream implementation. Compared to LLVM's SmallVectors:. - memory adoption capabilities have been added; - patches have been applied to make RVec work with (ROOT's version of) cppyy (notably `using` declarations had to be; lowered in the inheritance hierarchy for cppyy to pick them up); - `operator[](mask)` has been added, as well as several other ""numpy-like"" helper; functions (these latter ones are free functions); - logical operators `==`, `<`, `>` etc. return vectors rather than booleans; - the type of fSize and fCapacity is signed rather than unsigned, and fixed to 32 bits; - a number of minor patches have been applied for backward compatibility with the previous; implementation of RVec (which did not have a small buffer optimization and was implemented; in terms of `std::vector` with a custom allocator) and to make the code more consistent; with ROOT's coding conventions. ## RVec design. `SmallVectorBase`; - `fBeginX`; - `fSize`; - `fCapacity`. Basically the same as the corresponding LLVM class, with the template parameter removed: LLVM's SmallVectorBase; is templated over the type of fSize and fCapacity. It contains the parts of `RVec` that do not depend on the value; type.; No other classes in the hierarchy can contain data members! We expect the memory after `SmallVectorBase` to be; occupied by the small inline buffer. `SmallVectorTemplateCommon<T>`; - `getFirstEl()`: returns the address of the beginning of the small buffer; - `begin()`, `end()`, `front()`, `back()`, etc. Basically the same as the corresponding LLVM class.; It contains the parts that are independent of whether T is a POD or not. `SmallVectorTemplateBase<T, bool TriviallyC",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/math/vecops/ARCHITECTURE.md:1693,optimiz,optimization,1693,math/vecops/ARCHITECTURE.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/math/vecops/ARCHITECTURE.md,1,['optimiz'],['optimization']
Performance,"s; ------------------------------. These functions get properties of floating-point values. .. _llvm.is.fpclass:. '``llvm.is.fpclass``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare i1 @llvm.is.fpclass(<fptype> <op>, i32 <test>); declare <N x i1> @llvm.is.fpclass(<vector-fptype> <op>, i32 <test>). Overview:; """""""""""""""""". The '``llvm.is.fpclass``' intrinsic returns a boolean value or vector of boolean; values depending on whether the first argument satisfies the test specified by; the second argument. If the first argument is a floating-point scalar, then the result type is a; boolean (:ref:`i1 <t_integer>`). If the first argument is a floating-point vector, then the result type is a; vector of boolean with the same number of elements as the first argument. Arguments:; """""""""""""""""""". The first argument to the '``llvm.is.fpclass``' intrinsic must be; :ref:`floating-point <t_floating>` or :ref:`vector <t_vector>`; of floating-point values. The second argument specifies, which tests to perform. It must be a compile-time; integer constant, each bit in which specifies floating-point class:. +-------+----------------------+; | Bit # | floating-point class |; +=======+======================+; | 0 | Signaling NaN |; +-------+----------------------+; | 1 | Quiet NaN |; +-------+----------------------+; | 2 | Negative infinity |; +-------+----------------------+; | 3 | Negative normal |; +-------+----------------------+; | 4 | Negative subnormal |; +-------+----------------------+; | 5 | Negative zero |; +-------+----------------------+; | 6 | Positive zero |; +-------+----------------------+; | 7 | Positive subnormal |; +-------+----------------------+; | 8 | Positive normal |; +-------+----------------------+; | 9 | Positive infinity |; +-------+----------------------+. Semantics:; """""""""""""""""""". The function checks if ``op`` belongs to any of the floating-point classes; specified by ``test``. If ``op`` is a vector, then the check is made element by; eleme",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:922208,perform,perform,922208,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['perform']
Performance,"s; /showFilenames- Don't print the name of each compiled file (default); /showFilenames Print the name of each compiled file; /showIncludes Print info about included files to stderr; /source-charset:<value> Source encoding, supports only UTF-8; /std:<value> Language standard to compile for; /TC Treat all source files as C; /Tc <filename> Specify a C source file; /TP Treat all source files as C++; /Tp <filename> Specify a C++ source file; /utf-8 Set source and runtime encoding to UTF-8 (default); /U <macro> Undefine macro; /vd<value> Control vtordisp placement; /vmb Use a best-case representation method for member pointers; /vmg Use a most-general representation for member pointers; /vmm Set the default most-general representation to multiple inheritance; /vms Set the default most-general representation to single inheritance; /vmv Set the default most-general representation to virtual inheritance; /volatile:iso Volatile loads and stores have standard semantics; /volatile:ms Volatile loads and stores have acquire and release semantics; /W0 Disable all warnings; /W1 Enable -Wall; /W2 Enable -Wall; /W3 Enable -Wall; /W4 Enable -Wall and -Wextra; /Wall Enable -Weverything; /WX- Do not treat warnings as errors; /WX Treat warnings as errors; /w Disable all warnings; /X Don't add %INCLUDE% to the include search path; /Y- Disable precompiled headers, overrides /Yc and /Yu; /Yc<filename> Generate a pch file for all code up to and including <filename>; /Yu<filename> Load a pch file and use it instead of all code up to and including <filename>; /Z7 Enable CodeView debug information in object files; /Zc:char8_t Enable C++20 char8_t type; /Zc:char8_t- Disable C++20 char8_t type; /Zc:dllexportInlines- Don't dllexport/dllimport inline member functions of dllexport/import classes; /Zc:dllexportInlines dllexport/dllimport inline member functions of dllexport/import classes (default); /Zc:sizedDealloc- Disable C++14 sized global deallocation functions; /Zc:sizedDealloc Enable C++14 si",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:175903,load,loads,175903,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,1,['load'],['loads']
Performance,"s; AST Visitors; Testing; Useful Commands/Debugging Hints. Attaching the Debugger; Narrowing Down the Problem; Visualizing the Analysis; Debug Prints and Tricks. Additional Sources of Information; Useful Links. Getting Started. To check out the source code and build the project, follow steps 1-4 of; the Clang Getting Started; page.; The analyzer source code is located under the Clang source tree:; ; $ cd llvm/tools/clang. See: include/clang/StaticAnalyzer, lib/StaticAnalyzer,; test/Analysis.; The analyzer regression tests can be executed from the Clang's build; directory:; ; $ cd ../../../; cd build/tools/clang; TESTDIRS=Analysis make test. Analyze a file with the specified checker:; ; $ clang -cc1 -analyze -analyzer-checker=core.DivideZero test.c. List the available checkers:; ; $ clang -cc1 -analyzer-checker-help. See the analyzer help for different output formats, fine tuning, and; debug options:; ; $ clang -cc1 -help | grep ""analyzer"". Static Analyzer Overview; The analyzer core performs symbolic execution of the given program. All the; input values are represented with symbolic values; further, the engine deduces; the values of all the expressions in the program based on the input symbols; and the path. The execution is path sensitive and every possible path through; the program is explored. The explored execution traces are represented with; ExplodedGraph object.; Each node of the graph is; ExplodedNode,; which consists of a ProgramPoint and a ProgramState.; ; ProgramPoint; represents the corresponding location in the program (or the CFG).; ProgramPoint is also used to record additional information on; when/how the state was added. For example, PostPurgeDeadSymbolsKind; kind means that the state is the result of purging dead symbols - the; analyzer's equivalent of garbage collection.; ; ProgramState; represents abstract state of the program. It consists of:; ; Environment - a mapping from source code expressions to symbolic; values; Store - a mapping from memo",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/analyzer/checker_dev_manual.html:1946,perform,performs,1946,interpreter/llvm-project/clang/www/analyzer/checker_dev_manual.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/analyzer/checker_dev_manual.html,2,['perform'],['performs']
Performance,"s; N1791; Clang 2.9. Extending sizeof; N2253; DR850; Clang 3.1. Inline namespaces; N2535; Clang 2.9. Unrestricted unions; N2544; Clang 3.1. Local and unnamed types as template arguments; N2657; Clang 2.9. Range-based for; N2930; Clang 3.0. P0962R1 (DR); Clang 8. Explicit virtual overrides; N2928; N3206; N3272; Clang 3.0. Minimal support for garbage collection and reachability-based leak detection; N2670; N/A (2). Allowing move constructors to throw [noexcept]; N3050; Clang 3.0. Defining move special member functions; N3053; Clang 3.0. Concurrency. Sequence points; N2239; Clang 3.3. Atomic operations; N2427; Clang 3.1. Strong Compare and Exchange; N2748; Clang 3.1 (3). Bidirectional Fences; N2752; Clang 3.1. Memory model; N2429; Clang 3.2. Data-dependency ordering: atomics and memory model; N2664; Clang 3.2 (4). Propagating exceptions; N2179; Clang 2.9. Allow atomics use in signal handlers; N2547; Clang 3.1. Thread-local storage; N2659; Clang 3.3 (5). Dynamic initialization and destruction with concurrency; N2660; Clang 2.9. C99 Features in C++11. __func__ predefined identifier; N2340; Clang 2.9. C99 preprocessor; N1653; Clang 2.9. long long; N1811; Clang 2.9. Extended integral types; N1988; N/A (6). (1): The [[carries_dependency]] attribute; has no effect.; (2): No compiler changes are required for an implementation; such as Clang that does not provide garbage collection.; (3): All compare-exchange operations are emitted as; strong compare-exchanges.; (4): memory_order_consume is lowered to; memory_order_acquire.; (5): thread_local support; requires a C++ runtime library providing __cxa_thread_atexit, such; as libc++abi 3.6 or later,; or libsupc++ 4.8 or later.; (6): No compiler changes are required for an implementation; such as Clang that does not provide any extended integer types.; __int128 is not treated as an extended integer type,; because changing intmax_t would be an ABI-incompatible; change. C++98 implementation status; Clang implements all of the ISO C++ 1",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/cxx_status.html:17976,concurren,concurrency,17976,interpreter/llvm-project/clang/www/cxx_status.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/cxx_status.html,2,['concurren'],['concurrency']
Performance,"s; Using ccache materially improves average build times. Incremental builds; can be slightly faster, but introduce the risk of build corruption due to; e.g. state changes, etc... At this point, the recommendation is not to; use incremental builds and instead use ccache as the latter captures the; majority of the benefit with less risk of false positives. One of the non-obvious benefits of using ccache is that it makes the; builder less sensitive to which projects are being monitored vs built.; If a change triggers a build request, but doesn't change the build output; (e.g. doc changes, python utility changes, etc..), the build will entirely; hit in cache and the build request will complete in just the testing time. With multiple workers, it is tempting to try to configure a shared cache; between the workers. Experience to date indicates this is difficult to; well, and that having local per-worker caches gets most of the benefit; anyways. We don't currently recommend shared caches. CCache does depend on the builder hardware having sufficient IO to access; the cache with reasonable access times - i.e. a fast disk, or enough memory; for a RAM cache, etc.. For builders without, incremental may be your best; option, but is likely to require higher ongoing involvement from the; sponsor. Enable batch builds; As a last resort, you can configure your builder to batch build requests.; This makes the build failure notifications markedly less actionable, and; should only be done once all other reasonable measures have been taken. Leave it on the staging buildmaster; While most of this section has been biased towards builders intended for; the main buildmaster, it is worth highlighting that builders can run; indefinitely on the staging buildmaster. Such a builder may still be; useful for the sponsoring organization, without concern of negatively; impacting the broader community. The sponsoring organization simply; has to take on the responsibility of all bisection and triage. ; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToAddABuilder.rst:12157,cache,cache,12157,interpreter/llvm-project/llvm/docs/HowToAddABuilder.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToAddABuilder.rst,2,['cache'],['cache']
Performance,"s; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. .. note::. This section is the same as DWARF Version 5 section 2.5.1.4. .. _amdgpu-dwarf-type-conversions-operations:. A.2.5.4.3.3 Type Conversion Operations; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. .. note::. This section is the same as DWARF Version 5 section 2.5.1.6. .. _amdgpu-dwarf-general-operations:. A.2.5.4.3.4 Special Value Operations; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. .. note::. This section replaces parts of DWARF Version 5 sections 2.5.1.2, 2.5.1.3, and; 2.5.1.7. There are these special value operations currently defined:. 1. ``DW_OP_regval_type``. ``DW_OP_regval_type`` has two operands. The first is an unsigned LEB128; integer that represents a register number R. The second is an unsigned; LEB128 integer DR that represents the byte offset of a debugging information; entry D relative to the beginning of the current compilation unit, that; provides the type T of the register value. The operation is equivalent to performing ``DW_OP_regx R; DW_OP_deref_type; DR``. .. note::. Should DWARF allow the type T to be a larger size than the size of the; register R? Restricting a larger bit size avoids any issue of conversion; as the, possibly truncated, bit contents of the register is simply; interpreted as a value of T. If a conversion is wanted it can be done; explicitly using a ``DW_OP_convert`` operation. GDB has a per register hook that allows a target specific conversion on a; register by register basis. It defaults to truncation of bigger registers.; Removing use of the target hook does not cause any test failures in common; architectures. If the compiler for a target architecture did want some; form of conversion, including a larger result type, it could always; explicitly use the ``DW_OP_convert`` operation. If T is a larger type than the register size, then the default GDB; register hook reads bytes from the next register (or reads out of bounds; for the last register!). Removing use of the target hook",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst:87401,perform,performing,87401,interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst,1,['perform'],['performing']
Performance,"s; a pointer to a material and the additional data members representing the; properties related to tracking. ``` {.cpp}; TGeoMedium(const char *name,Int_t numed,TGeoMaterial *mat,; Double_t *params=0);; ```. - `name:` name assigned to the medium; - `mat:` pointer to a material; - `params:` array of additional parameters. Another constructor allows effectively defining tracking parameters in; GEANT3 style:. ``` {.cpp}; TGeoMedium(const char *name,Int_t numed,Int_t imat,Int_t ifield,; Double_t fieldm,Double_t tmaxfd,Double_t stemax,; Double_t deemax,Double_t epsil,Double_t stmin);; ```. This constructor is reserved for creating tracking media from the VMC; interface [...]:. - `numed:` user-defined medium index; - `imat:` unique ID of the material; - `others:` see G3 documentation. Looking at our simple world example, one can see that for creating; volumes one needs to create tracking media before. The way to proceed; for those not interested in performing tracking with external MC's is to; define and use only one `dummy tracking medium` as in the example (or a; `NULL` pointer). ### User Interface for Handling Materials and Media. The **`TGeoManager`** class contains the API for accessing and handling; defined materials:. ``` {.cpp}; TGeoManager::GetMaterial(name);; ```. ## Shapes. Shapes are geometrical objects that provide the basic modeling; functionality. They provide the definition of the `local` coordinate; system of the volume. Any volume must have a shape. Any shape recognized; by the modeller has to derive from the base **`TGeoShape`** class,; providing methods for:. - Finding out if a point defined in their local frame is contained or; not by the shape;; - Computing the distance to enter/exit the shape from a local point,; given a known direction;; - Computing the maximum distance in any direction from a local point; that does NOT result in a boundary crossing of the shape (safe; distance);; - Computing the cosines of the normal vector to the crossed shape; s",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md:22799,perform,performing,22799,documentation/users-guide/Geometry.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md,1,['perform'],['performing']
Performance,"s; are. Some performance, without cost in terms of programmer effort, may; be gained by using `psyco`, see the next link:; <http://psyco.sourceforge.net>, a Python just in time compiler (JIT).; Note, however, that `psyco` is limited to Intel i386 CPUs. Since `psyco`; optimizes Python, not `PyROOT` calls; it generally does not improve; performance that much if most of your code consists of ROOT API calls.; Mathematical computations in Python, on the other hand, benefit a lot. Every call to a Python member function results in a lookup of that; member function and an association of this method with `'self'`.; Furthermore, a temporary object is created during this process that is; discarded after the method call. In inner loops, it may be worth your; while (up to 30%), to short-cut this process by looking up and binding; the method before the loop, and discarding it afterwards. Here is an; example:. ``` {.cpp}; hpx = TH1F('hpx','px',100,-4,4); hpxFill = hpx.Fill # cache bound method; for i in xrange(25000):; px = gRandom.Gaus(); hpxFill(px) # use bound method: no lookup needed; del hpxFill # done with cached method; ```. Note that if you do not discard the bound method, a reference to the; histogram will remain outstanding, and it will not be deleted when it; should be. It is therefore important to delete the method when you're; done with it. ### Use of Python Functions. It is possible to mix Python functions with ROOT and perform such; operations as plotting and fitting of histograms with them. In all; cases, the procedure consists of instantiating a ROOT **`TF1`**,; **`TF2`**, or **`TF3`** with the Python function and working with that; ROOT object. There are some memory issues, so it is for example not yet; possible to delete a **`TF1`** instance and then create another one with; the same name. In addition, the Python function, once used for; instantiating the **`TF1`**, is never deleted. Instead of a Python function, you can also use callable instances (e.g.,; an ins",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/PythonRuby.md:24515,cache,cache,24515,documentation/users-guide/PythonRuby.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/PythonRuby.md,1,['cache'],['cache']
Performance,"s; determined by the value specified in the kernarg_preload_spec_length field of; the kernel descriptor. This data is then loaded into consecutive User SGPRs. The; number of SGPRs receiving preloaded kernarg data corresponds with the value; given by kernarg_preload_spec_length. The preloading starts at the dword offset; within the kernarg segment, which is specified by the; kernarg_preload_spec_offset field. If the kernarg_preload_spec_length is non-zero, the CP firmware will append an; additional 256 bytes to the kernel_code_entry_byte_offset. This addition; facilitates the incorporation of a prologue to the kernel entry to handle cases; where code designed for kernarg preloading is executed on hardware equipped with; incompatible firmware. If hardware has compatible firmware the 256 bytes at the; start of the kernel entry will be skipped. .. _amdgpu-amdhsa-kernel-prolog:. Kernel Prolog; ~~~~~~~~~~~~~. The compiler performs initialization in the kernel prologue depending on the; target and information about things like stack usage in the kernel and called; functions. Some of this initialization requires the compiler to request certain; User and System SGPRs be present in the; :ref:`amdgpu-amdhsa-initial-kernel-execution-state` via the; :ref:`amdgpu-amdhsa-kernel-descriptor`. .. _amdgpu-amdhsa-kernel-prolog-cfi:. CFI; +++. 1. The CFI return address is undefined. 2. The CFI CFA is defined using an expression which evaluates to a location; description that comprises one memory location description for the; ``DW_ASPACE_AMDGPU_private_lane`` address space address ``0``. .. _amdgpu-amdhsa-kernel-prolog-m0:. M0; ++. GFX6-GFX8; The M0 register must be initialized with a value at least the total LDS size; if the kernel may access LDS via DS or flat operations. Total LDS size is; available in dispatch packet. For M0, it is also possible to use maximum; possible value of LDS for given target (0x7FFF for GFX6 and 0xFFFF for; GFX7-GFX8).; GFX9-GFX11; The M0 register is not used",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:191885,perform,performs,191885,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['perform'],['performs']
Performance,"s; nobody wants to see extra blank labels. - The Confidence interval colors set by SetConfidenceIntervalColors (TRatioPlot) were inverted. - Add GetZaxis for THStack. - Fix Graph Errorbar Offsets for the new Marker Styles and thick markers. - When the palette width is bigger than the palette height, the palette; is automatically drawn horizontally. - THStack::GetXaxis->SetRange did not auto-zoom Yaxis range. - The Paint method of THStack always redrew the histograms in the sub-pads defined by the; THStack drawing option ""pads"". Like the ""pad dividing"" the ""histograms' drawing"" should be; done only the first time the THStack is painted otherwise any additional graphics objects; added in one of the pads (created by the ""pads"" option) will be removed. - Improve TRatioPlot axes drawing. ## Math Libraries. - `RVec` has been heavily re-engineered in order to add a small buffer optimization and to streamline its internals. The change should provide a small performance boost to; applications that make heavy use of `RVec`s and should otherwise be user-transparent. Please report any issues you should encounter.; - I/O support of `RVec` objects has been optimized. As a side-effect, `RVec`s can now be read back as `std::vector`s and vice-versa.; - Add `ROOT::VecOps::Drop`, an operation that removes `RVec` elements at the specified indices.; - handy aliases `ROOT::RVecI`, `ROOT::RVecD`, `ROOT::RVecF`, ..., have been introduced as short-hands for `RVec<int>`, `RVec<double>`, `RVec<float>`, ...; - Add `VecOps::StableArgsort` and `VecOps::StableSort` operations. ## RooFit Libraries. ### Experimental CUDA support for RooFit's `BatchMode`. RooFit's [`BatchMode`](https://root.cern/doc/master/classRooAbsPdf.html#a8f802a3a93467d5b7b089e3ccaec0fa8) has been around; [since ROOT 6.20](https://root.cern/doc/v620/release-notes.html#fast-function-evaluation-and-vectorisation).; It was further [improved in ROOT 6.24](https://root.cern/doc/v624/release-notes.html#massive-speed-up-of-roofits-bat",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v626/index.md:14139,perform,performance,14139,README/ReleaseNotes/v626/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v626/index.md,1,['perform'],['performance']
Performance,"s; passed in the kernarg. "".value_type"" string Unused and deprecated. This should no longer; be emitted, but is accepted for compatibility. "".pointee_align"" integer Alignment in bytes of pointee; type for pointer type kernel; argument. Must be a power; of 2. Only present if; "".value_kind"" is; ""dynamic_shared_pointer"".; "".address_space"" string Kernel argument address space; qualifier. Only present if; "".value_kind"" is ""global_buffer"" or; ""dynamic_shared_pointer"". Values; are:. - ""private""; - ""global""; - ""constant""; - ""local""; - ""generic""; - ""region"". .. TODO::. Is ""global_buffer"" only ""global""; or ""constant""? Is; ""dynamic_shared_pointer"" always; ""local""? Can HCC allow ""generic""?; How can ""private"" or ""region""; ever happen?. "".access"" string Kernel argument access; qualifier. Only present if; "".value_kind"" is ""image"" or; ""pipe"". Values; are:. - ""read_only""; - ""write_only""; - ""read_write"". .. TODO::. Does this apply to; ""global_buffer""?. "".actual_access"" string The actual memory accesses; performed by the kernel on the; kernel argument. Only present if; "".value_kind"" is ""global_buffer"",; ""image"", or ""pipe"". This may be; more restrictive than indicated; by "".access"" to reflect what the; kernel actual does. If not; present then the runtime must; assume what is implied by; "".access"" and "".is_const"" . Values; are:. - ""read_only""; - ""write_only""; - ""read_write"". "".is_const"" boolean Indicates if the kernel argument; is const qualified. Only present; if "".value_kind"" is; ""global_buffer"". "".is_restrict"" boolean Indicates if the kernel argument; is restrict qualified. Only; present if "".value_kind"" is; ""global_buffer"". "".is_volatile"" boolean Indicates if the kernel argument; is volatile qualified. Only; present if "".value_kind"" is; ""global_buffer"". "".is_pipe"" boolean Indicates if the kernel argument; is pipe qualified. Only present; if "".value_kind"" is ""pipe"". .. TODO::. Can ""global_buffer"" be pipe; qualified?. ====================== ============== ========= ===================",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:139789,perform,performed,139789,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['perform'],['performed']
Performance,"s; safe. The next stage is to check if computation of the distance to a give; physical object specified by a path was required. If this is the case,; the modeller changes the state to point to the required object, converts; the current point and direction coordinates to the local frame of this; object and computes the distance to its shape. The node returned is the; one pointed by the input path in case the shape is crossed; otherwise; the returned value is NULL. In case the distance to next crossed; boundary is required, the current point has to be physically INSIDE the; shape pointed by the current volume. This is only insured in case a call; to TGeoManager::FindNode() was performed for the current point.; Therefore, the first step is to convert the global current point and; direction in the local reference frame of the current volume and to; compute the distance to exit its shape from inside. The returned value; is again compared to the maximum allowed step (the proposed one) and in; case the distance is safe no other action is performed and the proposed; step is approved. In case the boundary is closer, the computed distance; is taken as maximum allowed step. For optimization purposed, for; particles starting very close to the current volume boundary (less than; 0.01 microns) and exiting the algorithm stops here. After computing the distance to exit the current node, the distance to; the daughter of the current volume which is crossed next is computed by; TGeoManager::FindNextDaughterBoundary(). This computes the; distance to all daughter candidates that can be possibly crossed by; using volume voxelization. The algorithm is efficient in average only in; case the number of daughters is greater than 4. For fewer nodes, a; simple loop is performed and the minimum distance (from a point outside; each shape) is taken and compared to the maximum allowed step. The step; value is again updated if `step<stepmax` . A special case is when the current node is declared as p",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md:123335,perform,performed,123335,geom/geom/doc/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md,1,['perform'],['performed']
Performance,"s; ~~~~~~~~~~~~~~~~~~~~~~~. .. option:: -O0, -O1, -O2, -O3, -Ofast, -Os, -Oz, -Og, -O, -O4. Specify which optimization level to use:. :option:`-O0` Means ""no optimization"": this level compiles the fastest and; generates the most debuggable code. :option:`-O1` Somewhere between :option:`-O0` and :option:`-O2`. :option:`-O2` Moderate level of optimization which enables most; optimizations. :option:`-O3` Like :option:`-O2`, except that it enables optimizations that; take longer to perform or that may generate larger code (in an attempt to; make the program run faster). :option:`-Ofast` Enables all the optimizations from :option:`-O3` along; with other aggressive optimizations that may violate strict compliance with; language standards. :option:`-Os` Like :option:`-O2` with extra optimizations to reduce code; size. :option:`-Oz` Like :option:`-Os` (and thus :option:`-O2`), but reduces code; size further. :option:`-Og` Like :option:`-O1`. In future versions, this option might; disable different optimizations in order to improve debuggability. :option:`-O` Equivalent to :option:`-O1`. :option:`-O4` and higher. Currently equivalent to :option:`-O3`. .. option:: -g, -gline-tables-only, -gmodules. Control debug information output. Note that Clang debug information works; best at :option:`-O0`. When more than one option starting with `-g` is; specified, the last one wins:. :option:`-g` Generate debug information. :option:`-gline-tables-only` Generate only line table debug information. This; allows for symbolicated backtraces with inlining information, but does not; include any information about variables, their locations or types. :option:`-gmodules` Generate debug information that contains external; references to types defined in Clang modules or precompiled headers instead; of emitting redundant debug type information into every object file. This; option transparently switches the Clang module format to object file; containers that hold the Clang module together with the de",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/CommandGuide/clang.rst:11381,optimiz,optimizations,11381,interpreter/llvm-project/clang/docs/CommandGuide/clang.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/CommandGuide/clang.rst,1,['optimiz'],['optimizations']
Performance,"s_store; atomicrmw monotonic - singlethread - global 1. buffer/global/flat_atomic; - wavefront - generic; - workgroup; - agent; atomicrmw monotonic - system - global 1. buffer/global/flat_atomic; - generic; atomicrmw monotonic - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; - workgroup be used.*. 1. ds_atomic; **Acquire Atomic**; ------------------------------------------------------------------------------------; load atomic acquire - singlethread - global 1. buffer/global/ds/flat_load; - wavefront - local; - generic; load atomic acquire - workgroup - global 1. buffer/global_load glc=1. - If not TgSplit execution; mode, omit glc=1. 2. s_waitcnt vmcnt(0). - If not TgSplit execution; mode, omit.; - Must happen before the; following buffer_wbinvl1_vol. 3. buffer_wbinvl1_vol. - If not TgSplit execution; mode, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale data. load atomic acquire - workgroup - local *If TgSplit execution mode,; local address space cannot; be used.*. 1. ds_load; 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than the local load; atomic value being; acquired. load atomic acquire - workgroup - generic 1. flat_load glc=1. - If not TgSplit execution; mode, omit glc=1. 2. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL, omit lgkmcnt(0).; - Must happen before; the following; buffer_wbinvl1_vol and any; following global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local load; atomic value being; acquired. 3. buffer_wbinvl1_vol. - If not TgSplit execution; mode, omit.; - Ensures that; following;",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:245725,load,loads,245725,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['loads']
Performance,"s_waitcnt lgkmcnt(0) &; - system vmcnt(0). - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global have; completed before; performing the; atomicrmw that is; being released. 2. flat_atomic; 3. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If OpenCL, omit; lgkmcnt(0).; - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the; atomicrmw has; completed before; invalidating the; cache. 4. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. fence acq_rel - singlethread *none* *none*; - wavefront; fence acq_rel - workgroup *none* 1. s_waitcnt lgkmcnt(0). - If OpenCL and; address space is; not generic, omit.; - However,; since LLVM; currently has no; address space on; the fence need to; conservatively; always generate; (see comment for; previous fence).; - Must happen after; any preceding; local/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that all; memory operations; to local have; completed before; performing any; following global; memory operations.; - Ensures that the; preceding; local/generic load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; acquire-fence-paired-atomic); has completed; before following; global memory; operations. This; satisfies the; requirements of; acquire.; - Ensur",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:227219,load,loads,227219,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['loads']
Performance,"s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate; (see comment for; previous fence).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - Must happen before; the following; buffer_wbinvl1_vol.; - Ensures that the; fence-paired atomic; has completed; before invalidating; the; cache. Therefore; any following; locations read must; be no older than; the value read by; the; fence-paired-atomic. 2. buffer_wbinvl1_vol. - Must happen before any; following global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. fence acquire - system *none* 1. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate; (see comment for; previous fence).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than;",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:254929,cache,cache,254929,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['cache'],['cache']
Performance,"sage:. ``` {.cpp}; >>> from ROOT import std; >>> v = std.vector(int)(); >>> for i in range(0,10):; ... v.push_back(i); ...; >>> for i in v:; ... print(i, end=' '); 1 2 3 4 5 6 7 8 9; >>>; >>> list(v); [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]; >>>; ```. The parameters to the template instantiation can either be an actual; type or value (as is used here, ""int""), or a string representation of; the parameters (e.g. ""'double'""), or a mixture of both (e.g. ""'TCanvas,; 0'"" or ""'double', 0"" ). The ""std::vector\<int\>"" class is one of the; classes builtin by default into the Cling extension dlls. You will get a; non-functional class (instances of which can still be passed around to; C++) if the corresponding dictionary doesn't exist. #### Access to ROOT Globals. Most globals and global functions can be imported directly from the; ROOT.py module, but some common ones (most notably **`gMinuit`**,; although that variable now exists at startup from release 5.08 onward); do not exist yet at program startup, as they exist in modules that are; loaded later (e.g. through the auto-loading mechanism). An example; session should make this clear:. ``` {.cpp}; >>> from ROOT import *; >>> gROOT # directly available; <ROOT.TROOT object at 0x399c30>; >>> gMinuit # library not yet loaded: not available; Traceback (most recent call last):; File ""<stdin>"", line 1, in ?; NameError: name 'gMinuit' is not defined; >>> TMinuit # use of TMinuit class forces auto-loading; <class '__main__.TMinuit'>; >>> gMinuit # now gMinuit is available; <__main__.TMinuit object at 0x1458c70>; >>> not not gMinuit # but it is the null pointer, until set; False; >>> g = TMinuit(); >>> not not gMinuit; True; ```. It is also possible to create globals interactively, either by executing; a Cling macro, or by a call to `gROOT.ProcessLine()`. These globals are; made available in the same way: either use them directly after creation; in 'from ROOT import \*' more, or get them from the ROOT namespace after; an 'import ROOT'. As of 5.",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/PythonRuby.md:13088,load,loaded,13088,documentation/users-guide/PythonRuby.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/PythonRuby.md,1,['load'],['loaded']
Performance,"same effect as specifying ``-ffinite-math-only``. .. option:: -f[no-]approx-func. Allow certain math function calls (such as ``log``, ``sqrt``, ``pow``, etc); to be replaced with an approximately equivalent set of instructions; or alternative math function calls. For example, a ``pow(x, 0.25)``; may be replaced with ``sqrt(sqrt(x))``, despite being an inexact result; in cases where ``x`` is ``-0.0`` or ``-inf``.; Defaults to ``-fno-approx-func``. .. option:: -f[no-]signed-zeros. Allow optimizations that ignore the sign of floating point zeros.; Defaults to ``-fsigned-zeros``. .. option:: -f[no-]associative-math. Allow floating point operations to be reassociated.; Defaults to ``-fno-associative-math``. .. option:: -f[no-]reciprocal-math. Allow division operations to be transformed into multiplication by a; reciprocal. This can be significantly faster than an ordinary division; but can also have significantly less precision. Defaults to; ``-fno-reciprocal-math``. .. option:: -f[no-]unsafe-math-optimizations. Allow unsafe floating-point optimizations.; ``-funsafe-math-optimizations`` also implies:. * ``-fapprox-func``; * ``-fassociative-math``; * ``-freciprocal-math``; * ``-fno-signed-zeros``; * ``-fno-trapping-math``; * ``-ffp-contract=fast``. ``-fno-unsafe-math-optimizations`` implies:. * ``-fno-approx-func``; * ``-fno-associative-math``; * ``-fno-reciprocal-math``; * ``-fsigned-zeros``; * ``-ftrapping-math``; * ``-ffp-contract=on``; * ``-fdenormal-fp-math=ieee``. There is ambiguity about how ``-ffp-contract``,; ``-funsafe-math-optimizations``, and ``-fno-unsafe-math-optimizations``; behave when combined. Explanation in :option:`-fno-fast-math` also applies; to these options. Defaults to ``-fno-unsafe-math-optimizations``. .. option:: -f[no-]finite-math-only. Allow floating-point optimizations that assume arguments and results are; not NaNs or +-Inf. ``-ffinite-math-only`` defines the; ``__FINITE_MATH_ONLY__`` preprocessor macro.; ``-ffinite-math-only`` implies:. * `",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:59229,optimiz,optimizations,59229,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,1,['optimiz'],['optimizations']
Performance,"sary to build additional targets before running perf training, such as; builtins and runtime libraries. You can use the :code:`CLANG_PGO_TRAINING_DEPS` CMake; variable for that purpose:. .. code-block:: cmake. set(CLANG_PGO_TRAINING_DEPS builtins runtimes CACHE STRING """"). The PGO cache has a slightly different stage naming scheme than other; multi-stage builds. It generates three stages: stage1, stage2-instrumented, and; stage2. Both of the stage2 builds are built using the stage1 compiler. The PGO cache generates the following additional targets:. **stage2-instrumented**; Builds a stage1 compiler, runtime, and required tools (llvm-config,; llvm-profdata) then uses that compiler to build an instrumented stage2 compiler. **stage2-instrumented-generate-profdata**; Depends on stage2-instrumented and will use the instrumented compiler to; generate profdata based on the training files in clang/utils/perf-training. **stage2**; Depends on stage2-instrumented-generate-profdata and will use the stage1; compiler with the stage2 profdata to build a PGO-optimized compiler. **stage2-check-llvm**; Depends on stage2 and runs check-llvm using the stage2 compiler. **stage2-check-clang**; Depends on stage2 and runs check-clang using the stage2 compiler. **stage2-check-all**; Depends on stage2 and runs check-all using the stage2 compiler. **stage2-test-suite**; Depends on stage2 and runs the test-suite using the stage2 compiler (requires; in-tree test-suite). BOLT; ====. `BOLT <https://github.com/llvm/llvm-project/blob/main/bolt/README.md>`_; (Binary Optimization and Layout Tool) is a tool that optimizes binaries; post-link by profiling them at runtime and then using that information to; optimize the layout of the final binary among other optimizations performed; at the binary level. There are also CMake caches available to build; LLVM/Clang with BOLT. To configure a single-stage build that builds LLVM/Clang and then optimizes; it with BOLT, use the following CMake configuration:. .. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AdvancedBuilds.rst:9404,optimiz,optimized,9404,interpreter/llvm-project/llvm/docs/AdvancedBuilds.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AdvancedBuilds.rst,1,['optimiz'],['optimized']
Performance,"saved. ``` {.cpp}; void tree3AddBranch() {; TFile f(""tree3.root"",""update"");; Float_t new_v;; TTree *t3 = (TTree*)f->Get(""t3"");; TBranch *newBranch = t3-> Branch(""new_v"",&new_v,""new_v/F"");; //read the number of entries in the t3; Int_t nentries = (Int_t)t3->GetEntries();; for (Int_t i = 0; i < nentries; i++){; new_v= gRandom->Gaus(0,1);; newBranch->Fill();; }; t3->Write("""",TObject::kOverwrite); // save only the new version of; // the tree; }; ```. Adding a branch is often not possible because the tree is in a read-only; file and you do not have permission to save the modified tree with the; new branch. Even if you do have the permission, you risk loosing the; original tree with an unsuccessful attempt to save the modification.; Since trees are usually large, adding a branch could extend it over the; 2GB limit. In this case, the attempt to write the tree fails, and the; original data is may also be corrupted. In addition, adding a branch to; a tree enlarges the tree and increases the amount of memory needed to; read an entry, and therefore decreases the performance. For these; reasons, ROOT offers the concept of friends for trees (and chains). We; encourage you to use `TTree::AddFriend` rather than adding a branch; manually. ### TTree::AddFriend. A tree keeps a list of friends. In the context of a tree (or a chain),; friendship means unrestricted access to the friends data. In this way it; is much like adding another branch to the tree without taking the risk; of damaging it. To add a friend to the list, you can use the; `TTree::AddFriend` method. The **`TTree`** (`tree`) below has two; friends (`ft1` and `ft2`) and now has access to the variables; `a,b,c,i,j,k,l` and `m`. ![](pictures/02000101.jpg). The `AddFriend` method has two parameters, the first is the tree name; and the second is the name of the ROOT file where the friend tree is; saved. `AddFriend` automatically opens the friend file. If no file name; is given, the tree called `ft1` is assumed to be in the sa",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Trees.md:52558,perform,performance,52558,documentation/users-guide/Trees.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Trees.md,1,['perform'],['performance']
Performance,"scnt(0)`` is; required to ensure synchronization between vector memory operations of; different SAs. It ensures a previous vector memory operation has completed; before executing a subsequent vector memory and so can be used to meet the; requirements of acquire, release and sequential consistency.; * The L2 cache can be kept coherent with other agents on some targets, or ranges; of virtual addresses can be set up to bypass it to ensure system coherence.; * On GFX10.3 and GFX11 a memory attached last level (MALL) cache exists for GPU memory.; The MALL cache is fully coherent with GPU memory and has no impact on system; coherence. All agents (GPU and CPU) access GPU memory through the MALL cache. Scalar memory operations are only used to access memory that is proven to not; change during the execution of the kernel dispatch. This includes constant; address space and global address space for program scope ``const`` variables.; Therefore, the kernel machine code does not have to maintain the scalar cache to; ensure it is coherent with the vector caches. The scalar and vector caches are; invalidated between kernel dispatches by CP since constant address space data; may change between kernel dispatch executions. See; :ref:`amdgpu-amdhsa-memory-spaces`. The one exception is if scalar writes are used to spill SGPR registers. In this; case the AMDGPU backend ensures the memory location used to spill is never; accessed by vector memory operations at the same time. If scalar writes are used; then a ``s_dcache_wb`` is inserted before the ``s_endpgm`` and before a function; return since the locations may be used for vector memory instructions by a; future wavefront that uses the same scratch area, or a function call that; creates a frame at the same address, respectively. There is no need for a; ``s_dcache_inv`` as all scalar writes are write-before-read in the same thread. For kernarg backing memory:. * CP invalidates the L0 and L1 caches at the start of each kernel dispatch.; *",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:340271,cache,cache,340271,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['cache'],"['cache', 'caches']"
Performance,"scompilation debuggers, ``bugpoint`` only works; with programs that have deterministic output. Thus, if the program outputs; ``argv[0]``, the date, time, or any other ""random"" data, ``bugpoint`` may; misinterpret differences in these data, when output, as the result of a; miscompilation. Programs should be temporarily modified to disable outputs; that are likely to vary from run to run. * In the `crash debugger`_, ``bugpoint`` does not distinguish different crashes; during reduction. Thus, if new crash or miscompilation happens, ``bugpoint``; will continue with the new crash instead. If you would like to stick to; particular crash, you should write check scripts to validate the error; message, see ``-compile-command`` in :doc:`CommandGuide/bugpoint`. * In the code generator and miscompilation debuggers, debugging will go faster; if you manually modify the program or its inputs to reduce the runtime, but; still exhibit the problem. * ``bugpoint`` is extremely useful when working on a new optimization: it helps; track down regressions quickly. To avoid having to relink ``bugpoint`` every; time you change your optimization however, have ``bugpoint`` dynamically load; your optimization with the ``-load`` option. * ``bugpoint`` can generate a lot of output and run for a long period of time.; It is often useful to capture the output of the program to file. For example,; in the C shell, you can run:. .. code-block:: console. $ bugpoint ... |& tee bugpoint.log. to get a copy of ``bugpoint``'s output in the file ``bugpoint.log``, as well; as on your terminal. * ``bugpoint`` cannot debug problems with the LLVM linker. If ``bugpoint``; crashes before you see its ""All input ok"" message, you might try ``llvm-link; -v`` on the same set of input files. If that also crashes, you may be; experiencing a linker bug. * ``bugpoint`` is useful for proactively finding bugs in LLVM. Invoking; ``bugpoint`` with the ``-find-bugs`` option will cause the list of specified; optimizations to be r",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Bugpoint.rst:6778,optimiz,optimization,6778,interpreter/llvm-project/llvm/docs/Bugpoint.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Bugpoint.rst,1,['optimiz'],['optimization']
Performance,"scope. However, some passes want to peek up or; down the IR hierarchy. For example, an SCC pass may want to look at function; analyses for the functions inside the SCC. Or it may want to look at some; immutable global analysis. In these cases, the analysis manager can provide a; proxy to an outer or inner level analysis manager. For example, to get a; ``FunctionAnalysisManager`` from a ``CGSCCAnalysisManager``, you can call. .. code-block:: c++. FunctionAnalysisManager &FAM =; AM.getResult<FunctionAnalysisManagerCGSCCProxy>(InitialC, CG); .getManager();. and use ``FAM`` as a typical ``FunctionAnalysisManager`` that a function pass; would have access to. To get access to an outer level IR analysis, you can; call. .. code-block:: c++. const auto &MAMProxy =; AM.getResult<ModuleAnalysisManagerCGSCCProxy>(InitialC, CG);; FooAnalysisResult *AR = MAMProxy.getCachedResult<FooAnalysis>(M);. Asking for a cached and immutable outer level IR analysis works via; ``getCachedResult()``, but getting direct access to an outer level IR analysis; manager to compute an outer level IR analysis is not allowed. This is for a; couple reasons. The first reason is that running analyses across outer level IR in inner level; IR passes can result in quadratic compile time behavior. For example, a module; analysis often scans every function and allowing function passes to run a module; analysis may cause us to scan functions a quadratic number of times. If passes; could keep outer level analyses up to date rather than computing them on demand; this wouldn't be an issue, but that would be a lot of work to ensure every pass; updates all outer level analyses, and so far this hasn't been necessary and; there isn't infrastructure for this (aside from function analyses in loop passes; as described below). Self-updating analyses that gracefully degrade also handle; this problem (e.g. GlobalsAA), but they run into the issue of having to be; manually recomputed somewhere in the optimization pipeline if w",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:8656,cache,cached,8656,interpreter/llvm-project/llvm/docs/NewPassManager.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst,1,['cache'],['cached']
Performance,"scripting of the creation of a Python 'wheel' package; for the bindings.; * An facility for CMake-based projects to automate the entire bindings; generation process, including basic automated tests. .. note::. The JIT needs to resolve linker symbols in order to call them through; generated wrappers.; Thus, any classes, functions, and data that will be used in Python need; to be exported.; This is the default behavior on Mac and Linux, but not on Windows.; On that platform, use ``__declspec(dllexport)`` to explicitly export the; classes and function you expect to call.; CMake has simple `support for exporting all`_ C++ symbols. Python packaging; ----------------. Modern Python packaging usage is based on the 'wheel'. This is places the onus; on the creation of binary artifacts in the package on the distributor. In this; case, this includes the platform-dependent steps necessary to compile the .cpp; file. The generated package also takes advantage of the __init__.py load-time; mechanism to enhance the bindings:. * The bindings are rehosted in a ""native"" namespace so that C++ code from; KF5::Config appears in Python via ""import KF5.Config"".; * (TBD) Load Pythonizations. Both of these need/can use the output of the; :ref:`cppyy-generator <cppyy-generator>` (included in the package) as well as; other runtime support included in ``cppyy``. CMake usage; -----------. The CMake usage is via two modules:. * FindLibClang.cmake provides some bootstrap support needed to locate clang.; This is provided mostly as a temporary measure; hopefully upstream support; will allow this to be eliminated in due course.; * FindCppyy.cmake provides the interface described further here. Details of the usage of these modules is within the modules themselves, but; here is a summary of the usage. ``FindLibClang.cmake`` sets the following; variables:. ::. LibClang_FOUND - True if libclang is found.; LibClang_LIBRARY - Clang library to link against.; LibClang_VERSION - Version number as a string (e.g",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/cmake_interface.rst:1964,load,load-time,1964,bindings/pyroot/cppyy/cppyy/doc/source/cmake_interface.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/cmake_interface.rst,1,['load'],['load-time']
Performance,"scription; -------- ---------- ------------------- ---------------------------; 0x1 Register Reg Value in a register; 0x2 Direct Reg + Offset Frame index value; 0x3 Indirect [Reg + Offset] Spilled value; 0x4 Constant Offset Small constant; 0x5 ConstIndex Constants[Offset] Large constant; ======== ========== =================== ===========================. In the common case, a value is available in a register, and the; ``Offset`` field will be zero. Values spilled to the stack are encoded; as ``Indirect`` locations. The runtime must load those values from a; stack address, typically in the form ``[BP + Offset]``. If an; ``alloca`` value is passed directly to a stack map intrinsic, then; LLVM may fold the frame index into the stack map as an optimization to; avoid allocating a register or stack slot. These frame indices will be; encoded as ``Direct`` locations in the form ``BP + Offset``. LLVM may; also optimize constants by emitting them directly in the stack map,; either in the ``Offset`` of a ``Constant`` location or in the constant; pool, referred to by ``ConstantIndex`` locations. At each callsite, a ""liveout"" register list is also recorded. These; are the registers that are live across the stackmap and therefore must; be saved by the runtime. This is an important optimization when the; patchpoint intrinsic is used with a calling convention that by default; preserves most registers as callee-save. Each entry in the liveout register list contains a DWARF register; number and size in bytes. The stackmap format deliberately omits; specific subregister information. Instead the runtime must interpret; this information conservatively. For example, if the stackmap reports; one byte at ``%rax``, then the value may be in either ``%al`` or; ``%ah``. It doesn't matter in practice, because the runtime will; simply save ``%rax``. However, if the stackmap reports 16 bytes at; ``%ymm0``, then the runtime can safely optimize by saving only; ``%xmm0``. The stack map format is a c",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/StackMaps.rst:14276,optimiz,optimize,14276,interpreter/llvm-project/llvm/docs/StackMaps.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/StackMaps.rst,1,['optimiz'],['optimize']
Performance,"scriptors; plus a 32-bit offset into the buffer (in total encapsulating a 160-bit; *pointer*), allowing normal LLVM load/store/atomic operations to be used to; model the buffer descriptors used heavily in graphics workloads targeting; the backend. The buffer descriptor used to construct a buffer fat pointer must be *raw*:; the stride must be 0, the ""add tid"" flag must be 0, the swizzle enable bits; must be off, and the extent must be measured in bytes. (On subtargets where; bounds checking may be disabled, buffer fat pointers may choose to enable; it or not). **Buffer Resource**; The buffer resource pointer, in address space 8, is the newer form; for representing buffer descriptors in AMDGPU IR, replacing their; previous representation as `<4 x i32>`. It is a non-integral pointer; that represents a 128-bit buffer descriptor resource (`V#`). Since, in general, a buffer resource supports complex addressing modes that cannot; be easily represented in LLVM (such as implicit swizzled access to structured; buffers), it is **illegal** to perform non-trivial address computations, such as; ``getelementptr`` operations, on buffer resources. They may be passed to; AMDGPU buffer intrinsics, and they may be converted to and from ``i128``. Casting a buffer resource to a buffer fat pointer is permitted and adds an offset; of 0. Buffer resources can be created from 64-bit pointers (which should be either; generic or global) using the `llvm.amdgcn.make.buffer.rsrc` intrinsic, which; takes the pointer, which becomes the base of the resource,; the 16-bit stride (and swzizzle control) field stored in bits `63:48` of a `V#`,; the 32-bit NumRecords/extent field (bits `95:64`), and the 32-bit flags field; (bits `127:96`). The specific interpretation of these fields varies by the; target architecture and is detailed in the ISA descriptions. **Buffer Strided Pointer**; The buffer index pointer is an experimental address space. It represents; a 128-bit buffer descriptor and a 32-bit offset, ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:29788,perform,perform,29788,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['perform'],['perform']
Performance,"se `servers()`. - `RooAbsCollection::createIterator()`: use `begin()`, `end()` and range-based for loops; - `RooAbsCollection::iterator()`: same; - `RooAbsCollection::fwdIterator()`: same. - `RooWorkspace::componentIterator()`: use `RooWorkspace::components()` with range-based loop. ### Deprecation of legacy test statistics classes in public interface. Instantiating the following classes and even including their header files is deprecated, and the headers will be removed in ROOT 6.34:. * RooAbsTestStatistic; * RooAbsOptTestStatistic; * RooNLLVar; * RooChi2Var; * RooXYChi2Var. Please use the higher-level functions `RooAbsPdf::createNLL()` and `RooAbsPdf::createChi2()` if you want to create objects that represent test statistics. ### Change of RooParamHistFunc. The `RooParamHistFunc` didn't take any observable `RooRealVar` as constructor; argument. It assumes as observable the internal variables in the passed; RooDataHist. This means it was in most contexts unusable, because the input; can't be changed, other than loading a different bin in the dataset. Furthermore, there was actually a constructor that took a `RooAbsArg x`, but it; was simply ignored. To fix all these problems, the existing constructors were replaced by a new one; that takes the observable explicitly. Since the old constructors resulted in wrong computation graphs that caused; trouble with the new CPU evaluation backend, they had to be removed without; deprecation. Please adapt your code if necessary. ### Renaming of some RooFit classes. The `RooPower` was renamed to `RooPowerSum`, and `RooExpPoly` was renamed to `RooLegacyExpPoly`. This was a necessary change, because the names of these classes introduced in ROOT 6.28 collided with some classes in CMS combine, which were around already long before. Therefore, the classes had to be renamed to not cause any problems for CMS. In the unlikeliy case where you should have used these new classes for analysis already, please adapt your code to the new names ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v632/index.md:13435,load,loading,13435,README/ReleaseNotes/v632/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v632/index.md,1,['load'],['loading']
Performance,"se statement for your node in; ``LegalizeOp`` which calls LegalizeOp on the node's operands, and returns a; new node if any of the operands changed as a result of being legalized. It; is likely that not all targets supported by the SelectionDAG framework will; natively support the new node. In this case, you must also add code in your; node's case statement in ``LegalizeOp`` to Expand your node into simpler,; legal operations. The case for ``ISD::UREM`` for expanding a remainder into; a divide, multiply, and a subtract is a good example. #. ``lib/CodeGen/SelectionDAG/LegalizeDAG.cpp``:. If targets may support the new node being added only at certain sizes, you; will also need to add code to your node's case statement in ``LegalizeOp``; to Promote your node's operands to a larger size, and perform the correct; operation. You will also need to add code to ``PromoteOp`` to do this as; well. For a good example, see ``ISD::BSWAP``, which promotes its operand to; a wider size, performs the byteswap, and then shifts the correct bytes right; to emulate the narrower byteswap in the wider type. #. ``lib/CodeGen/SelectionDAG/LegalizeDAG.cpp``:. Add a case for your node in ``ExpandOp`` to teach the legalizer how to; perform the action represented by the new node on a value that has been split; into high and low halves. This case will be used to support your node with a; 64 bit operand on a 32 bit target. #. ``lib/CodeGen/SelectionDAG/DAGCombiner.cpp``:. If your node can be combined with itself, or other existing nodes in a; peephole-like fashion, add a visit function for it, and call that function; from. There are several good examples for simple combines you can do;; ``visitFABS`` and ``visitSRL`` are good starting places. #. ``lib/Target/PowerPC/PPCISelLowering.cpp``:. Each target has an implementation of the ``TargetLowering`` class, usually in; its own file (although some targets include it in the same file as the; DAGToDAGISel). The default behavior for a target is to assum",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ExtendingLLVM.rst:5675,perform,performs,5675,interpreter/llvm-project/llvm/docs/ExtendingLLVM.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ExtendingLLVM.rst,1,['perform'],['performs']
Performance,"se the behavior is undefined. Globals can also have a :ref:`DLL storage class <dllstorageclass>`,; an optional :ref:`runtime preemption specifier <runtime_preemption_model>`,; an optional :ref:`global attributes <glattrs>` and; an optional list of attached :ref:`metadata <metadata>`. Variables and aliases can have a; :ref:`Thread Local Storage Model <tls_model>`. Globals cannot be or contain :ref:`Scalable vectors <t_vector>` because their; size is unknown at compile time. They are allowed in structs to facilitate; intrinsics returning multiple values. Generally, structs containing scalable; vectors are not considered ""sized"" and cannot be used in loads, stores, allocas,; or GEPs. The only exception to this rule is for structs that contain scalable; vectors of the same type (e.g. ``{<vscale x 2 x i32>, <vscale x 2 x i32>}``; contains the same type while ``{<vscale x 2 x i32>, <vscale x 2 x i64>}``; doesn't). These kinds of structs (we may call them homogeneous scalable vector; structs) are considered sized and can be used in loads, stores, allocas, but; not GEPs. Syntax::. @<GlobalVarName> = [Linkage] [PreemptionSpecifier] [Visibility]; [DLLStorageClass] [ThreadLocal]; [(unnamed_addr|local_unnamed_addr)] [AddrSpace]; [ExternallyInitialized]; <global | constant> <Type> [<InitializerConstant>]; [, section ""name""] [, partition ""name""]; [, comdat [($name)]] [, align <Alignment>]; [, code_model ""model""]; [, no_sanitize_address] [, no_sanitize_hwaddress]; [, sanitize_address_dyninit] [, sanitize_memtag]; (, !name !N)*. For example, the following defines a global in a numbered address space; with an initializer, section, and alignment:. .. code-block:: llvm. @G = addrspace(5) constant float 1.0, section ""foo"", align 4. The following example just declares a global variable. .. code-block:: llvm. @G = external global i32. The following example defines a global variable with the; ``large`` code model:. .. code-block:: llvm. @G = internal global i32 0, code_model ""large"". The ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:36554,scalab,scalable,36554,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,2,"['load', 'scalab']","['loads', 'scalable']"
Performance,"se, it will read from ``filename``. Inputs can be in either the LLVM; assembly language format (``.ll``) or the LLVM bitcode format (``.bc``). If the :option:`-o` option is omitted, then :program:`llc` will send its output; to standard output if the input is from standard input. If the :option:`-o`; option specifies ""``-``"", then the output will also be sent to standard output. If no :option:`-o` option is specified and an input file other than ""``-``"" is; specified, then :program:`llc` creates the output filename by taking the input; filename, removing any existing ``.bc`` extension, and adding a ``.s`` suffix. Other :program:`llc` options are described below. End-user Options; ~~~~~~~~~~~~~~~~. .. option:: -help. Print a summary of command line options. .. option:: -o <filename>. Use ``<filename>`` as the output filename. See the summary above for more; details. .. option:: -O=uint. Generate code at different optimization levels. These correspond to the; ``-O0``, ``-O1``, ``-O2``, and ``-O3`` optimization levels used by; :program:`clang`. .. option:: -mtriple=<target triple>. Override the target triple specified in the input file with the specified; string. .. option:: -march=<arch>. Specify the architecture for which to generate assembly, overriding the target; encoded in the input file. See the output of ``llc -help`` for a list of; valid architectures. By default this is inferred from the target triple or; autodetected to the current architecture. .. option:: -mcpu=<cpuname>. Specify a specific chip in the current architecture to generate code for.; By default this is inferred from the target triple and autodetected to; the current architecture. For a list of available CPUs, use:. .. code-block:: none. llvm-as < /dev/null | llc -march=xyz -mcpu=help. .. option:: -filetype=<output file type>. Specify what kind of output ``llc`` should generated. Options are: ``asm``; for textual assembly ( ``'.s'``), ``obj`` for native object files (``'.o'``); and ``null`` for no",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llc.rst:1682,optimiz,optimization,1682,interpreter/llvm-project/llvm/docs/CommandGuide/llc.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llc.rst,1,['optimiz'],['optimization']
Performance,"se. Describing Language Specific Properties; =======================================. When translating a source language to LLVM, finding ways to express concepts; and guarantees available in your source language which are not natively; provided by LLVM IR will greatly improve LLVM's ability to optimize your code.; As an example, C/C++'s ability to mark every add as ""no signed wrap (nsw)"" goes; a long way to assisting the optimizer in reasoning about loop induction; variables and thus generating more optimal code for loops. The LLVM LangRef includes a number of mechanisms for annotating the IR with; additional semantic information. It is *strongly* recommended that you become; highly familiar with this document. The list below is intended to highlight a; couple of items of particular interest, but is by no means exhaustive. Restricted Operation Semantics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; #. Add nsw/nuw flags as appropriate. Reasoning about overflow is; generally hard for an optimizer so providing these facts from the frontend; can be very impactful. #. Use fast-math flags on floating point operations if legal. If you don't; need strict IEEE floating point semantics, there are a number of additional; optimizations that can be performed. This can be highly impactful for; floating point intensive computations. Describing Aliasing Properties; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. #. Add noalias/align/dereferenceable/nonnull to function arguments and return; values as appropriate. #. Use pointer aliasing metadata, especially tbaa metadata, to communicate; otherwise-non-deducible pointer aliasing facts. #. Use inbounds on geps. This can help to disambiguate some aliasing queries. Undefined Values; ^^^^^^^^^^^^^^^^. #. Use poison values instead of undef values whenever possible. #. Tag function parameters with the noundef attribute whenever possible. Modeling Memory Effects; ^^^^^^^^^^^^^^^^^^^^^^^^. #. Mark functions as readnone/readonly/argmemonly or noreturn/nounwind when; kno",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Frontend/PerformanceTips.rst:10781,optimiz,optimizer,10781,interpreter/llvm-project/llvm/docs/Frontend/PerformanceTips.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Frontend/PerformanceTips.rst,1,['optimiz'],['optimizer']
Performance,"se. This technique works transparently; for any p.d.f. stored in a RooWorkspace. One can store numeric integral values for problems with zero, one or two floating parameters.; In the first case, the value is simply stored. In cases with one or two floating parameters; a grid (histogram) of integral values is stored, which are interpolated to return integral; values for each value of the parameters. A new tutorial macro rf903_numintcache.C has been added to $ROOTSYS/tutorials/roofit; to illustrate the use of this feature. Representation of function and p.d.f. derivatives; A new class has been added that can represent the derivative of any p.d.f or function w.r.t. any; parameter or observable. To construct e.g. a first order derivative of a Gaussian p.d.f, do. RooAbsReal* dgdx = gauss.derivative(x,1) ;. A more complete example is available in the new tutorial macro rf111_derivatives.C. Improved handling of chi-squared fits; Chi-squared fits can now be performed through the same style of interface as likelihood fits,; through the newly added method RooAbsReal::chi2FitTo(const RooDataHist&,...). Functions that can be fitted with chi-squared minimization are any RooAbsReal based function; as well as RooAbsPdf based p.d.f.s. In case of non-extended p.d.f.s the probability density; calculated by the p.d.f. is multiplied with the number of events in the histogram to adjust; the scale of the function. In case of extended p.d.f.s, the adjustment is made with the expected; number of events, rather than the observed number of events. Tutorial macro rf602_chi2fit.C has been updated to use this new interface. Chi-squared fits to X-Y datasets now possible; In addition to the ability to perform chi-squared fits to histograms it is now also possible; to perform chi-squared fits to unbinned datasets containing a series of X and Y values; with associated errors on Y and optionally on X. These 'X-Y' chi-squared fits are interfaced through newly added method; RooAbsReal::chi2FitTo(const",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v524/index.html:7494,perform,performed,7494,roofit/doc/v524/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v524/index.html,2,['perform'],['performed']
Performance,"se.c -fmodules -fimplicit-module-maps -fprebuilt-module-path=prebuilt -fprebuilt-implicit-modules -DENABLE_A. Finally we want to allow implicit modules for configurations that were not prebuilt. When using the clang driver a module cache path is implicitly selected. Using ``-cc1``, we simply add use the ``-fmodules-cache-path`` option. .. code-block:: sh. clang -cc1 -emit-obj -o use.o use.c -fmodules -fimplicit-module-maps -fprebuilt-module-path=prebuilt -fprebuilt-implicit-modules -fmodules-cache-path=cache; clang -cc1 -emit-obj -o use.o use.c -fmodules -fimplicit-module-maps -fprebuilt-module-path=prebuilt -fprebuilt-implicit-modules -fmodules-cache-path=cache -DENABLE_A; clang -cc1 -emit-obj -o use.o use.c -fmodules -fimplicit-module-maps -fprebuilt-module-path=prebuilt -fprebuilt-implicit-modules -fmodules-cache-path=cache -DENABLE_A -DOTHER_OPTIONS. This way, a single directory containing multiple variants of modules can be prepared and reused. The options configuring the module cache are independent of other options. Module Semantics; ================. Modules are modeled as if each submodule were a separate translation unit, and a module import makes names from the other translation unit visible. Each submodule starts with a new preprocessor state and an empty translation unit. .. note::. This behavior is currently only approximated when building a module with submodules. Entities within a submodule that has already been built are visible when building later submodules in that module. This can lead to fragile modules that depend on the build order used for the submodules of the module, and should not be relied upon. This behavior is subject to change. As an example, in C, this implies that if two structs are defined in different submodules with the same name, those two types are distinct types (but may be *compatible* types if their definitions match). In C++, two structs defined with the same name in different submodules are the *same* type, and must be equiv",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/Modules.rst:24041,cache,cache,24041,interpreter/llvm-project/clang/docs/Modules.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/Modules.rst,1,['cache'],['cache']
Performance,"se; vector code to be generated.; ``noinline``; This attribute indicates that the inliner should never inline this; function in any situation. This attribute may not be used together; with the ``alwaysinline`` attribute.; ``nomerge``; This attribute indicates that calls to this function should never be merged; during optimization. For example, it will prevent tail merging otherwise; identical code sequences that raise an exception or terminate the program.; Tail merging normally reduces the precision of source location information,; making stack traces less useful for debugging. This attribute gives the; user control over the tradeoff between code size and debug information; precision.; ``nonlazybind``; This attribute suppresses lazy symbol binding for the function. This; may make calls to the function faster, at the cost of extra program; startup time if the function is not called during program startup.; ``noprofile``; This function attribute prevents instrumentation based profiling, used for; coverage or profile based optimization, from being added to a function. It; also blocks inlining if the caller and callee have different values of this; attribute.; ``skipprofile``; This function attribute prevents instrumentation based profiling, used for; coverage or profile based optimization, from being added to a function. This; attribute does not restrict inlining, so instrumented instruction could end; up in this function.; ``noredzone``; This attribute indicates that the code generator should not use a; red zone, even if the target-specific ABI normally permits it.; ``indirect-tls-seg-refs``; This attribute indicates that the code generator should not use; direct TLS access through segment registers, even if the; target-specific ABI normally permits it.; ``noreturn``; This function attribute indicates that the function never returns; normally, hence through a return instruction. This produces undefined; behavior at runtime if the function ever does dynamically return",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:91707,optimiz,optimization,91707,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['optimiz'],['optimization']
Performance,"sed by pipeline resource pressure and data dependencies) to; dynamic dispatch stalls. Below is an example of ``-bottleneck-analysis`` output generated by; :program:`llvm-mca` for 500 iterations of the dot-product example on btver2. .. code-block:: none. Cycles with backend pressure increase [ 48.07% ]; Throughput Bottlenecks:; Resource Pressure [ 47.77% ]; - JFPA [ 47.77% ]; - JFPU0 [ 47.77% ]; Data Dependencies: [ 0.30% ]; - Register Dependencies [ 0.30% ]; - Memory Dependencies [ 0.00% ]. Critical sequence based on the simulation:. Instruction Dependency Information; +----< 2. vhaddps %xmm3, %xmm3, %xmm4; |; | < loop carried >; |; | 0. vmulps %xmm0, %xmm1, %xmm2; +----> 1. vhaddps %xmm2, %xmm2, %xmm3 ## RESOURCE interference: JFPA [ probability: 74% ]; +----> 2. vhaddps %xmm3, %xmm3, %xmm4 ## REGISTER dependency: %xmm3; |; | < loop carried >; |; +----> 1. vhaddps %xmm2, %xmm2, %xmm3 ## RESOURCE interference: JFPA [ probability: 74% ]. According to the analysis, throughput is limited by resource pressure and not by; data dependencies. The analysis observed increases in backend pressure during; 48.07% of the simulated run. Almost all those pressure increase events were; caused by contention on processor resources JFPA/JFPU0. The `critical sequence` is the most expensive sequence of instructions according; to the simulation. It is annotated to provide extra information about critical; register dependencies and resource interferences between instructions. Instructions from the critical sequence are expected to significantly impact; performance. By construction, the accuracy of this analysis is strongly; dependent on the simulation and (as always) by the quality of the processor; model in llvm. Bottleneck analysis is currently not supported for processors with an in-order; backend. Extra Statistics to Further Diagnose Performance Issues; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; The ``-all-stats`` command line option enables extra statistics and performan",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst:27709,throughput,throughput,27709,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,1,['throughput'],['throughput']
Performance,"sed, bit `kMatrixSet` must have been set. |; | | |; | `kDetermined` | `det` $A$ calculated, bit `kDecomposed` must have been set. |; | | |; | `kCondition` | ||*A*||1 ||*A*-1||1 is calculated bit `kDecomposed` must have been set. |; | | |; | `kSingular` | $A$ is singular |; +---------------+-------------------------------------------------------------------------+. The state is reset by assigning a new matrix through; `SetMatrix(TMatrixD &A)` for **`TDecompBK`** and **`TDecompChol`**; (actually `SetMatrix(`**`TMatrixDSym &A)`** and; `SetMatrix(`**`TMatrixDSparse`** `&A)` for **`TMatrixDSparse`**). As the code example below shows, the user does not have to worry about; the decomposition step before calling a solve method, because the; decomposition class checks before invoking `Solve` that the matrix has; been decomposed. ``` {.cpp}; TVectorD b = ..;; TMatrixD a = ..;; .; TDecompLU lu(a);; Bool_t ok;; lu.Solve(b,ok);; ```. In the next example, we show again the same decomposition but now; performed in a loop and all necessary steps are manually invoked. This; example also demonstrates another very important point concerning memory; management! Note that the vector, matrix and decomposition class are; constructed outside the loop since the dimensions of vector/matrix are; constant. If we would have replaced `lu.SetMatrix(a)` by **`TDecompLU`**; `lu(a)`, we would construct/deconstruct the array elements of `lu` on; the stack*.*. ``` {.cpp}; TVectorD b(n);; TMatrixD a(n,n);; TDecompLU lu(n);; Bool_t ok;; for (....) {; b = ..;; a = ..;; lu.SetMatrix(a);; lu.Decompose();; lu.Solve(b,ok);; }; ```. ### Tolerances and Scaling. The tolerance parameter `fTol` (a member of the base class; **`TDecompBase`**) plays a crucial role in all operations of the; decomposition classes. It gives the user a tool to monitor and steer the; operations its default value is $\varepsilon$ where $1+\varepsilon=1$. If you do not want to be bothered by the following considerations, like; in most othe",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/LinearAlgebra.md:38952,perform,performed,38952,documentation/users-guide/LinearAlgebra.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/LinearAlgebra.md,1,['perform'],['performed']
Performance,"sed: one which allows to explicitly list the types of the columns and another one allowing to let the system infer them (the same mechanism of the `Snapshot` method). Only columns containing instances of classes which have a copy constructor can be cached.; - Add `DefineSlot`, a `Define` transformation that is aware of the multi-threading slot where the workload is executed; - Add `DefineSlotEntry`, a `Define` transformation that is aware of the multi-threading slot and of the current entry number; - Add `GetColumnsNames`: users can now get the names of the available columns coming from trees, data sources or `Define`d columns; - Add `OnPartialResult` and `OnPartialResultSlot`: users can now register one or more functions to be executed on partial results of TDF actions during the event loop.; This mechanism is meant to be used to inspect partial results of the analysis or print useful debug information.; For example, both in single- and multi-thread event loops, one can draw a result histogram and update the canvas every 100 entries like this:; ```c++; auto h = tdf.Histo1D(""x"");; TCanvas c(""c"",""x hist"");; h.OnPartialResult(100, [&c](TH1D &h_) { c.cd(); h_.Draw(); c.Update(); });; ```; See the tutorials for more examples.; - Add `Sum`, an action that sums all values of a column for the processed entries; - The new TDataSource interface allows developers to pipe any kind of columnar data format into TDataFrame. Two example data sources have been provided: the TRootDS and the TTrivialDS. The former allows to read via the novel data source mechanism ROOT data, while the latter is a simple generator, created for testing and didactic purposes. It is therefore now possible to interface *any* kind of dataset/data format to ROOT as long as an adaptor which implements the pure virtual methods of the TDataSource interface can be written in C++.; - TDF can now read CSV files through a specialized TDataSource. Just create the TDF with `MakeCsvDataFrame(""f.csv"")`. Just create th",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v612/index.md:12032,multi-thread,multi-thread,12032,README/ReleaseNotes/v612/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v612/index.md,1,['multi-thread'],['multi-thread']
Performance,"seful in; combination with ``-DLLVM_ENABLE_DOXYGEN_QT_HELP=ON``; otherwise; it has no effect. **LLVM_DOXYGEN_SVG**:BOOL; Uses .svg files instead of .png files for graphs in the Doxygen output.; Defaults to OFF. .. _llvm_enable_assertions:. **LLVM_ENABLE_ASSERTIONS**:BOOL; Enables code assertions. Defaults to ON if and only if ``CMAKE_BUILD_TYPE``; is *Debug*. **LLVM_ENABLE_BINDINGS**:BOOL; If disabled, do not try to build the OCaml bindings. **LLVM_ENABLE_DIA_SDK**:BOOL; Enable building with MSVC DIA SDK for PDB debugging support. Available; only with MSVC. Defaults to ON. **LLVM_ENABLE_DOXYGEN**:BOOL; Enables the generation of browsable HTML documentation using doxygen.; Defaults to OFF. **LLVM_ENABLE_DOXYGEN_QT_HELP**:BOOL; Enables the generation of a Qt Compressed Help file. Defaults to OFF.; This affects the make target ``doxygen-llvm``. When enabled, apart from; the normal HTML output generated by doxygen, this will produce a QCH file; named ``org.llvm.qch``. You can then load this file into Qt Creator.; This option is only useful in combination with ``-DLLVM_ENABLE_DOXYGEN=ON``;; otherwise this has no effect. **LLVM_ENABLE_EH**:BOOL; Build LLVM with exception-handling support. This is necessary if you wish to; link against LLVM libraries and make use of C++ exceptions in your own code; that need to propagate through LLVM code. Defaults to OFF. **LLVM_ENABLE_EXPENSIVE_CHECKS**:BOOL; Enable additional time/memory expensive checking. Defaults to OFF. **LLVM_ENABLE_HTTPLIB**:BOOL; Enables the optional cpp-httplib dependency which is used by llvm-debuginfod; to serve debug info over HTTP. `cpp-httplib <https://github.com/yhirose/cpp-httplib>`_; must be installed, or `httplib_ROOT` must be set. Defaults to OFF. **LLVM_ENABLE_FFI**:BOOL; Indicates whether the LLVM Interpreter will be linked with the Foreign Function; Interface library (libffi) in order to enable calling external functions.; If the library or its headers are installed in a custom; location, you can als",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CMake.rst:20932,load,load,20932,interpreter/llvm-project/llvm/docs/CMake.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CMake.rst,1,['load'],['load']
Performance,"select <4 x i1> %mask, <4 x i1> %t, <4 x i1> poison. .. _int_vp_ceil:. '``llvm.vp.ceil.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x float> @llvm.vp.ceil.v16f32 (<16 x float> <op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x float> @llvm.vp.ceil.nxv4f32 (<vscale x 4 x float> <op>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x double> @llvm.vp.ceil.v256f64 (<256 x double> <op>, <256 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". Predicated floating-point ceiling of a vector of floating-point values. Arguments:; """""""""""""""""""". The first operand and the result have the same vector of floating-point type.; The second operand is the vector mask and has the same number of elements as the; result vector type. The third operand is the explicit vector length of the; operation. Semantics:; """""""""""""""""""". The '``llvm.vp.ceil``' intrinsic performs floating-point ceiling; (:ref:`ceil <int_ceil>`) of the first vector operand on each enabled lane. The; result on disabled lanes is a :ref:`poison value <poisonvalues>`. Examples:; """""""""""""""""". .. code-block:: llvm. %r = call <4 x float> @llvm.vp.ceil.v4f32(<4 x float> %a, <4 x i1> %mask, i32 %evl); ;; For all lanes below %evl, %r is lane-wise equivalent to %also.r. %t = call <4 x float> @llvm.ceil.v4f32(<4 x float> %a); %also.r = select <4 x i1> %mask, <4 x float> %t, <4 x float> poison. .. _int_vp_floor:. '``llvm.vp.floor.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x float> @llvm.vp.floor.v16f32 (<16 x float> <op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x float> @llvm.vp.floor.nxv4f32 (<vscale x 4 x float> <op>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x double> @llvm.vp.floor.v256f64 (<256 x double> <op>, <256 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". Predicated floating-point floor of a ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:822188,perform,performs,822188,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performs']
Performance,"selector call:. ``` {.cpp}; root[] T->MakeSelector(""MySelector"");; ```. Where `T` is the **`TTree`** and `MySelector` is the name of created; class and the name of the `.h` and `.C` files. The resulting; **`TSelector`** is the argument to **`TTree::Process`**. The argument can; be the file name or a pointer to the selector object. ``` {.cpp}; root[] T->Process(""MySelector.C"","""",1000,100);; ```. This call will interpret the class defined in `MySelector.C` and process; 1000 entries beginning with entry 100. The file name can be appended; with a ""+"" or a ""++"" to use `ACLiC`. ``` {.cpp}; root[] T->Process(""MySelector.C++"","""",1000,100);; ```. When appending a ""++"", the class will be compiled and dynamically; loaded. ``` {.cpp}; root[] T->Process(""MySelector.C+"","""",1000,100);; ```. When appending a ""+"", the class will also be compiled and dynamically; loaded. When it is called again, it recompiles only if the macro; (`MySelector.C`) has changed since it was compiled last. If not, it; loads the existing library. The next example shows how to create a; selector with a pointer:. ``` {.cpp}; MySelector *selector = (MySelector *)TSelector::GetSelector(""MySelector.C+"");; T->Process(selector);; ```. `Using this form, you can do things like:`. ``` {.cpp}; selector->public_attribute1 = init_value;; for (int i=0; i<limit; i++) {; T->Process(selector);; selector->public_attribute1 =; function(selector->public_attribute2);; }; ```. `TTree::Process()` is aware of PROOF, ROOT parallel processing facility.; If PROOF is setup, it divides the processing amongst the slave CPUs. ### Performance Benchmarks; \index{benchmarks}. The program `$ROOTSYS/test/bench.cxx` compares the I/O performance of; STL vectors to the ROOT native **`TClonesArray`**`s` collection class.; It creates trees with and without compression for the following cases:; `vector<THit>`, `vector<THit*>`, `TClonesArray(`**`TObjHit`**`)`; not split `TClonesArray(`**`TObjHit`**`)` split. The next graphs show the two columns on t",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Trees.md:134511,load,loads,134511,documentation/users-guide/Trees.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Trees.md,1,['load'],['loads']
Performance,"ser is for parsing enum; values, which allows you to use the CommandLine library for all of the error; checking to make sure that only valid enum values are specified (as opposed to; accepting arbitrary strings). Despite this, however, the generic parser class; can be used for any data type. .. _boolean flags:; .. _bool parser:. * The **parser<bool> specialization** is used to convert boolean strings to a; boolean value. Currently accepted strings are ""``true``"", ""``TRUE``"",; ""``True``"", ""``1``"", ""``false``"", ""``FALSE``"", ""``False``"", and ""``0``"". * The **parser<boolOrDefault> specialization** is used for cases where the value; is boolean, but we also need to know whether the option was specified at all.; boolOrDefault is an enum with 3 values, BOU_UNSET, BOU_TRUE and BOU_FALSE.; This parser accepts the same strings as **``parser<bool>``**. .. _strings:. * The **parser<string> specialization** simply stores the parsed string into the; string value specified. No conversion or modification of the data is; performed. .. _integers:; .. _int:. * The **parser<int> specialization** uses the C ``strtol`` function to parse the; string input. As such, it will accept a decimal number (with an optional '+'; or '-' prefix) which must start with a non-zero digit. It accepts octal; numbers, which are identified with a '``0``' prefix digit, and hexadecimal; numbers with a prefix of '``0x``' or '``0X``'. .. _doubles:; .. _float:; .. _double:. * The **parser<double>** and **parser<float> specializations** use the standard; C ``strtod`` function to convert floating point strings into floating point; values. As such, a broad range of string formats is supported, including; exponential notation (ex: ``1.7e15``) and properly supports locales. .. _Extension Guide:; .. _extending the library:. Extension Guide; ===============. Although the CommandLine library has a lot of functionality built into it; already (as discussed previously), one of its true strengths lie in its; extensibility. Thi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandLine.rst:60613,perform,performed,60613,interpreter/llvm-project/llvm/docs/CommandLine.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandLine.rst,1,['perform'],['performed']
Performance,"ser-specific as opposed to a system-wide directory.).; 4. Otherwise, take the token from `/tmp/bt_u$ID`. ## GUI Libraries. ### RBrowser improvements. - central factory methods to handle browsing, editing and drawing of different classes; - simple possibility to extend RBrowser on user-defined classes; - support of web-based geometry viewer; - better support of TTree drawing; - server-side handling of code editor and image viewer widgets; - rbrowser content is fully recovered when web-browser is reloaded; - load of widgets code only when really required (shorter startup time for RBrowser). ## Montecarlo Libraries. ## PROOF Libraries. ## Language Bindings. ## JavaScript ROOT. ### Major JSROOT update to version 6. - update all used libraries `d3.js`, `three.js`, `MathJax.js`, openui5; - change to Promise based interface for all async methods, remove call-back arguments; - change scripts names, core scripts name now `JSRoot.core.js`; - unify function/methods naming conventions, many changes in method names; - provide central code loader via `JSROOT.require`, supporting 4 different loading engines; - many nice features and many bug fixes; see JSROOT v6 release notes. ## Tutorials. ## Class Reference Guide. ## Build, Configuration and Testing Infrastructure. - a new cmake variable, `CMAKE_INSTALL_PYTHONDIR`, has been added: it allows customization of the installation directory of ROOT's python modules; - the developer build option `asserts` is introduced to enable/disable asserts via the `NDEBUG` C/CXX flag. Asserts are always enabled for `CMAKE_BUILD_TYPE=Debug` and `dev=ON`. The previous behavior of the builds set via the `CMAKE_BUILD_TYPE` variable has not changed.; - `CMAKE_CXX_STANDARD`, i.e. the C++ standard ROOT is built with, now defaults to the compiler default (or C++11 if the compiler default is older than that) rather than always defaulting to C++11. In turn this means that v6.24 is the first ROOT release for which ROOT's pre-compiled binaries are not compiled",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v624/index.md:27712,load,loader,27712,README/ReleaseNotes/v624/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v624/index.md,1,['load'],['loader']
Performance,"ses"" in order to print a; human readable version of the analysis results. This is useful for debugging; an analysis itself, as well as for other people to figure out how an analysis; works. Use the opt ``-analyze`` argument to invoke this method. The ``llvm::raw_ostream`` parameter specifies the stream to write the results; on, and the ``Module`` parameter gives a pointer to the top level module of the; program that has been analyzed. Note however that this pointer may be ``NULL``; in certain circumstances (such as calling the ``Pass::dump()`` from a; debugger), so it should only be used to enhance debug output, it should not be; depended on. .. _writing-an-llvm-pass-interaction:. Specifying interactions between passes; --------------------------------------. One of the main responsibilities of the ``PassManager`` is to make sure that; passes interact with each other correctly. Because ``PassManager`` tries to; :ref:`optimize the execution of passes <writing-an-llvm-pass-passmanager>` it; must know how the passes interact with each other and what dependencies exist; between the various passes. To track this, each pass can declare the set of; passes that are required to be executed before the current pass, and the passes; which are invalidated by the current pass. Typically this functionality is used to require that analysis results are; computed before your pass is run. Running arbitrary transformation passes can; invalidate the computed analysis results, which is what the invalidation set; specifies. If a pass does not implement the :ref:`getAnalysisUsage; <writing-an-llvm-pass-getAnalysisUsage>` method, it defaults to not having any; prerequisite passes, and invalidating **all** other passes. .. _writing-an-llvm-pass-getAnalysisUsage:. The ``getAnalysisUsage`` method; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. .. code-block:: c++. virtual void getAnalysisUsage(AnalysisUsage &Info) const;. By implementing the ``getAnalysisUsage`` method, the required and invalidated; sets ma",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst:28657,optimiz,optimize,28657,interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst,1,['optimiz'],['optimize']
Performance,"ses=1]; br i1 %cond1, label %exit, label %bb.nph; bb.nph: ; preds = %entry; %tmp = mul i32 %b, %a ; <i32> [#uses=1]; ret i32 %tmp; exit: ; preds = %entry; ret i32 0; }. could be reduced to:. define i32 @mul(i32 %a, i32 %b) nounwind readnone {; entry:; %tmp = mul i32 %b, %a; ret i32 %tmp; }. //===---------------------------------------------------------------------===//. We should use DSE + llvm.lifetime.end to delete dead vtable pointer updates.; See GCC PR34949. Another interesting case is that something related could be used for variables; that go const after their ctor has finished. In these cases, globalopt (which; can statically run the constructor) could mark the global const (so it gets put; in the readonly section). A testcase would be:. #include <complex>; using namespace std;; const complex<char> should_be_in_rodata (42,-42);; complex<char> should_be_in_data (42,-42);; complex<char> should_be_in_bss;. Where we currently evaluate the ctors but the globals don't become const because; the optimizer doesn't know they ""become const"" after the ctor is done. See; GCC PR4131 for more examples. //===---------------------------------------------------------------------===//. In this code:. long foo(long x) {; return x > 1 ? x : 1;; }. LLVM emits a comparison with 1 instead of 0. 0 would be equivalent; and cheaper on most targets. LLVM prefers comparisons with zero over non-zero in general, but in this; case it choses instead to keep the max operation obvious. //===---------------------------------------------------------------------===//. define void @a(i32 %x) nounwind {; entry:; switch i32 %x, label %if.end [; i32 0, label %if.then; i32 1, label %if.then; i32 2, label %if.then; i32 3, label %if.then; i32 5, label %if.then; ]; if.then:; tail call void @foo() nounwind; ret void; if.end:; ret void; }; declare void @foo(). Generated code on x86-64 (other platforms give similar results):; a:; 	cmpl	$5, %edi; 	ja	LBB2_2; 	cmpl	$4, %edi; 	jne	LBB2_3; .LBB0_2:; 	ret; .LBB0",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:51360,optimiz,optimizer,51360,interpreter/llvm-project/llvm/lib/Target/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt,2,['optimiz'],['optimizer']
Performance,"sessions running; concurrently on the cluster (started by the same daemon); this plot is; present only is the number is at least onec different from 1. If enabled, send monitoring information from the master; at each GetNextPacket (at each call of TPerfStat::PacketEvent) to allow; extrnal real-time progress monitoring.; Save the status of a 'proofserv' session into a new file; in the 'activesessions' area. The full path of the new file is;          ; <admin_path>/.xproofd.<port>/activesessions/<user>.<group>.<pid>.status. The status indicates whether the session is idle, running or queued.; The status is updated every 'checkfq' secs (see xpd.proofservmgr;; default 30 s). The status is dumped by the reader thread of TXProofServ; and therefore its r/w access is protected. Enable the use of the tree cache also for local files,; adapting the default settings for the cache to the recent changes; In the XrdProofd plug-in. Improve synchronization between parent and child during; fork; Optimize loops over directory entries; Improve error and notification messages. Improved handling of Ctrl-C; this follows from a fix in; TMonitor and an improved handling of non-finished query state in the; workers (results are not send to master if the query was aborted) . Fixes. TFileMerger. Fix a problem preventing correct transmission of all; non-mergeable objects (fixes bug #52886); Remove the argument isdir from the function; MergeRecursive; Do not remove the first file in the list when returning; from MergeRecursive (fixes bug #54591); Fix a major leak when merging files with collections; written using kSingleKey option.  The merger was reading each; key in memory and deleted the object at the end, but the container is; not owner by default, so all objects inside leaked. PROOF-Lite. Fix a couple of memory leaks showing up when running; repeated queries; Fix a problem in TProofServ::CopyFromCache affecting; the case where the sandbox dir has a '.' and the macro name has no '.',; e.g. com",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/proof/doc/v526/index.html:6199,Optimiz,Optimize,6199,proof/doc/v526/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/proof/doc/v526/index.html,1,['Optimiz'],['Optimize']
Performance,"set(LLVM_BLAKE3_FILES; blake3.c; blake3_dispatch.c; blake3_portable.c; blake3_neon.c; ). if (LLVM_DISABLE_ASSEMBLY_FILES); set(CAN_USE_ASSEMBLER FALSE); else(); set(CAN_USE_ASSEMBLER TRUE); endif(). macro(disable_blake3_x86_simd); add_compile_definitions(BLAKE3_NO_AVX512 BLAKE3_NO_AVX2 BLAKE3_NO_SSE41 BLAKE3_NO_SSE2); endmacro(). # The BLAKE3 team recommends using the assembly versions, from the README:; #; # ""For each of the x86 SIMD instruction sets, four versions are available:; # three flavors of assembly (Unix, Windows MSVC, and Windows GNU) and one; # version using C intrinsics. The assembly versions are generally; # preferred. They perform better, they perform more consistently across; # different compilers, and they build more quickly."". if (CAN_USE_ASSEMBLER); if (MSVC); check_symbol_exists(_M_X64 """" IS_X64); if (IS_X64); enable_language(ASM_MASM); list(APPEND LLVM_BLAKE3_FILES; blake3_sse2_x86-64_windows_msvc.asm; blake3_sse41_x86-64_windows_msvc.asm; blake3_avx2_x86-64_windows_msvc.asm; blake3_avx512_x86-64_windows_msvc.asm; ); else(); disable_blake3_x86_simd(); endif(); elseif(WIN32 OR CYGWIN); check_symbol_exists(__x86_64__ """" IS_X64); if (IS_X64); list(APPEND LLVM_BLAKE3_FILES; blake3_sse2_x86-64_windows_gnu.S; blake3_sse41_x86-64_windows_gnu.S; blake3_avx2_x86-64_windows_gnu.S; blake3_avx512_x86-64_windows_gnu.S; ); # Clang before 7 needs -mavx512vl to assemble some instructions.; set_source_files_properties(blake3_avx512_x86-64_windows_gnu.S; PROPERTIES COMPILE_OPTIONS ""-mavx512vl""); else(); disable_blake3_x86_simd(); endif(); else(); check_symbol_exists(__x86_64__ """" IS_X64); if (IS_X64 OR CMAKE_OSX_ARCHITECTURES MATCHES ""x86_64""); # In a macOS Universal build (setting CMAKE_OSX_ARCHITECTURES to multiple; # values), compilation of the source files will target multiple architectures; # (each source file is internally compiled once for each architecture).; # To accomodate this configuration we include these assembly files without a; # CMake check but t",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/BLAKE3/CMakeLists.txt:647,perform,perform,647,interpreter/llvm-project/llvm/lib/Support/BLAKE3/CMakeLists.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/BLAKE3/CMakeLists.txt,4,['perform'],['perform']
Performance,"set(LLVM_LINK_COMPONENTS; AggressiveInstCombine; Analysis; BitReader; BitWriter; CodeGenTypes; Core; Coroutines; Coverage; Demangle; Extensions; FrontendDriver; FrontendHLSL; FrontendOpenMP; FrontendOffloading; HIPStdPar; IPO; IRPrinter; IRReader; InstCombine; Instrumentation; LTO; Linker; MC; ObjCARCOpts; Object; Passes; ProfileData; ScalarOpts; Support; Target; TargetParser; TransformUtils; ). # Workaround for MSVC ARM64 performance regression:; # https://developercommunity.visualstudio.com/t/Compiling-a-specific-code-for-ARM64-with/10444970; # Since /O1 and /O2 represent a set of optimizations,; # our goal is to disable the /Og flag while retaining the other optimizations from the /O1|/O2 set; if(MSVC AND NOT CMAKE_CXX_COMPILER_ID MATCHES Clang; AND MSVC_VERSION VERSION_GREATER_EQUAL 1932; AND CMAKE_SYSTEM_PROCESSOR MATCHES ""ARM64""). string(TOUPPER ""${CMAKE_BUILD_TYPE}"" uppercase_CMAKE_BUILD_TYPE); string(REGEX MATCHALL ""/[Oo][12]"" opt_flags ""${CMAKE_CXX_FLAGS} ${CMAKE_CXX_FLAGS_${uppercase_CMAKE_BUILD_TYPE}}""); if (opt_flags); if(opt_flags MATCHES ""1$""); set(opt_flags ""/Od;/Os;/Oy;/Ob2;/GF;/Gy""); elseif (opt_flags MATCHES ""2$""); set(opt_flags ""/Od;/Oi;/Ot;/Oy;/Ob2;/GF;/Gy""); endif(); set_source_files_properties(CGBuiltin.cpp PROPERTIES COMPILE_OPTIONS ""${opt_flags}""); endif(); endif(). add_clang_library(clangCodeGen; ABIInfo.cpp; ABIInfoImpl.cpp; BackendUtil.cpp; CGAtomic.cpp; CGBlocks.cpp; CGBuiltin.cpp; CGCUDANV.cpp; CGCUDARuntime.cpp; CGCXX.cpp; CGCXXABI.cpp; CGCall.cpp; CGClass.cpp; CGCleanup.cpp; CGCoroutine.cpp; CGDebugInfo.cpp; CGDecl.cpp; CGDeclCXX.cpp; CGException.cpp; CGExpr.cpp; CGExprAgg.cpp; CGExprCXX.cpp; CGExprComplex.cpp; CGExprConstant.cpp; CGExprScalar.cpp; CGGPUBuiltin.cpp; CGHLSLRuntime.cpp; CGLoopInfo.cpp; CGNonTrivialStruct.cpp; CGObjC.cpp; CGObjCGNU.cpp; CGObjCMac.cpp; CGObjCRuntime.cpp; CGOpenCLRuntime.cpp; CGOpenMPRuntime.cpp; CGOpenMPRuntimeGPU.cpp; CGRecordLayoutBuilder.cpp; CGStmt.cpp; CGStmtOpenMP.cpp; CGVTT.cpp; CGVTables.cpp; CodeGe",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CMakeLists.txt:427,perform,performance,427,interpreter/llvm-project/clang/lib/CodeGen/CMakeLists.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CMakeLists.txt,3,"['optimiz', 'perform']","['optimizations', 'performance']"
Performance,set(LLVM_LINK_COMPONENTS; AllTargetsAsmParsers; AllTargetsCodeGens; AllTargetsDescs; AllTargetsInfos; Analysis; BitWriter; CodeGen; Extensions; Core; IPO; IRReader; AggressiveInstCombine; InstCombine; Instrumentation; Linker; ObjCARCOpts; ScalarOpts; Support; Target; TargetParser; TransformUtils; Vectorize; ). add_llvm_tool(bugpoint; BugDriver.cpp; CrashDebugger.cpp; ExecutionDriver.cpp; ExtractFunction.cpp; FindBugs.cpp; Miscompilation.cpp; OptimizerDriver.cpp; ToolRunner.cpp; bugpoint.cpp. DEPENDS; intrinsics_gen; SUPPORT_PLUGINS; ); export_executable_symbols_for_plugins(bugpoint); ,MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/bugpoint/CMakeLists.txt:446,Optimiz,OptimizerDriver,446,interpreter/llvm-project/llvm/tools/bugpoint/CMakeLists.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/bugpoint/CMakeLists.txt,1,['Optimiz'],['OptimizerDriver']
Performance,"setting.; Offsetting has instead been implemented in the calculators that we'll describe next.; This is one of the consequences of the conceptual splitting of functionality into statistics and calculator classes.; Offsetting is a feature of calculation in a fitting context; it enhances numerical precision by subtracting the initial likelihood value from the value that the minimizer sees, thus setting it to zero for the minimizer.; Since this does not impact the derivative terms, it does not affect the fitting result, except for added numerical precision. ## Calculators; `RooFit::TestStatistics` provides two abstract base classes for likelihood calculation: `LikelihoodWrapper` and `LikelihoodGradientWrapper`.; These are used by the `RooAbsMinimizerFcn` implementation `MinuitFcnGrad` which expects them to, respectively, provide likelihood and likelihood gradient values for use by `Minuit2` in fitting the pdf to the dataset. The `Wrapper`s can be implemented for different kinds of algorithms, or with different kinds of optimization ""back-ends"" in mind.; One implementation of each is ready for use in `RooFit` currently:. 1. `LikelihoodSerial` is more or less simply a rewrite of the existing serial calculation of a `RooNLLVar`.; 2. `LikelihoodGradientJob` calculates the partial derivatives or the gradient in parallel on multiple CPUs/cores, based on `RooFit::MultiProcess`, which is a fork-based multi-processing task execution framework with dynamic load balancing. Other possible implementations could use the GPU or external tools like TensorFlow. The coupling of all these classes to `RooMinimizer` is made via the `MinuitFcnGrad` class, which owns the `Wrappers` that calculate the likelihood components. Note: a second `LikelihoodWrapper` class called `LikelihoodJob` is also available.; This class emulates the existing `NumCPU(>1)` functionality of the `RooAbsTestStatistic` tree, which is implemented based on `RooRealMPFE`.; This class is not yet thoroughly tested and shou",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/developers/test_statistics.md:6982,optimiz,optimization,6982,roofit/doc/developers/test_statistics.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/developers/test_statistics.md,1,['optimiz'],['optimization']
Performance,"sform No None − List of variable transformations performed before training, e.g., D_Background,P_Signal,G,N_AllClasses for: Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed). H No False − Print method-specific help message. CreateMVAPdfs No False − Create PDFs for classifier outputs (signal and background). IgnoreNegWeightsInTraining No False − Events with negative weights are ignored in the training (but are included for testing and performance evaluation). Configuration options for MVA method :. Configuration options reference for MVA method: Fisher. Option Array Default value Predefined values Description. V No False − Verbose output (short form of VerbosityLevel below - overrides the latter one). VerbosityLevel No Default Default, Debug, Verbose, Info, Warning, Error, Fatal Verbosity level. VarTransform No None − List of variable transformations performed before training, e.g., D_Background,P_Signal,G,N_AllClasses for: Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed). H No False − Print method-specific help message. CreateMVAPdfs No False − Create PDFs for classifier outputs (signal and background). IgnoreNegWeightsInTraining No False − Events with negative weights are ignored in the training (but are included for testing and performance evaluation). Method No Fisher Fisher, Mahalanobis Discrimination method. Configuration options for MVA method :. Configuration options reference for MVA method: PDERS. Option Array Default value Predefined values Description. V No False − Verbose output (short form of VerbosityLevel below - overrides the latter one). VerbosityLevel No Default Default, Debug, Verbose, Info, Warning, Error, Fatal Verbosity level. VarTransform No No",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/tmva/UsersGuide/optionRef.html:2777,perform,performed,2777,documentation/tmva/UsersGuide/optionRef.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/tmva/UsersGuide/optionRef.html,2,['perform'],['performed']
Performance,"sh all branch buffers to disk. CASE 2 : autof < 0. When filling the Tree the branch buffers will be flushed to disk when; more than autof bytes have been written to the file. At the first FlushBaskets; TTree::Fill will replace fAutoFlush by the current value of fEntries. Calling this function with autof < 0 is interesting when it is hard to estimate; the size of one entry. This value is also independent of the Tree. When calling SetAutoFlush with no arguments, the; default value is -30000000, ie that the first AutoFlush will be done when; 30 MBytes of data are written to the file. CASE 3 : autof = 0; The AutoFlush mechanism is disabled. Flushing the buffers at regular intervals optimize the location of; consecutive entries on the disk. Changed the default value of AutoSave from 10 to 30 MBytes. New class TTreePerfStats; This new class is an important tool to measure the I/O performance of a Tree.; It shows the locations in the file when reading a Tree. In particular it is easy; to see the performance of the Tree Cache. The results can be:. drawn in a canvas.; printed on standard output.; saved to a file for processing later. Example of use; {; TFile *f = TFile::Open(""RelValMinBias-GEN-SIM-RECO.root"");; T = (TTree*)f->Get(""Events"");; Long64_t nentries = T->GetEntries();; T->SetCacheSize(10000000);; T->AddBranchToCache(""*"");. TTreePerfStats *ps= new TTreePerfStats(""ioperf"",T);. for (Int_t i=0;i<nentries;i++) {; T->GetEntry(i);; }; ps->SaveAs(""atlas_perf.root"");; }. then, in a root interactive session, one can do:. root > TFile f(""atlas_perf.root"");; root > ioperf->Draw();; root > ioperf->Print();. The Draw or Print functions print the following information:. TreeCache = TTree cache size in MBytes; N leaves = Number of leaves in the TTree; ReadTotal = Total number of zipped bytes read; ReadUnZip = Total number of unzipped bytes read; ReadCalls = Total number of disk reads; ReadSize = Average read size in KBytes; Readahead = Readahead size in KBytes; Readextra = Readahe",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/doc/v526/index.html:7368,perform,performance,7368,tree/doc/v526/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/doc/v526/index.html,3,"['Cache', 'perform']","['Cache', 'performance']"
Performance,"shared by all MVA methods (through common base class). Bluish rows:; Specific MVA options. Yellowish rows:; Configuration options for minimiser (fitter) classes. Redish rows:; Options for other configurable classes. Available MVA methods (1st row), minimisation tools (2nd row), and other configurables (3rd row):. [MVA::HMatrix] [MVA::Fisher] [MVA::PDERS] [MVA::FDA] [MVA::LD] [MVA::SVM] [MVA::CFMlpANN] [MVA::KNN] [MVA::BDT] [MVA::Boost] [MVA::RuleFit] [MVA::Likelihood] [MVA::MLP] [MVA::Cuts] [MVA::PDEFoam] [MVA::TMlpANN]. [Fitter_SA] [Fitter_MC] [Fitter_Minuit] [Fitter_GA]. [DataSetFactory] [PDF] [Factory]. Configuration options for MVA method :. Configuration options reference for MVA method: HMatrix. Option Array Default value Predefined values Description. V No False − Verbose output (short form of VerbosityLevel below - overrides the latter one). VerbosityLevel No Default Default, Debug, Verbose, Info, Warning, Error, Fatal Verbosity level. VarTransform No None − List of variable transformations performed before training, e.g., D_Background,P_Signal,G,N_AllClasses for: Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed). H No False − Print method-specific help message. CreateMVAPdfs No False − Create PDFs for classifier outputs (signal and background). IgnoreNegWeightsInTraining No False − Events with negative weights are ignored in the training (but are included for testing and performance evaluation). Configuration options for MVA method :. Configuration options reference for MVA method: Fisher. Option Array Default value Predefined values Description. V No False − Verbose output (short form of VerbosityLevel below - overrides the latter one). VerbosityLevel No Default Default, Debug, Verbose, Info, Warning, Error, Fatal Verbosity level. VarTransform No None − List of variable transformations performed before trai",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/tmva/UsersGuide/optionRef.html:1812,perform,performed,1812,documentation/tmva/UsersGuide/optionRef.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/tmva/UsersGuide/optionRef.html,2,['perform'],['performed']
Performance,"shared with llvm-pdbutil that was moved; to the PDB library: https://reviews.llvm.org/D122226. //===----------------------------------------------------------------------===//; // Move 'getSymbolKindName'/'formatRegisterId' to the CodeView Library.; //===----------------------------------------------------------------------===//; There is some code in the CodeView reader that was extracted/adapted; from 'lib/DebugInfo/CodeView/SymbolDumper.cpp' that can be used. //===----------------------------------------------------------------------===//; // Use of std::unordered_set instead of std::set.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125784#inline-1221421. Replace the std::set usage for DeducedScopes, UnresolvedScopes and; IdentifiedNamespaces with std::unordered_set and get the benefit; of the O(1) while inserting/searching, as the order is not important. //===----------------------------------------------------------------------===//; // Optimize 'LVNamespaceDeduction::find' funtion.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125784#inline-1296195. Optimize the 'find' method to use the proposed code:. LVStringRefs::iterator Iter = std::find_if(Components.begin(), Components.end(),; [](StringRef Name) {; return IdentifiedNamespaces.find(Name) == IdentifiedNamespaces.end();; });; LVStringRefs::size_type FirstNonNamespace = std::distance(Components.begin(), Iter);. //===----------------------------------------------------------------------===//; // Move all the printing support to a common module.; //===----------------------------------------------------------------------===//; Factor out printing functionality from the logical elements into a; common module. //===----------------------------------------------------------------------===//; // Refactor 'LVBinaryReader::processLines'.; //===------------------------------------------------",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt:6740,Optimiz,Optimize,6740,interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt,1,['Optimiz'],['Optimize']
Performance,"shelf *ORC layers*: IRCompileLayer and; ObjectLinkingLayer, to do much of the heavy lifting. In this layer we'll learn more about the ORC layer concept by using a new layer,; IRTransformLayer, to add IR optimization support to KaleidoscopeJIT. Optimizing Modules using the IRTransformLayer; =============================================. In `Chapter 4 <LangImpl04.html>`_ of the ""Implementing a language with LLVM""; tutorial series the llvm *FunctionPassManager* is introduced as a means for; optimizing LLVM IR. Interested readers may read that chapter for details, but; in short: to optimize a Module we create an llvm::FunctionPassManager; instance, configure it with a set of optimizations, then run the PassManager on; a Module to mutate it into a (hopefully) more optimized but semantically; equivalent form. In the original tutorial series the FunctionPassManager was; created outside the KaleidoscopeJIT and modules were optimized before being; added to it. In this Chapter we will make optimization a phase of our JIT; instead. For now this will provide us a motivation to learn more about ORC; layers, but in the long term making optimization part of our JIT will yield an; important benefit: When we begin lazily compiling code (i.e. deferring; compilation of each function until the first time it's run) having; optimization managed by our JIT will allow us to optimize lazily too, rather; than having to do all our optimization up-front. To add optimization support to our JIT we will take the KaleidoscopeJIT from; Chapter 1 and compose an ORC *IRTransformLayer* on top. We will look at how the; IRTransformLayer works in more detail below, but the interface is simple: the; constructor for this layer takes a reference to the execution session and the; layer below (as all layers do) plus an *IR optimization function* that it will; apply to each Module that is added via addModule:. .. code-block:: c++. class KaleidoscopeJIT {; private:; ExecutionSession ES;; RTDyldObjectLinkingLayer",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:2024,optimiz,optimization,2024,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,1,['optimiz'],['optimization']
Performance,"shes in the LLVM LTO phase when using; the ``-flto`` option, follow these steps to diagnose and report the issue:. Compile your source file to a ``.bc`` (Bitcode) file with the following options,; in addition to your existing compilation options:. .. code-block:: bash. export CFLAGS=""-flto -fuse-ld=lld"" CXXFLAGS=""-flto -fuse-ld=lld"" LDFLAGS=""-Wl,-plugin-opt=save-temps"". These options enable LTO and save temporary files generated during compilation; for later analysis. On Windows, you should be using lld-link as the linker. Adjust your compilation ; flags as follows:; * Add ``/lldsavetemps`` to the linker flags.; * When linking from the compiler driver, add ``/link /lldsavetemps`` in order to forward that flag to the linker. Using the specified flags will generate four intermediate bytecode files:. #. a.out.0.0.preopt.bc (Before any link-time optimizations (LTO) are applied); #. a.out.0.2.internalize.bc (After initial optimizations are applied); #. a.out.0.4.opt.bc (After an extensive set of optimizations); #. a.out.0.5.precodegen.bc (After LTO but before translating into machine code). Execute one of the following commands to identify the source of the problem:. #. ``opt ""-passes=lto<O3>"" a.out.0.2.internalize.bc``; #. ``llc a.out.0.5.precodegen.bc``. If one of these do crash, you should be able to reduce; this with :program:`llvm-reduce`; command line (use the bc file corresponding to the command above that failed):. .. code-block:: bash. llvm-reduce --test reduce.sh a.out.0.2.internalize.bc. Example of reduce.sh script. .. code-block:: bash. $ cat reduce.sh; #!/bin/bash -e. path/to/not --crash path/to/opt ""-passes=lto<O3>"" $1 -o temp.bc 2> err.log; grep -q ""It->second == &Insn"" err.log. Here we have grepped the failed assert message. Please run this, then file a bug with the instructions and reduced .bc file; that llvm-reduce emits. .. _miscompiling:. Miscompilations; ===============. If clang successfully produces an executable, but that executable doesn't run; r",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToSubmitABug.rst:7063,optimiz,optimizations,7063,interpreter/llvm-project/llvm/docs/HowToSubmitABug.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToSubmitABug.rst,1,['optimiz'],['optimizations']
Performance,"significant bit. A reader can read an 8 bit value and apply the mask; ``0x01`` for the discriminant. Similarly, they can read 32 bits and unsigned; shift right by ``0x04`` to obtain the function_id field. On big-endian machine, the bitfields are written in order from most significant; bit to least significant bit. A reader would read an 8 bit value and unsigned; shift right by 7 bits for the discriminant. The function_id field could be; obtained by reading a 32 bit value and applying the mask ``0x0FFFFFFF``. Function action types are as follows. +---------------+--------------+-----------------------------------------------+; | Type | Number | Description |; +===============+==============+===============================================+; | Entry | ``0`` | Typical function entry. |; +---------------+--------------+-----------------------------------------------+; | Exit | ``1`` | Typical function exit. |; +---------------+--------------+-----------------------------------------------+; | Tail_Exit | ``2`` | An exit from a function due to tail call |; | | | optimization. |; +---------------+--------------+-----------------------------------------------+; | Entry_Args | ``3`` | A function entry that records arguments. |; +---------------+--------------+-----------------------------------------------+. Entry_Args records do not contain the arguments themselves. Instead, metadata; records for each of the logged args follow the function record in the stream. Metadata Records; ----------------. Interspersed throughout the buffer are 16 byte Metadata records. For typically; instrumented binaries, they will be sparser than Function records, and they; provide a fuller picture of the binary execution state. Metadata record layout is partially record dependent, but they share a common; structure. The same bit field rules described for function records apply to the first byte; of MetadataRecords. Within this byte, little endian machines use lsb to msb; ordering and big endian ma",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/XRayFDRFormat.rst:7575,optimiz,optimization,7575,interpreter/llvm-project/llvm/docs/XRayFDRFormat.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/XRayFDRFormat.rst,1,['optimiz'],['optimization']
Performance,"sing LLVM, relative to the *CMAKE_INSTALL_PREFIX*.; Only matters if *LLVM_BUILD_EXAMPLES* is enabled.; Defaults to ""examples"". CMake Caches; ============. Recently LLVM and Clang have been adding some more complicated build system; features. Utilizing these new features often involves a complicated chain of; CMake variables passed on the command line. Clang provides a collection of CMake; cache scripts to make these features more approachable. CMake cache files are utilized using CMake's -C flag:. .. code-block:: console. $ cmake -C <path to cache file> <path to sources>. CMake cache scripts are processed in an isolated scope, only cached variables; remain set when the main configuration runs. CMake cached variables do not reset; variables that are already set unless the FORCE option is specified. A few notes about CMake Caches:. - Order of command line arguments is important. - -D arguments specified before -C are set before the cache is processed and; can be read inside the cache file; - -D arguments specified after -C are set after the cache is processed and; are unset inside the cache file. - All -D arguments will override cache file settings; - CMAKE_TOOLCHAIN_FILE is evaluated after both the cache file and the command; line arguments; - It is recommended that all -D options should be specified *before* -C. For more information about some of the advanced build configurations supported; via Cache files see :doc:`AdvancedBuilds`. Executing the Tests; ===================. Testing is performed when the *check-all* target is built. For instance, if you are; using Makefiles, execute this command in the root of your build directory:. .. code-block:: console. $ make check-all. On Visual Studio, you may run tests by building the project ""check-all"".; For more information about testing, see the :doc:`TestingGuide`. Cross compiling; ===============. See `this wiki page <https://gitlab.kitware.com/cmake/community/wikis/doc/cmake/CrossCompiling>`_ for; generic instructions ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CMake.rst:39346,cache,cache,39346,interpreter/llvm-project/llvm/docs/CMake.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CMake.rst,4,['cache'],['cache']
Performance,"sing and, andnot, or. Various SSE compare translations. //===---------------------------------------------------------------------===//. Add hooks to commute some CMPP operations. //===---------------------------------------------------------------------===//. Apply the same transformation that merged four float into a single 128-bit load; to loads from constant pool. //===---------------------------------------------------------------------===//. Floating point max / min are commutable when -enable-unsafe-fp-path is; specified. We should turn int_x86_sse_max_ss and X86ISD::FMIN etc. into other; nodes which are selected to max / min instructions that are marked commutable. //===---------------------------------------------------------------------===//. We should materialize vector constants like ""all ones"" and ""signbit"" with ; code like:. cmpeqps xmm1, xmm1 ; xmm1 = all-ones. and:; cmpeqps xmm1, xmm1 ; xmm1 = all-ones; psrlq xmm1, 31 ; xmm1 = all 100000000000... instead of using a load from the constant pool. The later is important for; ABS/NEG/copysign etc. //===---------------------------------------------------------------------===//. These functions:. #include <xmmintrin.h>; __m128i a;; void x(unsigned short n) {; a = _mm_slli_epi32 (a, n);; }; void y(unsigned n) {; a = _mm_slli_epi32 (a, n);; }. compile to ( -O3 -static -fomit-frame-pointer):; _x:; movzwl 4(%esp), %eax; movd %eax, %xmm0; movaps _a, %xmm1; pslld %xmm0, %xmm1; movaps %xmm1, _a; ret; _y:; movd 4(%esp), %xmm0; movaps _a, %xmm1; pslld %xmm0, %xmm1; movaps %xmm1, _a; ret. ""y"" looks good, but ""x"" does silly movzwl stuff around into a GPR. It seems; like movd would be sufficient in both cases as the value is already zero ; extended in the 32-bit stack slot IIRC. For signed short, it should also be; save, as a really-signed value would be undefined for pslld. //===---------------------------------------------------------------------===//. #include <math.h>; int t1(double d) { return signbit(d); }. This ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:10114,load,load,10114,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,2,['load'],['load']
Performance,"sing different portions of; LLVM. Even in this situation using *BUILD_SHARED_LIBS* is not supported. If you; want to distribute LLVM as a shared library for use in a tool, the recommended; method is using *LLVM_BUILD_LLVM_DYLIB*, and you can use *LLVM_DYLIB_COMPONENTS*; to configure which LLVM components are part of libLLVM.; Note: *LLVM_BUILD_LLVM_DYLIB* is not available on Windows. Options for Optimizing LLVM; ===========================. There are four main build optimizations that our CMake build system supports.; When performing a bootstrap build it is not beneficial to do anything other than; setting *CMAKE_BUILD_TYPE* to ``Release`` for the stage-1 compiler. This is; because the more intensive optimizations are expensive to perform and the; stage-1 compiler is thrown away. All of the further options described should be; set on the stage-2 compiler either using a CMake cache file, or by prefixing the; option with *BOOTSTRAP_*. The first and simplest to use is the compiler optimization level by setting the; *CMAKE_BUILD_TYPE* option. The main values of interest are ``Release`` or; ``RelWithDebInfo``. By default the ``Release`` option uses the ``-O3``; optimization level, and ``RelWithDebInfo`` uses ``-O2``. If you want to generate; debug information and use ``-O3`` you can override the; *CMAKE_<LANG>_FLAGS_RELWITHDEBINFO* option for C and CXX.; DistributionExample.cmake does this. Another easy to use option is Link-Time-Optimization. You can set the; *LLVM_ENABLE_LTO* option on your stage-2 build to ``Thin`` or ``Full`` to enable; building LLVM with LTO. These options will significantly increase link time of; the binaries in the distribution, but it will create much faster binaries. This; option should not be used if your distribution includes static archives, as the; objects inside the archive will be LLVM bitcode, which is not portable. The :doc:`AdvancedBuilds` documentation describes the built-in tooling for; generating LLVM profiling information to drive P",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BuildingADistribution.rst:8082,optimiz,optimization,8082,interpreter/llvm-project/llvm/docs/BuildingADistribution.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BuildingADistribution.rst,1,['optimiz'],['optimization']
Performance,"sing fitted chi2 values). The initial parameter values can be set directly in the input model function object.; However, for setting parameter bounds and step sizes to values different than the automatically computed ones, one needs to use the `ROOT::Fit::ParameterSetting` class.; This example code will set the lower/upper bounds for the first parameter and a lower bound for the second parameter. ``` {.cpp}; fitter.SetFunction( fitFunction, false);; fitter.Config().ParSettings(0).SetLimits(0,1.E6);; fitter.Config().ParSettings(2).SetLowerLimit(0);; ```. Note that a `ROOT::Fit::ParameterSettings` objects exists for each fit parameter and it created by the `ROOT::Fit::FitConfig` class, after the model function has been set in the Fitter.; Only when the function is set, the number of parameter is known and; automatically the `FitConfig` creates the corresponding `ParameterSetting` objects. When fitting, different minimizer can be used. The can be implemented in different libraries and loaded ar run time by the plug-in manager system of ROOT.; Each different minimizer (e.g. *Minuit, Minuit2, Fumili,* etc.) consists of a different implementation of the `ROOT::Math::Minimizer` interface.; Within the same minimizer, thus within the same class implementing the `Minimizer` interface, different algorithms can exist.; For example in the case of Minuit, we have *Migrad, Simplex* or *Minimize*. The minimizer and its corresponding algorithm, when available,; can be set by using the function `FitConfig::SetMinimizer(""minimizerName"")` or by using directly the `ROOT:Math::MinimizerOptions` class. If the requested minimizer is not available in ROOT, the default one is used. The default minimizer type and algorithm can be specified by using the; static function `ROOT::Math::MinimizerOptions::SetDefaultMinimizer(""minimizerName"")`. ### Minimizer Libraries and Algorithms. The list of available minimizer libraries currently available in ROOT, with their corresponding available algorithms ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/FittingHistograms.md:39816,load,loaded,39816,documentation/users-guide/FittingHistograms.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/FittingHistograms.md,1,['load'],['loaded']
Performance,"sing. EXAMPLE 1: benchmarking instructions; ------------------------------------. Assume you have an X86-64 machine. To measure the latency of a single; instruction, run:. .. code-block:: bash. $ llvm-exegesis --mode=latency --opcode-name=ADD64rr. Measuring the uop decomposition or inverse throughput of an instruction works similarly:. .. code-block:: bash. $ llvm-exegesis --mode=uops --opcode-name=ADD64rr; $ llvm-exegesis --mode=inverse_throughput --opcode-name=ADD64rr. The output is a YAML document (the default is to write to stdout, but you can; redirect the output to a file using `--benchmarks-file`):. .. code-block:: none. ---; key:; opcode_name: ADD64rr; mode: latency; config: ''; cpu_name: haswell; llvm_triple: x86_64-unknown-linux-gnu; num_repetitions: 10000; measurements:; - { key: latency, value: 1.0058, debug_string: '' }; error: ''; info: 'explicit self cycles, selecting one aliasing configuration.; Snippet:; ADD64rr R8, R8, R10; '; ... To measure the latency of all instructions for the host architecture, run:. .. code-block:: bash. $ llvm-exegesis --mode=latency --opcode-index=-1. EXAMPLE 2: benchmarking a custom code snippet; ---------------------------------------------. To measure the latency/uops of a custom piece of code, you can specify the; `snippets-file` option (`-` reads from standard input). .. code-block:: bash. $ echo ""vzeroupper"" | llvm-exegesis --mode=uops --snippets-file=-. Real-life code snippets typically depend on registers or memory.; :program:`llvm-exegesis` checks the liveliness of registers (i.e. any register; use has a corresponding def or is a ""live in""). If your code depends on the; value of some registers, you need to use snippet annotations to ensure setup; is performed properly. For example, the following code snippet depends on the values of XMM1 (which; will be set by the tool) and the memory buffer passed in RDI (live in). .. code-block:: none. # LLVM-EXEGESIS-LIVEIN RDI; # LLVM-EXEGESIS-DEFREG XMM1 42; vmulps	(%rdi), %xm",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-exegesis.rst:5687,latency,latency,5687,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-exegesis.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-exegesis.rst,1,['latency'],['latency']
Performance,"single value, each node in the graph may; define multiple values. For example, a combined div/rem operation will define; both the dividend and the remainder. Many other situations require multiple; values as well. Each node also has some number of operands, which are edges to; the node defining the used value. Because nodes may define multiple values,; edges are represented by instances of the ``SDValue`` class, which is a; ``<SDNode, unsigned>`` pair, indicating the node and result value being used,; respectively. Each value produced by an ``SDNode`` has an associated ``MVT``; (Machine Value Type) indicating what the type of the value is. SelectionDAGs contain two different kinds of values: those that represent data; flow and those that represent control flow dependencies. Data values are simple; edges with an integer or floating point value type. Control edges are; represented as ""chain"" edges which are of type ``MVT::Other``. These edges; provide an ordering between nodes that have side effects (such as loads, stores,; calls, returns, etc). All nodes that have side effects should take a token; chain as input and produce a new one as output. By convention, token chain; inputs are always operand #0, and chain results are always the last value; produced by an operation. However, after instruction selection, the; machine nodes have their chain after the instruction's operands, and; may be followed by glue nodes. A SelectionDAG has designated ""Entry"" and ""Root"" nodes. The Entry node is; always a marker node with an Opcode of ``ISD::EntryToken``. The Root node is; the final side-effecting node in the token chain. For example, in a single basic; block function it would be the return node. One important concept for SelectionDAGs is the notion of a ""legal"" vs.; ""illegal"" DAG. A legal DAG for a target is one that only uses supported; operations and supported types. On a 32-bit PowerPC, for example, a DAG with a; value of type i1, i8, i16, or i64 would be illegal, as would a",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst:35606,load,loads,35606,interpreter/llvm-project/llvm/docs/CodeGenerator.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst,1,['load'],['loads']
Performance,"sion of PoD on CernVM-FS you can use; that one. Experiment-independent versions are available from the PH-SFT; cvmfs repository. Only if you have specific reasons while you want to use a customly built; PoD version, download the source code and compile it using the; installation instructions. Please note that [CMake](http://www.cmake.org/) and; [Boost](http://www.boost.org/) are required to build PoD. - After you have built PoD, install it with:. make install. - After installing PoD, run:. pod-server getbins. This has to be done only once and downloads the binary packages that; will be dynamically transferred to the worker nodes as binary; payload, and prevents us from installing PoD on each cluster node. It is important to do this step now, because in case PoD has been; installed in a directory where the user has no write privileges, as; in the case of system-wide installations, the user won't be able to; download those required packages in the PoD binary directory. > There is no need to ""configure"" PoD for your specific cluster: it is; > just enough to install it on your head node.; >; > PoD does not have any system-wide persistent daemon running or any; > system-wide configuration to be performed. Also, no part of PoD will; > be ever run as root.; >; > Do not worry about environment or software configuration at this time:; > there is no system configuration for that. All the environment for; > your software dependencies will be set via proper scripts from the PoD; > client.; >; > PoD client configuration and running is properly covered in the; > appropriate manual page. ### Firewall configuration. The head node only requires **TCP ports 22 (SSH) and 443 (HTTPS)** to accept; connections from the outside. Users will get an authentication ""token""; from port 443 and all PROOF traffic will be automatically tunneled in a; SSH connection on port 22 by PoD. In case you are not using the HTTPS+SSH token+authentication method, access to; the sole port 22 is all you need.; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/proof/doc/confman/ConfigProofPoD.md:6261,perform,performed,6261,proof/doc/confman/ConfigProofPoD.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/proof/doc/confman/ConfigProofPoD.md,1,['perform'],['performed']
Performance,"sisType=multiclass"". Training data is; specified with an additional classname, e.g. via; factory->AddTree(tree,""classname"");. After the; training a genetic algorithm is invoked to determine the best; cuts for selecting a specific class, based on the figure of; merit: purity*efficiency. TMVA comes with two examples in; $ROOTSYS/tmva/test: TMVAMulticlass.C; and TMVAMulticlassApplication.C. New TMVA event vector building. The code; for splitting the input data into training and test samples for; all classes and the mixing of those samples to one training and; one test sample has been rewritten completely. The new code is; more performant and has a clearer structure. This fixes several; bugs which have been reported by some users of TMVA.; Code and performance test framework: A unit; test framework for daily software and method performance; validation has been implemented.; . Methods. BDT Automatic parameter optimisation for building the; tree architecture: The optimisation procedure uses the; performance of the trained classifier on the ""test sample"" for; finding the set of optimal parameters. Two different methods to; traverse the parameter space are available (scanning, genetic; algorithm). Currently parameter optimization is implemented only; for these three parameters that influence the tree architectur:; the maximum depth of a tree, MaxDepth, the minimum; number of events in each node, NodeMinEvents, and; the number of tress, NTrees. Optimization can; is invoked by calling; factory->OptimizeAllMethods(); prior to the call; factory->TrainAllMethods();. Automated and configurable parameter optimization is soon to; be enabled for all methods (for those parameters where; optimization is applicable).; . BDT node splitting: While Decision Trees; typically have only univariate splits, in TMVA one can now; also opt for multivariate splits that use a ""Fisher; Discriminant"" (option: UseFisherCuts), built from all; observables that show correlations larger than some threshol",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tmva/doc/v528/index.html:1668,perform,performance,1668,tmva/doc/v528/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tmva/doc/v528/index.html,2,['perform'],['performance']
Performance,"size argument, which can lead to buffer overflows.; set-xcode-analyzer now supports self-contained Xcode.app (Xcode 4.3 and later).; Contains a newer version of the analyzer than Xcode 4.3.; Misc. bug fixes and performance work. checker-260; built: January 25, 2012; highlights:; This is essentially the same as checker-259, but enables the following experimental checkers (please provide feedback):. Warns about unsafe uses of CFArrayCreate, CFSetCreate, and CFDictionaryCreate; Warns about unsafe uses of getpw, gets, which are sources of buffer overflows; Warns about unsafe uses of mktemp and mktemps, which can lead to insecure temporary files; Warns about unsafe uses of vfork, which is insecure to use; Warns about not checking the return values of setuid, setgid, seteuid, setegid, setreuid, setregid (another security issue). checker-259; built: January 25, 2012; highlights:. Contains a newer version of the analyzer than the one shipped in Xcode 4.2.; Significant performance optimizations to reduce memory usage of the analyzer.; Tweaks to scan-build to have it work more easily with Xcode projects using Clang.; Numerous bug fixes to better support code using ARC. checker-258; built: October 13, 2011; highlights:. Contains a newer version of the analyzer than the one shipped in Xcode 4.2.; Adds a new security checker for looking at correct uses of the Mac OS KeyChain API.; Supports ARC (please file bugs where you see issues); Major under-the-cover changes. This should result in more precise results in some cases, but this is laying the groundwork for major improvements. Please file bugs where you see regressions or issues. checker-257; built: May 25, 2011; highlights:. The analyzer is now far more aggressive with checking conformance with Core Foundation conventions. Any function that returns a CF type must now obey the Core Foundation naming conventions, or use the cf_returns_retained or cf_returns_not_retained annotations.; Fixed a serious regression where the analyzer ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/analyzer/release_notes.html:10009,perform,performance,10009,interpreter/llvm-project/clang/www/analyzer/release_notes.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/analyzer/release_notes.html,4,"['optimiz', 'perform']","['optimizations', 'performance']"
Performance,"size, encoding, and endianity specified by V's base type. It pushes a location description L with one implicit location description SL; on the stack. SL specifies LS with a bit offset of 0. *The* ``DW_OP_stack_value`` *operation specifies that the object does not; exist in memory, but its value is nonetheless known. In this form, the; location description specifies the actual value of the object, rather than; specifying the memory or register storage that holds the value.*. See ``DW_OP_implicit_pointer`` (following) for special rules concerning; implicit pointer values produced by dereferencing implicit location; descriptions created by the ``DW_OP_implicit_pointer`` and; ``DW_OP_LLVM_aspace_implicit_pointer`` operations. Note: Since location descriptions are allowed on the stack, the; ``DW_OP_stack_value`` operation no longer terminates the DWARF operation; expression execution as in DWARF Version 5. 3. ``DW_OP_implicit_pointer``. *An optimizing compiler may eliminate a pointer, while still retaining the; value that the pointer addressed.* ``DW_OP_implicit_pointer`` *allows a; producer to describe this value.*. ``DW_OP_implicit_pointer`` *specifies an object is a pointer to the target; architecture default address space that cannot be represented as a real; pointer, even though the value it would point to can be described. In this; form, the location description specifies a debugging information entry that; represents the actual location description of the object to which the; pointer would point. Thus, a consumer of the debug information would be able; to access the dereferenced pointer, even when it cannot access the pointer; itself.*. ``DW_OP_implicit_pointer`` has two operands. The first operand is a 4-byte; unsigned value in the 32-bit DWARF format, or an 8-byte unsigned value in; the 64-bit DWARF format, that represents the byte offset DR of a debugging; information entry D relative to the beginning of the ``.debug_info`` section; that contains the current co",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst:122481,optimiz,optimizing,122481,interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst,1,['optimiz'],['optimizing']
Performance,"size=<size>. Specify the size of the register file. When specified, this flag limits how; many physical registers are available for register renaming purposes. A value; of zero for this flag means ""unlimited number of physical registers"". .. option:: -iterations=<number of iterations>. Specify the number of iterations to run. If this flag is set to 0, then the; tool sets the number of iterations to a default value (i.e. 100). .. option:: -noalias=<bool>. If set, the tool assumes that loads and stores don't alias. This is the; default behavior. .. option:: -lqueue=<load queue size>. Specify the size of the load queue in the load/store unit emulated by the tool.; By default, the tool assumes an unbound number of entries in the load queue.; A value of zero for this flag is ignored, and the default load queue size is; used instead. .. option:: -squeue=<store queue size>. Specify the size of the store queue in the load/store unit emulated by the; tool. By default, the tool assumes an unbound number of entries in the store; queue. A value of zero for this flag is ignored, and the default store queue; size is used instead. .. option:: -timeline. Enable the timeline view. .. option:: -timeline-max-iterations=<iterations>. Limit the number of iterations to print in the timeline view. By default, the; timeline view prints information for up to 10 iterations. .. option:: -timeline-max-cycles=<cycles>. Limit the number of cycles in the timeline view, or use 0 for no limit. By; default, the number of cycles is set to 80. .. option:: -resource-pressure. Enable the resource pressure view. This is enabled by default. .. option:: -register-file-stats. Enable register file usage statistics. .. option:: -dispatch-stats. Enable extra dispatch statistics. This view collects and analyzes instruction; dispatch events, as well as static/dynamic dispatch stall events. This view; is disabled by default. .. option:: -scheduler-stats. Enable extra scheduler statistics. This view collects and an",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst:4591,queue,queue,4591,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,1,['queue'],['queue']
Performance,"sk were false for those lanes. %masked.a = select <4 x i1> %mask, <4 x i32> %a, <4 x i32> <i32 0, i32 0, i32 0, i32 0>; %reduction = call i32 @llvm.vector.reduce.xor.v4i32(<4 x i32> %masked.a); %also.r = xor i32 %reduction, %start. .. _int_vp_reduce_smax:. '``llvm.vp.reduce.smax.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare i32 @llvm.vp.reduce.smax.v4i32(i32 <start_value>, <4 x i32> <val>, <4 x i1> <mask>, i32 <vector_length>); declare i16 @llvm.vp.reduce.smax.nxv8i16(i16 <start_value>, <vscale x 8 x i16> <val>, <vscale x 8 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". Predicated signed-integer ``MAX`` reduction of a vector and a scalar starting; value, returning the result as a scalar. Arguments:; """""""""""""""""""". The first operand is the start value of the reduction, which must be a scalar; integer type equal to the result type. The second operand is the vector on; which the reduction is performed and must be a vector of integer values whose; element type is the result/start type. The third operand is the vector mask and; is a vector of boolean values with the same number of elements as the vector; operand. The fourth operand is the explicit vector length of the operation. Semantics:; """""""""""""""""""". The '``llvm.vp.reduce.smax``' intrinsic performs the signed-integer ``MAX``; reduction (:ref:`llvm.vector.reduce.smax <int_vector_reduce_smax>`) of the; vector operand ``val`` on each enabled lane, and taking the maximum of that and; the scalar ``start_value``. Disabled lanes are treated as containing the; neutral value ``INT_MIN`` (i.e. having no effect on the reduction operation).; If the vector length is zero, the result is the start value. To ignore the start value, the neutral value can be used. Examples:; """""""""""""""""". .. code-block:: llvm. %r = call i8 @llvm.vp.reduce.smax.v4i8(i8 %start, <4 x i8> %a, <4 x i1> %mask, i32 %evl); ; %r is equivalent to %also.r, where lanes greater than or equ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:763456,perform,performed,763456,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performed']
Performance,"skets); When the amount of data written so far (fTotBytes) is greater than fAutoFlush (see SetAutoFlush) all the baskets are flushed to disk.; This makes future reading faster as it guarantees that baskets belonging to nearby entries will be on the same disk region.; When the first call to flush the baskets happens, we also take this opportunity to optimize the baskets buffers.; We also check if the number of bytes written is greater than fAutoSave (see SetAutoSave).; In this case we also write the Tree header. This makes the Tree recoverable up to this point in case the program writing the Tree crashes.; Note that the user can also decide to call FlushBaskets and AutoSave in her event loop on the base of the number of events written instead of the number of bytes written.; New function TTree::OptimizeBaskets. void TTree::OptimizeBaskets(Int_t maxMemory, Float_t minComp, Option_t *option). This function may be called after having filled some entries in a Tree; using the information in the existing branch buffers, it will reassign; new branch buffer sizes to optimize time and memory.; The function computes the best values for branch buffer sizes such that; the total buffer sizes is less than maxMemory and nearby entries written; at the same time.; In case the branch compression factor for the data written so far is less; than compMin, the compression is disabled. if option =""d"" an analysis report is printed.; This function may also be called on an existing Tree to figure out the best values; given the information in the Tree header. TFile f(""myfile.root"");; TTree *T = (TTree*)f.Get(""mytreename"");; T->Print(); //show the branch buffer sizes before optimization; T->OptimizeBaskets(10000000,1,""d"");; T->Print(); //show the branch buffer sizes after optimization. New interface functions to customize the TreeCache; virtual void AddBranchToCache(const char *bname, Bool_t subbranches = kFALSE);; virtual void AddBranchToCache(TBranch *branch, Bool_t subbranches = kFALSE);; vir",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/doc/v526/index.html:4588,optimiz,optimize,4588,tree/doc/v526/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/doc/v526/index.html,2,['optimiz'],['optimize']
Performance,"sking someone to take a look at the failures. #. Once the pull request has been merged push it to the official release branch; with the script ``llvm/utils/git/sync-release-repo.sh``. Then add a comment to the issue stating that the fix has been merged along with; the git hashes from the release branch. Add the release:merged label to the issue; and close it. Release Patch Rules; -------------------. Below are the rules regarding patching the release branch:. #. Patches applied to the release branch may only be applied by the release; manager, the official release testers or the code owners with approval from; the release manager. #. Release managers are encouraged, but not required, to get approval from code; owners before approving patches. If there is no code owner or the code owner; is unreachable then release managers can ask approval from patch reviewers or; other developers active in that area. #. *Before RC1* Patches should be limited to bug fixes, important optimization; improvements, or completion of features that were started before the branch; was created. As with all phases, release managers and code owners can reject; patches that are deemed too invasive. #. *Before RC2* Patches should be limited to bug fixes or backend specific; improvements that are determined to be very safe. #. *Before RC3/Final Major Release* Patches should be limited to critical; bugs or regressions. #. *Bug fix releases* Patches should be limited to bug fixes or very safe; and critical performance improvements. Patches must maintain both API and; ABI compatibility with the previous major release. Release Final Tasks; -------------------. The final stages of the release process involves tagging the ""final"" release; branch, updating documentation that refers to the release, and updating the; demo page. Update Documentation; ^^^^^^^^^^^^^^^^^^^^. Review the documentation in the release branch and ensure that it is up; to date. The ""Release Notes"" must be updated to reflect new featu",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToReleaseLLVM.rst:13063,optimiz,optimization,13063,interpreter/llvm-project/llvm/docs/HowToReleaseLLVM.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToReleaseLLVM.rst,1,['optimiz'],['optimization']
Performance,"skipping the checking of loads which; use a fixed frame pointer offset. The result of this optimization is that patterns like reloading a spilled; register or accessing a global field don't get checked. This is a very; significant performance win. ###### Don't check dependent loads. A core part of why this mitigation strategy works is that it establishes a; data-flow check on the loaded address. However, this means that if the address; itself was already loaded using a checked load, there is no need to check a; dependent load provided it is within the same basic block as the checked load,; and therefore has no additional predicates guarding it. Consider code like the; following:; ```; ... .LBB0_4: # %danger; movq (%rcx), %rdi; movl (%rdi), %edx; ```. This will get transformed into:; ```; ... .LBB0_4: # %danger; cmovneq %r8, %rax # Conditionally update predicate state.; orq %rax, %rcx # Mask the pointer if misspeculating.; movq (%rcx), %rdi # Hardened load.; movl (%rdi), %edx # Unhardened load due to dependent addr.; ```. This doesn't check the load through `%rdi` as that pointer is dependent on a; checked load already. ###### Protect large, load-heavy blocks with a single lfence. It may be worth using a single `lfence` instruction at the start of a block; which begins with a (very) large number of loads that require independent; protection *and* which require hardening the address of the load. However, this; is unlikely to be profitable in practice. The latency hit of the hardening; would need to exceed that of an `lfence` when *correctly* speculatively; executed. But in that case, the `lfence` cost is a complete loss of speculative; execution (at a minimum). So far, the evidence we have of the performance cost; of using `lfence` indicates few if any hot code patterns where this trade off; would make sense. ###### Tempting optimizations that break the security model. Several optimizations were considered which didn't pan out due to failure to; uphold the security mo",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md:36037,load,load,36037,interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,1,['load'],['load']
Performance,"sky. Whether this kind of; mixture is a sufficient mitigation requires careful analysis. Unfortunately, experimental results indicate that the performance overhead of; this approach is very high for certain patterns of code. A classic example is; any form of recursive evaluation engine. The hot, rapid call and return; sequences exhibit dramatic performance loss when mitigated with `lfence`. This; component alone can regress performance by 2x or more, making it an unpleasant; tradeoff even when only used in a mixture of code. ##### Use an internal TLS location to pass predicate state. We can define a special thread-local value to hold the predicate state between; functions. This avoids direct ABI implications by using a side channel between; callers and callees to communicate the predicate state. It also allows implicit; zero-initialization of the state, which allows non-checked code to be the first; code executed. However, this requires a load from TLS in the entry block, a store to TLS; before every call and every ret, and a load from TLS after every call. As a; consequence it is expected to be substantially more expensive even than using; `%rsp` and potentially `lfence` within the function entry block. ##### Define a new ABI and/or calling convention. We could define a new ABI and/or calling convention to explicitly pass the; predicate state in and out of functions. This may be interesting if none of the; alternatives have adequate performance, but it makes deployment and adoption; dramatically more complex, and potentially infeasible. ## High-Level Alternative Mitigation Strategies. There are completely different alternative approaches to mitigating variant 1; attacks. [Most](https://lwn.net/Articles/743265/); [discussion](https://lwn.net/Articles/744287/) so far focuses on mitigating; specific known attackable components in the Linux kernel (or other kernels) by; manually rewriting the code to contain an instruction sequence that is not; vulnerable. For x86 syst",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md:42814,load,load,42814,interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,2,['load'],['load']
Performance,"snmsubadp. . isCommutable = 1; // xsmaddqp; [(set f128:$vT, (fma f128:$vA, f128:$vB, f128:$vTi))]>,; RegConstraint<""$vTi = $vT"">, NoEncode<""$vTi"">,; AltVSXFMARel;. // xsmsubqp; [(set f128:$vT, (fma f128:$vA, f128:$vB, (fneg f128:$vTi)))]>,; RegConstraint<""$vTi = $vT"">, NoEncode<""$vTi"">,; AltVSXFMARel;. // xsnmaddqp; [(set f128:$vT, (fneg (fma f128:$vA, f128:$vB, f128:$vTi)))]>,; RegConstraint<""$vTi = $vT"">, NoEncode<""$vTi"">,; AltVSXFMARel;. // xsnmsubqp; [(set f128:$vT, (fneg (fma f128:$vA, f128:$vB, (fneg f128:$vTi))))]>,; RegConstraint<""$vTi = $vT"">, NoEncode<""$vTi"">,; AltVSXFMARel;. - Round to Odd of QP (Negative) Multiply-{Add/Subtract}:; xsmaddqpo xsmsubqpo xsnmaddqpo xsnmsubqpo; . Similar to xsrsqrtedp??. . Define DAG Node in PPCInstrInfo.td:; def PPCfmarto: SDNode<""PPCISD::FMARTO"", SDTFPTernaryOp, []>;. It looks like we only need to define ""PPCfmarto"" for these instructions,; because according to PowerISA_V3.0, these instructions perform RTO on; fma's result:; xsmaddqp(o); v ← bfp_MULTIPLY_ADD(src1, src3, src2); rnd ← bfp_ROUND_TO_BFP128(RO, FPSCR.RN, v); result ← bfp_CONVERT_TO_BFP128(rnd). xsmsubqp(o); v ← bfp_MULTIPLY_ADD(src1, src3, bfp_NEGATE(src2)); rnd ← bfp_ROUND_TO_BFP128(RO, FPSCR.RN, v); result ← bfp_CONVERT_TO_BFP128(rnd). xsnmaddqp(o); v ← bfp_MULTIPLY_ADD(src1,src3,src2); rnd ← bfp_NEGATE(bfp_ROUND_TO_BFP128(RO, FPSCR.RN, v)); result ← bfp_CONVERT_TO_BFP128(rnd). xsnmsubqp(o); v ← bfp_MULTIPLY_ADD(src1, src3, bfp_NEGATE(src2)); rnd ← bfp_NEGATE(bfp_ROUND_TO_BFP128(RO, FPSCR.RN, v)); result ← bfp_CONVERT_TO_BFP128(rnd). DAG patterns of each instruction (PPCInstrVSX.td):; . isCommutable = 1; // xsmaddqpo; [(set f128:$vT, (PPCfmarto f128:$vA, f128:$vB, f128:$vTi))]>,; RegConstraint<""$vTi = $vT"">, NoEncode<""$vTi"">,; AltVSXFMARel;. // xsmsubqpo; [(set f128:$vT, (PPCfmarto f128:$vA, f128:$vB, (fneg f128:$vTi)))]>,; RegConstraint<""$vTi = $vT"">, NoEncode<""$vTi"">,; AltVSXFMARel;. // xsnmaddqpo; [(set f128:$vT, (fneg (PPCfmarto f128:$vA, f128:$vB, f128:$v",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt:8927,perform,perform,8927,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt,2,['perform'],['perform']
Performance,"so; guarantees the operation to be lock-free, so it does not depend on the data; being part of a special atomic structure or depend on a separate per-process; global lock. Note that code generation will fail for unsupported atomic; operations; if you need such an operation, use explicit locking. Relevant standard; This is intended to match the Java memory model for shared variables. Notes for frontends; This cannot be used for synchronization, but is useful for Java and other; ""safe"" languages which need to guarantee that the generated code never; exhibits undefined behavior. Note that this guarantee is cheap on common; platforms for loads of a native width, but can be expensive or unavailable for; wider loads, like a 64-bit store on ARM. (A frontend for Java or other ""safe""; languages would normally split a 64-bit store on ARM into two 32-bit unordered; stores.). Notes for optimizers; In terms of the optimizer, this prohibits any transformation that transforms a; single load into multiple loads, transforms a store into multiple stores,; narrows a store, or stores a value which would not be stored otherwise. Some; examples of unsafe optimizations are narrowing an assignment into a bitfield,; rematerializing a load, and turning loads and stores into a memcpy; call. Reordering unordered operations is safe, though, and optimizers should; take advantage of that because unordered operations are common in languages; that need them. Notes for code generation; These operations are required to be atomic in the sense that if you use; unordered loads and unordered stores, a load cannot see a value which was; never stored. A normal load or store instruction is usually sufficient, but; note that an unordered load or store cannot be split into multiple; instructions (or an instruction which does multiple memory operations, like; ``LDRD`` on ARM without LPAE, or not naturally-aligned ``LDRD`` on LPAE ARM). Monotonic; ---------. Monotonic is the weakest level of atomicity that can b",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst:8989,optimiz,optimizers,8989,interpreter/llvm-project/llvm/docs/Atomics.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst,4,"['load', 'optimiz']","['load', 'loads', 'optimizer', 'optimizers']"
Performance,"so; that the returned string was meaningless.; Reset; the list if dir entries in FreeDirectory.; Fix problem affecting repeated calls. The implementation of TFile throughput and info sending was; just sending 'regular' samples about the activity of the single TFile; instance that happened to trigger an activity in the right moment.; Now TMonaLisaWriter keeps internally track of every; activity; and regularly sends summaries valid for all the files which had; activity in the last time interval.; Additionally, it's now finalized the infrastructure able to; measure; and keep track of the file Open latency. A packet is sent for each; successful Open, sending the measures of the latencies for the; various phases of the open. Currently exploited fully by TAlienFile; and TXNetFile. Easy to report from other TFiles too.; Now, the hook for the Close() func triggers sending of a; packet containing various information about the performance related to; that file only.; Added support also for performance monitoring when writing. RGLITE: A ROOT GRID interface. RGLite plug-in - a ROOT plug-in module, which implements the ROOT Grid; interface and offers to ROOT users possibilities to perform a number of; operations using gLite middleware from within ROOT. Supported features:. Workload Management System operations:; ; job submission – normal, DAG and parametric; jobs (gLite; WMProxy API), ; smart look-up algorithm for WMP-Endpoints, ; job status querying (gLite LB API), ; job output retrieving (Globus GridFTP). . File Catalog operations (gLite/LCG LFC API):; ; smart session manager, ; set/query the current working catalog directory, ; list files, directories and their stats, ; add/remove files in a catalog namespace, ; add/remove directories, ; add/remove replicas from a given file. . An executive logging. ; Support of an external XML configuration file with; according XML; schema. . Usage examples:. Job operations. // loading RGLite plug-in. TGrid::Connect(""glite"");; // submitting G",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/net/doc/v520/index.html:2997,perform,performance,2997,net/doc/v520/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/net/doc/v520/index.html,2,['perform'],['performance']
Performance,"sociated; MaterializationResponsibility object. This object tracks the definitions; that must be materialized and provides a way to notify the JITDylib once they; are either successfully materialized or a failure occurs. Absolute Symbols, Aliases, and Reexports; ========================================. ORC makes it easy to define symbols with absolute addresses, or symbols that; are simply aliases of other symbols:. Absolute Symbols; ----------------. Absolute symbols are symbols that map directly to addresses without requiring; further materialization, for example: ""foo"" = 0x1234. One use case for; absolute symbols is allowing resolution of process symbols. E.g. .. code-block:: c++. JD.define(absoluteSymbols(SymbolMap({; { Mangle(""printf""),; { ExecutorAddr::fromPtr(&printf),; JITSymbolFlags::Callable } }; });. With this mapping established code added to the JIT can refer to printf; symbolically rather than requiring the address of printf to be ""baked in"".; This in turn allows cached versions of the JIT'd code (e.g. compiled objects); to be re-used across JIT sessions as the JIT'd code no longer changes, only the; absolute symbol definition does. For process and library symbols the DynamicLibrarySearchGenerator utility (See; :ref:`How to Add Process and Library Symbols to JITDylibs; <ProcessAndLibrarySymbols>`) can be used to automatically build absolute; symbol mappings for you. However the absoluteSymbols function is still useful; for making non-global objects in your JIT visible to JIT'd code. For example,; imagine that your JIT standard library needs access to your JIT object to make; some calls. We could bake the address of your object into the library, but then; it would need to be recompiled for each session:. .. code-block:: c++. // From standard library for JIT'd code:. class MyJIT {; public:; void log(const char *Msg);; };. void log(const char *Msg) { ((MyJIT*)0x1234)->log(Msg); }. We can turn this into a symbolic reference in the JIT standard library:. ..",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:13660,cache,cached,13660,interpreter/llvm-project/llvm/docs/ORCv2.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst,1,['cache'],['cached']
Performance,"something incorrectly when they wrote; the annotation. Regardless of why, it is useful to detect these situations so; that the optimizer can make more useful decisions about the code. MisExpect; diagnostics are intended to help developers identify and address these; situations, by comparing the use of the ``llvm.expect`` intrinsic to the ground; truth provided by a profiling input. The MisExpect checks in the LLVM backend follow a simple procedure: if there is; a mismatch between the branch weights collected during profiling and those; supplied by an ``llvm.expect`` intrinsic, then it will emit a diagnostic; message to the user. The most natural place to perform the verification is just prior to when; branch weights are assigned to the target instruction in the form of; branch weight metadata. There are 3 key places in the LLVM backend where branch weights are; created and assigned based on profiling information or the use of the; ``llvm.expect`` intrinsic, and our implementation focuses on these; places to perform the verification. We calculate the threshold for emitting MisExpect related diagnostics; based on the values the compiler assigns to ``llvm.expect`` intrinsics,; which can be set through the ``-likely-branch-weight`` and; ``-unlikely-branch-weight`` LLVM options. During verification, if the; profile weights mismatch the calculated threshold, then we will emit a; remark or warning detailing a potential performance regression. The; diagnostic also reports the percentage of the time the annotation was; correct during profiling to help developers reason about how to proceed. The diagnostics are also available in the form of optimization remarks,; which can be serialized and processed through the ``opt-viewer.py``; scripts in LLVM. .. option:: -pass-remarks=misexpect. Enables optimization remarks for misexpect when profiling data conflicts with; use of ``llvm.expect`` intrinsics. .. option:: -pgo-warn-misexpect. Enables misexpect warnings when profiling data c",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MisExpect.rst:1546,perform,perform,1546,interpreter/llvm-project/llvm/docs/MisExpect.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MisExpect.rst,1,['perform'],['perform']
Performance,"son; about for the programmer than other kinds of operations, and using them is; generally a practical performance tradeoff. Notes for optimizers; Optimizers not aware of atomics can treat this like a nothrow call. For; SequentiallyConsistent loads and stores, the same reorderings are allowed as; for Acquire loads and Release stores, except that SequentiallyConsistent; operations may not be reordered. Notes for code generation; SequentiallyConsistent loads minimally require the same barriers as Acquire; operations and SequentiallyConsistent stores require Release; barriers. Additionally, the code generator must enforce ordering between; SequentiallyConsistent stores followed by SequentiallyConsistent loads. This; is usually done by emitting either a full fence before the loads or a full; fence after the stores; which is preferred varies by architecture. Atomics and IR optimization; ===========================. Predicates for optimizer writers to query:. * ``isSimple()``: A load or store which is not volatile or atomic. This is; what, for example, memcpyopt would check for operations it might transform. * ``isUnordered()``: A load or store which is not volatile and at most; Unordered. This would be checked, for example, by LICM before hoisting an; operation. * ``mayReadFromMemory()``/``mayWriteToMemory()``: Existing predicate, but note; that they return true for any operation which is volatile or at least; Monotonic. * ``isStrongerThan`` / ``isAtLeastOrStrongerThan``: These are predicates on; orderings. They can be useful for passes that are aware of atomics, for; example to do DSE across a single atomic access, but not across a; release-acquire pair (see MemoryDependencyAnalysis for an example of this). * Alias analysis: Note that AA will return ModRef for anything Acquire or; Release, and for the address accessed by any Monotonic operation. To support optimizing around atomic operations, make sure you are using the; right predicates; everything should work if that i",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst:15892,load,load,15892,interpreter/llvm-project/llvm/docs/Atomics.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst,1,['load'],['load']
Performance,"space 0, unless the function is; marked with the ``null_pointer_is_valid`` attribute. If an object can be proven accessible through a pointer with a; different address space, the access may be modified to use that; address space. Exceptions apply if the operation is ``volatile``. Prior to LLVM 15, pointer types also specified a pointee type, such as; ``i8*``, ``[4 x i32]*`` or ``i32 (i32*)*``. In LLVM 15, such ""typed; pointers"" are still supported under non-default options. See the; `opaque pointers document <OpaquePointers.html>`__ for more information. .. _t_target_type:. Target Extension Type; """""""""""""""""""""""""""""""""""""""""". :Overview:. Target extension types represent types that must be preserved through; optimization, but are otherwise generally opaque to the compiler. They may be; used as function parameters or arguments, and in :ref:`phi <i_phi>` or; :ref:`select <i_select>` instructions. Some types may be also used in; :ref:`alloca <i_alloca>` instructions or as global values, and correspondingly; it is legal to use :ref:`load <i_load>` and :ref:`store <i_store>` instructions; on them. Full semantics for these types are defined by the target. The only constants that target extension types may have are ``zeroinitializer``,; ``undef``, and ``poison``. Other possible values for target extension types may; arise from target-specific intrinsics and functions. These types cannot be converted to other types. As such, it is not legal to use; them in :ref:`bitcast <i_bitcast>` instructions (as a source or target type),; nor is it legal to use them in :ref:`ptrtoint <i_ptrtoint>` or; :ref:`inttoptr <i_inttoptr>` instructions. Similarly, they are not legal to use; in an :ref:`icmp <i_icmp>` instruction. Target extension types have a name and optional type or integer parameters. The; meanings of name and parameters are defined by the target. When being defined in; LLVM IR, all of the type parameters must precede all of the integer parameters. Specific target extension types are ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:172716,load,load,172716,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,"space. The default address space is number zero. The semantics of non-zero address spaces are target-specific. Memory; access through a non-dereferenceable pointer is undefined behavior in; any address space. Pointers with the bit-value 0 are only assumed to; be non-dereferenceable in address space 0, unless the function is; marked with the ``null_pointer_is_valid`` attribute. If an object can be proven accessible through a pointer with a; different address space, the access may be modified to use that; address space. Exceptions apply if the operation is ``volatile``. Prior to LLVM 15, pointer types also specified a pointee type, such as; ``i8*``, ``[4 x i32]*`` or ``i32 (i32*)*``. In LLVM 15, such ""typed; pointers"" are still supported under non-default options. See the; `opaque pointers document <OpaquePointers.html>`__ for more information. .. _t_target_type:. Target Extension Type; """""""""""""""""""""""""""""""""""""""""". :Overview:. Target extension types represent types that must be preserved through; optimization, but are otherwise generally opaque to the compiler. They may be; used as function parameters or arguments, and in :ref:`phi <i_phi>` or; :ref:`select <i_select>` instructions. Some types may be also used in; :ref:`alloca <i_alloca>` instructions or as global values, and correspondingly; it is legal to use :ref:`load <i_load>` and :ref:`store <i_store>` instructions; on them. Full semantics for these types are defined by the target. The only constants that target extension types may have are ``zeroinitializer``,; ``undef``, and ``poison``. Other possible values for target extension types may; arise from target-specific intrinsics and functions. These types cannot be converted to other types. As such, it is not legal to use; them in :ref:`bitcast <i_bitcast>` instructions (as a source or target type),; nor is it legal to use them in :ref:`ptrtoint <i_ptrtoint>` or; :ref:`inttoptr <i_inttoptr>` instructions. Similarly, they are not legal to use; in an :ref:`icmp <i_icmp>`",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:172389,optimiz,optimization,172389,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['optimiz'],['optimization']
Performance,"special register or a hardware-loop instruction. '``llvm.start.loop.iterations.*``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". This is an overloaded intrinsic. ::. declare i32 @llvm.start.loop.iterations.i32(i32); declare i64 @llvm.start.loop.iterations.i64(i64). Overview:; """""""""""""""""". The '``llvm.start.loop.iterations.*``' intrinsics are similar to the; '``llvm.set.loop.iterations.*``' intrinsics, used to specify the; hardware-loop trip count but also produce a value identical to the input; that can be used as the input to the loop. They are placed in the loop; preheader basic block and the output is expected to be the input to the; phi for the induction variable of the loop, decremented by the; '``llvm.loop.decrement.reg.*``'. Arguments:; """""""""""""""""""". The integer operand is the loop trip count of the hardware-loop, and thus; not e.g. the loop back-edge taken count. Semantics:; """""""""""""""""""". The '``llvm.start.loop.iterations.*``' intrinsics do not perform any arithmetic; on their operand. It's a hint to the backend that can use this to set up the; hardware-loop count with a target specific instruction, usually a move of this; value to a special register or a hardware-loop instruction. '``llvm.test.set.loop.iterations.*``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". This is an overloaded intrinsic. ::. declare i1 @llvm.test.set.loop.iterations.i32(i32); declare i1 @llvm.test.set.loop.iterations.i64(i64). Overview:; """""""""""""""""". The '``llvm.test.set.loop.iterations.*``' intrinsics are used to specify the; the loop trip count, and also test that the given count is not zero, allowing; it to control entry to a while-loop. They are placed in the loop preheader's; predecessor basic block, and are marked as ``IntrNoDuplicate`` to avoid; optimizers duplicating these instructions. Arguments:; """""""""""""""""""". The integer operand is the loop trip count of the hardware-loop, and thus; not e.g. the loop back-edge taken coun",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:644994,perform,perform,644994,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['perform']
Performance,"specified; through ``--target=<architecture>`` or :option:`-arch` ``<architecture>``). If no; target is specified, the system default target will be used. .. option:: -mcpu=?, -mtune=?. Acts as an alias for :option:`--print-supported-cpus`. .. option:: -mcpu=help, -mtune=help. Acts as an alias for :option:`--print-supported-cpus`. .. option:: -march=<cpu>. Specify that Clang should generate code for a specific processor family; member and later. For example, if you specify -march=i486, the compiler is; allowed to generate instructions that are valid on i486 and later processors,; but which may not exist on earlier ones. Code Generation Options; ~~~~~~~~~~~~~~~~~~~~~~~. .. option:: -O0, -O1, -O2, -O3, -Ofast, -Os, -Oz, -Og, -O, -O4. Specify which optimization level to use:. :option:`-O0` Means ""no optimization"": this level compiles the fastest and; generates the most debuggable code. :option:`-O1` Somewhere between :option:`-O0` and :option:`-O2`. :option:`-O2` Moderate level of optimization which enables most; optimizations. :option:`-O3` Like :option:`-O2`, except that it enables optimizations that; take longer to perform or that may generate larger code (in an attempt to; make the program run faster). :option:`-Ofast` Enables all the optimizations from :option:`-O3` along; with other aggressive optimizations that may violate strict compliance with; language standards. :option:`-Os` Like :option:`-O2` with extra optimizations to reduce code; size. :option:`-Oz` Like :option:`-Os` (and thus :option:`-O2`), but reduces code; size further. :option:`-Og` Like :option:`-O1`. In future versions, this option might; disable different optimizations in order to improve debuggability. :option:`-O` Equivalent to :option:`-O1`. :option:`-O4` and higher. Currently equivalent to :option:`-O3`. .. option:: -g, -gline-tables-only, -gmodules. Control debug information output. Note that Clang debug information works; best at :option:`-O0`. When more than one option starting with `-g`",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/CommandGuide/clang.rst:10719,optimiz,optimization,10719,interpreter/llvm-project/clang/docs/CommandGuide/clang.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/CommandGuide/clang.rst,2,['optimiz'],"['optimization', 'optimizations']"
Performance,"specifies type 'char *' but the argument has type 'int' [-Wformat]; t.c:3:11: warning: conversion specifies type 'char *' but the argument has type 'int' [-Wformat,1]; t.c:3:11: warning: conversion specifies type 'char *' but the argument has type 'int' [-Wformat,Format String]. This category can be used by clients that want to group diagnostics; by category, so it should be a high level category. We want dozens; of these, not hundreds or thousands of them. .. _opt_fsave-optimization-record:. .. option:: -f[no-]save-optimization-record[=<format>]. Enable optimization remarks during compilation and write them to a separate; file. This option, which defaults to off, controls whether Clang writes; optimization reports to a separate file. By recording diagnostics in a file,; users can parse or sort the remarks in a convenient way. By default, the serialization format is YAML. The supported serialization formats are:. - .. _opt_fsave_optimization_record_yaml:. ``-fsave-optimization-record=yaml``: A structured YAML format. - .. _opt_fsave_optimization_record_bitstream:. ``-fsave-optimization-record=bitstream``: A binary format based on LLVM; Bitstream. The output file is controlled by :option:`-foptimization-record-file`. In the absence of an explicit output file, the file is chosen using the; following scheme:. ``<base>.opt.<format>``. where ``<base>`` is based on the output file of the compilation (whether; it's explicitly specified through `-o` or not) when used with `-c` or `-S`.; For example:. * ``clang -fsave-optimization-record -c in.c -o out.o`` will generate; ``out.opt.yaml``. * ``clang -fsave-optimization-record -c in.c`` will generate; ``in.opt.yaml``. When targeting (Thin)LTO, the base is derived from the output filename, and; the extension is not dropped. When targeting ThinLTO, the following scheme is used:. ``<base>.opt.<format>.thin.<num>.<format>``. Darwin-only: when used for generating a linked binary from a source file; (through an intermediate object fi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:11288,optimiz,optimization-record,11288,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,1,['optimiz'],['optimization-record']
Performance,"spill SGPR registers. In this; case the AMDGPU backend ensures the memory location used to spill is never; accessed by vector memory operations at the same time. If scalar writes are used; then a ``s_dcache_wb`` is inserted before the ``s_endpgm`` and before a function; return since the locations may be used for vector memory instructions by a; future wavefront that uses the same scratch area, or a function call that; creates a frame at the same address, respectively. There is no need for a; ``s_dcache_inv`` as all scalar writes are write-before-read in the same thread. For kernarg backing memory:. * CP invalidates the L1 cache at the start of each kernel dispatch.; * On dGPU over XGMI or PCIe the kernarg backing memory is allocated in host; memory accessed as MTYPE UC (uncached) to avoid needing to invalidate the L2; cache. This also causes it to be treated as non-volatile and so is not; invalidated by ``*_vol``.; * On APU the kernarg backing memory is accessed as MTYPE CC (cache coherent) and; so the L2 cache will be coherent with the CPU and other agents. Scratch backing memory (which is used for the private address space) is accessed; with MTYPE NC_NV (non-coherent non-volatile). Since the private address space is; only accessed by a single thread, and is always write-before-read, there is; never a need to invalidate these entries from the L1 cache. Hence all cache; invalidates are done as ``*_vol`` to only invalidate the volatile cache lines. The code sequences used to implement the memory model for GFX90A are defined; in table :ref:`amdgpu-amdhsa-memory-model-code-sequences-gfx90a-table`. .. table:: AMDHSA Memory Model Code Sequences GFX90A; :name: amdgpu-amdhsa-memory-model-code-sequences-gfx90a-table. ============ ============ ============== ========== ================================; LLVM Instr LLVM Memory LLVM Memory AMDGPU AMDGPU Machine Code; Ordering Sync Scope Address GFX90A; Space; ============ ============ ============== ========== ==================",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:241377,cache,cache,241377,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['cache'],['cache']
Performance,"spill SGPR registers. In this; case the AMDGPU backend ensures the memory location used to spill is never; accessed by vector memory operations at the same time. If scalar writes are used; then a ``s_dcache_wb`` is inserted before the ``s_endpgm`` and before a function; return since the locations may be used for vector memory instructions by a; future wavefront that uses the same scratch area, or a function call that; creates a frame at the same address, respectively. There is no need for a; ``s_dcache_inv`` as all scalar writes are write-before-read in the same thread. For kernarg backing memory:. * CP invalidates the L1 cache at the start of each kernel dispatch.; * On dGPU over XGMI or PCIe the kernarg backing memory is allocated in host; memory accessed as MTYPE UC (uncached) to avoid needing to invalidate the L2; cache. This also causes it to be treated as non-volatile and so is not; invalidated by ``*_vol``.; * On APU the kernarg backing memory is accessed as MTYPE CC (cache coherent) and; so the L2 cache will be coherent with the CPU and other agents. Scratch backing memory (which is used for the private address space) is accessed; with MTYPE NC_NV (non-coherent non-volatile). Since the private address space is; only accessed by a single thread, and is always write-before-read, there is; never a need to invalidate these entries from the L1 cache. Hence all cache; invalidates are done as ``*_vol`` to only invalidate the volatile cache lines. The code sequences used to implement the memory model for GFX940, GFX941, GFX942; are defined in table :ref:`amdgpu-amdhsa-memory-model-code-sequences-gfx940-gfx941-gfx942-table`. .. table:: AMDHSA Memory Model Code Sequences GFX940, GFX941, GFX942; :name: amdgpu-amdhsa-memory-model-code-sequences-gfx940-gfx941-gfx942-table. ============ ============ ============== ========== ================================; LLVM Instr LLVM Memory LLVM Memory AMDGPU AMDGPU Machine Code; Ordering Sync Scope Address GFX940, GFX941, GFX942; S",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:291395,cache,cache,291395,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['cache'],['cache']
Performance,"sponding to a metadata node with no; entries. If a load instruction tagged with the ``!invariant.load``; metadata is executed, the memory location referenced by the load has; to contain the same value at all points in the program where the; memory location is dereferenceable; otherwise, the behavior is; undefined. The optional ``!invariant.group`` metadata must reference a single metadata name; ``<empty_node>`` corresponding to a metadata node with no entries.; See ``invariant.group`` metadata :ref:`invariant.group <md_invariant.group>`. The optional ``!nonnull`` metadata must reference a single; metadata name ``<empty_node>`` corresponding to a metadata node with no; entries. The existence of the ``!nonnull`` metadata on the; instruction tells the optimizer that the value loaded is known to; never be null. If the value is null at runtime, a poison value is returned; instead. This is analogous to the ``nonnull`` attribute on parameters and; return values. This metadata can only be applied to loads of a pointer type. The optional ``!dereferenceable`` metadata must reference a single metadata; name ``<deref_bytes_node>`` corresponding to a metadata node with one ``i64``; entry.; See ``dereferenceable`` metadata :ref:`dereferenceable <md_dereferenceable>`. The optional ``!dereferenceable_or_null`` metadata must reference a single; metadata name ``<deref_bytes_node>`` corresponding to a metadata node with one; ``i64`` entry.; See ``dereferenceable_or_null`` metadata :ref:`dereferenceable_or_null; <md_dereferenceable_or_null>`. The optional ``!align`` metadata must reference a single metadata name; ``<align_node>`` corresponding to a metadata node with one ``i64`` entry.; The existence of the ``!align`` metadata on the instruction tells the; optimizer that the value loaded is known to be aligned to a boundary specified; by the integer value in the metadata node. The alignment must be a power of 2.; This is analogous to the ''align'' attribute on parameters and return val",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:416406,load,loads,416406,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['loads']
Performance,"ss an older store provided that the load does; not alias with the store.; 3. A younger store is not allowed to pass an older store.; 4. A younger store is not allowed to pass an older load. By default, the LSUnit optimistically assumes that loads do not alias; (`-noalias=true`) store operations. Under this assumption, younger loads are; always allowed to pass older stores. Essentially, the LSUnit does not attempt; to run any alias analysis to predict when loads and stores do not alias with; each other. Note that, in the case of write-combining memory, rule 3 could be relaxed to; allow reordering of non-aliasing store operations. That being said, at the; moment, there is no way to further relax the memory model (``-noalias`` is the; only option). Essentially, there is no option to specify a different memory; type (e.g., write-back, write-combining, write-through; etc.) and consequently; to weaken, or strengthen, the memory model. Other limitations are:. * The LSUnit does not know when store-to-load forwarding may occur.; * The LSUnit does not know anything about cache hierarchy and memory types.; * The LSUnit does not know how to identify serializing operations and memory; fences. The LSUnit does not attempt to predict if a load or store hits or misses the L1; cache. It only knows if an instruction ""MayLoad"" and/or ""MayStore."" For; loads, the scheduling model provides an ""optimistic"" load-to-use latency (which; usually matches the load-to-use latency for when there is a hit in the L1D). :program:`llvm-mca` does not (on its own) know about serializing operations or; memory-barrier like instructions. The LSUnit used to conservatively use an; instruction's ""MayLoad"", ""MayStore"", and unmodeled side effects flags to; determine whether an instruction should be treated as a memory-barrier. This was; inaccurate in general and was changed so that now each instruction has an; IsAStoreBarrier and IsALoadBarrier flag. These flags are mca specific and; default to false for every ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst:40964,load,load,40964,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,1,['load'],['load']
Performance,"ss of the code to be executed needs to be; different than the address where the trampoline is actually stored. This; intrinsic returns the executable address corresponding to ``tramp``; after performing the required machine specific adjustments. The pointer; returned can then be :ref:`bitcast and executed <int_trampoline>`. .. _int_vp:. Vector Predication Intrinsics; -----------------------------; VP intrinsics are intended for predicated SIMD/vector code. A typical VP; operation takes a vector mask and an explicit vector length parameter as in:. ::. <W x T> llvm.vp.<opcode>.*(<W x T> %x, <W x T> %y, <W x i1> %mask, i32 %evl). The vector mask parameter (%mask) always has a vector of `i1` type, for example; `<32 x i1>`. The explicit vector length parameter always has the type `i32` and; is an unsigned integer value. The explicit vector length parameter (%evl) is in; the range:. ::. 0 <= %evl <= W, where W is the number of vector elements. Note that for :ref:`scalable vector types <t_vector>` ``W`` is the runtime; length of the vector. The VP intrinsic has undefined behavior if ``%evl > W``. The explicit vector; length (%evl) creates a mask, %EVLmask, with all elements ``0 <= i < %evl`` set; to True, and all other lanes ``%evl <= i < W`` to False. A new mask %M is; calculated with an element-wise AND from %mask and %EVLmask:. ::. M = %mask AND %EVLmask. A vector operation ``<opcode>`` on vectors ``A`` and ``B`` calculates:. ::. A <opcode> B = { A[i] <opcode> B[i] M[i] = True, and; { undef otherwise. Optimization Hint; ^^^^^^^^^^^^^^^^^. Some targets, such as AVX512, do not support the %evl parameter in hardware.; The use of an effective %evl is discouraged for those targets. The function; ``TargetTransformInfo::hasActiveVectorLength()`` returns true when the target; has native support for %evl. .. _int_vp_select:. '``llvm.vp.select.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x i32> @llvm.vp.sel",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:692247,scalab,scalable,692247,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['scalab'],['scalable']
Performance,"ss space on; the fence need to; conservatively; always generate. If; fence had an; address space then; set to address; space of OpenCL; fence flag, or to; generic if both; local and global; flags are; specified.; - Could be split into; separate s_waitcnt; vmcnt(0), s_waitcnt; vscnt(0) and s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/load; atomic/; atomicrmw-with-return-value.; - s_waitcnt vscnt(0); must happen after; any preceding; global/generic; store/store atomic/; atomicrmw-no-return-value.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store atomic/; atomicrmw.; - Must happen before; any following store; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - Ensures that all; memory operations; have; completed before; performing the; following; fence-paired-atomic. fence release - agent *none* 1. s_waitcnt lgkmcnt(0) &; - system vmcnt(0) & vscnt(0). - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0) and vscnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate. If; fence had an; address space then; set to address; space of OpenCL; fence flag, or to; generic if both; local and global; flags are; specified.; - Could be split into; separate s_waitcnt; vmcnt(0), s_waitcnt; vscnt(0) and s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/load atomic/; atomicrmw-with-return-value.; - s_waitcnt vscnt(0); must happen after; any preceding; global/generic; store/store atomic/; atomicrmw-no-return-value.; - s_waitcnt lgkmcnt(0); must happen aft",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:361438,perform,performing,361438,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['perform'],['performing']
Performance,"ss space on; the fence need to; conservatively; always generate. If; fence had an; address space then; set to address; space of OpenCL; fence flag, or to; generic if both; local and global; flags are; specified.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load; atomic/; atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - Must happen before; the following; buffer_wbinvl1_vol and; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than the; value read by the; fence-paired-atomic. 2. buffer_wbinvl1_vol. - If not TgSplit execution; mode, omit.; - Ensures that; following; loads will not see; stale data. fence acquire - agent *none* 1. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate; (see comment for; previous fence).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - Must happen before; the following; b",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:253826,load,loads,253826,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['loads']
Performance,"ss works, you should inherit from the :ref:`ModulePass; <writing-an-llvm-pass-ModulePass>` , :ref:`CallGraphSCCPass; <writing-an-llvm-pass-CallGraphSCCPass>`, :ref:`FunctionPass; <writing-an-llvm-pass-FunctionPass>` , or :ref:`LoopPass; <writing-an-llvm-pass-LoopPass>`, or :ref:`RegionPass; <writing-an-llvm-pass-RegionPass>` classes, which gives the system more; information about what your pass does, and how it can be combined with other; passes. One of the main features of the LLVM Pass Framework is that it; schedules passes to run in an efficient way based on the constraints that your; pass meets (which are indicated by which class they derive from). We start by showing you how to construct a pass, everything from setting up the; code, to compiling, loading, and executing it. After the basics are down, more; advanced features are discussed. .. warning::; This document deals with the legacy pass manager. LLVM uses the new pass; manager for the optimization pipeline (the codegen pipeline; still uses the legacy pass manager), which has its own way of defining; passes. For more details, see :doc:`WritingAnLLVMNewPMPass` and; :doc:`NewPassManager`. Quick Start --- Writing hello world; ===================================. Here we describe how to write the ""hello world"" of passes. The ""Hello"" pass is; designed to simply print out the name of non-external functions that exist in; the program being compiled. It does not modify the program at all, it just; inspects it. The source code and files for this pass are available in the LLVM; source tree in the ``lib/Transforms/Hello`` directory. .. _writing-an-llvm-pass-makefile:. Setting up the build environment; --------------------------------. First, configure and build LLVM. Next, you need to create a new directory; somewhere in the LLVM source base. For this example, we'll assume that you; made ``lib/Transforms/Hello``. Finally, you must set up a build script; that will compile the source code for the new pass. To do this,; c",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst:1722,optimiz,optimization,1722,interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst,1,['optimiz'],['optimization']
Performance,"ss; from the stack), we can compute the expected return address prior to the call; into a register preserved across the call and use that similarly to the above. Indirect calls (and returns in the absence of a red zone ABI) pose the most; significant challenge to propagate. The simplest technique would be to define a; new ABI such that the intended call target is passed into the called function; and checked in the entry. Unfortunately, new ABIs are quite expensive to deploy; in C and C++. While the target function could be passed in TLS, we would still; require complex logic to handle a mixture of functions compiled with and; without this extra logic (essentially, making the ABI backwards compatible).; Currently, we suggest using retpolines here and will continue to investigate; ways of mitigating this. ##### Optimizations, Alternatives, and Tradeoffs. Merely accumulating predicate state involves significant cost. There are; several key optimizations we employ to minimize this and various alternatives; that present different tradeoffs in the generated code. First, we work to reduce the number of instructions used to track the state:; * Rather than inserting a `cmovCC` instruction along every conditional edge in; the original program, we track each set of condition flags we need to capture; prior to entering each basic block and reuse a common `cmovCC` sequence for; those.; * We could further reuse suffixes when there are multiple `cmovCC`; instructions required to capture the set of flags. Currently this is; believed to not be worth the cost as paired flags are relatively rare and; suffixes of them are exceedingly rare.; * A common pattern in x86 is to have multiple conditional jump instructions; that use the same flags but handle different conditions. Naively, we could; consider each fallthrough between them an ""edge"" but this causes a much more; complex control flow graph. Instead, we accumulate the set of conditions; necessary for fallthrough and use a sequence o",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md:20680,optimiz,optimizations,20680,interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,1,['optimiz'],['optimizations']
Performance,"ss; space*. The generic address space uses the hardware flat address support for two fixed; ranges of virtual addresses (the private and local apertures), that are; outside the range of addressable global memory, to map from a flat address to; a private or local address. This uses FLAT instructions that can take a flat; address and access global, private (scratch), and group (LDS) memory depending; on if the address is within one of the aperture ranges. Flat access to scratch requires hardware aperture setup and setup in the; kernel prologue (see :ref:`amdgpu-amdhsa-kernel-prolog-flat-scratch`). Flat; access to LDS requires hardware aperture setup and M0 (GFX7-GFX8) register; setup (see :ref:`amdgpu-amdhsa-kernel-prolog-m0`). To convert between a private or group address space address (termed a segment; address) and a flat address the base address of the corresponding aperture; can be used. For GFX7-GFX8 these are available in the; :ref:`amdgpu-amdhsa-hsa-aql-queue` the address of which can be obtained with; Queue Ptr SGPR (see :ref:`amdgpu-amdhsa-initial-kernel-execution-state`). For; GFX9-GFX11 the aperture base addresses are directly available as inline; constant registers ``SRC_SHARED_BASE/LIMIT`` and ``SRC_PRIVATE_BASE/LIMIT``.; In 64-bit address mode the aperture sizes are 2^32 bytes and the base is; aligned to 2^32 which makes it easier to convert from flat to segment or; segment to flat. A global address space address has the same value when used as a flat address; so no conversion is needed. **Global and Constant**; The global and constant address spaces both use global virtual addresses,; which are the same virtual address space used by the CPU. However, some; virtual addresses may only be accessible to the CPU, some only accessible; by the GPU, and some by both. Using the constant address space indicates that the data will not change; during the execution of the kernel. This allows scalar read instructions to; be used. As the constant address space could ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:24074,queue,queue,24074,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['queue'],['queue']
Performance,"ssageExpr(matchesSelector(hasUnarySelector());; matches self.bodyView in the code below, but NOT the outer message; invocation of ""loadHTMLString:baseURL:"".; [self.bodyView loadHTMLString:html baseURL:NULL];. Matcher<ObjCMessageExpr>isClassMessage; Returns true when the Objective-C message is sent to a class. Example; matcher = objcMessageExpr(isClassMessage()); matches; [NSString stringWithFormat:@""format""];; but not; NSString *x = @""hello"";; [x containsString:@""h""];. Matcher<ObjCMessageExpr>isInstanceMessage; Returns true when the Objective-C message is sent to an instance. Example; matcher = objcMessageExpr(isInstanceMessage()); matches; NSString *x = @""hello"";; [x containsString:@""h""];; but not; [NSString stringWithFormat:@""format""];. Matcher<ObjCMessageExpr>matchesSelectorStringRef RegExp, Regex::RegexFlags Flags = NoFlags; Matches ObjC selectors whose name contains; a substring matched by the given RegExp.; matcher = objCMessageExpr(matchesSelector(""loadHTMLStringmatches the outer message expr in the code below, but NOT the message; invocation for self.bodyView.; [self.bodyView loadHTMLString:html baseURL:NULL];. If the matcher is used in clang-query, RegexFlags parameter; should be passed as a quoted string. e.g: ""NoFlags"".; Flags can be combined with '|' example ""IgnoreCase | BasicRegex"". Matcher<ObjCMessageExpr>numSelectorArgsunsigned N; Matches when the selector has the specified number of arguments. matcher = objCMessageExpr(numSelectorArgs(0));; matches self.bodyView in the code below. matcher = objCMessageExpr(numSelectorArgs(2));; matches the invocation of ""loadHTMLString:baseURL:"" but not that; of self.bodyView; [self.bodyView loadHTMLString:html baseURL:NULL];. Matcher<ObjCMethodDecl>isClassMethod; Returns true when the Objective-C method declaration is a class method. Example; matcher = objcMethodDecl(isClassMethod()); matches; @interface I + (void)foo; @end; but not; @interface I - (void)bar; @end. Matcher<ObjCMethodDecl>isDefinition; Matches if a ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LibASTMatchersReference.html:107992,load,loadHTMLStringmatches,107992,interpreter/llvm-project/clang/docs/LibASTMatchersReference.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LibASTMatchersReference.html,2,['load'],['loadHTMLStringmatches']
Performance,"ssarily imply an; ordering between threads for the purposes of the memory model. Therefore,; an operation can be both `convergent` and `nosync`. If a `nosync` function does ever synchronize with another thread,; the behavior is undefined.; ``nounwind``; This function attribute indicates that the function never raises an; exception. If the function does raise an exception, its runtime; behavior is undefined. However, functions marked nounwind may still; trap or generate asynchronous exceptions. Exception handling schemes; that are recognized by LLVM to handle asynchronous exceptions, such; as SEH, will still provide their implementation defined semantics.; ``nosanitize_bounds``; This attribute indicates that bounds checking sanitizer instrumentation; is disabled for this function.; ``nosanitize_coverage``; This attribute indicates that SanitizerCoverage instrumentation is disabled; for this function.; ``null_pointer_is_valid``; If ``null_pointer_is_valid`` is set, then the ``null`` address; in address-space 0 is considered to be a valid address for memory loads and; stores. Any analysis or optimization should not treat dereferencing a; pointer to ``null`` as undefined behavior in this function.; Note: Comparing address of a global variable to ``null`` may still; evaluate to false because of a limitation in querying this attribute inside; constant expressions.; ``optdebug``; This attribute suggests that optimization passes and code generator passes; should make choices that try to preserve debug info without significantly; degrading runtime performance.; This attribute is incompatible with the ``minsize``, ``optsize``, and; ``optnone`` attributes.; ``optforfuzzing``; This attribute indicates that this function should be optimized; for maximum fuzzing signal.; ``optnone``; This function attribute indicates that most optimization passes will skip; this function, with the exception of interprocedural optimization passes.; Code generation defaults to the ""fast"" instruction",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:95021,load,loads,95021,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['loads']
Performance,"ssembly.github.io/spec/core/>`_.; In this section, when we refer to reference types, we are referring to; WebAssembly reference types, not C++ reference types unless stated; otherwise. ``__builtin_wasm_table_set``; ----------------------------. This builtin function stores a value in a WebAssembly table.; It takes three arguments.; The first argument is the table to store a value into, the second; argument is the index to which to store the value into, and the; third argument is a value of reference type to store in the table.; It returns nothing. .. code-block:: c++. static __externref_t table[0];; extern __externref_t JSObj;. void store(int index) {; __builtin_wasm_table_set(table, index, JSObj);; }. ``__builtin_wasm_table_get``; ----------------------------. This builtin function is the counterpart to ``__builtin_wasm_table_set``; and loads a value from a WebAssembly table of reference typed values.; It takes 2 arguments.; The first argument is a table of reference typed values and the; second argument is an index from which to load the value. It returns; the loaded reference typed value. .. code-block:: c++. static __externref_t table[0];. __externref_t load(int index) {; __externref_t Obj = __builtin_wasm_table_get(table, index);; return Obj;; }. ``__builtin_wasm_table_size``; -----------------------------. This builtin function returns the size of the WebAssembly table.; Takes the table as an argument and returns an unsigned integer (``size_t``); with the current table size. .. code-block:: c++. typedef void (*__funcref funcref_t)();; static __funcref table[0];. size_t getSize() {; return __builtin_wasm_table_size(table);; }. ``__builtin_wasm_table_grow``; -----------------------------. This builtin function grows the WebAssembly table by a certain amount.; Currently, as all WebAssembly tables created in C/C++ are zero-sized,; this always needs to be called to grow the table. It takes three arguments. The first argument is the WebAssembly table; to grow. The s",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst:92862,load,load,92862,interpreter/llvm-project/clang/docs/LanguageExtensions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst,1,['load'],['load']
Performance,"sses may only be accessible to the CPU, some only accessible; by the GPU, and some by both. Using the constant address space indicates that the data will not change; during the execution of the kernel. This allows scalar read instructions to; be used. As the constant address space could only be modified on the host; side, a generic pointer loaded from the constant address space is safe to be; assumed as a global pointer since only the device global memory is visible; and managed on the host side. The vector and scalar L1 caches are invalidated; of volatile data before each kernel dispatch execution to allow constant; memory to change values between kernel dispatches. **Region**; The region address space uses the hardware Global Data Store (GDS). All; wavefronts executing on the same device will access the same memory for any; given region address. However, the same region address accessed by wavefronts; executing on different devices will access different memory. It is higher; performance than global memory. It is allocated by the runtime. The data; store (DS) instructions can be used to access it. **Local**; The local address space uses the hardware Local Data Store (LDS) which is; automatically allocated when the hardware creates the wavefronts of a; work-group, and freed when all the wavefronts of a work-group have; terminated. All wavefronts belonging to the same work-group will access the; same memory for any given local address. However, the same local address; accessed by wavefronts belonging to different work-groups will access; different memory. It is higher performance than global memory. The data store; (DS) instructions can be used to access it. **Private**; The private address space uses the hardware scratch memory support which; automatically allocates memory when it creates a wavefront and frees it when; a wavefronts terminates. The memory accessed by a lane of a wavefront for any; given private address will be different to the memory accessed by anot",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:25804,perform,performance,25804,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['perform'],['performance']
Performance,"sses ptr, ptr+1, ptr+2 and places them in lanes 0, 3 and 7 accordingly. The masked-off lanes are filled by elements from the corresponding lanes of the '``passthru``' operand. Arguments:; """""""""""""""""""". The first operand is the base pointer for the load. It has the same underlying type as the element of the returned vector. The second operand, mask, is a vector of boolean values with the same number of elements as the return type. The third is a pass-through value that is used to fill the masked-off lanes of the result. The return type and the type of the '``passthru``' operand have the same vector type. Semantics:; """""""""""""""""""". The '``llvm.masked.expandload``' intrinsic is designed for reading multiple scalar values from adjacent memory addresses into possibly non-adjacent vector lanes. It is useful for targets that support vector expanding loads and allows vectorizing loop with cross-iteration dependency like in the following example:. .. code-block:: c. // In this loop we load from B and spread the elements into array A.; double *A, B; int *C;; for (int i = 0; i < size; ++i) {; if (C[i] != 0); A[i] = B[j++];; }. .. code-block:: llvm. ; Load several elements from array B and expand them in a vector.; ; The number of loaded elements is equal to the number of '1' elements in the Mask.; %Tmp = call <8 x double> @llvm.masked.expandload.v8f64(ptr %Bptr, <8 x i1> %Mask, <8 x double> poison); ; Store the result in A; call void @llvm.masked.store.v8f64.p0(<8 x double> %Tmp, ptr %Aptr, i32 8, <8 x i1> %Mask). ; %Bptr should be increased on each iteration according to the number of '1' elements in the Mask.; %MaskI = bitcast <8 x i1> %Mask to i8; %MaskIPopcnt = call i8 @llvm.ctpop.i8(i8 %MaskI); %MaskI64 = zext i8 %MaskIPopcnt to i64; %BNextInd = add i64 %BInd, %MaskI64. Other targets may support this intrinsic differently, for example, by lowering it into a sequence of conditional scalar load operations and shuffles.; If all mask elements are '1', the intrinsic behavior is equi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:857098,load,load,857098,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,"ssible classification scheme to; describe thread-safety of libraries:. - All public and protected functions are reentrant. The library; provides protection against multiple threads trying to modify static; and global data used within a library. The developer must explicitly; lock access to objects shared between threads. No other thread can; write to a locked object unless it is unlocked. The developer needs; to lock local objects. The spirit, if not the letter of this; definition, requires the user of the library only to be familiar; with the semantic content of the objects in use. Locking access to; objects that are being shared due to extra-semantic details of; implementation (for example, copy-on-write) should remain the; responsibility of the library. - All public and protected functions are reentrant. The library; provides protection against multiple threads trying to modify static; and global data used within the library. The preferred way of; providing this protection is to use mutex locks. The library also; locks an object before writing to it. The developer is not required; to explicitly lock or unlock a class object (static, global or; local) to perform a single operation on the object. Note that even; multithread safe level II hardly relieves the user of the library; from the burden of locking. A thread suffers from **`deadlock`** if it is blocked waiting for a; condition that will never occur. Typically, this occurs when one thread; needs to access a resource that is already locked by another thread, and; that other thread is trying to access a resource that has already been; locked by the first thread. In this situation, neither thread is able to; progress; they are deadlocked. A **`multiprocessor`** is a hardware system with multiple processors or; multiple, simultaneous execution units. - Examples can be found at; <http://www-linux.gsi.de/~go4/HOWTOthreads/howtothreadsbody.html>; (the thread authors' web site - Jörn Adamczewski and Marc; Hemberger). ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Threads.md:24556,perform,perform,24556,documentation/users-guide/Threads.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Threads.md,1,['perform'],['perform']
Performance,"ssible to move loads from after a Release store or read-modify-write; operation to before it, and move non-Release stores from after a Release; operation to before it. Notes for code generation; See the section on Acquire; a fence before the relevant operation is usually; sufficient for Release. Note that a store-store fence is not sufficient to; implement Release semantics; store-store fences are generally not exposed to; IR because they are extremely difficult to use correctly. AcquireRelease; --------------. AcquireRelease (``acq_rel`` in IR) provides both an Acquire and a Release; barrier (for fences and operations which both read and write memory). Relevant standard; This corresponds to the C++/C ``memory_order_acq_rel``. Notes for frontends; If you are writing a frontend which uses this directly, use with caution.; Acquire only provides a semantic guarantee when paired with a Release; operation, and vice versa. Notes for optimizers; In general, optimizers should treat this like a nothrow call; the possible; optimizations are usually not interesting. Notes for code generation; This operation has Acquire and Release semantics; see the sections on Acquire; and Release. SequentiallyConsistent; ----------------------. SequentiallyConsistent (``seq_cst`` in IR) provides Acquire semantics for loads; and Release semantics for stores. Additionally, it guarantees that a total; ordering exists between all SequentiallyConsistent operations. Relevant standard; This corresponds to the C++/C ``memory_order_seq_cst``, Java volatile, and; the gcc-compatible ``__sync_*`` builtins which do not specify otherwise. Notes for frontends; If a frontend is exposing atomic operations, these are much easier to reason; about for the programmer than other kinds of operations, and using them is; generally a practical performance tradeoff. Notes for optimizers; Optimizers not aware of atomics can treat this like a nothrow call. For; SequentiallyConsistent loads and stores, the same reordering",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst:14124,optimiz,optimizers,14124,interpreter/llvm-project/llvm/docs/Atomics.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst,3,['optimiz'],"['optimizations', 'optimizers']"
Performance,"ssible values for ``N`` are:. - 0:; Use one thread per physical core (default); - 1:; Use a single thread only (disable multi-threading); - all:; Use one thread per logical core (uses all hyper-threads). Incremental; -----------; .. _incremental:. ThinLTO supports fast incremental builds through the use of a cache,; which currently must be enabled through a linker option. - gold (as of LLVM 4.0):; ``-Wl,-plugin-opt,cache-dir=/path/to/cache``; - ld64 (supported since clang 3.9 and Xcode 8) and Mach-O ld64.lld (as of LLVM; 15.0):; ``-Wl,-cache_path_lto,/path/to/cache``; - ELF ld.lld (as of LLVM 5.0):; ``-Wl,--thinlto-cache-dir=/path/to/cache``; - COFF lld-link (as of LLVM 6.0):; ``/lldltocache:/path/to/cache``. Cache Pruning; -------------. To help keep the size of the cache under control, ThinLTO supports cache; pruning. Cache pruning is supported with gold, ld64, and lld, but currently only; gold and lld allow you to control the policy with a policy string. The cache; policy must be specified with a linker option. - gold (as of LLVM 6.0):; ``-Wl,-plugin-opt,cache-policy=POLICY``; - ELF ld.lld (as of LLVM 5.0), Mach-O ld64.lld (as of LLVM 15.0):; ``-Wl,--thinlto-cache-policy=POLICY``; - COFF lld-link (as of LLVM 6.0):; ``/lldltocachepolicy:POLICY``. A policy string is a series of key-value pairs separated by ``:`` characters.; Possible key-value pairs are:. - ``cache_size=X%``: The maximum size for the cache directory is ``X`` percent; of the available space on the disk. Set to 100 to indicate no limit,; 50 to indicate that the cache size will not be left over half the available; disk space. A value over 100 is invalid. A value of 0 disables the percentage; size-based pruning. The default is 75%. - ``cache_size_bytes=X``, ``cache_size_bytes=Xk``, ``cache_size_bytes=Xm``,; ``cache_size_bytes=Xg``:; Sets the maximum size for the cache directory to ``X`` bytes (or KB, MB,; GB respectively). A value over the amount of available space on the disk; will be reduced to the a",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ThinLTO.rst:5300,cache,cache,5300,interpreter/llvm-project/clang/docs/ThinLTO.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ThinLTO.rst,1,['cache'],['cache']
Performance,"ssibly bitcasted) value; produced by the call, undef, or void.; - The calling conventions of the caller and callee must match.; - The callee must be varargs iff the caller is varargs. Bitcasting a; non-varargs function to the appropriate varargs type is legal so; long as the non-varargs prefixes obey the other rules.; - The return type must not undergo automatic conversion to an `sret` pointer. In addition, if the calling convention is not `swifttailcc` or `tailcc`:. - All ABI-impacting function attributes, such as sret, byval, inreg,; returned, and inalloca, must match.; - The caller and callee prototypes must match. Pointer types of parameters; or return types may differ in pointee type, but not in address space. On the other hand, if the calling convention is `swifttailcc` or `swiftcc`:. - Only these ABI-impacting attributes attributes are allowed: sret, byval,; swiftself, and swiftasync.; - Prototypes are not required to match. Tail call optimization for calls marked ``tail`` is guaranteed to occur if; the following conditions are met:. - Caller and callee both have the calling convention ``fastcc`` or ``tailcc``.; - The call is in tail position (ret immediately follows call and ret; uses value of call or is void).; - Option ``-tailcallopt`` is enabled,; ``llvm::GuaranteedTailCallOpt`` is ``true``, or the calling convention; is ``tailcc``; - `Platform-specific constraints are; met. <CodeGenerator.html#tailcallopt>`_. #. The optional ``notail`` marker indicates that the optimizers should not add; ``tail`` or ``musttail`` markers to the call. It is used to prevent tail; call optimization from being performed on the call. #. The optional ``fast-math flags`` marker indicates that the call has one or more; :ref:`fast-math flags <fastmath>`, which are optimization hints to enable; otherwise unsafe floating-point optimizations. Fast-math flags are only valid; for calls that return a floating-point scalar or vector type, or an array; (nested to any depth) of floating-poi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:474942,optimiz,optimization,474942,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['optimiz'],['optimization']
Performance,"ssing a user defined PDF or CDF.; Example 1: perform a 2 sample GoF test from two arrays,; sample1[n1] and sample2[n2] containing the data; ; ROOT::Math::GoFTest goftest(n1, sample1, n2, sample2);; double pValueAD = goftest.AndersonDarling2SamplesTest();; double pValueKS = goftest.KolmogorovSmirnov2SamplesTest();; ; The class can return optionally also the test statistics instead of; the p value.; Example 2: perform a 1 sample test with a pre-defined; distribution starting from a data set sample[n]. ROOT::Math::GoFTest goftest(n, sample, ROOT::Math::GoFTest::kGaussian);; double pValueAD = goftest.AndersonDarlingTest();; double pValueKS = goftest.KolmogorovSmirnovTest();; . Example 3: perform a 1 sample test with a user-defined; distribution provided as cdf; ; ROOT::Math::Functor1D cdf_func(&ROOT::Math::landau_cdf);; ROOT::Math::GofTest goftest(n, sample, cdf_func, ROOT::Math::GoFTest::kCDF);; double pValueAD = goftest.AndersonDarlingTest();; . Example 4: perform a 1 sample test with a user-defined; distribution provided as pdf. Note that in this case to avoid; integration problems is sometimes recommended to give some; reasonable xmin and xmax values. xmin (and xmax) should however be; smaller (larger) than the minimum (maximum) data value.; ; ROOT::Math::Functor1D pdf_func(&ROOT::Math::landau_pdf);; double xmin = 5*TMath::Min_Element(n,sample);; double xmax = 5*TMath::Max_Element(n,sample);; ROOT::Math::GofTest goftest(n, sample, pdf_func, ROOT::Math::GoFTest::kPDF,xmin,xmax);; double pValueAD = goftest.AndersonDarlingTest();; . The tutorial math/goftest.C is an example on; how to use the ROOT::Math::GofTest class. New class TKDTreeBinning for binning multidimensional data.; ; The class implements multidimensional binning by constructing a; TKDTree inner structure form the data which is used as the bins.; The bins are retrieved as two double*, one for the minimum bin edges,; the other as the maximum bin edges. For one dimension one of these is enough; to correctly ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/math/doc/v528/index.html:2075,perform,perform,2075,math/doc/v528/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/math/doc/v528/index.html,2,['perform'],['perform']
Performance,"ssion object (``ExecutionSession``). Compiling eagerly by default makes it; easy to use ORC as an in-memory compiler for an existing JIT (similar to how; MCJIT is commonly used). However ORC also provides built-in support for lazy; compilation via lazy-reexports (see :ref:`Laziness`). **Support for Custom Compilers and Program Representations**; Clients can supply custom compilers for each symbol that they define in their; JIT session. ORC will run the user-supplied compiler when the a definition of; a symbol is needed. ORC is actually fully language agnostic: LLVM IR is not; treated specially, and is supported via the same wrapper mechanism (the; ``MaterializationUnit`` class) that is used for custom compilers. **Concurrent JIT'd code** and **Concurrent Compilation**; JIT'd code may be executed in multiple threads, may spawn new threads, and may; re-enter the ORC (e.g. to request lazy compilation) concurrently from multiple; threads. Compilers launched my ORC can run concurrently (provided the client; sets up an appropriate dispatcher). Built-in dependency tracking ensures that; ORC does not release pointers to JIT'd code or data until all dependencies; have also been JIT'd and they are safe to call or use. **Removable Code**; Resources for JIT'd program representations. **Orthogonality** and **Composability**; Each of the features above can be used independently. It is possible to put; ORC components together to make a non-lazy, in-process, single threaded JIT; or a lazy, out-of-process, concurrent JIT, or anything in between. LLJIT and LLLazyJIT; ===================. ORC provides two basic JIT classes off-the-shelf. These are useful both as; examples of how to assemble ORC components to make a JIT, and as replacements; for earlier LLVM JIT APIs (e.g. MCJIT). The LLJIT class uses an IRCompileLayer and RTDyldObjectLinkingLayer to support; compilation of LLVM IR and linking of relocatable object files. All operations; are performed eagerly on symbol lookup (i.e. a sy",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:2984,concurren,concurrently,2984,interpreter/llvm-project/llvm/docs/ORCv2.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst,1,['concurren'],['concurrently']
Performance,"ssions are composed of 64-bit integer operands and operations.; Operands include :ref:`integer numbers<amdgpu_synid_integer_number>`; and :ref:`symbols<amdgpu_synid_symbol>`. Expressions may also use ""."" which is a reference; to the current PC (program counter). :ref:`Unary<amdgpu_synid_expression_un_op>` and; :ref:`binary<amdgpu_synid_expression_bin_op>`; operations produce 64-bit integer results. Syntax of Expressions; ---------------------. Syntax of expressions is shown below::. expr ::= expr binop expr | primaryexpr ;. primaryexpr ::= '(' expr ')' | symbol | number | '.' | unop primaryexpr ;. binop ::= '&&'; | '||'; | '|'; | '^'; | '&'; | '!'; | '=='; | '!='; | '<>'; | '<'; | '<='; | '>'; | '>='; | '<<'; | '>>'; | '+'; | '-'; | '*'; | '/'; | '%' ;. unop ::= '~'; | '+'; | '-'; | '!' ;. .. _amdgpu_synid_expression_bin_op:. Binary Operators; ----------------. Binary operators are described in the following table.; They operate on and produce 64-bit integers.; Operators with higher priority are performed first. ========== ========= ===============================================; Operator Priority Meaning; ========== ========= ===============================================; \* 5 Integer multiplication.; / 5 Integer division.; % 5 Integer signed remainder.; \+ 4 Integer addition.; \- 4 Integer subtraction.; << 3 Integer shift left.; >> 3 Logical shift right.; == 2 Equality comparison.; != 2 Inequality comparison.; <> 2 Inequality comparison.; < 2 Signed less than comparison.; <= 2 Signed less than or equal comparison.; > 2 Signed greater than comparison.; >= 2 Signed greater than or equal comparison.; \| 1 Bitwise or.; ^ 1 Bitwise xor.; & 1 Bitwise and.; && 0 Logical and.; || 0 Logical or.; ========== ========= ===============================================. .. _amdgpu_synid_expression_un_op:. Unary Operators; ---------------. Unary operators are described in the following table.; They operate on and produce 64-bit integers. ========== =============================",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUOperandSyntax.rst:31677,perform,performed,31677,interpreter/llvm-project/llvm/docs/AMDGPUOperandSyntax.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUOperandSyntax.rst,1,['perform'],['performed']
Performance,"st char *newroot,; const char *oldroot = 0, const char *enlnm = 0); Relocates all paths starting with 'oldroot' to 'newroot' for the; entry-list 'enlnm' in file 'fn'. Remove 'protocol+server' from file tagging and matching, i.e. use; only filepath+anchor; in this way a list is valid even after re-staging; of the dataset files, which typically changes the end-point data servers.; Entry-lists created with the full path should still be matched correctly. Miscellaneous. Repaired the behavior of TTreeCache when the TTree has a dramatic dynamic range with a lots of very small entriesat the beginning and very large entries at the end, the size in bytes of the cluster for the later entries will be very large (because of the cluster size in entries is large!). TTreeCache::FillBuffer was always attempting to load complete clusters not matter the; size (even with the size was larger than 2GB!). This patch resolves the issue by limiting the amount of memory used to:. The requested size if more than one cluster fits in the cache.; Twice the requested size if at least one basket per branch fits in the cache.; Four time the requested size in the case where the cache can not even hold one basket per branch. The filling will restart at the next cluster boundary in the case a) and will; restart at the maximum of entry number read in the cache in the case b) and c).; Baskets that are below this boundary and did not fit in the cache will be read; individually.; Repaired the basket flushing frequency when the TTree has already more than one cluster size.; Repaired binning of string histogram generated by TTree::Draw.; Many bug fixes and fix for issues discovery by Coverity, see change log for more details.; In TTree::MakeProxy add proper support for top level stl collection of objects and for stl collection of objects that are 'empty' in the file (and thus we know nothing about its content).; Avoid deficiency in hadd when the resulting TTree is longer than the AutoSave length *and* the T",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/doc/v534/index.html:3255,cache,cache,3255,tree/doc/v534/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/doc/v534/index.html,2,['cache'],['cache']
Performance,"st element of the sequence at position zero. This type is used throughout the ROOT code to avoid copying strings when a; sub-string is needed and to extent interfaces that uses to take a const char*; to take a std::string_view as thus be able to be directly directly passed a; TString, a std::string or a std::string_view. Usage example:. ``` {.cpp}; // With SetName(std::string_view); std::string str; …; obj.SetName( str );; obj.SetName( {str.data()+pos, len} );; ```. ### Meta library. #### Backward Incompatibilities. TIsAProxy's constructor no longer take the optional and unused 2nd argument which was reserved for a 'context'. This context was unused in TIsAProxy itself and was not accessible from derived classes. #### Interpreter. The new interface `TInterpreter::Declare(const char* code)` will declare the; code to the interpreter with all interpreter extensions disabled, i.e. as; ""proper"" C++ code. No autoloading or synamic lookup will be performed. A new R__LOAD_LIBRARY(libWhatever) will load libWhatever at parse time. This allows ROOT to resolve symbols from this library very early on. It is a work-around for the following code from ROOT 5:. ``` {.cpp}; // ROOT 5:; void func() {; gSystem->Load(""libEvent"");; Event* e = new Event;; }; ```. Instead, write:. ``` {.cpp}; // ROOT 6:; R__LOAD_LIBRARY(libEvent); #include ""Event.h"". void func() {; Event* e = new Event;; }; ```. #### TClass. Introduced new overload for calculating the TClass CheckSum:. ``` {.cpp}; UInt_t TClass::GetCheckSum(ECheckSum code, Bool_t &isvalid) const;; ```. which indicates via the 'isvalid' boolean whether the checksum could be; calculated correctly or not. ### TROOT. Implemented new gROOT->GetTutorialsDir() static method to return the actual location of the tutorials directory.; This is $ROOTSYS/tutorials when not configuring with --prefix or -Dgnuinstall for CMake. ### TColor. Add an enum to access the palette by name. Add new palettes with 255 colors. Names and colors' definitions have been ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v604/index.md:2980,load,load,2980,README/ReleaseNotes/v604/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v604/index.md,1,['load'],['load']
Performance,"st entry; IDs. You can add or remove a list box entry using them in the following; way:. ``` {.cpp}; // adding an entry; fLastEntry++;; sprintf(tmp, ""Entry %i"", fLastEntry);; fListBox->AddEntry(tmp, fLastEntry);; fListBox->MapSubwindows();; fListBox->Layout();; . . .; // removing an entry; if (fFirstEntry < fLastEntry) {; fListBox->RemoveEntry(fFirstEntry);; fListBox->Layout();; fFirstEntry++;; }; ```. A single-selection list box is used for selecting only one item in a; list. A multiple-selection list box permits selection of more than one item.; The selected choices should be visible - you have several choices to do; this:. - to mark selected choices with a check mark or highlight them. - to provide a summary list box to the right of the list box,; containing the selected choices. - to provide a display-only text control indicating the number of; selected choices (its position should be justified upper-right above; the list box). - if the actions `Select All` or `Deselect All` must be quickly or; frequently performed, use command buttons. ### Combo Boxes. A combo box is as single-selection list box that shows only the; currently selected entry and a prompt button displayed as a downward; arrow. The prompt button provides a visual cue that a list box is; hidden. Its main advantage is consuming of quite a bit of screen space.; When the user clicks on it, a list pops up, from which a new choice can; be made. After a new item is chosen the combo box folds again showing; the new selection. ![](pictures/0200021A.jpg). The combo box widget is represented by the user callable class; **`TGComboBox`**. The class **`TGComboBoxPopup`** is a service class.; The combo box constructor is very similar to the list box one. The first; parameter is a parent widget pointer again, the second - an integer; value that will be used as combo box ID. The method used for adding; entries is very similar to the list box method we used before. The; method `Select(entryID)` sets the current comb",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/WritingGUI.md:82454,perform,performed,82454,documentation/users-guide/WritingGUI.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/WritingGUI.md,1,['perform'],['performed']
Performance,"st happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global and local; have completed; before performing; the atomicrmw that; is being released. 3. buffer/global/flat_atomic sc1=1; atomicrmw release - system - global 1. buffer_wbl2 sc0=1 sc1=1; - generic; - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at system scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to memory and the L2; writeback have; completed before; performing the; store that is being; released. 3. buffer/global/flat_atomic; sc0=1 sc1=1; fence release - singlethread *none* *none*; - wavefront; fence release - workgroup *none* 1. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate. If; fence had an; address space then; set to address; space of OpenCL; fence flag, or to; generic if both; local and global; flags are",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:312307,load,load,312307,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"st not suffice, and instead would simply; complicate the picture further because it adds an extra variant in; addition to the one each language provides. Instead, providing a default library version of malloc and free; (and perhaps a malloc_gc with garbage collection instead of free); would make a good implementation available to anyone who wants it. I don't recall all your arguments in favor so let's discuss this again,; and soon. o 'alloca' on the other hand sounds like a good idea, and the; implementation seems fairly language-independent so it doesn't have the; problems with malloc listed above. o About indirect call:; Your option #2 sounded good to me. I'm not sure I understand your; concern about an explicit 'icall' instruction?. o A pair of important synchronization instr'ns to think about:; load-linked; store-conditional. o Other classes of instructions that are valuable for pipeline performance:; conditional-move		 ; predicated instructions. o I believe tail calls are relatively easy to identify; do you know why; .NET has a tailcall instruction?. o I agree that we need a static data space. Otherwise, emulating global; data gets unnecessarily complex. o About explicit parallelism:. We once talked about adding a symbolic thread-id field to each; instruction. (It could be optional so single-threaded codes are; not penalized.) This could map well to multi-threaded architectures; while providing easy ILP for single-threaded onces. But it is probably; too radical an idea to include in a base version of LLVM. Instead, it; could a great topic for a separate study. What is the semantics of the IA64 stop bit?. o And finally, another thought about the syntax for arrays :-). Although this syntax:; 	 array <dimension-list> of <type>; is verbose, it will be used only in the human-readable assembly code so; size should not matter. I think we should consider it because I find it; to be the clearest syntax. It could even make arrays of function; pointers somewhat readable. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveComments.txt:3914,multi-thread,multi-threaded,3914,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveComments.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveComments.txt,1,['multi-thread'],['multi-threaded']
Performance,"st reason is that running analyses across outer level IR in inner level; IR passes can result in quadratic compile time behavior. For example, a module; analysis often scans every function and allowing function passes to run a module; analysis may cause us to scan functions a quadratic number of times. If passes; could keep outer level analyses up to date rather than computing them on demand; this wouldn't be an issue, but that would be a lot of work to ensure every pass; updates all outer level analyses, and so far this hasn't been necessary and; there isn't infrastructure for this (aside from function analyses in loop passes; as described below). Self-updating analyses that gracefully degrade also handle; this problem (e.g. GlobalsAA), but they run into the issue of having to be; manually recomputed somewhere in the optimization pipeline if we want precision,; and they block potential future concurrency. The second reason is to keep in mind potential future pass concurrency, for; example parallelizing function passes over different functions in a CGSCC or; module. Since passes can ask for a cached analysis result, allowing passes to; trigger outer level analysis computation could result in non-determinism if; concurrency was supported. A related limitation is that outer level IR analyses; that are used must be immutable, or else they could be invalidated by changes to; inner level IR. Outer analyses unused by inner passes can and often will be; invalidated by changes to inner level IR. These invalidations happen after the; inner pass manager finishes, so accessing mutable analyses would give invalid; results. The exception to not being able to access outer level analyses is accessing; function analyses in loop passes. Loop passes often use function analyses such; as the dominator tree. Loop passes inherently require modifying the function the; loop is in, and that includes some function analyses the loop analyses depend; on. This discounts future concurrency over s",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:9871,concurren,concurrency,9871,interpreter/llvm-project/llvm/docs/NewPassManager.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst,1,['concurren'],['concurrency']
Performance,"st,; therefore be indexed and requires an index operand. Consider this example:. .. code-block:: c++. struct munger_struct {; int f1;; int f2;; };; void munge(struct munger_struct *P) {; P[0].f1 = P[1].f1 + P[2].f2;; }; ...; struct munger_struct Array[3];; ...; munge(Array);. In this ""C"" example, the front end compiler (Clang) will generate three GEP; instructions for the three indices through ""P"" in the assignment statement. The; function argument ``P`` will be the second operand of each of these GEP; instructions. The third operand indexes through that pointer. The fourth; operand will be the field offset into the ``struct munger_struct`` type, for; either the ``f1`` or ``f2`` field. So, in LLVM assembly the ``munge`` function; looks like:. .. code-block:: llvm. define void @munge(ptr %P) {; entry:; %tmp = getelementptr %struct.munger_struct, ptr %P, i32 1, i32 0; %tmp1 = load i32, ptr %tmp; %tmp2 = getelementptr %struct.munger_struct, ptr %P, i32 2, i32 1; %tmp3 = load i32, ptr %tmp2; %tmp4 = add i32 %tmp3, %tmp1; %tmp5 = getelementptr %struct.munger_struct, ptr %P, i32 0, i32 0; store i32 %tmp4, ptr %tmp5; ret void; }. In each case the second operand is the pointer through which the GEP instruction; starts. The same is true whether the second operand is an argument, allocated; memory, or a global variable. To make this clear, let's consider a more obtuse example:. .. code-block:: text. @MyVar = external global i32; ...; %idx1 = getelementptr i32, ptr @MyVar, i64 0; %idx2 = getelementptr i32, ptr @MyVar, i64 1; %idx3 = getelementptr i32, ptr @MyVar, i64 2. These GEP instructions are simply making address computations from the base; address of ``MyVar``. They compute, as follows (using C syntax):. .. code-block:: c++. idx1 = (char*) &MyVar + 0; idx2 = (char*) &MyVar + 4; idx3 = (char*) &MyVar + 8. Since the type ``i32`` is known to be four bytes long, the indices 0, 1 and 2; translate into memory offsets of 0, 4, and 8, respectively. No memory is; accessed to make",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GetElementPtr.rst:3343,load,load,3343,interpreter/llvm-project/llvm/docs/GetElementPtr.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GetElementPtr.rst,1,['load'],['load']
Performance,"st2*. The lists must have the same element type. ``!listsplat(``\ *value*\ ``,`` *count*\ ``)``; This operator produces a list of length *count* whose elements are all; equal to the *value*. For example, ``!listsplat(42, 3)`` results in; ``[42, 42, 42]``. ``!logtwo(``\ *a*\ ``)``; This operator produces the base 2 log of *a* and produces the integer; result. The log of 0 or a negative number produces an error. This; is a flooring operation. ``!lt(``\ *a*\ `,` *b*\ ``)``; This operator produces 1 if *a* is less than *b*; 0 otherwise.; The arguments must be ``bit``, ``bits``, ``int``, or ``string`` values. ``!mul(``\ *a*\ ``,`` *b*\ ``, ...)``; This operator multiplies *a*, *b*, etc., and produces the product. ``!ne(``\ *a*\ `,` *b*\ ``)``; This operator produces 1 if *a* is not equal to *b*; 0 otherwise.; The arguments must be ``bit``, ``bits``, ``int``, ``string``,; or record values. Use ``!cast<string>`` to compare other types of objects. ``!not(``\ *a*\ ``)``; This operator performs a logical NOT on *a*, which must be; an integer. The argument 0 results in 1 (true); any other; argument results in 0 (false). ``!or(``\ *a*\ ``,`` *b*\ ``, ...)``; This operator does a bitwise OR on *a*, *b*, etc., and produces the; result. A logical OR can be performed if all the arguments are either; 0 or 1. ``!range([``\ *start*\ ``,]`` *end*\ ``[, ``\ *step*\ ``])``; This operator produces half-open range sequence ``[start : end : step)`` as; ``list<int>``. *start* is ``0`` and *step* is ``1`` by default. *step* can; be negative and cannot be 0. If *start* ``<`` *end* and *step* is negative,; or *start* ``>`` *end* and *step* is positive, the result is an empty list; ``[]<list<int>>``. For example:. * ``!range(4)`` is equivalent to ``!range(0, 4, 1)`` and the result is; `[0, 1, 2, 3]`.; * ``!range(1, 4)`` is equivalent to ``!range(1, 4, 1)`` and the result is; `[1, 2, 3]`.; * The result of ``!range(0, 4, 2)`` is `[0, 2]`.; * The results of ``!range(0, 4, -1)`` and ``!range(4, 0, 1)",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:69264,perform,performs,69264,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,1,['perform'],['performs']
Performance,"st[int](std.move(val)) # wrong cast; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; cppyy.gbl.boost.bad_any_cast: Could not instantiate any_cast<int>:; int boost::any_cast(boost::any&& operand) =>; wrapexcept<boost::bad_any_cast>: boost::bad_any_cast: failed conversion using boost::any_cast; >>> extract = boost.any_cast[std.vector[int]](val) # correct cast; >>> type(extract) is std.vector[int]; True; >>> extract += xrange(100); >>> len(extract); 100; >>> val.__assign__(std.move(extract)) # move forced; <cppyy.gbl.boost.any object at 0xf6a8a0>; >>> len(extract) # now empty (or invalid); 0; >>> extract = boost.any_cast[std.vector[int]](val); >>> list(extract); [0, 1, 2, 3, 4, 5, 6, ..., 97, 98, 99]; >>>. Of course, there is no reason to use Boost from Python (in fact, this example; calls out for :doc:`pythonizations <pythonizations>`), but it shows that; cppyy seamlessly supports many advanced C++ features. cppyy is available for both `CPython`_ (v2 and v3) and `PyPy`_, reaching; C++-like performance with the latter.; It makes judicious use of precompiled headers, dynamic loading, and lazy; instantiation, to support C++ programs consisting of millions of lines of; code and many thousands of classes.; cppyy minimizes dependencies to allow its use in distributed, heterogeneous,; development environments. .. _Cling: https://github.com/vgvassilev/cling; .. _tutorial: https://github.com/wlav/cppyy/blob/master/doc/tutorial/CppyyTutorial.ipynb; .. _`PyHPC'16 paper`: http://wlav.web.cern.ch/wlav/Cppyy_LavrijsenDutta_PyHPC16.pdf; .. _`CAAS presentation`: https://www.youtube.com/watch?v=stMD7VDWlVU; .. _`Jason Turner's`: https://www.youtube.com/watch?v=TL83P77vZ1k; .. _`Boost`: http://www.boost.org/; .. _`CPython`: http://python.org; .. _`PyPy`: http://pypy.org. .. only: not latex. Contents:. .. toctree::; :maxdepth: 1. changelog; license. .. toctree::; :caption: Getting Started; :maxdepth: 1. installation; starting; examples; bugs. .. toctree::; :capt",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/index.rst:4331,perform,performance,4331,bindings/pyroot/cppyy/cppyy/doc/source/index.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/index.rst,1,['perform'],['performance']
Performance,"stack map format generated by this GC strategy can be found in the; :ref:`stackmap-section` using a format documented :ref:`here; <statepoint-stackmap-format>`. This format is intended to be the standard; format supported by LLVM going forward. The CoreCLR GC; -------------------------. .. code-block:: c++. F.setGC(""coreclr"");. This GC leverages the ``gc.statepoint`` mechanism to support the; `CoreCLR <https://github.com/dotnet/coreclr>`__ runtime. Support for this GC strategy is a work in progress. This strategy will; differ from; :ref:`statepoint-example GC<statepoint_example_gc>` strategy in; certain aspects like:. * Base-pointers of interior pointers are not explicitly; tracked and reported. * A different format is used for encoding stack maps. * Safe-point polls are only needed before loop-back edges; and before tail-calls (not needed at function-entry). Custom GC Strategies; ====================. If none of the built in GC strategy descriptions met your needs above, you will; need to define a custom GCStrategy and possibly, a custom LLVM pass to perform; lowering. Your best example of where to start defining a custom GCStrategy; would be to look at one of the built in strategies. You may be able to structure this additional code as a loadable plugin library.; Loadable plugins are sufficient if all you need is to enable a different; combination of built in functionality, but if you need to provide a custom; lowering pass, you will need to build a patched version of LLVM. If you think; you need a patched build, please ask for advice on llvm-dev. There may be an; easy way we can extend the support to make it work for your use case without; requiring a custom build. Collector Requirements; ----------------------. You should be able to leverage any existing collector library that includes the following elements:. #. A memory allocator which exposes an allocation function your compiled; code can call. #. A binary format for the stack map. A stack map describes the lo",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GarbageCollection.rst:21707,perform,perform,21707,interpreter/llvm-project/llvm/docs/GarbageCollection.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GarbageCollection.rst,1,['perform'],['perform']
Performance,"stale MTYPE NC global data.; MTYPE RW and CC memory will; never be stale due to the; memory probes. atomicrmw acquire - agent - generic 1. flat_atomic; 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Must happen before; following; buffer_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; cache. 3. buffer_inv sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acquire - system - generic 1. flat_atomic sc1=1; 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Must happen before; following; buffer_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; caches. 3. buffer_inv sc0=1 sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale MTYPE NC global data.; MTYPE RW and CC memory will; never be stale due to the; memory probes. fence acquire - singlethread *none* *none*; - wavefront; fence acquire - workgroup *none* 1. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate. If; fence had an; address space then; set to address; space of OpenCL; fence flag, or to; generic if both; local and global; flags are; specified.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load; atomic/; atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:302234,load,loads,302234,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['loads']
Performance,"stale data. load atomic acquire - workgroup - generic 1. flat_load glc=1. - If CU wavefront execution; mode, omit glc=1. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If CU wavefront execution; mode, omit vmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Must happen before; the following; buffer_gl0_inv and any; following global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local load; atomic value being; acquired. 3. buffer_gl0_inv. - If CU wavefront execution; mode, omit.; - Ensures that; following; loads will not see; stale data. load atomic acquire - agent - global 1. buffer/global_load; - system glc=1 dlc=1. - If GFX11, omit dlc=1. 2. s_waitcnt vmcnt(0). - Must happen before; following; buffer_gl*_inv.; - Ensures the load; has completed; before invalidating; the caches. 3. buffer_gl0_inv;; buffer_gl1_inv. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale global data. load atomic acquire - agent - generic 1. flat_load glc=1 dlc=1; - system; - If GFX11, omit dlc=1. 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If OpenCL omit; lgkmcnt(0).; - Must happen before; following; buffer_gl*_invl.; - Ensures the flat_load; has completed; before invalidating; the caches. 3. buffer_gl0_inv;; buffer_gl1_inv. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acquire - singlethread - global 1. buffer/global/ds/flat_atomic; - wavefront - local; - generic; atomicrmw acquire - workgroup - global 1. buffer/global_atomic; 2. s_waitcnt vm/vscnt(0). - If CU wavefront execution; mode, omit.; - Use vmcnt(0) if atomic with; return and vscnt(0) if; atomic with no-return.; - Must happen before; the following buffer_gl0_inv; and before any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw. 3. buffer_gl0_inv. - If CU wav",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:348127,load,loads,348127,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['loads']
Performance,"stance, running a `migrad` fit:; ``` {.cpp}; m.migrad(); ```. ## Constant term optimization; The `RooAbsTestStatistic` based classes not only combine statistics and calculation, but also constant term optimization routines.; These can be run on PDFs and datasets before starting a fit.; They search the calculation graph for parts that are independent of the fit parameters, precalculates them, and adds them to (a clone of) the dataset so that these values can be used during calculation. In `RooFit::TestStatistics`, we separated this functionality out into the `ConstantTermsOptimizer` class.; In fact, it is not so much a class, as it is a collection of static functions that can be applied to any combination of pdf and dataset.; This class does essentially the same as `constOptimizeTestStatistic` did on a `RooNLLVar`, except that it has been factored out into a separate class. ### Usage example: apply constant term optimization on pdf and dataset inside a likelihood; Applying the default `ConstantTermsOptimizer` optimization routines on the pdf and dataset inside a `RooAbsL` likelihood is as simple as:. ``` {.cpp}; likelihood.constOptimizeTestStatistic();; ```; This applies constant term optimization to the cloned pdf and dataset inside the likelihood object.; It will not modify anything outside of the likelihood. Optimization can also be activated through the minimizer, which may be more familiar to most users.; Given the `RooMinimizer` object `m` as defined in the example above, we can do:; ``` {.cpp}; m.optimizeConst(2);; ```. For the adventurous user, it is also possible to apply constant term optimization to a pdf and dataset directly without needing a likelihood object, e.g. given some `RooArgSet` set of observables `normSet`:; ``` {.cpp}; bool applyTrackingOpt = true;; ConstantTermsOptimizer::enableConstantTermsOptimization(&pdf, &normSet, dataset, applyTrackingOpt);; ```; We refer to RooFit documentation for more about ""tracking optimization"" which can be enabled",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/developers/test_statistics.md:10320,optimiz,optimization,10320,roofit/doc/developers/test_statistics.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/developers/test_statistics.md,2,['optimiz'],['optimization']
Performance,"state of the pragma at the point of an; instantiation is not necessarily relevant. Consider the following example:. .. code-block:: c++. template<typename T> T twice(T t) {; return 2 * t;; }. #pragma clang optimize off; template<typename T> T thrice(T t) {; return 3 * t;; }. int container(int a, int b) {; return twice(a) + thrice(b);; }; #pragma clang optimize on. In this example, the definition of the template function ``twice`` is outside; the pragma region, whereas the definition of ``thrice`` is inside the region.; The ``container`` function is also in the region and will not be optimized, but; it causes the instantiation of ``twice`` and ``thrice`` with an ``int`` type; of; these two instantiations, ``twice`` will be optimized (because its definition; was outside the region) and ``thrice`` will not be optimized. Clang also implements MSVC's range-based pragma,; ``#pragma optimize(""[optimization-list]"", on | off)``. At the moment, Clang only; supports an empty optimization list, whereas MSVC supports the arguments, ``s``,; ``g``, ``t``, and ``y``. Currently, the implementation of ``pragma optimize`` behaves; the same as ``#pragma clang optimize``. All functions; between ``off`` and ``on`` will be decorated with the ``optnone`` attribute. .. code-block:: c++. #pragma optimize("""", off); // This function will be decorated with optnone.; void f1() {}. #pragma optimize("""", on); // This function will be optimized with whatever was specified on; // the commandline.; void f2() {}. // This will warn with Clang's current implementation.; #pragma optimize(""g"", on); void f3() {}. For MSVC, an empty optimization list and ``off`` parameter will turn off; all optimizations, ``s``, ``g``, ``t``, and ``y``. An empty optimization and; ``on`` parameter will reset the optimizations to the ones specified on the; commandline. .. list-table:: Parameters (unsupported by Clang). * - Parameter; - Type of optimization; * - g; - Deprecated; * - s or t; - Short or fast sequences of machine ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst:161155,optimiz,optimization,161155,interpreter/llvm-project/clang/docs/LanguageExtensions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst,1,['optimiz'],['optimization']
Performance,"stateless AA impl); Function Alias Analysis Results; Memory Dependence Analysis; Global Value Numbering; Natural Loop Information; Canonicalize natural loops; Loop-Closed SSA Form Pass; Basic Alias Analysis (stateless AA impl); Function Alias Analysis Results; Scalar Evolution Analysis; Loop Pass Manager; Loop Invariant Code Motion; Module Verifier; Bitcode Writer. This output shows us when passes are constructed.; Here we see that GVN uses dominator tree information to do its job. The LICM pass; uses natural loop information, which uses dominator tree as well. After the LICM pass, the module verifier runs (which is automatically added by; the :program:`opt` tool), which uses the dominator tree to check that the; resultant LLVM code is well formed. Note that the dominator tree is computed; once, and shared by three passes. Lets see how this changes when we run the :ref:`Hello World; <writing-an-llvm-pass-basiccode>` pass in between the two passes:. .. code-block:: console. $ opt -load lib/LLVMHello.so -gvn -hello -licm --debug-pass=Structure < hello.bc > /dev/null; ModulePass Manager; FunctionPass Manager; Dominator Tree Construction; Basic Alias Analysis (stateless AA impl); Function Alias Analysis Results; Memory Dependence Analysis; Global Value Numbering; Hello World Pass; Dominator Tree Construction; Natural Loop Information; Canonicalize natural loops; Loop-Closed SSA Form Pass; Basic Alias Analysis (stateless AA impl); Function Alias Analysis Results; Scalar Evolution Analysis; Loop Pass Manager; Loop Invariant Code Motion; Module Verifier; Bitcode Writer; Hello: __main; Hello: puts; Hello: main. Here we see that the :ref:`Hello World <writing-an-llvm-pass-basiccode>` pass; has killed the Dominator Tree pass, even though it doesn't modify the code at; all! To fix this, we need to add the following :ref:`getAnalysisUsage; <writing-an-llvm-pass-getAnalysisUsage>` method to our pass:. .. code-block:: c++. // We don't modify the program, so we preserve all analyse",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst:44892,load,load,44892,interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst,1,['load'],['load']
Performance,"std::vector, std::list,; std::deque, std::set or std::multiset containing pointers to objects.; and where the splitlevel is a value bigger than 100 then the collection; will be written in split mode. Ie. if it contains objects of any; types deriving from TTrack this function will sort the objects; basing on their type and store them in separate branches in split; mode.; The ROOT test example in ROOTSYS/test/bench.cxx shows many examples of collections; and storage in a TTree when using split mode or not. This program illustrates the important; gain in space and time when using this new facility. Parallel unzipping. Introducing a parallel unzipping algorithm for pre-fetched buffers. Since we already know what buffers are going to be read, we can decompress a few of them in advance in an additional thread and give the impression that the data decompression comes for free (we gain up to 30% in reading intensive jobs). The size of this unzipping cache is 20% the size of the TTreeCache and can be modified with TTreeCache::SetUnzipBufferSize(Long64_t bufferSize). Theoretically, we only need one buffer in advance but in practice we might fall short if the unzipping cache is too small (synchronization costs). This experimental feature is disabled by default, to activate it use the static function TTreeCache::SetParallelUnzip(TTreeCacheUnzip::EParUnzipMode option = TTreeCacheUnzip::kEnable). The possible values to pass are: TTreeCacheUnzip::kEnable to enable itTTreeCacheUnzip::kDisable to disable itTTreeCacheUnzip::kForce to force it.The TTreeCacheUnzip is actived; only if you have more than one core. To activate it with only one core useTTreeCacheUnzip::kForce option (for example to measure the overhead). Disk and Memory Space Gain. In ROOT older than v5.20/00, the branches' last basket, also known as the write basket, was always saved in the same ""key"" as the TTree object and was always present in memory when reading or writing.; When reading this write basket was always pr",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/doc/v520/index.html:5008,cache,cache,5008,tree/doc/v520/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/doc/v520/index.html,2,['cache'],['cache']
Performance,"stead of the ``class`` tag, and ``pattern`` instead of ``name`` with; wildcarding in the value string. Next, use ``genreflex`` to generate the dictionary (here:; ``MyClass_rflx.cxx``) and module files::. $ genreflex MyClass.h --selection=myclass_selection.xml -o MyClass_rflx.cxx. From here, compile and link the generated dictionary file with the project; and/or system specific options and libraries into a shared library, using; ``cling-config`` for the relevant cppyy compiler/linker flags.; (For work on MS Windows, this `helper script`_ may be useful.); To continue the example, assuming Linux::. $ g++ `cling-config --cppflags` -fPIC -O2 -shared MyClass_rflx.cxx -o MyClassDict.so. Instead of loading the header text into ``cling``, you can now load the; dictionary:. .. code-block:: python. >>> import cppyy; >>> cppyy.load_reflection_info('MyClassDict'); >>> cppyy.gbl.MyClass(42); <cppyy.gbl.MyClass object at 0x7ffb9f230950>; >>> print(_.get_int()); 42; >>>. and use the selected C++ entities as if the header was loaded. The dictionary shared library can be relocated, as long as it can be found; by the dynamic loader (e.g. through ``LD_LIBRARY_PATH``) and the header file; is fully embedded or still accessible (e.g. through a path added to; ``cppyy.add_include_path`` at run-time, or with ``-I`` to; ``rootcling``/``genreflex`` during build time).; When relocating the shared library, move the .pcm with it.; Once support for C++ modules is fully fleshed out, access to the header file; will no longer be needed. .. _`rootcling manual`: https://root.cern.ch/root/html/guides/users-guide/AddingaClass.html#the-linkdef.h-file; .. _`helper script`: https://github.com/wlav/cppyy/blob/master/test/make_dict_win32.py. Class loader; ^^^^^^^^^^^^. Explicitly loading dictionaries is fine if this is hidden under the hood of; a Python package and thus transparently done on ``import``.; Otherwise, the automatic class loader is more convenient, as it allows direct; use without having to manual",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/utilities.rst:7450,load,loaded,7450,bindings/pyroot/cppyy/cppyy/doc/source/utilities.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/utilities.rst,1,['load'],['loaded']
Performance,"sted; member function. This shortcut is quite natural for an interactive; system and saves much typing. In this example, ROOT searches for `hpx`; and finds it in `hsimple.root.`. The next, fundamental extension is shown below. There is no need to put a semicolon at; the end of a line. When you leave it off the value of the expression will; be printed on the next line. For example:. ``` {.cpp}; root[] 23+5; (int)28; root[] 23+5;; root[] TMath::Sin; (Double_t (*)(Double_t)) Function @0x7ffff7ebb090; at include/TMath.h:418:; inline Double_t TMath::Sin(Double_t x); { return sin(x); }; ```. Be aware that these extensions do not work when a compiler replaces; the interpreter. Your code will not compile, hence when writing large; scripts, it is best to stay away from these shortcuts. It will save; you from having problems compiling your scripts using a real C++; compiler. ## ACLiC: Compiling Scripts Into Libraries. Instead of having Cling interpret your script there is a way to have your; scripts compiled, linked and dynamically loaded using the C++ compiler; and linker. The advantage of this is that your scripts will run with the; speed of compiled C++ and that you can use language constructs that are; not fully supported by Cling. On the other hand, you cannot use any Cling; shortcuts (see ""C++ Extensions To Ease Scripting"" above) and for small scripts, the; overhead of the compile/link cycle might be larger than just executing; the script in the interpreter. ACLiC will build a dictionary and a shared library from your C++; script, using the compiler and the compiler options that were used to; compile the ROOT executable. You do not have to write a Makefile; remembering the correct compiler options, and you do not have to exit; ROOT. ### Usage. Before you can compile your interpreted script you need to add include; statements for the classes used in the script. Once you did that, you; can build and load a shared library containing your script. To load it; use the command ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Cling.md:12704,load,loaded,12704,documentation/users-guide/Cling.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Cling.md,1,['load'],['loaded']
Performance,"stem *none* 1. buffer_wbl2 sc0=1 sc1=1. - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at system scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate. If; fence had an; address space then; set to address; space of OpenCL; fence flag, or to; generic if both; local and global; flags are; specified.; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; any following store; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - Ensures that all; memory operations; have; completed before; performing the; following; fence-paired-atomic. **Acquire-Release Atomic**; ------------------------------------------------------------------------------------; atomicrmw acq_rel - singlethread - global 1. buffer/global/flat_atomic; - wavefront - generic; atomicrmw acq_rel - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; be used.*. 1. ds_atomic; atomicrmw acq_rel - workgroup - global 1. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL, omit; lgkmcnt(0).; - Must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:316308,load,load,316308,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"stent store to the same; address, the first store can be erased. This transformation is not allowed for a; pair of volatile stores. On the other hand, a non-volatile non-atomic load can; be moved across a volatile load freely, but not an Acquire load. This document is intended to provide a guide to anyone either writing a frontend; for LLVM or working on optimization passes for LLVM with a guide for how to deal; with instructions with special semantics in the presence of concurrency. This; is not intended to be a precise guide to the semantics; the details can get; extremely complicated and unreadable, and are not usually necessary. .. _Optimization outside atomic:. Optimization outside atomic; ===========================. The basic ``'load'`` and ``'store'`` allow a variety of optimizations, but can; lead to undefined results in a concurrent environment; see `NotAtomic`_. This; section specifically goes into the one optimizer restriction which applies in; concurrent environments, which gets a bit more of an extended description; because any optimization dealing with stores needs to be aware of it. From the optimizer's point of view, the rule is that if there are not any; instructions with atomic ordering involved, concurrency does not matter, with; one exception: if a variable might be visible to another thread or signal; handler, a store cannot be inserted along a path where it might not execute; otherwise. Take the following example:. .. code-block:: c. /* C code, for readability; run through clang -O2 -S -emit-llvm to get; equivalent IR */; int x;; void f(int* a) {; for (int i = 0; i < 100; i++) {; if (a[i]); x += 1;; }; }. The following is equivalent in non-concurrent situations:. .. code-block:: c. int x;; void f(int* a) {; int xtemp = x;; for (int i = 0; i < 100; i++) {; if (a[i]); xtemp += 1;; }; x = xtemp;; }. However, LLVM is not allowed to transform the former to the latter: it could; indirectly introduce undefined behavior if another thread can access ``",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst:2660,optimiz,optimizer,2660,interpreter/llvm-project/llvm/docs/Atomics.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst,3,"['concurren', 'optimiz']","['concurrent', 'optimization', 'optimizer']"
Performance,"ster an arbitrary command to the server, which become visible in the web browser. Then, when the item is clicked by the user, the command ends-up in a gROOT->ProcessLineSync() call. ##### Custom Properties ; Custom properties can be configured for any item in the server. For example, one could configure an icon for each item visible in the browser. Or one could 'hide' any item from the user (but keep access with normal http requests). With such properties one could specify which item is drawn when web page is loaded, or configure monitoring. See tutorials/http/httpcontrol.C macro for more details. ##### Method Calls; Implement exe.json requests to be able to execute any method of registered objects. This request is used to provide remote TTree::Draw() functionality. ##### Misc; Correctly set 'Cache-Control' headers when replying to http requests.; Better support of STL containers when converting objects into json with TBufferJSON class. ## JavaScript ROOT. - Several files can now be loaded simultaneously; - Use d3.time.scale to display time scales; - Implemented drag and drop to superimpose histograms or graphs; - Allow selection of drawing option via context menu; - Better support of touch devices; - Provide simple layout, making it default; - Allow to open ROOT files in online session (via url parameter); - One could monitor simultaneously objects from server and root files; - Implement 'autocol' draw option - when superimposing histograms,; their line colors will be automatically assigned; - Implement 'nostat' draw option - disabled stat drawing; - Using '_same_' identifier in item name, one can easily draw or superimpose; similar items from different files. Could be used in URL like:; `...&files=[file1.root,file2.root]&items=[file1.root/hpx, file2.root/_same_]`; `...&files=[file1.root,file2.root]&item=file1.root/hpx+file2.root/_same_`; Main limitation - file names should have similar length.; - When 'autozoom' specified in draw options, histogram zoomed into; no",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v604/index.md:10405,load,loaded,10405,README/ReleaseNotes/v604/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v604/index.md,1,['load'],['loaded']
Performance,"stimator calculations from small deviations from the desired value to large deviations only over the weight range. Configuration options for MVA method :. Configuration options reference for MVA method: Cuts. Option Array Default value Predefined values Description. V No False − Verbose output (short form of VerbosityLevel below - overrides the latter one). VerbosityLevel No Default Default, Debug, Verbose, Info, Warning, Error, Fatal Verbosity level. VarTransform No None − List of variable transformations performed before training, e.g., D_Background,P_Signal,G,N_AllClasses for: Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed). H No False − Print method-specific help message. CreateMVAPdfs No False − Create PDFs for classifier outputs (signal and background). IgnoreNegWeightsInTraining No False − Events with negative weights are ignored in the training (but are included for testing and performance evaluation). FitMethod No GA GA, SA, MC, MCEvents, MINUIT, EventScan Minimisation Method (GA, SA, and MC are the primary methods to be used; the others have been introduced for testing purposes and are depreciated). EffMethod No EffSel EffSel, EffPDF Selection Method. CutRangeMin Yes -1 − Minimum of allowed cut range (set per variable). CutRangeMax Yes -1 − Maximum of allowed cut range (set per variable). VarProp Yes NotEnforced NotEnforced, FMax, FMin, FSmart Categorisation of cuts. Configuration options for MVA method :. Configuration options reference for MVA method: PDEFoam. Option Array Default value Predefined values Description. V No False − Verbose output (short form of VerbosityLevel below - overrides the latter one). VerbosityLevel No Default Default, Debug, Verbose, Info, Warning, Error, Fatal Verbosity level. VarTransform No None − List of variable transformations performed before training, e.g., D_Background,",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/tmva/UsersGuide/optionRef.html:25129,perform,performance,25129,documentation/tmva/UsersGuide/optionRef.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/tmva/UsersGuide/optionRef.html,2,['perform'],['performance']
Performance,"sting facilities. For example, duplicate; information is automatically merged by the linker, and unused information; is automatically removed. Basically, the debug information allows you to compile a program with; ""``-O0 -g``"" and get full debug information, allowing you to arbitrarily modify; the program as it executes from a debugger. Compiling a program with; ""``-O3 -g``"" gives you full debug information that is always available and; accurate for reading (e.g., you get accurate stack traces despite tail call; elimination and inlining), but you might lose the ability to modify the program; and call functions which were optimized out of the program, or inlined away; completely. The :doc:`LLVM test-suite <TestSuiteMakefileGuide>` provides a framework to; test the optimizer's handling of debugging information. It can be run like; this:. .. code-block:: bash. % cd llvm/projects/test-suite/MultiSource/Benchmarks # or some other level; % make TEST=dbgopt. This will test impact of debugging information on optimization passes. If; debugging information influences optimization passes then it will be reported; as a failure. See :doc:`TestingGuide` for more information on LLVM test; infrastructure and how to run various tests. .. _format:. Debugging information format; ============================. LLVM debugging information has been carefully designed to make it possible for; the optimizer to optimize the program and debugging information without; necessarily having to know anything about debugging information. In; particular, the use of metadata avoids duplicated debugging information from; the beginning, and the global dead code elimination pass automatically deletes; debugging information for a function if it decides to delete the function. To do this, most of the debugging information (descriptors for types,; variables, functions, source files, etc) is inserted by the language front-end; in the form of LLVM metadata. Debug information is designed to be agnostic about the",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst:6006,optimiz,optimization,6006,interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,1,['optimiz'],['optimization']
Performance,"sting from a vector to an integral type can be seen as; ; concatenating the values:; ; %val now has the hexadecimal value 0x5321. store i16 %val, ptr %ptr. ; In memory the content will be (8-bit addressing):; ;; ; [%ptr + 0]: 00100001 (0x21); ; [%ptr + 1]: 01010011 (0x53). When ``<N*M>`` isn't evenly divisible by the byte size the exact memory layout; is unspecified (just like it is for an integral type of the same size). This; is because different targets could put the padding at different positions when; the type size is smaller than the type's store size. :Syntax:. ::. < <# elements> x <elementtype> > ; Fixed-length vector; < vscale x <# elements> x <elementtype> > ; Scalable vector. The number of elements is a constant integer value larger than 0;; elementtype may be any integer, floating-point or pointer type. Vectors; of size zero are not allowed. For scalable vectors, the total number of; elements is a constant multiple (called vscale) of the specified number; of elements; vscale is a positive integer that is unknown at compile time; and the same hardware-dependent constant for all scalable vectors at run; time. The size of a specific scalable vector type is thus constant within; IR, even if the exact size in bytes cannot be determined until run time. :Examples:. +------------------------+----------------------------------------------------+; | ``<4 x i32>`` | Vector of 4 32-bit integer values. |; +------------------------+----------------------------------------------------+; | ``<8 x float>`` | Vector of 8 32-bit floating-point values. |; +------------------------+----------------------------------------------------+; | ``<2 x i64>`` | Vector of 2 64-bit integer values. |; +------------------------+----------------------------------------------------+; | ``<4 x ptr>`` | Vector of 4 pointers |; +------------------------+----------------------------------------------------+; | ``<vscale x 4 x i32>`` | Vector with a multiple of 4 32-bit integer values. |; +----",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:177171,scalab,scalable,177171,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,2,['scalab'],['scalable']
Performance,"storage, like ``inreg``, ``nest``, ``sret``, or ``byval``. The; ``inalloca`` attribute also disables LLVM's implicit lowering of; large aggregate return values, which means that frontend authors; must lower them with ``sret`` pointers. When the call site is reached, the argument allocation must have; been the most recent stack allocation that is still live, or the; behavior is undefined. It is possible to allocate additional stack; space after an argument allocation and before its call site, but it; must be cleared off with :ref:`llvm.stackrestore; <int_stackrestore>`. The inalloca attribute requires a type argument, which must be the; same as the pointee type of the argument. See :doc:`InAlloca` for more information on how to use this; attribute. ``sret(<ty>)``; This indicates that the pointer parameter specifies the address of a; structure that is the return value of the function in the source; program. This pointer must be guaranteed by the caller to be valid:; loads and stores to the structure may be assumed by the callee not; to trap and to be properly aligned. This is not a valid attribute; for return values. The sret type argument specifies the in memory type, which must be; the same as the pointee type of the argument. .. _attr_elementtype:. ``elementtype(<ty>)``. The ``elementtype`` argument attribute can be used to specify a pointer; element type in a way that is compatible with `opaque pointers; <OpaquePointers.html>`__. The ``elementtype`` attribute by itself does not carry any specific; semantics. However, certain intrinsics may require this attribute to be; present and assign it particular semantics. This will be documented on; individual intrinsics. The attribute may only be applied to pointer typed arguments of intrinsic; calls. It cannot be applied to non-intrinsic calls, and cannot be applied; to parameters on function declarations. For non-opaque pointers, the type; passed to ``elementtype`` must match the pointer element type. .. _attr_align:. ``",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:56105,load,loads,56105,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['loads']
Performance,"store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; have; completed before; performing the; atomicrmw that is; being released. 2. buffer/global_atomic; 3. s_waitcnt vm/vscnt(0). - If CU wavefront execution; mode, omit.; - Use vmcnt(0) if atomic with; return and vscnt(0) if; atomic with no-return.; - Must happen before; the following; buffer_gl0_inv.; - Ensures any; following global; data read is no; older than the; atomicrmw value; being acquired. 4. buffer_gl0_inv. - If CU wavefront execution; mode, omit.; - Ensures that; following; loads will not see; stale data. atomicrmw acq_rel - workgroup - local 1. s_waitcnt vmcnt(0) & vscnt(0). - If CU wavefront execution; mode, omit.; - If OpenCL, omit.; - Could be split into; separate s_waitcnt; vmcnt(0) and s_waitcnt; vscnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load/load; atomic/; atomicrmw-with-return-value.; - s_waitcnt vscnt(0); must happen after; any preceding; global/generic; store/store atomic/; atomicrmw-no-return-value.; - Must happen before; the following; store.; - Ensures that all; global memory; operations have; completed before; performing the; store that is being; released. 2. ds_atomic; 3. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; the following; buffer_gl0_inv.; - Ensures any; following global; data read is no; older than the local load; atomic value being; acquired. 4. buffer_gl0_inv. - If CU wavefront execution; mode, omit.; - If OpenCL omit.; - Ensures that; following; loads will not see; stale data. atomicrmw acq_rel - workgroup - generic 1. s_waitcnt lgkmcnt(0) &; vmcnt(0) & vscnt(0). - If CU wavefront execution; mode, omit vmcnt(0) and; vscnt(0).; - If OpenCL, omit lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0), s_waitcnt; vscnt(0) and s_waitcnt; lgkmcnt(0) to allow; the",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:364831,load,load,364831,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"store; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale data. load atomic acquire - workgroup - local *If TgSplit execution mode,; local address space cannot; be used.*. 1. ds_load; 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than the local load; atomic value being; acquired. load atomic acquire - workgroup - generic 1. flat_load sc0=1; 2. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL, omit lgkmcnt(0).; - Must happen before; the following; buffer_inv and any; following global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local load; atomic value being; acquired. 3. buffer_inv sc0=1. - If not TgSplit execution; mode, omit.; - Ensures that; following; loads will not see; stale data. load atomic acquire - agent - global 1. buffer/global_load; sc1=1; 2. s_waitcnt vmcnt(0). - Must happen before; following; buffer_inv.; - Ensures the load; has completed; before invalidating; the cache. 3. buffer_inv sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale global data. load atomic acquire - system - global 1. buffer/global/flat_load; sc0=1 sc1=1; 2. s_waitcnt vmcnt(0). - Must happen before; following; buffer_inv.; - Ensures the load; has completed; before invalidating; the cache. 3. buffer_inv sc0=1 sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale MTYPE NC global data.; MTYPE RW and CC memory will; never be stale due to the; memory probes. load atomic acquire - agent - generic 1. flat_load sc1=1; 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If TgSplit execution mode,;",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:296976,load,loads,296976,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['loads']
Performance,"store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global have; completed before; performing the; atomicrmw that is; being released. 2. buffer/global_atomic; 3. s_waitcnt vm/vscnt(0). - Use vmcnt(0) if atomic with; return and vscnt(0) if; atomic with no-return.; - Must happen before; following; buffer_gl*_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; caches. 4. buffer_gl0_inv;; buffer_gl1_inv. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acq_rel - agent - generic 1. s_waitcnt lgkmcnt(0) &; - system vmcnt(0) & vscnt(0). - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0), s_waitcnt; vscnt(0), and s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/load atomic; atomicrmw-with-return-value.; - s_waitcnt vscnt(0); must happen after; any preceding; global/generic; store/store atomic/; atomicrmw-no-return-value.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; have; completed before; performing the; atomicrmw that is; being released. 2. flat_atomic; 3. s_waitcnt vm/vscnt(0) &; lgkmcnt(0). - If OpenCL, omit; lgkmcnt(0).; - Use vmcnt(0) if atomic with; return and vscnt(0) if; atomic with no-return.; - Must happen before; following; buffer_gl*_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; caches. 4. buffer_gl0_inv;; buffer_gl1_inv. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. fence acq_rel - singlethread *none* *none*; - wavefront",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:368504,load,load,368504,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global have; completed before; performing the; atomicrmw that is; being released. 2. buffer/global_atomic; 3. s_waitcnt vmcnt(0). - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the; atomicrmw has; completed before; invalidating the; cache. 4. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acq_rel - agent - generic 1. s_waitcnt lgkmcnt(0) &; - system vmcnt(0). - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global have; completed before; performing the; atomicrmw that is; being released. 2. flat_atomic; 3. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If OpenCL, omit; lgkmcnt(0).; - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the; atomicrmw has; completed before; invalidating the; cache. 4. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. fence acq_rel - singlethread *none* *none*; - wavefront; fence acq_rel - workgroup *none* 1. s_waitcnt lgkmcnt(0). - If OpenCL and; address space is; not generic, omit.; - However,; since LLVM; currently has no; address space on; the fence need to; conserva",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:226529,load,load,226529,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"store; followed by a; seq_cst load. (Note; that seq_cst is; stronger than; acquire/release as; the reordering of; load acquire; followed by a store; release is; prevented by the; s_waitcnt of; the release, but; there is nothing; preventing a store; release followed by; load acquire from; completing out of; order. The s_waitcnt; could be placed after; seq_store or before; the seq_load. We; choose the load to; make the s_waitcnt be; as late as possible; so that the store; may have already; completed.). 2. *Following; instructions same as; corresponding load; atomic acquire,; except must generate; all instructions even; for OpenCL.*; load atomic seq_cst - workgroup - local. 1. s_waitcnt vmcnt(0) & vscnt(0). - If CU wavefront execution; mode, omit.; - Could be split into; separate s_waitcnt; vmcnt(0) and s_waitcnt; vscnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); Must happen after; preceding; global/generic load; atomic/; atomicrmw-with-return-value; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; vmcnt(0) and so do; not need to be; considered.); - s_waitcnt vscnt(0); Must happen after; preceding; global/generic store; atomic/; atomicrmw-no-return-value; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; vscnt(0) and so do; not need to be; considered.); - Ensures any; preceding; sequential; consistent global; memory instructions; have completed; before executing; this sequentially; consistent; instruction. This; prevents reordering; a seq_cst store; followed by a; seq_cst load. (Note; that seq_cst is; stronger than; acquire/release as; the reordering of; load acquire; followed by a store; release is; prevented by the; s_waitcnt of; the release, but; there is nothing; preventing a store; release followed by; load acquire from; completing out of; order. The s_wai",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:376214,load,load,376214,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"store`` (GFX9-GFX11). Since only a single thread; is accessing the memory, atomic memory orderings are not meaningful, and all; accesses are treated as non-atomic. Constant address space uses ``buffer/global_load`` instructions (or equivalent; scalar memory instructions). Since the constant address space contents do not; change during the execution of a kernel dispatch it is not legal to perform; stores, and atomic memory orderings are not meaningful, and all accesses are; treated as non-atomic. A memory synchronization scope wider than work-group is not meaningful for the; group (LDS) address space and is treated as work-group. The memory model does not support the region address space which is treated as; non-atomic. Acquire memory ordering is not meaningful on store atomic instructions and is; treated as non-atomic. Release memory ordering is not meaningful on load atomic instructions and is; treated a non-atomic. Acquire-release memory ordering is not meaningful on load or store atomic; instructions and is treated as acquire and release respectively. The memory order also adds the single thread optimization constraints defined in; table; :ref:`amdgpu-amdhsa-memory-model-single-thread-optimization-constraints-table`. .. table:: AMDHSA Memory Model Single Thread Optimization Constraints; :name: amdgpu-amdhsa-memory-model-single-thread-optimization-constraints-table. ============ ==============================================================; LLVM Memory Optimization Constraints; Ordering; ============ ==============================================================; unordered *none*; monotonic *none*; acquire - If a load atomic/atomicrmw then no following load/load; atomic/store/store atomic/atomicrmw/fence instruction can be; moved before the acquire.; - If a fence then same as load atomic, plus no preceding; associated fence-paired-atomic can be moved after the fence.; release - If a store atomic/atomicrmw then no preceding load/load; atomic/store/store atomic/ato",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:204094,load,load,204094,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"stores, etc). Experience with SSA forms for memory in other compilers has shown that; it is simply not possible to do this precisely, and in fact, doing it; precisely is not worth it, because now all the optimizations have to; walk tons and tons of virtual variables and phi nodes. So we partition. At the point at which you partition, again,; experience has shown us there is no point in partitioning to more than; one variable. It simply generates more IR, and optimizations still; have to query something to disambiguate further anyway. As a result, LLVM partitions to one variable. Precision in practice; ^^^^^^^^^^^^^^^^^^^^^. In practice, there are implementation details in LLVM that also affect the; results' precision provided by ``MemorySSA``. For example, AliasAnalysis has various; caps, or restrictions on looking through phis which can affect what ``MemorySSA``; can infer. Changes made by different passes may make MemorySSA either ""overly; optimized"" (it can provide a more accurate result than if it were recomputed; from scratch), or ""under optimized"" (it could infer more if it were recomputed).; This can lead to challenges to reproduced results in isolation with a single pass; when the result relies on the state acquired by ``MemorySSA`` due to being updated by; multiple subsequent passes.; Passes that use and update ``MemorySSA`` should do so through the APIs provided by the; ``MemorySSAUpdater``, or through calls on the Walker.; Direct optimizations to ``MemorySSA`` are not permitted.; There is currently a single, narrowly scoped exception where DSE (DeadStoreElimination); updates an optimized access of a store, after a traversal that guarantees the; optimization is correct. This is solely allowed due to the traversals and inferences; being beyond what ``MemorySSA`` does and them being ""free"" (i.e. DSE does them anyway).; This exception is set under a flag (""-dse-optimize-memoryssa"") and can be disabled to; help reproduce optimizations in isolation. LLVM Develop",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:18825,optimiz,optimized,18825,interpreter/llvm-project/llvm/docs/MemorySSA.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst,2,['optimiz'],['optimized']
Performance,"strained. This is required for correct LLVM IR. Optimizations that; move code around can create miscompiles if mixing of constrained and normal; operations is done. The correct way to mix constrained and less constrained; operations is to use the rounding mode and exception handling metadata to; mark constrained intrinsics as having LLVM's default behavior. Each of these intrinsics corresponds to a normal floating-point operation. The; data arguments and the return value are the same as the corresponding FP; operation. The rounding mode argument is a metadata string specifying what; assumptions, if any, the optimizer can make when transforming constant; values. Some constrained FP intrinsics omit this argument. If required; by the intrinsic, this argument must be one of the following strings:. ::. ""round.dynamic""; ""round.tonearest""; ""round.downward""; ""round.upward""; ""round.towardzero""; ""round.tonearestaway"". If this argument is ""round.dynamic"" optimization passes must assume that the; rounding mode is unknown and may change at runtime. No transformations that; depend on rounding mode may be performed in this case. The other possible values for the rounding mode argument correspond to the; similarly named IEEE rounding modes. If the argument is any of these values; optimization passes may perform transformations as long as they are consistent; with the specified rounding mode. For example, 'x-0'->'x' is not a valid transformation if the rounding mode is; ""round.downward"" or ""round.dynamic"" because if the value of 'x' is +0 then; 'x-0' should evaluate to '-0' when rounding downward. However, this; transformation is legal for all other rounding modes. For values other than ""round.dynamic"" optimization passes may assume that the; actual runtime rounding mode (as defined in a target-specific manner) matches; the specified rounding mode, but this is not guaranteed. Using a specific; non-dynamic rounding mode which does not match the actual rounding mode at; runtime result",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:868716,optimiz,optimization,868716,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['optimiz'],['optimization']
Performance,"streamerinfo object in the record for that class. In this way, each; class is recursively decomposed into its atomic elements, each of which is a simple; type (e.g. ""int""). A ""long"" or ""unsigned long"" member is always written; as an 8 byte quantity, even if it occupies only 4 bytes in memory. A data member of a class is marked transient on the line of its declaration by a; comment beginning with ""//!"". Such members are not written to disk, nor is there; any streamerinfo for such a member. A data member that is a C++ pointer (not to be confused with ""pointers to persistent; objects"" described below) is never written to disk as a pointer value. If it is a; pointer to an object, the object itself (or 0 (4 bytes) if the pointer value is NULL); is written. If the declaration line has a comment beginning with ""//->"", this indicates; that the pointer value will never be null, which allows a performance optimization.; Another optimization is that if two or more pointers pointing to the same object are; streamed in the same I/O operation, the object is written only once. The remaining; pointers reference the object through a unique object identifier. This saves space; and avoids the infinite loop that might otherwise arise if the directed graph of object; instance pointer references contains a cycle. If a data member is a pointer to a simple type, the Streamer presumes it is an array,; with the dimension defined in a comment of the form ""//[<length>]"", where length is; either an integer constant or a variable that is an integer data member of the class.; If a variable is used, it must be defined ahead of its use or in a base class. The above describes the function of the StreamerInfo record in decomposing a; self-identifying object if the user uses the streamer generated by ""rootcint"".; There are two reasons why a user may need to write a specialized streamer for a class.; One reason is that it may be necessary to execute some code before or after data is read; or written, f",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/io/doc/TFile/README.md:8095,optimiz,optimization,8095,io/doc/TFile/README.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/io/doc/TFile/README.md,1,['optimiz'],['optimization']
Performance,"struction if the; instruction either remains in its basic block, or if its basic block is folded; into a predecessor that branches unconditionally. The APIs to use are; ``IRBuilder``, or ``Instruction::setDebugLoc``. The purpose of this rule is to ensure that common block-local optimizations; preserve the ability to set breakpoints on source locations corresponding to; the instructions they touch. Debugging, crash logs, and SamplePGO accuracy; would be severely impacted if that ability were lost. Examples of transformations that should follow this rule include:. * Instruction scheduling. Block-local instruction reordering should not drop; source locations, even though this may lead to jumpy single-stepping; behavior. * Simple jump threading. For example, if block ``B1`` unconditionally jumps to; ``B2``, *and* is its unique predecessor, instructions from ``B2`` can be; hoisted into ``B1``. Source locations from ``B2`` should be preserved. * Peephole optimizations that replace or expand an instruction, like ``(add X; X) => (shl X 1)``. The location of the ``shl`` instruction should be the same; as the location of the ``add`` instruction. * Tail duplication. For example, if blocks ``B1`` and ``B2`` both; unconditionally branch to ``B3`` and ``B3`` can be folded into its; predecessors, source locations from ``B3`` should be preserved. Examples of transformations for which this rule *does not* apply include:. * LICM. E.g., if an instruction is moved from the loop body to the preheader,; the rule for :ref:`dropping locations<WhenToDropLocation>` applies. In addition to the rule above, a transformation should also preserve the debug; location of an instruction that is moved between basic blocks, if the; destination block already contains an instruction with an identical debug; location. Examples of transformations that should follow this rule include:. * Moving instructions between basic blocks. For example, if instruction ``I1``; in ``BB1`` is moved before ``I2`` in ``BB2",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToUpdateDebugInfo.rst:1880,optimiz,optimizations,1880,interpreter/llvm-project/llvm/docs/HowToUpdateDebugInfo.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToUpdateDebugInfo.rst,1,['optimiz'],['optimizations']
Performance,"structions and the variable debug information from MMI is serialized right; now. These limitations impose restrictions on what you can test with the MIR format.; For now, tests that would like to test some behaviour that depends on the state; of temporary or local ``MCSymbol`` operands or the exception handling state in; MMI, can't use the MIR format. As well as that, tests that test some behaviour; that depends on the state of the target specific ``MachineFunctionInfo`` or; ``MachineConstantPoolValue`` subclasses can't use the MIR format at the moment. High Level Structure; ====================. .. _embedded-module:. Embedded Module; ---------------. When the first YAML document contains a `YAML block literal string`_, the MIR; parser will treat this string as an LLVM assembly language string that; represents an embedded LLVM IR module.; Here is an example of a YAML document that contains an LLVM module:. .. code-block:: llvm. define i32 @inc(i32* %x) {; entry:; %0 = load i32, i32* %x; %1 = add i32 %0, 1; store i32 %1, i32* %x; ret i32 %1; }. .. _YAML block literal string: http://www.yaml.org/spec/1.2/spec.html#id2795688. Machine Functions; -----------------. The remaining YAML documents contain the machine functions. This is an example; of such YAML document:. .. code-block:: text. ---; name: inc; tracksRegLiveness: true; liveins:; - { reg: '$rdi' }; callSites:; - { bb: 0, offset: 3, fwdArgRegs:; - { arg: 0, reg: '$edi' } }; body: |; bb.0.entry:; liveins: $rdi. $eax = MOV32rm $rdi, 1, _, 0, _; $eax = INC32r killed $eax, implicit-def dead $eflags; MOV32mr killed $rdi, 1, _, 0, _, $eax; CALL64pcrel32 @foo <regmask...>; RETQ $eax; ... The document above consists of attributes that represent the various; properties and data structures in a machine function. The attribute ``name`` is required, and its value should be identical to the; name of a function that this machine function is based on. The attribute ``body`` is a `YAML block literal string`_. Its value represent",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:6784,load,load,6784,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst,1,['load'],['load']
Performance,"structions are often used; when implementing cryptographic primitives dealing with private key data; because they are not believed to provide any side-channels. Similarly, we can; defer hardening until after them as they will not in-and-of-themselves; introduce a speculative execution side-channel. This results in code sequences; that look like:; ```; ... .LBB0_4: # %danger; cmovneq %r8, %rax # Conditionally update predicate state.; addl (%rsi), %edi # Load and accumulate without leaking.; orl %eax, %edi; ```. While an addition happens to the loaded (potentially secret) value, that; doesn't leak any data and we then immediately harden it. ###### Hardening of loaded values deferred down the data-invariant expression graph. We can generalize the previous idea and sink the hardening down the expression; graph across as many data-invariant operations as desirable. This can use very; conservative rules for whether something is data-invariant. The primary goal; should be to handle multiple loads with a single hardening instruction:; ```; ... .LBB0_4: # %danger; cmovneq %r8, %rax # Conditionally update predicate state.; addl (%rsi), %edi # Load and accumulate without leaking.; addl 4(%rsi), %edi # Continue without leaking.; addl 8(%rsi), %edi; orl %eax, %edi # Mask out bits from all three loads.; ```. ###### Preserving the flags while hardening loaded values on Haswell, Zen, and newer processors. Sadly, there are no useful instructions on x86 that apply a mask to all 64 bits; without touching the flag registers. However, we can harden loaded values that; are narrower than a word (fewer than 32-bits on 32-bit systems and fewer than; 64-bits on 64-bit systems) by zero-extending the value to the full word size; and then shifting right by at least the number of original bits using the BMI2; `shrx` instruction:; ```; ... .LBB0_4: # %danger; cmovneq %r8, %rax # Conditionally update predicate state.; addl (%rsi), %edi # Load and accumulate 32 bits of data.; shrxq %rax, %rdi, %rdi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md:26364,load,loads,26364,interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,1,['load'],['loads']
Performance,"structions can textually reference it. However, the isomorphic in-memory; representation that you manipulate from C++ has no such restriction since; instructions can simply keep pointers to any other ``Value``'s that they; reference. In fact, the names of dummy numbered temporaries like ``%1`` are; not explicitly represented in the in-memory representation at all (see; ``Value::getName()``). Source Languages; ================. What source languages are supported?; ------------------------------------. LLVM currently has full support for C and C++ source languages through; `Clang <https://clang.llvm.org/>`_. Many other language frontends have; been written using LLVM, and an incomplete list is available at; `projects with LLVM <https://llvm.org/ProjectsWithLLVM/>`_. I'd like to write a self-hosting LLVM compiler. How should I interface with the LLVM middle-end optimizers and back-end code generators?; ----------------------------------------------------------------------------------------------------------------------------------------; Your compiler front-end will communicate with LLVM by creating a module in the; LLVM intermediate representation (IR) format. Assuming you want to write your; language's compiler in the language itself (rather than C++), there are 3; major ways to tackle generating LLVM IR from a front-end:. 1. **Call into the LLVM libraries code using your language's FFI (foreign; function interface).**. * *for:* best tracks changes to the LLVM IR, .ll syntax, and .bc format. * *for:* enables running LLVM optimization passes without a emit/parse; overhead. * *for:* adapts well to a JIT context. * *against:* lots of ugly glue code to write. 2. **Emit LLVM assembly from your compiler's native language.**. * *for:* very straightforward to get started. * *against:* the .ll parser is slower than the bitcode reader when; interfacing to the middle end. * *against:* it may be harder to track changes to the IR. 3. **Emit LLVM bitcode from your compiler's nati",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/FAQ.rst:3143,optimiz,optimizers,3143,interpreter/llvm-project/llvm/docs/FAQ.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/FAQ.rst,1,['optimiz'],['optimizers']
Performance,"strumentation is disabled; for this function.; ``null_pointer_is_valid``; If ``null_pointer_is_valid`` is set, then the ``null`` address; in address-space 0 is considered to be a valid address for memory loads and; stores. Any analysis or optimization should not treat dereferencing a; pointer to ``null`` as undefined behavior in this function.; Note: Comparing address of a global variable to ``null`` may still; evaluate to false because of a limitation in querying this attribute inside; constant expressions.; ``optdebug``; This attribute suggests that optimization passes and code generator passes; should make choices that try to preserve debug info without significantly; degrading runtime performance.; This attribute is incompatible with the ``minsize``, ``optsize``, and; ``optnone`` attributes.; ``optforfuzzing``; This attribute indicates that this function should be optimized; for maximum fuzzing signal.; ``optnone``; This function attribute indicates that most optimization passes will skip; this function, with the exception of interprocedural optimization passes.; Code generation defaults to the ""fast"" instruction selector.; This attribute cannot be used together with the ``alwaysinline``; attribute; this attribute is also incompatible; with the ``minsize``, ``optsize``, and ``optdebug`` attributes. This attribute requires the ``noinline`` attribute to be specified on; the function as well, so the function is never inlined into any caller.; Only functions with the ``alwaysinline`` attribute are valid; candidates for inlining into the body of this function.; ``optsize``; This attribute suggests that optimization passes and code generator; passes make choices that keep the code size of this function low,; and otherwise do optimizations specifically to reduce code size as; long as they do not significantly impact runtime performance.; This attribute is incompatible with the ``optdebug`` and ``optnone``; attributes.; ``""patchable-function""``; This attribute tells the ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:95795,optimiz,optimization,95795,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,2,['optimiz'],['optimization']
Performance,"sts whether the given pointer is associated; with the given type identifier. .. _type.checked.load:. '``llvm.type.checked.load``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare {ptr, i1} @llvm.type.checked.load(ptr %ptr, i32 %offset, metadata %type) nounwind memory(argmem: read). Arguments:; """""""""""""""""""". The first argument is a pointer from which to load a function pointer. The; second argument is the byte offset from which to load the function pointer. The; third argument is a metadata object representing a :doc:`type identifier; <TypeMetadata>`. Overview:; """""""""""""""""". The ``llvm.type.checked.load`` intrinsic safely loads a function pointer from a; virtual table pointer using type metadata. This intrinsic is used to implement; control flow integrity in conjunction with virtual call optimization. The; virtual call optimization pass will optimize away ``llvm.type.checked.load``; intrinsics associated with devirtualized calls, thereby removing the type; check in cases where it is not needed to enforce the control flow integrity; constraint. If the given pointer is associated with a type metadata identifier, this; function returns true as the second element of its return value. (Note that; the function may also return true if the given pointer is not associated; with a type metadata identifier.) If the function's return value's second; element is true, the following rules apply to the first element:. - If the given pointer is associated with the given type metadata identifier,; it is the function pointer loaded from the given byte offset from the given; pointer. - If the given pointer is not associated with the given type metadata; identifier, it is one of the following (the choice of which is unspecified):. 1. The function pointer that would have been loaded from an arbitrarily chosen; (through an unspecified mechanism) pointer associated with the type; metadata. 2. If the function has a non-void return type, a pointer to a function that",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:938500,load,load,938500,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,"subsequent loads from; speculatively executing until all prior mispredicted predicates have resolved.; We can use this broader barrier to speculative loads executing between; functions. We emit it in the entry block to handle calls, and prior to each; return. This approach also has the advantage of providing the strongest degree; of mitigation when mixed with unmitigated code by halting all misspeculation; entering a function which is mitigated, regardless of what occurred in the; caller. However, such a mixture is inherently more risky. Whether this kind of; mixture is a sufficient mitigation requires careful analysis. Unfortunately, experimental results indicate that the performance overhead of; this approach is very high for certain patterns of code. A classic example is; any form of recursive evaluation engine. The hot, rapid call and return; sequences exhibit dramatic performance loss when mitigated with `lfence`. This; component alone can regress performance by 2x or more, making it an unpleasant; tradeoff even when only used in a mixture of code. ##### Use an internal TLS location to pass predicate state. We can define a special thread-local value to hold the predicate state between; functions. This avoids direct ABI implications by using a side channel between; callers and callees to communicate the predicate state. It also allows implicit; zero-initialization of the state, which allows non-checked code to be the first; code executed. However, this requires a load from TLS in the entry block, a store to TLS; before every call and every ret, and a load from TLS after every call. As a; consequence it is expected to be substantially more expensive even than using; `%rsp` and potentially `lfence` within the function entry block. ##### Define a new ABI and/or calling convention. We could define a new ABI and/or calling convention to explicitly pass the; predicate state in and out of functions. This may be interesting if none of the; alternatives have adequate perf",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md:42289,perform,performance,42289,interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,1,['perform'],['performance']
Performance,"such as ``llvm-tblgen`` and ``clang-tblgen``). This is; intended for cross-compiling: if the user sets this variable and the; directory contains executables with the expected names, no separate; native versions of those executables will be built. **LLVM_NO_INSTALL_NAME_DIR_FOR_BUILD_TREE**:BOOL; Defaults to ``OFF``. If set to ``ON``, CMake's default logic for library IDs; on Darwin in the build tree will be used. Otherwise the install-time library; IDs will be used in the build tree as well. Mainly useful when other CMake; library ID control variables (e.g., ``CMAKE_INSTALL_NAME_DIR``) are being; set to non-standard values. **LLVM_OPTIMIZED_TABLEGEN**:BOOL; If enabled and building a debug or asserts build the CMake build system will; generate a Release build tree to build a fully optimized tablegen for use; during the build. Enabling this option can significantly speed up build times; especially when building LLVM in Debug configurations. **LLVM_PARALLEL_COMPILE_JOBS**:STRING; Define the maximum number of concurrent compilation jobs. **LLVM_PARALLEL_LINK_JOBS**:STRING; Define the maximum number of concurrent link jobs. **LLVM_RAM_PER_COMPILE_JOB**:STRING; Calculates the amount of Ninja compile jobs according to available resources.; Value has to be in MB, overwrites LLVM_PARALLEL_COMPILE_JOBS. Compile jobs ; will be between one and amount of logical cores. **LLVM_RAM_PER_LINK_JOB**:STRING; Calculates the amount of Ninja link jobs according to available resources.; Value has to be in MB, overwrites LLVM_PARALLEL_LINK_JOBS. Link jobs will ; be between one and amount of logical cores. Link jobs will not run ; exclusively therefore you should add an offset of one or two compile jobs ; to be sure its not terminated in your memory restricted environment. On ELF; platforms also consider ``LLVM_USE_SPLIT_DWARF`` in Debug build. **LLVM_PROFDATA_FILE**:PATH; Path to a profdata file to pass into clang's -fprofile-instr-use flag. This; can only be specified if you're building wi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CMake.rst:32463,concurren,concurrent,32463,interpreter/llvm-project/llvm/docs/CMake.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CMake.rst,1,['concurren'],['concurrent']
Performance,"sult type larger than the vector element type.; However, the reduction is performed using the vector element type and the value; in the top bits is unspecified. Memory Operations; -----------------. G_LOAD, G_SEXTLOAD, G_ZEXTLOAD; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Generic load. Expects a MachineMemOperand in addition to explicit; operands. If the result size is larger than the memory size, the; high bits are undefined, sign-extended, or zero-extended respectively. Only G_LOAD is valid if the result is a vector type. If the result is larger; than the memory size, the high elements are undefined (i.e. this is not a; per-element, vector anyextload). Unlike in SelectionDAG, atomic loads are expressed with the same; opcodes as regular loads. G_LOAD, G_SEXTLOAD and G_ZEXTLOAD may all; have atomic memory operands. G_INDEXED_LOAD; ^^^^^^^^^^^^^^. Generic indexed load. Combines a GEP with a load. $newaddr is set to $base + $offset.; If $am is 0 (post-indexed), then the value is loaded from $base; if $am is 1 (pre-indexed); then the value is loaded from $newaddr. G_INDEXED_SEXTLOAD; ^^^^^^^^^^^^^^^^^^. Same as G_INDEXED_LOAD except that the load performed is sign-extending, as with G_SEXTLOAD. G_INDEXED_ZEXTLOAD; ^^^^^^^^^^^^^^^^^^. Same as G_INDEXED_LOAD except that the load performed is zero-extending, as with G_ZEXTLOAD. G_STORE; ^^^^^^^. Generic store. Expects a MachineMemOperand in addition to explicit; operands. If the stored value size is greater than the memory size,; the high bits are implicitly truncated. If this is a vector store, the; high elements are discarded (i.e. this does not function as a per-lane; vector, truncating store). G_INDEXED_STORE; ^^^^^^^^^^^^^^^. Combines a store with a GEP. See description of G_INDEXED_LOAD for indexing behaviour. G_ATOMIC_CMPXCHG_WITH_SUCCESS; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Generic atomic cmpxchg with internal success check. Expects a; MachineMemOperand in addition to explicit operands. G_ATOMIC_CMPXCHG; ^^^^^^^^^^^^^^^^. Generic",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GlobalISel/GenericOpcode.rst:16219,load,loaded,16219,interpreter/llvm-project/llvm/docs/GlobalISel/GenericOpcode.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GlobalISel/GenericOpcode.rst,2,['load'],['loaded']
Performance,"sults in a poison value.; %poison2 = sub i32 poison, 1 ; Also results in a poison value.; %still_poison = and i32 %poison, 0 ; 0, but also poison.; %poison_yet_again = getelementptr i32, ptr @h, i32 %still_poison; store i32 0, ptr %poison_yet_again ; Undefined behavior due to; ; store to poison. store i32 %poison, ptr @g ; Poison value stored to memory.; %poison3 = load i32, ptr @g ; Poison value loaded back from memory. %poison4 = load i16, ptr @g ; Returns a poison value.; %poison5 = load i64, ptr @g ; Returns a poison value. %cmp = icmp slt i32 %poison, 0 ; Returns a poison value.; br i1 %cmp, label %end, label %end ; undefined behavior. end:. .. _welldefinedvalues:. Well-Defined Values; -------------------. Given a program execution, a value is *well defined* if the value does not; have an undef bit and is not poison in the execution.; An aggregate value or vector is well defined if its elements are well defined.; The padding of an aggregate isn't considered, since it isn't visible; without storing it into memory and loading it with a different type. A constant of a :ref:`single value <t_single_value>`, non-vector type is well; defined if it is neither '``undef``' constant nor '``poison``' constant.; The result of :ref:`freeze instruction <i_freeze>` is well defined regardless; of its operand. .. _blockaddress:. Addresses of Basic Blocks; -------------------------. ``blockaddress(@function, %block)``. The '``blockaddress``' constant computes the address of the specified; basic block in the specified function. It always has an ``ptr addrspace(P)`` type, where ``P`` is the address space; of the function containing ``%block`` (usually ``addrspace(0)``). Taking the address of the entry block is illegal. This value only has defined behavior when used as an operand to the; ':ref:`indirectbr <i_indirectbr>`' or for comparisons against null. Pointer; equality tests between labels addresses results in undefined behavior ---; though, again, comparison against null is ok, ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:199536,load,loading,199536,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['loading']
Performance,"sure that the shared library is rebuilt you can use the ++; syntax:. ``` {.cpp}; root[] .L MyScript.C++; ```. To build, load, and execute the function with the same name as the; file you can use the `.x` command. This is the same as executing a; named script; you can also provide parameters. The only; difference is you need to append a + or a ++. ``` {.cpp}; root[] .x MyScript.C+(4000); Creating shared library /home/./MyScript_C.so; ```. You can select whether the script in compiled with debug symbol or; with optimization by appending the letter 'g' or 'O' after the '+' or; '++'. Without the specification, the script is compiled with the same; level of debugging symbol and optimization as the currently running; ROOT executable. For example:. ``` {.cpp}; root[] .L MyScript.C++g; ```. will compile `MyScript.C` with debug symbols; usually this means; giving the `-g` option to compiler. ``` {.cpp}; root[] .L MyScript.C++O; ```. will compile `MyScript.C` with optimizations; usually this means; giving the `-O` option to compiler. The syntax:. ``` {.cpp}; root[] .L MyScript.C++; ```. is using the default optimization level. The initial default is to; compile with the same level of optimization as the root executable; itself. The default can be changed by:. ``` {.cpp}; root[] gSystem->SetAclicMode(TSystem::kDebug);; root[] gSystem->SetAclicMode(TSystem::kOpt);; ```. Note that the commands:. ``` {.cpp}; root[] .L MyScript.C+g; root[] .L MyScript.C+O; ```. respectively compile `MyScript.C` with debug and optimization if the; library does not exist yet; they will not change the debug and the; optimization level if the library already exist and it is up to date.; To use ACLiC from compiled code or from inside another macro, we; recommend using `gROOT->ProcessLine()`. For; example, in one script you can use ACLiC to compile and load another; script. ``` {.cpp}; gROOT->ProcessLine("".L MyScript.C+""); gROOT->ProcessLine("".L MyScript.C++""); ```. ### Setting the Include Path. You can",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Cling.md:15387,optimiz,optimizations,15387,documentation/users-guide/Cling.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Cling.md,1,['optimiz'],['optimizations']
Performance,"sures that; following; loads will not see; stale data. load atomic acquire - agent - global 1. buffer/global_load; glc=1; 2. s_waitcnt vmcnt(0). - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the load; has completed; before invalidating; the cache. 3. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale global data. load atomic acquire - system - global 1. buffer/global/flat_load; glc=1; 2. s_waitcnt vmcnt(0). - Must happen before; following buffer_invl2 and; buffer_wbinvl1_vol.; - Ensures the load; has completed; before invalidating; the cache. 3. buffer_invl2;; buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale L1 global data,; nor see stale L2 MTYPE; NC global data.; MTYPE RW and CC memory will; never be stale in L2 due to; the memory probes. load atomic acquire - agent - generic 1. flat_load glc=1; 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL omit; lgkmcnt(0).; - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the flat_load; has completed; before invalidating; the cache. 3. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. load atomic acquire - system - generic 1. flat_load glc=1; 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL omit; lgkmcnt(0).; - Must happen before; following; buffer_invl2 and; buffer_wbinvl1_vol.; - Ensures the flat_load; has completed; before invalidating; the caches. 3. buffer_invl2;; buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale L1 global data,; nor see stale L2 MTYPE; NC global dat",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:247685,load,load,247685,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"sures the load; has completed; before invalidating; the cache. 3. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale global data. load atomic acquire - agent - generic 1. flat_load glc=1; - system 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If OpenCL omit; lgkmcnt(0).; - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the flat_load; has completed; before invalidating; the cache. 3. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acquire - singlethread - global 1. buffer/global/ds/flat_atomic; - wavefront - local; - generic; atomicrmw acquire - workgroup - global 1. buffer/global_atomic; atomicrmw acquire - workgroup - local 1. ds/flat_atomic; - generic 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local; atomicrmw value; being acquired. atomicrmw acquire - agent - global 1. buffer/global_atomic; - system 2. s_waitcnt vmcnt(0). - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the; atomicrmw has; completed before; invalidating the; cache. 3. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acquire - agent - generic 1. flat_atomic; - system 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If OpenCL, omit; lgkmcnt(0).; - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the; atomicrmw has; completed before; invalidating the; cache. 3. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. fence acquire - singleth",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:215745,load,load,215745,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"sy to use and obtain, and strives to maximize the quantity and impact of scientific ; results obtained per unit cost, both of human effort and computing resources. ROOT provides a very efficient storage system for data models, ; that demonstrated to scale at the Large Hadron Collider experiments: Exabytes ; of scientific data are written in columnar ROOT format.; ROOT comes with histogramming capabilities in an arbitrary number of ; dimensions, curve fitting, statistical modelling, minimization, to allow; the easy setup of a data analysis system that can query and process the data; interactively or in batch mode, as well as a general parallel processing; framework, RDataFrame, that can considerably speed up an analysis, taking ; full advantage of multi-core and distributed systems. ROOT is performance critical software written in C++ and enables rapid prototyping ; powered by a unique C++ compliant interpreter called Cling. ; Cling also enables performant C++ type introspection which is a building block of automatic ; interoperability with Python. Thanks to PyROOT, leveraging the cppyy technology, ; ROOT offers efficient, on-demand C++/Python interoperability in a uniform cross-language ; execution environment. ROOT fully embraces open-source, it's made with passion by its community,; for the benefit of its community. [![License: LGPL v2.1+](https://img.shields.io/badge/License-LGPL%20v2.1+-blue.svg)](https://www.gnu.org/licenses/lgpl.html); [![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/5060/badge)](https://bestpractices.coreinfrastructure.org/projects/5060). ## Contribution Guidelines; - [How to contribute](https://github.com/root-project/root/blob/master/CONTRIBUTING.md); - [Coding conventions](https://root.cern/coding-conventions); - [Meetings](https://root.cern/meetings). ## Cite; When citing ROOT, please use both the reference reported below and the DOI specific to your ROOT version available [on Zenodo](https://zenodo.org/badge/l",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README.md:1367,perform,performant,1367,README.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README.md,1,['perform'],['performant']
Performance,"symbol table it creates; includes both native and bitcode symbols. *Deterministic Archives*. By default, :program:`llvm-ar` always uses zero for timestamps and UIDs/GIDs; to write archives in a deterministic mode. This is equivalent to the; :option:`D` modifier being enabled by default. If you wish to maintain; compatibility with other :program:`ar` implementations, you can pass the; :option:`U` modifier to write actual timestamps and UIDs/GIDs. *Windows Paths*. When on Windows :program:`llvm-ar` treats the names of archived *files* in the same; case sensitive manner as the operating system. When on a non-Windows machine; :program:`llvm-ar` does not consider character case. OPTIONS; -------. :program:`llvm-ar` operations are compatible with other :program:`ar`; implementations. However, there are a few modifiers (:option:`L`) that are not; found in other :program:`ar` implementations. The options for; :program:`llvm-ar` specify a single basic Operation to perform on the archive,; a variety of Modifiers for that Operation, the name of the archive file, and an; optional list of file names. If the *files* option is not specified, it; generally means either ""none"" or ""all"" members, depending on the operation. The; Options, Operations and Modifiers are explained in the sections below. The minimal set of options is at least one operator and the name of the; archive. Operations; ~~~~~~~~~~. .. option:: d [NT]. Delete files from the ``archive``. The :option:`N` and :option:`T` modifiers; apply to this operation. The *files* options specify which members should be; removed from the archive. It is not an error if a specified file does not; appear in the archive. If no *files* are specified, the archive is not; modified. .. option:: m [abi]. Move files from one location in the ``archive`` to another. The :option:`a`,; :option:`b`, and :option:`i` modifiers apply to this operation. The *files*; will all be moved to the location given by the modifiers. If no modifiers are; used, ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-ar.rst:2455,perform,perform,2455,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-ar.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-ar.rst,1,['perform'],['perform']
Performance,"symbol. Unlike regular reexports however, lookups of lazy reexports; do not trigger immediate materialization of the reexported symbol. Instead, they; only trigger materialization of a function stub. This function stub is; initialized to point at a *lazy call-through*, which provides reentry into the; JIT. If the stub is called at runtime then the lazy call-through will look up; the reexported symbol (triggering materialization for it if necessary), update; the stub (to call directly to the reexported symbol on subsequent calls), and; then return via the reexported symbol. By re-using the existing symbol lookup; mechanism, lazy reexports inherit the same concurrency guarantees: calls to lazy; reexports can be made from multiple threads concurrently, and the reexported; symbol can be any state of compilation (uncompiled, already in the process of; being compiled, or already compiled) and the call will succeed. This allows; laziness to be safely mixed with features like remote compilation, concurrent; compilation, concurrent JIT'd code, and speculative compilation. There is one other key difference between regular reexports and lazy reexports; that some clients must be aware of: The address of a lazy reexport will be; *different* from the address of the reexported symbol (whereas a regular; reexport is guaranteed to have the same address as the reexported symbol).; Clients who care about pointer equality will generally want to use the address; of the reexport as the canonical address of the reexported symbol. This will; allow the address to be taken without forcing materialization of the reexport. Usage example:. If JITDylib ``JD`` contains definitions for symbols ``foo_body`` and; ``bar_body``, we can create lazy entry points ``Foo`` and ``Bar`` in JITDylib; ``JD2`` by calling:. .. code-block:: c++. auto ReexportFlags = JITSymbolFlags::Exported | JITSymbolFlags::Callable;; JD2.define(; lazyReexports(CallThroughMgr, StubsMgr, JD,; SymbolAliasMap({; { Mangle(""foo""), { M",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:17754,concurren,concurrent,17754,interpreter/llvm-project/llvm/docs/ORCv2.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst,2,['concurren'],['concurrent']
Performance,"t %reg1037 is an; operand of the PHI node in bb76 and its operand %reg1039 is the result of the; PHI node. We should treat it as a two-address code and make sure the ADDri is; scheduled after any node that reads %reg1039. //===---------------------------------------------------------------------===//. Use local info (i.e. register scavenger) to assign it a free register to allow; reuse:; ldr r3, [sp, #+4]; add r3, r3, #3; ldr r2, [sp, #+8]; add r2, r2, #2; ldr r1, [sp, #+4] <==; add r1, r1, #1; ldr r0, [sp, #+4]; add r0, r0, #2. //===---------------------------------------------------------------------===//. LLVM aggressively lift CSE out of loop. Sometimes this can be negative side-; effects:. R1 = X + 4; R2 = X + 7; R3 = X + 15. loop:; load [i + R1]; ...; load [i + R2]; ...; load [i + R3]. Suppose there is high register pressure, R1, R2, R3, can be spilled. We need; to implement proper re-materialization to handle this:. R1 = X + 4; R2 = X + 7; R3 = X + 15. loop:; R1 = X + 4 @ re-materialized; load [i + R1]; ...; R2 = X + 7 @ re-materialized; load [i + R2]; ...; R3 = X + 15 @ re-materialized; load [i + R3]. Furthermore, with re-association, we can enable sharing:. R1 = X + 4; R2 = X + 7; R3 = X + 15. loop:; T = i + X; load [T + 4]; ...; load [T + 7]; ...; load [T + 15]; //===---------------------------------------------------------------------===//. It's not always a good idea to choose rematerialization over spilling. If all; the load / store instructions would be folded then spilling is cheaper because; it won't require new live intervals / registers. See 2003-05-31-LongShifts for; an example. //===---------------------------------------------------------------------===//. With a copying garbage collector, derived pointers must not be retained across; collector safe points; the collector could move the objects and invalidate the; derived pointer. This is bad enough in the first place, but safe points can; crop up unpredictably. Consider:. %array = load { i32, [0",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/README.txt:2011,load,load,2011,interpreter/llvm-project/llvm/lib/CodeGen/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/README.txt,1,['load'],['load']
Performance,"t CUs and so a ``buffer_inv sc0`` is required which will invalidate; the L1 cache. * A ``buffer_inv sc0`` is required to invalidate the L1 cache for coherence; between wavefronts executing in different work-groups as they may be; executing on different CUs. * Atomic read-modify-write instructions implicitly bypass the L1 cache.; Therefore, they do not use the sc0 bit for coherence and instead use it to; indicate if the instruction returns the original value being updated. They; do use sc1 to indicate system or agent scope coherence. * The scalar memory operations access a scalar L1 cache shared by all wavefronts; on a group of CUs. The scalar and vector L1 caches are not coherent. However,; scalar operations are used in a restricted way so do not impact the memory; model. See :ref:`amdgpu-amdhsa-memory-spaces`.; * The vector and scalar memory operations use an L2 cache. * The gfx942 can be configured as a number of smaller agents with each having; a single L2 shared by all CUs on the same agent, or as fewer (possibly one); larger agents with groups of CUs on each agent each sharing separate L2; caches.; * The L2 cache has independent channels to service disjoint ranges of virtual; addresses.; * Each CU has a separate request queue per channel for its associated L2.; Therefore, the vector and scalar memory operations performed by wavefronts; executing with different L1 caches and the same L2 cache can be reordered; relative to each other.; * A ``s_waitcnt vmcnt(0)`` is required to ensure synchronization between; vector memory operations of different CUs. It ensures a previous vector; memory operation has completed before executing a subsequent vector memory; or LDS operation and so can be used to meet the requirements of acquire and; release.; * An L2 cache can be kept coherent with other L2 caches by using the MTYPE RW; (read-write) for memory local to the L2, and MTYPE NC (non-coherent) with; the PTE C-bit set for memory not local to the L2. * Any local memory cache",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:287526,cache,caches,287526,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['cache'],['caches']
Performance,"t Crossing Point. Supposing we have found out that a particle will cross a boundary during; the next step, it is sometimes useful to compute the normal to the; crossed surface. The modeller uses the following convention: we define; as `normal` (\f$\vec{n}\f$) the unit vector perpendicular; to a surface in the `next crossing point`, having the orientation such; that: \f$\vec{n}.\vec{d}>0\f$. Here \f$\vec{d}\f$; represents the current direction. The next crossing point represents the; point where a ray shot from the current point along the current; direction crosses the surface. ~~~{.cpp}; Double_t *TGeoManager::FindNormal(Bool_t forward=kTRUE);; ~~~. The method above computes the normal to the next crossed surface in; forward or backward direction (i.e. the current one), assuming the state; corresponding to a current arbitrary point is initialized. An example of; usage of normal computation is ray tracing. The two most important features of the geometrical modeller concerning; tracking are scalability and performance as function of the total number; of physical nodes. The first refers to the possibility to make use of; the available memory resources and at the same time be able to resolve; any geometrical query, while the second defines the capability of the; modeller to respond quickly even for huge geometries. These parameters; can become critical when simulating big experiments like those at LHC. \anchor GP02h; ### Creating and Visualizing Tracks. In case the modeller is interfaced with a tracking engine, one might; consider quite useful being able to store and visualize at least a part; of the tracks in the context of the geometry. The base class; TVirtualGeoTrack provides this functionality. It currently has one; implementation inside the drawing package (TGeoTrack class). A; track can be defined like:. ~~~{.cpp}; TVirtualGeoTrack(Int_t id,Int_t pdg,TVirtualGeoTrack *parent=0,; TObject *particle=0);; ~~~. Where: `id` is user-defined id of the track, `pdg` - `pdg`",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md:84319,scalab,scalability,84319,geom/geom/doc/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md,2,"['perform', 'scalab']","['performance', 'scalability']"
Performance,"t MaterializationResponsibility &R) {; // Create a function pass manager.; auto FPM = std::make_unique<legacy::FunctionPassManager>(M.get());. // Add some optimizations.; FPM->add(createInstructionCombiningPass());; FPM->add(createReassociatePass());; FPM->add(createGVNPass());; FPM->add(createCFGSimplificationPass());; FPM->doInitialization();. // Run the optimizations over all functions in the module being added to; // the JIT.; for (auto &F : *M); FPM->run(F);. return M;; }. At the bottom of our JIT we add a private method to do the actual optimization:; *optimizeModule*. This function takes the module to be transformed as input (as; a ThreadSafeModule) along with a reference to a reference to a new class:; ``MaterializationResponsibility``. The MaterializationResponsibility argument; can be used to query JIT state for the module being transformed, such as the set; of definitions in the module that JIT'd code is actively trying to call/access.; For now we will ignore this argument and use a standard optimization; pipeline. To do this we set up a FunctionPassManager, add some passes to it, run; it over every function in the module, and then return the mutated module. The; specific optimizations are the same ones used in `Chapter 4 <LangImpl04.html>`_; of the ""Implementing a language with LLVM"" tutorial series. Readers may visit; that chapter for a more in-depth discussion of these, and of IR optimization in; general. And that's it in terms of changes to KaleidoscopeJIT: When a module is added via; addModule the OptimizeLayer will call our optimizeModule function before passing; the transformed module on to the CompileLayer below. Of course, we could have; called optimizeModule directly in our addModule function and not gone to the; bother of using the IRTransformLayer, but doing so gives us another opportunity; to see how layers compose. It also provides a neat entry point to the *layer*; concept itself, because IRTransformLayer is one of the simplest layers that; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:5451,optimiz,optimization,5451,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,1,['optimiz'],['optimization']
Performance,"t No True − Use weight to count kNN events. UseLDA No False − Use local linear discriminant - experimental feature. Configuration options for MVA method :. Configuration options reference for MVA method: BDT. Option Array Default value Predefined values Description. V No False − Verbose output (short form of VerbosityLevel below - overrides the latter one). VerbosityLevel No Default Default, Debug, Verbose, Info, Warning, Error, Fatal Verbosity level. VarTransform No None − List of variable transformations performed before training, e.g., D_Background,P_Signal,G,N_AllClasses for: Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed). H No False − Print method-specific help message. CreateMVAPdfs No False − Create PDFs for classifier outputs (signal and background). IgnoreNegWeightsInTraining No False − Events with negative weights are ignored in the training (but are included for testing and performance evaluation). NTrees No 800 − Number of trees in the forest. MaxDepth No 3 − Max depth of the decision tree allowed. MinNodeSize No 5% − Minimum percentage of training events required in a leaf node (default: Classification: 5%, Regression: 0.2%). nCuts No 20 − Number of grid points in variable range used in finding optimal cut in node splitting. BoostType No AdaBoost AdaBoost, RealAdaBoost, Bagging, AdaBoostR2, Grad Boosting type for the trees in the forest . AdaBoostR2Loss No Quadratic Linear, Quadratic, Exponential Type of Loss function in AdaBoostR2. UseBaggedGrad No False − Use only a random subsample of all events for growing the trees in each iteration. (Only valid for GradBoost). Shrinkage No 1 − Learning rate for GradBoost algorithm. AdaBoostBeta No 0.5 − Learning rate for AdaBoost algorithm. UseRandomisedTrees No False − Determine at each node splitting the cut variable only as the best out of a random subset of ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/tmva/UsersGuide/optionRef.html:11844,perform,performance,11844,documentation/tmva/UsersGuide/optionRef.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/tmva/UsersGuide/optionRef.html,2,['perform'],['performance']
Performance,"t Opaque *__attribute__((NSObject)) objectPointer = ...;; ...; void (^foo)(void) = ^{ CFPrint(objectPointer); };. would have the following helper functions generated:. .. code-block:: c. void __block_copy_foo(struct __block_literal_5 *dst, struct __block_literal_5 *src) {; _Block_object_assign(&dst->objectPointer, src-> objectPointer, BLOCK_FIELD_IS_OBJECT);; }. void __block_dispose_foo(struct __block_literal_5 *src) {; _Block_object_dispose(src->objectPointer, BLOCK_FIELD_IS_OBJECT);; }. Imported ``__block`` marked variables; -------------------------------------. Layout of ``__block`` marked variables; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. The compiler must embed variables that are marked ``__block`` in a specialized; structure of the form:. .. code-block:: c. struct _block_byref_foo {; void *isa;; struct Block_byref *forwarding;; int flags; //refcount;; int size;; typeof(marked_variable) marked_variable;; };. Variables of certain types require helper functions for when ``Block_copy()``; and ``Block_release()`` are performed upon a referencing ``Block``. At the ""C""; level only variables that are of type ``Block`` or ones that have; ``__attribute__((NSObject))`` marked require helper functions. In Objective-C; objects require helper functions and in C++ stack based objects require helper; functions. Variables that require helper functions use the form:. .. code-block:: c. struct _block_byref_foo {; void *isa;; struct _block_byref_foo *forwarding;; int flags; //refcount;; int size;; // helper functions called via Block_copy() and Block_release(); void (*byref_keep)(void *dst, void *src);; void (*byref_dispose)(void *);; typeof(marked_variable) marked_variable;; };. The structure is initialized such that:. a. The ``forwarding`` pointer is set to the beginning of its enclosing; structure. b. The ``size`` field is initialized to the total size of the enclosing; structure. c. The ``flags`` field is set to either 0 if no helper functions are needed; or (1<<25) if they a",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/Block-ABI-Apple.rst:11879,perform,performed,11879,interpreter/llvm-project/clang/docs/Block-ABI-Apple.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/Block-ABI-Apple.rst,1,['perform'],['performed']
Performance,"t RFC`: https://lists.llvm.org/pipermail/llvm-dev/2021-September/153007.html. Binary Ids; ^^^^^^^^^^^^^^^^^^^^^^; The section is used to carry on `binary id`_ information from raw profiles. Temporal Profile Traces; ^^^^^^^^^^^^^^^^^^^^^^^^; The section is used to carry on temporal profile information from raw profiles.; See `temporal profiling`_ for the design. Profile Data Usage; =======================================. ``llvm-profdata`` is the command line tool to display and process instrumentation-; based profile data. For supported usages, check out `llvm-profdata documentation <https://llvm.org/docs/CommandGuide/llvm-profdata.html>`_. .. [1] For usage, see https://clang.llvm.org/docs/UsersManual.html#profiling-with-instrumentation; .. [2] For example, IR-based instrumentation supports `lightweight instrumentation`_; and `temporal profiling`_. Frontend instrumentation could support `single-byte counters`_.; .. [3] A raw profile file could contain the concatenation of multiple raw; profiles, for example, from an executable and its shared libraries. Raw; profile reader could parse all raw profiles from the file correctly.; .. [4] The counter section is used by a few variant types (like temporal; profiling) and might have different semantics there.; .. [5] The step size of data pointer is the ``sizeof(ProfileData)``, and the step; size of value profile pointer is calcuated based on the number of collected; values. .. _`lightweight instrumentation`: https://groups.google.com/g/llvm-dev/c/r03Z6JoN7d4; .. _`temporal profiling`: https://discourse.llvm.org/t/rfc-temporal-profiling-extension-for-irpgo/68068; .. _`single-byte counters`: https://discourse.llvm.org/t/rfc-single-byte-counters-for-source-based-code-coverage/75685; .. _`binary profile correlation`: https://discourse.llvm.org/t/rfc-add-binary-profile-correlation-to-not-load-profile-metadata-sections-into-memory-at-runtime/74565; .. _`binary id`: https://lists.llvm.org/pipermail/llvm-dev/2021-June/151154.html; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/InstrProfileFormat.rst:18302,load,load-profile-metadata-sections-into-memory-at-runtime,18302,interpreter/llvm-project/llvm/docs/InstrProfileFormat.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/InstrProfileFormat.rst,1,['load'],['load-profile-metadata-sections-into-memory-at-runtime']
Performance,"t ``-cl-kernel-arg-info`` enables more information about the original; kernel code to be added e.g. kernel parameter names will appear in the OpenCL; metadata along with other information. The IDs used to encode the OpenCL's logical address spaces in the argument info; metadata follows the SPIR address space mapping as defined in the SPIR; specification `section 2.2; <https://www.khronos.org/registry/spir/specs/spir_spec-2.0.pdf#18>`_. OpenCL Specific Options; -----------------------. In addition to the options described in :doc:`UsersManual` there are the; following options specific to the OpenCL frontend. All the options in this section are frontend-only and therefore if used; with regular clang driver they require frontend forwarding, e.g. ``-cc1``; or ``-Xclang``. .. _opencl_finclude_default_header:. .. option:: -finclude-default-header. Adds most of builtin types and function declarations during compilations. By; default the OpenCL headers are not loaded by the frontend and therefore certain; builtin types and most of builtin functions are not declared. To load them; automatically this flag can be passed to the frontend (see also :ref:`the; section on the OpenCL Header <opencl_header>`):. .. code-block:: console. $ clang -Xclang -finclude-default-header test.cl. Alternatively the internal header `opencl-c.h` containing the declarations; can be included manually using ``-include`` or ``-I`` followed by the path; to the header location. The header can be found in the clang source tree or; installation directory. .. code-block:: console. $ clang -I<path to clang sources>/lib/Headers/opencl-c.h test.cl; $ clang -I<path to clang installation>/lib/clang/<llvm version>/include/opencl-c.h/opencl-c.h test.cl. In this example it is assumed that the kernel code contains; ``#include <opencl-c.h>`` just as a regular C include. Because the header is very large and long to parse, PCH (:doc:`PCHInternals`); and modules (:doc:`Modules`) can be used internally to improve the comp",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/OpenCLSupport.rst:2908,load,loaded,2908,interpreter/llvm-project/clang/docs/OpenCLSupport.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/OpenCLSupport.rst,1,['load'],['loaded']
Performance,"t all; memory operations; to global and L2 writeback; have completed before; performing the; atomicrmw that is; being released. 3. buffer/global_atomic; 4. s_waitcnt vmcnt(0). - Must happen before; following buffer_invl2 and; buffer_wbinvl1_vol.; - Ensures the; atomicrmw has; completed before; invalidating the; caches. 5. buffer_invl2;; buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale L1 global data,; nor see stale L2 MTYPE; NC global data.; MTYPE RW and CC memory will; never be stale in L2 due to; the memory probes. atomicrmw acq_rel - agent - generic 1. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global have; completed before; performing the; atomicrmw that is; being released. 2. flat_atomic; 3. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the; atomicrmw has; completed before; invalidating the; cache. 4. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acq_rel - system - generic 1. buffer_wbl2. - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at system scope. 2. s_",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:271900,load,load,271900,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"t an otherwise loop invariant load unless it can prove the exiting; condition is not taken.) It can be profitable, in some cases, to emit such; instructions into the header even if they are not used along a rarely; executed path that exits the loop. This guidance specifically does not; apply if the condition which terminates the loop header is itself invariant,; or can be easily discharged by inspecting the loop index variables. #. In hot loops, consider duplicating instructions from small basic blocks; which end in highly predictable terminators into their successor blocks.; If a hot successor block contains instructions which can be vectorized; with the duplicated ones, this can provide a noticeable throughput; improvement. Note that this is not always profitable and does involve a; potentially large increase in code size. #. When checking a value against a constant, emit the check using a consistent; comparison type. The GVN pass *will* optimize redundant equalities even if; the type of comparison is inverted, but GVN only runs late in the pipeline.; As a result, you may miss the opportunity to run other important; optimizations. #. Avoid using arithmetic intrinsics unless you are *required* by your source; language specification to emit a particular code sequence. The optimizer; is quite good at reasoning about general control flow and arithmetic, it is; not anywhere near as strong at reasoning about the various intrinsics. If; profitable for code generation purposes, the optimizer will likely form the; intrinsics itself late in the optimization pipeline. It is *very* rarely; profitable to emit these directly in the language frontend. This item; explicitly includes the use of the :ref:`overflow intrinsics <int_overflow>`. #. Avoid using the :ref:`assume intrinsic <int_assume>` until you've; established that a) there's no other way to express the given fact and b); that fact is critical for optimization purposes. Assumes are a great; prototyping mechanism, but the",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Frontend/PerformanceTips.rst:8562,optimiz,optimize,8562,interpreter/llvm-project/llvm/docs/Frontend/PerformanceTips.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Frontend/PerformanceTips.rst,1,['optimiz'],['optimize']
Performance,"t aware of atomics can treat this like a nothrow call. For; SequentiallyConsistent loads and stores, the same reorderings are allowed as; for Acquire loads and Release stores, except that SequentiallyConsistent; operations may not be reordered. Notes for code generation; SequentiallyConsistent loads minimally require the same barriers as Acquire; operations and SequentiallyConsistent stores require Release; barriers. Additionally, the code generator must enforce ordering between; SequentiallyConsistent stores followed by SequentiallyConsistent loads. This; is usually done by emitting either a full fence before the loads or a full; fence after the stores; which is preferred varies by architecture. Atomics and IR optimization; ===========================. Predicates for optimizer writers to query:. * ``isSimple()``: A load or store which is not volatile or atomic. This is; what, for example, memcpyopt would check for operations it might transform. * ``isUnordered()``: A load or store which is not volatile and at most; Unordered. This would be checked, for example, by LICM before hoisting an; operation. * ``mayReadFromMemory()``/``mayWriteToMemory()``: Existing predicate, but note; that they return true for any operation which is volatile or at least; Monotonic. * ``isStrongerThan`` / ``isAtLeastOrStrongerThan``: These are predicates on; orderings. They can be useful for passes that are aware of atomics, for; example to do DSE across a single atomic access, but not across a; release-acquire pair (see MemoryDependencyAnalysis for an example of this). * Alias analysis: Note that AA will return ModRef for anything Acquire or; Release, and for the address accessed by any Monotonic operation. To support optimizing around atomic operations, make sure you are using the; right predicates; everything should work if that is done. If your pass should; optimize some atomic operations (Unordered operations in particular), make sure; it doesn't replace an atomic load or store with a ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst:16047,load,load,16047,interpreter/llvm-project/llvm/docs/Atomics.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst,1,['load'],['load']
Performance,"t by 1. Possible solutions; ^^^^^^^^^^^^^^^^^^; Let's briefly consider possible options about how and what we have to implement; in order to create full-featured functions merging, and also what it would; mean for us. Equal function detection obviously supposes that a ""detector"" method to be; implemented and latter should answer the question ""whether functions are equal"".; This ""detector"" method consists of tiny ""sub-detectors"", which each answers; exactly the same question, but for function parts. As the second step, we should merge equal functions. So it should be a ""merger""; method. ""Merger"" accepts two functions *F1* and *F2*, and produces *F1F2*; function, the result of merging. Having such routines in our hands, we can process a whole module, and merge all; equal functions. In this case, we have to compare every function with every another function. As; the reader may notice, this way seems to be quite expensive. Of course we could; introduce hashing and other helpers, but it is still just an optimization, and; thus the level of O(N*N) complexity. Can we reach another level? Could we introduce logarithmical search, or random; access lookup? The answer is: ""yes"". Random-access; """"""""""""""""""""""""""; How it could this be done? Just convert each function to a number, and gather; all of them in a special hash-table. Functions with equal hashes are equal.; Good hashing means, that every function part must be taken into account. That; means we have to convert every function part into some number, and then add it; into the hash. The lookup-up time would be small, but such an approach adds some; delay due to the hashing routine. Logarithmical search; """"""""""""""""""""""""""""""""""""""""; We could introduce total ordering among the functions set, once ordered we; could then implement a logarithmical search. Lookup time still depends on N,; but adds a little of delay (*log(N)*). Present state; """"""""""""""""""""""""""; Both of the approaches (random-access and logarithmical) have been implemented; and te",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:5462,optimiz,optimization,5462,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst,1,['optimiz'],['optimization']
Performance,"t care about block livein lists. - The (global) `liveins:` list is typically only interesting for early; instruction selection passes and can be removed when testing later passes.; The per-block `liveins:` on the other hand are necessary if; `tracksRegLiveness` is true. - Branch probability data in block `successors:` lists can be dropped if the; test doesn't depend on it. Example:; `successors: %bb.1(0x40000000), %bb.2(0x40000000)` can be replaced with; `successors: %bb.1, %bb.2`. - MIR code contains a whole IR module. This is necessary because there are; no equivalents in MIR for global variables, references to external functions,; function attributes, metadata, debug info. Instead some MIR data references; the IR constructs. You can often remove them if the test doesn't depend on; them. - Alias Analysis is performed on IR values. These are referenced by memory; operands in MIR. Example: `:: (load 8 from %ir.foobar, !alias.scope !9)`.; If the test doesn't depend on (good) alias analysis the references can be; dropped: `:: (load 8)`. - MIR blocks can reference IR blocks for debug printing, profile information; or debug locations. Example: `bb.42.myblock` in MIR references the IR block; `myblock`. It is usually possible to drop the `.myblock` reference and simply; use `bb.42`. - If there are no memory operands or blocks referencing the IR then the; IR function can be replaced by a parameterless dummy function like; `define @func() { ret void }`. - It is possible to drop the whole IR section of the MIR file if it only; contains dummy functions (see above). The .mir loader will create the; IR functions automatically in this case. .. _limitations:. Limitations; -----------. Currently the MIR format has several limitations in terms of which state it; can serialize:. - The target-specific state in the target-specific ``MachineFunctionInfo``; subclasses isn't serialized at the moment. - The target-specific ``MachineConstantPoolValue`` subclasses (in the ARM and; SystemZ b",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:4641,load,load,4641,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst,1,['load'],['load']
Performance,"t command line option can be passed directly to tools such; as opt, llc and lli. The syntax is as follows:. ::. <tool name> [other options] -opt-bisect-limit=<limit>. If a value of -1 is used the tool will perform all optimizations but a message; will be printed to stderr for each optimization that could be skipped; indicating the index value that is associated with that optimization. To skip; optimizations, pass the value of the last optimization to be performed as the; opt-bisect-limit. All optimizations with a higher index value will be skipped. In order to use the -opt-bisect-limit option with a driver that provides a; wrapper around the LLVM core library, an additional prefix option may be; required, as defined by the driver. For example, to use this option with; clang, the ""-mllvm"" prefix must be used. A typical clang invocation would look; like this:. ::. clang -O2 -mllvm -opt-bisect-limit=256 my_file.c. The -opt-bisect-limit option may also be applied to link-time optimizations by; using a prefix to indicate that this is a plug-in option for the linker. The; following syntax will set a bisect limit for LTO transformations:. ::. # When using lld, or ld64 (macOS); clang -flto -Wl,-mllvm,-opt-bisect-limit=256 my_file.o my_other_file.o; # When using Gold; clang -flto -Wl,-plugin-opt,-opt-bisect-limit=256 my_file.o my_other_file.o. LTO passes are run by a library instance invoked by the linker. Therefore any; passes run in the primary driver compilation phase are not affected by options; passed via '-Wl,-plugin-opt' and LTO passes are not affected by options; passed to the driver-invoked LLVM invocation via '-mllvm'. Passing ``-opt-bisect-print-ir-path=path/foo.ll`` will dump the IR to; ``path/foo.ll`` when -opt-bisect-limit starts skipping passes. Bisection Index Values; ======================. The granularity of the optimizations associated with a single index value is; variable. Depending on how the optimization pass has been instrumented the; value may be asso",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:2668,optimiz,optimizations,2668,interpreter/llvm-project/llvm/docs/OptBisect.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst,1,['optimiz'],['optimizations']
Performance,"t fMass2; //The mass square of this particle; Float_t fBx; //X intercept at the vertex; Float_t fBy; //Y intercept at the vertex; Float_t fMeanCharge; //Mean charge deposition of all hits; Float_t fXfirst; //X coordinate of the first point; Float_t fXlast; //X coordinate of the last point; Float_t fYfirst; //Y coordinate of the first point; Float_t fYlast; //Y coordinate of the last point; Float_t fZfirst; //Z coordinate of the first point; Float_t fZlast; //Z coordinate of the last point; Float_t fCharge; //Charge of this track; Float_t fVertex[3]; //Track vertex position; Int_t fNpoint; //Number of points for this track; Short_t fValid; //Validity criterion. // method definitions ...; ClassDef(Track,1) //A track segment; };; ```. ### Writing the Tree. We create a simple tree with two branches both holding `Event` objects.; One is split and the other is not. We also create a pointer to an; `Event` object (`event`). ``` {.cpp}; void tree4w() {; // check to see if the event class is in the dictionary; // if it is not load the definition in libEvent.so; if (!TClassTable::GetDict(""Event"")) {; gSystem->Load(""$ROOTSYS/test/libEvent.so"");; }; // create a Tree file tree4.root; TFile f(""tree4.root"",""RECREATE"");; // create a ROOT Tree; TTree t4(""t4"",""A Tree with Events"");; // create a pointer to an Event object; Event *event = new Event();; // create two branches, split one; t4.Branch(""event_branch"", ""Event"", &event,16000,2);; t4.Branch(""event_not_split"", ""Event"", &event,16000,0);. // a local variable for the event type; char etype[20];. // fill the tree; for (Int_t ev = 0; ev <100; ev++) {; Float_t sigmat, sigmas;; gRandom->Rannor(sigmat,sigmas);; Int_t ntrack = Int_t(600 + 600 *sigmat/120.);; Float_t random = gRandom->Rndm(1);; sprintf(etype,""type%d"",ev%5);; event->SetType(etype);; event->SetHeader(ev, 200, 960312, random);; event->SetNseg(Int_t(10*ntrack+20*sigmas));; event->SetNvertex(Int_t(1+20*gRandom->Rndm()));; event->SetFlag(UInt_t(random+0.5));; event->SetTemperatu",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Trees.md:62339,load,load,62339,documentation/users-guide/Trees.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Trees.md,1,['load'],['load']
Performance,"t from hadd a better way to invoke Merge for; generic objects; add option to merge histograms in one go, instead of; one-by-one as for generic objects (this option is not yet supported by; hadd). TProofOutputFile. Add support for the placeholder <file>; the definition of the outputfile. This allows to have complete URL and; to pass options to TFile::Open. XrdProofd plugin. Add automatically the line 'Path.ForceRemote 1' to the; session rootrc file if the ROOT version is < 5.24/00 ; this acts; as a workaround for the wrong TTreeCache initialization at the; transition between local and remote files fixed in 5.24/00 . Enable mass storage domain settings when working with; TChain's; in multi-master mode. The Mass Storage Domain must be specified as; option in the URL.              ; chain.AddFile(""root:// .....?msd=CERN"").  and the string must match the value specified in defining the; submaster node.; Improved performance monitoring: the 'Rate plot' button; in the dialog box has been renamed 'Performance Plot' and now shows up; to 4 plots as a function of the processing time:. Instantaneous processing rate, which is now better; estimated by a better estimation of the normalizing times; Average read chunck size, defined as; TFile::GetFileBytesRead() / TFile::GetFileReadCalls() during the last; unit of time; this allows to monitor the usage of the cache; this plot; is present only if some I/O is done, i.e. not for pure CPU tasks.; The number of active workers; The number of total and effecive sessions running; concurrently on the cluster (started by the same daemon); this plot is; present only is the number is at least onec different from 1. If enabled, send monitoring information from the master; at each GetNextPacket (at each call of TPerfStat::PacketEvent) to allow; extrnal real-time progress monitoring.; Save the status of a 'proofserv' session into a new file; in the 'activesessions' area. The full path of the new file is;          ; <admin_path>/.xproofd.<port>/act",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/proof/doc/v526/index.html:4614,perform,performance,4614,proof/doc/v526/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/proof/doc/v526/index.html,3,"['Perform', 'perform']","['Performance', 'performance']"
Performance,"t from significantly faster times for fitting by calling `fitTo()` and providing a `BatchMode(""cpu"")` or a `BatchMode(""cuda"")` option.; ``` {.cpp}; // fit using the most efficient library that the computer's CPU can support; RooMyPDF.fitTo(data, BatchMode(""cpu""));. // fit using the CUDA library along with the most efficient library that the computer's CPU can support; RooMyPDF.fitTo(data, BatchMode(""cuda""));; ```; **Note: In case the system does not support vector instructions, the `RooBatchCompute::Cpu` option is guaranteed to work properly by using a generic CPU library. In contrast, users must first make sure that their system supports CUDA in order to use the `RooBatchCompute::Cuda` option. If this is not the case, an exception will be thrown.**. If `""cuda""` is selected, RooFit will launch CUDA kernels for computing likelihoods and potentially other intense computations. At the same time, the most efficient CPU library loaded will also handle parts of the computations in parallel with the GPU (or potentially, if it's faster, all of them), thus gaining full advantage of the available hardware. For this purpose `RooFitDriver`, a newly created RooFit class (in roofitcore) takes over the task of analyzing the computations and assigning each to the correct piece of hardware, taking into consideration the performance boost or penalty that may arise with every method of computing. #### Multithread computations; The CPU instance of the computing library can furthermore execute multithread computations. This also applies for computations handled by the CPU in the `""cuda""` mode. To use them, one needs to set the desired number of parallel tasks before calling `fitTo()` as shown below:; ``` {.cpp}; ROOT::EnableImplicitMT(nThreads);; RooMyPDF.fitTo(data, BatchMode(""cuda"")); // can also use ""cuda""; ```. ### User-made PDFs; The easiest and most efficient way of accelerating your PDFs is to request their addition to the official RooFit by submitting a ticket [here](https://git",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/developers/batchcompute.md:2670,load,loaded,2670,roofit/doc/developers/batchcompute.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/developers/batchcompute.md,1,['load'],['loaded']
Performance,"t guarantees that a total; ordering exists between all SequentiallyConsistent operations. Relevant standard; This corresponds to the C++/C ``memory_order_seq_cst``, Java volatile, and; the gcc-compatible ``__sync_*`` builtins which do not specify otherwise. Notes for frontends; If a frontend is exposing atomic operations, these are much easier to reason; about for the programmer than other kinds of operations, and using them is; generally a practical performance tradeoff. Notes for optimizers; Optimizers not aware of atomics can treat this like a nothrow call. For; SequentiallyConsistent loads and stores, the same reorderings are allowed as; for Acquire loads and Release stores, except that SequentiallyConsistent; operations may not be reordered. Notes for code generation; SequentiallyConsistent loads minimally require the same barriers as Acquire; operations and SequentiallyConsistent stores require Release; barriers. Additionally, the code generator must enforce ordering between; SequentiallyConsistent stores followed by SequentiallyConsistent loads. This; is usually done by emitting either a full fence before the loads or a full; fence after the stores; which is preferred varies by architecture. Atomics and IR optimization; ===========================. Predicates for optimizer writers to query:. * ``isSimple()``: A load or store which is not volatile or atomic. This is; what, for example, memcpyopt would check for operations it might transform. * ``isUnordered()``: A load or store which is not volatile and at most; Unordered. This would be checked, for example, by LICM before hoisting an; operation. * ``mayReadFromMemory()``/``mayWriteToMemory()``: Existing predicate, but note; that they return true for any operation which is volatile or at least; Monotonic. * ``isStrongerThan`` / ``isAtLeastOrStrongerThan``: These are predicates on; orderings. They can be useful for passes that are aware of atomics, for; example to do DSE across a single atomic access, but not a",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst:15614,load,loads,15614,interpreter/llvm-project/llvm/docs/Atomics.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst,1,['load'],['loads']
Performance,"t happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; store.; - Ensures that all; memory operations; to memory have; completed before; performing the; store that is being; released. 2. buffer/global/flat_store; store atomic release - system - global 1. buffer_wbl2; - generic; - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at system scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after any; preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after any; preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; store.; - Ensures that all; memory operations; to memory and the L2; writeback have; completed before; performing the; store that is being; released. 3. buffer/global/flat_store; atomicrmw release - singlethread - global 1. buffer/global/flat_atomic; - wavefront - generic; atomicrmw release - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; be used.*. 1. ds_atomic; atomicrmw release - workgroup - global 1. s_waitcnt lgkm/vmcnt(0); - generic; - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL, omit; lgkmcnt(0).; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load/store/; load atomic/store atomic/; atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; a",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:259263,load,load,259263,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"t have an address available. When; the function returns (either with the ``ret`` or ``resume`` instructions),; the memory is reclaimed. Allocating zero bytes is legal, but the returned; pointer may not be unique. The order in which memory is allocated (ie.,; which way the stack grows) is not specified. Note that '``alloca``' outside of the alloca address space from the; :ref:`datalayout string<langref_datalayout>` is meaningful only if the; target has assigned it a semantics. If the returned pointer is used by :ref:`llvm.lifetime.start <int_lifestart>`,; the returned object is initially dead.; See :ref:`llvm.lifetime.start <int_lifestart>` and; :ref:`llvm.lifetime.end <int_lifeend>` for the precise semantics of; lifetime-manipulating intrinsics. Example:; """""""""""""""". .. code-block:: llvm. %ptr = alloca i32 ; yields ptr; %ptr = alloca i32, i32 4 ; yields ptr; %ptr = alloca i32, i32 4, align 1024 ; yields ptr; %ptr = alloca i32, align 1024 ; yields ptr. .. _i_load:. '``load``' Instruction; ^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. <result> = load [volatile] <ty>, ptr <pointer>[, align <alignment>][, !nontemporal !<nontemp_node>][, !invariant.load !<empty_node>][, !invariant.group !<empty_node>][, !nonnull !<empty_node>][, !dereferenceable !<deref_bytes_node>][, !dereferenceable_or_null !<deref_bytes_node>][, !align !<align_node>][, !noundef !<empty_node>]; <result> = load atomic [volatile] <ty>, ptr <pointer> [syncscope(""<target-scope>"")] <ordering>, align <alignment> [, !invariant.group !<empty_node>]; !<nontemp_node> = !{ i32 1 }; !<empty_node> = !{}; !<deref_bytes_node> = !{ i64 <dereferenceable_bytes> }; !<align_node> = !{ i64 <value_alignment> }. Overview:; """""""""""""""""". The '``load``' instruction is used to read from memory. Arguments:; """""""""""""""""""". The argument to the ``load`` instruction specifies the memory address from which; to load. The type specified must be a :ref:`first class <t_firstclass>` type of; known size (i.e. not containing an :ref:`opaque structu",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:411826,load,load,411826,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,"t helps streamline `RDataFrame` workflows in a distributed environment. Currently only a subset of `RDataFrame` actions have their corresponding mergeable class, but in the future it will be possible to extend it to any action through the creation of a new `RMergeableValue` derived class. ### Behavior changes. - `Snapshot` now respects the basket size and split level of the original branch when copying branches to a new `TTree`.; - `Snapshot` now writes branches coming from friend `TTree`s even if they have the same name as branches in the main tree (`friendname_` is prepended to the name of the output branches). More details at [#7181](https://github.com/root-project/root/issues/7181).; - Just-in-time compilation of string expressions passed to `Filter` and `Define` now generates functions that take fundamental types by const value (rather than by non-const reference as before). This will break code that was assigning to column values in string expressions: this is an intended side effect as we want to prevent non-expert users from performing assignments (`=`) rather than comparisons (`==`). Expert users can resort to compiled callables if they absolutely have to assign to column values (not recommended). See [ROOT-11009](https://sft.its.cern.ch/jira/browse/ROOT-11009) for further discussion.; - For some `TTrees`, `RDataFrame::GetColumnNames` might now returns multiple valid spellings for a given column. For example, leaf `""l""` under branch `""b""` might now be mentioned as `""l""` as well as `""b.l""`, while only one of the two spellings might have been recognized before.; - Certain RDF-related types in the `ROOT::Detail` and `ROOT::Internal` namespaces have been renamed, most notably `RCustomColumn` is now `RDefine`. This does not impact code that only makes use of entities in the public ROOT namespace, and should not impact downstream code unless it was patching or reusing internal `RDataFrame` types. ### Notable bug fixes and improvements. - A critical issue has been ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v624/index.md:8448,perform,performing,8448,README/ReleaseNotes/v624/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v624/index.md,1,['perform'],['performing']
Performance,"t i32 %added, %bar.0, !dbg !5; br i1 %cond, label %bb1, label %bb2, !dbg !5. bb2: ; preds = %bb1; ret i32 0, !dbg !5; }. If one compiles this IR with ``llc -o - -start-after=codegen-prepare -stop-after=expand-isel-pseudos -mtriple=x86_64--``, the following MIR is produced:. .. code-block:: text. bb.0.entry:; successors: %bb.1(0x80000000); liveins: $rdi. %2:gr64 = COPY $rdi; %3:gr32 = MOV32r0 implicit-def dead $eflags; DBG_VALUE 0, $noreg, !3, !DIExpression(), debug-location !5. bb.1.bb1:; successors: %bb.1(0x7c000000), %bb.2(0x04000000). %0:gr32 = PHI %3, %bb.0, %1, %bb.1; DBG_VALUE %0, $noreg, !3, !DIExpression(), debug-location !5; DBG_VALUE %2, $noreg, !3, !DIExpression(DW_OP_plus_uconst, 4, DW_OP_stack_value), debug-location !5; %4:gr32 = MOV32rm %2, 1, $noreg, 4, $noreg, debug-location !5 :: (load 4 from %ir.addr1); %5:gr64_nosp = MOVSX64rr32 %0, debug-location !5; DBG_VALUE $noreg, $noreg, !3, !DIExpression(), debug-location !5; %1:gr32 = INC32r %0, implicit-def dead $eflags, debug-location !5; DBG_VALUE %1, $noreg, !3, !DIExpression(), debug-location !5; %6:gr32 = ADD32rm %4, %2, 4, killed %5, 0, $noreg, implicit-def dead $eflags :: (load 4 from %ir.addr2); %7:gr32 = SUB32rr %6, %0, implicit-def $eflags, debug-location !5; JB_1 %bb.1, implicit $eflags, debug-location !5; JMP_1 %bb.2, debug-location !5. bb.2.bb2:; %8:gr32 = MOV32r0 implicit-def dead $eflags; $eax = COPY %8, debug-location !5; RET 0, $eax, debug-location !5. Observe first that there is a DBG_VALUE instruction for every ``llvm.dbg.value``; intrinsic in the source IR, ensuring no source level assignments go missing.; Then consider the different ways in which variable locations have been recorded:. * For the first dbg.value an immediate operand is used to record a zero value.; * The dbg.value of the PHI instruction leads to a DBG_VALUE of virtual register; ``%0``.; * The first GEP has its effect folded into the first load instruction; (as a 4-byte offset), but the variable location is salvaged by ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst:31463,load,load,31463,interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,1,['load'],['load']
Performance,"t identify itself as Windows, and thus gets path::Style::posix; # as native path style, regardless of what this is set to.; set(WINDOWS_PREFER_FORWARD_SLASH_DEFAULT ON); endif(); option(LLVM_WINDOWS_PREFER_FORWARD_SLASH ""Prefer path names with forward slashes on Windows."" ${WINDOWS_PREFER_FORWARD_SLASH_DEFAULT}). option(LLVM_ENABLE_FFI ""Use libffi to call external functions from the interpreter"" OFF); set(FFI_LIBRARY_DIR """" CACHE PATH ""Additional directory, where CMake should search for libffi.so""); set(FFI_INCLUDE_DIR """" CACHE PATH ""Additional directory, where CMake should search for ffi.h or ffi/ffi.h""). set(LLVM_TARGET_ARCH ""host""; CACHE STRING ""Set target to use for LLVM JIT or use \""host\"" for automatic detection.""). option(LLVM_ENABLE_TERMINFO ""Use terminfo database if available."" ON). set(LLVM_ENABLE_LIBXML2 ""ON"" CACHE STRING ""Use libxml2 if available. Can be ON, OFF, or FORCE_ON""). option(LLVM_ENABLE_LIBEDIT ""Use libedit if available."" ON). option(LLVM_ENABLE_LIBPFM ""Use libpfm for performance counters if available."" ON). # On z/OS, threads cannot be used because TLS is not supported.; if (CMAKE_SYSTEM_NAME MATCHES ""OS390""); option(LLVM_ENABLE_THREADS ""Use threads if available."" OFF); else(); option(LLVM_ENABLE_THREADS ""Use threads if available."" ON); endif(). set(LLVM_ENABLE_ZLIB ""ON"" CACHE STRING ""Use zlib for compression/decompression if available. Can be ON, OFF, or FORCE_ON""). set(LLVM_ENABLE_ZSTD ""ON"" CACHE STRING ""Use zstd for compression/decompression if available. Can be ON, OFF, or FORCE_ON""). set(LLVM_USE_STATIC_ZSTD FALSE CACHE BOOL ""Use static version of zstd. Can be TRUE, FALSE""). set(LLVM_ENABLE_CURL ""OFF"" CACHE STRING ""Use libcurl for the HTTP client if available. Can be ON, OFF, or FORCE_ON""). set(LLVM_ENABLE_HTTPLIB ""OFF"" CACHE STRING ""Use cpp-httplib HTTP server library if available. Can be ON, OFF, or FORCE_ON""). set(LLVM_Z3_INSTALL_DIR """" CACHE STRING ""Install directory of the Z3 solver.""). option(LLVM_ENABLE_Z3_SOLVER; ""Enable Support fo",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/CMakeLists.txt:22176,perform,performance,22176,interpreter/llvm-project/llvm/CMakeLists.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/CMakeLists.txt,1,['perform'],['performance']
Performance,"t in memory when reading or writing.; When reading this write basket was always present in memory even if the branch was never accessed. Starting in v5.20/00, TTree::Write closes out, compresses (when requested) and writes to disk in their own file record the write baskets of all the branches.; (This is implemented via the new function TTree::FlushBaskets, TBranch::FlushBaskets, TBranch::FlushOneBaskets). TTree::AutoSave supports a new option ""FlushBaskets"" which will call FlushBaskets before saving the TTree object. Benefits. Flushing the write baskets has several advantages:. Reduce the file size of the TTree object (it not longer contains the last basket), improving read time of the TTree object; Reduce memory footprint of the TTree object.; In a TTree which ""flushed"" buffer, there is now usually only zero or one buffer in memory.; Previously each branch always had at least one basket in memory and usually 2 (the write basket and one read basket).; Now only the basket of the branches actually read are loaded in memory. allow for the basket to be compressed and stored separated, increasing the compression factor. Note: Calling FlushBaskets too often (either directly of via AutoSave(""FlushBaskets"")) can lead to unnecessary fragmentation of the ROOT file,; since it write the baskets to disk (and a new basket will be started at the next fill) whether or not the content was close to filling the basket or not. Others. The fast tree cloning (TTreeCloner) was enhanced to support copying in-memory TTrees (that have been save as a single key on file). This issue was preventing hadd to fast clone files containing any 'in-memory' tree. Re-enabled the splitting of TVector3 and of any classes starting by TVector; that is not a TVectorT.; Fix the list of StreamerInfo stored in the TFile in the case of a slow; CloneTree, previously some of the classes whose named contained '::' and any; of the STL container names was inadvertently omitted (in case of classes; that are part of the",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/doc/v520/index.html:6991,load,loaded,6991,tree/doc/v520/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/doc/v520/index.html,2,['load'],['loaded']
Performance,"t is that it evaluates ""``BB->end()``"" every time; through the loop. Instead of writing the loop like this, we strongly prefer; loops to be written so that they evaluate it once before the loop starts. A; convenient way to do this is like so:. .. code-block:: c++. BasicBlock *BB = ...; for (auto I = BB->begin(), E = BB->end(); I != E; ++I); ... use I ... The observant may quickly point out that these two loops may have different; semantics: if the container (a basic block in this case) is being mutated, then; ""``BB->end()``"" may change its value every time through the loop and the second; loop may not in fact be correct. If you actually do depend on this behavior,; please write the loop in the first form and add a comment indicating that you; did it intentionally. Why do we prefer the second form (when correct)? Writing the loop in the first; form has two problems. First it may be less efficient than evaluating it at the; start of the loop. In this case, the cost is probably minor --- a few extra; loads every time through the loop. However, if the base expression is more; complex, then the cost can rise quickly. I've seen loops where the end; expression was actually something like: ""``SomeMap[X]->end()``"" and map lookups; really aren't cheap. By writing it in the second form consistently, you; eliminate the issue entirely and don't even have to think about it. The second (even bigger) issue is that writing the loop in the first form hints; to the reader that the loop is mutating the container (a fact that a comment; would handily confirm!). If you write the loop in the second form, it is; immediately obvious without even looking at the body of the loop that the; container isn't being modified, which makes it easier to read the code and; understand what it does. While the second form of the loop is a few extra keystrokes, we do strongly; prefer it. ``#include <iostream>`` is Forbidden; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. The use of ``#include <iostream>`` in library",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodingStandards.rst:54092,load,loads,54092,interpreter/llvm-project/llvm/docs/CodingStandards.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodingStandards.rst,1,['load'],['loads']
Performance,"t is; currently assumed that the operation is of pointer size and the alignment is; assumed to be the target machine's default alignment. Write barrier: ``llvm.gcwrite``; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. .. code-block:: llvm. void @llvm.gcwrite(i8* %value, i8* %object, i8** %derived). For write barriers, LLVM provides the ``llvm.gcwrite`` intrinsic function. It; has exactly the same semantics as a non-volatile ``store`` to the derived; pointer (the third argument). The exact code generated is specified by the; Function's selected :ref:`GC strategy <plugin>`. Many important algorithms require write barriers, including generational and; concurrent collectors. Additionally, write barriers could be used to implement; reference counting. Read barrier: ``llvm.gcread``; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. .. code-block:: llvm. i8* @llvm.gcread(i8* %object, i8** %derived). For read barriers, LLVM provides the ``llvm.gcread`` intrinsic function. It has; exactly the same semantics as a non-volatile ``load`` from the derived pointer; (the second argument). The exact code generated is specified by the Function's; selected :ref:`GC strategy <plugin>`. Read barriers are needed by fewer algorithms than write barriers, and may have a; greater performance impact since pointer reads are more frequent than writes. .. _plugin:. .. _builtin-gc-strategies:. Built In GC Strategies; ======================. LLVM includes built in support for several varieties of garbage collectors. The Shadow Stack GC; ----------------------. To use this collector strategy, mark your functions with:. .. code-block:: c++. F.setGC(""shadow-stack"");. Unlike many GC algorithms which rely on a cooperative code generator to compile; stack maps, this algorithm carefully maintains a linked list of stack roots; [:ref:`Henderson2002 <henderson02>`]. This so-called ""shadow stack"" mirrors the; machine stack. Maintaining this data structure is slower than using a stack map; compiled into the executable as constant data, but ha",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GarbageCollection.rst:15496,load,load,15496,interpreter/llvm-project/llvm/docs/GarbageCollection.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GarbageCollection.rst,1,['load'],['load']
Performance,"t it, follow the example at the end of the :doc:`GettingStarted` to; compile ""Hello World"" to LLVM. We can now run the bitcode file (hello.bc) for; the program through our transformation like this (or course, any bitcode file; will work):. .. code-block:: console. $ opt -load lib/LLVMHello.so -hello < hello.bc > /dev/null; Hello: __main; Hello: puts; Hello: main. The :option:`-load` option specifies that :program:`opt` should load your pass; as a shared object, which makes ""``-hello``"" a valid command line argument; (which is one reason you need to :ref:`register your pass; <writing-an-llvm-pass-registration>`). Because the Hello pass does not modify; the program in any interesting way, we just throw away the result of; :program:`opt` (sending it to ``/dev/null``). To see what happened to the other string you registered, try running; :program:`opt` with the :option:`-help` option:. .. code-block:: console. $ opt -load lib/LLVMHello.so -help; OVERVIEW: llvm .bc -> .bc modular optimizer and analysis printer. USAGE: opt [subcommand] [options] <input bitcode file>. OPTIONS:; Optimizations available:; ...; -guard-widening - Widen guards; -gvn - Global Value Numbering; -gvn-hoist - Early GVN Hoisting of Expressions; -hello - Hello World Pass; -indvars - Induction Variable Simplification; -inferattrs - Infer set function attributes; ... The pass name gets added as the information string for your pass, giving some; documentation to users of :program:`opt`. Now that you have a working pass,; you would go ahead and make it do the cool transformations you want. Once you; get it all working and tested, it may become useful to find out how fast your; pass is. The :ref:`PassManager <writing-an-llvm-pass-passmanager>` provides a; nice command line option (:option:`-time-passes`) that allows you to get; information about the execution time of your pass along with the other passes; you queue up. For example:. .. code-block:: console. $ opt -load lib/LLVMHello.so -hello -time-passes ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst:8967,optimiz,optimizer,8967,interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst,1,['optimiz'],['optimizer']
Performance,"t it; will be in-bounds. In that case, the body of the `if` will be executed; speculatively, and may read secret data into `value` and leak it via a; cache-timing side channel when a dependent access is made to populate `value2`. ## High Level Mitigation Approach. While several approaches are being actively pursued to mitigate specific; branches and/or loads inside especially risky software (most notably various OS; kernels), these approaches require manual and/or static analysis aided auditing; of code and explicit source changes to apply the mitigation. They are unlikely; to scale well to large applications. We are proposing a comprehensive; mitigation approach that would apply automatically across an entire program; rather than through manual changes to the code. While this is likely to have a; high performance cost, some applications may be in a good position to take this; performance / security tradeoff. The specific technique we propose is to cause loads to be checked using; branchless code to ensure that they are executing along a valid control flow; path. Consider the following C-pseudo-code representing the core idea of a; predicate guarding potentially invalid loads:; ```; void leak(int data);; void example(int* pointer1, int* pointer2) {; if (condition) {; // ... lots of code ...; leak(*pointer1);; } else {; // ... more code ...; leak(*pointer2);; }; }; ```. This would get transformed into something resembling the following:; ```; uintptr_t all_ones_mask = std::numerical_limits<uintptr_t>::max();; uintptr_t all_zeros_mask = 0;; void leak(int data);; void example(int* pointer1, int* pointer2) {; uintptr_t predicate_state = all_ones_mask;; if (condition) {; // Assuming ?: is implemented using branchless logic...; predicate_state = !condition ? all_zeros_mask : predicate_state;; // ... lots of code ...; //; // Harden the pointer so it can't be loaded; pointer1 &= predicate_state;; leak(*pointer1);; } else {; predicate_state = condition ? all_zeros_mask : pred",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md:3064,load,loads,3064,interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,1,['load'],['loads']
Performance,"t node or path has not; been changed by the user. \anchor GP02ga; #### Finding If Current State Is Changed For a New Point. One can find fast if a point different from the current one has or not; the same location inside the geometry tree. To do that, the new point; should not be introduced by using TGeoManager::SetCurrentPoint(); method, but rather by calling the specific method:. ~~~{.cpp}; Bool_t TGeoManager::IsSameLocation(Double_t x,Double_t y,; Double_t z,Bool_t change=kFALSE);; ~~~. In the prototype above, `x, y` and `z` are the coordinates of the new; point. The modeller will check whether the current volume still contains; the new point or its location has changed in the geometry hierarchy. If; the new location is different, two actions are possible according to the; value of `change`:. - `change = kFALSE` (default) - the modeller does not change the; current state but just inform the caller about this change.; - `change = kTRUE` - the modeller will actually perform a new; `‘Where am I?' `search after finding out that the location has; changed. The current state will be actualized accordingly. Note that even when performing a normal search on the current state; after changing the current point coordinates (e.g.; `gGeoManager->FindNode(newX,newY,newZ)`), users can always query if the; previous state has changed by using a method having the same name but; without parameters:. ~~~{.cpp}; Bool_t TGeoManager::IsSameLocation();; ~~~. \anchor GP02gb; #### Finding the Distance to the Next Boundary. All tracking engines need to compare the currently proposed physical; step with the maximum allowed distance in the current material. The; modeller provides this information by computing the distance to the; first boundary starting from the current point along a straight line.; The starting point and direction for this procedure are the ones; corresponding to the current state. The boundary search is initialized; inside the current volume and the crossed boundary can belo",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md:75418,perform,perform,75418,geom/geom/doc/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md,1,['perform'],['perform']
Performance,"t of an erroneous operation.; In order to facilitate speculative execution, many instructions do not; invoke immediate undefined behavior when provided with illegal operands,; and return a poison value instead.; The string '``poison``' can be used anywhere a constant is expected, and; operations such as :ref:`add <i_add>` with the ``nsw`` flag can produce; a poison value. Most instructions return '``poison``' when one of their arguments is; '``poison``'. A notable exception is the :ref:`select instruction <i_select>`.; Propagation of poison can be stopped with the; :ref:`freeze instruction <i_freeze>`. It is correct to replace a poison value with an; :ref:`undef value <undefvalues>` or any value of the type. This means that immediate undefined behavior occurs if a poison value is; used as an instruction operand that has any values that trigger undefined; behavior. Notably this includes (but is not limited to):. - The pointer operand of a :ref:`load <i_load>`, :ref:`store <i_store>` or; any other pointer dereferencing instruction (independent of address; space).; - The divisor operand of a ``udiv``, ``sdiv``, ``urem`` or ``srem``; instruction.; - The condition operand of a :ref:`br <i_br>` instruction.; - The callee operand of a :ref:`call <i_call>` or :ref:`invoke <i_invoke>`; instruction.; - The parameter operand of a :ref:`call <i_call>` or :ref:`invoke <i_invoke>`; instruction, when the function or invoking call site has a ``noundef``; attribute in the corresponding position.; - The operand of a :ref:`ret <i_ret>` instruction if the function or invoking; call site has a `noundef` attribute in the return value position. Here are some examples:. .. code-block:: llvm. entry:; %poison = sub nuw i32 0, 1 ; Results in a poison value.; %poison2 = sub i32 poison, 1 ; Also results in a poison value.; %still_poison = and i32 %poison, 0 ; 0, but also poison.; %poison_yet_again = getelementptr i32, ptr @h, i32 %still_poison; store i32 0, ptr %poison_yet_again ; Undefined beha",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:197721,load,load,197721,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,"t of known class methods. It can be extended by users.; 12. Use of cached methods improves binary I/O performance by 20%; 13. Support TGaxis; 14. Project now can be obtained via 'bower install jsroot'; 15. Support 'scat' and 'text' draw options for TH2; 16. Support in binary I/O zipped buffer bigger than 16M; 17. Correctly handle in binary I/O pointer on TArray object (like in THnSparseArrayChunk). ## Changes in 4.3; 1. Implement TGeoCtub, TGeoParaboloid and TGeoHype shapes; 2. Support TGeoTube with Rmin==0; 3. Exclude empty faces in TGeoArb8; 4. Improve TGeoSphere creation - handle all parameters combinations; 5. Introduce JSROOT.cleanup() function to safely clear all drawn objects; 6. Fix wrong resize method in 'tabs' and 'collapsible' layouts; 7. Fix canvas resize problem (issue #27); 8. Fix zero-height canvas when draw TGeo in collapsible layout; 9. Fix problem of simultaneous move TGeo drawings and canvas in flexible layout. ## Changes in 4.2; 1. Significant performance improvements in 3D drawings - TGeo/TH2/TH3; 2. Implement TGeoPara, TGeoGtra, TGeoXtru and TGeoEltu shapes; 3. Optimize (reduce vertices number) for others TGeo shapes; 4. Correct rotation/translation/scaling of TGeo nodes; 5. Workaround for axis reflection (not directly supported in three.js); 6. Support array of objects in I/O (like in TAxis3D); 7. Correct reading of multi-dim arrays like Double_t fXY[8][2];; 8. Provide canvas toolbar for actions like savepng or unzoom; 9. Implement JSROOT.resize() function to let resize drawing after changes in page layout; 10. Fix error with title display/update. ## Changes in 4.1; 1. Introduce object inspector - one could browse object members of any class; 2. Let draw sub-items from TCanvas list of primitives like sub-pad or TLatex; 3. Provide possibility to save drawn SVG canvas as PNG; 4. TGraph drawing optimization - limit number of drawn points; 5. Implement painter for TPolyMarker3D; 6. Improve drawing and update of TMultiGraph; 7. Reorganize 3D drawing",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/js/changes.md:58236,perform,performance,58236,js/changes.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/js/changes.md,1,['perform'],['performance']
Performance,"t one has or not; the same location inside the geometry tree. To do that, the new point; should not be introduced by using TGeoManager::SetCurrentPoint(); method, but rather by calling the specific method:. ~~~{.cpp}; Bool_t TGeoManager::IsSameLocation(Double_t x,Double_t y,; Double_t z,Bool_t change=kFALSE);; ~~~. In the prototype above, `x, y` and `z` are the coordinates of the new; point. The modeller will check whether the current volume still contains; the new point or its location has changed in the geometry hierarchy. If; the new location is different, two actions are possible according to the; value of `change`:. - `change = kFALSE` (default) - the modeller does not change the; current state but just inform the caller about this change.; - `change = kTRUE` - the modeller will actually perform a new; `‘Where am I?' `search after finding out that the location has; changed. The current state will be actualized accordingly. Note that even when performing a normal search on the current state; after changing the current point coordinates (e.g.; `gGeoManager->FindNode(newX,newY,newZ)`), users can always query if the; previous state has changed by using a method having the same name but; without parameters:. ~~~{.cpp}; Bool_t TGeoManager::IsSameLocation();; ~~~. \anchor GP02gb; #### Finding the Distance to the Next Boundary. All tracking engines need to compare the currently proposed physical; step with the maximum allowed distance in the current material. The; modeller provides this information by computing the distance to the; first boundary starting from the current point along a straight line.; The starting point and direction for this procedure are the ones; corresponding to the current state. The boundary search is initialized; inside the current volume and the crossed boundary can belong either to; the current node or to one of its daughters. The full prototype of the; method is:. ~~~{.cpp}; TGeoNode *TGeoManager::FindNextBoundary(Double_t step=kBig);; ~~~. I",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md:75576,perform,performing,75576,geom/geom/doc/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md,1,['perform'],['performing']
Performance,"t operates within the same; address space as other independent flows of controls within a process.; In most UNIX systems, thread and process characteristics are grouped; into a single entity called a process. Sometimes, threads are called; ""lightweight processes''. Note: This introduction is adapted from the AIX 4.3 Programmer's Manual. ## Threads and Processes. In traditional single-threaded process systems, a process has a set of; properties. In multi-threaded systems, these properties are divided; between processes and threads. ### Process Properties. A process in a multi-threaded system is the changeable entity. It must; be considered as an execution frame. It has all traditional process; attributes, such as:. - Process ID, process group ID, user ID, and group ID. - Environment. - Working directory. A process also provides a common address space and common system; resources:. - File descriptors. - Signal actions. - Shared libraries. - Inter-process communication tools (such as message queues, pipes,; semaphores, or shared memory). ### Thread Properties. A thread is the schedulable entity. It has only those properties that; are required to ensure its independent flow of control. These include; the following properties:. - Stack. - Scheduling properties (such as policy or priority). - Set of pending and blocked signals. - Some thread-specific data (TSD). An example of thread-specific data is the error indicator, `errno`. In; multi-threaded systems, `errno` is no longer a global variable, but; usually a subroutine returning a thread-specific `errno` value. Some; other systems may provide other implementations of `errno`. With respect; to ROOT, a thread specific data is for example the ***`gPad`*** pointer,; which is treated in a different way, whether it is accessed from any; thread or the main thread. Threads within a process must not be considered as a group of processes; (even though in Linux each thread receives an own process id, so that it; can be scheduled b",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Threads.md:1061,queue,queues,1061,documentation/users-guide/Threads.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Threads.md,1,['queue'],['queues']
Performance,"t optimizations in the middle-end. Rematerialization; of hoisted instructions to reduce register pressure is the responsibility of; the back-end, which has more accurate information about register pressure and; also handles other optimizations than LICM that increase live-ranges. This pass uses alias analysis for two purposes:. #. Moving loop invariant loads and calls out of loops. If we can determine; that a load or call inside of a loop never aliases anything stored to, we; can hoist it or sink it like any other instruction. #. Scalar Promotion of Memory. If there is a store instruction inside of the; loop, we try to move the store to happen AFTER the loop instead of inside of; the loop. This can only happen if a few conditions are true:. #. The pointer stored through is loop invariant.; #. There are no stores or loads in the loop which *may* alias the pointer.; There are no calls in the loop which mod/ref the pointer. If these conditions are true, we can promote the loads and stores in the; loop of the pointer to use a temporary alloca'd variable. We then use the; :ref:`mem2reg <passes-mem2reg>` functionality to construct the appropriate; SSA form for the variable. ``loop-deletion``: Delete dead loops; ------------------------------------. This file implements the Dead Loop Deletion Pass. This pass is responsible for; eliminating loops with non-infinite computable trip counts that have no side; effects or volatile instructions, and do not contribute to the computation of; the function's return value. .. _passes-loop-extract:. ``loop-extract``: Extract loops into new functions; --------------------------------------------------. A pass wrapper around the ``ExtractLoop()`` scalar transformation to extract; each top-level loop into its own new function. If the loop is the *only* loop; in a given function, it is not touched. This is a pass most useful for; debugging via bugpoint. ``loop-reduce``: Loop Strength Reduction; ----------------------------------------. This",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:24959,load,loads,24959,interpreter/llvm-project/llvm/docs/Passes.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst,1,['load'],['loads']
Performance,"t possible to specify that a particular one of the input; location descriptions is undefined. See the ``DW_OP_LLVM_undefined`` operation in; :ref:`amdgpu-dwarf-undefined-location-description-operations`. 2.6 Generalize Creation of Composite Location Descriptions; ----------------------------------------------------------. To allow composition of composite location descriptions, an explicit operation; that indicates the end of the definition of a composite location description is; required. This can be implied if the end of a DWARF expression is reached,; allowing current DWARF expressions to remain legal. See ``DW_OP_LLVM_piece_end`` in; :ref:`amdgpu-dwarf-composite-location-description-operations`. 2.7 Generalize DWARF Base Objects to Allow Any Location Description Kind; ------------------------------------------------------------------------. The number of registers and the cost of memory operations is much higher for; AMDGPU than a typical CPU. The compiler attempts to optimize whole variables and; arrays into registers. Currently DWARF only allows ``DW_OP_push_object_address`` and related operations; to work with a global memory location. To support AMDGPU optimized code it is; required to generalize DWARF to allow any location description to be used. This; allows registers, or composite location descriptions that may be a mixture of; memory, registers, or even implicit values. See ``DW_OP_push_object_address`` in; :ref:`amdgpu-dwarf-general-location-description-operations`. 2.8 General Support for Address Spaces; --------------------------------------. AMDGPU needs to be able to describe addresses that are in different kinds of; memory. Optimized code may need to describe a variable that resides in pieces; that are in different kinds of storage which may include parts of registers,; memory that is in a mixture of memory kinds, implicit values, or be undefined. DWARF has the concept of segment addresses. However, the segment cannot be; specified within a DWARF e",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst:16262,optimiz,optimize,16262,interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst,1,['optimiz'],['optimize']
Performance,"t present) does steps 6/7/8 for you. Common Problems; ===============. If you are having problems building or using LLVM, or if you have any other; general questions about LLVM, please consult the `Frequently Asked; Questions <FAQ.html>`_ page. If you are having problems with limited memory and build time, please try; building with ninja instead of make. Please consider configuring the; following options with cmake:. * -G Ninja; Setting this option will allow you to build with ninja instead of make.; Building with ninja significantly improves your build time, especially with; incremental builds, and improves your memory usage. * -DLLVM_USE_LINKER; Setting this option to lld will significantly reduce linking time for LLVM; executables on ELF-based platforms, such as Linux. If you are building LLVM; for the first time and lld is not available to you as a binary package, then; you may want to use the gold linker as a faster alternative to GNU ld. * -DCMAKE_BUILD_TYPE; Controls optimization level and debug information of the build. This setting; can affect RAM and disk usage, see :ref:`CMAKE_BUILD_TYPE <cmake_build_type>`; for more information. * -DLLVM_ENABLE_ASSERTIONS; This option defaults to ON for Debug builds and defaults to OFF for Release; builds. As mentioned in the previous option, using the Release build type and; enabling assertions may be a good alternative to using the Debug build type. * -DLLVM_PARALLEL_LINK_JOBS; Set this equal to number of jobs you wish to run simultaneously. This is; similar to the -j option used with make, but only for link jobs. This option; can only be used with ninja. You may wish to use a very low number of jobs,; as this will greatly reduce the amount of memory used during the build; process. If you have limited memory, you may wish to set this to 1. * -DLLVM_TARGETS_TO_BUILD; Set this equal to the target you wish to build. You may wish to set this to; X86; however, you will find a full list of targets within the; llvm-project/ll",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GettingStarted.rst:44764,optimiz,optimization,44764,interpreter/llvm-project/llvm/docs/GettingStarted.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GettingStarted.rst,1,['optimiz'],['optimization']
Performance,"t require a definition.; root [] foo::bar *baz1; // #2: does not require a definition.; root [] foo::bar baz2; // #3: requires a definition.; root [] TCanvas* c = new TCanvas(); // #4 requires a definition; ```. The implementation avoids recursive actions and relies on a well-defined (by; the C++ standard) behavior. Currently, this comes with a constant performance; overhead which we go in details bellow. ROOT uses the global module index (GMI) to avoid the performance overhead. ROOT; only preloads the set of C++ modules which are not present in the GMI. The; example becomes equivalent to:. ```cpp; // ROOT prompt; root [] import Foo.*; // Preload Foo if it is not in the GMI.; root [] S *s; // #1: does not require a definition.; root [] foo::bar *baz1; // #2: does not require a definition.; root [] foo::bar baz2; // #3: requires a definition.; root [] TCanvas* c = new TCanvas(); // #4 requires a definition; ```. Line #4 forces cling to send ROOT a callback that TCanvas in unknown but; the GMI resolves it to module Gpad, loads it and returns the control to cling. ### Performance; This section compares ROOT PCH technology with C++ Modules which is important but; unfair comparison. As we noted earlier, PCH is very efficient, it cannot be; extended to the experiments’ software stacks because of its design constraints.; On the contrary, the C++ Modules can be used in third-party code where the PCH; is not available. The comparisons are to give a good metric when we are ready to switch ROOT to use; C++ Modules by default. However, since it is essentially the same technology,; optimizations of C++ Modules also affect the PCH. We have a few tricks up in; the sleeves to but they come with given trade-offs. #### Preloading of C++ Modules. The main focus for the technology preview was not in performance until recently.; We have invested some resources in optimizations and we would like to show you; (probably outdated) performance results:. * Memory footprint -- mostly due to imp",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/README.CXXMODULES.md:17249,load,loads,17249,README/README.CXXMODULES.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/README.CXXMODULES.md,1,['load'],['loads']
Performance,"t sampling windows of 'resolution model' p.d.f. centered around zero, even if fit range of convolution observable does not bracket zero. Improve internal efficiency; RooAbsData - Add ability to plot efficiency distribution with correct binomial errors given a RooRealVar and a RooCategory category observable encoding distribution and accept/reject state respectively. See rf701_efficiencyfit.C for details ; RooAbsPdf - Included extended ML term by default in fit if p.d.f is extendable. You can still use Extended() to override default behavior. Do not run MINOS by default anymore if no fit options are provided.; RooProfileLL - Add option to always start minimization from global minimimum (takes more time, but improves reproducibility). Can now profile multi-core paralellized likelihoods as well.; RooRealSumPdf - Enable plotting of component p.d.f.s using same scheme as RooAddPdf, i.e. just use the Components() specified in plotOn().; RooExpensiveObjectCache - New cache manager for sharing and storing of expensive components cached by operator p.d.f.s ; RooMCStudy - Add Silence() argument to constructor to request minimal verbosity during running; RooMinuit - Improve contour() method to return RooPlots rather than drawing TGraphs straight on a canvas; RooWorkspace - Add private expensive object cache to workspace; RooBinningCategory - New real-to-category function that maps values of input RooRealVar to categories with labels that correspond to bins of input RooRealVar. See rf405_realtocatfuncs.C for details . RooStats; This is a new package introduced in this version for statistical tools built on top of RooFit. It is a joint effort between the LHC experiments and the ROOT team (see the RooStats Wiki page).; ; This version contains the interfaces for performing the statistical calculations and dealing with the obtained results and concrete classes implementing the statistical methods.; ; All the classes and functions in RooStats are provided in the namespace RooStats.; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v522/index.html:8778,cache,cache,8778,roofit/doc/v522/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v522/index.html,4,['cache'],"['cache', 'cached']"
Performance,"t search for module map files named ``module.modulemap`` and similar. This option is implied by ``-fmodules``. If this is disabled with ``-fno-implicit-module-maps``, module map files will only be loaded if they are explicitly specified via ``-fmodule-map-file`` or transitively used by another module map file. ``-fmodules-cache-path=<directory>``; Specify the path to the modules cache. If not provided, Clang will select a system-appropriate default. ``-fno-autolink``; Disable automatic linking against the libraries associated with imported modules. ``-fmodules-ignore-macro=macroname``; Instruct modules to ignore the named macro when selecting an appropriate module variant. Use this for macros defined on the command line that don't affect how modules are built, to improve sharing of compiled module files. ``-fmodules-prune-interval=seconds``; Specify the minimum delay (in seconds) between attempts to prune the module cache. Module cache pruning attempts to clear out old, unused module files so that the module cache itself does not grow without bound. The default delay is large (604,800 seconds, or 7 days) because this is an expensive operation. Set this value to 0 to turn off pruning. ``-fmodules-prune-after=seconds``; Specify the minimum time (in seconds) for which a file in the module cache must be unused (according to access time) before module pruning will remove it. The default delay is large (2,678,400 seconds, or 31 days) to avoid excessive module rebuilding. ``-module-file-info <module file name>``; Debugging aid that prints information about a given module file (with a ``.pcm`` extension), including the language and preprocessor options that particular module variant was built with. ``-fmodules-decluse``; Enable checking of module ``use`` declarations. ``-fmodule-name=module-id``; Consider a source file as a part of the given module. ``-fmodule-map-file=<file>``; Load the given module map file if a header from its directory or one of its subdirectories is loa",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/Modules.rst:15551,cache,cache,15551,interpreter/llvm-project/clang/docs/Modules.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/Modules.rst,2,['cache'],['cache']
Performance,"t shooting for the; ultimate optimization experience in this setting, but we also want to; catch the easy and quick stuff where possible. As such, we will choose; to run a few per-function optimizations as the user types the function; in. If we wanted to make a ""static Kaleidoscope compiler"", we would use; exactly the code we have now, except that we would defer running the; optimizer until the entire file has been parsed. In addition to the distinction between function and module passes, passes can be; divided into transform and analysis passes. Transform passes mutate the IR, and; analysis passes compute information that other passes can use. In order to add; a transform pass, all analysis passes it depends upon must be registered in; advance. In order to get per-function optimizations going, we need to set up a; `FunctionPassManager <../../WritingAnLLVMPass.html#what-passmanager-doesr>`_ to hold; and organize the LLVM optimizations that we want to run. Once we have; that, we can add a set of optimizations to run. We'll need a new; FunctionPassManager for each module that we want to optimize, so we'll; add to a function created in the previous chapter (``InitializeModule()``):. .. code-block:: c++. void InitializeModuleAndManagers(void) {; // Open a new context and module.; TheContext = std::make_unique<LLVMContext>();; TheModule = std::make_unique<Module>(""KaleidoscopeJIT"", *TheContext);; TheModule->setDataLayout(TheJIT->getDataLayout());. // Create a new builder for the module.; Builder = std::make_unique<IRBuilder<>>(*TheContext);. // Create new pass and analysis managers.; TheFPM = std::make_unique<FunctionPassManager>();; TheLAM = std::make_unique<LoopAnalysisManager>();; TheFAM = std::make_unique<FunctionAnalysisManager>();; TheCGAM = std::make_unique<CGSCCAnalysisManager>();; TheMAM = std::make_unique<ModuleAnalysisManager>();; ThePIC = std::make_unique<PassInstrumentationCallbacks>();; TheSI = std::make_unique<StandardInstrumentations>(*TheContext,; /*Debu",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:5403,optimiz,optimizations,5403,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,1,['optimiz'],['optimizations']
Performance,"t syntax tree (AST);; .dynamicExtensions - Turns on cling's dynamic extensions. This in turn enables the dynamic lookup and the late resolving of the identifier. With that option cling tries to heal the compile-time failed lookups at runtime;. Details; Command line. The interactive prompt supports an emacs-like command line editor, just like bash terminal, which makes it easy to integrate and use. Cling uses TextInput and doesn't depend on ncurses.; . Autocompletion should be coming soon!; ; #Include Declarations. Cling allows #include-s to be not only before the declarations. The includes could be mixed with other declarations. For example:; [cling]$ #include ""math.h""; [cling]$ sin(1); (double const) 8.414710e-01; [cling]$ #include ""stdio.h""; [cling]$ printf(""%f\n"", sin(1));; 0.841471. More statements could be combined using semicolon (;). This doesn't stay when the command is #include; The following example is invalid:[cling]$ #include ""math.h""; sin(1). The same rules are applicable for the other preprocessor directives (commands starting with # - such as #define); ; Variable Declarations. Cling allows statements to be entered onto the global scope. In order to be compiled and executed by the compiler these statements need to be wrapped into functions, which body contains the statement and afterwards to run the function. The semantics of the statements that declare variables is that variables should be accessed by other statements. If the statement that declare variable is wrapped into function the variables won't be accessible from outside anymore. In this case variables are extracted onto the global scope.; ; TODO: There should be dedicated entry for that in the docs; Builtins; Cling starts with very few builtins loaded. Users could extend the available builtins via extending the RuntimeUniverse.h, which is loaded at cling's startup.; . Copyright © Cling Team; . The ROOT Framework |; LLVM |; Clang |; Web Design. Page was modified on $Date$ in $Rev$ by $Author$. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/www/old/index.html:4226,load,loaded,4226,interpreter/cling/www/old/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/www/old/index.html,4,['load'],['loaded']
Performance,"t the shared library defining the classes theand avoid splicing if the data is copied. TStreamerInfo::New inserts the address of the creating TStreamerInfo into; the object. This address is inserted in each emulated that does not inherit; from an emulated class and is positioned after all the base classes (which; are compiled classes). A derived class will set this value inside each; of its emulated base class.; TStreamerInfo::Destruct and the new method TStreamerInfo::GetActualClass; use this information to detect the TStreamerInfo actually used to create; the object and hence run the proper emulated destructor. Add a new function GenerateDictionary to TInterpreter which allows for the quick and easy creation of a dictionary; given one (or more) class name(s) and the name(s) of its header files. gInterpreter->GenerateDictionary(""vector<vector<float> >;list<vector<float> >"",""list;vector"");; gInterpreter->GenerateDictionary(""myclass"",""myclass.h;myhelper.h"");; This replaces the recommendation of creating a small 'loader.C' script to create the dictionaries. Implement a ShowMembers function for interpreted classes, by querying the interpreter for the data member; information.; In order to fix possible buffer overflow of parent string buffer in TMemberInspector,; the signature of ShowMember() was changed to no longer require (nor request) the; caller to provide a buffer (of length unknown to the callee.); Improve the uniqueness of globally visible symbols to allow for the mixing of; dictionaries with very similar layout. Cont. New functions for TClonesArray:. AbsorbObjects(TClonesArray* otherTCA):; Allows one to directly move the object pointers from otherTCA to the calling; TCA without cloning (copying). The calling TCA takes over ownership of all of; the moved objects. otherTCA is left empty upon return. MultiSort(Int_t nTCs, TClonesArray** tcs):; Sorts multiple TClonesArrays simultaneously using the calling TCA's objects; as the sorting key. New function for TSeqColle",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/core/doc/v528/index.html:3566,load,loader,3566,core/doc/v528/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/core/doc/v528/index.html,2,['load'],['loader']
Performance,"t the top level in a file, in which case they cover all subsequent function; bodies until they're turned off. Note that it is undefined behavior to enter; code that is *not* covered by one of these pragmas from code that *is* covered; by one of these pragmas unless the floating point environment has been restored; to its default state. See the C standard for more information about these pragmas. The command line option ``-frounding-math`` behaves as if the translation unit; began with ``#pragma STDC FENV_ROUND FE_DYNAMIC``. The command line option; ``-ffp-model=strict`` behaves as if the translation unit began with ``#pragma STDC FENV_ACCESS ON``. Code that just wants to use a specific rounding mode for specific floating point; operations can avoid most of the hazards of the dynamic floating point environment; by using ``#pragma STDC FENV_ROUND`` with a value other than ``FE_DYNAMIC``. .. _crtfastmath.o:. A note about ``crtfastmath.o``; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; ``-ffast-math`` and ``-funsafe-math-optimizations`` cause ``crtfastmath.o`` to be; automatically linked, which adds a static constructor that sets the FTZ/DAZ; bits in MXCSR, affecting not only the current compilation unit but all static; and shared libraries included in the program. .. _FLT_EVAL_METHOD:. A note about ``__FLT_EVAL_METHOD__``; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; The ``__FLT_EVAL_METHOD__`` is not defined as a traditional macro, and so it; will not appear when dumping preprocessor macros. Instead, the value; ``__FLT_EVAL_METHOD__`` expands to is determined at the point of expansion; either from the value set by the ``-ffp-eval-method`` command line option or; from the target. This is because the ``__FLT_EVAL_METHOD__`` macro; cannot expand to the correct evaluation method in the presence of a ``#pragma``; which alters the evaluation method. An error is issued if; ``__FLT_EVAL_METHOD__`` is expanded inside a scope modified by; ``#pragma clang fp eval_method``. .. _fp-constant-eval:. A no",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:71698,optimiz,optimizations,71698,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,1,['optimiz'],['optimizations']
Performance,"t the; output to an area specific to the logged user.; Addition of a new class TProofProgressStatus, which is used to keep; the query progress stauts in all the TProofPlayer objects and in the; TPacketizerAdaptive. It is also send in kPROOF_GETPACKET and; kPROOF_STOPPROCESS messages. ; The class TPacketizerProgressive is removed. . Fixes. Enable; the max number of sessions ('mxsess' parameter in the xpd.schedparam; directive); users are just refused to start a session if this limit is; reached.Make sure to collect consistently input messages when running in asynchronous modeFix; a few problems with TProof::SendFile (used by UploadPackage, Load); appearing when a rapid sequence of these commands was submitted Invalidate the TProofMgr when the physical connection is; closed; avoids; crashing when trying to get the logs after a failure. ; Fix a memory leak in log retrieval (the TProofLog object; was never; deleted); Add protections for the cases the manager cannot be; initialized; Fix a race condition possibly affecting the handling of; workers death; Avoid duplicating worker logs in the master log file; unless; when explicitly needed by the request (Exec(...), Print(...)) or when; an error occuredFix; problem with the determination and transmission of the name of the; object to be processed. The problem appeared when processing files; containing >1 trees in changing order.Fix problem with TProof::Load loading the macro to one worker only per machineFix wrong return code preventing the correct propagation of the full ClearPackage to workersFix a problem causing the whole query to stop even in the case a worker was terminated gently with SIGTERM.; Fix a problem triggering full re-build of a package upon change of a; single file; the version info file was wrongly reset; this should; happen only after a re-build.Make sure that in case multiple TProofOutputFile are present, each get merged correctlyFix problem in TProofServLogHandler::Notify due to bad usage of Form(...). ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/proof/doc/v522/index.html:6597,race condition,race condition,6597,proof/doc/v522/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/proof/doc/v522/index.html,5,"['Load', 'load', 'race condition']","['Load', 'loading', 'race condition']"
Performance,"t their own personal PROOF cluster,; separated from the others: a problem occurring on one personal; cluster does not affect the workflow of other users. - **Easier administration and self-servicing.** A user can restart their; personal PROOF cluster in case of troubles without waiting for a; system administrator's intervention. - **Efficient multiuser scheduling.** PROOF on Demand makes PROOF run on; top of an existing resource management system, moving the problem of; scheduling many concurrent users outside of PROOF. This guide particularly refers to the setup of a static PROOF cluster; running on physical hosts: the recommended setup is in practice the same; as the ready-to-go Virtual Analysis Facility. If you want to use PROOF; on the clouds there is no configuration to go through. Setup a resource management system; ----------------------------------. Although PROOF on Demand can run on a cluster of nodes without using a; resource management system (using `pod-ssh`), it is recommended to setup a; dedicated one to benefit from the scheduling in a multiuser environment, or a; dedicated queue on an existing one. As there's a variety of resource management systems, this guide does not cover; their setup. The RMS preconfigured for the Virtual Analysis Facility is; [HTCondor](http://research.cs.wisc.edu/htcondor/), which we recommend primarily; because it has dynamic addition of workers built in. Configuration steps for all nodes; ---------------------------------. ### Setup CernVM-FS. [CernVM-FS](http://cernvm.cern.ch/portal/filesystem) should be installed; on all machines as the preferred method for software distribution. > Configuration instructions for the latest CernVM-FS can be found; > [here](http://cernvm.cern.ch/portal/filesystem/techinformation). A brief step-by-step procedure to install CernVM-FS is hereby described. - Download and install the latest stable version from; [here](http://cernvm.cern.ch/portal/filesystem): pick one which is; appropriate to you",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/proof/doc/confman/ConfigProofPoD.md:1420,queue,queue,1420,proof/doc/confman/ConfigProofPoD.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/proof/doc/confman/ConfigProofPoD.md,1,['queue'],['queue']
Performance,"t those elements actually used). .. _dss_tinyptrvector:. llvm/ADT/TinyPtrVector.h; ^^^^^^^^^^^^^^^^^^^^^^^^. ``TinyPtrVector<Type>`` is a highly specialized collection class that is; optimized to avoid allocation in the case when a vector has zero or one; elements. It has two major restrictions: 1) it can only hold values of pointer; type, and 2) it cannot hold a null pointer. Since this container is highly specialized, it is rarely used. .. _dss_smallvector:. llvm/ADT/SmallVector.h; ^^^^^^^^^^^^^^^^^^^^^^. ``SmallVector<Type, N>`` is a simple class that looks and smells just like; ``vector<Type>``: it supports efficient iteration, lays out elements in memory; order (so you can do pointer arithmetic between elements), supports efficient; push_back/pop_back operations, supports efficient random access to its elements,; etc. The main advantage of SmallVector is that it allocates space for some number of; elements (N) **in the object itself**. Because of this, if the SmallVector is; dynamically smaller than N, no malloc is performed. This can be a big win in; cases where the malloc/free call is far more expensive than the code that; fiddles around with the elements. This is good for vectors that are ""usually small"" (e.g. the number of; predecessors/successors of a block is usually less than 8). On the other hand,; this makes the size of the SmallVector itself large, so you don't want to; allocate lots of them (doing so will waste a lot of space). As such,; SmallVectors are most useful when on the stack. In the absence of a well-motivated choice for the number of; inlined elements ``N``, it is recommended to use ``SmallVector<T>`` (that is,; omitting the ``N``). This will choose a default number of; inlined elements reasonable for allocation on the stack (for example, trying; to keep ``sizeof(SmallVector<T>)`` around 64 bytes). SmallVector also provides a nice portable and efficient replacement for; ``alloca``. SmallVector has grown a few other minor advantages over std",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ProgrammersManual.rst:59943,perform,performed,59943,interpreter/llvm-project/llvm/docs/ProgrammersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ProgrammersManual.rst,1,['perform'],['performed']
Performance,"t uses a local PassManager; instance and creates a new ObjectBufferStream instance, both of which it; passes to TargetMachine::addPassesToEmitMC before calling PassManager::run; on the Module with which it was created. .. image:: MCJIT-load.png. The PassManager::run call causes the MC code generation mechanisms to emit; a complete relocatable binary object image (either in either ELF or MachO; format, depending on the target) into the ObjectBufferStream object, which; is flushed to complete the process. If an ObjectCache is being used, the; image will be passed to the ObjectCache here. At this point, the ObjectBufferStream contains the raw object image.; Before the code can be executed, the code and data sections from this; image must be loaded into suitable memory, relocations must be applied and; memory permission and code cache invalidation (if required) must be completed. Object Loading; ==============. Once an object image has been obtained, either through code generation or; having been retrieved from an ObjectCache, it is passed to RuntimeDyld to; be loaded. The RuntimeDyld wrapper class examines the object to determine; its file format and creates an instance of either RuntimeDyldELF or; RuntimeDyldMachO (both of which derive from the RuntimeDyldImpl base; class) and calls the RuntimeDyldImpl::loadObject method to perform that; actual loading. .. image:: MCJIT-dyld-load.png. RuntimeDyldImpl::loadObject begins by creating an ObjectImage instance; from the ObjectBuffer it received. ObjectImage, which wraps the; ObjectFile class, is a helper class which parses the binary object image; and provides access to the information contained in the format-specific; headers, including section, symbol and relocation information. RuntimeDyldImpl::loadObject then iterates through the symbols in the; image. Information about common symbols is collected for later use. For; each function or data symbol, the associated section is loaded into memory; and the symbol is stored in a",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst:3672,load,loaded,3672,interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,1,['load'],['loaded']
Performance,"t using a just-in-time compiler or an; interpreter. :program:`lli` is *not* an emulator. It will not execute IR of different architectures; and it can only interpret (or JIT-compile) for the host architecture. The JIT compiler takes the same arguments as other tools, like :program:`llc`,; but they don't necessarily work for the interpreter. If `filename` is not specified, then :program:`lli` reads the LLVM bitcode for the; program from standard input. The optional *args* specified on the command line are passed to the program as; arguments. GENERAL OPTIONS; ---------------. .. option:: -fake-argv0=executable. Override the ``argv[0]`` value passed into the executing program. .. option:: -force-interpreter={false,true}. If set to true, use the interpreter even if a just-in-time compiler is available; for this architecture. Defaults to false. .. option:: -help. Print a summary of command line options. .. option:: -load=pluginfilename. Causes :program:`lli` to load the plugin (shared object) named *pluginfilename* and use; it for optimization. .. option:: -stats. Print statistics from the code-generation passes. This is only meaningful for; the just-in-time compiler, at present. .. option:: -time-passes. Record the amount of time needed for each code-generation pass and print it to; standard error. .. option:: -version. Print out the version of :program:`lli` and exit without doing anything else. TARGET OPTIONS; --------------. .. option:: -mtriple=target triple. Override the target triple specified in the input bitcode file with the; specified string. This may result in a crash if you pick an; architecture which is not compatible with the current system. .. option:: -march=arch. Specify the architecture for which to generate assembly, overriding the target; encoded in the bitcode file. See the output of **llc -help** for a list of; valid architectures. By default this is inferred from the target triple or; autodetected to the current architecture. .. option:: -mcpu=cpu",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/lli.rst:1318,load,load,1318,interpreter/llvm-project/llvm/docs/CommandGuide/lli.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/lli.rst,2,"['load', 'optimiz']","['load', 'optimization']"
Performance,"t which denormal numbers the code is permitted to require. Valid values are:. * ``ieee`` - IEEE 754 denormal numbers; * ``preserve-sign`` - the sign of a flushed-to-zero number is preserved in the sign of 0; * ``positive-zero`` - denormals are flushed to positive zero. The default value depends on the target. For most targets, defaults to; ``ieee``. .. option:: -f[no-]strict-float-cast-overflow. When a floating-point value is not representable in a destination integer; type, the code has undefined behavior according to the language standard.; By default, Clang will not guarantee any particular result in that case.; With the 'no-strict' option, Clang will saturate towards the smallest and; largest representable integer values instead. NaNs will be converted to zero.; Defaults to ``-fstrict-float-cast-overflow``. .. option:: -f[no-]math-errno. Require math functions to indicate errors by setting errno.; The default varies by ToolChain. ``-fno-math-errno`` allows optimizations; that might cause standard C math functions to not set ``errno``.; For example, on some systems, the math function ``sqrt`` is specified; as setting ``errno`` to ``EDOM`` when the input is negative. On these; systems, the compiler cannot normally optimize a call to ``sqrt`` to use; inline code (e.g. the x86 ``sqrtsd`` instruction) without additional; checking to ensure that ``errno`` is set appropriately.; ``-fno-math-errno`` permits these transformations. On some targets, math library functions never set ``errno``, and so; ``-fno-math-errno`` is the default. This includes most BSD-derived; systems, including Darwin. .. option:: -f[no-]trapping-math. Control floating point exception behavior. ``-fno-trapping-math`` allows optimizations that assume that floating point operations cannot generate traps such as divide-by-zero, overflow and underflow. - The option ``-ftrapping-math`` behaves identically to ``-ffp-exception-behavior=strict``.; - The option ``-fno-trapping-math`` behaves identically to `",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:55612,optimiz,optimizations,55612,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,1,['optimiz'],['optimizations']
Performance,"t which will be used to; discover and run tests in the test suite. Generally this will be a builtin test; format available from the *lit.formats* module. **test_source_root** The filesystem path to the test suite root. For out-of-dir; builds this is the directory that will be scanned for tests. **test_exec_root** For out-of-dir builds, the path to the test suite root inside; the object directory. This is where tests will be run and temporary output files; placed. **environment** A dictionary representing the environment to use when executing; tests in the suite. **standalone_tests** When true, mark a directory with tests expected to be run; standalone. Test discovery is disabled for that directory. *lit.suffixes* and; *lit.excludes* must be empty when this variable is true. **suffixes** For **lit** test formats which scan directories for tests, this; variable is a list of suffixes to identify test files. Used by: *ShTest*. **substitutions** For **lit** test formats which substitute variables into a test; script, the list of substitutions to perform. Used by: *ShTest*. **unsupported** Mark an unsupported directory, all tests within it will be; reported as unsupported. Used by: *ShTest*. **parent** The parent configuration, this is the config object for the directory; containing the test suite, or None. **root** The root configuration. This is the top-most :program:`lit` configuration in; the project. **pipefail** Normally a test using a shell pipe fails if any of the commands; on the pipe fail. If this is not desired, setting this variable to false; makes the test fail only if the last command in the pipe fails. **available_features** A set of features that can be used in `XFAIL`,; `REQUIRES`, and `UNSUPPORTED` directives. TEST DISCOVERY; ~~~~~~~~~~~~~~. Once test suites are located, :program:`lit` recursively traverses the source; directory (following *test_source_root*) looking for tests. When :program:`lit`; enters a sub-directory, it first checks to see if a neste",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/lit.rst:16692,perform,perform,16692,interpreter/llvm-project/llvm/docs/CommandGuide/lit.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/lit.rst,1,['perform'],['perform']
Performance,"t within the range of; the fixed point type. Examples; """""""""""""""""". .. code-block:: llvm. %res = call i4 @llvm.umul.fix.i4(i4 3, i4 2, i32 0) ; %res = 6 (2 x 3 = 6); %res = call i4 @llvm.umul.fix.i4(i4 3, i4 2, i32 1) ; %res = 3 (1.5 x 1 = 1.5). ; The result in the following could be rounded down to 3.5 or up to 4; %res = call i4 @llvm.umul.fix.i4(i4 15, i4 1, i32 1) ; %res = 7 (or 8) (7.5 x 0.5 = 3.75). '``llvm.smul.fix.sat.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax; """""""""""""". This is an overloaded intrinsic. You can use ``llvm.smul.fix.sat``; on any integer bit width or vectors of integers. ::. declare i16 @llvm.smul.fix.sat.i16(i16 %a, i16 %b, i32 %scale); declare i32 @llvm.smul.fix.sat.i32(i32 %a, i32 %b, i32 %scale); declare i64 @llvm.smul.fix.sat.i64(i64 %a, i64 %b, i32 %scale); declare <4 x i32> @llvm.smul.fix.sat.v4i32(<4 x i32> %a, <4 x i32> %b, i32 %scale). Overview; """""""""""""""""". The '``llvm.smul.fix.sat``' family of intrinsic functions perform signed; fixed point saturating multiplication on 2 arguments of the same scale. Arguments; """""""""""""""""""". The arguments (%a and %b) and the result may be of integer types of any bit; width, but they must have the same bit width. ``%a`` and ``%b`` are the two; values that will undergo signed fixed point multiplication. The argument; ``%scale`` represents the scale of both operands, and must be a constant; integer. Semantics:; """""""""""""""""""". This operation performs fixed point multiplication on the 2 arguments of a; specified scale. The result will also be returned in the same scale specified; in the third argument. If the result value cannot be precisely represented in the given scale, the; value is rounded up or down to the closest representable value. The rounding; direction is unspecified. The maximum value this operation can clamp to is the largest signed value; representable by the bit width of the first 2 arguments. The minimum value is the; smallest signed value representable by this bit width. Example",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:625524,perform,perform,625524,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['perform']
Performance,"t work for your use case without; requiring a custom build. Collector Requirements; ----------------------. You should be able to leverage any existing collector library that includes the following elements:. #. A memory allocator which exposes an allocation function your compiled; code can call. #. A binary format for the stack map. A stack map describes the location; of references at a safepoint and is used by precise collectors to identify; references within a stack frame on the machine stack. Note that collectors; which conservatively scan the stack don't require such a structure. #. A stack crawler to discover functions on the call stack, and enumerate the; references listed in the stack map for each call site. #. A mechanism for identifying references in global locations (e.g. global; variables). #. If you collector requires them, an LLVM IR implementation of your collectors; load and store barriers. Note that since many collectors don't require; barriers at all, LLVM defaults to lowering such barriers to normal loads; and stores unless you arrange otherwise. Implementing a collector plugin; -------------------------------. User code specifies which GC code generation to use with the ``gc`` function; attribute or, equivalently, with the ``setGC`` method of ``Function``. To implement a GC plugin, it is necessary to subclass ``llvm::GCStrategy``,; which can be accomplished in a few lines of boilerplate code. LLVM's; infrastructure provides access to several important algorithms. For an; uncontroversial collector, all that remains may be to compile LLVM's computed; stack map to assembly code (using the binary representation expected by the; runtime library). This can be accomplished in about 100 lines of code. This is not the appropriate place to implement a garbage collected heap or a; garbage collector itself. That code should exist in the language's runtime; library. The compiler plugin is responsible for generating code which conforms; to the binary interface",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GarbageCollection.rst:23310,load,loads,23310,interpreter/llvm-project/llvm/docs/GarbageCollection.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GarbageCollection.rst,1,['load'],['loads']
Performance,"t {; int data;; struct LinkedList *next;; };. struct LinkedList * _Nullable getNext(struct LinkedList *l);. void updateNextData(struct LinkedList *list, int newData) {; struct LinkedList *next = getNext(list);; // Warning: Nullable pointer is dereferenced; next->data = 7;; }. .. _nullability-NullablePassedToNonnull:. nullability.NullablePassedToNonnull (ObjC); """"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""; Warns when a nullable pointer is passed to a pointer which has a _Nonnull type. .. code-block:: objc. typedef struct Dummy { int val; } Dummy;; Dummy *_Nullable returnsNullable();; void takesNonnull(Dummy *_Nonnull);. void test() {; Dummy *p = returnsNullable();; takesNonnull(p); // warn; }. .. _nullability-NullableReturnedFromNonnull:. nullability.NullableReturnedFromNonnull (ObjC); """"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""; Warns when a nullable pointer is returned from a function that has _Nonnull return type. .. _optin-checkers:. optin; ^^^^^. Checkers for portability, performance or coding style specific rules. .. _optin-core-EnumCastOutOfRange:. optin.core.EnumCastOutOfRange (C, C++); """"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""; Check for integer to enumeration casts that would produce a value with no; corresponding enumerator. This is not necessarily undefined behavior, but can; lead to nasty surprises, so projects may decide to use a coding standard that; disallows these ""unusual"" conversions. Note that no warnings are produced when the enum type (e.g. `std::byte`) has no; enumerators at all. .. code-block:: cpp. enum WidgetKind { A=1, B, C, X=99 };. void foo() {; WidgetKind c = static_cast<WidgetKind>(3); // OK; WidgetKind x = static_cast<WidgetKind>(99); // OK; WidgetKind d = static_cast<WidgetKind>(4); // warn; }. **Limitations**. This checker does not accept the coding pattern where an enum type is used to; store combinations of flag values:. .. code-block:: cpp. enum AnimalFlags; {; HasClaws = 1,; CanFly = 2,; EatsFish = 4,; Endangered = 8; };. AnimalFla",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/analyzer/checkers.rst:12852,perform,performance,12852,interpreter/llvm-project/clang/docs/analyzer/checkers.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/analyzer/checkers.rst,1,['perform'],['performance']
Performance,"t's all together, compile the file with a simple ""``gmake``"" command; from the top level of your build directory and you should get a new file; ""``lib/LLVMHello.so``"". Note that everything in this file is; contained in an anonymous namespace --- this reflects the fact that passes; are self contained units that do not need external interfaces (although they; can have them) to be useful. Running a pass with ``opt``; ---------------------------. Now that you have a brand new shiny shared object file, we can use the; :program:`opt` command to run an LLVM program through your pass. Because you; registered your pass with ``RegisterPass``, you will be able to use the; :program:`opt` tool to access it, once loaded. To test it, follow the example at the end of the :doc:`GettingStarted` to; compile ""Hello World"" to LLVM. We can now run the bitcode file (hello.bc) for; the program through our transformation like this (or course, any bitcode file; will work):. .. code-block:: console. $ opt -load lib/LLVMHello.so -hello < hello.bc > /dev/null; Hello: __main; Hello: puts; Hello: main. The :option:`-load` option specifies that :program:`opt` should load your pass; as a shared object, which makes ""``-hello``"" a valid command line argument; (which is one reason you need to :ref:`register your pass; <writing-an-llvm-pass-registration>`). Because the Hello pass does not modify; the program in any interesting way, we just throw away the result of; :program:`opt` (sending it to ``/dev/null``). To see what happened to the other string you registered, try running; :program:`opt` with the :option:`-help` option:. .. code-block:: console. $ opt -load lib/LLVMHello.so -help; OVERVIEW: llvm .bc -> .bc modular optimizer and analysis printer. USAGE: opt [subcommand] [options] <input bitcode file>. OPTIONS:; Optimizations available:; ...; -guard-widening - Widen guards; -gvn - Global Value Numbering; -gvn-hoist - Early GVN Hoisting of Expressions; -hello - Hello World Pass; -indvars - Induction ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst:8249,load,load,8249,interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst,1,['load'],['load']
Performance,"t(0) &; - system - generic vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0); and s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt lgkmcnt(0); must happen after; preceding; global/generic load; atomic/store; atomic/atomicrmw; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; lgkmcnt(0) and so do; not need to be; considered.); - s_waitcnt vmcnt(0); must happen after; preceding; global/generic load; atomic/store; atomic/atomicrmw; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; vmcnt(0) and so do; not need to be; considered.); - Ensures any; preceding; sequential; consistent global; memory instructions; have completed; before executing; this sequentially; consistent; instruction. This; prevents reordering; a seq_cst store; followed by a; seq_cst load. (Note; that seq_cst is; stronger than; acquire/release as; the reordering of; load acquire; followed by a store; release is; prevented by the; s_waitcnt of; the release, but; there is nothing; preventing a store; release followed by; load acquire from; completing out of; order. The s_waitcnt; could be placed after; seq_store or before; the seq_load. We; choose the load to; make the s_waitcnt be; as late as possible; so that the store; may have already; completed.). 2. *Following; instructions same as; corresponding load; atomic acquire,; except must generate; all instructions even; for OpenCL.*; store atomic seq_cst - singlethread - global *Same as corresponding; - wavefront - local store atomic release,; - workgroup - generic except must generate; - agent all instructions even; - system for OpenCL.*; atomicrmw seq_cst - singlethread - global *Same as corresponding; - wavefront - local atomicrmw acq_rel,; - workgroup - generic except must generate; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:282934,load,load,282934,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"t(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global and L2 writeback; have completed before; performing the; atomicrmw that is; being released. 3. buffer/global_atomic; sc1=1; 4. s_waitcnt vmcnt(0). - Must happen before; following; buffer_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; caches. 5. buffer_inv sc0=1 sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; MTYPE NC global data.; MTYPE RW and CC memory will; never be stale due to the; memory probes. atomicrmw acq_rel - agent - generic 1. buffer_wbl2 sc1=1. - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at agent scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global have; completed before; performing the; atomicrmw that is; bein",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:322003,load,loads,322003,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['loads']
Performance,"t(0).; - If OpenCL, omit; lgkmcnt(0).; - Must happen before; following; buffer_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; cache. 5. buffer_inv sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acq_rel - system - generic 1. buffer_wbl2 sc0=1 sc1=1. - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at system scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global and L2 writeback; have completed before; performing the; atomicrmw that is; being released. 3. flat_atomic sc1=1; 4. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Must happen before; following; buffer_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; caches. 5. buffer_inv sc0=1 sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; MTYPE NC global data.; MTYPE RW and CC memory will; never be stale due to the; memory probes. fence acq_rel - singlethread *none* *none*; - wavefront; fence acq_rel - workgroup *none* 1. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL and; addre",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:324133,load,load,324133,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"t, it will emit globals to the section specified.; Additionally, the global can placed in a comdat if the target has the necessary; support. External declarations may have an explicit section specified. Section; information is retained in LLVM IR for targets that make use of this; information. Attaching section information to an external declaration is an; assertion that its definition is located in the specified section. If the; definition is located in a different section, the behavior is undefined. LLVM allows an explicit code model to be specified for globals. If the; target supports it, it will emit globals in the code model specified,; overriding the code model used to compile the translation unit.; The allowed values are ""tiny"", ""small"", ""kernel"", ""medium"", ""large"".; This may be extended in the future to specify global data layout that; doesn't cleanly fit into a specific code model. By default, global initializers are optimized by assuming that global; variables defined within the module are not modified from their; initial values before the start of the global initializer. This is; true even for variables potentially accessible from outside the; module, including those with external linkage or appearing in; ``@llvm.used`` or dllexported variables. This assumption may be suppressed; by marking the variable with ``externally_initialized``. An explicit alignment may be specified for a global, which must be a; power of 2. If not present, or if the alignment is set to zero, the; alignment of the global is set by the target to whatever it feels; convenient. If an explicit alignment is specified, the global is forced; to have exactly that alignment. Targets and optimizers are not allowed; to over-align the global if the global has an assigned section. In this; case, the extra alignment could be observable: for example, code could; assume that the globals are densely packed in their section and try to; iterate over them as an array, alignment padding would break thi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:33955,optimiz,optimized,33955,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['optimiz'],['optimized']
Performance,"t. The Darwin and Linux implementation; relies on padding and the ability to map a file over the existing memory; mapping which is generally only available on POSIX systems and isn't suitable; for other platforms. On Fuchsia, we rely on the ability to relocate counters at runtime using a; level of indirection. On every counter access, we add a bias to the counter; address. This bias is stored in ``__llvm_profile_counter_bias`` symbol that's; provided by the profile runtime and is initially set to zero, meaning no; relocation. The runtime can map the profile into memory at arbitrary locations,; and set bias to the offset between the original and the new counter location,; at which point every subsequent counter access will be to the new location,; which allows updating profile directly akin to the continuous mode. The advantage of this approach is that doesn't require any special OS support.; The disadvantage is the extra overhead due to additional instructions required; for each counter access (overhead both in terms of binary size and performance); plus duplication of counters (i.e. one copy in the binary itself and another; copy that's mapped into memory). This implementation can be also enabled for; other platforms by passing the ``-runtime-counter-relocation`` option to the; backend during compilation. For a program such as the `Lit <https://llvm.org/docs/CommandGuide/lit.html>`_; testing tool which invokes other programs, it may be necessary to set; ``LLVM_PROFILE_FILE`` for each invocation. The pattern strings ""%p"" or ""%Nm""; may help to avoid corruption due to concurrency. Note that ""%p"" is also a Lit; token and needs to be escaped as ""%%p"". .. code-block:: console. % clang++ -fprofile-instr-generate -fcoverage-mapping -mllvm -runtime-counter-relocation foo.cc -o foo. Creating coverage reports; =========================. Raw profiles have to be **indexed** before they can be used to generate; coverage reports. This is done using the ""merge"" tool in ``llvm-profd",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/SourceBasedCodeCoverage.rst:5094,perform,performance,5094,interpreter/llvm-project/clang/docs/SourceBasedCodeCoverage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/SourceBasedCodeCoverage.rst,1,['perform'],['performance']
Performance,"t. Therefore, the; vector and scalar memory operations performed by different wavefronts, whether; executing in the same or different work-groups (which may be executing on; different CUs accessing different L0s), can be reordered relative to each; other. A ``s_waitcnt vmcnt(0) & vscnt(0)`` is required to ensure; synchronization between vector memory operations of different wavefronts. It; ensures a previous vector memory operation has completed before executing a; subsequent vector memory or LDS operation and so can be used to meet the; requirements of acquire, release and sequential consistency.; * The L1 caches use an L2 cache shared by all SAs on the same agent.; * The L2 cache has independent channels to service disjoint ranges of virtual; addresses.; * Each L1 quadrant of a single SA accesses a different L2 channel. Each L1; quadrant has a separate request queue per L2 channel. Therefore, the vector; and scalar memory operations performed by wavefronts executing in different; work-groups (which may be executing on different SAs) of an agent can be; reordered relative to each other. A ``s_waitcnt vmcnt(0) & vscnt(0)`` is; required to ensure synchronization between vector memory operations of; different SAs. It ensures a previous vector memory operation has completed; before executing a subsequent vector memory and so can be used to meet the; requirements of acquire, release and sequential consistency.; * The L2 cache can be kept coherent with other agents on some targets, or ranges; of virtual addresses can be set up to bypass it to ensure system coherence.; * On GFX10.3 and GFX11 a memory attached last level (MALL) cache exists for GPU memory.; The MALL cache is fully coherent with GPU memory and has no impact on system; coherence. All agents (GPU and CPU) access GPU memory through the MALL cache. Scalar memory operations are only used to access memory that is proven to not; change during the execution of the kernel dispatch. This includes constant; address sp",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:339079,perform,performed,339079,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['perform'],['performed']
Performance,"t.*. 1. The ``DW_AT_data_location`` attribute may be used with any type that; provides one or more levels of hidden indirection and/or run-time parameters; in its representation. Its value is a DWARF operation expression E which; computes the location description of the data for an object. When this; attribute is omitted, the location description of the data is the same as; the location description of the object. The result of the attribute is obtained by evaluating E with a context that; has a result kind of a location description, an object that is the location; description of the data descriptor, the compilation unit that contains E, an; empty initial stack, and other context elements corresponding to the source; language thread of execution upon which the user is focused, if any. The; result of the evaluation is the location description of the base of the; member entry. *E will typically involve an operation expression that begins with a*; ``DW_OP_push_object_address`` *operation which loads the location; description of the object which can then serve as a descriptor in subsequent; calculation.*. .. note::. Since ``DW_AT_data_member_location``, ``DW_AT_use_location``, and; ``DW_AT_vtable_elem_location`` allow both operation expressions and; location list expressions, why does ``DW_AT_data_location`` not allow; both? In all cases they apply to data objects so less likely that; optimization would cause different operation expressions for different; program location ranges. But if supporting for some then should be for; all. It seems odd this attribute is not the same as; ``DW_AT_data_member_location`` in having an initial stack with the; location description of the object since the expression has to need it. A.6 Other Debugging Information; -------------------------------. .. note::. This section provides changes to existing debugger information entry; attributes. These would be incorporated into the corresponding DWARF Version 5; chapter 6 sections. A.6.1 Accelera",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst:184732,load,loads,184732,interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst,1,['load'],['loads']
Performance,"t...\n"");. int main(int argc, const char **argv) {; auto ExpectedParser = CommonOptionsParser::create(argc, argv, MyToolCategory);; if (!ExpectedParser) {; // Fail gracefully for unsupported options.; llvm::errs() << ExpectedParser.takeError();; return 1;; }; CommonOptionsParser& OptionsParser = ExpectedParser.get();; ClangTool Tool(OptionsParser.getCompilations(),; OptionsParser.getSourcePathList());; return Tool.run(newFrontendActionFactory<clang::SyntaxOnlyAction>().get());; }. And that's it! You can compile our new tool by running ninja from the; ``build`` directory. .. code-block:: console. cd ~/clang-llvm/build; ninja. You should now be able to run the syntax checker, which is located in; ``~/clang-llvm/build/bin``, on any source file. Try it!. .. code-block:: console. echo ""int main() { return 0; }"" > test.cpp; bin/loop-convert test.cpp --. Note the two dashes after we specify the source file. The additional; options for the compiler are passed after the dashes rather than loading; them from a compilation database - there just aren't any options needed; right now. Intermezzo: Learn AST matcher basics; ====================================. Clang recently introduced the :doc:`ASTMatcher; library <LibASTMatchers>` to provide a simple, powerful, and; concise way to describe specific patterns in the AST. Implemented as a; DSL powered by macros and templates (see; `ASTMatchers.h <../doxygen/ASTMatchers_8h_source.html>`_ if you're; curious), matchers offer the feel of algebraic data types common to; functional programming languages. For example, suppose you wanted to examine only binary operators. There; is a matcher to do exactly that, conveniently named ``binaryOperator``.; I'll give you one guess what this matcher does:. .. code-block:: c++. binaryOperator(hasOperatorName(""+""), hasLHS(integerLiteral(equals(0)))). Shockingly, it will match against addition expressions whose left hand; side is exactly the literal 0. It will not match against other forms of; 0, such",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LibASTMatchersTutorial.rst:5536,load,loading,5536,interpreter/llvm-project/clang/docs/LibASTMatchersTutorial.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LibASTMatchersTutorial.rst,1,['load'],['loading']
Performance,t/NonTrivialTypesLibcMemoryCallsCheck.cpp; clang-tools-extra/clang-tidy/cert/NonTrivialTypesLibcMemoryCallsCheck.h; clang-tools-extra/clang-tidy/cert/PostfixOperatorCheck.cpp; clang-tools-extra/clang-tidy/cert/PostfixOperatorCheck.h; clang-tools-extra/clang-tidy/cert/ProperlySeededRandomGeneratorCheck.cpp; clang-tools-extra/clang-tidy/cert/ProperlySeededRandomGeneratorCheck.h; clang-tools-extra/clang-tidy/cert/SetLongJmpCheck.cpp; clang-tools-extra/clang-tidy/cert/SetLongJmpCheck.h; clang-tools-extra/clang-tidy/cert/StaticObjectExceptionCheck.cpp; clang-tools-extra/clang-tidy/cert/StaticObjectExceptionCheck.h; clang-tools-extra/clang-tidy/cert/StrToNumCheck.cpp; clang-tools-extra/clang-tidy/cert/StrToNumCheck.h; clang-tools-extra/clang-tidy/cert/ThrownExceptionTypeCheck.cpp; clang-tools-extra/clang-tidy/cert/ThrownExceptionTypeCheck.h; clang-tools-extra/clang-tidy/cert/VariadicFunctionDefCheck.cpp; clang-tools-extra/clang-tidy/cert/VariadicFunctionDefCheck.h; clang-tools-extra/clang-tidy/concurrency/MtUnsafeCheck.cpp; clang-tools-extra/clang-tidy/concurrency/MtUnsafeCheck.h; clang-tools-extra/clang-tidy/concurrency/ThreadCanceltypeAsynchronousCheck.cpp; clang-tools-extra/clang-tidy/concurrency/ThreadCanceltypeAsynchronousCheck.h; clang-tools-extra/clang-tidy/cppcoreguidelines/AvoidGotoCheck.cpp; clang-tools-extra/clang-tidy/cppcoreguidelines/AvoidGotoCheck.h; clang-tools-extra/clang-tidy/cppcoreguidelines/AvoidNonConstGlobalVariablesCheck.cpp; clang-tools-extra/clang-tidy/cppcoreguidelines/AvoidNonConstGlobalVariablesCheck.h; clang-tools-extra/clang-tidy/cppcoreguidelines/CppCoreGuidelinesTidyModule.cpp; clang-tools-extra/clang-tidy/cppcoreguidelines/InitVariablesCheck.cpp; clang-tools-extra/clang-tidy/cppcoreguidelines/InitVariablesCheck.h; clang-tools-extra/clang-tidy/cppcoreguidelines/InterfacesGlobalInitCheck.cpp; clang-tools-extra/clang-tidy/cppcoreguidelines/InterfacesGlobalInitCheck.h; clang-tools-extra/clang-tidy/cppcoreguidelines/MacroUsageCheck.cpp; clang-,MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/tools/clang-formatted-files.txt:51205,concurren,concurrency,51205,interpreter/llvm-project/clang/docs/tools/clang-formatted-files.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/tools/clang-formatted-files.txt,1,['concurren'],['concurrency']
Performance,"t2"");; root[] list2->SetTree(""tree2"", ""file2"");; root[] list2->Enter(0);; root[] list2->Enter(3);; root[] list1->Add(list2);; root[] list1->Print(""all""); tree1 file1; 0; 2; tree2 file2; 0; 3; ```. The result is a **`TEntryList`** for a **`TChain`** of `tree1` and; `tree2`. If the second list was for the same **`TTree`** in the same; file as the first list, the result would be as follows:. ``` {.cpp}; root[] TEntryList *list2_2 = new TEntryList(""list2_2"", ""list2_2"");; root[] list2_2->SetTree(""tree2"", ""file2"");; root[] list2_2->Enter(1);; root[] list2_2->Enter(2);; root[] list2->Add(list2_2);; root[] list2->Print(""all""); tree2 file2; 0; 1; 2; 3; ```. #### TEntryListFromFile. This is a special kind of **`TEntryList`**, used only when processing; **`TChain`** objects (see the method `TChain::SetEntryListFile()`). It; is used in the case, when the entry lists, corresponding to the trees of; this chain, are stored in separate files. It allows to load the entry; lists in memory one by one, keeping only the list for the currently; processed tree loaded. For more details on entry lists, see **`TEntryList`**,; **`TEntryListBlock`** and **`TEntryListFromFile`** class descriptions,; functions **`TChain`**`::SetEntryList()`, `TChain::SetEntryListFile()`,; and the macro `$ROOTSYS/test/stressEntryList.C`. ### Filling a Histogram. The `TTree::Draw` method can also be used to fill a specific histogram.; The syntax is:. ``` {.cpp}; root[] TFile *f = new TFile(""Event.root""); root[] T->Draw(""fNtrack >> myHisto""); root[] myHisto->Print(); TH1.Print Name= myHisto, Entries= 100, Total sum= 100; ```. As we can see, this created a **`TH1`**, called `myHisto`. If you want; to append more entries to the histogram, you can use this syntax:. ``` {.cpp}; root[] T->Draw(""fNtrack >>+ myHisto""); ```. If you do not create a histogram ahead of time, ROOT will create one at; the time of the Draw command (as is the case above). If you would like; to draw the variable into a specific histogram where you,",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Trees.md:114677,load,load,114677,documentation/users-guide/Trees.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Trees.md,2,['load'],"['load', 'loaded']"
Performance,"t; address, and a new value to place at that address if the compared values; are equal. The type of '<cmp>' must be an integer or pointer type whose; bit width is a power of two greater than or equal to eight and less; than or equal to a target-specific size limit. '<cmp>' and '<new>' must; have the same type, and the type of '<pointer>' must be a pointer to; that type. If the ``cmpxchg`` is marked as ``volatile``, then the; optimizer is not allowed to modify the number or order of execution of; this ``cmpxchg`` with other :ref:`volatile operations <volatile>`. The success and failure :ref:`ordering <ordering>` arguments specify how this; ``cmpxchg`` synchronizes with other atomic operations. Both ordering parameters; must be at least ``monotonic``, the failure ordering cannot be either; ``release`` or ``acq_rel``. A ``cmpxchg`` instruction can also take an optional; "":ref:`syncscope <syncscope>`"" argument. Note: if the alignment is not greater or equal to the size of the `<value>`; type, the atomic operation is likely to require a lock and have poor; performance. The alignment is only optional when parsing textual IR; for in-memory IR, it is; always present. If unspecified, the alignment is assumed to be equal to the; size of the '<value>' type. Note that this default alignment assumption is; different from the alignment used for the load/store instructions when align; isn't specified. The pointer passed into cmpxchg must have alignment greater than or; equal to the size in memory of the operand. Semantics:; """""""""""""""""""". The contents of memory at the location specified by the '``<pointer>``' operand; is read and compared to '``<cmp>``'; if the values are equal, '``<new>``' is; written to the location. The original value at the location is returned,; together with a flag indicating success (true) or failure (false). If the cmpxchg operation is marked as ``weak`` then a spurious failure is; permitted: the operation may not write ``<new>`` even if the comparison; match",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:427229,perform,performance,427229,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performance']
Performance,"t; value as an i16, then convert it to float with; :ref:`llvm.convert.from.fp16 <int_convert_from_fp16>`. Computation can; then be performed on the float value (including extending to double; etc). To store the value back to memory, it is first converted to float; if needed, then converted to i16 with; :ref:`llvm.convert.to.fp16 <int_convert_to_fp16>`, then storing as an; i16 value. .. _int_convert_to_fp16:. '``llvm.convert.to.fp16``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare i16 @llvm.convert.to.fp16.f32(float %a); declare i16 @llvm.convert.to.fp16.f64(double %a). Overview:; """""""""""""""""". The '``llvm.convert.to.fp16``' intrinsic function performs a conversion from a; conventional floating-point type to half precision floating-point format. Arguments:; """""""""""""""""""". The intrinsic function contains single argument - the value to be; converted. Semantics:; """""""""""""""""""". The '``llvm.convert.to.fp16``' intrinsic function performs a conversion from a; conventional floating-point format to half precision floating-point format. The; return value is an ``i16`` which contains the converted number. Examples:; """""""""""""""""". .. code-block:: llvm. %res = call i16 @llvm.convert.to.fp16.f32(float %a); store i16 %res, i16* @x, align 2. .. _int_convert_from_fp16:. '``llvm.convert.from.fp16``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare float @llvm.convert.from.fp16.f32(i16 %a); declare double @llvm.convert.from.fp16.f64(i16 %a). Overview:; """""""""""""""""". The '``llvm.convert.from.fp16``' intrinsic function performs a; conversion from half precision floating-point format to single precision; floating-point format. Arguments:; """""""""""""""""""". The intrinsic function contains single argument - the value to be; converted. Semantics:; """""""""""""""""""". The '``llvm.convert.from.fp16``' intrinsic function performs a; conversion from half single precision floating-point format to single; precision floating-point format. The input half-float va",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:682024,perform,performs,682024,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performs']
Performance,"t> = icmp eq i32 4, 5 ; yields: result=false; <result> = icmp ne ptr %X, %X ; yields: result=false; <result> = icmp ult i16 4, 5 ; yields: result=true; <result> = icmp sgt i16 4, 5 ; yields: result=false; <result> = icmp ule i16 -4, 5 ; yields: result=false; <result> = icmp sge i16 4, 5 ; yields: result=false. .. _i_fcmp:. '``fcmp``' Instruction; ^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. <result> = fcmp [fast-math flags]* <cond> <ty> <op1>, <op2> ; yields i1 or <N x i1>:result. Overview:; """""""""""""""""". The '``fcmp``' instruction returns a boolean value or vector of boolean; values based on comparison of its operands. If the operands are floating-point scalars, then the result type is a; boolean (:ref:`i1 <t_integer>`). If the operands are floating-point vectors, then the result type is a; vector of boolean with the same number of elements as the operands being; compared. Arguments:; """""""""""""""""""". The '``fcmp``' instruction takes three operands. The first operand is; the condition code indicating the kind of comparison to perform. It is; not a value, just a keyword. The possible condition codes are:. #. ``false``: no comparison, always returns false; #. ``oeq``: ordered and equal; #. ``ogt``: ordered and greater than; #. ``oge``: ordered and greater than or equal; #. ``olt``: ordered and less than; #. ``ole``: ordered and less than or equal; #. ``one``: ordered and not equal; #. ``ord``: ordered (no nans); #. ``ueq``: unordered or equal; #. ``ugt``: unordered or greater than; #. ``uge``: unordered or greater than or equal; #. ``ult``: unordered or less than; #. ``ule``: unordered or less than or equal; #. ``une``: unordered or not equal; #. ``uno``: unordered (either nans); #. ``true``: no comparison, always returns true. *Ordered* means that neither operand is a QNAN while *unordered* means; that either operand may be a QNAN. Each of ``val1`` and ``val2`` arguments must be either a :ref:`floating-point; <t_floating>` type or a :ref:`vector <t_vector>` of floating-poin",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:463399,perform,perform,463399,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['perform']
Performance,tEquals.cpp; clang-tools-extra/clang-tidy/objc/AssertEquals.h; clang-tools-extra/clang-tidy/objc/DeallocInCategoryCheck.cpp; clang-tools-extra/clang-tidy/objc/DeallocInCategoryCheck.h; clang-tools-extra/clang-tidy/objc/ForbiddenSubclassingCheck.h; clang-tools-extra/clang-tidy/objc/MissingHashCheck.cpp; clang-tools-extra/clang-tidy/objc/MissingHashCheck.h; clang-tools-extra/clang-tidy/objc/NSInvocationArgumentLifetimeCheck.cpp; clang-tools-extra/clang-tidy/objc/NSInvocationArgumentLifetimeCheck.h; clang-tools-extra/clang-tidy/objc/PropertyDeclarationCheck.h; clang-tools-extra/clang-tidy/objc/SuperSelfCheck.cpp; clang-tools-extra/clang-tidy/objc/SuperSelfCheck.h; clang-tools-extra/clang-tidy/openmp/ExceptionEscapeCheck.cpp; clang-tools-extra/clang-tidy/openmp/ExceptionEscapeCheck.h; clang-tools-extra/clang-tidy/openmp/OpenMPTidyModule.cpp; clang-tools-extra/clang-tidy/openmp/UseDefaultNoneCheck.cpp; clang-tools-extra/clang-tidy/openmp/UseDefaultNoneCheck.h; clang-tools-extra/clang-tidy/performance/FasterStringFindCheck.cpp; clang-tools-extra/clang-tidy/performance/ForRangeCopyCheck.cpp; clang-tools-extra/clang-tidy/performance/InefficientAlgorithmCheck.cpp; clang-tools-extra/clang-tidy/performance/InefficientAlgorithmCheck.h; clang-tools-extra/clang-tidy/performance/InefficientStringConcatenationCheck.cpp; clang-tools-extra/clang-tidy/performance/InefficientStringConcatenationCheck.h; clang-tools-extra/clang-tidy/performance/MoveConstArgCheck.cpp; clang-tools-extra/clang-tidy/performance/MoveConstArgCheck.h; clang-tools-extra/clang-tidy/performance/MoveConstructorInitCheck.cpp; clang-tools-extra/clang-tidy/performance/MoveConstructorInitCheck.h; clang-tools-extra/clang-tidy/performance/NoAutomaticMoveCheck.cpp; clang-tools-extra/clang-tidy/performance/NoAutomaticMoveCheck.h; clang-tools-extra/clang-tidy/performance/NoexceptMoveConstructorCheck.cpp; clang-tools-extra/clang-tidy/performance/NoexceptMoveConstructorCheck.h; clang-tools-extra/clang-tidy/performance/NoIntTo,MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/tools/clang-formatted-files.txt:64705,perform,performance,64705,interpreter/llvm-project/clang/docs/tools/clang-formatted-files.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/tools/clang-formatted-files.txt,1,['perform'],['performance']
Performance,"tGenerateBinned).; An estimated of the error in the obtained p values is now computed in the HybridResult class thanks to Matthias Wolf. The errors can be obtained with HybridResult::CLbError(), HybridResult::CLsplusbError() or HybridResult::CLsError().; A new tutorial has been added for showing the usage of the hybrid calculator: rs505_HybridCalculator_significance.C. new class HypoTestInverter. New class for performing an hypothesis test inversion by scanning; the hypothesis test results of the HybridCalculator for; various values of the parameter of interest. An upper (or lower) limit can be derived by looking at the; confidence level curve of the result as function of the parameter of; interest, where it intersects the desired confidence level. The class implements the IntervalCalculator interface and returns an HypoTestInverterResult class. The result is a SimpleInterval, which via the method UpperLimit returns to the user the upper limit value. The HypoTestInverter implements various option for performing the scan. HypoTestInverter::RunFixedScan will scan using a fixed grid the parameter of interest. HypoTestInverter::RunAutoScan will perform an automatic scan to find optimally the curve and it will stop when the desired precision is obtained.; The confidence level value at a given point can also be done via HypoTestInverter::RunOnePoint.; The class can scan the CLs+b values (default) or alternatively CLs (if the; method HypoTestInverter::UseCLs has been called).; The estimated error due to the MC toys statistics from the HybridCalculator is propagated into the limits obtained from the HypoTestResult; A new tutorial rs801_HypoTestInverter.C has been added in the tutorials/roostats directory to show the usage of this class. New class BayesianCalculator. New class for calculating Bayesian interval using numerical integration. It implements the IntervalCalculator interface and returns as result a SimpleInterval. . The BayesianCalculator::GetInterval() method retu",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v526/index.html:15176,perform,performing,15176,roofit/doc/v526/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v526/index.html,2,['perform'],['performing']
Performance,"tLevel::None and passes which are required for register; allocation. The -opt-bisect-limit option can be used with any tool, including front ends; such as clang, that uses the core LLVM library for optimization and code; generation. The exact syntax for invoking the option is discussed below. This feature is not intended to replace other debugging tools such as bugpoint.; Rather it provides an alternate course of action when reproducing the problem; requires a complex build infrastructure that would make using bugpoint; impractical or when reproducing the failure requires a sequence of; transformations that is difficult to replicate with tools like opt and llc. Getting Started; ===============. The -opt-bisect-limit command line option can be passed directly to tools such; as opt, llc and lli. The syntax is as follows:. ::. <tool name> [other options] -opt-bisect-limit=<limit>. If a value of -1 is used the tool will perform all optimizations but a message; will be printed to stderr for each optimization that could be skipped; indicating the index value that is associated with that optimization. To skip; optimizations, pass the value of the last optimization to be performed as the; opt-bisect-limit. All optimizations with a higher index value will be skipped. In order to use the -opt-bisect-limit option with a driver that provides a; wrapper around the LLVM core library, an additional prefix option may be; required, as defined by the driver. For example, to use this option with; clang, the ""-mllvm"" prefix must be used. A typical clang invocation would look; like this:. ::. clang -O2 -mllvm -opt-bisect-limit=256 my_file.c. The -opt-bisect-limit option may also be applied to link-time optimizations by; using a prefix to indicate that this is a plug-in option for the linker. The; following syntax will set a bisect limit for LTO transformations:. ::. # When using lld, or ld64 (macOS); clang -flto -Wl,-mllvm,-opt-bisect-limit=256 my_file.o my_other_file.o; # When using Gol",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:1887,perform,perform,1887,interpreter/llvm-project/llvm/docs/OptBisect.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst,4,"['optimiz', 'perform']","['optimization', 'optimizations', 'perform']"
Performance,"t`** is used and the default ROOT interrupt handler is not; desired, you should use `GetSignalHandler()` of **`TApplication`** to; get the interrupt handler and to remove it by `RemoveSignalHandler()`of; **`TSystem`** . ## Glossary. The following glossary is adapted from the description of the Rogue Wave; `Threads.h`++ package. A **`process`** is a program that is loaded into memory and prepared for; execution. Each process has a private address space. Processes begin; with a single thread. A **`thread`** is a sequence of instructions being executed in a; program. A thread has a program counter and a private stack to keep; track of local variables and return addresses. A multithreaded process; is associated with one or more threads. Threads execute independently.; All threads in a given process share the private address space of that; process. **`Concurrency`** exists when at least two threads are in progress at; the same time. A system with only a single processor can support; concurrency by switching execution contexts among multiple threads. **`Parallelism`** arises when at least two threads are executing; simultaneously. This requires a system with multiple processors.; Parallelism implies concurrency, but not vice-versa. A function is **`reentrant`** if it will behave correctly even if a; thread of execution enters the function while one or more threads are; already executing within the function. These could be the same thread,; in the case of recursion, or different threads, in the case of; concurrency. **`Thread-specific data`** (**`TSD`**) is also known as thread-local; storage (TLS). Normally, any data that has lifetime beyond the local; variables on the thread's private stack are shared among all threads; within the process. Thread-specific data is a form of static or global; data that is maintained on a per-thread basis. That is, each thread gets; its own private copy of the data. Left to their own devices, threads execute independently.; **`Synchronizatio",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Threads.md:20304,concurren,concurrency,20304,documentation/users-guide/Threads.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Threads.md,1,['concurren'],['concurrency']
Performance,"ta read is no; older than the local load; atomic value being; acquired. atomicrmw acq_rel - workgroup - generic 1. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL, omit; lgkmcnt(0).; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load/store/; load atomic/store atomic/; atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; have; completed before; performing the; atomicrmw that is; being released. 2. flat_atomic; 3. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If not TgSplit execution; mode, omit vmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Must happen before; the following; buffer_wbinvl1_vol and; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local load; atomic value being; acquired. 3. buffer_wbinvl1_vol. - If not TgSplit execution; mode, omit.; - Ensures that; following; loads will not see; stale data. atomicrmw acq_rel - agent - global 1. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global have; completed before; performing the; atomicrmw that is; being released. 2. buffer/global_atomic; 3. s_waitcnt vmcnt(0). - Must happen before; following; buffer_wbinvl1",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:268868,load,load,268868,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"table entries, so that addresses taken outside the module will pass; any verification done inside the module. In more concrete terms, suppose we have three functions ``f``, ``g``,; ``h`` which are all of the same type, and a function foo that returns their; addresses:. .. code-block:: none. f:; mov 0, %eax; ret. g:; mov 1, %eax; ret. h:; mov 2, %eax; ret. foo:; mov f, %eax; mov g, %edx; mov h, %ecx; ret. Our jump table will (conceptually) look like this:. .. code-block:: none. f:; jmp .Ltmp0 ; 5 bytes; int3 ; 1 byte; int3 ; 1 byte; int3 ; 1 byte. g:; jmp .Ltmp1 ; 5 bytes; int3 ; 1 byte; int3 ; 1 byte; int3 ; 1 byte. h:; jmp .Ltmp2 ; 5 bytes; int3 ; 1 byte; int3 ; 1 byte; int3 ; 1 byte. .Ltmp0:; mov 0, %eax; ret. .Ltmp1:; mov 1, %eax; ret. .Ltmp2:; mov 2, %eax; ret. foo:; mov f, %eax; mov g, %edx; mov h, %ecx; ret. Because the addresses of ``f``, ``g``, ``h`` are evenly spaced at a power of; 2, and function types do not overlap (unlike class types with base classes),; we can normally apply the `Alignment`_ and `Eliminating Bit Vector Checks; for All-Ones Bit Vectors`_ optimizations thus simplifying the check at each; call site to a range and alignment check. Shared library support; ======================. **EXPERIMENTAL**. The basic CFI mode described above assumes that the application is a; monolithic binary; at least that all possible virtual/indirect call; targets and the entire class hierarchy are known at link time. The; cross-DSO mode, enabled with **-f[no-]sanitize-cfi-cross-dso** relaxes; this requirement by allowing virtual and indirect calls to cross the; DSO boundary. Assuming the following setup: the binary consists of several; instrumented and several uninstrumented DSOs. Some of them may be; dlopen-ed/dlclose-d periodically, even frequently. - Calls made from uninstrumented DSOs are not checked and just work.; - Calls inside any instrumented DSO are fully protected.; - Calls between different instrumented DSOs are also protected, with; a performance pen",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ControlFlowIntegrityDesign.rst:18519,optimiz,optimizations,18519,interpreter/llvm-project/clang/docs/ControlFlowIntegrityDesign.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ControlFlowIntegrityDesign.rst,1,['optimiz'],['optimizations']
Performance,"table is constructed for the; equivalence class of functions instead of a single function. Cross-DSO calls; ---------------; Consider two instrumented DSOs, `A` and `B`. `A` defines `f()` and `B` calls it. This case will be handled similarly to the cross-DSO scheme using the slow path callback. Non-goals; ---------. RCFI does not protect `RET` instructions:; * in non-instrumented DSOs,; * in instrumented DSOs for functions that are called from non-instrumented DSOs,; * embedded into other instructions (e.g. `0f4fc3 cmovg %ebx,%eax`). .. _SafeStack: https://clang.llvm.org/docs/SafeStack.html; .. _RFG: https://xlab.tencent.com/en/2016/11/02/return-flow-guard; .. _Intel CET: https://software.intel.com/en-us/blogs/2016/06/09/intel-release-new-technology-specifications-protect-rop-attacks. Hardware support; ================. We believe that the above design can be efficiently implemented in hardware.; A single new instruction added to an ISA would allow to perform the forward-edge CFI check; with fewer bytes per check (smaller code size overhead) and potentially more; efficiently. The current software-only instrumentation requires at least; 32-bytes per check (on x86_64).; A hardware instruction may probably be less than ~ 12 bytes.; Such instruction would check that the argument pointer is in-bounds,; and is properly aligned, and if the checks fail it will either trap (in monolithic scheme); or call the slow path function (cross-DSO scheme).; The bit vector lookup is probably too complex for a hardware implementation. .. code-block:: none. // This instruction checks that 'Ptr'; // * is aligned by (1 << kAlignment) and; // * is inside [kRangeBeg, kRangeBeg+(kRangeSize<<kAlignment)); // and if the check fails it jumps to the given target (slow path).; //; // 'Ptr' is a register, pointing to the virtual function table; // or to the function which we need to check. We may require an explicit; // fixed register to be used.; // 'kAlignment' is a 4-bit constant.; // 'kRangeSiz",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ControlFlowIntegrityDesign.rst:27972,perform,perform,27972,interpreter/llvm-project/clang/docs/ControlFlowIntegrityDesign.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ControlFlowIntegrityDesign.rst,1,['perform'],['perform']
Performance,"tadata may be attached to; ``load``/``store`` instructions referencing a single metadata with no entries.; The existence of the ``invariant.group`` metadata on the instruction tells; the optimizer that every ``load`` and ``store`` to the same pointer operand; can be assumed to load or store the same; value (but see the ``llvm.launder.invariant.group`` intrinsic which affects; when two pointers are considered the same). Pointers returned by bitcast or; getelementptr with only zero indices are considered the same. Examples:. .. code-block:: llvm. @unknownPtr = external global i8; ...; %ptr = alloca i8; store i8 42, ptr %ptr, !invariant.group !0; call void @foo(ptr %ptr). %a = load i8, ptr %ptr, !invariant.group !0 ; Can assume that value under %ptr didn't change; call void @foo(ptr %ptr). %newPtr = call ptr @getPointer(ptr %ptr); %c = load i8, ptr %newPtr, !invariant.group !0 ; Can't assume anything, because we only have information about %ptr. %unknownValue = load i8, ptr @unknownPtr; store i8 %unknownValue, ptr %ptr, !invariant.group !0 ; Can assume that %unknownValue == 42. call void @foo(ptr %ptr); %newPtr2 = call ptr @llvm.launder.invariant.group.p0(ptr %ptr); %d = load i8, ptr %newPtr2, !invariant.group !0 ; Can't step through launder.invariant.group to get value of %ptr. ...; declare void @foo(ptr); declare ptr @getPointer(ptr); declare ptr @llvm.launder.invariant.group.p0(ptr). !0 = !{}. The invariant.group metadata must be dropped when replacing one pointer by; another based on aliasing information. This is because invariant.group is tied; to the SSA value of the pointer operand. .. code-block:: llvm. %v = load i8, ptr %x, !invariant.group !0; ; if %x mustalias %y then we can replace the above instruction with; %v = load i8, ptr %y. Note that this is an experimental feature, which means that its semantics might; change in the future. '``type``' Metadata; ^^^^^^^^^^^^^^^^^^^. See :doc:`TypeMetadata`. '``associated``' Metadata; ^^^^^^^^^^^^^^^^^^^^^^^^^. The ``",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:316921,load,load,316921,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,"tainer volume, then positioning the; container itself. However, there are many practical cases when defining; such a container is not straightforward or even possible without; generating overlaps with the rest of the geometry. There are few ways; out of this:. - Defining the container for the structure as ""overlapping"" (see also; ""Overlapping Volumes""); - Representing the container as a composite shape - the Boolean union; of all components (see also ""Composite Shapes""); - Using an assembly volume - this will be described in the following. The first two approaches have the disadvantage of penalizing the; navigation performance with a factor increasing more than linear of the; number of components in the structure. The best solution is the third; one because it uses all volume-related navigation optimizations. The; class TGeoVolumeAssembly represents an assembly volume. Its shape; is represented by TGeoShapeAssembly class that is the union of all; components. It uses volume voxelization to perform navigation tasks. An assembly volume creates a hierarchical level and it geometrically; insulates the structure from the rest (as a normal volume). Physically,; a point that is INSIDE a TGeoShapeAssembly is always inside one of; the components, so a TGeoVolumeAssembly does not need to have a; medium. Due to the self-containment of assemblies, they are very; practical to use when a container is hard to define due to possible; overlaps during positioning. For instance, it is very easy creating; honeycomb structures. A very useful example for creating and using; assemblies can be found at: assembly.C. Creation of an assembly is very easy: one has just to create a; TGeoVolumeAssembly object and position the components inside as; for any volume:. ~~~{.cpp}; TGeoVolume *vol = new TGeoVolumeAssembly(name);; vol->AddNode(vdaughter1, cpy1, matrix1);; vol->AddNode(vdaughter2, cpy2, matrix2);; ~~~. Note that components cannot be declared as ""overlapping"" and that a; component can be an",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md:49801,perform,perform,49801,geom/geom/doc/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md,1,['perform'],['perform']
Performance,"taken from the fit result; Corr(a,a') = the correlation matrix from the fit result; Z = requested significance 'Z sigma band'. The linear method is fast (requires 2*N evaluations of the curve,; where N is the number of parameters), but may not be accurate in the; presence of strong correlations (~>0.9) and at Z>2 due to linear and; Gaussian approximations made. Alternatively, errors can be visualized using a sampling method. In; this method a number of curves is calculated with variations of the; parameter values, as sampled from a multi-variate Gaussian p.d.f. that; is constructed from the fit results covariance matrix. The error(x); is determined by calculating a central interval that capture N% of the; variations for each value of x, where N% is controlled by Z (i.e. Z=1; gives N=68%). The number of sampling curves is chosen to be such that; at least 100 curves are expected to be outside the N% interval. Intervals from; the sampling method can be asymmetric, and may perform better in the; presence of strong correlations, but may take (much) longer to; calculate. The sampling method also assumes that the uncertainty on the; parameters can modeled by a multi-variate Gaussian distribution. A complete example is provided in a new tutorial macro rf610_visualerror.C,; the output of which is shown below. It is also possible to visualize partial errors (from a subset of the parameters),; as shown above. Binned dataset generation. A new method RooAbsPdf::generateBinned() has been implemented; that samples binned datasets (RooDataHist) from any; p.d.f. RooDataHist* data = pdf.generateBinned(x,10000) ;. This binned generation interface samples the p.d.f. at each bin; center and applies a Poisson fluctuation to each sampled value.; The binning of the returned RooDataHist is controlled by the default; binning associated with the observables generated. To set the number; of bins in x to 200, do e.g. x.setBins(200) prior to the call; to generateBinned(). The binned dataset gener",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v524/index.html:1940,perform,perform,1940,roofit/doc/v524/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v524/index.html,2,['perform'],['perform']
Performance,"tance and; class methods, respectively). As with identifiers, selectors are represented by numeric values within the AST; file. A separate index maps these numeric selector values to the offset of the; selector within the on-disk hash table, and will be used when de-serializing an; Objective-C method declaration (or other Objective-C construct) that refers to; the selector. AST Reader Integration Points; -----------------------------. The ""lazy"" deserialization behavior of AST files requires their integration; into several completely different submodules of Clang. For example, lazily; deserializing the declarations during name lookup requires that the name-lookup; routines be able to query the AST file to find entities stored there. For each Clang data structure that requires direct interaction with the AST; reader logic, there is an abstract class that provides the interface between; the two modules. The ``ASTReader`` class, which handles the loading of an AST; file, inherits from all of these abstract classes to provide lazy; deserialization of Clang's data structures. ``ASTReader`` implements the; following abstract classes:. ``ExternalSLocEntrySource``; This abstract interface is associated with the ``SourceManager`` class, and; is used whenever the :ref:`source manager <pchinternals-sourcemgr>` needs to; load the details of a file, buffer, or macro instantiation. ``IdentifierInfoLookup``; This abstract interface is associated with the ``IdentifierTable`` class, and; is used whenever the program source refers to an identifier that has not yet; been seen. In this case, the AST reader searches for this identifier within; its :ref:`identifier table <pchinternals-ident-table>` to load any top-level; declarations or macros associated with that identifier. ``ExternalASTSource``; This abstract interface is associated with the ``ASTContext`` class, and is; used whenever the abstract syntax tree nodes need to loaded from the AST; file. It provides the ability to de-serial",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/PCHInternals.rst:22021,load,loading,22021,interpreter/llvm-project/clang/docs/PCHInternals.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/PCHInternals.rst,1,['load'],['loading']
Performance,"tance,; user's client-generated Grid credentials to make PROOF workers; capable of accessing a remote authenticated storage). Configuration files are searched for in two different locations:. - a system-wide directory: `<client_install_dir>/etc`. - user's home directory: `~/.vaf`. > A system-wide configuration file always has precedence over user's; > configuration. It is thus possible for the sysadmin to enforce a; > policy where some scripts cannot ever be overridden. Thanks to this separation, users can maintain an uncluttered directory; with very simple configuration files that contain only what really needs; or is allowed to be customized: for instance, user might specify a single line; containing the needed ROOT version, while all the technicalities to set; up the environment are taken care of inside system-installed scripts,; leaving the user's configuration directory clean and uncluttered. ### Local environment configuration. All the local environment files are loaded at the time of the; client's startup following a certain order. - `common.before`. - `local.before`. - `local.conf`. - `$VafConf_LocalPodLocation/PoD_env.sh`. - `common.after`. - `local.after`. The `common.*` files are sourced both for the local and the remote; environment. This might be convenient to avoid repeating the same; configuration in different places. Each file is looked for first in the system-wide directory and then in; the user's directory. If a configuration file does not exist, it is; silently skipped. The `$VafConf_LocalPodLocation/PoD_env.sh` environment script, provided; with each PROOF on Demand installation, *must exist*: without this file,; the VAF client won't start. ### List of VAF-specific variables. There are some special variables that need to be set in one of the above; configuration files. `$VafConf_LocalPodLocation`; : Full path to the PoD installation on the client. > The `$VafConf_LocalPodLocation` variable must be set before the; > `PoD_env.sh` script gets sourced",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/proof/doc/confman/UsingVirtualAnalysisFacility.md:2577,load,loaded,2577,proof/doc/confman/UsingVirtualAnalysisFacility.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/proof/doc/confman/UsingVirtualAnalysisFacility.md,1,['load'],['loaded']
Performance,"tarted in a file must end in that file; that is, must have its; ``#endif`` in the same file. A :token:`MacroName` may be defined externally using the ``-D`` option on the; ``*-tblgen`` command line::. llvm-tblgen self-reference.td -Dmacro1 -Dmacro3. Appendix A: Bang Operators; ==========================. Bang operators act as functions in value expressions. A bang operator takes; one or more arguments, operates on them, and produces a result. If the; operator produces a boolean result, the result value will be 1 for true or 0; for false. When an operator tests a boolean argument, it interprets 0 as false; and non-0 as true. .. warning::; The ``!getop`` and ``!setop`` bang operators are deprecated in favor of; ``!getdagop`` and ``!setdagop``. ``!add(``\ *a*\ ``,`` *b*\ ``, ...)``; This operator adds *a*, *b*, etc., and produces the sum. ``!and(``\ *a*\ ``,`` *b*\ ``, ...)``; This operator does a bitwise AND on *a*, *b*, etc., and produces the; result. A logical AND can be performed if all the arguments are either; 0 or 1. ``!cast<``\ *type*\ ``>(``\ *a*\ ``)``; This operator performs a cast on *a* and produces the result.; If *a* is not a string, then a straightforward cast is performed, say; between an ``int`` and a ``bit``, or between record types. This allows; casting a record to a class. If a record is cast to ``string``, the; record's name is produced. If *a* is a string, then it is treated as a record name and looked up in; the list of all defined records. The resulting record is expected to be of; the specified *type*. For example, if ``!cast<``\ *type*\ ``>(``\ *name*\ ``)``; appears in a multiclass definition, or in a; class instantiated inside a multiclass definition, and the *name* does not; reference any template arguments of the multiclass, then a record by; that name must have been instantiated earlier; in the source file. If *name* does reference; a template argument, then the lookup is delayed until ``defm`` statements; instantiating the multiclass (o",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:59490,perform,performed,59490,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,1,['perform'],['performed']
Performance,"tary stages of compilation. We feel that ; providing a solution to these two goals will yield an excellent solution ; to the performance problem faced by modern architectures and programming ; languages. A key insight into current compiler and runtime systems is that a ; compiler may fall in anywhere in a ""continuum of compilation"" to do its ; job. On one side, scripting languages statically compile nothing and ; dynamically compile (or equivalently, interpret) everything. On the far ; other side, traditional static compilers process everything statically and ; nothing dynamically. These approaches have typically been seen as a ; tradeoff between performance and portability. On a deeper level, however, ; there are two reasons that optimal system performance may be obtained by a; system somewhere in between these two extremes: Dynamic application ; behavior and social constraints. From a technical perspective, pure static compilation cannot ever give ; optimal performance in all cases, because applications have varying dynamic; behavior that the static compiler cannot take into consideration. Even ; compilers that support profile guided optimization generate poor code in ; the real world, because using such optimization tunes that application ; to one particular usage pattern, whereas real programs (as opposed to ; benchmarks) often have several different usage patterns. On a social level, static compilation is a very shortsighted solution to ; the performance problem. Instruction set architectures (ISAs) continuously ; evolve, and each implementation of an ISA (a processor) must choose a set ; of tradeoffs that make sense in the market context that it is designed for. ; With every new processor introduced, the vendor faces two fundamental ; problems: First, there is a lag time between when a processor is introduced ; to when compilers generate quality code for the architecture. Secondly, ; even when compilers catch up to the new architecture there is often a large ;",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt:1227,perform,performance,1227,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt,1,['perform'],['performance']
Performance,"tates-1. The mapping from a given logical; node to a state number is generally not possible, as for the node; B\_1 that appears as current node for 2 different states. The; numbering order of states is therefore not important, but it can be; used as in the following lines:. ~~~{.cpp}; gGeoManager->InitTrack(pt,dir); // anything to initialize a state; Int_t istate = gGeoManager->GetCurrentNodeId(); // in fact state Id; {; //... code changing the current state; }; gGeoManager->CdNode(istate); // forces state's re-initialization; ~~~. - Current `global transformation`. This represents the transformation; from `MARS` to the local reference of the current node, being the; product of all local mother-daughter transformations in the branch.; The global transformation can be referenced or copied:. ~~~{.cpp}; const TGeoHMatrix *global = gGeoManager->GetCurrentMatrix();; TGeoHMatrix *copy = new TGeoHMatrix(*global);; ~~~. - One often needs to perform `master-to-local` and `local-to-master`; point and vector conversions to get from `MARS` to the local node; coordinates. This can be done by using the global transformation or; directly the **`TGeoManager`** corresponding interfaces:. ~~~{.cpp}; Double_t *glob_pt = gGeoManager->GetCurrentPoint();; Double_t *glob_dir = gGeoManager->GetCurrentDirection();; Double_t loc_pt[3], loc_dir[3];; // Go from MARS to local coordinates:; gGeoManager->MasterToLocal(glob_pt,loc_pt); // or:; global->MasterToLocal(glob_pt,loc_pt); // will be omitted from now; ~~~. \anchor GP02f; ### Saving and Restoring the Current State. As we already described, saving and restoring modeller states can be; quite useful during tracking and is a feature extensively used by; external tracking engines. We will call this navigation history; management, which in most of the cases can be performed by handling the; state identifiers. For quite big geometries, state indexing is not; possible anymore and will be automatically disabled by the modeller.; Fortunately there i",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md:72115,perform,perform,72115,geom/geom/doc/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md,1,['perform'],['perform']
Performance,"tation remains however the same.; A new method TF1::operator()(double x, double y=0, double z) which is equivalent to TF1::Eval has been added for using TF1 as a callable object.; New templated methods TF1::SetFunction for generic C++ callable objects or for class member functions. TH1. Fixed a bug in the TH1::KolmogorovTest function in the case of scaled or weighted histograms. The routine has been improved and; now could also be used for comparing an histogram with a function if it is represented as an histogram with zero errors (equivalent to the case of options ""F1"" or ""F2"" in the original HDIFF routine of HBOOK). The bug has been fixed also for the TH2 and TH3 corresponding method. In addition in the case of TH3 use now all 6 axis combinations for estimating the maximum deviation. This is consistent with what is done in the 2D case.; Improved the TH1::Chi2Test for the treatment of empty bins in the histograms. If both histograms have one empty bin, the number of degree of freedom is decreased by one but the test is performed without reporting an error. If only one histogram is having an empty bin it is considered in the comparison. Fixed a bug in preserving the global statistic information after scaling, adding or rebinning the histogram. TH2. Improve TH2::FitFitSliceX and TH2::FitFitSliceY by adding the possibility to return the generated histograms in a TObjArray when the passed pointer is not null. Support also variable bin size histograms. Improve histogram projections. The implementation of TH2::ProjectionX and TH2::ProjectionY has been combined in a single private method. TH3. Fixed a couple of bugs in TH3::Project3DProfile. TProfile and TProfile2D. Add a new option ""W"" in TProfile::ProjectionX and TProfile::ProjectionXY to be able to return the equivalent weighted filled histogram. Its bin content is equal to the profile bin content multiplied by the bin entries. Implement in the TProfile a new option, ""G"" for the bin error. This option can be used, bin b",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/hist/doc/v520/index.html:1414,perform,performed,1414,hist/doc/v520/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/hist/doc/v520/index.html,2,['perform'],['performed']
Performance,"tatistic classes:; ; SimpleLikelihoodRatioTestStat : log L_1 / L_0; RatioOfProfiledLikelihoodsTestStat: log L(mu_1, hat(nu_1))/L(mu_0,hat(nu_0)); MaxLikelihoodEstimateTestStat: the MLE of a specified parameter. ToyMCSampler. New version of ToyMCSampler which can smear the nuisance; parameters according to their distributions for use with; HybridCalculator; Updated class structure: ToyMCSampler is a particular implementation of a TestStatSampler and runs with any TestStatistic. It returns the result in an instance of SamplingDistribution.; Supports Importance Sampling: Improves sampling the tails of a distribution by generating toys from a user supplied importance density and a reweighing procedure of the result.; Supports Adaptive Sampling: extends the run until a given number of toys is reached in the tail(s).; Parallelization using PROOF(-Lite) is supported. It is enabled by supplying a ProofConfig instance. BayesianCalculator. Improve the way the class performs the numerical integration to; find the interval and/or the posterior function.; In case of complex; numerical calculation add the method SetScanOfPosterior(nbins) for; scanning the posterior function in a givn number of nbins; Add possibility to compute lower/upper limits using the method; SetLeftSideTailFraction(fraction); Add possibility to compute shortest interval using; SetShortestInterval. MCMCCalculator. Various improvements including possibility to compute; lower/central/upper limits using; SetLeftSideTailFraction(fraction). New Tutorials. New Demos that take name for file, workspace, modelconfig, and data, then use the corresponding calculator tool. If the file is not specified it will read an file produced from running the HistFactory tutorial example. StandardProfileLikelihoodDemo.C: ; StandardFeldmanCousinsDemo.C: ; StandardBayesianMCMCDemo.C: ; StandardBayesianNumericalDemo.C: ; StandardProfileInspectorDemo.C: . Demonstrate some new PDFs. TestNonCentral.C: demonstrates non central chi-square; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v528/index.html:8077,perform,performs,8077,roofit/doc/v528/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v528/index.html,2,['perform'],['performs']
Performance,"tax:; """""""""""""". ::. declare i1 @llvm.type.test(ptr %ptr, metadata %type) nounwind memory(none). Arguments:; """""""""""""""""""". The first argument is a pointer to be tested. The second argument is a; metadata object representing a :doc:`type identifier <TypeMetadata>`. Overview:; """""""""""""""""". The ``llvm.type.test`` intrinsic tests whether the given pointer is associated; with the given type identifier. .. _type.checked.load:. '``llvm.type.checked.load``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare {ptr, i1} @llvm.type.checked.load(ptr %ptr, i32 %offset, metadata %type) nounwind memory(argmem: read). Arguments:; """""""""""""""""""". The first argument is a pointer from which to load a function pointer. The; second argument is the byte offset from which to load the function pointer. The; third argument is a metadata object representing a :doc:`type identifier; <TypeMetadata>`. Overview:; """""""""""""""""". The ``llvm.type.checked.load`` intrinsic safely loads a function pointer from a; virtual table pointer using type metadata. This intrinsic is used to implement; control flow integrity in conjunction with virtual call optimization. The; virtual call optimization pass will optimize away ``llvm.type.checked.load``; intrinsics associated with devirtualized calls, thereby removing the type; check in cases where it is not needed to enforce the control flow integrity; constraint. If the given pointer is associated with a type metadata identifier, this; function returns true as the second element of its return value. (Note that; the function may also return true if the given pointer is not associated; with a type metadata identifier.) If the function's return value's second; element is true, the following rules apply to the first element:. - If the given pointer is associated with the given type metadata identifier,; it is the function pointer loaded from the given byte offset from the given; pointer. - If the given pointer is not associated with the given type meta",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:938218,load,load,938218,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,2,['load'],"['load', 'loads']"
Performance,"tchCompute` backend now also supports ROOT's implicit multithreading (similar to RDataFrame), which can be enabled as follows:; ```C++; ROOT::EnableImplicitMT(nThreads);; ```. For more information, please have a look at this [contribution to the ACAT 2021 conference](https://indico.cern.ch/event/855454/contributions/4596763/) or consult the [RooBatchComupte README](https://github.com/root-project/root/tree/v6-26-00-patches/roofit/batchcompute).; The README also describes how to enable BatchMode support for your own PDFs. ### Parallel calculation of likelihood gradients during fitting; This release features two new optional RooFit libraries: `RooFit::MultiProcess` and `RooFit::TestStatistics`.; To activate both, build with `-Droofit_multiprocess=ON`. The `RooFit::TestStatistics` namespace contains a major refactoring of the `RooAbsTestStatistic`-`RooAbsOptTestStatistic`-`RooNLLVar` inheritance tree into:. 1. statistics-based classes on the one hand;; 2. calculation/evaluation/optimization based classes on the other hand. The main selling point of using `RooFit::TestStatistics` from a performance point of view is the implementation of the `RooFit::MultiProcess` based `LikelihoodGradientJob` calculator class.; To use it to perform a ""migrad"" fit (using Minuit2), one should create a `RooMinimizer` using a new constructor with a `RooAbsL` likelihood parameter as follows:. ```c++; using RooFit::TestStatistics::RooAbsL;; using RooFit::TestStatistics::buildLikelihood;. RooAbsPdf* pdf = ...; // build a pdf; RooAbsData* data = ...; // get some data. std::shared_ptr<RooAbsL> likelihood = buildLikelihood(pdf, data, [OPTIONAL ARGUMENTS]);. RooMinimizer m(likelihood);; m.migrad();; ```. The `RooMinimizer` object behaves as usual, except that behind the scenes it will now calculate each partial derivative on a separate process, ideally running on a separate CPU core.; This can be used to speed up fits with many parameters (at least as many as there are cores to parallelize over), s",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v626/index.md:16899,optimiz,optimization,16899,README/ReleaseNotes/v626/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v626/index.md,1,['optimiz'],['optimization']
Performance,"tcnt vmcnt(0)`` is required to ensure; synchronization between vector memory operations of different CUs. It; ensures a previous vector memory operation has completed before executing a; subsequent vector memory or LDS operation and so can be used to meet the; requirements of acquire and release.; * The L2 cache of one agent can be kept coherent with other agents by:; using the MTYPE RW (read-write) or MTYPE CC (cache-coherent) with the PTE; C-bit for memory local to the L2; and using the MTYPE NC (non-coherent) with; the PTE C-bit set or MTYPE UC (uncached) for memory not local to the L2. * Any local memory cache lines will be automatically invalidated by writes; from CUs associated with other L2 caches, or writes from the CPU, due to; the cache probe caused by coherent requests. Coherent requests are caused; by GPU accesses to pages with the PTE C-bit set, by CPU accesses over; XGMI, and by PCIe requests that are configured to be coherent requests.; * XGMI accesses from the CPU to local memory may be cached on the CPU.; Subsequent access from the GPU will automatically invalidate or writeback; the CPU cache due to the L2 probe filter and and the PTE C-bit being set.; * Since all work-groups on the same agent share the same L2, no L2; invalidation or writeback is required for coherence.; * To ensure coherence of local and remote memory writes of work-groups in; different agents a ``buffer_wbl2`` is required. It will writeback dirty L2; cache lines of MTYPE RW (used for local coarse grain memory) and MTYPE NC; ()used for remote coarse grain memory). Note that MTYPE CC (used for local; fine grain memory) causes write through to DRAM, and MTYPE UC (used for; remote fine grain memory) bypasses the L2, so both will never result in; dirty L2 cache lines.; * To ensure coherence of local and remote memory reads of work-groups in; different agents a ``buffer_invl2`` is required. It will invalidate L2; cache lines with MTYPE NC (used for remote coarse grain memory). Note tha",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:238319,cache,cached,238319,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['cache'],['cached']
Performance,"tcnt`` instructions when there are; no intervening memory instructions which access the corresponding address; space. The code sequences in the table indicate what can be omitted for the; OpenCL memory. The target triple environment is used to determine if the; source language is OpenCL (see :ref:`amdgpu-opencl`). ``ds/flat_load/store/atomic`` instructions to local memory are termed LDS; operations. ``buffer/global/flat_load/store/atomic`` instructions to global memory are; termed vector memory operations. Private address space uses ``buffer_load/store`` using the scratch V#; (GFX6-GFX8), or ``scratch_load/store`` (GFX9-GFX11). Since only a single thread; is accessing the memory, atomic memory orderings are not meaningful, and all; accesses are treated as non-atomic. Constant address space uses ``buffer/global_load`` instructions (or equivalent; scalar memory instructions). Since the constant address space contents do not; change during the execution of a kernel dispatch it is not legal to perform; stores, and atomic memory orderings are not meaningful, and all accesses are; treated as non-atomic. A memory synchronization scope wider than work-group is not meaningful for the; group (LDS) address space and is treated as work-group. The memory model does not support the region address space which is treated as; non-atomic. Acquire memory ordering is not meaningful on store atomic instructions and is; treated as non-atomic. Release memory ordering is not meaningful on load atomic instructions and is; treated a non-atomic. Acquire-release memory ordering is not meaningful on load or store atomic; instructions and is treated as acquire and release respectively. The memory order also adds the single thread optimization constraints defined in; table; :ref:`amdgpu-amdhsa-memory-model-single-thread-optimization-constraints-table`. .. table:: AMDHSA Memory Model Single Thread Optimization Constraints; :name: amdgpu-amdhsa-memory-model-single-thread-optimization-constraints-ta",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:203501,perform,perform,203501,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['perform'],['perform']
Performance,"tcode format. It takes a program; in LLVM bitcode format and executes it using a just-in-time compiler or an; interpreter. :program:`lli` is *not* an emulator. It will not execute IR of different architectures; and it can only interpret (or JIT-compile) for the host architecture. The JIT compiler takes the same arguments as other tools, like :program:`llc`,; but they don't necessarily work for the interpreter. If `filename` is not specified, then :program:`lli` reads the LLVM bitcode for the; program from standard input. The optional *args* specified on the command line are passed to the program as; arguments. GENERAL OPTIONS; ---------------. .. option:: -fake-argv0=executable. Override the ``argv[0]`` value passed into the executing program. .. option:: -force-interpreter={false,true}. If set to true, use the interpreter even if a just-in-time compiler is available; for this architecture. Defaults to false. .. option:: -help. Print a summary of command line options. .. option:: -load=pluginfilename. Causes :program:`lli` to load the plugin (shared object) named *pluginfilename* and use; it for optimization. .. option:: -stats. Print statistics from the code-generation passes. This is only meaningful for; the just-in-time compiler, at present. .. option:: -time-passes. Record the amount of time needed for each code-generation pass and print it to; standard error. .. option:: -version. Print out the version of :program:`lli` and exit without doing anything else. TARGET OPTIONS; --------------. .. option:: -mtriple=target triple. Override the target triple specified in the input bitcode file with the; specified string. This may result in a crash if you pick an; architecture which is not compatible with the current system. .. option:: -march=arch. Specify the architecture for which to generate assembly, overriding the target; encoded in the bitcode file. See the output of **llc -help** for a list of; valid architectures. By default this is inferred from the target trip",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/lli.rst:1272,load,load,1272,interpreter/llvm-project/llvm/docs/CommandGuide/lli.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/lli.rst,1,['load'],['load']
Performance,"te make no assumptions about the value of `vscale`.; ``""nooutline""``; This attribute indicates that outlining passes should not modify the; function. Call Site Attributes; ----------------------. In addition to function attributes the following call site only; attributes are supported:. ``vector-function-abi-variant``; This attribute can be attached to a :ref:`call <i_call>` to list; the vector functions associated to the function. Notice that the; attribute cannot be attached to a :ref:`invoke <i_invoke>` or a; :ref:`callbr <i_callbr>` instruction. The attribute consists of a; comma separated list of mangled names. The order of the list does; not imply preference (it is logically a set). The compiler is free; to pick any listed vector function of its choosing. The syntax for the mangled names is as follows:::. _ZGV<isa><mask><vlen><parameters>_<scalar_name>[(<vector_redirection>)]. When present, the attribute informs the compiler that the function; ``<scalar_name>`` has a corresponding vector variant that can be; used to perform the concurrent invocation of ``<scalar_name>`` on; vectors. The shape of the vector function is described by the; tokens between the prefix ``_ZGV`` and the ``<scalar_name>``; token. The standard name of the vector function is; ``_ZGV<isa><mask><vlen><parameters>_<scalar_name>``. When present,; the optional token ``(<vector_redirection>)`` informs the compiler; that a custom name is provided in addition to the standard one; (custom names can be provided for example via the use of ``declare; variant`` in OpenMP 5.0). The declaration of the variant must be; present in the IR Module. The signature of the vector variant is; determined by the rules of the Vector Function ABI (VFABI); specifications of the target. For Arm and X86, the VFABI can be; found at https://github.com/ARM-software/abi-aa and; https://software.intel.com/content/www/us/en/develop/download/vector-simd-function-abi.html,; respectively. For X86 and Arm targets, the values of t",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:112472,perform,perform,112472,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,2,"['concurren', 'perform']","['concurrent', 'perform']"
Performance,"te many branches. Each branch has; its own buffer in memory. In case of many branches (say more than 100),; you should adjust the buffer size accordingly. A recommended buffer size; is 32000 bytes if you have less than 50 branches. Around 16000 bytes if; you have less than 100 branches and 4000 bytes if you have more than 500; branches. These numbers are recommended for computers with memory size; ranging from 32MB to 256MB. If you have more memory, you should specify; larger buffer sizes. However, in this case, do not forget that your file; might be used on another machine with a smaller memory configuration. #### Performance Considerations when Splitting a Branch. A split branch is faster to read, but slightly slower to write. The; reading is quicker because variables of the same type are stored; consecutively and the type does not have to be read each time. It is; slower to write because of the large number of buffers as described; above. See "". Performance Benchmarks"" for performance impact of split and non-split; mode. #### Rules for Splitting. When splitting a branch, variables of different types are handled; differently. Here are the rules that apply when splitting a branch. - If a data member is a basic type, it becomes one branch of class; **`TBranchElement`**. - A data member can be an array of basic types. In this case, one; single branch is created for the array. - A data member can be a pointer to an array of basic types. The; length can vary, and must be specified in the comment field of the; data member in the class definition. See ""Input/Output"". - Pointer data member are not split, except for pointers to a; **`TClonesArray`**. The **`TClonesArray`** (pointed to) is split if; the split level is greater than two. When the split level is one,; the **`TClonesArray`** is not split. - If a data member is a pointer to an object, a special branch is; created. The branch will be filled by calling the class `Streamer`; function to serialize the object into the",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Trees.md:26777,perform,performance,26777,documentation/users-guide/Trees.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Trees.md,1,['perform'],['performance']
Performance,"te request queue per channel. Therefore, the vector and; scalar memory operations performed by wavefronts executing in different; work-groups (which may be executing on different CUs) of an agent can be; reordered relative to each other. A ``s_waitcnt vmcnt(0)`` is required to; ensure synchronization between vector memory operations of different CUs. It; ensures a previous vector memory operation has completed before executing a; subsequent vector memory or LDS operation and so can be used to meet the; requirements of acquire and release.; * The L2 cache can be kept coherent with other agents on some targets, or ranges; of virtual addresses can be set up to bypass it to ensure system coherence. Scalar memory operations are only used to access memory that is proven to not; change during the execution of the kernel dispatch. This includes constant; address space and global address space for program scope ``const`` variables.; Therefore, the kernel machine code does not have to maintain the scalar cache to; ensure it is coherent with the vector caches. The scalar and vector caches are; invalidated between kernel dispatches by CP since constant address space data; may change between kernel dispatch executions. See; :ref:`amdgpu-amdhsa-memory-spaces`. The one exception is if scalar writes are used to spill SGPR registers. In this; case the AMDGPU backend ensures the memory location used to spill is never; accessed by vector memory operations at the same time. If scalar writes are used; then a ``s_dcache_wb`` is inserted before the ``s_endpgm`` and before a function; return since the locations may be used for vector memory instructions by a; future wavefront that uses the same scratch area, or a function call that; creates a frame at the same address, respectively. There is no need for a; ``s_dcache_inv`` as all scalar writes are write-before-read in the same thread. For kernarg backing memory:. * CP invalidates the L1 cache at the start of each kernel dispatch.; * On dGPU",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:209592,cache,cache,209592,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['cache'],"['cache', 'caches']"
Performance,"te-Back and Retire Stage; """"""""""""""""""""""""""""""""""""""""""""""""""""""; Issued instructions are moved from the ReadySet to the IssuedSet. There,; instructions wait until they reach the write-back stage. At that point, they; get removed from the queue and the retire control unit is notified. When instructions are executed, the retire control unit flags the instruction as; ""ready to retire."". Instructions are retired in program order. The register file is notified of the; retirement so that it can free the physical registers that were allocated for; the instruction during the register renaming stage. Load/Store Unit and Memory Consistency Model; """"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""; To simulate an out-of-order execution of memory operations, :program:`llvm-mca`; utilizes a simulated load/store unit (LSUnit) to simulate the speculative; execution of loads and stores. Each load (or store) consumes an entry in the load (or store) queue. Users can; specify flags ``-lqueue`` and ``-squeue`` to limit the number of entries in the; load and store queues respectively. The queues are unbounded by default. The LSUnit implements a relaxed consistency model for memory loads and stores.; The rules are:. 1. A younger load is allowed to pass an older load only if there are no; intervening stores or barriers between the two loads.; 2. A younger load is allowed to pass an older store provided that the load does; not alias with the store.; 3. A younger store is not allowed to pass an older store.; 4. A younger store is not allowed to pass an older load. By default, the LSUnit optimistically assumes that loads do not alias; (`-noalias=true`) store operations. Under this assumption, younger loads are; always allowed to pass older stores. Essentially, the LSUnit does not attempt; to run any alias analysis to predict when loads and stores do not alias with; each other. Note that, in the case of write-combining memory, rule 3 could be relaxed to; allow reordering of non-aliasing store operations. Tha",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst:39625,load,load,39625,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,2,"['load', 'queue']","['load', 'queues']"
Performance,"te: xxspltib; . Similar to XXSPLTW:; def XXSPLTW : XX2Form_2<60, 164,; (outs vsrc:$XT), (ins vsrc:$XB, u2imm:$UIM),; ""xxspltw $XT, $XB, $UIM"", IIC_VecPerm, []>;. . No SDAG, intrinsic, builtin are required?. - Load/Store Vector: lxv stxv; . Has likely SDAG match:; (set v?:$XT, (load ix16addr:$src)); (set v?:$XT, (store ix16addr:$dst)). . Need define ix16addr in PPCInstrInfo.td; ix16addr: 16-byte aligned, see ""def memrix16"" in PPCInstrInfo.td. - Load/Store Vector Indexed: lxvx stxvx; . Has likely SDAG match:; (set v?:$XT, (load xoaddr:$src)); (set v?:$XT, (store xoaddr:$dst)). - Load/Store DWord: lxsd stxsd; . Similar to lxsdx/stxsdx:; def LXSDX : XX1Form<31, 588,; (outs vsfrc:$XT), (ins memrr:$src),; ""lxsdx $XT, $src"", IIC_LdStLFD,; [(set f64:$XT, (load xoaddr:$src))]>;. . (set f64:$XT, (load iaddrX4:$src)); (set f64:$XT, (store iaddrX4:$dst)). - Load/Store SP, with conversion from/to DP: lxssp stxssp; . Similar to lxsspx/stxsspx:; def LXSSPX : XX1Form<31, 524, (outs vssrc:$XT), (ins memrr:$src),; ""lxsspx $XT, $src"", IIC_LdStLFD,; [(set f32:$XT, (load xoaddr:$src))]>;. . (set f32:$XT, (load iaddrX4:$src)); (set f32:$XT, (store iaddrX4:$dst)). - Load as Integer Byte/Halfword & Zero Indexed: lxsibzx lxsihzx; . Similar to lxsiwzx:; def LXSIWZX : XX1Form<31, 12, (outs vsfrc:$XT), (ins memrr:$src),; ""lxsiwzx $XT, $src"", IIC_LdStLFD,; [(set f64:$XT, (PPClfiwzx xoaddr:$src))]>;. . (set f64:$XT, (PPClfiwzx xoaddr:$src)). - Store as Integer Byte/Halfword Indexed: stxsibx stxsihx; . Similar to stxsiwx:; def STXSIWX : XX1Form<31, 140, (outs), (ins vsfrc:$XT, memrr:$dst),; ""stxsiwx $XT, $dst"", IIC_LdStSTFD,; [(PPCstfiwx f64:$XT, xoaddr:$dst)]>;. . (PPCstfiwx f64:$XT, xoaddr:$dst). - Load Vector Halfword*8/Byte*16 Indexed: lxvh8x lxvb16x; . Similar to lxvd2x/lxvw4x:; def LXVD2X : XX1Form<31, 844,; (outs vsrc:$XT), (ins memrr:$src),; ""lxvd2x $XT, $src"", IIC_LdStLFD,; [(set v2f64:$XT, (int_ppc_vsx_lxvd2x xoaddr:$src))]>;. . (set v8i16:$XT, (int_ppc_vsx_lxvh8x xoaddr:$src)); (set v1",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt:18149,load,load,18149,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt,2,['load'],['load']
Performance,"tead a real binary. There are 4 potential solutions to the problem:. * (1) End users can resolve the issue by pointing the specified compiler executable to; the real binary instead of the symlink.; * (2) End users can invoke ``<path-to-compiler-executable>/clang++ -print-resource-dir``; to get the corresponding resource directory for your compiler and add that directory; to the include search paths manually in the build scripts.; * (3) Build systems that use a compilation database as the input for clang-scan-deps; scanner, the build system can add the flag ``--resource-dir-recipe invoke-compiler`` to; the clang-scan-deps scanner to calculate the resources directory dynamically.; The calculation happens only once for a unique ``<path-to-compiler-executable>/clang++``.; * (4) For build systems that invokes the clang-scan-deps scanner per file, repeatedly; calculating the resource directory may be inefficient. In such cases, the build; system can cache the resource directory by itself and pass ``-resource-dir <resource-dir>``; explicitly in the command line options:. .. code-block:: console. $ clang-scan-deps -format=p1689 -- <path-to-compiler-executable>/clang++ -std=c++20 -resource-dir <resource-dir> mod.cppm -c -o mod.o. Possible Questions; ==================. How modules speed up compilation; --------------------------------. A classic theory for the reason why modules speed up the compilation is:; if there are ``n`` headers and ``m`` source files and each header is included by each source file,; then the complexity of the compilation is ``O(n*m)``;; But if there are ``n`` module interfaces and ``m`` source files, the complexity of the compilation is; ``O(n+m)``. So, using modules would be a big win when scaling.; In a simpler word, we could get rid of many redundant compilations by using modules. Roughly, this theory is correct. But the problem is that it is too rough.; The behavior depends on the optimization level, as we will illustrate below. First is ``O0``. T",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/StandardCPlusPlusModules.rst:39243,cache,cache,39243,interpreter/llvm-project/clang/docs/StandardCPlusPlusModules.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/StandardCPlusPlusModules.rst,1,['cache'],['cache']
Performance,"tead of falling back to the complex; omnipotent API. I'm thinking of AST matchers vs. AST visitors as a roughly; similar situation: matchers are not omnipotent, but they're so nice. * Separation between core and checkers is usually quite strange. Once we have; shared state traits, i generally wouldn't mind having region store or range; constraint manager as checkers (though it's probably not worth it to transform; them - just a mood). The main thing to avoid here would be the situation when; the checker overwrites stuff written by the core because it thinks it has a; better idea what's going on, so the core should provide a good default behavior. * Yeah, i totally care about performance as well, and if i try to implement; approach, i'd make sure it's good. **Artem:**. > Approach (2): We could teach the Store to scan itself for bindings to; > metadata-symbolic-based regions during scanReachableSymbols() whenever; > a region turns out to be reachable. This requires no work on checker side,; > but it sounds performance-heavy. Nope, this approach is wrong. Metadata symbols may become out-of-date: when the; object changes, metadata symbols attached to it aren't changing (because symbols; simply don't change). The same metadata may have different symbols to denote its; value in different moments of time, but at most one of them represents the; actual metadata value. So we'd be escaping more stuff than necessary. If only we had ""ghost fields""; (https://lists.llvm.org/pipermail/cfe-dev/2016-May/049000.html), it would have; been much easier, because the ghost field would only contain the actual; metadata, and the Store would always know about it. This example adds to my; belief that ghost fields are exactly what we need for most C++ checkers. **Devin:**. In this case, I would be fine with some sort of; AbstractStorageMemoryRegion that meant ""here is a memory region and somewhere; reachable from here exists another region of type T"". Or even multiple regions; with different id",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/analyzer/developer-docs/InitializerLists.rst:7597,perform,performance-heavy,7597,interpreter/llvm-project/clang/docs/analyzer/developer-docs/InitializerLists.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/analyzer/developer-docs/InitializerLists.rst,1,['perform'],['performance-heavy']
Performance,"tect against an otherwise serious risk of mis-inferring an; ""array"" argument as an out-parameter. Second, it makes it much less likely; that the user will see confusing aliasing problems due to the implementation,; below, where their store to the writeback temporary is not immediately seen; in the original argument variable. A pass-by-writeback is evaluated as follows:. #. The argument is evaluated to yield a pointer ``p`` of type ``U oq *``.; #. If ``p`` is a null pointer, then a null pointer is passed as the argument,; and no further work is required for the pass-by-writeback.; #. Otherwise, a temporary of type ``T __autoreleasing`` is created and; initialized to a null pointer.; #. If the parameter is not an Objective-C method parameter marked ``out``,; then ``*p`` is read, and the result is written into the temporary with; primitive semantics.; #. The address of the temporary is passed as the argument to the actual call.; #. After the call completes, the temporary is loaded with primitive; semantics, and that value is assigned into ``*p``. .. admonition:: Rationale. This is all admittedly convoluted. In an ideal world, we would see that a; local variable is being passed to an out-parameter and retroactively modify; its type to be ``__autoreleasing`` rather than ``__strong``. This would be; remarkably difficult and not always well-founded under the C type system.; However, it was judged unacceptably invasive to require programmers to write; ``__autoreleasing`` on all the variables they intend to use for; out-parameters. This was the least bad solution. .. _arc.ownership.restrictions.records:. Ownership-qualified fields of structs and unions; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. A member of a struct or union may be declared to have ownership-qualified; type. If the type is qualified with ``__unsafe_unretained``, the semantics; of the containing aggregate are unchanged from the semantics of an unqualified type in a non-ARC mode. If the type is qualified",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst:50573,load,loaded,50573,interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,1,['load'],['loaded']
Performance,"tecture. .. option:: -mcpu=cpuname. Specify a specific chip in the current architecture to generate code for.; By default this is inferred from the target triple and autodetected to; the current architecture. For a list of available CPUs, use:; **llvm-as < /dev/null | llc -march=xyz -mcpu=help**. .. option:: -mattr=a1,+a2,-a3,... Override or control specific attributes of the target, such as whether SIMD; operations are enabled or not. The default set of attributes is set by the; current CPU. For a list of available attributes, use:; **llvm-as < /dev/null | llc -march=xyz -mattr=help**. FLOATING POINT OPTIONS; ----------------------. .. option:: -disable-excess-fp-precision. Disable optimizations that may increase floating point precision. .. option:: -enable-no-infs-fp-math. Enable optimizations that assume no Inf values. .. option:: -enable-no-nans-fp-math. Enable optimizations that assume no NAN values. .. option:: -enable-unsafe-fp-math. Causes :program:`lli` to enable optimizations that may decrease floating point; precision. .. option:: -soft-float. Causes :program:`lli` to generate software floating point library calls instead of; equivalent hardware instructions. CODE GENERATION OPTIONS; -----------------------. .. option:: -code-model=model. Choose the code model from:. .. code-block:: text. default: Target default code model; tiny: Tiny code model; small: Small code model; kernel: Kernel code model; medium: Medium code model; large: Large code model. .. option:: -disable-post-RA-scheduler. Disable scheduling after register allocation. .. option:: -disable-spill-fusing. Disable fusing of spill code into instructions. .. option:: -jit-enable-eh. Exception handling should be enabled in the just-in-time compiler. .. option:: -join-liveintervals. Coalesce copies (default=true). .. option:: -nozero-initialized-in-bss. Don't place zero-initialized symbols into the BSS section. .. option:: -pre-RA-sched=scheduler. Instruction schedulers available (before register a",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/lli.rst:3305,optimiz,optimizations,3305,interpreter/llvm-project/llvm/docs/CommandGuide/lli.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/lli.rst,1,['optimiz'],['optimizations']
Performance,"ted compiler to; generate profdata based on the training files in clang/utils/perf-training. **stage2**; Depends on stage2-instrumented-generate-profdata and will use the stage1; compiler with the stage2 profdata to build a PGO-optimized compiler. **stage2-check-llvm**; Depends on stage2 and runs check-llvm using the stage2 compiler. **stage2-check-clang**; Depends on stage2 and runs check-clang using the stage2 compiler. **stage2-check-all**; Depends on stage2 and runs check-all using the stage2 compiler. **stage2-test-suite**; Depends on stage2 and runs the test-suite using the stage2 compiler (requires; in-tree test-suite). BOLT; ====. `BOLT <https://github.com/llvm/llvm-project/blob/main/bolt/README.md>`_; (Binary Optimization and Layout Tool) is a tool that optimizes binaries; post-link by profiling them at runtime and then using that information to; optimize the layout of the final binary among other optimizations performed; at the binary level. There are also CMake caches available to build; LLVM/Clang with BOLT. To configure a single-stage build that builds LLVM/Clang and then optimizes; it with BOLT, use the following CMake configuration:. .. code-block:: console. $ cmake <path to source>/llvm -C <path to source>/clang/cmake/caches/BOLT.cmake. Then, build the BOLT-optimized binary by running the following ninja command:. .. code-block:: console. $ ninja clang-bolt. If you're seeing errors in the build process, try building with a recent; version of Clang/LLVM by setting the CMAKE_C_COMPILER and; CMAKE_CXX_COMPILER flags to the appropriate values. It is also possible to use BOLT on top of PGO and (Thin)LTO for an even more; significant runtime speedup. To configure a three stage PGO build with ThinLTO; that optimizes the resulting binary with BOLT, use the following CMake; configuration command:. .. code-block:: console. $ cmake -G Ninja <path to source>/llvm \; -C <path to source>/clang/cmake/caches/BOLT-PGO.cmake \; -DBOOTSTRAP_LLVM_ENABLE_LLD=ON \; -DBOOT",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AdvancedBuilds.rst:10163,cache,caches,10163,interpreter/llvm-project/llvm/docs/AdvancedBuilds.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AdvancedBuilds.rst,1,['cache'],['caches']
Performance,"ted projects. > 2. Design issues to consider (an initial list that we should continue; > to modify). Note that I'm not trying to suggest actual solutions here,; > but just various directions we can pursue:. Understood. :). > a. A single-assignment VM, which we've both already been thinking; > about. Yup, I think that this makes a lot of sense. I am still intrigued,; however, by the prospect of a minimally allocated VM representation... I; think that it could have definite advantages for certain applications; (think very small machines, like PDAs). I don't, however, think that our; initial implementations should focus on this. :). Here are some other auxiliary goals that I think we should consider:. 1. Primary goal: Support a high performance dynamic compilation; system. This means that we have an ""ideal"" division of labor between; the runtime and static compilers. Of course, the other goals of the; system somewhat reduce the importance of this point (f.e. portability; reduces performance, but hopefully not much); 2. Portability to different processors. Since we are most familiar with; x86 and solaris, I think that these two are excellent candidates when; we get that far...; 3. Support for all languages & styles of programming (general purpose; VM). This is the point that disallows java style bytecodes, where all; array refs are checked for bounds, etc...; 4. Support linking between different language families. For example, call; C functions directly from Java without using the nasty/slow/gross JNI; layer. This involves several subpoints:; A. Support for languages that require garbage collectors and integration; with languages that don't. As a base point, we could insist on; always using a conservative GC, but implement free as a noop, f.e. > b. A strongly-typed VM. One question is do we need the types to be; > explicitly declared or should they be inferred by the dynamic; > compiler?. B. This is kind of similar to another idea that I have: make OOP; constructs (virt",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt:3472,perform,performance,3472,interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt,1,['perform'],['performance']
Performance,"ted with a file ID is loaded only when; required by the front end, e.g., to emit a diagnostic that includes a macro; instantiation history inside the header itself. The source manager block also contains information about all of the headers; that were included when building the AST file. This includes information about; the controlling macro for the header (e.g., when the preprocessor identified; that the contents of the header dependent on a macro like; ``LLVM_CLANG_SOURCEMANAGER_H``). .. _pchinternals-preprocessor:. Preprocessor Block; ^^^^^^^^^^^^^^^^^^. The preprocessor block contains the serialized representation of the; preprocessor. Specifically, it contains all of the macros that have been; defined by the end of the header used to build the AST file, along with the; token sequences that comprise each macro. The macro definitions are only read; from the AST file when the name of the macro first occurs in the program. This; lazy loading of macro definitions is triggered by lookups into the; :ref:`identifier table <pchinternals-ident-table>`. .. _pchinternals-types:. Types Block; ^^^^^^^^^^^. The types block contains the serialized representation of all of the types; referenced in the translation unit. Each Clang type node (``PointerType``,; ``FunctionProtoType``, etc.) has a corresponding record type in the AST file.; When types are deserialized from the AST file, the data within the record is; used to reconstruct the appropriate type node using the AST context. Each type has a unique type ID, which is an integer that uniquely identifies; that type. Type ID 0 represents the NULL type, type IDs less than; ``NUM_PREDEF_TYPE_IDS`` represent predefined types (``void``, ``float``, etc.),; while other ""user-defined"" type IDs are assigned consecutively from; ``NUM_PREDEF_TYPE_IDS`` upward as the types are encountered. The AST file has; an associated mapping from the user-defined types block to the location within; the types block where the serialized representation o",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/PCHInternals.rst:11643,load,loading,11643,interpreter/llvm-project/clang/docs/PCHInternals.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/PCHInternals.rst,1,['load'],['loading']
Performance,"tegy`` and register it with the compiler:. .. code-block:: c++. // lib/MyGC/MyGC.cpp - Example LLVM GC plugin. #include ""llvm/CodeGen/GCStrategy.h""; #include ""llvm/CodeGen/GCMetadata.h""; #include ""llvm/Support/Compiler.h"". using namespace llvm;. namespace {; class LLVM_LIBRARY_VISIBILITY MyGC : public GCStrategy {; public:; MyGC() {}; };. GCRegistry::Add<MyGC>; X(""mygc"", ""My bespoke garbage collector."");; }. This boilerplate collector does nothing. More specifically:. * ``llvm.gcread`` calls are replaced with the corresponding ``load``; instruction. * ``llvm.gcwrite`` calls are replaced with the corresponding ``store``; instruction. * No safe points are added to the code. * The stack map is not compiled into the executable. Using the LLVM makefiles, this code; can be compiled as a plugin using a simple makefile:. .. code-block:: make. # lib/MyGC/Makefile. LEVEL := ../..; LIBRARYNAME = MyGC; LOADABLE_MODULE = 1. include $(LEVEL)/Makefile.common. Once the plugin is compiled, code using it may be compiled using ``llc; -load=MyGC.so`` (though MyGC.so may have some other platform-specific; extension):. ::. $ cat sample.ll; define void @f() gc ""mygc"" {; entry:; ret void; }; $ llvm-as < sample.ll | llc -load=MyGC.so. It is also possible to statically link the collector plugin into tools, such as; a language-specific compiler front-end. .. _collector-algos:. Overview of available features; ------------------------------. ``GCStrategy`` provides a range of features through which a plugin may do useful; work. Some of these are callbacks, some are algorithms that can be enabled,; disabled, or customized. This matrix summarizes the supported (and planned); features and correlates them with the collection techniques which typically; require them. .. |v| unicode:: 0x2714; :trim:. .. |x| unicode:: 0x2718; :trim:. +------------+------+--------+----------+-------+---------+-------------+----------+------------+; | Algorithm | Done | Shadow | refcount | mark- | copying | incremental ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GarbageCollection.rst:25407,load,load,25407,interpreter/llvm-project/llvm/docs/GarbageCollection.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GarbageCollection.rst,1,['load'],['load']
Performance,"tely after the link stage. The ``internalize`` pass is also; recommended to remove unused math functions from the resulting PTX. For an; input IR module ``module.bc``, the following compilation flow is recommended:. 1. Save list of external functions in ``module.bc``; 2. Link ``module.bc`` with ``libdevice.compute_XX.YY.bc``; 3. Internalize all functions not in list from (1); 4. Eliminate all unused internal functions; 5. Run ``NVVMReflect`` pass; 6. Run standard optimization pipeline. .. note::. ``linkonce`` and ``linkonce_odr`` linkage types are not suitable for the; libdevice functions. It is possible to link two IR modules that have been; linked against libdevice using different reflection variables. Since the ``NVVMReflect`` pass replaces conditionals with constants, it will; often leave behind dead code of the form:. .. code-block:: llvm. entry:; ..; br i1 true, label %foo, label %bar; foo:; ..; bar:; ; Dead code; .. Therefore, it is recommended that ``NVVMReflect`` is executed early in the; optimization pipeline before dead-code elimination. The NVPTX TargetMachine knows how to schedule ``NVVMReflect`` at the beginning; of your pass manager; just use the following code when setting up your pass; manager and the PassBuilder will use ``registerPassBuilderCallbacks`` to let; NVPTXTargetMachine::registerPassBuilderCallbacks add the pass to the; pass manager:. .. code-block:: c++. std::unique_ptr<TargetMachine> TM = ...;; PassBuilder PB(TM);; ModulePassManager MPM;; PB.parsePassPipeline(MPM, ...);. Reflection Parameters; ---------------------. The libdevice library currently uses the following reflection parameters to; control code generation:. ==================== ======================================================; Flag Description; ==================== ======================================================; ``__CUDA_FTZ=[0,1]`` Use optimized code paths that flush subnormals to zero; ==================== ======================================================.",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:9838,optimiz,optimization,9838,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,1,['optimiz'],['optimization']
Performance,"tem with other draw options (before one should clear drawings); 11. Several improvements in THttpServer user interface - repair hierarchy reload,; hide unsupported context menu entries, status line update. ## Changes in 3.4; 1. Support usage of minimized versions of .js and .css files.; Minimized scripts used by default on web servers.; 2. Implement JSROOT.extend instead of jQuery.extend, reduce; usage of jquery.js in core JSROOT classes; 3. Implement main graphics without jquery at all,; such mode used in `nobrowser` mode.; 4. Provide optional latex drawing with MathJax SVG.; TMathText always drawn with MathJax,; other classes require `mathjax` option in URL; 5. Improve drawing of different text classes, correctly handle; their alignment and scaling, special handling for IE; 6. Fix error with time axes - time offset was not correctly interpreted. ## Changes in 3.3; 1. Use d3.time.scale for display of time scales; 2. Within JSRootCore.js script URL one could specify JSROOT; functionality to be loaded: '2d', '3d', 'io', 'load', 'onload'.; Old method with JSROOT.AssertPrerequisites will also work.; 3. With THttpServer JSROOT now provides simple control functionality.; One could publish commands and execute them from the browser; 4. One could open several ROOT files simultaneously; 5. Add 'simple' layout - drawing uses full space on the right side; 6. Allow to open ROOT files in online session (via url parameter); 7. One could monitor simultaneously objects from server and root files; 8. Implement 'autocol' draw option - when superimposing histograms,; their line colors will be automatically assigned; 9. Implement 'nostat' draw option - disabled stat drawing; 10. Using '_same_' identifier in item name, one can easily draw or superimpose; similar items from different files. Could be used in URL like:; `...&files=[file1.root,file2.root]&items=[file1.root/hpx, file2.root/_same_]`; `...&files=[file1.root,file2.root]&item=file1.root/hpx+file2.root/_same_`; Main limitation - ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/js/changes.md:67070,load,loaded,67070,js/changes.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/js/changes.md,2,['load'],"['load', 'loaded']"
Performance,"template pattern parameterized over the load upper immediate; instruction, add operation, the zero register, and register class.; Here the instantiation of MipsHiLoRelocs in MipsInstrInfo.td is used; to MIPS32 to compute addresses for the static relocation model. // lib/Target/Mips/MipsInstrInfo.td; multiclass MipsHiLoRelocs<Instruction Lui, Instruction Addiu,; Register ZeroReg, RegisterOperand GPROpnd> {; def : MipsPat<(MipsHi tglobaladdr:$in), (Lui tglobaladdr:$in)>;; ...; def : MipsPat<(MipsLo tglobaladdr:$in), (Addiu ZeroReg, tglobaladdr:$in)>;; ...; def : MipsPat<(add GPROpnd:$hi, (MipsLo tglobaladdr:$lo)),; (Addiu GPROpnd:$hi, tglobaladdr:$lo)>;; ...; }; defm : MipsHiLoRelocs<LUi, ADDiu, ZERO, GPR32Opnd>;. // lib/Target/Mips/Mips64InstrInfo.td; defm : MipsHiLoRelocs<LUi64, DADDiu, ZERO_64, GPR64Opnd>, SYM_32;. The instantiation in Mips64InstrInfo.td is used for MIPS64 in ILP32; mode, as guarded by the predicate ""SYM_32"" and also for a submode of; LP64 where symbols are assumed to be 32 bits wide. More details on how multiclasses in TableGen work can be found in the; section ""Multiclass definitions and instances"" in the document; ""TableGen Language Introduction"". 4. Instruction definitions are multiply defined to cover the different; register classes. In some cases, such as LW/LW64, this also accounts; for the difference in the results of instruction execution. On MIPS32,; ""lw"" loads a 32 bit value from memory. On MIPS64, ""lw"" loads a 32 bit; value from memory and sign extends the value to 64 bits. // lib/Target/Mips/MipsInstrInfo.td; def LUi : MMRel, LoadUpper<""lui"", GPR32Opnd, uimm16_relaxed>, LUI_FM;; // lib/Target/Mips/Mips64InstrInfo.td; def LUi64 : LoadUpper<""lui"", GPR64Opnd, uimm16_64_relaxed>, LUI_FM;. defines two names ""LUi"" and ""LUi64"" with two different register; classes, but with the same encoding---""LUI_FM"". These instructions load a; 16-bit immediate into bits 31-16 and clear the lower 15 bits. On MIPS64,; the result is sign-extended to 64 bits.; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt:3293,load,loads,3293,interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt,8,"['Load', 'load']","['LoadUpper', 'load', 'loads']"
Performance,"tempting to; reduce test programs. If you're trying to find a bug in one of these passes,; **bugpoint** may crash. **--enable-valgrind**. Use valgrind to find faults in the optimization phase. This will allow; bugpoint to find otherwise asymptomatic problems caused by memory; mis-management. **-find-bugs**. Continually randomize the specified passes and run them on the test program; until a bug is found or the user kills **bugpoint**. **-help**. Print a summary of command line options. **--input** *filename*. Open *filename* and redirect the standard input of the test program, whenever; it runs, to come from that file. **--load** *plugin*. Load the dynamic object *plugin* into **bugpoint** itself. This object should; register new optimization passes. Once loaded, the object will add new command; line options to enable various optimizations. To see the new complete list of; optimizations, use the **-help** and **--load** options together; for example:. .. code-block:: bash. bugpoint --load myNewPass.so -help. **--mlimit** *megabytes*. Specifies an upper limit on memory usage of the optimization and codegen. Set; to zero to disable the limit. **--output** *filename*. Whenever the test program produces output on its standard output stream, it; should match the contents of *filename* (the ""reference output""). If you; do not use this option, **bugpoint** will attempt to generate a reference output; by compiling the program with the ""safe"" backend and running it. **--run-{int,jit,llc,custom}**. Whenever the test program is compiled, **bugpoint** should generate code for it; using the specified code generator. These options allow you to choose the; interpreter, the JIT compiler, the static native code compiler, or a; custom command (see **--exec-command**) respectively. **--safe-{llc,custom}**. When debugging a code generator, **bugpoint** should use the specified code; generator as the ""safe"" code generator. This is a known-good code generator; used to generate the ""refere",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/bugpoint.rst:3664,load,load,3664,interpreter/llvm-project/llvm/docs/CommandGuide/bugpoint.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/bugpoint.rst,1,['load'],['load']
Performance,"ten in the previous format anymore.**; - Support has been added for several new field types: `std::unordered_set<T>`, `std::map<K,V>`, `std::unordered_map<K,V>`; - Support has been added for on-disk half-precision (IEEE 754-2008 16-bit) float fields. This can be enabled through `RField<float>::SetHalfPrecision()`. On reading, values of such fields are represented as regular, 32-bit floats.; - A new `RNTupleInspector` utility class has been added, to provide information about the on-disk metadata of an RNTuple.; - A new `RNTupleParallelWriter` class has been added, providing (initial) support for parallel writing of RNTuples.; - A new static method `RFieldBase::Check()` has been added, which produces a support status report of a type with regards to RNTuple I/O.; - A new internal `RNTupleMerger` class has been added, enabling the merging of different page sources into one page sink. This also means that RNTuples can be merged through `hadd`.; - Zero-copy bulk reading has been added, with extra optimizations for `ROOT::RVec` fields.; - It is now possible to use the `RNTupleView` with an external address with type erasure, e.g.:; ```cpp; std::shared_ptr<void> data{new float()};; auto view = reader->GetView(""pt"", data);; ```; This enables use cases such as reading one specific entry of one specific field into a previously allocated memory location.; - Further integration with [RDataFrame](#rdataframe): it is now possible to create RDataFrame for chains of RNTuples. This addition also comes with improvements to the multi-threaded work scheduling.; - Many additional bug fixes and improvements. Please, report any issues regarding the above mentioned features should you encounter them. RNTuple is still in pre-production. The on-disk format is scheduled to be finalized by the end of 2024. Thus, we appreciate feedback and suggestions for improvement. ## Histogram Libraries. - Implement the FLT_MAX mechanism for `THStack::GetMaximum()` and `THStack::GetMiniumum()`.; - Print a ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v632/index.md:5734,optimiz,optimizations,5734,README/ReleaseNotes/v632/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v632/index.md,1,['optimiz'],['optimizations']
Performance,"tentative scratch V#, then its register numbers; are shifted to the first four-aligned SGPR index after the highest one; allocated by the register allocator, and all uses are updated. The; register count includes them in the shifted location.; - In either case, if the processor has the SGPR allocation bug, the; tentative allocation is not shifted or unreserved in order to ensure; the register count is higher to workaround the bug. .. note::. This approach of using a tentative scratch V# and shifting the register; numbers if used avoids having to perform register allocation a second; time if the tentative V# is eliminated. This is more efficient and; avoids the problem that the second register allocation may perform; spilling which will fail as there is no longer a scratch V#. When the kernel prolog code is being emitted it is known whether the scratch V#; described above is actually used. If it is, the prolog code must set it up by; copying the Private Segment Buffer to the scratch V# registers and then adding; the Private Segment Wavefront Offset to the queue base address in the V#. The; result is a V# with a base address pointing to the beginning of the wavefront; scratch backing memory. The Private Segment Buffer is always requested, but the Private Segment; Wavefront Offset is only requested if it is used (see; :ref:`amdgpu-amdhsa-initial-kernel-execution-state`). .. _amdgpu-amdhsa-memory-model:. Memory Model; ~~~~~~~~~~~~. This section describes the mapping of the LLVM memory model onto AMDGPU machine; code (see :ref:`memmodel`). The AMDGPU backend supports the memory synchronization scopes specified in; :ref:`amdgpu-memory-scopes`. The code sequences used to implement the memory model specify the order of; instructions that a single thread must execute. The ``s_waitcnt`` and cache; management instructions such as ``buffer_wbinvl1_vol`` are defined with respect; to other memory instructions executed by the same thread. This allows them to be; moved earlier or la",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:200437,queue,queue,200437,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['queue'],['queue']
Performance,"ter value to; be loaded as any other type or vice versa. In practice, this restriction is; well isolated to isSafeToSpeculate in ValueTracking.cpp. Explicit Representation; ^^^^^^^^^^^^^^^^^^^^^^^. A frontend could directly generate this low level explicit form, but; doing so may inhibit optimization. Instead, it is recommended that; compilers with relocating collectors target the abstract machine model just; described. The heart of the explicit approach is to construct (or rewrite) the IR in a; manner where the possible updates performed by the garbage collector are; explicitly visible in the IR. Doing so requires that we:. #. create a new SSA value for each potentially relocated pointer, and; ensure that no uses of the original (non relocated) value is; reachable after the safepoint,; #. specify the relocation in a way which is opaque to the compiler to; ensure that the optimizer can not introduce new uses of an; unrelocated value after a statepoint. This prevents the optimizer; from performing unsound optimizations.; #. recording a mapping of live pointers (and the allocation they're; associated with) for each statepoint. At the most abstract level, inserting a safepoint can be thought of as; replacing a call instruction with a call to a multiple return value; function which both calls the original target of the call, returns; its result, and returns updated values for any live pointers to; garbage collected objects. Note that the task of identifying all live pointers to garbage; collected values, transforming the IR to expose a pointer giving the; base object for every such live pointer, and inserting all the; intrinsics correctly is explicitly out of scope for this document.; The recommended approach is to use the :ref:`utility passes; <statepoint-utilities>` described below. This abstract function call is concretely represented by a sequence of; intrinsic calls known collectively as a ""statepoint relocation sequence"". Let's consider a simple call in LLVM IR:. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Statepoints.rst:6993,optimiz,optimizer,6993,interpreter/llvm-project/llvm/docs/Statepoints.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Statepoints.rst,3,"['optimiz', 'perform']","['optimizations', 'optimizer', 'performing']"
Performance,"teration through the elements in sorted order.; Set-like containers are more expensive than sequential containers. * a :ref:`sequential <ds_sequential>` container provides the most efficient way; to add elements and keeps track of the order they are added to the collection.; They permit duplicates and support efficient iteration, but do not support; efficient look-up based on a key. * a :ref:`string <ds_string>` container is a specialized sequential container or; reference structure that is used for character or byte arrays. * a :ref:`bit <ds_bit>` container provides an efficient way to store and; perform set operations on sets of numeric id's, while automatically; eliminating duplicates. Bit containers require a maximum of 1 bit for each; identifier you want to store. Once the proper category of container is determined, you can fine tune the; memory use, constant factors, and cache behaviors of access by intelligently; picking a member of the category. Note that constant factors and cache behavior; can be a big deal. If you have a vector that usually only contains a few; elements (but could contain many), for example, it's much better to use; :ref:`SmallVector <dss_smallvector>` than :ref:`vector <dss_vector>`. Doing so; avoids (relatively) expensive malloc/free calls, which dwarf the cost of adding; the elements to the container. .. _ds_sequential:. Sequential Containers (std::vector, std::list, etc); ---------------------------------------------------. There are a variety of sequential containers available for you, based on your; needs. Pick the first in this section that will do what you want. .. _dss_arrayref:. llvm/ADT/ArrayRef.h; ^^^^^^^^^^^^^^^^^^^. The ``llvm::ArrayRef`` class is the preferred class to use in an interface that; accepts a sequential list of elements in memory and just reads from them. By; taking an ``ArrayRef``, the API can be passed a fixed size array, an; ``std::vector``, an ``llvm::SmallVector`` and anything else that is contiguous; in me",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ProgrammersManual.rst:57044,cache,cache,57044,interpreter/llvm-project/llvm/docs/ProgrammersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ProgrammersManual.rst,1,['cache'],['cache']
Performance,"termed the; fence-paired-atomic).; - Must happen before; the following; buffer_wbinvl1_vol.; - Ensures that the; fence-paired atomic; has completed; before invalidating; the; cache. Therefore; any following; locations read must; be no older than; the value read by; the; fence-paired-atomic. 2. buffer_wbinvl1_vol. - Must happen before any; following global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. **Release Atomic**; ------------------------------------------------------------------------------------; store atomic release - singlethread - global 1. buffer/global/ds/flat_store; - wavefront - local; - generic; store atomic release - workgroup - global 1. s_waitcnt lgkmcnt(0); - generic; - If OpenCL, omit.; - Must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; store.; - Ensures that all; memory operations; to local have; completed before; performing the; store that is being; released. 2. buffer/global/flat_store; store atomic release - workgroup - local 1. ds_store; store atomic release - agent - global 1. s_waitcnt lgkmcnt(0) &; - system - generic vmcnt(0). - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; store.; - Ensures that all; memory operations; to memory have; completed before; performing the; store that is being; released. 2. buffer/global/flat_store; atomicrmw release - singlethread - global 1. buffer/global/ds/flat_atomic; - wavefront - local; - ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:219428,perform,performing,219428,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['perform'],['performing']
Performance,"ternal global i32; ...; %idx1 = getelementptr i32, ptr @MyVar, i64 0; %idx2 = getelementptr i32, ptr @MyVar, i64 1; %idx3 = getelementptr i32, ptr @MyVar, i64 2. These GEP instructions are simply making address computations from the base; address of ``MyVar``. They compute, as follows (using C syntax):. .. code-block:: c++. idx1 = (char*) &MyVar + 0; idx2 = (char*) &MyVar + 4; idx3 = (char*) &MyVar + 8. Since the type ``i32`` is known to be four bytes long, the indices 0, 1 and 2; translate into memory offsets of 0, 4, and 8, respectively. No memory is; accessed to make these computations because the address of ``@MyVar`` is passed; directly to the GEP instructions. The obtuse part of this example is in the cases of ``%idx2`` and ``%idx3``. They; result in the computation of addresses that point to memory past the end of the; ``@MyVar`` global, which is only one ``i32`` long, not three ``i32``\s long.; While this is legal in LLVM, it is inadvisable because any load or store with; the pointer that results from these GEP instructions would trigger undefined; behavior (UB). Why is the extra 0 index required?; ----------------------------------. Quick answer: there are no superfluous indices. This question arises most often when the GEP instruction is applied to a global; variable which is always a pointer type. For example, consider this:. .. code-block:: text. %MyStruct = external global { ptr, i32 }; ...; %idx = getelementptr { ptr, i32 }, ptr %MyStruct, i64 0, i32 1. The GEP above yields a ``ptr`` by indexing the ``i32`` typed field of the; structure ``%MyStruct``. When people first look at it, they wonder why the ``i64; 0`` index is needed. However, a closer inspection of how globals and GEPs work; reveals the need. Becoming aware of the following facts will dispel the; confusion:. #. The type of ``%MyStruct`` is *not* ``{ ptr, i32 }`` but rather ``ptr``.; That is, ``%MyStruct`` is a pointer (to a structure), not a structure itself. #. Point #1 is evidenced by notic",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GetElementPtr.rst:4760,load,load,4760,interpreter/llvm-project/llvm/docs/GetElementPtr.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GetElementPtr.rst,1,['load'],['load']
Performance,"ters. The main design goals for the; PROOF system are:. *Transparency* : there should be as little difference as possible; between a local ROOT based analysis session and a remote parallel PROOF; session, both being interactive and giving the same results. *Scalability* : the basic architecture should not put any implicit; limitations on the number of computers that can be used in parallel. *Adaptability* : the system should be able to adapt itself to variations; in the remote environment (changing load on the cluster nodes, network; interruptions, etc.). Being an extension of the ROOT system, PROOF is designed to work on; objects in ROOT data stores, though, for the time being, it mainly; addresses the case of **`TTree`** based object collections. PROOF is primarily meant as an interactive alternative to batch systems; for Central Analysis Facilities and departmental workgroups (Tier-2's).; However, thanks to a multi-tier architecture allowing multiple levels of; masters, it can be easily adapted to wide range virtual clusters; distributed over geographically separated domains and heterogeneous; machines (GRIDs). While pure interactivity might not always be possible when performing a; complicated analysis on a very large data set, PROOF still tries to give; the user the interactive experience with something we call ""interactive; batch"". With ""interactive batch"" the user can start very long running; queries, disconnect the client and at any time, any location and from; any computer reconnect to the query to monitor its progress or retrieve; the results. This feature gives it a distinct advantage over purely; batch based solutions, that only provide an answer once all sub-jobs; have been finished. ![The Multi-tier structure of a PROOF cluster](pictures/03000200.png). Details about the PROOF system and the way to use it can be found at; <PROOFWiki> [^1]. The PROOF development is a joint effort between CERN and MIT. [^1]: http://root.cern.ch/twiki/bin/view/ROOT/PROOF; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/PROOF.md:1443,perform,performing,1443,documentation/users-guide/PROOF.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/PROOF.md,1,['perform'],['performing']
Performance,"tes are used to spill SGPR registers. In this; case the AMDGPU backend ensures the memory location used to spill is never; accessed by vector memory operations at the same time. If scalar writes are used; then a ``s_dcache_wb`` is inserted before the ``s_endpgm`` and before a function; return since the locations may be used for vector memory instructions by a; future wavefront that uses the same scratch area, or a function call that; creates a frame at the same address, respectively. There is no need for a; ``s_dcache_inv`` as all scalar writes are write-before-read in the same thread. For kernarg backing memory:. * CP invalidates the L1 cache at the start of each kernel dispatch.; * On dGPU the kernarg backing memory is allocated in host memory accessed as; MTYPE UC (uncached) to avoid needing to invalidate the L2 cache. This also; causes it to be treated as non-volatile and so is not invalidated by; ``*_vol``.; * On APU the kernarg backing memory it is accessed as MTYPE CC (cache coherent); and so the L2 cache will be coherent with the CPU and other agents. Scratch backing memory (which is used for the private address space) is accessed; with MTYPE NC_NV (non-coherent non-volatile). Since the private address space is; only accessed by a single thread, and is always write-before-read, there is; never a need to invalidate these entries from the L1 cache. Hence all cache; invalidates are done as ``*_vol`` to only invalidate the volatile cache lines. The code sequences used to implement the memory model for GFX6-GFX9 are defined; in table :ref:`amdgpu-amdhsa-memory-model-code-sequences-gfx6-gfx9-table`. .. table:: AMDHSA Memory Model Code Sequences GFX6-GFX9; :name: amdgpu-amdhsa-memory-model-code-sequences-gfx6-gfx9-table. ============ ============ ============== ========== ================================; LLVM Instr LLVM Memory LLVM Memory AMDGPU AMDGPU Machine Code; Ordering Sync Scope Address GFX6-GFX9; Space; ============ ============ ============== ========== =",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:210874,cache,cache,210874,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['cache'],['cache']
Performance,"tes, such as sret, byval, inreg,; returned, and inalloca, must match.; - The caller and callee prototypes must match. Pointer types of parameters; or return types may differ in pointee type, but not in address space. On the other hand, if the calling convention is `swifttailcc` or `swiftcc`:. - Only these ABI-impacting attributes attributes are allowed: sret, byval,; swiftself, and swiftasync.; - Prototypes are not required to match. Tail call optimization for calls marked ``tail`` is guaranteed to occur if; the following conditions are met:. - Caller and callee both have the calling convention ``fastcc`` or ``tailcc``.; - The call is in tail position (ret immediately follows call and ret; uses value of call or is void).; - Option ``-tailcallopt`` is enabled,; ``llvm::GuaranteedTailCallOpt`` is ``true``, or the calling convention; is ``tailcc``; - `Platform-specific constraints are; met. <CodeGenerator.html#tailcallopt>`_. #. The optional ``notail`` marker indicates that the optimizers should not add; ``tail`` or ``musttail`` markers to the call. It is used to prevent tail; call optimization from being performed on the call. #. The optional ``fast-math flags`` marker indicates that the call has one or more; :ref:`fast-math flags <fastmath>`, which are optimization hints to enable; otherwise unsafe floating-point optimizations. Fast-math flags are only valid; for calls that return a floating-point scalar or vector type, or an array; (nested to any depth) of floating-point scalar or vector types. #. The optional ""cconv"" marker indicates which :ref:`calling; convention <callingconv>` the call should use. If none is; specified, the call defaults to using C calling conventions. The; calling convention of the call must match the calling convention of; the target function, or else the behavior is undefined.; #. The optional :ref:`Parameter Attributes <paramattrs>` list for return; values. Only '``zeroext``', '``signext``', and '``inreg``' attributes; are valid here.; #. The",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:475484,optimiz,optimizers,475484,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['optimiz'],['optimizers']
Performance,"tes-1. The mapping from a given logical; node to a state number is generally not possible, as for the node; B\_1 that appears as current node for 2 different states. The; numbering order of states is therefore not important, but it can be; used as in the following lines:. ``` {.cpp}; gGeoManager->InitTrack(pt,dir); // anything to initialize a state; Int_t istate = gGeoManager->GetCurrentNodeId(); // in fact state Id; {; //... code changing the current state; }; gGeoManager->CdNode(istate); // forces state's re-initialization; ```. - Current `global transformation`. This represents the transformation; from `MARS` to the local reference of the current node, being the; product of all local mother-daughter transformations in the branch.; The global transformation can be referenced or copied:. ``` {.cpp}; const TGeoHMatrix *global = gGeoManager->GetCurrentMatrix();; TGeoHMatrix *copy = new TGeoHMatrix(*global);; ```. - One often needs to perform `master-to-local` and `local-to-master`; point and vector conversions to get from `MARS` to the local node; coordinates. This can be done by using the global transformation or; directly the **`TGeoManager`** corresponding interfaces:. ``` {.cpp}; Double_t *glob_pt = gGeoManager->GetCurrentPoint();; Double_t *glob_dir = gGeoManager->GetCurrentDirection();; Double_t loc_pt[3], loc_dir[3];; // Go from MARS to local coordinates:; gGeoManager->MasterToLocal(glob_pt,loc_pt); // or:; global->MasterToLocal(glob_pt,loc_pt); // will be omitted from now; ```. ### Saving and Restoring the Current State. As we already described, saving and restoring modeller states can be; quite useful during tracking and is a feature extensively used by; external tracking engines. We will call this navigation history; management, which in most of the cases can be performed by handling the; state identifiers. For quite big geometries, state indexing is not; possible anymore and will be automatically disabled by the modeller.; Fortunately there is a backup sol",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md:111714,perform,perform,111714,documentation/users-guide/Geometry.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md,1,['perform'],['perform']
Performance,"teseer.ist.psu.edu/debus04linktime.html. //===---------------------------------------------------------------------===//. gcc generates smaller code for this function at -O2 or -Os:. void foo(signed char* p) {; if (*p == 3); bar();; else if (*p == 4); baz();; else if (*p == 5); quux();; }. llvm decides it's a good idea to turn the repeated if...else into a; binary tree, as if it were a switch; the resulting code requires -1; compare-and-branches when *p<=2 or *p==5, the same number if *p==4; or *p>6, and +1 if *p==3. So it should be a speed win; (on balance). However, the revised code is larger, with 4 conditional; branches instead of 3. More seriously, there is a byte->word extend before; each comparison, where there should be only one, and the condition codes; are not remembered when the same two values are compared twice. //===---------------------------------------------------------------------===//. More LSR enhancements possible:. 1. Teach LSR about pre- and post- indexed ops to allow iv increment be merged; in a load / store.; 2. Allow iv reuse even when a type conversion is required. For example, i8; and i32 load / store addressing modes are identical. //===---------------------------------------------------------------------===//. This:. int foo(int a, int b, int c, int d) {; long long acc = (long long)a * (long long)b;; acc += (long long)c * (long long)d;; return (int)(acc >> 32);; }. Should compile to use SMLAL (Signed Multiply Accumulate Long) which multiplies; two signed 32-bit values to produce a 64-bit value, and accumulates this with; a 64-bit value. We currently get this with both v4 and v6:. _foo:; smull r1, r0, r1, r0; smull r3, r2, r3, r2; adds r3, r3, r1; adc r0, r2, r0; bx lr. //===---------------------------------------------------------------------===//. This:; #include <algorithm>; std::pair<unsigned, bool> full_add(unsigned a, unsigned b); { return std::make_pair(a + b, a + b < a); }; bool no_overflow(unsigned a, unsigned b); { return !full_",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:12524,load,load,12524,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,2,['load'],['load']
Performance,"test {; NSString *string = NSLocalizedString(@""LocalizedString"", nil); // warn; NSString *string2 = NSLocalizedString(@""LocalizedString"", @"" ""); // warn; NSString *string3 = NSLocalizedStringWithDefaultValue(; @""LocalizedString"", nil, [[NSBundle alloc] init], nil,@""""); // warn; }. .. _optin-osx-cocoa-localizability-NonLocalizedStringChecker:. optin.osx.cocoa.localizability.NonLocalizedStringChecker (ObjC); """"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""; Warns about uses of non-localized NSStrings passed to UI methods expecting localized NSStrings. .. code-block:: objc. NSString *alarmText =; NSLocalizedString(@""Enabled"", @""Indicates alarm is turned on"");; if (!isEnabled) {; alarmText = @""Disabled"";; }; UILabel *alarmStateLabel = [[UILabel alloc] init];. // Warning: User-facing text should use localized string macro; [alarmStateLabel setText:alarmText];. .. _optin-performance-GCDAntipattern:. optin.performance.GCDAntipattern; """"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""; Check for performance anti-patterns when using Grand Central Dispatch. .. _optin-performance-Padding:. optin.performance.Padding; """"""""""""""""""""""""""""""""""""""""""""""""""; Check for excessively padded structs. .. _optin-portability-UnixAPI:. optin.portability.UnixAPI; """"""""""""""""""""""""""""""""""""""""""""""""""; Finds implementation-defined behavior in UNIX/Posix functions. .. _security-checkers:. security; ^^^^^^^^. Security related checkers. .. _security-cert-env-InvalidPtr:. security.cert.env.InvalidPtr; """""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""". Corresponds to SEI CERT Rules `ENV31-C <https://wiki.sei.cmu.edu/confluence/display/c/ENV31-C.+Do+not+rely+on+an+environment+pointer+following+an+operation+that+may+invalidate+it>`_ and `ENV34-C <https://wiki.sei.cmu.edu/confluence/display/c/ENV34-C.+Do+not+store+pointers+returned+by+certain+functions>`_. * **ENV31-C**:; Rule is about the possible problem with ``main`` function's third argument, environment pointer,; ""envp"". When environment array is modified using some modification functi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/analyzer/checkers.rst:19413,perform,performance,19413,interpreter/llvm-project/clang/docs/analyzer/checkers.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/analyzer/checkers.rst,1,['perform'],['performance']
Performance,"test-suite Guide; ================. Quickstart; ----------. 1. The lit test runner is required to run the tests. You can either use one; from an LLVM build:. ```bash; % <path to llvm build>/bin/llvm-lit --version; lit 0.8.0dev; ```. An alternative is installing it as a python package in a python virtual; environment:. ```bash; % mkdir venv; % virtualenv venv; % . venv/bin/activate; % pip install svn+https://llvm.org/svn/llvm-project/llvm/trunk/utils/lit; % lit --version; lit 0.8.0dev; ```. 2. Check out the `test-suite` module with:. ```bash; % git clone https://github.com/llvm/llvm-test-suite.git test-suite; ```. 3. Create a build directory and use CMake to configure the suite. Use the; `CMAKE_C_COMPILER` option to specify the compiler to test. Use a cache file; to choose a typical build configuration:. ```bash; % mkdir test-suite-build; % cd test-suite-build; % cmake -DCMAKE_C_COMPILER=<path to llvm build>/bin/clang \; -C../test-suite/cmake/caches/O3.cmake \; ../test-suite; ```. **NOTE!** if you are using your built clang, and you want to build and run the; MicroBenchmarks/XRay microbenchmarks, you need to add `compiler-rt` to your; `LLVM_ENABLE_RUNTIMES` cmake flag. 4. Build the benchmarks:. ```text; % make; Scanning dependencies of target timeit-target; [ 0%] Building C object tools/CMakeFiles/timeit-target.dir/timeit.c.o; [ 0%] Linking C executable timeit-target; ...; ```. 5. Run the tests with lit:. ```text; % llvm-lit -v -j 1 -o results.json .; -- Testing: 474 tests, 1 threads --; PASS: test-suite :: MultiSource/Applications/ALAC/decode/alacconvert-decode.test (1 of 474); ********** TEST 'test-suite :: MultiSource/Applications/ALAC/decode/alacconvert-decode.test' RESULTS **********; compile_time: 0.2192; exec_time: 0.0462; hash: ""59620e187c6ac38b36382685ccd2b63b""; size: 83348; **********; PASS: test-suite :: MultiSource/Applications/ALAC/encode/alacconvert-encode.test (2 of 474); ...; ```. 6. Show and compare result files (optional):. ```bash; # Make sure panda",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TestSuiteGuide.md:761,cache,cache,761,interpreter/llvm-project/llvm/docs/TestSuiteGuide.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TestSuiteGuide.md,2,['cache'],"['cache', 'caches']"
Performance,"tforward. Precompiled header; ------------------. For performance reasons (reduced memory and CPU usage), a precompiled header; (PCH) of the system and compiler header files will be installed or, failing; that, generated on startup.; Obviously, this PCH is not portable and should not be part of any wheel. Some compiler features, such as AVX, OpenMP, fast math, etc. need to be; active during compilation of the PCH, as they depend both on compiler flags; and system headers (for intrinsics, or API calls).; You can control compiler flags through the ``EXTRA_CLING_ARGS`` envar and thus; what is active in the PCH.; In principle, you can also change the C++ language standard by setting the; appropriate flag on ``EXTRA_CLING_ARGS`` and rebuilding the PCH.; However, if done at this stage, that disables some automatic conversion for; C++ types that were introduced after C++11 (such as ``string_view`` and; ``optional``). If you want multiple PCHs living side-by-side, you can generate them; yourself (note that the given path must be absolute)::. >>> import cppyy_backend.loader as l; >>> l.set_cling_compile_options(True) # adds defaults to EXTRA_CLING_ARGS; >>> install_path = '/full/path/to/target/location/for/PCH'; >>> l.ensure_precompiled_header(install_path). You can then select the appropriate PCH with the ``CLING_STANDARD_PCH`` envar::. $ export CLING_STANDARD_PCH=/full/path/to/target/location/for/PCH/allDict.cxx.pch. Or disable it completely by setting that envar to ""none"". .. note::. Without the PCH, the default C++ standard will be the one with which; ``cppyy-cling`` was built. .. _`conda-forge`: https://anaconda.org/conda-forge/cppyy; .. _`Anaconda`: https://www.anaconda.com/distribution/; .. _`miniconda`: https://docs.conda.io/en/latest/miniconda.html; .. _`PyPI`: https://pypi.python.org/pypi/cppyy/; .. _`virtualenv`: https://pypi.python.org/pypi/virtualenv; .. _`venv`: https://docs.python.org/3/library/venv.html; .. _`Reflex`: https://root.cern.ch/how/how-use-reflex; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/installation.rst:8541,load,loader,8541,bindings/pyroot/cppyy/cppyy/doc/source/installation.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/installation.rst,1,['load'],['loader']
Performance,"th a Release; operation, and vice versa. Notes for optimizers; In general, optimizers should treat this like a nothrow call; the possible; optimizations are usually not interesting. Notes for code generation; This operation has Acquire and Release semantics; see the sections on Acquire; and Release. SequentiallyConsistent; ----------------------. SequentiallyConsistent (``seq_cst`` in IR) provides Acquire semantics for loads; and Release semantics for stores. Additionally, it guarantees that a total; ordering exists between all SequentiallyConsistent operations. Relevant standard; This corresponds to the C++/C ``memory_order_seq_cst``, Java volatile, and; the gcc-compatible ``__sync_*`` builtins which do not specify otherwise. Notes for frontends; If a frontend is exposing atomic operations, these are much easier to reason; about for the programmer than other kinds of operations, and using them is; generally a practical performance tradeoff. Notes for optimizers; Optimizers not aware of atomics can treat this like a nothrow call. For; SequentiallyConsistent loads and stores, the same reorderings are allowed as; for Acquire loads and Release stores, except that SequentiallyConsistent; operations may not be reordered. Notes for code generation; SequentiallyConsistent loads minimally require the same barriers as Acquire; operations and SequentiallyConsistent stores require Release; barriers. Additionally, the code generator must enforce ordering between; SequentiallyConsistent stores followed by SequentiallyConsistent loads. This; is usually done by emitting either a full fence before the loads or a full; fence after the stores; which is preferred varies by architecture. Atomics and IR optimization; ===========================. Predicates for optimizer writers to query:. * ``isSimple()``: A load or store which is not volatile or atomic. This is; what, for example, memcpyopt would check for operations it might transform. * ``isUnordered()``: A load or store which is not",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst:15039,optimiz,optimizers,15039,interpreter/llvm-project/llvm/docs/Atomics.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst,1,['optimiz'],['optimizers']
Performance,"th access groups; in its ``llvm.loop.parallel_accesses`` metadata, then the compiler can; assume that there is no dependency between ``m1`` and ``m2`` carried by; this loop. Instructions that belong to multiple access groups are; considered having this property if at least one of the access groups; matches the ``llvm.loop.parallel_accesses`` list. If all memory-accessing instructions in a loop have; ``llvm.access.group`` metadata that each refer to one of the access; groups of a loop's ``llvm.loop.parallel_accesses`` metadata, then the; loop has no loop carried memory dependences and is considered to be a; parallel loop. Note that if not all memory access instructions belong to an access; group referred to by ``llvm.loop.parallel_accesses``, then the loop must; not be considered trivially parallel. Additional; memory dependence analysis is required to make that determination. As a fail; safe mechanism, this causes loops that were originally parallel to be considered; sequential (if optimization passes that are unaware of the parallel semantics; insert new memory instructions into the loop body). Example of a loop that is considered parallel due to its correct use of; both ``llvm.access.group`` and ``llvm.loop.parallel_accesses``; metadata types. .. code-block:: llvm. for.body:; ...; %val0 = load i32, ptr %arrayidx, !llvm.access.group !1; ...; store i32 %val0, ptr %arrayidx1, !llvm.access.group !1; ...; br i1 %exitcond, label %for.end, label %for.body, !llvm.loop !0. for.end:; ...; !0 = distinct !{!0, !{!""llvm.loop.parallel_accesses"", !1}}; !1 = distinct !{}. It is also possible to have nested parallel loops:. .. code-block:: llvm. outer.for.body:; ...; %val1 = load i32, ptr %arrayidx3, !llvm.access.group !4; ...; br label %inner.for.body. inner.for.body:; ...; %val0 = load i32, ptr %arrayidx1, !llvm.access.group !3; ...; store i32 %val0, ptr %arrayidx2, !llvm.access.group !3; ...; br i1 %exitcond, label %inner.for.end, label %inner.for.body, !llvm.loop !1. inner.for",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:312879,optimiz,optimization,312879,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['optimiz'],['optimization']
Performance,"th are optional. The ""selection of target"" behavior is defined as follows:. (1) If the user does not specify -triple, we default to the host triple.; (2) If the user specifies a -arch, that overrides the arch in the host or; specified triple. //===---------------------------------------------------------------------===//. verifyInputConstraint and verifyOutputConstraint should not return bool. Instead we should return something like:. enum VerifyConstraintResult {; Valid,. // Output only; OutputOperandConstraintLacksEqualsCharacter,; MatchingConstraintNotValidInOutputOperand,. // Input only; InputOperandConstraintContainsEqualsCharacter,; MatchingConstraintReferencesInvalidOperandNumber,. // Both; PercentConstraintUsedWithLastOperand; };. //===---------------------------------------------------------------------===//. Blocks should not capture variables that are only used in dead code. The rule that we came up with is that blocks are required to capture; variables if they're referenced in evaluated code, even if that code; doesn't actually rely on the value of the captured variable. For example, this requires a capture:; (void) var;; But this does not:; if (false) puts(var);. Summary of <rdar://problem/9851835>: if we implement this, we should; warn about non-POD variables that are referenced but not captured, but; only if the non-reachability is not due to macro or template; metaprogramming. //===---------------------------------------------------------------------===//. We can still apply a modified version of the constructor/destructor; delegation optimization in cases of virtual inheritance where:; - there is no function-try-block,; - the constructor signature is not variadic, and; - the parameter variables can safely be copied and repassed; to the base constructor because either; - they have not had their addresses taken by the vbase initializers or; - they were passed indirectly. //===---------------------------------------------------------------------===//; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/NOTES.txt:3576,optimiz,optimization,3576,interpreter/llvm-project/clang/NOTES.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/NOTES.txt,1,['optimiz'],['optimization']
Performance,"th caution. The; guarantees in terms of synchronization are very weak, so make sure these are; only used in a pattern which you know is correct. Generally, these would; either be used for atomic operations which do not protect other memory (like; an atomic counter), or along with a ``fence``. Notes for optimizers; In terms of the optimizer, this can be treated as a read+write on the relevant; memory location (and alias analysis will take advantage of that). In addition,; it is legal to reorder non-atomic and Unordered loads around Monotonic; loads. CSE/DSE and a few other optimizations are allowed, but Monotonic; operations are unlikely to be used in ways which would make those; optimizations useful. Notes for code generation; Code generation is essentially the same as that for unordered for loads and; stores. No fences are required. ``cmpxchg`` and ``atomicrmw`` are required; to appear as a single operation. Acquire; -------. Acquire provides a barrier of the sort necessary to acquire a lock to access; other memory with normal loads and stores. Relevant standard; This corresponds to the C++/C ``memory_order_acquire``. It should also be; used for C++/C ``memory_order_consume``. Notes for frontends; If you are writing a frontend which uses this directly, use with caution.; Acquire only provides a semantic guarantee when paired with a Release; operation. Notes for optimizers; Optimizers not aware of atomics can treat this like a nothrow call. It is; also possible to move stores from before an Acquire load or read-modify-write; operation to after it, and move non-Acquire loads from before an Acquire; operation to after it. Notes for code generation; Architectures with weak memory ordering (essentially everything relevant today; except x86 and SPARC) require some sort of fence to maintain the Acquire; semantics. The precise fences required varies widely by architecture, but for; a simple implementation, most architectures provide a barrier which is strong; enough for eve",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst:11573,load,loads,11573,interpreter/llvm-project/llvm/docs/Atomics.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst,1,['load'],['loads']
Performance,"th optimization remarks (see; :ref:`Options to Emit Optimization Reports <rpass>`). The hotness information; allows users to focus on the hot optimization remarks that are likely to be; more relevant for run-time performance. For example, in this output, the block containing the callsite of `foo` was; executed 3000 times according to the profile data:. ::. s.c:7:10: remark: foo inlined into bar (hotness: 3000) [-Rpass-analysis=inline]; sum += foo(x, x - 2);; ^. This option is implied when; :ref:`-fsave-optimization-record <opt_fsave-optimization-record>` is used.; Otherwise, it defaults to off. .. option:: -fdiagnostics-hotness-threshold. Prevent optimization remarks from being output if they do not have at least; this hotness value. This option, which defaults to zero, controls the minimum hotness an; optimization remark would need in order to be output by Clang. This is; currently supported with optimization remarks (see :ref:`Options to Emit; Optimization Reports <rpass>`) when profile hotness information in; diagnostics is enabled (see; :ref:`-fdiagnostics-show-hotness <opt_fdiagnostics-show-hotness>`). .. _opt_fdiagnostics-fixit-info:. .. option:: -f[no-]diagnostics-fixit-info. Enable ""FixIt"" information in the diagnostics output. This option, which defaults to on, controls whether or not Clang; prints the information on how to fix a specific diagnostic; underneath it when it knows. For example, in this output:. ::. test.c:28:8: warning: extra tokens at end of #endif directive [-Wextra-tokens]; #endif bad; ^; //. Passing **-fno-diagnostics-fixit-info** will prevent Clang from; printing the ""//"" line at the end of the message. This information; is useful for users who may not understand what is wrong, but can be; confusing for machine parsing. .. _opt_fdiagnostics-print-source-range-info:. .. option:: -fdiagnostics-print-source-range-info. Print machine parsable information about source ranges.; This option makes Clang print information about source ranges in a ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:14960,optimiz,optimization,14960,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,1,['optimiz'],['optimization']
Performance,"th should combine to ((a|b) & (c-1)) != 0. Currently not optimized with; ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. From GCC Bug 20192:; #define PMD_MASK (~((1UL << 23) - 1)); void clear_pmd_range(unsigned long start, unsigned long end); {; if (!(start & ~PMD_MASK) && !(end & ~PMD_MASK)); f();; }; The expression should optimize to something like; ""!((start|end)&~PMD_MASK). Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned int f(unsigned int i, unsigned int n) {++i; if (i == n) ++i; return; i;}; unsigned int f2(unsigned int i, unsigned int n) {++i; i += i == n; return i;}; These should combine to the same thing. Currently, the first function; produces better code on X86. //===---------------------------------------------------------------------===//. From GCC Bug 15784:; #define abs(x) x>0?x:-x; int f(int x, int y); {; return (abs(x)) >= 0;; }; This should optimize to x == INT_MIN. (With -fwrapv.) Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. From GCC Bug 14753:; void; rotate_cst (unsigned int a); {; a = (a << 10) | (a >> 22);; if (a == 123); bar ();; }; void; minus_cst (unsigned int a); {; unsigned int tem;. tem = 20 - a;; if (tem == 5); bar ();; }; void; mask_gt (unsigned int a); {; /* This is equivalent to a > 15. */; if ((a & ~7) > 8); bar ();; }; void; rshift_gt (unsigned int a); {; /* This is equivalent to a > 23. */; if ((a >> 2) > 5); bar ();; }. All should simplify to a single comparison. All of these are; currently not optimized with ""clang -emit-llvm-bc | opt; -O3"". //===---------------------------------------------------------------------===//. From GCC Bug 32605:; int c(int* x) {return (char*)x+2 == (char*)x;}; Should combine to 0. Currently not optimized with ""clang; -emit-llvm-bc | opt",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:22566,optimiz,optimize,22566,interpreter/llvm-project/llvm/lib/Target/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt,2,['optimiz'],['optimize']
Performance,"th the static superclass instead of the dynamic class. The actual methods; dynamically found in a class are not those declared in the ``@interface``, but; those defined in a separate ``@implementation`` declaration; however, when; compiling a call, typechecking is done based on the methods declared in the; ``@interface``. Method declarations may also be grouped into :arc-term:`protocols`, which are not; inherently associated with any class, but which classes may claim to follow.; Object pointer types may be qualified with additional protocols that the object; is known to support. :arc-term:`Class extensions` are collections of ivars and methods, designed to; allow a class's ``@interface`` to be split across multiple files; however,; there is still a primary implementation file which must see the; ``@interface``\ s of all class extensions. :arc-term:`Categories` allow; methods (but not ivars) to be declared *post hoc* on an arbitrary class; the; methods in the category's ``@implementation`` will be dynamically added to that; class's method tables which the category is loaded at runtime, replacing those; methods in case of a collision. In the standard environment, objects are allocated on the heap, and their; lifetime is manually managed using a reference count. This is done using two; instance methods which all classes are expected to implement: ``retain``; increases the object's reference count by 1, whereas ``release`` decreases it; by 1 and calls the instance method ``dealloc`` if the count reaches 0. To; simplify certain operations, there is also an :arc-term:`autorelease pool`, a; thread-local list of objects to call ``release`` on later; an object can be; added to this pool by calling ``autorelease`` on it. Block pointers may be converted to type ``id``; block objects are laid out in a; way that makes them compatible with Objective-C objects. There is a builtin; class that all block objects are considered to be objects of; this class; implements ``retain`` by ad",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst:6525,load,loaded,6525,interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,1,['load'],['loaded']
Performance,"th this type metadata, we can now use the ``llvm.type.test`` intrinsic to; test whether a given pointer is compatible with a type identifier. Working; backwards, if ``llvm.type.test`` returns true for a particular pointer,; we can also statically determine the identities of the virtual functions; that a particular virtual call may call. For example, if a program assumes; a pointer to be a member of ``!""_ZST1A""``, we know that the address can; be only be one of ``_ZTV1A+16``, ``_ZTV1B+16`` or ``_ZTV1D+16`` (i.e. the; address points of the vtables of A, B and D respectively). If we then load; an address from that pointer, we know that the address can only be one of; ``&A::f``, ``&B::f`` or ``&D::f``. .. _address point: https://itanium-cxx-abi.github.io/cxx-abi/abi.html#vtable-general. Testing Addresses For Type Membership; =====================================. If a program tests an address using ``llvm.type.test``, this will cause; a link-time optimization pass, ``LowerTypeTests``, to replace calls to this; intrinsic with efficient code to perform type member tests. At a high level,; the pass will lay out referenced globals in a consecutive memory region in; the object file, construct bit vectors that map onto that memory region,; and generate code at each of the ``llvm.type.test`` call sites to test; pointers against those bit vectors. Because of the layout manipulation, the; globals' definitions must be available at LTO time. For more information,; see the `control flow integrity design document`_. A type identifier that identifies functions is transformed into a jump table,; which is a block of code consisting of one branch instruction for each; of the functions associated with the type identifier that branches to the; target function. The pass will redirect any taken function addresses to the; corresponding jump table entry. In the object file's symbol table, the jump; table entries take the identities of the original functions, so that addresses; taken outside th",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TypeMetadata.rst:5073,optimiz,optimization,5073,interpreter/llvm-project/llvm/docs/TypeMetadata.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TypeMetadata.rst,2,"['optimiz', 'perform']","['optimization', 'perform']"
Performance,"th very short (often purely numeric) names. What to do when bugpoint isn't enough; =====================================; 	; Sometimes, ``bugpoint`` is not enough. In particular, InstCombine and; TargetLowering both have visitor structured code with lots of potential; transformations. If the process of using bugpoint has left you with still too; much code to figure out and the problem seems to be in instcombine, the; following steps may help. These same techniques are useful with TargetLowering; as well. Turn on ``-debug-only=instcombine`` and see which transformations within; instcombine are firing by selecting out lines with ""``IC``"" in them. At this point, you have a decision to make. Is the number of transformations; small enough to step through them using a debugger? If so, then try that. If there are too many transformations, then a source modification approach may; be helpful. In this approach, you can modify the source code of instcombine to; disable just those transformations that are being performed on your test input; and perform a binary search over the set of transformations. One set of places; to modify are the ""``visit*``"" methods of ``InstCombiner`` (*e.g.*; ``visitICmpInst``) by adding a ""``return false``"" as the first line of the; method. If that still doesn't remove enough, then change the caller of; ``InstCombiner::DoOneIteration``, ``InstCombiner::runOnFunction`` to limit the; number of iterations. You may also find it useful to use ""``-stats``"" now to see what parts of; instcombine are firing. This can guide where to put additional reporting code. At this point, if the amount of transformations is still too large, then; inserting code to limit whether or not to execute the body of the code in the; visit function can be helpful. Add a static counter which is incremented on; every invocation of the function. Then add code which simply returns false on; desired ranges. For example:. .. code-block:: c++. static int calledCount = 0;; calledCount++;; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Bugpoint.rst:9149,perform,performed,9149,interpreter/llvm-project/llvm/docs/Bugpoint.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Bugpoint.rst,2,['perform'],"['perform', 'performed']"
Performance,"th, clang-format will format up to the end; of the file.; Can only be used with one input file.; --lines=<string> - <start line>:<end line> - format a range of; lines (both 1-based).; Multiple ranges can be formatted by specifying; several -lines arguments.; Can't be used with -offset and -length.; Can only be used with one input file.; -n - Alias for --dry-run; --offset=<uint> - Format a range starting at this byte offset.; Multiple ranges can be formatted by specifying; several -offset and -length pairs.; Can only be used with one input file.; --output-replacements-xml - Output replacements as XML.; --qualifier-alignment=<string> - If set, overrides the qualifier alignment style; determined by the QualifierAlignment style flag; --sort-includes - If set, overrides the include sorting behavior; determined by the SortIncludes style flag; --style=<string> - Set coding style. <string> can be:; 1. A preset: LLVM, GNU, Google, Chromium, Microsoft,; Mozilla, WebKit.; 2. 'file' to load style configuration from a; .clang-format file in one of the parent directories; of the source file (for stdin, see --assume-filename).; If no .clang-format file is found, falls back to; --fallback-style.; --style=file is the default.; 3. 'file:<format_file_path>' to explicitly specify; the configuration file.; 4. ""{key: value, ...}"" to set specific parameters, e.g.:; --style=""{BasedOnStyle: llvm, IndentWidth: 8}""; --verbose - If set, shows the list of processed files. Generic Options:. --help - Display available options (--help-hidden for more); --help-list - Display list of available options (--help-list-hidden for more); --version - Display the version of this program. .. END_FORMAT_HELP. When the desired code formatting style is different from the available options,; the style can be customized using the ``-style=""{key: value, ...}""`` option or; by putting your style configuration in the ``.clang-format`` or ``_clang-format``; file in your project's directory and using ``clang-format -sty",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ClangFormat.rst:3859,load,load,3859,interpreter/llvm-project/clang/docs/ClangFormat.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ClangFormat.rst,1,['load'],['load']
Performance,"than 100 bins per axis) that are; not sparse, the COL2 option will provide up to 20 times performance improvements.; For example, a 1000x1000 bin TH2 that is not sparse will render an order of; magnitude faster with the COL2 option. The COL2 option will also scale its performance based on the size of the pixmap; the histogram image is being rendered into. It also is much better optimized for; sessions where the user is forwarding X11 windows through an `ssh` connection. For the most part, the COL2 and COLZ2 options are a drop in replacement to the COL; and COLZ options. There is one major difference and that concerns the treatment of; bins with zero content. The COL2 and COLZ2 options color these bins the color of zero. This has been implemented by Jeromy Tompkins <Tompkins@nscl.msu.edu>. ## Geometry Libraries; A new module geom/vecgeom was introduced to give transparent access to VecGeom ; solid primitives. VecGeom is a high performance geometry package (link) providing ; SIMD vectorization for the CPU-intensive geometry algorithms used for geometry; navigation. The module creates a new library libConverterVG.so depending on the; VecGeom main library and loaded using the ROOT plug-in mechanism. The main functionality provided by the new vecgeom module is to make a conversion ; in memory of all the shapes in a loaded TGeo geometry into a special adapter; shape TGeoVGShape, redirecting all navigation calls to the corresponding VecGeom ; solid. The library loading and geometry conversion can be done with a single call ; `TVirtualGeoConverter::Instance()->ConvertGeometry()`; . After the conversion is done, all existing TGeo functionality is available as for; a native geometry, only that most of the converted solids provide better navigation ; performance, despite the overhead introduced by the new adapter shape. Prerequisites: installation of VecGeom. ; The installation instructions are available at <http://geant.web.cern.ch/content/installation>; Due to the fact that ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v608/index.md:23164,perform,performance,23164,README/ReleaseNotes/v608/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v608/index.md,1,['perform'],['performance']
Performance,"than one node in it. All of the virtual methods; described below should return ``true`` if they modified the program, or; ``false`` if they didn't. The ``doInitialization(CallGraph &)`` method; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. .. code-block:: c++. virtual bool doInitialization(CallGraph &CG);. The ``doInitialization`` method is allowed to do most of the things that; ``CallGraphSCCPass``\ es are not allowed to do. They can add and remove; functions, get pointers to functions, etc. The ``doInitialization`` method is; designed to do simple initialization type of stuff that does not depend on the; SCCs being processed. The ``doInitialization`` method call is not scheduled to; overlap with any other pass executions (thus it should be very fast). .. _writing-an-llvm-pass-runOnSCC:. The ``runOnSCC`` method; ^^^^^^^^^^^^^^^^^^^^^^^. .. code-block:: c++. virtual bool runOnSCC(CallGraphSCC &SCC) = 0;. The ``runOnSCC`` method performs the interesting work of the pass, and should; return ``true`` if the module was modified by the transformation, ``false``; otherwise. The ``doFinalization(CallGraph &)`` method; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. .. code-block:: c++. virtual bool doFinalization(CallGraph &CG);. The ``doFinalization`` method is an infrequently used method that is called; when the pass framework has finished calling :ref:`runOnSCC; <writing-an-llvm-pass-runOnSCC>` for every SCC in the program being compiled. .. _writing-an-llvm-pass-FunctionPass:. The ``FunctionPass`` class; --------------------------. In contrast to ``ModulePass`` subclasses, `FunctionPass; <https://llvm.org/doxygen/classllvm_1_1Pass.html>`_ subclasses do have a; predictable, local behavior that can be expected by the system. All; ``FunctionPass`` execute on each function in the program independent of all of; the other functions in the program. ``FunctionPass``\ es do not require that; they are executed in a particular order, and ``FunctionPass``\ es do not modify; external fu",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst:16516,perform,performs,16516,interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst,1,['perform'],['performs']
Performance,"that all; memory operations; to global have; completed before; performing the; atomicrmw that is; being released. 3. buffer/global_atomic; 4. s_waitcnt vmcnt(0). - Must happen before; following; buffer_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; cache. 5. buffer_inv sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acq_rel - system - global 1. buffer_wbl2 sc0=1 sc1=1. - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at system scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global and L2 writeback; have completed before; performing the; atomicrmw that is; being released. 3. buffer/global_atomic; sc1=1; 4. s_waitcnt vmcnt(0). - Must happen before; following; buffer_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; caches. 5. buffer_inv sc0=1 sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; MTYPE NC global data.; MTYPE RW and CC memory will; never be stale due to the; memory probes. atomicrmw acq_rel - agent - generic 1. buffer_wbl2 sc1=1. - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:321329,load,load,321329,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"that partitions values computed by a; function into congruence classes. Values ending up in the same congruence; class are guaranteed to be the same for every execution of the program.; In that respect, congruency is a compile-time approximation of equivalence; of values at runtime. H; -. .. _heap:. **Heap**; In garbage collection, the region of memory which is managed using; reachability analysis. I; -. **ICE**; Internal Compiler Error. This abbreviation is used to describe errors; that occur in LLVM or Clang as they are compiling source code. For example,; if a valid C++ source program were to trigger an assert in Clang when; compiled, that could be referred to as an ""ICE"". **ICF**; Identical Code Folding. **ICP**; Indirect Call Promotion. **IPA**; Inter-Procedural Analysis. Refers to any variety of code analysis that; occurs between procedures, functions or compilation units (modules). **IPO**; Inter-Procedural Optimization. Refers to any variety of code optimization; that occurs between procedures, functions or compilation units (modules). **ISel**; Instruction Selection. L; -. **LCSSA**; Loop-Closed Static Single Assignment Form. **LGTM**; ""Looks Good To Me"". In a review thread, this indicates that the; reviewer thinks that the patch is okay to commit. **LICM**; Loop Invariant Code Motion. **LSDA**; Language Specific Data Area. C++ ""zero cost"" unwinding is built on top a; generic unwinding mechanism. As the unwinder walks each frame, it calls; a ""personality"" function to do language specific analysis. Each function's; FDE points to an optional LSDA which is passed to the personality function.; For C++, the LSDA contain info about the type and location of catch; statements in that function. **Load-VN**; Load Value Numbering. **LTO**; Link-Time Optimization. M; -. **MC**; Machine Code. N; -; .. _nfc:. **NFC**; ""No functional change"". Used in a commit message to indicate that a patch; is a pure refactoring/cleanup.; Usually used in the first line, so it is visible",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Lexicon.rst:4849,optimiz,optimization,4849,interpreter/llvm-project/llvm/docs/Lexicon.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Lexicon.rst,1,['optimiz'],['optimization']
Performance,"that seq_cst; fences have their; own s_waitcnt; lgkmcnt(0) and so do; not need to be; considered.); - Ensures any; preceding; sequential; consistent local; memory instructions; have completed; before executing; this sequentially; consistent; instruction. This; prevents reordering; a seq_cst store; followed by a; seq_cst load. (Note; that seq_cst is; stronger than; acquire/release as; the reordering of; load acquire; followed by a store; release is; prevented by the; s_waitcnt of; the release, but; there is nothing; preventing a store; release followed by; load acquire from; completing out of; order. The s_waitcnt; could be placed after; seq_store or before; the seq_load. We; choose the load to; make the s_waitcnt be; as late as possible; so that the store; may have already; completed.). 2. *Following; instructions same as; corresponding load; atomic acquire,; except must generate; all instructions even; for OpenCL.*; load atomic seq_cst - workgroup - local *Same as corresponding; load atomic acquire,; except must generate; all instructions even; for OpenCL.*. load atomic seq_cst - agent - global 1. s_waitcnt lgkmcnt(0) &; - system - generic vmcnt(0). - Could be split into; separate s_waitcnt; vmcnt(0); and s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt lgkmcnt(0); must happen after; preceding; global/generic load; atomic/store; atomic/atomicrmw; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; lgkmcnt(0) and so do; not need to be; considered.); - s_waitcnt vmcnt(0); must happen after; preceding; global/generic load; atomic/store; atomic/atomicrmw; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; vmcnt(0) and so do; not need to be; considered.); - Ensures any; preceding; sequential; consistent global; memory instructions; have completed; before executi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:231601,load,load,231601,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"that the set of threads is the same for all the; operations within the sequence. (If a subset of the convergent operations in the; sequence have additional, non-uniform control dependencies, then this is not; possible. However, the code may still require that the sets of threads are; logically consistent with the conditions of those control dependencies.) In this; case, :ref:`llvm.experimental.convergence.anchor; <llvm.experimental.convergence.anchor>` can be used to express the desired; semantics. The following example function could be part of a hypothetical ""append buffer""; implementation, where threads conditionally write fixed-sized records; contiguously into a global buffer. The function ``@reserveSpaceInBuffer``; returns the index into the buffer at which the calling thread should store its; data. This could be achieved by using a simple atomic operation in every thread to; bump an allocation counter. However, the following implementation can be more performant on some hardware,; because it uses only a single atomic operation for an entire group of threads.; To do this, it first determines the total size of the group, which will be the; operand to the atomic operation, and then later broadcasts the result of the; atomic operation to all threads of the group, so that each thread can compute; its individual position in the buffer:. .. code-block:: llvm. define i32 @reserveSpaceInBuffer() { ; NOTE: _not_ a convergent function!; entry:; %anchor = call token @llvm.experimental.convergence.anchor(). %ballot = call i64 @subgroupBallot(i1 true) [ ""convergencectrl""(token %anchor) ]; %numThreads.p = call i64 @llvm.ctpop.i64(i64 %ballot); %numThreads = trunc i64 %numThreads.p to i32. %absoluteThreadIdx = call i32 @getSubgroupLocalInvocationId(); %absoluteThreadIdx.ext = zext i32 %absoluteThreadIdx to i64; %mask.p = shl i64 1, %absoluteThreadIdx.ext; %mask = sub i64 %mask.p, 1. %maskedBallot = and i64 %ballot, %mask; %relativeThreadIdx.p = call i64 @llvm.ctpop.i64(i64 %ma",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ConvergentOperations.rst:12567,perform,performant,12567,interpreter/llvm-project/llvm/docs/ConvergentOperations.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ConvergentOperations.rst,1,['perform'],['performant']
Performance,"that the symbolizing filter processes a coherent stream of; log lines from a single process address space context. If a logging stream; interleaves log lines from more than one process, these must be collated into; separate per-process log streams and each stream processed by a separate; instance of the symbolizing filter. Because the kernel and user processes use; disjoint address regions in most operating systems, a single user process; address space plus the kernel address space can be treated as a single address; space for symbolization purposes if desired. Dependence on Build IDs; =======================. The symbolizer markup scheme relies on contextual information about runtime; memory address layout to make it possible to convert markup elements into useful; symbolic form. This relies on having an unmistakable identification of which; binary was loaded at each address. An ELF Build ID is the payload of an ELF note with name ``""GNU""`` and type; ``NT_GNU_BUILD_ID``, a unique byte sequence that identifies a particular binary; (executable, shared library, loadable module, or driver module). The linker; generates this automatically based on a hash that includes the complete symbol; table and debugging information, even if this is later stripped from the binary. This specification uses the ELF Build ID as the sole means of identifying; binaries. Each binary relevant to the log must have been linked with a unique; Build ID. The symbolizing filter must have some means of mapping a Build ID back; to the original ELF binary (either the whole unstripped binary, or a stripped; binary paired with a separate debug file). Colorization; ============. The markup format supports a restricted subset of ANSI X3.64 SGR (Select Graphic; Rendition) control sequences. These are unlike other markup elements:. * They specify presentation details (bold or colors) rather than semantic; information. The association of semantic meaning with color (e.g. red for; errors) is chosen by the c",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SymbolizerMarkupFormat.rst:3986,load,loadable,3986,interpreter/llvm-project/llvm/docs/SymbolizerMarkupFormat.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SymbolizerMarkupFormat.rst,1,['load'],['loadable']
Performance,"that values are dead until proven otherwise. This; is similar to :ref:`SCCP <passes-sccp>`, except applied to the liveness of; values. ``always-inline``: Inliner for ``always_inline`` functions; ----------------------------------------------------------. A custom inliner that handles only functions that are marked as ""always; inline"". ``argpromotion``: Promote 'by reference' arguments to scalars; -------------------------------------------------------------. This pass promotes ""by reference"" arguments to be ""by value"" arguments. In; practice, this means looking for internal functions that have pointer; arguments. If it can prove, through the use of alias analysis, that an; argument is *only* loaded, then it can pass the value into the function instead; of the address of the value. This can cause recursive simplification of code; and lead to the elimination of allocas (especially in C++ template code like; the STL). This pass also handles aggregate arguments that are passed into a function,; scalarizing them if the elements of the aggregate are only loaded. Note that; it refuses to scalarize aggregates which would require passing in more than; three operands to the function, because passing thousands of operands for a; large array or structure is unprofitable!. Note that this transformation could also be done for arguments that are only; stored to (returning the value instead), but does not currently. This case; would be best handled when and if LLVM starts supporting multiple return values; from functions. ``block-placement``: Profile Guided Basic Block Placement; ---------------------------------------------------------. This pass is a very simple profile guided basic block placement algorithm. The; idea is to put frequently executed blocks together at the start of the function; and hopefully increase the number of fall-through conditional branches. If; there is no profile information for a particular function, this pass basically; orders blocks in depth-first orde",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:13053,load,loaded,13053,interpreter/llvm-project/llvm/docs/Passes.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst,1,['load'],['loaded']
Performance,"that we'll have a source file; with a simple program written in Kaleidoscope rather than the; interactive JIT. It does involve a limitation that we can only; have one ""top level"" command at a time to reduce the number of; changes necessary. Here's the sample program we'll be compiling:. .. code-block:: python. def fib(x); if x < 3 then; 1; else; fib(x-1)+fib(x-2);. fib(10). Why is this a hard problem?; ===========================. Debug information is a hard problem for a few different reasons - mostly; centered around optimized code. First, optimization makes keeping source; locations more difficult. In LLVM IR we keep the original source location; for each IR level instruction on the instruction. Optimization passes; should keep the source locations for newly created instructions, but merged; instructions only get to keep a single location - this can cause jumping; around when stepping through optimized programs. Secondly, optimization; can move variables in ways that are either optimized out, shared in memory; with other variables, or difficult to track. For the purposes of this; tutorial we're going to avoid optimization (as you'll see with one of the; next sets of patches). Ahead-of-Time Compilation Mode; ==============================. To highlight only the aspects of adding debug information to a source; language without needing to worry about the complexities of JIT debugging; we're going to make a few changes to Kaleidoscope to support compiling; the IR emitted by the front end into a simple standalone program that; you can execute, debug, and see results. First we make our anonymous function that contains our top level; statement be our ""main"":. .. code-block:: udiff. - auto Proto = std::make_unique<PrototypeAST>("""", std::vector<std::string>());; + auto Proto = std::make_unique<PrototypeAST>(""main"", std::vector<std::string>());. just with the simple change of giving it a name. Then we're going to remove the command line code wherever it exists:. .. code-blo",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst:2166,optimiz,optimization,2166,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst,2,['optimiz'],"['optimization', 'optimized']"
Performance,"the ""t.c"" input into an object file, two to assemble the; ""t.s"" input, and one to link them together. A rather different compilation pipeline is shown here; in this; example there are two top level actions to compile the input files; into two separate object files, where each object file is built using; ``lipo`` to merge results built for two separate architectures. .. code-block:: console. $ clang -ccc-print-phases -c -arch i386 -arch x86_64 t0.c t1.c; 0: input, ""t0.c"", c; 1: preprocessor, {0}, cpp-output; 2: compiler, {1}, assembler; 3: assembler, {2}, object; 4: bind-arch, ""i386"", {3}, object; 5: bind-arch, ""x86_64"", {3}, object; 6: lipo, {4, 5}, object; 7: input, ""t1.c"", c; 8: preprocessor, {7}, cpp-output; 9: compiler, {8}, assembler; 10: assembler, {9}, object; 11: bind-arch, ""i386"", {10}, object; 12: bind-arch, ""x86_64"", {10}, object; 13: lipo, {11, 12}, object. After this stage is complete the compilation process is divided into; a simple set of actions which need to be performed to produce; intermediate or final outputs (in some cases, like ``-fsyntax-only``,; there is no ""real"" final output). Phases are well known compilation; steps, such as ""preprocess"", ""compile"", ""assemble"", ""link"", etc. #. **Bind: Tool & Filename Selection**. This stage (in conjunction with the Translate stage) turns the tree; of Actions into a list of actual subprocess to run. Conceptually, the; driver performs a top down matching to assign Action(s) to Tools. The; ToolChain is responsible for selecting the tool to perform a; particular action; once selected the driver interacts with the tool; to see if it can match additional actions (for example, by having an; integrated preprocessor). Once Tools have been selected for all actions, the driver determines; how the tools should be connected (for example, using an inprocess; module, pipes, temporary files, or user provided filenames). If an; output file is required, the driver also computes the appropriate; file name (the suffix and fil",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/DriverInternals.rst:8258,perform,performed,8258,interpreter/llvm-project/clang/docs/DriverInternals.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/DriverInternals.rst,1,['perform'],['performed']
Performance,"the 200m long detector in a meaningful way. Several generalizations of the projection infrastructure were; required:; TEveProjectable::ProjectedClass() takes an argument:; virtual TClass* ProjectedClass(const TEveProjection* p) const = 0;; thus allowing different projected classes for different projections.; All TEveProjection::ProjectPoint/Vector(...) functions have an; additional ""depth"" argument thus allowing the projected classes to; skip explicit setting of depth after the point has been projected; -- this could damage the 3rd component. Pre-scaling now supports 3 dimensions.; Abstract TEveProjected::SetDepth() has been split into two parts:; ; It has been implemented in the base class where it checks for; the projection type (2d) before calling the local function;; Abstract SetDepthLocal() has been added to provide the same; functionality. This allows for the 2d/3d check to be done in place only.; New projection class has been introduced: TEve3DProjection.; It performs pre-scaling and offsets the center.; To simplify the projection of lists TEveElementList has been made; projectable and corresponding TEveElementListProjected class; introduced. This also fixed the problem with render-state not being; propagated to projected classes. The check whether to project a sub-tree of elements is still performed.; TEveGeoShapeProjected has been introduced to represent the 3D; projection of a TEveGeoShape (2D projection is handled by; TEvePolygonSetProjected). Points, lines and tracks use the same projected class for both 2D; and 3D projections. An example showing this functionality has been added as a new tab in; projection_prescale.C.; TEveManager now allows simultaneous usage of several objects; editors. Simply click on the top name-button in object editor to; create a standalone editor for this object in a separate window. This; facilitates operation when several objects need to be modifed in; parallel.; New tutorial alice_vsd.C has been added. It shows; how to read Vi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/graf3d/doc/v526/index.html:2529,perform,performs,2529,graf3d/doc/v526/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/graf3d/doc/v526/index.html,2,['perform'],['performs']
Performance,"the Chirp filesystem. To configure and build, chirp 3.2.2 must be installed.; When a TFile object is deleted, make sure that CINT also 'removes' any global variables that might point to it.; Fix support for the automatic addition to the current directory (for TTree and TH1 for example) in TKey::Read(TObject*).; In TKey, properly handle error in the I/O routines.; Explicitly check the validity of the zipped buffer before calling R__unzip, this allow for better error recovery.; When double checking whether a checksum difference is sustantial, ignore the std namespace. Use CompareContent also in the case of where; the class is versioned but the 'current' streamerInfo has not yet been built.; Prevent the I/O engine from mistakenly applying schema evolution to the TObject::fBits.; Make sure that when a streamer info of a base class is used to stream memberwise that is always not-optimized. If the StreamerInfo on file; has the same version as the StreamerInfo in memory but the one on file need to be 'not optimized' while the one in memory is not yet built, make; sure it will not be optimized.; Fix the reading of empty collection of object when reading without the library.; If the sequence of actions for streaming member-wise is not created correctly (i.e. where fReadMemberWise was null previously),; we now explicitly issue a Fatal error:. Fatal in <ReadSequence>: The sequence of actions to read AliESDVertex:7 member-wise was not initialized.; aborting. Add new optional parameter maxbuf to TXMLEngine::ParseFile() allowing the specification of the XML file size to be parsed. This fixes issue #78864.; Add function TBuffer::AutoExpand to centralize the automatic buffer extension policy. This enable the ability to tweak it later (for example instead of always doubling the size, increasing by only at most 2Mb or take hints from the number of entries already; in a TBasket).; Migrate the class TFileMerger from the proofplayer library to ROOT I/O library and update hadd to rely on",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/io/doc/v530/index.html:1037,optimiz,optimized,1037,io/doc/v530/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/io/doc/v530/index.html,4,['optimiz'],['optimized']
Performance,"the DWARF is part of a code object, then A may need to be relocated. For; example, in the ELF code object format, A must be adjusted by the difference; between the ELF segment virtual address and the virtual address at which the; segment is loaded.*. 2. ``DW_OP_addrx``. ``DW_OP_addrx`` has a single unsigned LEB128 integer operand that represents; a zero-based index into the ``.debug_addr`` section relative to the value of; the ``DW_AT_addr_base`` attribute of the associated compilation unit. The; address value A in the ``.debug_addr`` section has the size of the generic; type. It pushes a location description L with one memory location description SL; on the stack. SL specifies the memory location storage corresponding to the; target architecture default address space with a bit offset equal to A; scaled by 8 (the byte size). *If the DWARF is part of a code object, then A may need to be relocated. For; example, in the ELF code object format, A must be adjusted by the difference; between the ELF segment virtual address and the virtual address at which the; segment is loaded.*. 3. ``DW_OP_LLVM_form_aspace_address`` *New*. ``DW_OP_LLVM_form_aspace_address`` pops top two stack entries. The first; must be an integral type value that represents a target architecture; specific address space identifier AS. The second must be an integral type; value that represents an address A. The address size S is defined as the address bit size of the target; architecture specific address space that corresponds to AS. A is adjusted to S bits by zero extending if necessary, and then treating; the least significant S bits as an unsigned value A'. It pushes a location description L with one memory location description SL; on the stack. SL specifies the memory location storage LS that corresponds; to AS with a bit offset equal to A' scaled by 8 (the byte size). If AS is an address space that is specific to context elements, then LS; corresponds to the location storage associated with the cur",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst:110210,load,loaded,110210,interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst,1,['load'],['loaded']
Performance,"the HSA compatible runtime; allocator for a memory region with the kernarg property for the kernel agent; that will execute the kernel. It must be at least 16-byte aligned.; 4. Kernel argument values are assigned to the kernel argument memory; allocation. The layout is defined in the *HSA Programmer's Language; Reference* [HSA]_. For AMDGPU the kernel execution directly accesses the; kernel argument memory in the same way constant memory is accessed. (Note; that the HSA specification allows an implementation to copy the kernel; argument contents to another location that is accessed by the kernel.); 5. An AQL kernel dispatch packet is created on the AQL queue. The HSA compatible; runtime api uses 64-bit atomic operations to reserve space in the AQL queue; for the packet. The packet must be set up, and the final write must use an; atomic store release to set the packet kind to ensure the packet contents are; visible to the kernel agent. AQL defines a doorbell signal mechanism to; notify the kernel agent that the AQL queue has been updated. These rules, and; the layout of the AQL queue and kernel dispatch packet is defined in the *HSA; System Architecture Specification* [HSA]_.; 6. A kernel dispatch packet includes information about the actual dispatch,; such as grid and work-group size, together with information from the code; object about the kernel, such as segment sizes. The HSA compatible runtime; queries on the kernel symbol can be used to obtain the code object values; which are recorded in the :ref:`amdgpu-amdhsa-code-object-metadata`.; 7. CP executes micro-code and is responsible for detecting and setting up the; GPU to execute the wavefronts of a kernel dispatch.; 8. CP ensures that when the a wavefront starts executing the kernel machine; code, the scalar general purpose registers (SGPR) and vector general purpose; registers (VGPR) are set up as required by the machine code. The required; setup is defined in the :ref:`amdgpu-amdhsa-kernel-descriptor`. The ini",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:151393,queue,queue,151393,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['queue'],['queue']
Performance,"the Poisson log likelihood function; Improve calculation of derivative in x for fitted function. This fixes some problem observed when fitting using the error on the coordinates.; Fitter class: add new methods for calculating the error matrix after minimization, Fitter::CalculateHessErrors() and for calculating the Minos errors Fitter::CalculateMinosErrors; FitConfig: add in the configuration the possibility to select a sub-set of the parameters for calculating the Minos errors by using the method FitConfig::SetMinosErrors( listOfParameters ). If no list is passed, by default the Minos error will be computed on all parameters.; UnBinData class: add new constructor for creating a unbin data set passing a range to select the data and copy in the internal array; FitResult: the class now stores a map of the Minos error using as key the parameter index. If the Minos error has not been calculated for the parameter, FitResult::LowerError(i) and FitResult::UpperError(i) returns the parabolic error; ; Add a new class, MinimTransformFunction to perform a transformation of the function object to deal with limited and fixed variables.; This class uses the same transformation which are also used inside Minuit, a sin transformation for double bounded variables and a sqrt transformation for single bound variable defined in the class MinimizerVariableTransformation.; These classes can be used by minimizer which do not support internally the bounds (like the GSL minimizers).; . Add two new method in ROOT::Math::Minimizer class:; ; int Minimizer::CovMatrixStatus() : returning the status of the covariance matrix. Implemented by Minuit and Minuit2 and follows original Minuit code meaning: code = 0 (not calculated), 1 (approximated), 2 (matrix was made pos def) , 3 (accurate); ; bool Hesse(): to perform a full calculation of the Hessian matrix; . TMath. Fix a numerical problem in TMath::ErfcInverse for small input values. Now the normal quantile function is used for implementing it.; . ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/math/doc/v524/index.html:1665,perform,perform,1665,math/doc/v524/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/math/doc/v524/index.html,2,['perform'],['perform']
Performance,"the SparseBitVector is that setting and testing of random bits is; O(N), and on large SparseBitVectors, this can be slower than BitVector. In our; implementation, setting or testing bits in sorted order (either forwards or; reverse) is O(1) worst case. Testing and setting bits within 128 bits (depends; on size) of the current bit is also O(1). As a general statement,; testing/setting bits in a SparseBitVector is O(distance away from last set bit). .. _dss_coalescingbitvector:. CoalescingBitVector; ^^^^^^^^^^^^^^^^^^^. The CoalescingBitVector container is similar in principle to a SparseBitVector,; but is optimized to represent large contiguous ranges of set bits compactly. It; does this by coalescing contiguous ranges of set bits into intervals. Searching; for a bit in a CoalescingBitVector is O(log(gaps between contiguous ranges)). CoalescingBitVector is a better choice than BitVector when gaps between ranges; of set bits are large. It's a better choice than SparseBitVector when find(); operations must have fast, predictable performance. However, it's not a good; choice for representing sets which have lots of very short ranges. E.g. the set; `{2*x : x \in [0, n)}` would be a pathological input. .. _utility_functions:. Useful Utility Functions; ========================. LLVM implements a number of general utility functions used across the; codebase. You can find the most common ones in ``STLExtras.h``; (`doxygen <https://llvm.org/doxygen/STLExtras_8h.html>`__). Some of these wrap; well-known C++ standard library functions, while others are unique to LLVM. .. _uf_iteration:. Iterating over ranges; ---------------------. Sometimes you may want to iterate over more than range at a time or know the; index of the index. LLVM provides custom utility functions to make that easier,; without having to manually manage all iterators and/or indices:. .. _uf_zip:. The ``zip``\ * functions; ^^^^^^^^^^^^^^^^^^^^^^^^. ``zip``\ * functions allow for iterating over elements from two ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ProgrammersManual.rst:99350,perform,performance,99350,interpreter/llvm-project/llvm/docs/ProgrammersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ProgrammersManual.rst,1,['perform'],['performance']
Performance,"the class,; the method `Delete` is called for the owning collection to delete; correctly its entire track objects. To delete the objects in the; container use `fTrack->Delete()`. To delete the container itself, do; '`delete fTracks'.`. ``` {.cpp}; class TEvent : public TObject {; private:; TList *fTracks; //list of all tracks; TList *fVertex1; //subset of tracks part of vertex1; TList *fVertex2; //subset of tracks part of vertex2; };; TEvent::~TEvent(); {; fTracks->Delete();; delete fTracks;; delete fVertex1;; delete fVertex2;; }; ```. The **`TIterator`** class defines the minimum set of member functions; that all iterators must support. These include:. - `Next`; `Returns the next member of the collection or 0 if no more members.`. - `Reset` `Resets the iterator so that ` `Next`; ` returns the first object.`. ## A Collectable Class. By default, all objects of **`TObject`** derived classes can be stored; in ROOT containers. However, the **`TObject`** class provides some; member functions that allow you to tune the behavior of objects in; containers. For example, by default two objects are considered equal if; their pointers point to the same address. This might be too strict for; some classes where equality is already achieved if some or all of the; data members are equal. By overriding the following **`TObject`** member; functions, you can change the behavior of objects in collections:. - `IsEqual()`is used by the `FindObject() `collection method. By; default, `IsEqual()` compares the two object pointers. - `Compare()`returns -1, 0 or 1 depending if the object is smaller,; equal or larger than the other object. By default, a **`TObject`**; has not a valid `Compare()` method. - `IsSortable() `returns true if the class is sort able (i.e. if it; has a valid `Compare(`) method). By default, a **`TObject`** is not; sort able. - `Hash() `returns a hash value. It needs to be implemented if an; object has to be stored in a collection using a hashing technique,; like **`THas",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/CollectionClasses.md:8483,tune,tune,8483,documentation/users-guide/CollectionClasses.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/CollectionClasses.md,1,['tune'],['tune']
Performance,"the code and has some runtime; overhead during the profiling, but it provides more detailed results than a; sampling profiler. It also provides reproducible results, at least to the; extent that the code behaves consistently across runs. Clang supports two types of instrumentation: frontend-based and IR-based.; Frontend-based instrumentation can be enabled with the option ``-fprofile-instr-generate``,; and IR-based instrumentation can be enabled with the option ``-fprofile-generate``.; For best performance with PGO, IR-based instrumentation should be used. It has; the benefits of lower instrumentation overhead, smaller raw profile size, and; better runtime performance. Frontend-based instrumentation, on the other hand,; has better source correlation, so it should be used with source line-based; coverage testing. The flag ``-fcs-profile-generate`` also instruments programs using the same; instrumentation method as ``-fprofile-generate``. However, it performs a; post-inline late instrumentation and can produce context-sensitive profiles. Here are the steps for using profile guided optimization with; instrumentation:. 1. Build an instrumented version of the code by compiling and linking with the; ``-fprofile-generate`` or ``-fprofile-instr-generate`` option. .. code-block:: console. $ clang++ -O2 -fprofile-instr-generate code.cc -o code. 2. Run the instrumented executable with inputs that reflect the typical usage.; By default, the profile data will be written to a ``default.profraw`` file; in the current directory. You can override that default by using option; ``-fprofile-instr-generate=`` or by setting the ``LLVM_PROFILE_FILE``; environment variable to specify an alternate file. If non-default file name; is specified by both the environment variable and the command line option,; the environment variable takes precedence. The file name pattern specified; can include different modifiers: ``%p``, ``%h``, ``%m``, ``%t``, and ``%c``. Any instance of ``%p`` in that file na",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:103236,perform,performs,103236,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,1,['perform'],['performs']
Performance,"the compiler that; the program is well defined no matter what value is used. This gives the; compiler more freedom to optimize. Here are some examples of; (potentially surprising) transformations that are valid (in pseudo IR):. .. code-block:: llvm. %A = add %X, undef; %B = sub %X, undef; %C = xor %X, undef; Safe:; %A = undef; %B = undef; %C = undef. This is safe because all of the output bits are affected by the undef; bits. Any output bit can have a zero or one depending on the input bits. .. code-block:: llvm. %A = or %X, undef; %B = and %X, undef; Safe:; %A = -1; %B = 0; Safe:; %A = %X ;; By choosing undef as 0; %B = %X ;; By choosing undef as -1; Unsafe:; %A = undef; %B = undef. These logical operations have bits that are not always affected by the; input. For example, if ``%X`` has a zero bit, then the output of the; '``and``' operation will always be a zero for that bit, no matter what; the corresponding bit from the '``undef``' is. As such, it is unsafe to; optimize or assume that the result of the '``and``' is '``undef``'.; However, it is safe to assume that all bits of the '``undef``' could be; 0, and optimize the '``and``' to 0. Likewise, it is safe to assume that; all the bits of the '``undef``' operand to the '``or``' could be set,; allowing the '``or``' to be folded to -1. .. code-block:: llvm. %A = select undef, %X, %Y; %B = select undef, 42, %Y; %C = select %X, %Y, undef; Safe:; %A = %X (or %Y); %B = 42 (or %Y); %C = %Y (if %Y is provably not poison; unsafe otherwise); Unsafe:; %A = undef; %B = undef; %C = undef. This set of examples shows that undefined '``select``' (and conditional; branch) conditions can go *either way*, but they have to come from one; of the two operands. In the ``%A`` example, if ``%X`` and ``%Y`` were; both known to have a clear low bit, then ``%A`` would have to have a; cleared low bit. However, in the ``%C`` example, the optimizer is; allowed to assume that the '``undef``' operand could be the same as; ``%Y`` if ``%Y`` is pro",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:192338,optimiz,optimize,192338,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['optimiz'],['optimize']
Performance,"the constructor, `fChain` will point to the tree named in; the parameter. Next is `fCurrent`, which is also a pointer to the; current tree/chain. Its role is only relevant when we have multiple; trees chained together in a **`TChain`**. The class definition shows us; that this tree has one branch and one leaf per data member. The methods; of `MyClass` are:. - `MyClass(TTree *tree=0) -` this constructor has an optional tree; for a parameter. If you pass a tree, `MyClass` will use it rather; than the tree from which it was created. - `void Init(TTree *tree) -` it is called by the constructor to; initialize the tree for reading. It associates each branch with the; corresponding leaf data member. - `~MyClass() - `the destructor, nothing special. - `Int_t GetEntry(Int_t entry) -` it loads the class with the entry; specified. Once you have executed `GetEntry`, the leaf data members; in `MyClass` are set to the values of the entry. For example,; `GetEntry(12)` loads the 13th event into the event data member of; `MyClass` (note that the first entry is 0). `GetEntry` returns the; number of bytes read from the file. In case the same entry is read; twice, ROOT does not have to do any I/O. In this case `GetEntry`; returns 1. It does not return 0, because many people assume a return; of 0 means an error has occurred while reading. - `Int_t LoadTree(Int_t entry)` and `void Notify()` - these two; methods are related to chains. `LoadTree` will load the tree; containing the specified entry from a chain of trees. Notify is; called by `LoadTree` to adjust the branch addresses. - `void Loop()` - it is the skeleton method that loops through each; entry of the tree. This is interesting to us, because we will need; to customize it for our analysis. ### MyClass.C. `MyClass::Loop` consists of a for-loop calling `GetEntry` for each; entry. In the template, the numbers of bytes are added up, but it does; nothing else. If we were to execute it now, there would be no output. ``` {.cpp}; void MyC",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Trees.md:127240,load,loads,127240,documentation/users-guide/Trees.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Trees.md,1,['load'],['loads']
Performance,"the false side of the second if. .. _passes-lcssa:. ``lcssa``: Loop-Closed SSA Form Pass; ------------------------------------. This pass transforms loops by placing phi nodes at the end of the loops for all; values that are live across the loop boundary. For example, it turns the left; into the right code:. .. code-block:: c++. for (...) for (...); if (c) if (c); X1 = ... X1 = ...; else else; X2 = ... X2 = ...; X3 = phi(X1, X2) X3 = phi(X1, X2); ... = X3 + 4 X4 = phi(X3); ... = X4 + 4. This is still valid LLVM; the extra phi nodes are purely redundant, and will be; trivially eliminated by ``InstCombine``. The major benefit of this; transformation is that it makes many other loop optimizations, such as; ``LoopUnswitch``\ ing, simpler. You can read more in the; :ref:`loop terminology section for the LCSSA form <loop-terminology-lcssa>`. .. _passes-licm:. ``licm``: Loop Invariant Code Motion; ------------------------------------. This pass performs loop invariant code motion, attempting to remove as much; code from the body of a loop as possible. It does this by either hoisting code; into the preheader block, or by sinking code to the exit blocks if it is safe.; This pass also promotes must-aliased memory locations in the loop to live in; registers, thus hoisting and sinking ""invariant"" loads and stores. Hoisting operations out of loops is a canonicalization transform. It enables; and simplifies subsequent optimizations in the middle-end. Rematerialization; of hoisted instructions to reduce register pressure is the responsibility of; the back-end, which has more accurate information about register pressure and; also handles other optimizations than LICM that increase live-ranges. This pass uses alias analysis for two purposes:. #. Moving loop invariant loads and calls out of loops. If we can determine; that a load or call inside of a loop never aliases anything stored to, we; can hoist it or sink it like any other instruction. #. Scalar Promotion of Memory. If there is",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:23501,perform,performs,23501,interpreter/llvm-project/llvm/docs/Passes.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst,1,['perform'],['performs']
Performance,"the following code; (from http://gcc.gnu.org/bugzilla/show_bug.cgi?id=34653):; extern unsigned long table[];; unsigned long foo(unsigned char *p) {; unsigned long tag = *p;; return table[tag >> 4] + table[tag & 0xf];; }. Current code generated:; 	movzbl	(%rdi), %eax; 	movq	%rax, %rcx; 	andq	$240, %rcx; 	shrq	%rcx; 	andq	$15, %rax; 	movq	table(,%rax,8), %rax; 	addq	table(%rcx), %rax; 	ret. Issues:; 1. First movq should be movl; saves a byte.; 2. Both andq's should be andl; saves another two bytes. I think this was; implemented at one point, but subsequently regressed.; 3. shrq should be shrl; saves another byte.; 4. The first andq can be completely eliminated by using a slightly more; expensive addressing mode. //===---------------------------------------------------------------------===//. Consider the following (contrived testcase, but contains common factors):. #include <stdarg.h>; int test(int x, ...) {; int sum, i;; va_list l;; va_start(l, x);; for (i = 0; i < x; i++); sum += va_arg(l, int);; va_end(l);; return sum;; }. Testcase given in C because fixing it will likely involve changing the IR; generated for it. The primary issue with the result is that it doesn't do any; of the optimizations which are possible if we know the address of a va_list; in the current function is never taken:; 1. We shouldn't spill the XMM registers because we only call va_arg with ""int"".; 2. It would be nice if we could sroa the va_list.; 3. Probably overkill, but it'd be cool if we could peel off the first five; iterations of the loop. Other optimizations involving functions which use va_arg on floats which don't; have the address of a va_list taken:; 1. Conversely to the above, we shouldn't spill general registers if we only; call va_arg on ""double"".; 2. If we know nothing more than 64 bits wide is read from the XMM registers,; we can change the spilling code to reduce the amount of stack used by half. //===---------------------------------------------------------------------===//; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt:5311,optimiz,optimizations,5311,interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt,4,['optimiz'],['optimizations']
Performance,"the instructions here to fit your own local situation. Prerequisites; =============. In this use case we'll be using cmake on a Debian-based Linux system,; cross-compiling from an x86_64 host to a hard-float Armv7-A target. We'll be; using as many of the LLVM tools as we can, but it is possible to use GNU; equivalents. * ``A build of LLVM/clang for the llvm-tools and llvm-config``; * ``A clang executable with support for the ARM target``; * ``compiler-rt sources``; * ``The qemu-arm user mode emulator``; * ``An arm-linux-gnueabihf sysroot``. In this example we will be using ninja. See https://compiler-rt.llvm.org/ for more information about the dependencies; on clang and LLVM. See https://llvm.org/docs/GettingStarted.html for information about obtaining; the source for LLVM and compiler-rt. Note that the getting started guide; places compiler-rt in the projects subdirectory, but this is not essential and; if you are using the BaremetalARM.cmake cache for v6-M, v7-M and v7-EM then; compiler-rt must be placed in the runtimes directory. ``qemu-arm`` should be available as a package for your Linux distribution. The most complicated of the prerequisites to satisfy is the arm-linux-gnueabihf; sysroot. In theory it is possible to use the Linux distributions multiarch; support to fulfill the dependencies for building but unfortunately due to; /usr/local/include being added some host includes are selected. The easiest way; to supply a sysroot is to download the arm-linux-gnueabihf toolchain. This can; be found at:; * https://developer.arm.com/open-source/gnu-toolchain/gnu-a/downloads for gcc 8 and above; * https://releases.linaro.org/components/toolchain/binaries/ for gcc 4.9 to 7.3. Building compiler-rt builtins for Arm; =====================================; We will be doing a standalone build of compiler-rt using the following cmake; options. * ``path/to/compiler-rt``; * ``-G Ninja``; * ``-DCMAKE_AR=/path/to/llvm-ar``; * ``-DCMAKE_ASM_COMPILER_TARGET=""arm-linux-gnueabihf""`",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToCrossCompileBuiltinsOnArm.rst:1680,cache,cache,1680,interpreter/llvm-project/llvm/docs/HowToCrossCompileBuiltinsOnArm.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToCrossCompileBuiltinsOnArm.rst,1,['cache'],['cache']
Performance,"the locality of frequently accessed values on the stack, such; as register spills, return addresses, and small local variables. Compatibility; -------------. Most programs, static libraries, or individual files can be compiled; with SafeStack as is. SafeStack requires basic runtime support, which, on most; platforms, is implemented as a compiler-rt library that is automatically linked; in when the program is compiled with SafeStack. Linking a DSO with SafeStack is not currently supported. Known compatibility limitations; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. Certain code that relies on low-level stack manipulations requires adaption to; work with SafeStack. One example is mark-and-sweep garbage collection; implementations for C/C++ (e.g., Oilpan in chromium/blink), which must be; changed to look for the live pointers on both safe and unsafe stacks. SafeStack supports linking statically modules that are compiled with and; without SafeStack. An executable compiled with SafeStack can load dynamic; libraries that are not compiled with SafeStack. At the moment, compiling; dynamic libraries with SafeStack is not supported. Signal handlers that use ``sigaltstack()`` must not use the unsafe stack (see; ``__attribute__((no_sanitize(""safe-stack"")))`` below). Programs that use APIs from ``ucontext.h`` are not supported yet. Security; --------. SafeStack protects return addresses, spilled registers and local variables that; are always accessed in a safe way by separating them in a dedicated safe stack; region. The safe stack is automatically protected against stack-based buffer; overflows, since it is disjoint from the unsafe stack in memory, and it itself; is always accessed in a safe way. In the current implementation, the safe stack; is protected against arbitrary memory write vulnerabilities though; randomization and information hiding: the safe stack is allocated at a random; address and the instrumentation ensures that no pointers to the safe stack are; ever stored outside of ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/SafeStack.rst:2515,load,load,2515,interpreter/llvm-project/clang/docs/SafeStack.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/SafeStack.rst,1,['load'],['load']
Performance,"the mask is taken from the; combination of the '``mask``' and '``evl``' operands in the usual VP way. The; alignment of the operation (corresponding to the '``alignment``' operand of; '``llvm.masked.store``') is specified by the ``align`` parameter attribute (see; above). If it is not provided then the ABI alignment of the type of the; '``value``' operand as specified by the :ref:`datalayout; string<langref_datalayout>` is used instead. Examples:; """""""""""""""""". .. code-block:: text. call void @llvm.vp.store.v8i8.p0(<8 x i8> %val, ptr align 4 %ptr, <8 x i1> %mask, i32 %evl); ;; For all lanes below %evl, the call above is lane-wise equivalent to the call below. call void @llvm.masked.store.v8i8.p0(<8 x i8> %val, ptr %ptr, i32 4, <8 x i1> %mask). .. _int_experimental_vp_strided_load:. '``llvm.experimental.vp.strided.load``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <4 x float> @llvm.experimental.vp.strided.load.v4f32.i64(ptr %ptr, i64 %stride, <4 x i1> %mask, i32 %evl); declare <vscale x 2 x i16> @llvm.experimental.vp.strided.load.nxv2i16.i64(ptr %ptr, i64 %stride, <vscale x 2 x i1> %mask, i32 %evl). Overview:; """""""""""""""""". The '``llvm.experimental.vp.strided.load``' intrinsic loads, into a vector, scalar values from; memory locations evenly spaced apart by '``stride``' number of bytes, starting from '``ptr``'. Arguments:; """""""""""""""""""". The first operand is the base pointer for the load. The second operand is the stride; value expressed in bytes. The third operand is a vector of boolean values; with the same number of elements as the return type. The fourth is the explicit; vector length of the operation. The base pointer underlying type matches the type of the scalar; elements of the return operand. The :ref:`align <attr_align>` parameter attribute can be provided for the first; operand. Semantics:; """""""""""""""""""". The '``llvm.experimental.vp.strided.load``' intrinsic loads, into a vector, multipl",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:787244,load,load,787244,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,"the new API ``ensureOptimizedUses()`` to keep the previous behavior and do a; one-time optimization of ``MemoryUse``\ s, if this was not done before.; New pass users are recommended to call ``ensureOptimizedUses()``. Initially it was not possible to optimize ``MemoryDef``\ s in the same way, as we; restricted ``MemorySSA`` to one operand per access.; This was changed and ``MemoryDef``\ s now keep two operands.; The first one, the defining access, is; always the previous ``MemoryDef`` or ``MemoryPhi`` in the same basic block, or; the last one in a dominating predecessor if the current block doesn't have any; other accesses writing to memory. This is needed for walking Def chains.; The second operand is the optimized access, if there was a previous call on the; walker's ``getClobberingMemoryAccess(MA)``. This API will cache information; as part of ``MA``.; Optimizing all ``MemoryDef``\ s has quadratic time complexity and is not done; by default. A walk of the uses for any MemoryDef can find the accesses that were optimized; to it.; A code snippet for such a walk looks like this:. .. code-block:: c++. MemoryDef *Def; // find who's optimized or defining for this MemoryDef; for (auto& U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *DefUser = cast_of_null<MemoryDef>MA); if (DefUser->isOptimized() && DefUser->getOptimized() == Def) {; // User who is optimized to Def; } else {; // User who's defining access is Def; optimized to something else or not optimized.; }; }. When ``MemoryUse``\ s are optimized, for a given store, you can find all loads; clobbered by that store by walking the immediate and transitive uses of; the store. .. code-block:: c++. checkUses(MemoryAccess *Def) { // Def can be a MemoryDef or a MemoryPhi.; for (auto& U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *MU = cast_of_null<MemoryUse>MA) {; // Process MemoryUse as needed.; }; else {; // Process MemoryDef or MemoryPhi as needed. /",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:12793,optimiz,optimized,12793,interpreter/llvm-project/llvm/docs/MemorySSA.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst,1,['optimiz'],['optimized']
Performance,"the object editor; should be the object class name concatenated with the word `‘Editor'`. (c) Provide a default constructor. (d) Use the signals/slots communication mechanism for event processing. (e) Implement the virtual method `SetModel(TObject *obj)` where all; widgets are set with the current object's attributes. This method is; called when the editor receives a signal from the canvas saying that an; object is the selected. (f) Implement all necessary slots and connect them to appropriate; signals that GUI widgets send out. The GUI classes in ROOT are developed; to emit signals whenever they change a state that others might be; interested. As we noted already, the signals/slots communication; mechanism allows total independence of the interacting classes. #### Creation and Destruction. GED-frames are constructed during traversal of class hierarchy of the; selected object, executed from method **`TGedEditor::SetModel()`**.; When a new object of a different class is selected, the unneeded; GED-frames are cached in memory for potential reuse. The frames are; deleted automatically when the editor is closed. Note: A deep cleanup is assumed for all frames put into the editor. This; implies:. - do not share the layout-hints among GUI components;. - do not delete child widgets in the destructor as this is done; automatically. #### Using Several Tabs. Sometimes you might need to use several tabs to organize properly your; class-editor. Each editor tab is a resource shared among all the; class-editors. Tabs must be created from the constructor of your; editor-class by using the method:. ``` {.cpp}; TGVerticalFrame* TGedFrame::CreateEditorTabSubFrame(const Text_t *name),; ```. It returns a pointer to a new tab container frame ready for use in your; class. If you need to hide/show this frame depending on the object's; status, you should store it in a data member. See for examples:; **`TH1Editor`**, **`TH2Editor`**. #### Base-Class Editors Control. Full control over base-cl",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/gui/ged/doc/index.md:4716,cache,cached,4716,gui/ged/doc/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/gui/ged/doc/index.md,1,['cache'],['cached']
Performance,"the overloaded function:. // lib/Target/Mips/MipsISelLowering.h; SDValue getTargetNode(JumpTableSDNode *N, EVT Ty, SelectionDAG &DAG,; unsigned Flag) const;. 2. Generic address nodes are lowered to some combination of target; independent and machine specific SDNodes (for example:; MipsISD::{Highest, Higher, Hi, Lo}) depending upon relocation model,; ABI, and compilation options. The choice of specific instructions that are to be used is delegated; to ISel which in turn relies on TableGen patterns to choose subtarget; specific instructions. For example, in getAddrLocal, the pseudo-code; generated is:. (add (load (wrapper $gp, %got(sym)), %lo(sym)). where ""%lo"" represents an instance of an SDNode with opcode; ""MipsISD::Lo"", ""wrapper"" indicates one with opcode ""MipsISD::Wrapper"",; and ""%got"" the global table pointer ""getGlobalReg(...)"". The ""add"" is; ""ISD::ADD"", not a target dependent one. 3. A TableGen multiclass pattern ""MipsHiLoRelocs"" is used to define a; template pattern parameterized over the load upper immediate; instruction, add operation, the zero register, and register class.; Here the instantiation of MipsHiLoRelocs in MipsInstrInfo.td is used; to MIPS32 to compute addresses for the static relocation model. // lib/Target/Mips/MipsInstrInfo.td; multiclass MipsHiLoRelocs<Instruction Lui, Instruction Addiu,; Register ZeroReg, RegisterOperand GPROpnd> {; def : MipsPat<(MipsHi tglobaladdr:$in), (Lui tglobaladdr:$in)>;; ...; def : MipsPat<(MipsLo tglobaladdr:$in), (Addiu ZeroReg, tglobaladdr:$in)>;; ...; def : MipsPat<(add GPROpnd:$hi, (MipsLo tglobaladdr:$lo)),; (Addiu GPROpnd:$hi, tglobaladdr:$lo)>;; ...; }; defm : MipsHiLoRelocs<LUi, ADDiu, ZERO, GPR32Opnd>;. // lib/Target/Mips/Mips64InstrInfo.td; defm : MipsHiLoRelocs<LUi64, DADDiu, ZERO_64, GPR64Opnd>, SYM_32;. The instantiation in Mips64InstrInfo.td is used for MIPS64 in ILP32; mode, as guarded by the predicate ""SYM_32"" and also for a submode of; LP64 where symbols are assumed to be 32 bits wide. More details",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt:1927,load,load,1927,interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt,2,['load'],['load']
Performance,"the pointee must be an integer, pointer, or; floating-point type whose bit width is a power of two greater than or equal to; eight and less than or equal to a target-specific size limit. ``align`` must be; explicitly specified on atomic loads. Note: if the alignment is not greater or; equal to the size of the `<value>` type, the atomic operation is likely to; require a lock and have poor performance. ``!nontemporal`` does not have any; defined semantics for atomic loads. The optional constant ``align`` argument specifies the alignment of the; operation (that is, the alignment of the memory address). It is the; responsibility of the code emitter to ensure that the alignment information is; correct. Overestimating the alignment results in undefined behavior.; Underestimating the alignment may produce less efficient code. An alignment of; 1 is always safe. The maximum possible alignment is ``1 << 32``. An alignment; value higher than the size of the loaded type implies memory up to the; alignment value bytes can be safely loaded without trapping in the default; address space. Access of the high bytes can interfere with debugging tools, so; should not be accessed if the function has the ``sanitize_thread`` or; ``sanitize_address`` attributes. The alignment is only optional when parsing textual IR; for in-memory IR, it is; always present. An omitted ``align`` argument means that the operation has the; ABI alignment for the target. The optional ``!nontemporal`` metadata must reference a single; metadata name ``<nontemp_node>`` corresponding to a metadata node with one; ``i32`` entry of value 1. The existence of the ``!nontemporal``; metadata on the instruction tells the optimizer and code generator; that this load is not expected to be reused in the cache. The code; generator may select special instructions to save cache bandwidth, such; as the ``MOVNT`` instruction on x86. The optional ``!invariant.load`` metadata must reference a single; metadata name ``<empty_node>`` co",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:414356,load,loaded,414356,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,2,['load'],['loaded']
Performance,"the size of the test; program. By default, **bugpoint** uses these passes internally when attempting to; reduce test programs. If you're trying to find a bug in one of these passes,; **bugpoint** may crash. **--enable-valgrind**. Use valgrind to find faults in the optimization phase. This will allow; bugpoint to find otherwise asymptomatic problems caused by memory; mis-management. **-find-bugs**. Continually randomize the specified passes and run them on the test program; until a bug is found or the user kills **bugpoint**. **-help**. Print a summary of command line options. **--input** *filename*. Open *filename* and redirect the standard input of the test program, whenever; it runs, to come from that file. **--load** *plugin*. Load the dynamic object *plugin* into **bugpoint** itself. This object should; register new optimization passes. Once loaded, the object will add new command; line options to enable various optimizations. To see the new complete list of; optimizations, use the **-help** and **--load** options together; for example:. .. code-block:: bash. bugpoint --load myNewPass.so -help. **--mlimit** *megabytes*. Specifies an upper limit on memory usage of the optimization and codegen. Set; to zero to disable the limit. **--output** *filename*. Whenever the test program produces output on its standard output stream, it; should match the contents of *filename* (the ""reference output""). If you; do not use this option, **bugpoint** will attempt to generate a reference output; by compiling the program with the ""safe"" backend and running it. **--run-{int,jit,llc,custom}**. Whenever the test program is compiled, **bugpoint** should generate code for it; using the specified code generator. These options allow you to choose the; interpreter, the JIT compiler, the static native code compiler, or a; custom command (see **--exec-command**) respectively. **--safe-{llc,custom}**. When debugging a code generator, **bugpoint** should use the specified code; generator as ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/bugpoint.rst:3551,optimiz,optimizations,3551,interpreter/llvm-project/llvm/docs/CommandGuide/bugpoint.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/bugpoint.rst,2,"['load', 'optimiz']","['load', 'optimizations']"
Performance,"the tdf, skipped otherwise.; - The TLazyDS data source has been added. It allows to create a source starting from ResultProxies to vectors.; - `TDataFrameInterface<T>::Report` returns a `TCutflowReport` object which can be inspected programmatically.; - Add `Aggregate` action and implement `Reduce` in terms of it.; - Add support for a more general leafname syntax that includes pathnames with multiple dots, such as ""myBranch.mySubBranch.myLeaf"". This is available both for jitted expressions and for lists of column names.; - The CSV data source (TCsvDS) can now be constructed with a chunk size parameter, and as a result the CSV file will be read progressively, in chunks of the specified size. This can be used to prevent the whole CSV file from being read into memory at once, thus reducing the memory footprint of this data source.; - Add the `ROOT::Experimental::TAdoptAllocator<T>`, an allocator which allows to adopt existing memory. If memory is adopted, upon allocation a copy is performed in the new, potentially more extended, memory region.; - Add `ROOT::Experimental::VecOps::TVec<T>` a class which represents a contiguous array, inspired by Numpy arrays. `TVec` offer a convenient interface, almost identical to the one of `std::vector`. It can own or adopt its memory. As well as a set of tools which make analysis of collections easier, avoiding to loop over the individual elements of the collections. Basic arithmetic operations such as +,-,*,/,% between TVecs and scalars and TVecs are supported. Most popular math functions which act on TVecs are provided. Helpers to calculate basic quantities such as sum, mean, variance or standard deviation of TVecs are provided.; A powerful and concise syntax for expressing cuts is available:; ```; // mu_pts_tvec and mu_etas_tvec are two equally sized TVecs holding kinematic properties of muons; // a filter on muons pseudorapidities is applied considering a range in pseudo rapidity.; filtered_mu_pts_tvec = mu_pts_tvec[abs(mu_etas_t",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v614/index.md:8355,perform,performed,8355,README/ReleaseNotes/v614/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v614/index.md,1,['perform'],['performed']
Performance,"the vector and scalar memory operations performed by wavefronts; executing with different L1 caches and the same L2 cache can be reordered; relative to each other.; * A ``s_waitcnt vmcnt(0)`` is required to ensure synchronization between; vector memory operations of different CUs. It ensures a previous vector; memory operation has completed before executing a subsequent vector memory; or LDS operation and so can be used to meet the requirements of acquire and; release.; * An L2 cache can be kept coherent with other L2 caches by using the MTYPE RW; (read-write) for memory local to the L2, and MTYPE NC (non-coherent) with; the PTE C-bit set for memory not local to the L2. * Any local memory cache lines will be automatically invalidated by writes; from CUs associated with other L2 caches, or writes from the CPU, due to; the cache probe caused by the PTE C-bit.; * XGMI accesses from the CPU to local memory may be cached on the CPU.; Subsequent access from the GPU will automatically invalidate or writeback; the CPU cache due to the L2 probe filter.; * To ensure coherence of local memory writes of CUs with different L1 caches; in the same agent a ``buffer_wbl2`` is required. It does nothing if the; agent is configured to have a single L2, or will writeback dirty L2 cache; lines if configured to have multiple L2 caches.; * To ensure coherence of local memory writes of CUs in different agents a; ``buffer_wbl2 sc1`` is required. It will writeback dirty L2 cache lines.; * To ensure coherence of local memory reads of CUs with different L1 caches; in the same agent a ``buffer_inv sc1`` is required. It does nothing if the; agent is configured to have a single L2, or will invalidate non-local L2; cache lines if configured to have multiple L2 caches.; * To ensure coherence of local memory reads of CUs in different agents a; ``buffer_inv sc0 sc1`` is required. It will invalidate non-local L2 cache; lines if configured to have multiple L2 caches. * PCIe access from the GPU to the CP",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:288738,cache,cache,288738,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['cache'],['cache']
Performance,"the; `Block RThroughput`) is an indicator of a performance bottleneck caused by the; lack of hardware resources.; In general, the lower the Block RThroughput, the better. In this example, ``uOps per iteration/Block RThroughput`` is 1.50. Since there; are no loop-carried dependencies, the observed `uOps Per Cycle` is expected to; approach 1.50 when the number of iterations tends to infinity. The delta between; the Dispatch Width (2.00), and the theoretical maximum uOp throughput (1.50) is; an indicator of a performance bottleneck caused by the lack of hardware; resources, and the *Resource pressure view* can help to identify the problematic; resource usage. The second section of the report is the `instruction info view`. It shows the; latency and reciprocal throughput of every instruction in the sequence. It also; reports extra information related to the number of micro opcodes, and opcode; properties (i.e., 'MayLoad', 'MayStore', and 'HasSideEffects'). Field *RThroughput* is the reciprocal of the instruction throughput. Throughput; is computed as the maximum number of instructions of a same type that can be; executed per clock cycle in the absence of operand dependencies. In this; example, the reciprocal throughput of a vector float multiply is 1; cycles/instruction. That is because the FP multiplier JFPM is only available; from pipeline JFPU1. Instruction encodings are displayed within the instruction info view when flag; `-show-encoding` is specified. Below is an example of `-show-encoding` output for the dot-product kernel:. .. code-block:: none. Instruction Info:; [1]: #uOps; [2]: Latency; [3]: RThroughput; [4]: MayLoad; [5]: MayStore; [6]: HasSideEffects (U); [7]: Encoding Size. [1] [2] [3] [4] [5] [6] [7] Encodings: Instructions:; 1 2 1.00 4 c5 f0 59 d0 vmulps	%xmm0, %xmm1, %xmm2; 1 4 1.00 4 c5 eb 7c da vhaddps	%xmm2, %xmm2, %xmm3; 1 4 1.00 4 c5 e3 7c e3 vhaddps	%xmm3, %xmm3, %xmm4. The `Encoding Size` column shows the size in bytes of instructions. The; `Encod",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst:18870,throughput,throughput,18870,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,1,['throughput'],['throughput']
Performance,"the; following; fence-paired-atomic. fence release - agent *none* 1. s_waitcnt lgkmcnt(0) &; - system vmcnt(0) & vscnt(0). - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0) and vscnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate. If; fence had an; address space then; set to address; space of OpenCL; fence flag, or to; generic if both; local and global; flags are; specified.; - Could be split into; separate s_waitcnt; vmcnt(0), s_waitcnt; vscnt(0) and s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/load atomic/; atomicrmw-with-return-value.; - s_waitcnt vscnt(0); must happen after; any preceding; global/generic; store/store atomic/; atomicrmw-no-return-value.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; any following store; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - Ensures that all; memory operations; have; completed before; performing the; following; fence-paired-atomic. **Acquire-Release Atomic**; ------------------------------------------------------------------------------------; atomicrmw acq_rel - singlethread - global 1. buffer/global/ds/flat_atomic; - wavefront - local; - generic; atomicrmw acq_rel - workgroup - global 1. s_waitcnt lgkmcnt(0) &; vmcnt(0) & vscnt(0). - If CU wavefront execution; mode, omit vmcnt(0) and; vscnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Could be split into; separate s_waitcnt; vmcnt(0), s_waitcnt; vscnt(0), and s_waitcnt; lgkmcnt(0) to allow; them to be; independen",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:362463,load,load,362463,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"ther RuntimeDyldELF or; RuntimeDyldMachO (both of which derive from the RuntimeDyldImpl base; class) and calls the RuntimeDyldImpl::loadObject method to perform that; actual loading. .. image:: MCJIT-dyld-load.png. RuntimeDyldImpl::loadObject begins by creating an ObjectImage instance; from the ObjectBuffer it received. ObjectImage, which wraps the; ObjectFile class, is a helper class which parses the binary object image; and provides access to the information contained in the format-specific; headers, including section, symbol and relocation information. RuntimeDyldImpl::loadObject then iterates through the symbols in the; image. Information about common symbols is collected for later use. For; each function or data symbol, the associated section is loaded into memory; and the symbol is stored in a symbol table map data structure. When the; iteration is complete, a section is emitted for the common symbols. Next, RuntimeDyldImpl::loadObject iterates through the sections in the; object image and for each section iterates through the relocations for; that sections. For each relocation, it calls the format-specific; processRelocationRef method, which will examine the relocation and store; it in one of two data structures, a section-based relocation list map and; an external symbol relocation map. .. image:: MCJIT-load-object.png. When RuntimeDyldImpl::loadObject returns, all of the code and data; sections for the object will have been loaded into memory allocated by the; memory manager and relocation information will have been prepared, but the; relocations have not yet been applied and the generated code is still not; ready to be executed. [Currently (as of August 2013) the MCJIT engine will immediately apply; relocations when loadObject completes. However, this shouldn't be; happening. Because the code may have been generated for a remote target,; the client should be given a chance to re-map the section addresses before; relocations are applied. It is possible to ap",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst:4734,load,loadObject,4734,interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,1,['load'],['loadObject']
Performance,"ther SIMD; operations are enabled or not. The default set of attributes is set by the; current CPU. For a list of available attributes, use:. .. code-block:: none. llvm-as < /dev/null | llc -march=xyz -mattr=help. .. option:: --frame-pointer. Specify effect of frame pointer elimination optimization (all,non-leaf,none). .. option:: --disable-excess-fp-precision. Disable optimizations that may produce excess precision for floating point.; Note that this option can dramatically slow down code on some systems; (e.g. X86). .. option:: --enable-no-infs-fp-math. Enable optimizations that assume no Inf values. .. option:: --enable-no-nans-fp-math. Enable optimizations that assume no NAN values. .. option:: --enable-no-signed-zeros-fp-math. Enable FP math optimizations that assume the sign of 0 is insignificant. .. option:: --enable-no-trapping-fp-math. Enable setting the FP exceptions build attribute not to use exceptions. .. option:: --enable-unsafe-fp-math. Enable optimizations that make unsafe assumptions about IEEE math (e.g. that; addition is associative) or may not work for all input ranges. These; optimizations allow the code generator to make use of some instructions which; would otherwise not be usable (such as ``fsin`` on X86). .. option:: --stats. Print statistics recorded by code-generation passes. .. option:: --time-passes. Record the amount of time needed for each pass and print a report to standard; error. .. option:: --load=<dso_path>. Dynamically load ``dso_path`` (a path to a dynamically shared object) that; implements an LLVM target. This will permit the target name to be used with; the :option:`-march` option so that code can be generated for that target. .. option:: -meabi=[default|gnu|4|5]. Specify which EABI version should conform to. Valid EABI versions are *gnu*,; *4* and *5*. Default value (*default*) depends on the triple. .. option:: -stack-size-section. Emit the .stack_sizes section which contains stack size metadata. The section; contains an ar",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llc.rst:3840,optimiz,optimizations,3840,interpreter/llvm-project/llvm/docs/CommandGuide/llc.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llc.rst,1,['optimiz'],['optimizations']
Performance,"ther with coverage mapping for; `source-based code coverage`_. The `coverage mapping format`_ is different from; profile format. .. _`source-based code coverage`: https://clang.llvm.org/docs/SourceBasedCodeCoverage.html; .. _`coverage mapping format`: https://llvm.org/docs/CoverageMappingFormat.html. Raw Profile Format; ===================. The raw profile is generated by running the instrumented binary. The raw profile; data from an executable or a shared library [3]_ consists of a header and; multiple sections, with each section as a memory dump. The raw profile data needs; to be reasonably compact and fast to generate. There are no backward or forward version compatiblity guarantees for the raw profile; format. That is, compilers and tools `require`_ a specific raw profile version; to parse the profiles. .. _`require`: https://github.com/llvm/llvm-project/blob/bffdde8b8e5d9a76a47949cd0f574f3ce656e181/llvm/lib/ProfileData/InstrProfReader.cpp#L551-L558. To feed profiles back into compilers for an optimized build (e.g., via; ``-fprofile-use`` for IR instrumentation), a raw profile must to be converted into; indexed format. General Storage Layout; -----------------------. The storage layout of raw profile data format is illustrated below. Basically,; when the raw profile is read into an memory buffer, the actual byte offset of a; section is inferred from the section's order in the layout and size information; of all the sections ahead of it. ::. +----+-----------------------+; | | Magic |; | +-----------------------+; | | Version |; | +-----------------------+; H | Size Info for |; E | Section 1 |; A +-----------------------+; D | Size Info for |; E | Section 2 |; R +-----------------------+; | | ... |; | +-----------------------+; | | Size Info for |; | | Section N |; +----+-----------------------+; P | Section 1 |; A +-----------------------+; Y | Section 2 |; L +-----------------------+; O | ... |; A +-----------------------+; D | Section N |; +----+--------------",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/InstrProfileFormat.rst:1677,optimiz,optimized,1677,interpreter/llvm-project/llvm/docs/InstrProfileFormat.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/InstrProfileFormat.rst,1,['optimiz'],['optimized']
Performance,"they are never allowed to be selected to generate vector loads and stores. The exception is one-lane vectors [1]_ - these by definition cannot have lane ordering problems so are fine to use ``LDR``/``STR``. 2. Create code generation patterns for bitconverts that create ``REV`` instructions. 3. Make sure appropriate bitconverts are created so that vector values get passed over call boundaries as 1-element vectors (which is the same as if they were loaded with ``LDR``). Bitconverts; -----------. .. image:: ARM-BE-bitcastfail.png; :align: right. The main problem with the ``LD1`` solution is dealing with bitconverts (or bitcasts, or reinterpret casts). These are pseudo instructions that only change the compiler's interpretation of data, not the underlying data itself. A requirement is that if data is loaded and then saved again (called a ""round trip""), the memory contents should be the same after the store as before the load. If a vector is loaded and is then bitconverted to a different vector type before storing, the round trip will currently be broken. Take for example this code sequence::. %0 = load <4 x i32> %x; %1 = bitcast <4 x i32> %0 to <2 x i64>; store <2 x i64> %1, <2 x i64>* %y. This would produce a code sequence such as that in the figure on the right. The mismatched ``LD1`` and ``ST1`` cause the stored data to differ from the loaded data. .. container:: clearer. When we see a bitcast from type ``X`` to type ``Y``, what we need to do is to change the in-register representation of the data to be *as if* it had just been loaded by a ``LD1`` of type ``Y``. .. image:: ARM-BE-bitcastsuccess.png; :align: right. Conceptually this is simple - we can insert a ``REV`` undoing the ``LD1`` of type ``X`` (converting the in-register representation to the same as if it had been loaded by ``LDR``) and then insert another ``REV`` to change the representation to be as if it had been loaded by an ``LD1`` of type ``Y``. For the previous example, this would be::. LD1 v0.4s, [x]. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BigEndianNEON.rst:10233,load,loaded,10233,interpreter/llvm-project/llvm/docs/BigEndianNEON.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BigEndianNEON.rst,1,['load'],['loaded']
Performance,"this is; termed the; fence-paired-atomic).; - Must happen before; the following; buffer_inv and; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than the; value read by the; fence-paired-atomic. 3. buffer_inv sc0=1. - If not TgSplit execution; mode, omit.; - Ensures that; following; loads will not see; stale data. fence acquire - agent *none* 1. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate; (see comment for; previous fence).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - Must happen before; the following; buffer_inv.; - Ensures that the; fence-paired atomic; has completed; before invalidating; the; cache. Therefore; any following; locations read must; be no older than; the value read by; the; fence-paired-atomic. 2. buffer_inv sc1=1. - Must happen before any; following global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. fence acquire - system *none* 1. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - However, since LLVM; currently has no; add",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:304313,load,load,304313,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"this optimization is that patterns like reloading a spilled; register or accessing a global field don't get checked. This is a very; significant performance win. ###### Don't check dependent loads. A core part of why this mitigation strategy works is that it establishes a; data-flow check on the loaded address. However, this means that if the address; itself was already loaded using a checked load, there is no need to check a; dependent load provided it is within the same basic block as the checked load,; and therefore has no additional predicates guarding it. Consider code like the; following:; ```; ... .LBB0_4: # %danger; movq (%rcx), %rdi; movl (%rdi), %edx; ```. This will get transformed into:; ```; ... .LBB0_4: # %danger; cmovneq %r8, %rax # Conditionally update predicate state.; orq %rax, %rcx # Mask the pointer if misspeculating.; movq (%rcx), %rdi # Hardened load.; movl (%rdi), %edx # Unhardened load due to dependent addr.; ```. This doesn't check the load through `%rdi` as that pointer is dependent on a; checked load already. ###### Protect large, load-heavy blocks with a single lfence. It may be worth using a single `lfence` instruction at the start of a block; which begins with a (very) large number of loads that require independent; protection *and* which require hardening the address of the load. However, this; is unlikely to be profitable in practice. The latency hit of the hardening; would need to exceed that of an `lfence` when *correctly* speculatively; executed. But in that case, the `lfence` cost is a complete loss of speculative; execution (at a minimum). So far, the evidence we have of the performance cost; of using `lfence` indicates few if any hot code patterns where this trade off; would make sense. ###### Tempting optimizations that break the security model. Several optimizations were considered which didn't pan out due to failure to; uphold the security model. One in particular is worth discussing as many others; will reduce to it. We wonde",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md:36094,load,load,36094,interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,2,['load'],['load']
Performance,"this simple; case, it seems to be pretty clear that `__int_32_0` represents `a`. However, it; is not true. An important note with optimization is that the value of a variable may not; properly express the intended value in the source code. For example:. .. code-block:: c++. static task coro_task(int v) {; int a = v;; co_await await_counter{};; a++; // __int_32_0 is 43 here; std::cout << a << ""\n"";; a++; // __int_32_0 is still 43 here; std::cout << a << ""\n"";; a++; // __int_32_0 is still 43 here!; std::cout << a << ""\n"";; co_await await_counter{};; a++; // __int_32_0 is still 43 here!!; std::cout << a << ""\n"";; a++; // Why is __int_32_0 still 43 here?; std::cout << a << ""\n"";; }. When debugging step-by-step, the value of `__int_32_0` seemingly does not; change, despite being frequently incremented, and instead is always `43`.; While this might be surprising, this is a result of the optimizer recognizing; that it can eliminate most of the load/store operations. The above code gets; optimized to the equivalent of:. .. code-block:: c++. static task coro_task(int v) {; store v to __int_32_0 in the frame; co_await await_counter{};; a = load __int_32_0; std::cout << a+1 << ""\n"";; std::cout << a+2 << ""\n"";; std::cout << a+3 << ""\n"";; co_await await_counter{};; a = load __int_32_0; std::cout << a+4 << ""\n"";; std::cout << a+5 << ""\n"";; }. It should now be obvious why the value of `__int_32_0` remains unchanged; throughout the function. It is important to recognize that `__int_32_0`; does not directly correspond to `a`, but is instead a variable generated; to assist the compiler in code generation. The variables in an optimized; coroutine frame should not be thought of as directly representing the; variables in the C++ source. Get the suspended points; ========================. An important requirement for debugging coroutines is to understand suspended; points, which are where the coroutine is currently suspended and awaiting. For simple cases like the above, inspecting the v",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/DebuggingCoroutines.rst:9700,optimiz,optimized,9700,interpreter/llvm-project/clang/docs/DebuggingCoroutines.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/DebuggingCoroutines.rst,1,['optimiz'],['optimized']
Performance,"this; function returns to its caller. If the address space is not explicitly; specified, the object is allocated in the alloca address space from the; :ref:`datalayout string<langref_datalayout>`. Arguments:; """""""""""""""""""". The '``alloca``' instruction allocates ``sizeof(<type>)*NumElements``; bytes of memory on the runtime stack, returning a pointer of the; appropriate type to the program. If ""NumElements"" is specified, it is; the number of elements allocated, otherwise ""NumElements"" is defaulted; to be one. If a constant alignment is specified, the value result of the; allocation is guaranteed to be aligned to at least that boundary. The; alignment may not be greater than ``1 << 32``. The alignment is only optional when parsing textual IR; for in-memory IR,; it is always present. If not specified, the target can choose to align the; allocation on any convenient boundary compatible with the type. '``type``' may be any sized type. Structs containing scalable vectors cannot be used in allocas unless all; fields are the same scalable vector type (e.g. ``{<vscale x 2 x i32>,; <vscale x 2 x i32>}`` contains the same type while ``{<vscale x 2 x i32>,; <vscale x 2 x i64>}`` doesn't). Semantics:; """""""""""""""""""". Memory is allocated; a pointer is returned. The allocated memory is; uninitialized, and loading from uninitialized memory produces an undefined; value. The operation itself is undefined if there is insufficient stack; space for the allocation.'``alloca``'d memory is automatically released; when the function returns. The '``alloca``' instruction is commonly used; to represent automatic variables that must have an address available. When; the function returns (either with the ``ret`` or ``resume`` instructions),; the memory is reclaimed. Allocating zero bytes is legal, but the returned; pointer may not be unique. The order in which memory is allocated (ie.,; which way the stack grows) is not specified. Note that '``alloca``' outside of the alloca address space from the; :ref",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:410183,scalab,scalable,410183,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,2,['scalab'],['scalable']
Performance,"thmetic using ``float``, Clang; does not truncate intermediate operands back to their true type unless the; operand is the result of an explicit cast or assignment. This is generally; much faster but can generate different results from strict operation-by-operation; emulation. Usually the results are more precise. This is permitted by the; C and C++ standards under the rules for excess precision in intermediate operands;; see the discussion of evaluation formats in the C standard and [expr.pre] in; the C++ standard. The use of excess precision can be independently controlled for these two; types with the ``-ffloat16-excess-precision=`` and; ``-fbfloat16-excess-precision=`` options. Valid values include:. * ``none``: meaning to perform strict operation-by-operation emulation; * ``standard``: meaning that excess precision is permitted under the rules; described in the standard, i.e. never across explicit casts or statements; * ``fast``: meaning that excess precision is permitted whenever the; optimizer sees an opportunity to avoid truncations; currently this has no; effect beyond ``standard``. The ``_Float16`` type is an interchange floating type specified in; ISO/IEC TS 18661-3:2015 (""Floating-point extensions for C""). It will; be supported on more targets as they define ABIs for it. The ``__bf16`` type is a non-standard extension, but it generally follows; the rules for arithmetic interchange floating types from ISO/IEC TS; 18661-3:2015. In previous versions of Clang, it was a storage-only type; that forbade arithmetic operations. It will be supported on more targets; as they define ABIs for it. The ``__fp16`` type was originally an ARM extension and is specified; by the `ARM C Language Extensions <https://github.com/ARM-software/acle/releases>`_.; Clang uses the ``binary16`` format from IEEE 754-2008 for ``__fp16``,; not the ARM alternative format. Operators that expect arithmetic operands; immediately promote ``__fp16`` operands to ``float``. It is recommended that",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst:33635,optimiz,optimizer,33635,interpreter/llvm-project/clang/docs/LanguageExtensions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst,1,['optimiz'],['optimizer']
Performance,"thon, for example.; Obvious, but completely wrong, however.; In fact, when it comes to Python, it is even the `wrong question.`. Everything in Python is run-time: modules, classes, functions, etc. are all; run-time constructs.; A Python module that defines a class is a set of instructions to the Python; interpreter that lead to the construction of the desired class object.; A C/C++ extension module that defines a class does the same thing by calling; a succession of Python interpreter Application Programming Interfaces (APIs;; the exact same that Python uses itself internally).; If you use a compile-time binder such as `SWIG`_ or `pybind11`_ to bind a C++; class, then what gets compiled is the series of API calls necessary to; construct a Python-side equivalent at `run-time` (when the module gets; loaded), not the Python class object.; In short, whether a binding is created at ""compile-time"" or at run-time has; no measurable bearing on performance. What does affect performance is the overhead to cross the language barrier.; This consists of unboxing Python objects to extract or convert the underlying; objects or data to something that matches what C++ expects; overload; resolution based on the unboxed arguments; offset calculations; and finally; the actual dispatch.; As a practical matter, overload resolution is the most costly part, followed; by the unboxing and conversion.; Best performance is achieved by specialization of the paths through the; run-time: recognize early the case at hand and select an optimized path.; For that reason, `PyPy`_ is so fast: JIT-ed traces operate on unboxed objects; and resolved overloads are baked into the trace, incurring no further cost.; Similarly, this is why pybind11 is so slow: its code generation is the C++; compiler's template engine, so complex path selection and specialization is; very hard to do in a performance-portable way. In cppyy, a great deal of attention has gone into built-in specialization; paths, which drives its",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/philosophy.rst:1754,perform,performance,1754,bindings/pyroot/cppyy/cppyy/doc/source/philosophy.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/philosophy.rst,1,['perform'],['performance']
Performance,"though %mask were false for those lanes. %masked.a = select <4 x i1> %mask, <4 x float> %a, <4 x float> <float -0.0, float -0.0, float -0.0, float -0.0>; %also.r = call float @llvm.vector.reduce.fadd.v4f32(float %start, <4 x float> %masked.a). .. _int_vp_reduce_mul:. '``llvm.vp.reduce.mul.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare i32 @llvm.vp.reduce.mul.v4i32(i32 <start_value>, <4 x i32> <val>, <4 x i1> <mask>, i32 <vector_length>); declare i16 @llvm.vp.reduce.mul.nxv8i16(i16 <start_value>, <vscale x 8 x i16> <val>, <vscale x 8 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". Predicated integer ``MUL`` reduction of a vector and a scalar starting value,; returning the result as a scalar. Arguments:; """""""""""""""""""". The first operand is the start value of the reduction, which must be a scalar; integer type equal to the result type. The second operand is the vector on; which the reduction is performed and must be a vector of integer values whose; element type is the result/start type. The third operand is the vector mask and; is a vector of boolean values with the same number of elements as the vector; operand. The fourth operand is the explicit vector length of the operation. Semantics:; """""""""""""""""""". The '``llvm.vp.reduce.mul``' intrinsic performs the integer ``MUL`` reduction; (:ref:`llvm.vector.reduce.mul <int_vector_reduce_mul>`) of the vector operand ``val``; on each enabled lane, multiplying it by the scalar ``start_value``. Disabled; lanes are treated as containing the neutral value ``1`` (i.e. having no effect; on the reduction operation). If the vector length is zero, the result is the; start value. To ignore the start value, the neutral value can be used. Examples:; """""""""""""""""". .. code-block:: llvm. %r = call i32 @llvm.vp.reduce.mul.v4i32(i32 %start, <4 x i32> %a, <4 x i1> %mask, i32 %evl); ; %r is equivalent to %also.r, where lanes greater than or equal to %evl; ; are treated as t",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:753182,perform,performed,753182,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performed']
Performance,"tic C++ uses smart pointers to express memory ownership, however in; pre-C++11 code one can often find raw pointers that own heap memory blocks. Imagine that we would like to refactor raw pointers that own memory to; `unique_ptr`. There are multiple ways to design a data flow analysis for this; problem; let's look at one way to do it. For example, we would like to refactor the following code that uses raw; pointers:. ```c++; void UniqueOwnership1() {; int *pi = new int;; if (...) {; Borrow(pi);; delete pi;; } else {; TakeOwnership(pi);; }; }; ```. into code that uses `unique_ptr`:. ```c++; void UniqueOwnership1() {; auto pi = std::make_unique<int>();; if (...) {; Borrow(pi.get());; } else {; TakeOwnership(pi.release());; }; }; ```. This problem can be solved with a lattice in form of map from value declarations; to pointer states:. ![Lattice that identifies candidates for unique_ptr refactoring](DataFlowAnalysisIntroImages/UniquePtrLattice.svg). We can perform the refactoring if at the exit of a function `pi` is; `Compatible`. ```c++; void UniqueOwnership1() {; int *pi; // pi is Compatible; pi = new int; // pi is Defined; if (...) {; Borrow(pi); // pi is Defined; delete pi; // pi is Compatible; } else {; TakeOwnership(pi); // pi is Compatible; }; // pi is Compatible; }; ```. Let's look at an example where the raw pointer owns two different memory blocks:. ```c++; void UniqueOwnership2() {; int *pi = new int; // pi is Defined; Borrow(pi);; delete pi; // pi is Compatible; if (smth) {; pi = new int; // pi is Defined; Borrow(pi);; delete pi; // pi is Compatible; }; // pi is Compatible; }; ```. It can be refactored to use `unique_ptr` like this:. ```c++; void UniqueOwnership2() {; auto pi = make_unique<int>();; Borrow(pi);; if (smth) {; pi = make_unique<int>();; Borrow(pi);; }; }; ```. In the following example, the raw pointer is used to access the heap object; after the ownership has been transferred. ```c++; void UniqueOwnership3() {; int *pi = new int; // pi is Defined",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/DataFlowAnalysisIntro.md:22457,perform,perform,22457,interpreter/llvm-project/clang/docs/DataFlowAnalysisIntro.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/DataFlowAnalysisIntro.md,1,['perform'],['perform']
Performance,"tice. * When `x` is assigned a concrete value, its possible set of values contains; just that specific value. * When `x` is assigned some unknown value, it can have any value. We represent; this fact as `⊤`. * When two control flow paths join, we compute the set union of incoming; values (limiting the number of elements to 3, representing larger sets as; `⊤`). The sets of possible values are influenced by:. * Statements, for example, assignments. * Joins in control flow, for example, ones that appear at the end of ""if""; statements. **Effects of statements** are modeled by what is formally known as a transfer; function. A transfer function takes two arguments: the statement, and the state; of `x` at the previous program point. It produces the state of `x` at the next; program point. For example, the transfer function for assignment ignores the; state at the previous program point:. ```c++; // GIVEN: x is {42; 44}; x = 0;; // CONCLUSION: x is {0}; ```. The transfer function for `+` performs arithmetic on every set member:. ```c++; // GIVEN: x is {42, 44}; x = x + 100;; // CONCLUSION: x is {142, 144}; ```. **Effects of control flow** are modeled by joining the knowledge from all; possible previous program points. ```c++; if (...) {; ...; // GIVEN: x is {42}; } else {; ...; // GIVEN: x is {44}; }; // CONCLUSION: x is {42; 44}; ```. ```c++; // GIVEN: x is {42}; while (...) {; ...; // GIVEN: x is {44}; }; // CONCLUSION: {42; 44}; ```. The predicate that we marked ""given"" is usually called a precondition, and the; conclusion is called a postcondition. In terms of the CFG, we join the information from all predecessor basic blocks. ![Modeling the effects of a CFG basic block](DataFlowAnalysisIntroImages/CFGJoinRule.svg). Putting it all together, to model the effects of a basic block we compute:. ```; out = transfer(basic_block, join(in_1, in_2, ..., in_n)); ```. (Note that there are other ways to write this equation that produce higher; precision analysis results. The trick ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/DataFlowAnalysisIntro.md:8890,perform,performs,8890,interpreter/llvm-project/clang/docs/DataFlowAnalysisIntro.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/DataFlowAnalysisIntro.md,1,['perform'],['performs']
Performance,"tient of the two operands.; This instruction is assumed to execute in the default :ref:`floating-point; environment <floatenv>`.; This instruction can also take any number of :ref:`fast-math; flags <fastmath>`, which are optimization hints to enable otherwise; unsafe floating-point optimizations:. Example:; """""""""""""""". .. code-block:: text. <result> = fdiv float 4.0, %var ; yields float:result = 4.0 / %var. .. _i_urem:. '``urem``' Instruction; ^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. <result> = urem <ty> <op1>, <op2> ; yields ty:result. Overview:; """""""""""""""""". The '``urem``' instruction returns the remainder from the unsigned; division of its two arguments. Arguments:; """""""""""""""""""". The two arguments to the '``urem``' instruction must be; :ref:`integer <t_integer>` or :ref:`vector <t_vector>` of integer values. Both; arguments must have identical types. Semantics:; """""""""""""""""""". This instruction returns the unsigned integer *remainder* of a division.; This instruction always performs an unsigned division to get the; remainder. Note that unsigned integer remainder and signed integer remainder are; distinct operations; for signed integer remainder, use '``srem``'. Taking the remainder of a division by zero is undefined behavior.; For vectors, if any element of the divisor is zero, the operation has; undefined behavior. Example:; """""""""""""""". .. code-block:: text. <result> = urem i32 4, %var ; yields i32:result = 4 % %var. .. _i_srem:. '``srem``' Instruction; ^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. <result> = srem <ty> <op1>, <op2> ; yields ty:result. Overview:; """""""""""""""""". The '``srem``' instruction returns the remainder from the signed; division of its two operands. This instruction can also take; :ref:`vector <t_vector>` versions of the values in which case the elements; must be integers. Arguments:; """""""""""""""""""". The two arguments to the '``srem``' instruction must be; :ref:`integer <t_integer>` or :ref:`vector <t_vector>` of integer values. Both; arguments must have ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:388338,perform,performs,388338,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performs']
Performance,"time functions; that have a hot path and a cold path. The hot path is usually a small piece; of code that doesn't use many registers. The cold path might need to call out to; another function and therefore only needs to preserve the caller-saved; registers, which haven't already been saved by the caller. The; `PreserveMost` calling convention is very similar to the `cold` calling; convention in terms of caller/callee-saved registers, but they are used for; different types of function calls. `coldcc` is for function calls that are; rarely executed, whereas `preserve_mostcc` function calls are intended to be; on the hot path and definitely executed a lot. Furthermore `preserve_mostcc`; doesn't prevent the inliner from inlining the function call. This calling convention will be used by a future version of the ObjectiveC; runtime and should therefore still be considered experimental at this time.; Although this convention was created to optimize certain runtime calls to; the ObjectiveC runtime, it is not limited to this runtime and might be used; by other runtimes in the future too. The current implementation only; supports X86-64, but the intention is to support more architectures in the; future.; ""``preserve_allcc``"" - The `PreserveAll` calling convention; This calling convention attempts to make the code in the caller even less; intrusive than the `PreserveMost` calling convention. This calling; convention also behaves identical to the `C` calling convention on how; arguments and return values are passed, but it uses a different set of; caller/callee-saved registers. This removes the burden of saving and; recovering a large register set before and after the call in the caller. If; the arguments are passed in callee-saved registers, then they will be; preserved by the callee across the call. This doesn't apply for values; returned in callee-saved registers. - On X86-64 the callee preserves all general purpose registers, except for; R11. R11 can be used as a scratch re",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:18256,optimiz,optimize,18256,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['optimiz'],['optimize']
Performance,"time, ROOT inspects the CPU capabilities, and loads the fastest supported version of this computation library.; This means that RooFit can now use vector extensions such as AVX2 without being recompiled, which enables a speed up of up to 4x for certain computations.; Combined with better data access patterns (~3x speed up, ROOT 6.20), computations with optimised PDFs speed up between 4x and 16x. The fast `BatchMode` now also works in combination with multi processing (`NumCPU`) and with binned data (`RooDataHist`). See [Demo notebook in SWAN](https://github.com/hageboeck/rootNotebooks),; [EPJ Web Conf. 245 (2020) 06007](https://www.epj-conferences.org/articles/epjconf/abs/2020/21/epjconf_chep2020_06007/epjconf_chep2020_06007.html),; [arxiv:2012.02746](https://arxiv.org/abs/2012.02746). #### RooBatchCompute Library. The library that contains the optimised computation functions is called `RooBatchCompute`. The PDFs contained in this library are highly optimized, and there is currently work in progress for further optimization using CUDA and multi-threaded computations. If you use PDFs that are not part of the official RooFit, you are very well invited to add them to RooFit by [submitting a ticket](https://github.com/root-project/root/issues/new) or a [pull request](https://github.com/root-project/root/pulls). #### Benefiting from batch computations by overriding `evaluateSpan()`. For PDFs that are not part of RooFit, it is possible to benefit from batch computations without vector extensions. To do so, consult the [RooBatchCompute readme](https://github.com/root-project/root/tree/v6-24-00-patches/roofit/batchcompute). #### Migrating PDFs that override the deprecated `evaluateBatch()`. In case you have created a custom PDF which overrides `evaluateBatch()`, please follow these steps to update your code to the newest version:. 1. Change the signature of the function both in the source and header file:; ```diff; - RooSpan<double> RooGaussian::evaluateBatch(std::size_t be",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v624/index.md:16418,optimiz,optimized,16418,README/ReleaseNotes/v624/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v624/index.md,3,"['multi-thread', 'optimiz']","['multi-threaded', 'optimization', 'optimized']"
Performance,"times (see :ref:`amdgpu-os`), the runtime installs a trap handler that; supports the ``s_trap`` instruction. For usage see:. - :ref:`amdgpu-trap-handler-for-amdhsa-os-v2-table`; - :ref:`amdgpu-trap-handler-for-amdhsa-os-v3-table`; - :ref:`amdgpu-trap-handler-for-amdhsa-os-v4-onwards-table`. .. table:: AMDGPU Trap Handler for AMDHSA OS Code Object V2; :name: amdgpu-trap-handler-for-amdhsa-os-v2-table. =================== =============== =============== =======================================; Usage Code Sequence Trap Handler Description; Inputs; =================== =============== =============== =======================================; reserved ``s_trap 0x00`` Reserved by hardware.; ``debugtrap(arg)`` ``s_trap 0x01`` ``SGPR0-1``: Reserved for Finalizer HSA ``debugtrap``; ``queue_ptr`` intrinsic (not implemented).; ``VGPR0``:; ``arg``; ``llvm.trap`` ``s_trap 0x02`` ``SGPR0-1``: Causes wave to be halted with the PC at; ``queue_ptr`` the trap instruction. The associated; queue is signalled to put it into the; error state. When the queue is put in; the error state, the waves executing; dispatches on the queue will be; terminated.; ``llvm.debugtrap`` ``s_trap 0x03`` *none* - If debugger not enabled then behaves; as a no-operation. The trap handler; is entered and immediately returns to; continue execution of the wavefront.; - If the debugger is enabled, causes; the debug trap to be reported by the; debugger and the wavefront is put in; the halt state with the PC at the; instruction. The debugger must; increment the PC and resume the wave.; reserved ``s_trap 0x04`` Reserved.; reserved ``s_trap 0x05`` Reserved.; reserved ``s_trap 0x06`` Reserved.; reserved ``s_trap 0x07`` Reserved.; reserved ``s_trap 0x08`` Reserved.; reserved ``s_trap 0xfe`` Reserved.; reserved ``s_trap 0xff`` Reserved.; =================== =============== =============== =======================================. .. .. table:: AMDGPU Trap Handler for AMDHSA OS Code Object V3; :name: amdgpu-trap-handler-for",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:381326,queue,queue,381326,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['queue'],['queue']
Performance,"times and memory usage; during linking significantly. With a build machine with sufficient; parallelism, link times tend to dominate critical path of the build, and are; thus worth optimizing. Use CCache and NOT incremental builds; Using ccache materially improves average build times. Incremental builds; can be slightly faster, but introduce the risk of build corruption due to; e.g. state changes, etc... At this point, the recommendation is not to; use incremental builds and instead use ccache as the latter captures the; majority of the benefit with less risk of false positives. One of the non-obvious benefits of using ccache is that it makes the; builder less sensitive to which projects are being monitored vs built.; If a change triggers a build request, but doesn't change the build output; (e.g. doc changes, python utility changes, etc..), the build will entirely; hit in cache and the build request will complete in just the testing time. With multiple workers, it is tempting to try to configure a shared cache; between the workers. Experience to date indicates this is difficult to; well, and that having local per-worker caches gets most of the benefit; anyways. We don't currently recommend shared caches. CCache does depend on the builder hardware having sufficient IO to access; the cache with reasonable access times - i.e. a fast disk, or enough memory; for a RAM cache, etc.. For builders without, incremental may be your best; option, but is likely to require higher ongoing involvement from the; sponsor. Enable batch builds; As a last resort, you can configure your builder to batch build requests.; This makes the build failure notifications markedly less actionable, and; should only be done once all other reasonable measures have been taken. Leave it on the staging buildmaster; While most of this section has been biased towards builders intended for; the main buildmaster, it is worth highlighting that builders can run; indefinitely on the staging buildmaster. Such a",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToAddABuilder.rst:11874,cache,cache,11874,interpreter/llvm-project/llvm/docs/HowToAddABuilder.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToAddABuilder.rst,1,['cache'],['cache']
Performance,"timizes; it with BOLT, use the following CMake configuration:. .. code-block:: console. $ cmake <path to source>/llvm -C <path to source>/clang/cmake/caches/BOLT.cmake. Then, build the BOLT-optimized binary by running the following ninja command:. .. code-block:: console. $ ninja clang-bolt. If you're seeing errors in the build process, try building with a recent; version of Clang/LLVM by setting the CMAKE_C_COMPILER and; CMAKE_CXX_COMPILER flags to the appropriate values. It is also possible to use BOLT on top of PGO and (Thin)LTO for an even more; significant runtime speedup. To configure a three stage PGO build with ThinLTO; that optimizes the resulting binary with BOLT, use the following CMake; configuration command:. .. code-block:: console. $ cmake -G Ninja <path to source>/llvm \; -C <path to source>/clang/cmake/caches/BOLT-PGO.cmake \; -DBOOTSTRAP_LLVM_ENABLE_LLD=ON \; -DBOOTSTRAP_BOOTSTRAP_LLVM_ENABLE_LLD=ON \; -DPGO_INSTRUMENT_LTO=Thin. Then, to build the final optimized binary, build the stage2-clang-bolt target:. .. code-block:: console. $ ninja stage2-clang-bolt. 3-Stage Non-Determinism; =======================. In the ancient lore of compilers non-determinism is like the multi-headed hydra.; Whenever its head pops up, terror and chaos ensue. Historically one of the tests to verify that a compiler was deterministic would; be a three stage build. The idea of a three stage build is you take your sources; and build a compiler (stage1), then use that compiler to rebuild the sources; (stage2), then you use that compiler to rebuild the sources a third time; (stage3) with an identical configuration to the stage2 build. At the end of; this, you have a stage2 and stage3 compiler that should be bit-for-bit; identical. You can perform one of these 3-stage builds with LLVM and clang using the; following commands:. .. code-block:: console. $ cmake -G Ninja -C <path to source>/clang/cmake/caches/3-stage.cmake <path to source>/llvm; $ ninja stage3. After the build you",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AdvancedBuilds.rst:11266,optimiz,optimized,11266,interpreter/llvm-project/llvm/docs/AdvancedBuilds.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AdvancedBuilds.rst,1,['optimiz'],['optimized']
Performance,"tin_isnormal``; * ``__builtin_nan``; * ``__builtin_nans``; * ``__builtin_parity``; * ``__builtin_parityl``; * ``__builtin_parityll``; * ``__builtin_popcount``; * ``__builtin_popcountl``; * ``__builtin_popcountll``; * ``__builtin_rotateleft8``; * ``__builtin_rotateleft16``; * ``__builtin_rotateleft32``; * ``__builtin_rotateleft64``; * ``__builtin_rotateright8``; * ``__builtin_rotateright16``; * ``__builtin_rotateright32``; * ``__builtin_rotateright64``. The following x86-specific intrinsics can be used in constant expressions:. * ``_bit_scan_forward``; * ``_bit_scan_reverse``; * ``__bsfd``; * ``__bsfq``; * ``__bsrd``; * ``__bsrq``; * ``__bswap``; * ``__bswapd``; * ``__bswap64``; * ``__bswapq``; * ``_castf32_u32``; * ``_castf64_u64``; * ``_castu32_f32``; * ``_castu64_f64``; * ``__lzcnt16``; * ``__lzcnt``; * ``__lzcnt64``; * ``_mm_popcnt_u32``; * ``_mm_popcnt_u64``; * ``_popcnt32``; * ``_popcnt64``; * ``__popcntd``; * ``__popcntq``; * ``__popcnt16``; * ``__popcnt``; * ``__popcnt64``; * ``__rolb``; * ``__rolw``; * ``__rold``; * ``__rolq``; * ``__rorb``; * ``__rorw``; * ``__rord``; * ``__rorq``; * ``_rotl``; * ``_rotr``; * ``_rotwl``; * ``_rotwr``; * ``_lrotl``; * ``_lrotr``. Debugging the Compiler; ======================. Clang supports a number of pragma directives that help debugging the compiler itself.; Syntax is the following: `#pragma clang __debug <command> <arguments>`.; Note, all of debugging pragmas are subject to change. `dump`; ------; Accepts either a single identifier or an expression. When a single identifier is passed,; the lookup results for the identifier are printed to `stderr`. When an expression is passed,; the AST for the expression is printed to `stderr`. The expression is an unevaluated operand,; so things like overload resolution and template instantiations are performed,; but the expression has no runtime effects.; Type- and value-dependent expressions are not supported yet. This facility is designed to aid with testing name lookup machinery.; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst:196465,perform,performed,196465,interpreter/llvm-project/clang/docs/LanguageExtensions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst,1,['perform'],['performed']
Performance,"ting it with the selected code generator. If the; selected code generator crashes, ``bugpoint`` starts the `crash debugger`_ on; the code generator. Otherwise, if the resulting output differs from the; reference output, it assumes the difference resulted from a code generator; failure, and starts the `code generator debugger`_. Finally, if the output of the selected code generator matches the reference; output, ``bugpoint`` runs the test program after all of the LLVM passes have; been applied to it. If its output differs from the reference output, it assumes; the difference resulted from a failure in one of the LLVM passes, and enters the; `miscompilation debugger`_. Otherwise, there is no problem ``bugpoint`` can; debug. .. _crash debugger:. Crash debugger; --------------. If an optimizer or code generator crashes, ``bugpoint`` will try as hard as it; can to reduce the list of passes (for optimizer crashes) and the size of the; test program. First, ``bugpoint`` figures out which combination of optimizer; passes triggers the bug. This is useful when debugging a problem exposed by; ``opt``, for example, because it runs over 38 passes. Next, ``bugpoint`` tries removing functions from the test program, to reduce its; size. Usually it is able to reduce a test program to a single function, when; debugging intraprocedural optimizations. Once the number of functions has been; reduced, it attempts to delete various edges in the control flow graph, to; reduce the size of the function as much as possible. Finally, ``bugpoint``; deletes any individual LLVM instructions whose absence does not eliminate the; failure. At the end, ``bugpoint`` should tell you what passes crash, give you a; bitcode file, and give you instructions on how to reproduce the failure with; ``opt`` or ``llc``. .. _code generator debugger:. Code generator debugger; -----------------------. The code generator debugger attempts to narrow down the amount of code that is; being miscompiled by the selected code ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Bugpoint.rst:3290,optimiz,optimizer,3290,interpreter/llvm-project/llvm/docs/Bugpoint.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Bugpoint.rst,1,['optimiz'],['optimizer']
Performance,"ting-point; semantics and floating-point exception behavior to be specified; for a section of the source code. This pragma can only appear at file or; namespace scope, within a language linkage specification or at the start of a; compound statement (excluding comments). When used within a compound statement,; the pragma is active within the scope of the compound statement. This pragma; is modeled after a Microsoft pragma with the same spelling and syntax. For; pragmas specified at file or namespace scope, or within a language linkage; specification, a stack is supported so that the ``pragma float_control``; settings can be pushed or popped. When ``pragma float_control(precise, on)`` is enabled, the section of code; governed by the pragma uses precise floating point semantics, effectively; ``-ffast-math`` is disabled and ``-ffp-contract=on``; (fused multiply add) is enabled. This pragma enables ``-fmath-errno``. When ``pragma float_control(precise, off)`` is enabled, unsafe-floating point; optimizations are enabled in the section of code governed by the pragma.; Effectively ``-ffast-math`` is enabled and ``-ffp-contract=fast``. This pragma; disables ``-fmath-errno``. When ``pragma float_control(except, on)`` is enabled, the section of code; governed by the pragma behaves as though the command-line option; ``-ffp-exception-behavior=strict`` is enabled,; when ``pragma float_control(except, off)`` is enabled, the section of code; governed by the pragma behaves as though the command-line option; ``-ffp-exception-behavior=ignore`` is enabled. The full syntax this pragma supports is; ``float_control(except|precise, on|off [, push])`` and; ``float_control(push|pop)``.; The ``push`` and ``pop`` forms, including using ``push`` as the optional; third argument, can only occur at file scope. .. code-block:: c++. for(...) {; // This block will be compiled with -fno-fast-math and -ffp-contract=on; #pragma float_control(precise, on); a = b[i] * c[i] + e;; }. Specifying an attribute",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst:175918,optimiz,optimizations,175918,interpreter/llvm-project/clang/docs/LanguageExtensions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst,1,['optimiz'],['optimizations']
Performance,"tion (LoopVectorize/SLP); entirely when the a function has either of the ``aarch64_pstate_sm_enabled``,; ``aarch64_pstate_sm_body`` or ``aarch64_pstate_sm_compatible`` attributes,; in order to avoid the use of vector instructions. Later on we'll aim to relax these restrictions to enable scalable; auto-vectorization with a subset of streaming-compatible instructions, but that; requires changes to the CostModel, Legalization and SelectionDAG lowering. We will also emit diagnostics in Clang to prevent the use of; non-streaming(-compatible) operations, e.g. through ACLE intrinsics, when a; function is decorated with the streaming mode attributes. Other things to consider; ------------------------. * Inlining must be disabled when the call-site needs to toggle PSTATE.SM or; when the callee's function body is executed in a different streaming mode than; its caller. This is needed because function calls are the boundaries for; streaming mode changes. * Tail call optimization must be disabled when the call-site needs to toggle; PSTATE.SM, such that the caller can restore the original value of PSTATE.SM. 3. Handling PSTATE.ZA; =====================. In contrast to PSTATE.SM, enabling PSTATE.ZA does not affect the SVE vector; length and also doesn't clobber FP/AdvSIMD/SVE registers. This means it is safe; to toggle PSTATE.ZA using intrinsics. This also makes it simpler to setup a; lazy-save mechanism for calls to private-ZA functions (i.e. functions that may; either directly or indirectly clobber ZA state). For the purpose of handling functions marked with ``aarch64_pstate_za_new``,; we have introduced a new LLVM IR pass (SMEABIPass) that is run just before; SelectionDAG. Any such functions dealt with by this pass are marked with; ``aarch64_expanded_pstate_za``. Setting up a lazy-save; ----------------------. Committing a lazy-save; ----------------------. Exception handling and ZA; -------------------------. 4. Types; ========. AArch64 Predicate-as-Counter Type; ------------",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AArch64SME.rst:16644,optimiz,optimization,16644,interpreter/llvm-project/llvm/docs/AArch64SME.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AArch64SME.rst,1,['optimiz'],['optimization']
Performance,"tion can then be used to speed-up fitting. It is not enabled by default, but it can be enabled by callig `TF1::SetVectorized(true)` or using the `""VEC""` option in the; constructor of TF1, when ROOT has been built with VecCore and one vectorization library such as Vc. ; - Added new auto-binning algorithm, referred to as `power-2`, which uses power of 2 bin widths to create bins; that are mergeable. The target use-case is support for auto-binning in multi-process or multi-thread execution,; e.g. `TDataFrame`, without the need of a synchronization point.; The new `power-2` algorithm is activated by setting the new `TH1::kAutoBinPTwo` status bit on the histogram.; The tutorial `tutorials/multicore/mt304_fillHistos.C` gives an example of how to use the functionality with; `TThreadedObject<TH1D>` . The `power-2` binning is currently available only for 1D histograms. ## Math Libraries; - The Fitting functions now support vectorization and parallelization.; - Added padding in the fit data classes for correct loading of SIMD arrays. ## RooFit Libraries. - Apply several fixes from the ATLAS Higgs combination branch of RooFit. These fixes include; - fix for computing the contraint normalization. This requires now the option GlobalObservables when creating the NLL.; - All the `RooAbsPdf::createNLL` used in The RooStats classes have been updated to include the `GlobalObservables` option.; - Remove the `Roo1DMomentMorphFunction` and replace it with `RooMomentMorphFunction` and `RooMomentMorphFunctionND`. ## TMVA Library. - Improvement and fixes in ROCCurve class.; - Add support for event weights in the DNN; - Add in the DNN the option to use a validation data set independent of the training/test set used for training the DNN.; - Add option to suppress correlation outputs; - Improvements in the support for multi-class classification.; - Improvements in the Gradient Boostig Trees; - Deprecate the TMVA DNN Reference Implementation. Support now only CPU and GPU implementations. . ## 2",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v612/index.md:18355,load,loading,18355,README/ReleaseNotes/v612/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v612/index.md,1,['load'],['loading']
Performance,"tion has an; IsAStoreBarrier and IsALoadBarrier flag. These flags are mca specific and; default to false for every instruction. If any instruction should have either of; these flags set, it should be done within the target's InstrPostProcess class.; For an example, look at the `X86InstrPostProcess::postProcessInstruction` method; within `llvm/lib/Target/X86/MCA/X86CustomBehaviour.cpp`. A load/store barrier consumes one entry of the load/store queue. A load/store; barrier enforces ordering of loads/stores. A younger load cannot pass a load; barrier. Also, a younger store cannot pass a store barrier. A younger load; has to wait for the memory/load barrier to execute. A load/store barrier is; ""executed"" when it becomes the oldest entry in the load/store queue(s). That; also means, by construction, all of the older loads/stores have been executed. In conclusion, the full set of load/store consistency rules are:. #. A store may not pass a previous store.; #. A store may not pass a previous load (regardless of ``-noalias``).; #. A store has to wait until an older store barrier is fully executed.; #. A load may pass a previous load.; #. A load may not pass a previous store unless ``-noalias`` is set.; #. A load has to wait until an older load barrier is fully executed. In-order Issue and Execute; """"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""; In-order processors are modelled as a single ``InOrderIssueStage`` stage. It; bypasses Dispatch, Scheduler and Load/Store unit. Instructions are issued as; soon as their operand registers are available and resource requirements are; met. Multiple instructions can be issued in one cycle according to the value of; the ``IssueWidth`` parameter in LLVM's scheduling model. Once issued, an instruction is moved to ``IssuedInst`` set until it is ready to; retire. :program:`llvm-mca` ensures that writes are committed in-order. However,; an instruction is allowed to commit writes and retire out-of-order if; ``RetireOOO`` property is true for at least one",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst:42841,load,load,42841,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,1,['load'],['load']
Performance,"tion in this model is divided into the following stages:. 1. `Instruction Selection`_ --- This phase determines an efficient way to; express the input LLVM code in the target instruction set. This stage; produces the initial code for the program in the target instruction set, then; makes use of virtual registers in SSA form and physical registers that; represent any required register assignments due to target constraints or; calling conventions. This step turns the LLVM code into a DAG of target; instructions. 2. `Scheduling and Formation`_ --- This phase takes the DAG of target; instructions produced by the instruction selection phase, determines an; ordering of the instructions, then emits the instructions as :raw-html:`<tt>`; `MachineInstr`_\s :raw-html:`</tt>` with that ordering. Note that we; describe this in the `instruction selection section`_ because it operates on; a `SelectionDAG`_. 3. `SSA-based Machine Code Optimizations`_ --- This optional stage consists of a; series of machine-code optimizations that operate on the SSA-form produced by; the instruction selector. Optimizations like modulo-scheduling or peephole; optimization work here. 4. `Register Allocation`_ --- The target code is transformed from an infinite; virtual register file in SSA form to the concrete register file used by the; target. This phase introduces spill code and eliminates all virtual register; references from the program. 5. `Prolog/Epilog Code Insertion`_ --- Once the machine code has been generated; for the function and the amount of stack space required is known (used for; LLVM alloca's and spill slots), the prolog and epilog code for the function; can be inserted and ""abstract stack location references"" can be eliminated.; This stage is responsible for implementing optimizations like frame-pointer; elimination and stack packing. 6. `Late Machine Code Optimizations`_ --- Optimizations that operate on ""final""; machine code can go here, such as spill code scheduling and peephole; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst:6121,optimiz,optimizations,6121,interpreter/llvm-project/llvm/docs/CodeGenerator.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst,1,['optimiz'],['optimizations']
Performance,"tion of each member function; and most data members. An update to the users manual is foreseen shortly after the 5.20; release. RooWorkspace. A new feature has been added that allows to persist source code of RooFit classes that; are not in ROOT distribution inside a RooWorkspace to facilitate sharing; of custom code with others. To import code of custom classes call. RooWorkspace::importClassCode(). after importing the objects themselves into the workspace. For all classes; that are compiled with ACliC RooWorkspace can automatically find the source; code using the ROOT TClass interface. For custom classes that are compiled; externally and loaded into ROOT as shared library it might be necessary to; provide the location of the source files manually using the static RooWorkspace; member functions addClassDeclImportDir() and addClassImplImportDir().; ; When a TFile with a RooWorkspace containing source code is opened in a ROOT; session that does not have the class code already loaded for the classes; contained in the workspace, the code in the workspace is written to file,; compiled and loaded into the ROOT session on the fly. The code repository of RooWorkspace is designed to handle classes that; have either their own implementation and header file, or are part of a group; of classes that share a common header and implementation file. More complicated; structuring of source code into files is not supported. ; ; Also new accessors have been added for discrete-valued functions catfunc(); and stored category functions are now also printed under their own heading in Print(); Parameterized ranges. It is now possible to use RooAbsReal derived functions as range definition for variables; to construct ranges that vary as function of another variable. For example. RooRealVar x(""x"",""x"",-10,10) ; // variable with fixed range [-10,10] ; RooRealVar y(""y"",""y"",0,20) ; // variable with fixed range [-10,10] ; ; RooFormulaVar x_lo(""x_lo"",""y-20"",y) ; ; RooFormulaVar x_hi(""x_hi"",""sin(y)*",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v520/index.html:2555,load,loaded,2555,roofit/doc/v520/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v520/index.html,4,['load'],['loaded']
Performance,"tion of the; code path ([mitigation G-2 +; V1-1](https://developer.amd.com/wp-content/resources/Managing-Speculation-on-AMD-Processors.pdf)). However, this relies on finding and enumerating all possible points in code; which could be attacked to leak information. While in some cases static; analysis is effective at doing this at scale, in many cases it still relies on; human judgement to evaluate whether code might be vulnerable. Especially for; software systems which receive less detailed scrutiny but remain sensitive to; these attacks, this seems like an impractical security model. We need an; automatic and systematic mitigation strategy. ### Automatic `lfence` on Conditional Edges. A natural way to scale up the existing hand-coded mitigations is simply to; inject an `lfence` instruction into both the target and fallthrough; destinations of every conditional branch. This ensures that no predicate or; bounds check can be bypassed speculatively. However, the performance overhead; of this approach is, simply put, catastrophic. Yet it remains the only truly; ""secure by default"" approach known prior to this effort and serves as the; baseline for performance. One attempt to address the performance overhead of this and make it more; realistic to deploy is [MSVC's /Qspectre; switch](https://blogs.msdn.microsoft.com/vcblog/2018/01/15/spectre-mitigations-in-msvc/).; Their technique is to use static analysis within the compiler to only insert; `lfence` instructions into conditional edges at risk of attack. However,; [initial](https://arstechnica.com/gadgets/2018/02/microsofts-compiler-level-spectre-fix-shows-how-hard-this-problem-will-be-to-solve/); [analysis](https://www.paulkocher.com/doc/MicrosoftCompilerSpectreMitigation.html); has shown that this approach is incomplete and only catches a small and limited; subset of attackable patterns which happen to resemble very closely the initial; proofs of concept. As such, while its performance is acceptable, it does not; appear ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md:45384,perform,performance,45384,interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,1,['perform'],['performance']
Performance,"tion that we can only; have one ""top level"" command at a time to reduce the number of; changes necessary. Here's the sample program we'll be compiling:. .. code-block:: python. def fib(x); if x < 3 then; 1; else; fib(x-1)+fib(x-2);. fib(10). Why is this a hard problem?; ===========================. Debug information is a hard problem for a few different reasons - mostly; centered around optimized code. First, optimization makes keeping source; locations more difficult. In LLVM IR we keep the original source location; for each IR level instruction on the instruction. Optimization passes; should keep the source locations for newly created instructions, but merged; instructions only get to keep a single location - this can cause jumping; around when stepping through optimized programs. Secondly, optimization; can move variables in ways that are either optimized out, shared in memory; with other variables, or difficult to track. For the purposes of this; tutorial we're going to avoid optimization (as you'll see with one of the; next sets of patches). Ahead-of-Time Compilation Mode; ==============================. To highlight only the aspects of adding debug information to a source; language without needing to worry about the complexities of JIT debugging; we're going to make a few changes to Kaleidoscope to support compiling; the IR emitted by the front end into a simple standalone program that; you can execute, debug, and see results. First we make our anonymous function that contains our top level; statement be our ""main"":. .. code-block:: udiff. - auto Proto = std::make_unique<PrototypeAST>("""", std::vector<std::string>());; + auto Proto = std::make_unique<PrototypeAST>(""main"", std::vector<std::string>());. just with the simple change of giving it a name. Then we're going to remove the command line code wherever it exists:. .. code-block:: udiff. @@ -1129,7 +1129,6 @@ static void HandleTopLevelExpression() {; /// top ::= definition | external | expression | ';'; stat",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst:2357,optimiz,optimization,2357,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst,1,['optimiz'],['optimization']
Performance,"tion, and; motion of ``Instructions``. The update API is being made on an as-needed basis.; If you'd like examples, ``GVNHoist`` and ``LICM`` are users of ``MemorySSA``\ s; update API.; Note that adding new ``MemoryDef``\ s (by calling ``insertDef``) can be a; time-consuming update, if the new access triggers many ``MemoryPhi`` insertions and; renaming (optimization invalidation) of many ``MemoryAccesses``\ es. Phi placement; ^^^^^^^^^^^^^. ``MemorySSA`` only places ``MemoryPhi``\ s where they're actually; needed. That is, it is a pruned SSA form, like LLVM's SSA form. For; example, consider:. .. code-block:: llvm. define void @foo() {; entry:; %p1 = alloca i8; %p2 = alloca i8; %p3 = alloca i8; ; 1 = MemoryDef(liveOnEntry); store i8 0, ptr %p3; br label %while.cond. while.cond:; ; 3 = MemoryPhi({%0,1},{if.end,2}); br i1 undef, label %if.then, label %if.else. if.then:; br label %if.end. if.else:; br label %if.end. if.end:; ; MemoryUse(1); %1 = load i8, ptr %p1; ; 2 = MemoryDef(3); store i8 2, ptr %p2; ; MemoryUse(1); %2 = load i8, ptr %p3; br label %while.cond; }. Because we removed the stores from ``if.then`` and ``if.else``, a ``MemoryPhi``; for ``if.end`` would be pointless, so we don't place one. So, if you need to; place a ``MemoryDef`` in ``if.then`` or ``if.else``, you'll need to also create; a ``MemoryPhi`` for ``if.end``. If it turns out that this is a large burden, we can just place ``MemoryPhi``\ s; everywhere. Because we have Walkers that are capable of optimizing above said; phis, doing so shouldn't prohibit optimizations. Non-Goals; ---------. ``MemorySSA`` is meant to reason about the relation between memory; operations, and enable quicker querying.; It isn't meant to be the single source of truth for all potential memory-related; optimizations. Specifically, care must be taken when trying to use ``MemorySSA``; to reason about atomic or volatile operations, as in:. .. code-block:: llvm. define i8 @foo(ptr %a) {; entry:; br i1 undef, label %if.then, labe",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:15214,load,load,15214,interpreter/llvm-project/llvm/docs/MemorySSA.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst,2,['load'],['load']
Performance,"tion. MemorySanitizer may still instrument such functions to; avoid false positives. This attribute may not be supported by other compilers,; so we suggest to use it together with ``__has_feature(memory_sanitizer)``. ``__attribute__((disable_sanitizer_instrumentation))``; --------------------------------------------------------. The ``disable_sanitizer_instrumentation`` attribute can be applied to functions; to prevent all kinds of instrumentation. As a result, it may introduce false; positives and therefore should be used with care, and only if absolutely; required; for example for certain code that cannot tolerate any instrumentation; and resulting side-effects. This attribute overrides ``no_sanitize(""memory"")``. Ignorelist; ----------. MemorySanitizer supports ``src`` and ``fun`` entity types in; :doc:`SanitizerSpecialCaseList`, that can be used to relax MemorySanitizer; checks for certain source files and functions. All ""Use of uninitialized value""; warnings will be suppressed and all values loaded from memory will be; considered fully initialized. Report symbolization; ====================. MemorySanitizer uses an external symbolizer to print files and line numbers in; reports. Make sure that ``llvm-symbolizer`` binary is in ``PATH``,; or set environment variable ``MSAN_SYMBOLIZER_PATH`` to point to it. .. _msan-origins:. Origin Tracking; ===============. MemorySanitizer can track origins of uninitialized values, similar to; Valgrind's --track-origins option. This feature is enabled by; ``-fsanitize-memory-track-origins=2`` (or simply; ``-fsanitize-memory-track-origins``) Clang option. With the code from; the example above,. .. code-block:: console. % cat umr2.cc; #include <stdio.h>. int main(int argc, char** argv) {; int* a = new int[10];; a[5] = 0;; volatile int b = a[argc];; if (b); printf(""xx\n"");; return 0;; }. % clang -fsanitize=memory -fsanitize-memory-track-origins=2 -fno-omit-frame-pointer -g -O2 umr2.cc; % ./a.out; WARNING: MemorySanitizer: use-of-unin",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/MemorySanitizer.rst:3447,load,loaded,3447,interpreter/llvm-project/clang/docs/MemorySanitizer.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/MemorySanitizer.rst,1,['load'],['loaded']
Performance,"tion:: -all-views. Enable all the view. .. option:: -instruction-tables. Prints resource pressure information based on the static information; available from the processor model. This differs from the resource pressure; view because it doesn't require that the code is simulated. It instead prints; the theoretical uniform distribution of resource pressure for every; instruction in sequence. .. option:: -bottleneck-analysis. Print information about bottlenecks that affect the throughput. This analysis; can be expensive, and it is disabled by default. Bottlenecks are highlighted; in the summary view. Bottleneck analysis is currently not supported for; processors with an in-order backend. .. option:: -json. Print the requested views in valid JSON format. The instructions and the; processor resources are printed as members of special top level JSON objects.; The individual views refer to them by index. However, not all views are; currently supported. For example, the report from the bottleneck analysis is; not printed out in JSON. All the default views are currently supported. .. option:: -disable-cb. Force usage of the generic CustomBehaviour and InstrPostProcess classes rather; than using the target specific implementation. The generic classes never; detect any custom hazards or make any post processing modifications to; instructions. .. option:: -disable-im. Force usage of the generic InstrumentManager rather than using the target; specific implementation. The generic class creates Instruments that provide; no extra information, and InstrumentManager never overrides the default; schedule class for a given instruction. EXIT STATUS; -----------. :program:`llvm-mca` returns 0 on success. Otherwise, an error message is printed; to standard error, and the tool returns 1. USING MARKERS TO ANALYZE SPECIFIC CODE BLOCKS; ---------------------------------------------; :program:`llvm-mca` allows for the optional usage of special code comments to; mark regions of the assembly cod",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst:7281,bottleneck,bottleneck,7281,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,1,['bottleneck'],['bottleneck']
Performance,"tion; of the pull values for each parameter has been modified accordingly. Alternatively, it is possible to specify constraints to both RooAbsPdf::fitTo() and the RooMCStudy constructor; using the ExternalConstraint() named argument to supply constraint p.d.f.s that are not part of the 'master'; p.d.f but rather an ad-hoc supplied external constraint. The argument supplied to ExternalConstraint() should; be (a set of) constraint p.d.f(s), rather than (a set of) parameters for which internal constraint p.d.f.s should; be picked up. New operator class RooLinearMorph. A new numeric operator class RooLinearMorph has been added that provides a continuous; transformation between two p.d.f.s shapes in terms of a linear parameter alpha. The algorithm ; for histograms is described in the paper by Alex Read in NUM A 425 (1999) 357-369 ; 'Linear interpolation of histograms'. The implementation in RooLinearMorph is for; continuous functions. . // Observable and sampling binning to be used by RooLinearMorph (""cache""); RooRealVar x(""x"",""x"",-20,20) ;; x.setBins(1000,""cache"") ;. // End point shapes : a gaussian on one end, a polynomial on the other; RooGaussian f1(""f1"",""f1"",x,RooConst(-10),RooConst(2)) ;; RooPolynomial f2(""f2"",""f2"",x,RooArgSet(RooConst(-0.03),RooConst(-0.001))) ;. // Interpolation parameter: rlm=f1 at alpha=0, rlm=f2 at alpha=1; RooRealVar alpha(""alpha"",""alpha"",0,1.0) ;; RooLinearMorph rlm(""rlm"",""rlm"",g1,g2,x,alpha) ;. // Plot halfway shape; alpha=0.5; RooPlot* frame = x.frame() ;; rlm.plotOn(frame) ;. In short the algorithm works as follows: for both f1(x) and f2(x), the cumulative distribution; functions F1(x) and F2(x) are calculated. One finds takes a value 'y' of both c.d.fs and ; determines the corresponding x values x1,x2 at which F1(x1)=F2(x2)=y. The value of the interpolated ; p.d.f fbar(x) is then calculated as fbar(alpha*x1+(1-alpha)*x2) = f1(x1)*f2(x2) / ( alpha*f2(x2) + ; (1-alpha)*f1(x1) ). Given that it is not easily possible to calculate the value o",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v520/index.html:10180,cache,cache,10180,roofit/doc/v520/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v520/index.html,2,['cache'],['cache']
Performance,"tional reads from occurring when reading events out of sequence.; By setting TTree::SetClusterPrefetch(), an entire clusters will be loaded into memory, rather than single baskets.; By setting the MaxVirtualSize of the tree to a negative value, previous clusters will be retained; (the absolute value of MaxVirtualSize indicates how many additional clusters will be kept in memory).; - Added ```TBranchProxy::GetEntries``` to support leaflist variable size array and added ```TBranchProxy::GetArrayLength```.; - In ```TBranch::Streamer``` insured that we never steam already basket already written to disk. ### TDataFrame. #### New features; - Add `Alias`, a facility to specify an alternative name for a given column: `auto histo = mytdf.Alias(""myAlias"", ""myColumn"").Histo1D(""myAlias"");`. Especially useful for pyROOT users to deal with column names that are not valid C++ identifiers (e.g. `Filter(""1branch > 0"") --> Alias(""1branch"", ""branch1"").Filter(""branch1 > 0"")`.; - Add `Cache`, a facility to cache `TDataFrame`s in memory. All or some columns can be cached. Two versions of the method are proposed: one which allows to explicitly list the types of the columns and another one allowing to let the system infer them (the same mechanism of the `Snapshot` method). Only columns containing instances of classes which have a copy constructor can be cached.; - Add `DefineSlot`, a `Define` transformation that is aware of the multi-threading slot where the workload is executed; - Add `DefineSlotEntry`, a `Define` transformation that is aware of the multi-threading slot and of the current entry number; - Add `GetColumnsNames`: users can now get the names of the available columns coming from trees, data sources or `Define`d columns; - Add `OnPartialResult` and `OnPartialResultSlot`: users can now register one or more functions to be executed on partial results of TDF actions during the event loop.; This mechanism is meant to be used to inspect partial results of the analysis or print useful",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v612/index.md:10978,cache,cache,10978,README/ReleaseNotes/v612/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v612/index.md,1,['cache'],['cache']
Performance,"tions (:ref:`fneg <i_fneg>`, :ref:`fadd <i_fadd>`,; :ref:`fsub <i_fsub>`, :ref:`fmul <i_fmul>`, :ref:`fdiv <i_fdiv>`,; :ref:`frem <i_frem>`, :ref:`fcmp <i_fcmp>`), :ref:`phi <i_phi>`,; :ref:`select <i_select>` and :ref:`call <i_call>`; may use the following flags to enable otherwise unsafe; floating-point transformations. ``nnan``; No NaNs - Allow optimizations to assume the arguments and result are not; NaN. If an argument is a nan, or the result would be a nan, it produces; a :ref:`poison value <poisonvalues>` instead. ``ninf``; No Infs - Allow optimizations to assume the arguments and result are not; +/-Inf. If an argument is +/-Inf, or the result would be +/-Inf, it; produces a :ref:`poison value <poisonvalues>` instead. ``nsz``; No Signed Zeros - Allow optimizations to treat the sign of a zero; argument or zero result as insignificant. This does not imply that -0.0; is poison and/or guaranteed to not exist in the operation. ``arcp``; Allow Reciprocal - Allow optimizations to use the reciprocal of an; argument rather than perform division. ``contract``; Allow floating-point contraction (e.g. fusing a multiply followed by an; addition into a fused multiply-and-add). This does not enable reassociating; to form arbitrary contractions. For example, ``(a*b) + (c*d) + e`` can not; be transformed into ``(a*b) + ((c*d) + e)`` to create two fma operations. .. _fastmath_afn:. ``afn``; Approximate functions - Allow substitution of approximate calculations for; functions (sin, log, sqrt, etc). See floating-point intrinsic definitions; for places where this can apply to LLVM's intrinsic math functions. ``reassoc``; Allow reassociation transformations for floating-point instructions.; This may dramatically change results in floating-point. ``fast``; This flag implies all of the others. .. _uselistorder:. Use-list Order Directives; -------------------------. Use-list directives encode the in-memory order of each use-list, allowing the; order to be recreated. ``<order-indexes>``",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:162476,optimiz,optimizations,162476,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,2,"['optimiz', 'perform']","['optimizations', 'perform']"
Performance,"tions have been performed. Additionally, on targets; where it is profitable, the loop could be transformed to count down to zero; (the ""do loop"" optimization). ``inline``: Function Integration/Inlining; -----------------------------------------. Bottom-up inlining of functions into callees. .. _passes-instcombine:. ``instcombine``: Combine redundant instructions; -----------------------------------------------. Combine instructions to form fewer, simple instructions. This pass does not; modify the CFG. This pass is where algebraic simplification happens. This pass combines things like:. .. code-block:: llvm. %Y = add i32 %X, 1; %Z = add i32 %Y, 1. into:. .. code-block:: llvm. %Z = add i32 %X, 2. This is a simple worklist driven algorithm. This pass guarantees that the following canonicalizations are performed on the; program:. #. If a binary operator has a constant operand, it is moved to the right-hand; side.; #. Bitwise operators with constant operands are always grouped so that shifts; are performed first, then ``or``\ s, then ``and``\ s, then ``xor``\ s.; #. Compare instructions are converted from ``<``, ``>``, ``≤``, or ``≥`` to; ``=`` or ``≠`` if possible.; #. All ``cmp`` instructions on boolean values are replaced with logical; operations.; #. ``add X, X`` is represented as ``mul X, 2`` ⇒ ``shl X, 1``; #. Multiplies with a constant power-of-two argument are transformed into; shifts.; #. … etc. This pass can also simplify calls to specific well-known function calls (e.g.; runtime library functions). For example, a call ``exit(3)`` that occurs within; the ``main()`` function can be transformed into simply ``return 3``. Whether or; not library calls are simplified is controlled by the; :ref:`-function-attrs <passes-function-attrs>` pass and LLVM's knowledge of; library calls on different targets. .. _passes-aggressive-instcombine:. ``aggressive-instcombine``: Combine expression patterns; --------------------------------------------------------. Combine expression",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:19983,perform,performed,19983,interpreter/llvm-project/llvm/docs/Passes.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst,1,['perform'],['performed']
Performance,"tions. 5. ``DW_OP_xderef`` *Deprecated*. ``DW_OP_xderef`` pops two stack entries. The first must be an integral type; value that represents an address A. The second must be an integral type; value that represents a target architecture specific address space; identifier AS. The operation is equivalent to performing ``DW_OP_swap;; DW_OP_LLVM_form_aspace_address; DW_OP_deref``. The value V retrieved is left; on the stack with the generic type. *This operation is deprecated as the* ``DW_OP_LLVM_form_aspace_address``; *operation can be used and provides greater expressiveness.*. 6. ``DW_OP_xderef_size`` *Deprecated*. ``DW_OP_xderef_size`` has a single 1-byte unsigned integral constant that; represents a byte result size S. It pops two stack entries. The first must be an integral type value that; represents an address A. The second must be an integral type value that; represents a target architecture specific address space identifier AS. The operation is equivalent to performing ``DW_OP_swap;; DW_OP_LLVM_form_aspace_address; DW_OP_deref_size S``. The zero-extended; value V retrieved is left on the stack with the generic type. *This operation is deprecated as the* ``DW_OP_LLVM_form_aspace_address``; *operation can be used and provides greater expressiveness.*. 7. ``DW_OP_xderef_type`` *Deprecated*. ``DW_OP_xderef_type`` has two operands. The first is a 1-byte unsigned; integral constant S. The second operand is an unsigned LEB128 integer DR; that represents the byte offset of a debugging information entry D relative; to the beginning of the current compilation unit, that provides the type T; of the result value. It pops two stack entries. The first must be an integral type value that; represents an address A. The second must be an integral type value that; represents a target architecture specific address space identifier AS. The operation is equivalent to performing ``DW_OP_swap;; DW_OP_LLVM_form_aspace_address; DW_OP_deref_type S DR``. The value V; retrieved is left on th",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst:94148,perform,performing,94148,interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst,1,['perform'],['performing']
Performance,"tions. ``-fmodule-name=module-id``; Consider a source file as a part of the given module. ``-fmodule-map-file=<file>``; Load the given module map file if a header from its directory or one of its subdirectories is loaded. ``-fmodules-search-all``; If a symbol is not found, search modules referenced in the current module maps but not imported for symbols, so the error message can reference the module by name. Note that if the global module index has not been built before, this might take some time as it needs to build all the modules. Note that this option doesn't apply in module builds, to avoid the recursion. ``-fno-implicit-modules``; All modules used by the build must be specified with ``-fmodule-file``. ``-fmodule-file=[<name>=]<file>``; Specify the mapping of module names to precompiled module files. If the; name is omitted, then the module file is loaded whether actually required; or not. If the name is specified, then the mapping is treated as another; prebuilt module search mechanism (in addition to ``-fprebuilt-module-path``); and the module is only loaded if required. Note that in this case the; specified file also overrides this module's paths that might be embedded; in other precompiled module files. ``-fprebuilt-module-path=<directory>``; Specify the path to the prebuilt modules. If specified, we will look for modules in this directory for a given top-level module name. We don't need a module map for loading prebuilt modules in this directory and the compiler will not try to rebuild these modules. This can be specified multiple times. ``-fprebuilt-implicit-modules``; Enable prebuilt implicit modules. If a prebuilt module is not found in the; prebuilt modules paths (specified via ``-fprebuilt-module-path``), we will; look for a matching implicit module in the prebuilt modules paths. -cc1 Options; ~~~~~~~~~~~~. ``-fmodules-strict-context-hash``; Enables hashing of all compiler options that could impact the semantics of a; module in an implicit build. This ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/Modules.rst:17466,load,loaded,17466,interpreter/llvm-project/clang/docs/Modules.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/Modules.rst,1,['load'],['loaded']
Performance,"tiple ; function calls could be made from the try block. In this case, trivial ; optimization could merge the two basic blocks. TryHandler is the code ; that actually determines the type of exception, based on the Exception object; itself. For this discussion, assume that the exception object contains *at; least*:. 1. A pointer to the RTTI info for the contained object; 2. A pointer to the dtor for the contained object; 3. The contained object itself. Note that it is necessary to maintain #1 & #2 in the exception object itself; because objects without virtual function tables may be thrown (as in this ; example). Assuming this, TryHandler would look something like this:. TryHandler: ; Exception *E = getThreadLocalException();; switch (E->RTTIType) {; case IntRTTIInfo:; ...int Stuff... // The action to perform from the catch block; break;; case DoubleRTTIInfo:; ...double Stuff... // The action to perform from the catch block; goto TryCleanup // This catch block rethrows the exception; break; // Redundant, eliminated by the optimizer; default:; goto TryCleanup // Exception not caught, rethrow; }. // Exception was consumed; if (E->dtor); E->dtor(E->object) // Invoke the dtor on the object if it exists; goto EndTry // Continue mainline code... And that is all there is to it. The throw(E) function would then be implemented like this (which may be ; inlined into the caller through standard optimization):. function throw(Exception *E) {; // Get the start of the stack trace...; %frame %f = call getStackCurrentFrame(). // Get the label information that corresponds to it; label * %L = call getFrameLabel(%f); while (%L == 0 && !isFirstFrame(%f)) {; // Loop until a cleanup handler is found; %f = call getNextFrame(%f); %L = call getFrameLabel(%f); }. if (%L != 0) {; call setThreadLocalException(E) // Allow handlers access to this...; call doNonLocalBranch(%L); }; // No handler found!; call BlowUp() // Ends up calling the terminate() method in use; }. That's a brief rundown of how",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-05-18-ExceptionHandling.txt:6009,perform,perform,6009,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-05-18-ExceptionHandling.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-05-18-ExceptionHandling.txt,2,"['optimiz', 'perform']","['optimizer', 'perform']"
Performance,"tity. Name lookup; When writing a chained precompiled header, Clang attempts to write only; information that has changed from the precompiled header on which it is; based. This changes the lookup algorithm for the various tables, such as the; :ref:`identifier table <pchinternals-ident-table>`: the search starts at the; most-recent precompiled header. If no entry is found, lookup then proceeds; to the identifier table in the precompiled header it depends on, and so one.; Once a lookup succeeds, that result is considered definitive, overriding any; results from earlier precompiled headers. Update records; There are various ways in which a later precompiled header can modify the; entities described in an earlier precompiled header. For example, later; precompiled headers can add entries into the various name-lookup tables for; the translation unit or namespaces, or add new categories to an Objective-C; class. Each of these updates is captured in an ""update record"" that is; stored in the chained precompiled header file and will be loaded along with; the original entity. .. _pchinternals-modules:. Modules; -------. Modules generalize the chained precompiled header model yet further, from a; linear chain of precompiled headers to an arbitrary directed acyclic graph; (DAG) of AST files. All of the same techniques used to make chained; precompiled headers work --- ID number, name lookup, update records --- are; shared with modules. However, the DAG nature of modules introduce a number of; additional complications to the model:. Numbering of IDs; The simple, linear numbering scheme used in chained precompiled headers falls; apart with the module DAG, because different modules may end up with; different numbering schemes for entities they imported from common shared; modules. To account for this, each module file provides information about; which modules it depends on and which ID numbers it assigned to the entities; in those modules, as well as which ID numbers it took for it",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/PCHInternals.rst:27029,load,loaded,27029,interpreter/llvm-project/clang/docs/PCHInternals.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/PCHInternals.rst,1,['load'],['loaded']
Performance,"tive in a file. The ""CHECK-EMPTY:"" directive; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~. If you need to check that the next line has nothing on it, not even whitespace,; you can use the ""``CHECK-EMPTY:``"" directive. .. code-block:: llvm. declare void @foo(). declare void @bar(); ; CHECK: foo; ; CHECK-EMPTY:; ; CHECK-NEXT: bar. Just like ""``CHECK-NEXT:``"" the directive will fail if there is more than one; newline before it finds the next blank line, and it cannot be the first; directive in a file. The ""CHECK-NOT:"" directive; ~~~~~~~~~~~~~~~~~~~~~~~~~~. The ""``CHECK-NOT:``"" directive is used to verify that a string doesn't occur; between two matches (or before the first match, or after the last match). For; example, to verify that a load is removed by a transformation, a test like this; can be used:. .. code-block:: llvm. define i8 @coerce_offset0(i32 %V, i32* %P) {; store i32 %V, i32* %P. %P2 = bitcast i32* %P to i8*; %P3 = getelementptr i8* %P2, i32 2. %A = load i8* %P3; ret i8 %A; ; CHECK: @coerce_offset0; ; CHECK-NOT: load; ; CHECK: ret i8; }. The ""CHECK-COUNT:"" directive; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~. If you need to match multiple lines with the same pattern over and over again; you can repeat a plain ``CHECK:`` as many times as needed. If that looks too; boring you can instead use a counted check ""``CHECK-COUNT-<num>:``"", where; ``<num>`` is a positive decimal number. It will match the pattern exactly; ``<num>`` times, no more and no less. If you specified a custom check prefix,; just use ""``<PREFIX>-COUNT-<num>:``"" for the same effect.; Here is a simple example:. .. code-block:: text. Loop at depth 1; Loop at depth 1; Loop at depth 1; Loop at depth 1; Loop at depth 2; Loop at depth 3. ; CHECK-COUNT-6: Loop at depth {{[0-9]+}}; ; CHECK-NOT: Loop at depth {{[0-9]+}}. The ""CHECK-DAG:"" directive; ~~~~~~~~~~~~~~~~~~~~~~~~~~. If it's necessary to match strings that don't occur in a strictly sequential; order, ""``CHECK-DAG:``"" could be used to verify them between two matches (or; b",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/FileCheck.rst:17037,load,load,17037,interpreter/llvm-project/llvm/docs/CommandGuide/FileCheck.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/FileCheck.rst,2,['load'],['load']
Performance,"tl;libcxx;compiler-rt;openmp;llvm-libgcc""); set(LLVM_ENABLE_RUNTIMES """" CACHE STRING; ""Semicolon-separated list of runtimes to build, or \""all\"" (${LLVM_DEFAULT_RUNTIMES}). Supported runtimes are ${LLVM_SUPPORTED_RUNTIMES}.""); if(LLVM_ENABLE_RUNTIMES STREQUAL ""all""); set(LLVM_ENABLE_RUNTIMES ${LLVM_DEFAULT_RUNTIMES}); endif(); foreach(proj IN LISTS LLVM_ENABLE_RUNTIMES); if (NOT ""${proj}"" IN_LIST LLVM_SUPPORTED_RUNTIMES); message(FATAL_ERROR ""Runtime \""${proj}\"" is not a supported runtime. Supported runtimes are: ${LLVM_SUPPORTED_RUNTIMES}""); endif(); endforeach(). if (""libc"" IN_LIST LLVM_ENABLE_RUNTIMES); # To build the libc runtime, we need to be able to build few libc build; # tools from the ""libc"" project. So, we add it to the list of enabled; # projects.; if (NOT ""libc"" IN_LIST LLVM_ENABLE_PROJECTS); message(STATUS ""Enabling libc project to build libc build tools""); list(APPEND LLVM_ENABLE_PROJECTS ""libc""); endif(); endif(). # LLVM_ENABLE_PROJECTS_USED is `ON` if the user has ever used the; # `LLVM_ENABLE_PROJECTS` CMake cache variable. This exists for; # several reasons:; #; # * As an indicator that the `LLVM_ENABLE_PROJECTS` list is now the single; # source of truth for which projects to build. This means we will ignore user; # supplied `LLVM_TOOL_<project>_BUILD` CMake cache variables and overwrite; # them.; #; # * The case where the user previously had `LLVM_ENABLE_PROJECTS` set to a; # non-empty list but now the user wishes to disable building all other projects; # by setting `LLVM_ENABLE_PROJECTS` to an empty string. In that case we still; # need to set the `LLVM_TOOL_${upper_proj}_BUILD` variables so that we disable; # building all the projects that were previously enabled.; set(LLVM_ENABLE_PROJECTS_USED OFF CACHE BOOL """"); mark_as_advanced(LLVM_ENABLE_PROJECTS_USED). if (LLVM_ENABLE_PROJECTS_USED OR NOT LLVM_ENABLE_PROJECTS STREQUAL """"); set(LLVM_ENABLE_PROJECTS_USED ON CACHE BOOL """" FORCE); foreach(proj ${LLVM_KNOWN_PROJECTS} ${LLVM_EXTERNAL_PROJECTS})",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/CMakeLists.txt:7939,cache,cache,7939,interpreter/llvm-project/llvm/CMakeLists.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/CMakeLists.txt,1,['cache'],['cache']
Performance,"tmia. I think option 2 is better but the current register; allocator cannot allocate a chunk of registers at a time. A feasible temporary solution is to use specific physical registers at the; lowering time for small (<= 4 words?) transfer size. * ARM CSRet calling convention requires the hidden argument to be returned by; the callee. //===---------------------------------------------------------------------===//. We can definitely do a better job on BB placements to eliminate some branches.; It's very common to see llvm generated assembly code that looks like this:. LBB3:; ...; LBB4:; ...; beq LBB3; b LBB2. If BB4 is the only predecessor of BB3, then we can emit BB3 after BB4. We can; then eliminate beq and turn the unconditional branch to LBB2 to a bne. See McCat/18-imp/ComputeBoundingBoxes for an example. //===---------------------------------------------------------------------===//. Pre-/post- indexed load / stores:. 1) We should not make the pre/post- indexed load/store transform if the base ptr; is guaranteed to be live beyond the load/store. This can happen if the base; ptr is live out of the block we are performing the optimization. e.g. mov r1, r2; ldr r3, [r1], #4; ... vs. ldr r3, [r2]; add r1, r2, #4; ... In most cases, this is just a wasted optimization. However, sometimes it can; negatively impact the performance because two-address code is more restrictive; when it comes to scheduling. Unfortunately, liveout information is currently unavailable during DAG combine; time. 2) Consider spliting a indexed load / store into a pair of add/sub + load/store; to solve #1 (in TwoAddressInstructionPass.cpp). 3) Enhance LSR to generate more opportunities for indexed ops. 4) Once we added support for multiple result patterns, write indexed loads; patterns instead of C++ instruction selection code. 5) Use VLDM / VSTM to emulate indexed FP load / store. //===---------------------------------------------------------------------===//. Implement support for some more tr",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:8272,load,load,8272,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,4,['load'],['load']
Performance,"tml#aec50335293c45a507d347c604bf9651f); ### Uniquely identifying RooArgSet and RooDataSet objects. Before v6.28, it was ensured that no `RooArgSet` and `RooDataSet` objects on the heap were located at an address that had already been used for an instance of the same class before.; With v6.28, this is not guaranteed anymore.; Hence, if your code uses pointer comparisons to uniquely identify RooArgSet or RooDataSet instances, please consider using the new `RooArgSet::uniqueId()` or `RooAbsData::uniqueId()`. ### Introducing binned likelihood fit optimization in HistFactory. In a binned likelihood fit, it is possible to skip the PDF normalization when; the unnormalized binned PDF can be interpreted directly in terms of event; yields. This is now done by default for HistFactory models, which; results in great speedups for binned fits with many channels. Some RooFit users; like ATLAS were already using this for a long time. To disable this optimization when using the `hist2workspace` executable, add the `-disable_binned_fit_optimization` command line argument.; Directly in C++, you can also set the `binnedFitOptimization` to `false` in the; HistFactory configuration as follows:; ```C++; RooStats::HistFactory::MakeModelAndMeasurementFast(measurement, {.binnedFitOptimization=false});; ```; If your compiler doesn't support aggregate initialization with designators, you; need to create and edit the configuration struct explicitely:; ```C++; RooStats::HistFactory::HistoToWorkspaceFactoryFast::Configuration hfCfg;; hfCfg.binnedFitOptimization = false;; RooStats::HistFactory::MakeModelAndMeasurementFast(measurement, hfCfg);; ```. ### Disable copy assignment for RooAbsArg and derived types. Copy assignment for RooAbsArgs was implemented in an unexpected and; inconsistent way. While one would expect that the copy assignment is copying; the object, it said in the documentation of `RooAbsArg::operator=` that it will; ""assign all boolean and string properties of the original bject. Tr",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v628/index.md:20521,optimiz,optimization,20521,README/ReleaseNotes/v628/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v628/index.md,1,['optimiz'],['optimization']
Performance,"tml:`</tr>`; :raw-html:`<tr>`; :raw-html:`<td>sm_21</td>`; :raw-html:`<td align=""left"">Set shader model/compute capability to 2.1</td>`; :raw-html:`</tr>`; :raw-html:`<tr>`; :raw-html:`<td>sm_30</td>`; :raw-html:`<td align=""left"">Set shader model/compute capability to 3.0</td>`; :raw-html:`</tr>`; :raw-html:`<tr>`; :raw-html:`<td>sm_35</td>`; :raw-html:`<td align=""left"">Set shader model/compute capability to 3.5</td>`; :raw-html:`</tr>`; :raw-html:`<tr>`; :raw-html:`<td>ptx30</td>`; :raw-html:`<td align=""left"">Target PTX 3.0</td>`; :raw-html:`</tr>`; :raw-html:`<tr>`; :raw-html:`<td>ptx31</td>`; :raw-html:`<td align=""left"">Target PTX 3.1</td>`; :raw-html:`</tr>`; :raw-html:`</table>`. The extended Berkeley Packet Filter (eBPF) backend; --------------------------------------------------. Extended BPF (or eBPF) is similar to the original (""classic"") BPF (cBPF) used; to filter network packets. The; `bpf() system call <http://man7.org/linux/man-pages/man2/bpf.2.html>`_; performs a range of operations related to eBPF. For both cBPF and eBPF; programs, the Linux kernel statically analyzes the programs before loading; them, in order to ensure that they cannot harm the running system. eBPF is; a 64-bit RISC instruction set designed for one to one mapping to 64-bit CPUs.; Opcodes are 8-bit encoded, and 87 instructions are defined. There are 10; registers, grouped by function as outlined below. ::. R0 return value from in-kernel functions; exit value for eBPF program; R1 - R5 function call arguments to in-kernel functions; R6 - R9 callee-saved registers preserved by in-kernel functions; R10 stack frame pointer (read only). Instruction encoding (arithmetic and jump); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; eBPF is reusing most of the opcode encoding from classic to simplify conversion; of classic BPF to eBPF. For arithmetic and jump instructions the 8-bit 'code'; field is divided into three parts:. ::. +----------------+--------+--------------------+; | 4 bits | 1 bit | 3 ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst:102752,perform,performs,102752,interpreter/llvm-project/llvm/docs/CodeGenerator.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst,1,['perform'],['performs']
Performance,"to 'CPU Time / number of threads' AND 'Compressed Throughput' is lower than expected; for your storage medium: this would imply that your CPU threads aren't decompressing data as fast as your storage; medium can provide it, and so decompression is the bottleneck.; The best way to decrease your runtime would be to utilise a system with a faster CPU, or make use; use of more threads when running, or use a compression algorithm with a higher decompression rate such as LZ4,; possibly at the cost of some extra file size. ### A note on caching. If your data is stored on a local disk, the system may cache some/all of the file in memory after it is; first read. If this is realistic of how your analysis will run - then there is no concern. However, if; you expect to only read files once in a while - and as such the files are unlikely to be in the cache -; consider clearing the cache before running rootreadspeed.; On Linux this can be done by running 'echo 3 > /proc/sys/vm/drop_caches' as a superuser,; or a specific file can be dropped from the cache with; `dd of=<FILENAME> oflag=nocache conv=notrunc,fdatasync count=0 > /dev/null 2>&1`. ### Known overhead of TTreeReader, RDataFrame. `rootreadspeed` is designed to read all data present in the specified branches, trees and files at the highest; possible speed. When the application bottleneck is not in the computations performed by analysis logic,; higher-level interfaces built on top of TTree such as TTreeReader and RDataFrame are known to add a significant; runtime overhead with respect to the runtimes reported by `rootreadspeed` (up to a factor 2). In realistic analysis; applications it has been observed that a large part of that overhead is compensated by the ability of TTreeReader and; RDataFrame to read branch values selectively, based on event cuts, and this overhead will be reduced significantly; when using RDataFrame in conjunction with RNTuple.; See also [this talk](https://indico.cern.ch/e/PPP138) (slides 16 to 19).; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/readspeed/README.md:3921,cache,cache,3921,tree/readspeed/README.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/readspeed/README.md,3,"['bottleneck', 'cache', 'perform']","['bottleneck', 'cache', 'performed']"
Performance,"to Quagliani, LPNHE, CNRS/IN2P3, Sorbonne Université,\; Fons Rademakers, CERN/SFT,\; Oksana Shadura, Nebraska,\; Enric Tejedor Saavedra, CERN/SFT,\; Matevz Tadel, UCSD/CMS,\; Vassil Vassilev, Princeton/CMS,\; Wouter Verkerke, NIKHEF/Atlas,\; Stefan Wunsch, CERN/SFT. ## Deprecation and Removal. - `ROOT::GetImplicitMTPoolSize` has been deprecated in favor of the newly added `ROOT::GetThreadPoolSize` and; will be removed in v6.24.; - Manually setting `TThreadedObject::fgMaxSlots` is deprecated: TThreadedObject now increases the number of slots; on-demand rather than running out and throwing an exception. ## Core Libraries. - ROOT comes with C++ Modules enabled. More details about the technology found [here](../../README.CXXMODULES.md).; - The `ACLiC` can be configured to pass options to the `rootcling` invocation by enabling in the `.rootrc` the `ACLiC.ExtraRootclingFlags [-opts]` line.; - A call to `ROOT::EnableThreadSafety` is not required before using `TThreadExecutor` or `TTreeProcessorMT` anymore; - `TTreeProcessorMT` does not silently activate implicit multi-threading features anymore. An explicit call to; `ROOT::EnableImplicitMT` is required instead; - `TTreeProcessorMT` now has a constructor argument to set the number of threads for its thread-pool. ## I/O Libraries. ## TTree Libraries. - A new status bit was added to `TTree`: `kEntriesReshuffled`, which indicates a `TTree` that is the output of the; processing of another tree during which its entry order has been changed (this can happen, for instance, when; processing a tree in a multi-thread application). To avoid silent entry number mismatches, trees with this bit set; cannot add friend trees nor can be added as friends, unless the friend `TTree` has an appropriate `TTreeIndex`. ## Histogram Libraries. ## Math Libraries. ## RooFit Libraries. ### RooWorkspace::Import() for Python; `RooWorkspace.import()` cannot be used in Python, since it is a reserved keyword. Users therefore had to resort; to; getattr(works",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v622/index.md:1863,multi-thread,multi-threading,1863,README/ReleaseNotes/v622/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v622/index.md,1,['multi-thread'],['multi-threading']
Performance,"to RDTSC. On Alpha, it; should map to RPCC. As the backing counters overflow quickly (on the; order of 9 seconds on alpha), this should only be used for small; timings. Semantics:; """""""""""""""""""". When directly supported, reading the cycle counter should not modify any; memory. Implementations are allowed to either return an application; specific value or a system wide value. On backends without support, this; is lowered to a constant 0. Note that runtime support may be conditional on the privilege-level code is; running at and the host platform. '``llvm.clear_cache``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare void @llvm.clear_cache(ptr, ptr). Overview:; """""""""""""""""". The '``llvm.clear_cache``' intrinsic ensures visibility of modifications; in the specified range to the execution unit of the processor. On; targets with non-unified instruction and data cache, the implementation; flushes the instruction cache. Semantics:; """""""""""""""""""". On platforms with coherent instruction and data caches (e.g. x86), this; intrinsic is a nop. On platforms with non-coherent instruction and data; cache (e.g. ARM, MIPS), the intrinsic is lowered either to appropriate; instructions or a system call, if cache flushing requires special; privileges. The default behavior is to emit a call to ``__clear_cache`` from the run; time library. This intrinsic does *not* empty the instruction pipeline. Modifications; of the current function are outside the scope of the intrinsic. '``llvm.instrprof.increment``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare void @llvm.instrprof.increment(ptr <name>, i64 <hash>,; i32 <num-counters>, i32 <index>). Overview:; """""""""""""""""". The '``llvm.instrprof.increment``' intrinsic can be emitted by a; frontend for use with instrumentation based profiling. These will be; lowered by the ``-instrprof`` pass to generate execution counts of a; program at runtime. Arguments:; """""""""""""""""""". The first argument is a pointer",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:526312,cache,caches,526312,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['cache'],['caches']
Performance,"to TS3WebFile by allowing the security token to be given.; * Resolve an issue when space is freed in a large `ROOT` file and a TDirectory is updated and stored the lower (less than 2GB) freed portion of the file [ROOT-8055]. - ##### TBufferJSON:; + support data members with `//[fN]` comment; + preliminary support of STL containers; + JSON data can be produced with `TObject::SaveAs()` method. ## TTree Libraries. * TChains can now be histogrammed without any C++ code, using the command line tool `rootdrawtree`. It is based on the new class `TSimpleAnalysis`.; * Do not automatically setup read cache during `TTree::Fill()`. This fixes [ROOT-8031].; * Make sure the option ""PARA"" in `TTree::Draw` is used with at least tow variables [ROOT-8196].; * The with `goff` option one can use as many variables as needed. There no more; limitation, like with the options `para`and `candle`.; * Fix detection of errors that appears in nested TTreeFormula [ROOT-8218]; * Better basket size optimization by taking into account meta data and rounding up to next 512 bytes, ensuring a complete cluster fits into a single basket. ### Fast Cloning. We added a cache specifically for the fast option of the TTreeCloner to significantly reduce the run-time when fast-cloning remote files to address [ROOT-5078]. It can be controlled from the `TTreeCloner`, `TTree::CopyEntries` or `hadd` interfaces. The new cache is enabled by default, to update the size of the cache or disable it from `TTreeCloner` use: `TTreeCloner::SetCacheSize`. To do the same from `TTree::CopyEntries` add to the option string ""cachesize=SIZE"". To update the size of the cache or disable it from `hadd`, use the command line option `-cachesize SIZE`. `SIZE` shouyld be given in number bytes and can be expressed in 'human readable form' (number followed by size unit like MB, MiB, GB or GiB, etc. or SIZE can be set zero to disable the cache. ### Other Changes. * Update `TChain::LoadTree` so that the user call back routine is actually call",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v608/index.md:9328,optimiz,optimization,9328,README/ReleaseNotes/v608/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v608/index.md,1,['optimiz'],['optimization']
Performance,"to ``-fsigned-zeros``. .. option:: -f[no-]associative-math. Allow floating point operations to be reassociated.; Defaults to ``-fno-associative-math``. .. option:: -f[no-]reciprocal-math. Allow division operations to be transformed into multiplication by a; reciprocal. This can be significantly faster than an ordinary division; but can also have significantly less precision. Defaults to; ``-fno-reciprocal-math``. .. option:: -f[no-]unsafe-math-optimizations. Allow unsafe floating-point optimizations.; ``-funsafe-math-optimizations`` also implies:. * ``-fapprox-func``; * ``-fassociative-math``; * ``-freciprocal-math``; * ``-fno-signed-zeros``; * ``-fno-trapping-math``; * ``-ffp-contract=fast``. ``-fno-unsafe-math-optimizations`` implies:. * ``-fno-approx-func``; * ``-fno-associative-math``; * ``-fno-reciprocal-math``; * ``-fsigned-zeros``; * ``-ftrapping-math``; * ``-ffp-contract=on``; * ``-fdenormal-fp-math=ieee``. There is ambiguity about how ``-ffp-contract``,; ``-funsafe-math-optimizations``, and ``-fno-unsafe-math-optimizations``; behave when combined. Explanation in :option:`-fno-fast-math` also applies; to these options. Defaults to ``-fno-unsafe-math-optimizations``. .. option:: -f[no-]finite-math-only. Allow floating-point optimizations that assume arguments and results are; not NaNs or +-Inf. ``-ffinite-math-only`` defines the; ``__FINITE_MATH_ONLY__`` preprocessor macro.; ``-ffinite-math-only`` implies:. * ``-fno-honor-infinities``; * ``-fno-honor-nans``. ``-ffno-inite-math-only`` implies:. * ``-fhonor-infinities``; * ``-fhonor-nans``. Defaults to ``-fno-finite-math-only``. .. option:: -f[no-]rounding-math. Force floating-point operations to honor the dynamically-set rounding mode by default. The result of a floating-point operation often cannot be exactly represented in the result type and therefore must be rounded. IEEE 754 describes different rounding modes that control how to perform this rounding, not all of which are supported by all implementations.",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:59775,optimiz,optimizations,59775,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,2,['optimiz'],['optimizations']
Performance,"to ``On`` if you want to; enforce a target appearing in only one distribution and umbrella distributions; being consistent with target distributions. We strongly encourage looking at ``clang/cmake/caches/MultiDistributionExample.cmake``; as an example of configuring multiple distributions. Special Notes for Library-only Distributions; --------------------------------------------. One of the most powerful features of LLVM is its library-first design mentality; and the way you can compose a wide variety of tools using different portions of; LLVM. Even in this situation using *BUILD_SHARED_LIBS* is not supported. If you; want to distribute LLVM as a shared library for use in a tool, the recommended; method is using *LLVM_BUILD_LLVM_DYLIB*, and you can use *LLVM_DYLIB_COMPONENTS*; to configure which LLVM components are part of libLLVM.; Note: *LLVM_BUILD_LLVM_DYLIB* is not available on Windows. Options for Optimizing LLVM; ===========================. There are four main build optimizations that our CMake build system supports.; When performing a bootstrap build it is not beneficial to do anything other than; setting *CMAKE_BUILD_TYPE* to ``Release`` for the stage-1 compiler. This is; because the more intensive optimizations are expensive to perform and the; stage-1 compiler is thrown away. All of the further options described should be; set on the stage-2 compiler either using a CMake cache file, or by prefixing the; option with *BOOTSTRAP_*. The first and simplest to use is the compiler optimization level by setting the; *CMAKE_BUILD_TYPE* option. The main values of interest are ``Release`` or; ``RelWithDebInfo``. By default the ``Release`` option uses the ``-O3``; optimization level, and ``RelWithDebInfo`` uses ``-O2``. If you want to generate; debug information and use ``-O3`` you can override the; *CMAKE_<LANG>_FLAGS_RELWITHDEBINFO* option for C and CXX.; DistributionExample.cmake does this. Another easy to use option is Link-Time-Optimization. You can set the; *LL",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BuildingADistribution.rst:7560,optimiz,optimizations,7560,interpreter/llvm-project/llvm/docs/BuildingADistribution.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BuildingADistribution.rst,1,['optimiz'],['optimizations']
Performance,"to a property not of retainable object; pointer type has the same behavior it does outside of ARC: it requires the; property type to be some sort of pointer and permits the use of modifiers other; than ``assign``. These modifiers only affect the synthesized getter and; setter; direct accesses to the ivar (even if synthesized) still have primitive; semantics, and the value in the ivar will not be automatically released during; deallocation. .. _arc.ownership.semantics:. Semantics; ---------. There are five :arc-term:`managed operations` which may be performed on an; object of retainable object pointer type. Each qualifier specifies different; semantics for each of these operations. It is still undefined behavior to; access an object outside of its lifetime. A load or store with ""primitive semantics"" has the same semantics as the; respective operation would have on an ``void*`` lvalue with the same alignment; and non-ownership qualification. :arc-term:`Reading` occurs when performing a lvalue-to-rvalue conversion on an; object lvalue. * For ``__weak`` objects, the current pointee is retained and then released at; the end of the current full-expression. In particular, messaging a ``__weak``; object keeps the object retained until the end of the full expression. .. code-block:: objc. __weak MyObject *weakObj;. void foo() {; // weakObj is retained before the message send and released at the end of; // the full expression.; [weakObj m];; }. This must execute atomically with respect to assignments and to the final; release of the pointee.; * For all other objects, the lvalue is loaded with primitive semantics. :arc-term:`Assignment` occurs when evaluating an assignment operator. The; semantics vary based on the qualification:. * For ``__strong`` objects, the new pointee is first retained; second, the; lvalue is loaded with primitive semantics; third, the new pointee is stored; into the lvalue with primitive semantics; and finally, the old pointee is; released. This is not p",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst:37887,perform,performing,37887,interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,1,['perform'],['performing']
Performance,"to a valid; object. Performs the complete sequence for assigning to a ``__strong`` object of; non-block type [*]_. Equivalent to the following code:. .. code-block:: objc. void objc_storeStrong(id *object, id value) {; id oldValue = *object;; value = [value retain];; *object = value;; [oldValue release];; }. .. [*] This does not imply that a ``__strong`` object of block type is an; invalid argument to this function. Rather it implies that an ``objc_retain``; and not an ``objc_retainBlock`` operation will be emitted if the argument is; a block. .. _arc.runtime.objc_storeWeak:. ``id objc_storeWeak(id *object, id value);``; --------------------------------------------. *Precondition:* ``object`` is a valid pointer which either contains a null; pointer or has been registered as a ``__weak`` object. ``value`` is null or a; pointer to a valid object. If ``value`` is a null pointer or the object to which it points has begun; deallocation, ``object`` is assigned null and unregistered as a ``__weak``; object. Otherwise, ``object`` is registered as a ``__weak`` object or has its; registration updated to point to ``value``. Returns the value of ``object`` after the call. .. _arc.runtime.objc_unsafeClaimAutoreleasedReturnValue:. ``id objc_unsafeClaimAutoreleasedReturnValue(id value);``; ---------------------------------------------------------. *Precondition:* ``value`` is null or a pointer to a valid object. If ``value`` is null, this call has no effect. Otherwise, it attempts to; accept a hand off of a retain count from a call to; :ref:`objc_autoreleaseReturnValue <arc.runtime.objc_autoreleaseReturnValue>` on; ``value`` in a recently-called function or something it tail-calls (in a manner; similar to :ref:`objc_retainAutoreleasedReturnValue; <arc.runtime.objc_retainAutoreleasedReturnValue>`). If that succeeds,; it performs a release operation exactly like :ref:`objc_release; <arc.runtime.objc_release>`. If the handoff fails, this call has no effect. Always returns ``value``. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst:118077,perform,performs,118077,interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,1,['perform'],['performs']
Performance,"to avoid terminating the header block of; the loop earlier than necessary. If the terminator of the loop header; block is a loop exiting conditional branch, the effectiveness of LICM will; be limited for loads not in the header. (This is due to the fact that LLVM; may not know such a load is safe to speculatively execute and thus can't; lift an otherwise loop invariant load unless it can prove the exiting; condition is not taken.) It can be profitable, in some cases, to emit such; instructions into the header even if they are not used along a rarely; executed path that exits the loop. This guidance specifically does not; apply if the condition which terminates the loop header is itself invariant,; or can be easily discharged by inspecting the loop index variables. #. In hot loops, consider duplicating instructions from small basic blocks; which end in highly predictable terminators into their successor blocks.; If a hot successor block contains instructions which can be vectorized; with the duplicated ones, this can provide a noticeable throughput; improvement. Note that this is not always profitable and does involve a; potentially large increase in code size. #. When checking a value against a constant, emit the check using a consistent; comparison type. The GVN pass *will* optimize redundant equalities even if; the type of comparison is inverted, but GVN only runs late in the pipeline.; As a result, you may miss the opportunity to run other important; optimizations. #. Avoid using arithmetic intrinsics unless you are *required* by your source; language specification to emit a particular code sequence. The optimizer; is quite good at reasoning about general control flow and arithmetic, it is; not anywhere near as strong at reasoning about the various intrinsics. If; profitable for code generation purposes, the optimizer will likely form the; intrinsics itself late in the optimization pipeline. It is *very* rarely; profitable to emit these directly in the language f",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Frontend/PerformanceTips.rst:8319,throughput,throughput,8319,interpreter/llvm-project/llvm/docs/Frontend/PerformanceTips.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Frontend/PerformanceTips.rst,1,['throughput'],['throughput']
Performance,"to be duplicated as well. For example, when the intrinsic is used inside a loop body, and that loop is; unrolled, the associated noalias scope must also be duplicated. Otherwise, the; noalias property it signifies would spill across loop iterations, whereas it; was only valid within a single iteration. .. code-block:: llvm. ; This examples shows two possible positions for noalias.decl and how they impact the semantics:; ; If it is outside the loop (Version 1), then %a and %b are noalias across *all* iterations.; ; If it is inside the loop (Version 2), then %a and %b are noalias only within *one* iteration.; declare void @decl_in_loop(ptr %a.base, ptr %b.base) {; entry:; ; call void @llvm.experimental.noalias.scope.decl(metadata !2) ; Version 1: noalias decl outside loop; br label %loop. loop:; %a = phi ptr [ %a.base, %entry ], [ %a.inc, %loop ]; %b = phi ptr [ %b.base, %entry ], [ %b.inc, %loop ]; ; call void @llvm.experimental.noalias.scope.decl(metadata !2) ; Version 2: noalias decl inside loop; %val = load i8, ptr %a, !alias.scope !2; store i8 %val, ptr %b, !noalias !2; %a.inc = getelementptr inbounds i8, ptr %a, i64 1; %b.inc = getelementptr inbounds i8, ptr %b, i64 1; %cond = call i1 @cond(); br i1 %cond, label %loop, label %exit. exit:; ret void; }. !0 = !{!0} ; domain; !1 = !{!1, !0} ; scope; !2 = !{!1} ; scope list. Multiple calls to `@llvm.experimental.noalias.scope.decl` for the same scope; are possible, but one should never dominate another. Violations are pointed out; by the verifier as they indicate a problem in either a transformation pass or; the input. Floating Point Environment Manipulation intrinsics; --------------------------------------------------. These functions read or write floating point environment, such as rounding; mode or state of floating point exceptions. Altering the floating point; environment requires special care. See :ref:`Floating Point Environment <floatenv>`. .. _int_get_rounding:. '``llvm.get.rounding``' Intrinsic; ^^^^^^^^^",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:915090,load,load,915090,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,"to be effectively optimized. It; can often be useful to write a quick C program with the semantics you're trying; to model and see what decisions Clang's IRGen makes about what IR to emit.; Studying Clang's CodeGen directory can also be a good source of ideas. Note; that Clang and LLVM are explicitly version locked so you'll need to make sure; you're using a Clang built from the same git revision or release as the LLVM; library you're using. As always, it's *strongly* recommended that you track; tip of tree development, particularly during bring up of a new project. The Basics; ^^^^^^^^^^^. #. Make sure that your Modules contain both a data layout specification and; target triple. Without these pieces, non of the target specific optimization; will be enabled. This can have a major effect on the generated code quality. #. For each function or global emitted, use the most private linkage type; possible (private, internal or linkonce_odr preferably). Doing so will; make LLVM's inter-procedural optimizations much more effective. #. Avoid high in-degree basic blocks (e.g. basic blocks with dozens or hundreds; of predecessors). Among other issues, the register allocator is known to; perform badly with confronted with such structures. The only exception to; this guidance is that a unified return block with high in-degree is fine. Use of allocas; ^^^^^^^^^^^^^^. An alloca instruction can be used to represent a function scoped stack slot,; but can also represent dynamic frame expansion. When representing function; scoped variables or locations, placing alloca instructions at the beginning of; the entry block should be preferred. In particular, place them before any; call instructions. Call instructions might get inlined and replaced with; multiple basic blocks. The end result is that a following alloca instruction; would no longer be in the entry basic block afterward. The SROA (Scalar Replacement Of Aggregates) and Mem2Reg passes only attempt; to eliminate alloca instruction",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Frontend/PerformanceTips.rst:1773,optimiz,optimizations,1773,interpreter/llvm-project/llvm/docs/Frontend/PerformanceTips.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Frontend/PerformanceTips.rst,1,['optimiz'],['optimizations']
Performance,"to be loaded as well to declare; classes, functions, and variables to Cling.; Instead of headers, pre-compiled code can be used; in particular all of the; standard C++ headers and several system headers are pre-compiled at startup.; cppyy provides the following helpers to load C++ code:. * ``cppdef``: direct access to the interpreter.; This function accepts C++ declarations as a string and JITs them (bindings; are not created until actual use).; The code is loaded into the global scope, thus any previously loaded code; is available from one ``cppdef`` call to the next, as are all standard; C++ headers that have been loaded through pre-compiled headers.; Example::. >>> cppyy.cppdef(r""""""\; ... void hello() {; ... std::cout << ""Hello, World!"" << std::endl;; ... }""""""); True; >>> cppyy.gbl.hello(); Hello, World!; >>> . * ``cppexec``: direct access to the interpreter.; This function accepts C++ statements as a string, JITs and executes them.; Just like ``cppdef``, execution is in the global scope and all previously; loaded code is available.; If the statements are declarations, the effect is the same as ``cppdef``,; but ``cppexec`` also accepts executable lines.; Example::. >>> cppyy.cppexec(r""""""std::string hello = ""Hello, World!"";""""""); True; >>> cppyy.cppexec(""std::cout << hello << std::endl;""); Hello, World!; True; >>> . * ``include``: load declarations into the interpreter.; This function accepts C++ declarations from a file, typically a header.; Files are located through include paths given to the Cling.; Example::. >>> cppyy.include(""vector"") # equivalent to ""#include <vector>""; True; >>> . * ``c_include``: load declarations into the interpreter.; This function accepts C++ declarations from a file, typically a header.; Name mangling is an important difference between C and C++ code.; The use of ``c_include`` instead of ``include`` prevents mangling. * ``load_library``: load compiled C++ into the interpreter.; This function takes the name of a shared library and loads",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/toplevel.rst:1713,load,loaded,1713,bindings/pyroot/cppyy/cppyy/doc/source/toplevel.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/toplevel.rst,1,['load'],['loaded']
Performance,"to evolve through collaboration with many individuals and; active prototyping within the GDB debugger and LLVM compiler. Input has also; been very much appreciated from the developers working on the Perforce TotalView; HPC Debugger and GCC compiler. The inputs provided and insights gained so far have been incorporated into this; current version. The plan is to participate in upstreaming the work and; addressing any feedback. If there is general interest then some or all of these; extensions could be submitted as future DWARF standard proposals. The general principles in designing the extensions have been:. 1. Be backwards compatible with the DWARF Version 5 [:ref:`DWARF; <amdgpu-dwarf-DWARF>`] standard. 2. Be vendor and architecture neutral. They are intended to apply to other; heterogeneous hardware devices including GPUs, DSPs, FPGAs, and other; specialized hardware. These collectively include similar characteristics and; requirements as AMDGPU devices. 3. Provide improved optimization support for non-GPU code. For example, some; extensions apply to traditional CPU hardware that supports large vector; registers. Compilers can map source languages, and source language; extensions, that describe large scale parallel execution, onto the lanes of; the vector registers. This is common in programming languages used in ML and; HPC. 4. Fully define well-formed DWARF in a consistent style based on the DWARF; Version 5 specification. It is possible that some of the generalizations may also benefit other DWARF; issues that have been raised. The remainder of this section enumerates the extensions and provides motivation; for each in terms of heterogeneous debugging. .. _amdgpu-dwarf-allow-location-description-on-the-dwarf-evaluation-stack:. 2.1 Allow Location Description on the DWARF Expression Stack; ------------------------------------------------------------. DWARF Version 5 does not allow location descriptions to be entries on the DWARF; expression stack. They can only be",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst:4203,optimiz,optimization,4203,interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst,1,['optimiz'],['optimization']
Performance,"to one register file so that we minimize the cost of copying data; back and forth to satisfy the (possibly conflicting) requirements of all the; instructions. Register Banks are a means to constrain the register allocator to; use a particular register file for a virtual register. In practice, register files A and B are rarely equal. They can typically store; the same data but there's usually some restrictions on what operations you can; do on each register file. A fairly common pattern is for one of them to be; accessible to integer operations and the other accessible to floating point; operations. To accommodate this, let's rename A and B to GPR (general purpose; registers) and FPR (floating point registers). We now have some additional constraints that limit us. An operation like G_FMUL; has to happen in FPR and G_ADD has to happen in GPR. However, even though this; prescribes a lot of the assignments we still have some freedom. A G_LOAD can; happen in both GPR and FPR, and which we want depends on who is going to consume; the loaded data. Similarly, G_FNEG can happen in both GPR and FPR. If we assign; it to FPR, then we'll use floating point negation. However, if we assign it to; GPR then we can equivalently G_XOR the sign bit with 1 to invert it. In summary, Register Banks are a means of disambiguating between seemingly; equivalent choices based on some analysis of the differences when each choice; is applied in a given context. To give some concrete examples:. AArch64. AArch64 has three main banks. GPR for integer operations, FPR for floating; point and also for the NEON vector instruction set. The third is CCR and; describes the condition code register used for predication. MIPS. MIPS has five main banks of which many programs only really use one or two.; GPR is the general purpose bank for integer operations. FGR or CP1 is for; the floating point operations as well as the MSA vector instructions and a; few other application specific extensions. CP0 is for syst",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GlobalISel/GMIR.rst:5093,load,loaded,5093,interpreter/llvm-project/llvm/docs/GlobalISel/GMIR.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GlobalISel/GMIR.rst,1,['load'],['loaded']
Performance,"to other target; debug info formats such as STABS. It would also be reasonable to use debug information to feed profiling tools; for analysis of generated code, or, tools for reconstructing the original; source from generated code. .. _intro_debugopt:. Debug information and optimizations; -----------------------------------. An extremely high priority of LLVM debugging information is to make it interact; well with optimizations and analysis. In particular, the LLVM debug; information provides the following guarantees:. * LLVM debug information **always provides information to accurately read; the source-level state of the program**, regardless of which LLVM; optimizations have been run. :doc:`HowToUpdateDebugInfo` specifies how debug; info should be updated in various kinds of code transformations to avoid; breaking this guarantee, and how to preserve as much useful debug info as; possible. Note that some optimizations may impact the ability to modify the; current state of the program with a debugger, such as setting program; variables, or calling functions that have been deleted. * As desired, LLVM optimizations can be upgraded to be aware of debugging; information, allowing them to update the debugging information as they; perform aggressive optimizations. This means that, with effort, the LLVM; optimizers could optimize debug code just as well as non-debug code. * LLVM debug information does not prevent optimizations from; happening (for example inlining, basic block reordering/merging/cleanup,; tail duplication, etc). * LLVM debug information is automatically optimized along with the rest of; the program, using existing facilities. For example, duplicate; information is automatically merged by the linker, and unused information; is automatically removed. Basically, the debug information allows you to compile a program with; ""``-O0 -g``"" and get full debug information, allowing you to arbitrarily modify; the program as it executes from a debugger. Compiling a pro",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst:4263,optimiz,optimizations,4263,interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,1,['optimiz'],['optimizations']
Performance,"to performing the following steps to; push a composite location description. *The composite location description is conceptually the base location; description BL with the overlay location description OL positioned as an; overlay starting at the overlay offset BO and covering overlay bit size BS.*. 1. If BO is not 0 then push BL followed by performing the ``DW_OP_bit_piece; BO, 0`` operation.; 2. Push OL followed by performing the ``DW_OP_bit_piece BS, 0`` operation.; 3. If *rbss(BL)* is greater than BO plus BS, push BL followed by performing; the ``DW_OP_bit_piece (rbss(BL) - BO - BS), (BO + BS)`` operation.; 4. Perform the ``DW_OP_LLVM_piece_end`` operation. .. _amdgpu-dwarf-location-list-expressions:. A.2.5.5 DWARF Location List Expressions; +++++++++++++++++++++++++++++++++++++++. .. note::. This section replaces DWARF Version 5 section 2.6.2. *To meet the needs of recent computer architectures and optimization techniques,; debugging information must be able to describe the location of an object whose; location changes over the object’s lifetime, and may reside at multiple; locations during parts of an object's lifetime. Location list expressions are; used in place of operation expressions whenever the object whose location is; being described has these requirements.*. A location list expression consists of a series of location list entries. Each; location list entry is one of the following kinds:. *Bounded location description*. This kind of location list entry provides an operation expression that; evaluates to the location description of an object that is valid over a; lifetime bounded by a starting and ending address. The starting address is the; lowest address of the address range over which the location is valid. The; ending address is the address of the first location past the highest address; of the address range. The location list entry matches when the current program location is within; the given range. There are several kinds of bounded location descr",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst:142552,optimiz,optimization,142552,interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst,1,['optimiz'],['optimization']
Performance,"to provide a library implementation of it. This gets even worse when code from different languages is linked; into a single executable (which is fairly common in large apps).; Having a single malloc would just not suffice, and instead would simply; complicate the picture further because it adds an extra variant in; addition to the one each language provides. Instead, providing a default library version of malloc and free; (and perhaps a malloc_gc with garbage collection instead of free); would make a good implementation available to anyone who wants it. I don't recall all your arguments in favor so let's discuss this again,; and soon. o 'alloca' on the other hand sounds like a good idea, and the; implementation seems fairly language-independent so it doesn't have the; problems with malloc listed above. o About indirect call:; Your option #2 sounded good to me. I'm not sure I understand your; concern about an explicit 'icall' instruction?. o A pair of important synchronization instr'ns to think about:; load-linked; store-conditional. o Other classes of instructions that are valuable for pipeline performance:; conditional-move		 ; predicated instructions. o I believe tail calls are relatively easy to identify; do you know why; .NET has a tailcall instruction?. o I agree that we need a static data space. Otherwise, emulating global; data gets unnecessarily complex. o About explicit parallelism:. We once talked about adding a symbolic thread-id field to each; instruction. (It could be optional so single-threaded codes are; not penalized.) This could map well to multi-threaded architectures; while providing easy ILP for single-threaded onces. But it is probably; too radical an idea to include in a base version of LLVM. Instead, it; could a great topic for a separate study. What is the semantics of the IA64 stop bit?. o And finally, another thought about the syntax for arrays :-). Although this syntax:; 	 array <dimension-list> of <type>; is verbose, it will be used only ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveComments.txt:3347,load,load-linked,3347,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveComments.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveComments.txt,1,['load'],['load-linked']
Performance,"to replace; it with any other `i1` value. Any pass can; freely do it if it can benefit from non-default lowering. '``llvm.load.relative``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare ptr @llvm.load.relative.iN(ptr %ptr, iN %offset) nounwind memory(argmem: read). Overview:; """""""""""""""""". This intrinsic loads a 32-bit value from the address ``%ptr + %offset``,; adds ``%ptr`` to that value and returns it. The constant folder specifically; recognizes the form of this intrinsic and the constant initializers it may; load from; if a loaded constant initializer is known to have the form; ``i32 trunc(x - %ptr)``, the intrinsic call is folded to ``x``. LLVM provides that the calculation of such a constant initializer will; not overflow at link time under the medium code model if ``x`` is an; ``unnamed_addr`` function. However, it does not provide this guarantee for; a constant initializer folded into a function body. This intrinsic can be; used to avoid the possibility of overflows when loading from such a constant. .. _llvm_sideeffect:. '``llvm.sideeffect``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare void @llvm.sideeffect() inaccessiblememonly nounwind willreturn. Overview:; """""""""""""""""". The ``llvm.sideeffect`` intrinsic doesn't perform any operation. Optimizers; treat it as having side effects, so it can be inserted into a loop to; indicate that the loop shouldn't be assumed to terminate (which could; potentially lead to the loop being optimized away entirely), even if it's; an infinite loop with no other side effects. Arguments:; """""""""""""""""""". None. Semantics:; """""""""""""""""""". This intrinsic actually does nothing, but optimizers must assume that it; has externally observable side effects. '``llvm.is.constant.*``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". This is an overloaded intrinsic. You can use llvm.is.constant with any argument type. ::. declare i1 @llvm.is.constant.i32(i32 %operand) nounwind m",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:951623,load,loading,951623,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['loading']
Performance,"to scalar/aggregate/complex and; lvalue/rvalue paths, depending on what kind of result your expression; produces. On occasion, this requires some careful factoring of code to; avoid duplication.; * ``CodeGenFunction`` contains functions ``ConvertType`` and; ``ConvertTypeForMem`` that convert Clang's types (``clang::Type*`` or; ``clang::QualType``) to LLVM types. Use the former for values, and the; latter for memory locations: test with the C++ ""``bool``"" type to check; this. If you find that you are having to use LLVM bitcasts to make the; subexpressions of your expression have the type that your expression; expects, STOP! Go fix semantic analysis and the AST so that you don't; need these bitcasts.; * The ``CodeGenFunction`` class has a number of helper functions to make; certain operations easy, such as generating code to produce an lvalue or; an rvalue, or to initialize a memory location with a given value. Prefer; to use these functions rather than directly writing loads and stores,; because these functions take care of some of the tricky details for you; (e.g., for exceptions).; * If your expression requires some special behavior in the event of an; exception, look at the ``push*Cleanup`` functions in ``CodeGenFunction``; to introduce a cleanup. You shouldn't have to deal with; exception-handling directly.; * Testing is extremely important in IR generation. Use ``clang -cc1; -emit-llvm`` and `FileCheck; <https://llvm.org/docs/CommandGuide/FileCheck.html>`_ to verify that you're; generating the right IR. #. Teach template instantiation how to cope with your AST node, which requires; some fairly simple code:. * Make sure that your expression's constructor properly computes the flags; for type dependence (i.e., the type your expression produces can change; from one instantiation to the next), value dependence (i.e., the constant; value your expression produces can change from one instantiation to the; next), instantiation dependence (i.e., a template parameter occu",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/InternalsManual.rst:151766,load,loads,151766,interpreter/llvm-project/clang/docs/InternalsManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/InternalsManual.rst,1,['load'],['loads']
Performance,"to separate the; arguments of the ``-debug-only`` option. For performance reasons, -debug-only is not available in optimized build; (``--enable-optimized``) of LLVM. The ``DEBUG_WITH_TYPE`` macro is also available for situations where you would; like to set ``DEBUG_TYPE``, but only for one specific ``DEBUG`` statement. It; takes an additional first parameter, which is the type to use. For example, the; preceding example could be written as:. .. code-block:: c++. DEBUG_WITH_TYPE(""foo"", dbgs() << ""'foo' debug type\n"");; DEBUG_WITH_TYPE(""bar"", dbgs() << ""'bar' debug type\n"");. .. _Statistic:. The ``Statistic`` class & ``-stats`` option; -------------------------------------------. The ``llvm/ADT/Statistic.h`` (`doxygen; <https://llvm.org/doxygen/Statistic_8h_source.html>`__) file provides a class; named ``Statistic`` that is used as a unified way to keep track of what the LLVM; compiler is doing and how effective various optimizations are. It is useful to; see what optimizations are contributing to making a particular program run; faster. Often you may run your pass on some big program, and you're interested to see; how many times it makes a certain transformation. Although you can do this with; hand inspection, or some ad-hoc method, this is a real pain and not very useful; for big programs. Using the ``Statistic`` class makes it very easy to keep; track of this information, and the calculated information is presented in a; uniform manner with the rest of the passes being executed. There are many examples of ``Statistic`` uses, but the basics of using it are as; follows:. Define your statistic like this:. .. code-block:: c++. #define DEBUG_TYPE ""mypassname"" // This goes after any #includes.; STATISTIC(NumXForms, ""The # of times I did stuff"");. The ``STATISTIC`` macro defines a static variable, whose name is specified by; the first argument. The pass name is taken from the ``DEBUG_TYPE`` macro, and; the description is taken from the second argument. The variable defined",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ProgrammersManual.rst:46605,optimiz,optimizations,46605,interpreter/llvm-project/llvm/docs/ProgrammersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ProgrammersManual.rst,1,['optimiz'],['optimizations']
Performance,"to the ReadySet. Every cycle, the scheduler checks if instructions can be moved from the WaitSet; to the ReadySet, and if instructions from the ReadySet can be issued to the; underlying pipelines. The algorithm prioritizes older instructions over younger; instructions. Write-Back and Retire Stage; """"""""""""""""""""""""""""""""""""""""""""""""""""""; Issued instructions are moved from the ReadySet to the IssuedSet. There,; instructions wait until they reach the write-back stage. At that point, they; get removed from the queue and the retire control unit is notified. When instructions are executed, the retire control unit flags the instruction as; ""ready to retire."". Instructions are retired in program order. The register file is notified of the; retirement so that it can free the physical registers that were allocated for; the instruction during the register renaming stage. Load/Store Unit and Memory Consistency Model; """"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""; To simulate an out-of-order execution of memory operations, :program:`llvm-mca`; utilizes a simulated load/store unit (LSUnit) to simulate the speculative; execution of loads and stores. Each load (or store) consumes an entry in the load (or store) queue. Users can; specify flags ``-lqueue`` and ``-squeue`` to limit the number of entries in the; load and store queues respectively. The queues are unbounded by default. The LSUnit implements a relaxed consistency model for memory loads and stores.; The rules are:. 1. A younger load is allowed to pass an older load only if there are no; intervening stores or barriers between the two loads.; 2. A younger load is allowed to pass an older store provided that the load does; not alias with the store.; 3. A younger store is not allowed to pass an older store.; 4. A younger store is not allowed to pass an older load. By default, the LSUnit optimistically assumes that loads do not alias; (`-noalias=true`) store operations. Under this assumption, younger loads are; always allowed to pass older ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst:39379,load,load,39379,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,2,['load'],"['load', 'loads']"
Performance,"to the `two_dim_fit` structure pointer, see manual; - **`sizex`**: length x of the source spectrum; - **`sizey`**: length y of the source spectrum. The `two_dim_fit` structure has the form of. ``` {.cpp}; class TSpectrumTwoDimFit{. public:. int number_of_peaks; // input parameter, should be>0; int number_of_iterations; // input parameter, should be >0; int xmin; // first fitted channel in x direction; int xmax; // last fitted channel in x direction; int ymin; // first fitted channel in y direction; int ymax; // last fitted channel in y direction; double alpha; // convergence coefficient, input parameter, it should be a positive number and <=1; double chi; // here the function returns resulting chi square; int statistic_type; // type of statistics, possible values are:; // FIT2_OPTIM_CHI_COUNTS (chi square statistics with counts as weighting coefficients),; // FIT2_OPTIM_CHI_FUNC_VALUES (chi square statistics with function values as weighting coefficients),; // FIT2_OPTIM_MAX_LIKELIHOOD; int alpha_optim; // optimization of convergence coefficients, possible values are:; // FIT2_ALPHA_HALVING, FIT2_ALPHA_OPTIMAL; int power; // possible values are: FIT21_FIT_POWER2,4,6,8,10,12; int fit_taylor; // order of Taylor expansion, possible values are:; // FIT2_TAYLOR_ORDER_FIRST,; // FIT2_TAYLOR_ORDER_SECOND; double position_init_x[MAX_NUMBER_OF_PEAKS2]; // initial values of x positions of 2D peaks, input parameters; double position_calc_x[MAX_NUMBER_OF_PEAKS2]; // calculated values of fitted x positions of 2D peaks, output parameters; double position_err_x[MAX_NUMBER_OF_PEAKS2]; // x position errors of 2D peaks; bool fix_position_x[MAX_NUMBER_OF_PEAKS2]; // logical vector which allows to fix the appropriate x positions of 2D peaks (not fit). However, they are present in the estimated functional; double position_init_y[MAX_NUMBER_OF_PEAKS2]; // initial values of y positions of 2D peaks, input parameters; double position_calc_y[MAX_NUMBER_OF_PEAKS2]; // calculated values of fitt",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/spectrum/Spectrum.md:49706,optimiz,optimization,49706,documentation/spectrum/Spectrum.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/spectrum/Spectrum.md,1,['optimiz'],['optimization']
Performance,"to the final visualization in form of highly ; customizable, publication-ready plots. It is reliable, performant and well supported,; easy to use and obtain, and strives to maximize the quantity and impact of scientific ; results obtained per unit cost, both of human effort and computing resources. ROOT provides a very efficient storage system for data models, ; that demonstrated to scale at the Large Hadron Collider experiments: Exabytes ; of scientific data are written in columnar ROOT format.; ROOT comes with histogramming capabilities in an arbitrary number of ; dimensions, curve fitting, statistical modelling, minimization, to allow; the easy setup of a data analysis system that can query and process the data; interactively or in batch mode, as well as a general parallel processing; framework, RDataFrame, that can considerably speed up an analysis, taking ; full advantage of multi-core and distributed systems. ROOT is performance critical software written in C++ and enables rapid prototyping ; powered by a unique C++ compliant interpreter called Cling. ; Cling also enables performant C++ type introspection which is a building block of automatic ; interoperability with Python. Thanks to PyROOT, leveraging the cppyy technology, ; ROOT offers efficient, on-demand C++/Python interoperability in a uniform cross-language ; execution environment. ROOT fully embraces open-source, it's made with passion by its community,; for the benefit of its community. [![License: LGPL v2.1+](https://img.shields.io/badge/License-LGPL%20v2.1+-blue.svg)](https://www.gnu.org/licenses/lgpl.html); [![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/5060/badge)](https://bestpractices.coreinfrastructure.org/projects/5060). ## Contribution Guidelines; - [How to contribute](https://github.com/root-project/root/blob/master/CONTRIBUTING.md); - [Coding conventions](https://root.cern/coding-conventions); - [Meetings](https://root.cern/meetings). ## Cite; When citing ROOT, ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README.md:1209,perform,performance,1209,README.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README.md,1,['perform'],['performance']
Performance,"to; ``%reg1 - reg2``. This list of values should be provided by the containing; intrinsic/instruction.; - ``DW_OP_breg`` (or ``DW_OP_bregx``) represents a content on the provided; signed offset of the specified register. The opcode is only generated by the; ``AsmPrinter`` pass to describe call site parameter value which requires an; expression over two registers.; - ``DW_OP_push_object_address`` pushes the address of the object which can then; serve as a descriptor in subsequent calculation. This opcode can be used to; calculate bounds of fortran allocatable array which has array descriptors.; - ``DW_OP_over`` duplicates the entry currently second in the stack at the top; of the stack. This opcode can be used to calculate bounds of fortran assumed; rank array which has rank known at run time and current dimension number is; implicitly first element of the stack.; - ``DW_OP_LLVM_implicit_pointer`` It specifies the dereferenced value. It can; be used to represent pointer variables which are optimized out but the value; it points to is known. This operator is required as it is different than DWARF; operator DW_OP_implicit_pointer in representation and specification (number; and types of operands) and later can not be used as multiple level. .. code-block:: text. IR for ""*ptr = 4;""; --------------; call void @llvm.dbg.value(metadata i32 4, metadata !17, metadata !20); !17 = !DILocalVariable(name: ""ptr1"", scope: !12, file: !3, line: 5,; type: !18); !18 = !DIDerivedType(tag: DW_TAG_pointer_type, baseType: !19, size: 64); !19 = !DIBasicType(name: ""int"", size: 32, encoding: DW_ATE_signed); !20 = !DIExpression(DW_OP_LLVM_implicit_pointer)). IR for ""**ptr = 4;""; --------------; call void @llvm.dbg.value(metadata i32 4, metadata !17, metadata !21); !17 = !DILocalVariable(name: ""ptr1"", scope: !12, file: !3, line: 5,; type: !18); !18 = !DIDerivedType(tag: DW_TAG_pointer_type, baseType: !19, size: 64); !19 = !DIDerivedType(tag: DW_TAG_pointer_type, baseType: !20, size: 64); !20 = ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:266129,optimiz,optimized,266129,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['optimiz'],['optimized']
Performance,"to; express the input LLVM code in the target instruction set. This stage; produces the initial code for the program in the target instruction set, then; makes use of virtual registers in SSA form and physical registers that; represent any required register assignments due to target constraints or; calling conventions. This step turns the LLVM code into a DAG of target; instructions. 2. `Scheduling and Formation`_ --- This phase takes the DAG of target; instructions produced by the instruction selection phase, determines an; ordering of the instructions, then emits the instructions as :raw-html:`<tt>`; `MachineInstr`_\s :raw-html:`</tt>` with that ordering. Note that we; describe this in the `instruction selection section`_ because it operates on; a `SelectionDAG`_. 3. `SSA-based Machine Code Optimizations`_ --- This optional stage consists of a; series of machine-code optimizations that operate on the SSA-form produced by; the instruction selector. Optimizations like modulo-scheduling or peephole; optimization work here. 4. `Register Allocation`_ --- The target code is transformed from an infinite; virtual register file in SSA form to the concrete register file used by the; target. This phase introduces spill code and eliminates all virtual register; references from the program. 5. `Prolog/Epilog Code Insertion`_ --- Once the machine code has been generated; for the function and the amount of stack space required is known (used for; LLVM alloca's and spill slots), the prolog and epilog code for the function; can be inserted and ""abstract stack location references"" can be eliminated.; This stage is responsible for implementing optimizations like frame-pointer; elimination and stack packing. 6. `Late Machine Code Optimizations`_ --- Optimizations that operate on ""final""; machine code can go here, such as spill code scheduling and peephole; optimizations. 7. `Code Emission`_ --- The final stage actually puts out the code for the; current function, either in the target",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst:6253,optimiz,optimization,6253,interpreter/llvm-project/llvm/docs/CodeGenerator.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst,1,['optimiz'],['optimization']
Performance,"toc@ha; addis 4, 2, .LCPI0_1@toc@ha; addi 3, 3, .LCPI0_0@toc@l; addi 4, 4, .LCPI0_1@toc@l; lxvw4x 0, 0, 3; addi 3, 1, -16; lxvw4x 35, 0, 4; stxvw4x 0, 0, 3; ori 2, 2, 0; lxvw4x 34, 0, 3; addi 3, 1, -32; stxvw4x 35, 0, 3; vpmsumb 2, 2, 3; blr; .long 0; .quad 0. The two stxvw4x instructions are not needed.; With -mtriple=powerpc64le-unknown-linux-gnu, the associated permutes; are present too. //===----------------------------------------------------------------------===//. The following example is found in test/CodeGen/PowerPC/vec_add_sub_doubleword.ll:. define <2 x i64> @increment_by_val(<2 x i64> %x, i64 %val) nounwind {; %tmpvec = insertelement <2 x i64> <i64 0, i64 0>, i64 %val, i32 0; %tmpvec2 = insertelement <2 x i64> %tmpvec, i64 %val, i32 1; %result = add <2 x i64> %x, %tmpvec2; ret <2 x i64> %result. This will generate the following instruction sequence:; std 5, -8(1); std 5, -16(1); addi 3, 1, -16; ori 2, 2, 0; lxvd2x 35, 0, 3; vaddudm 2, 2, 3; blr. This will almost certainly cause a load-hit-store hazard. ; Since val is a value parameter, it should not need to be saved onto; the stack, unless it's being done set up the vector register. Instead,; it would be better to splat the value into a vector register, and then; remove the (dead) stores to the stack. //===----------------------------------------------------------------------===//. At the moment we always generate a lxsdx in preference to lfd, or stxsdx in; preference to stfd. When we have a reg-immediate addressing mode, this is a; poor choice, since we have to load the address into an index register. This; should be fixed for P7/P8. . //===----------------------------------------------------------------------===//. Right now, ShuffleKind 0 is supported only on BE, and ShuffleKind 2 only on LE.; However, we could actually support both kinds on either endianness, if we check; for the appropriate shufflevector pattern for each case ... this would cause; some additional shufflevectors to be recognized and ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt:7910,load,load-hit-store,7910,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt,2,['load'],['load-hit-store']
Performance,"tolerance parameter `fTol` (a member of the base class; **`TDecompBase`**) plays a crucial role in all operations of the; decomposition classes. It gives the user a tool to monitor and steer the; operations its default value is $\varepsilon$ where $1+\varepsilon=1$. If you do not want to be bothered by the following considerations, like; in most other linear algebra packages, just set the tolerance with; `SetTol` to an arbitrary small number. The tolerance number is used by; each decomposition method to decide whether the matrix is near singular,; except of course SVD that can handle singular matrices. This will be; checked in a different way for any decomposition. For instance in LU, a; matrix is considered singular in the solving stage when a diagonal; element of the decomposed matrix is smaller than `fTol`. Here an; important point is raised. The `Decompose()` method is successful as; long no zero diagonal element is encountered. Therefore, the user could; perform decomposition and only after this step worry about the tolerance; number. If the matrix is flagged as being singular, operations with the; decomposition will fail and will return matrices or vectors that are; invalid. If one would like to monitor the tolerance parameter but not; have the code stop in case of a number smaller than `fTol`, one could; proceed as follows:. ``` {.cpp}; TVectorD b = ..;; TMatrixD a = ..;; .; TDecompLU lu(a);; Bool_t ok;; TVectorD x = lu.Solve(b,ok);; Int_t nr = 0;; while (!ok) {; lu.SetMatrix(a);; lu.SetTol(0.1*lu.GetTol());; if (nr++ > 10) break;; x = lu.Solve(b,ok);; }; if (x.IsValid()); cout << ""solved with tol ="" << lu.GetTol() << endl;; else; cout << ""solving failed "" << endl;; ```. The observant reader will notice that by scaling the complete matrix by; some small number the decomposition will detect a singular matrix. In; this case, the user will have to reduce the tolerance number by this; factor. (For CPU time saving we decided not to make this an automatic procedure)",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/LinearAlgebra.md:40573,perform,perform,40573,documentation/users-guide/LinearAlgebra.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/LinearAlgebra.md,1,['perform'],['perform']
Performance,"tomic value being; acquired. 4. buffer_gl0_inv. - If CU wavefront execution; mode, omit.; - If OpenCL omit.; - Ensures that; following; loads will not see; stale data. atomicrmw acq_rel - workgroup - generic 1. s_waitcnt lgkmcnt(0) &; vmcnt(0) & vscnt(0). - If CU wavefront execution; mode, omit vmcnt(0) and; vscnt(0).; - If OpenCL, omit lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0), s_waitcnt; vscnt(0) and s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load/load; atomic/; atomicrmw-with-return-value.; - s_waitcnt vscnt(0); must happen after; any preceding; global/generic; store/store; atomic/; atomicrmw-no-return-value.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; have; completed before; performing the; atomicrmw that is; being released. 2. flat_atomic; 3. s_waitcnt lgkmcnt(0) &; vmcnt(0) & vscnt(0). - If CU wavefront execution; mode, omit vmcnt(0) and; vscnt(0).; - If OpenCL, omit lgkmcnt(0).; - Must happen before; the following; buffer_gl0_inv.; - Ensures any; following global; data read is no; older than the load; atomic value being; acquired. 3. buffer_gl0_inv. - If CU wavefront execution; mode, omit.; - Ensures that; following; loads will not see; stale data. atomicrmw acq_rel - agent - global 1. s_waitcnt lgkmcnt(0) &; - system vmcnt(0) & vscnt(0). - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0), s_waitcnt; vscnt(0) and s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/load atomic/; atomicrmw-with-return-value.; - s_waitcnt vscnt(0); must happen after; any preceding; global/generic; store/store ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:366363,perform,performing,366363,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['perform'],['performing']
Performance,"tomic. fence acquire - agent *none* 1. s_waitcnt lgkmcnt(0) &; - system vmcnt(0). - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate; (see comment for; previous fence).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - Must happen before; the following; buffer_wbinvl1_vol.; - Ensures that the; fence-paired atomic; has completed; before invalidating; the; cache. Therefore; any following; locations read must; be no older than; the value read by; the; fence-paired-atomic. 2. buffer_wbinvl1_vol. - Must happen before any; following global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. **Release Atomic**; ------------------------------------------------------------------------------------; store atomic release - singlethread - global 1. buffer/global/ds/flat_store; - wavefront - local; - generic; store atomic release - workgroup - global 1. s_waitcnt lgkmcnt(0); - generic; - If OpenCL, omit.; - Must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; store.; - Ensures that all; memory operations; to local have; completed before; performing the; store that is being; released. 2. buffer/global/flat_store; store atomic release - workgroup - local 1. ds",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:218589,cache,cache,218589,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['cache'],['cache']
Performance,"tomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acquire - workgroup - local *If TgSplit execution mode,; local address space cannot; be used.*. 1. ds_atomic; 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than the local; atomicrmw value; being acquired. atomicrmw acquire - workgroup - generic 1. flat_atomic; 2. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL, omit lgkmcnt(0).; - Must happen before; the following; buffer_wbinvl1_vol and; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local; atomicrmw value; being acquired. 3. buffer_wbinvl1_vol. - If not TgSplit execution; mode, omit.; - Ensures that; following; loads will not see; stale data. atomicrmw acquire - agent - global 1. buffer/global_atomic; 2. s_waitcnt vmcnt(0). - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the; atomicrmw has; completed before; invalidating the; cache. 3. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acquire - system - global 1. buffer/global_atomic; 2. s_waitcnt vmcnt(0). - Must happen before; following buffer_invl2 and; buffer_wbinvl1_vol.; - Ensures the; atomicrmw has; completed before; invalidating the; caches. 3. buffer_invl2;; buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale L1 global data,; nor see stale L2 MTYPE; NC global data.; MTYPE RW and CC memory will; never be stale in L2 due to; the memory probes. atomicrmw acquire - agent - generic 1. flat_atomi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:250401,load,loads,250401,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['loads']
Performance,"tomic/atomicrmw.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load/store/; load atomic/store atomic/; atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; have; completed before; performing the; atomicrmw that is; being released. 2. buffer/global_atomic; 3. s_waitcnt vmcnt(0). - If not TgSplit execution; mode, omit.; - Must happen before; the following; buffer_inv.; - Ensures any; following global; data read is no; older than the; atomicrmw value; being acquired. 4. buffer_inv sc0=1. - If not TgSplit execution; mode, omit.; - Ensures that; following; loads will not see; stale data. atomicrmw acq_rel - workgroup - local *If TgSplit execution mode,; local address space cannot; be used.*. 1. ds_atomic; 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than the local load; atomic value being; acquired. atomicrmw acq_rel - workgroup - generic 1. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL, omit; lgkmcnt(0).; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load/store/; load atomic/store atomic/; atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; have; completed before; performing the; atomicrmw that is; being released. 2. flat_atomic; 3. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If not TgSplit execution; mode, omit vmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Must happen before; the following; buffer_inv and; any following; global/generic; load/load; atomic/sto",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:318278,load,load,318278,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"tomicExpand Codegen pass if; ``shouldInsertFencesForAtomic()`` returns true. The MachineMemOperand for all atomic operations is currently marked as volatile;; this is not correct in the IR sense of volatile, but CodeGen handles anything; marked volatile very conservatively. This should get fixed at some point. One very important property of the atomic operations is that if your backend; supports any inline lock-free atomic operations of a given size, you should; support *ALL* operations of that size in a lock-free manner. When the target implements atomic ``cmpxchg`` or LL/SC instructions (as most do); this is trivial: all the other operations can be implemented on top of those; primitives. However, on many older CPUs (e.g. ARMv5, SparcV8, Intel 80386) there; are atomic load and store instructions, but no ``cmpxchg`` or LL/SC. As it is; invalid to implement ``atomic load`` using the native instruction, but; ``cmpxchg`` using a library call to a function that uses a mutex, ``atomic; load`` must *also* expand to a library call on such architectures, so that it; can remain atomic with regards to a simultaneous ``cmpxchg``, by using the same; mutex. AtomicExpandPass can help with that: it will expand all atomic operations to the; proper ``__atomic_*`` libcalls for any size above the maximum set by; ``setMaxAtomicSizeInBitsSupported`` (which defaults to 0). On x86, all atomic loads generate a ``MOV``. SequentiallyConsistent stores; generate an ``XCHG``, other stores generate a ``MOV``. SequentiallyConsistent; fences generate an ``MFENCE``, other fences do not cause any code to be; generated. ``cmpxchg`` uses the ``LOCK CMPXCHG`` instruction. ``atomicrmw xchg``; uses ``XCHG``, ``atomicrmw add`` and ``atomicrmw sub`` use ``XADD``, and all; other ``atomicrmw`` operations generate a loop with ``LOCK CMPXCHG``. Depending; on the users of the result, some ``atomicrmw`` operations can be translated into; operations like ``LOCK AND``, but that does not work in general. On ARM (be",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst:19228,load,load,19228,interpreter/llvm-project/llvm/docs/Atomics.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst,2,['load'],['load']
Performance,"tomicrmw acq_rel - agent - global 1. s_waitcnt lgkmcnt(0) &; - system vmcnt(0). - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global have; completed before; performing the; atomicrmw that is; being released. 2. buffer/global_atomic; 3. s_waitcnt vmcnt(0). - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the; atomicrmw has; completed before; invalidating the; cache. 4. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acq_rel - agent - generic 1. s_waitcnt lgkmcnt(0) &; - system vmcnt(0). - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global have; completed before; performing the; atomicrmw that is; being released. 2. flat_atomic; 3. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If OpenCL, omit; lgkmcnt(0).; - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the; atomicrmw has; completed before; invalidating the; cache. 4. buffer_wbinvl1_vol. - Must happen before; any following; glob",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:226145,load,loads,226145,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['loads']
Performance,"tomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global and local; have completed; before performing; the atomicrmw that; is being released. 2. buffer/global/flat_atomic; fence release - singlethread *none* *none*; - wavefront; fence release - workgroup *none* 1. s_waitcnt lgkmcnt(0). - If OpenCL and; address space is; not generic, omit.; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate. If; fence had an; address space then; set to address; space of OpenCL; fence flag, or to; generic if both; local and global; flags are; specified.; - Must happen after; any preceding; local/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Must happen before; any following store; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - Ensures that all; memory operations; to local have; completed before; performing the; following; fence-paired-atomic. fence release - agent *none* 1. s_waitcnt lgkmcnt(0) &; - system vmcnt(0). - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate. If; fence had an; address space then; set to address; space of OpenCL; fence flag, or to; generic if both; local and global; flags are; specified.; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; any following store; atomic/atomicrmw; with an equal o",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:222402,perform,performing,222402,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['perform'],['performing']
Performance,"tonic - agent - global 1. buffer/global/flat_load; - system - generic glc=1 dlc=1. - If GFX11, omit dlc=1. store atomic monotonic - singlethread - global 1. buffer/global/flat_store; - wavefront - generic; - workgroup; - agent; - system; store atomic monotonic - singlethread - local 1. ds_store; - wavefront; - workgroup; atomicrmw monotonic - singlethread - global 1. buffer/global/flat_atomic; - wavefront - generic; - workgroup; - agent; - system; atomicrmw monotonic - singlethread - local 1. ds_atomic; - wavefront; - workgroup; **Acquire Atomic**; ------------------------------------------------------------------------------------; load atomic acquire - singlethread - global 1. buffer/global/ds/flat_load; - wavefront - local; - generic; load atomic acquire - workgroup - global 1. buffer/global_load glc=1. - If CU wavefront execution; mode, omit glc=1. 2. s_waitcnt vmcnt(0). - If CU wavefront execution; mode, omit.; - Must happen before; the following buffer_gl0_inv; and before any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw. 3. buffer_gl0_inv. - If CU wavefront execution; mode, omit.; - Ensures that; following; loads will not see; stale data. load atomic acquire - workgroup - local 1. ds_load; 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; the following buffer_gl0_inv; and before any following; global/generic load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than the local load; atomic value being; acquired. 3. buffer_gl0_inv. - If CU wavefront execution; mode, omit.; - If OpenCL, omit.; - Ensures that; following; loads will not see; stale data. load atomic acquire - workgroup - generic 1. flat_load glc=1. - If CU wavefront execution; mode, omit glc=1. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If CU wavefront execution; mode, omit vmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Must happen before; the following; buffer_gl0_inv and any; following global/generic; load",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:346487,load,load,346487,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"tor are taken from the corresponding lanes of the '``passthru``' operand. Arguments:; """""""""""""""""""". The first operand is a vector of pointers which holds all memory addresses to read. The second operand is an alignment of the source addresses. It must be 0 or a power of two constant integer value. The third operand, mask, is a vector of boolean values with the same number of elements as the return type. The fourth is a pass-through value that is used to fill the masked-off lanes of the result. The return type, underlying type of the vector of pointers and the type of the '``passthru``' operand are the same vector types. Semantics:; """""""""""""""""""". The '``llvm.masked.gather``' intrinsic is designed for conditional reading of multiple scalar values from arbitrary memory locations in a single IR operation. It is useful for targets that support vector masked gathers and allows vectorizing basic blocks with data and control divergence. Other targets may support this intrinsic differently, for example by lowering it into a sequence of scalar load operations.; The semantics of this operation are equivalent to a sequence of conditional scalar loads with subsequent gathering all loaded values into a single vector. The mask restricts memory access to certain lanes and facilitates vectorization of predicated basic blocks. ::. %res = call <4 x double> @llvm.masked.gather.v4f64.v4p0(<4 x ptr> %ptrs, i32 8, <4 x i1> <i1 true, i1 true, i1 true, i1 true>, <4 x double> poison). ;; The gather with all-true mask is equivalent to the following instruction sequence; %ptr0 = extractelement <4 x ptr> %ptrs, i32 0; %ptr1 = extractelement <4 x ptr> %ptrs, i32 1; %ptr2 = extractelement <4 x ptr> %ptrs, i32 2; %ptr3 = extractelement <4 x ptr> %ptrs, i32 3. %val0 = load double, ptr %ptr0, align 8; %val1 = load double, ptr %ptr1, align 8; %val2 = load double, ptr %ptr2, align 8; %val3 = load double, ptr %ptr3, align 8. %vec0 = insertelement <4 x double> poison, %val0, 0; %vec01 = insertelement <4 x d",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:850487,load,load,850487,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,"tor the second matrix column; // return a slice of the first row from element (0,1) : r2[0] = m(0,1); r2[1] = m(0,2); SVector2 r2 = m.SubRow<SVector2> (0,1);; // return a slice of the second column from element (0,1) : c2[0] = m(0,1); c2[1] = m(1,1);; SVector2 c2 = m.SubCol<SVector2> (1,0);; // return a sub-matrix 2x2 with the upper left corner at the values (1,1); SMatrix22 subM = m.Sub<SMatrix22> (1,1);; // return the diagonal element in a SVector; SVector3 diag = m.Diagonal();; // return the upper(lower) block of the matrix m; SVector6 vub = m.UpperBlock(); // vub = [ 1, 2, 3, 5, 6, 9 ]; SVector6 vlb = m.LowerBlock(); // vlb = [ 1, 4, 5, 7, 8, 9 ]; ~~~. ### Linear Algebra Functions. Only limited linear algebra functionality is available for SMatrix. It is possible; for squared matrices NxN, to find the inverse or to calculate the determinant.; Different inversion algorithms are used if the matrix is smaller than 6x6 or if it; is symmetric. In the case of a small matrix, a faster direct inversion is used.; For a large (N > 6) symmetric matrix the Bunch-Kaufman diagonal pivoting method; is used while for a large (N > 6) general matrix an LU factorization is performed; using the same algorithm as in the CERNLIB routine; [dinv](https://cern-tex.web.cern.ch/cern-tex/shortwrupsdir/f010/top.html). ~~~ {.cpp}; // Invert a NxN matrix. The inverted matrix replace the existing one and returns if the result is successful; bool ret = m.Invert(); // return the inverse matrix of m. If the inversion fails ifail is different than zero; int ifail = 0;; mInv = m.Inverse(ifail);; ~~~. The determinant of a square matrix can be obtained as follows:. ~~~ {.cpp}; double det;; // calculate the determinant modifying the matrix content. Returns if the calculation was successful; bool ret = m.Det(det);; // calculate the determinant using a temporary matrix but preserving the matrix content; bool ret = n.Det2(det);; ~~~. For additional Matrix functionality see the \ref MatVecFunctions page. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/math/smatrix/doc/SMatrixClass.md:8250,perform,performed,8250,math/smatrix/doc/SMatrixClass.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/math/smatrix/doc/SMatrixClass.md,1,['perform'],['performed']
Performance,"torLength()``?). Result: VP usable for IR-level vectorizers (LV, VPlan, RegionVectorizer),; potential integration in Clang with builtins. 2. CodeGen support; ------------------. - VP intrinsics translate to first-class SDNodes; (eg ``llvm.vp.fdiv.* -> vp_fdiv``).; - VP legalization (legalize explicit vector length to mask (AVX512), legalize VP; SDNodes to pre-existing ones (SSE, NEON)). Result: Backend development based on VP SDNodes. 3. Lift InstSimplify/InstCombine/DAGCombiner to VP; --------------------------------------------------. - Introduce PredicatedInstruction, PredicatedBinaryOperator, .. helper classes; that match standard vector IR and VP intrinsics.; - Add a matcher context to PatternMatch and context-aware IR Builder APIs.; - Incrementally lift DAGCombiner to work on VP SDNodes as well as on regular; vector instructions.; - Incrementally lift InstCombine/InstSimplify to operate on VP as well as; regular IR instructions. Result: Optimization of VP intrinsics on par with standard vector instructions. 4. Deprecate llvm.masked.* / llvm.experimental.reduce.*; -------------------------------------------------------. - Modernize llvm.masked.* / llvm.experimental.reduce* by translating to VP.; - DCE transitional APIs. Result: VP has superseded earlier vector intrinsics. 5. Predicated IR Instructions; -----------------------------. - Vector instructions have an optional mask and vector length parameter. These; lower to VP SDNodes (from Stage 2).; - Phase out VP intrinsics, only keeping those that are not equivalent to; vectorized scalar instructions (reduce, shuffles, ..); - InstCombine/InstSimplify expect predication in regular Instructions (Stage (3); has laid the groundwork). Result: Native vector predication in IR. References; ==========. .. [MaskedIR] `llvm.masked.*` intrinsics,; https://llvm.org/docs/LangRef.html#masked-vector-load-and-store-intrinsics. .. [VPRFC] RFC: Prototype & Roadmap for vector predication in LLVM,; https://reviews.llvm.org/D57504; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VectorPredication.rst:2988,load,load-and-store-intrinsics,2988,interpreter/llvm-project/llvm/docs/Proposals/VectorPredication.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VectorPredication.rst,1,['load'],['load-and-store-intrinsics']
Performance,"torT<Float_t> TEveVector;; typedef TEveVectorT<Float_t> TEveVectorF;; typedef TEveVectorT<Double_t> TEveVectorD;. All projectable classes now take into account their transformation; matrix. The projected versions are still stored in global; coordinates.; TEveShape -- a new abstract base-class for 2D/3D shapes that; require fill / outline color, line-width and various flags; controlling the area / outline drawing.; TEveGeoShape and projected classes: subclass from TEveShape. Add; support for TGeoCompositeShapes. In 2D projected class; (TEvePolygonSetProjected) improve detection of duplicate polygons; and add support for detection of minimal-outline (triggered via; Bool_t TEveShape::fMiniOutline).; TEveBox: New class to draw a simple cuboid with minimal memory; usage. It is projectable.; TEveBoxSet: for box-type kBT_FreeBox assure proper face; orientation at registration time and calculate normals when; rendering. TEveJetCone is now projectable.; Several performance improvements when dealing with large; collections of EVE objects. Profiled with simulated heavy-ion; data. In particular, for destruction of self-contained sub-hierarchies of objects; one can use TEveElement::Annihilate() and; TEveElement::AnnihilateElements(). See class docs for constraints. Minor changes. Add support for projecting a new child (all children) of an; element after the element and its old children have already been; projected. This is provided by the following virtual functions in; TEveElement:; void ProjectChild(TEveElement* el, Bool_t sameDepth=kTRUE);; void ProjectAllChildren(Bool_t same_depth=kTRUE);. Several improvements in drawing of TEveCalo axes and labels.; TEveTrackPropagator. Fix some issues with Runge-Kutta track; propagator. Move in controls specifying how to plot tracks that get; split in RhoZ projection.; Fix rendering of TEveJetCone: normals at apex were not changing as they should.; Support single-color for TEveDigitSet (call TEveDigitSet::UseSingleColor()).; Always add chil",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/graf3d/doc/v528/index.html:5678,perform,performance,5678,graf3d/doc/v528/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/graf3d/doc/v528/index.html,2,['perform'],['performance']
Performance,"tore atomic release - workgroup - local *If TgSplit execution mode,; local address space cannot; be used.*. 1. ds_store; store atomic release - agent - global 1. buffer_wbl2 sc1=1; - generic; - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at agent scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; store.; - Ensures that all; memory operations; to memory have; completed before; performing the; store that is being; released. 3. GFX940, GFX941; buffer/global/flat_store; sc0=1 sc1=1; GFX942; buffer/global/flat_store; sc1=1; store atomic release - system - global 1. buffer_wbl2 sc0=1 sc1=1; - generic; - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at system scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after any; preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after any; preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; store.; - Ensures t",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:308622,perform,performing,308622,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['perform'],['performing']
Performance,"tore i32 %val0, ptr %ptr0, align 4; store i32 %val1, ptr %ptr1, align 4; ..; store i32 %val7, ptr %ptr7, align 4. Masked Vector Expanding Load and Compressing Store Intrinsics; -------------------------------------------------------------. LLVM provides intrinsics for expanding load and compressing store operations. Data selected from a vector according to a mask is stored in consecutive memory addresses (compressed store), and vice-versa (expanding load). These operations effective map to ""if (cond.i) a[j++] = v.i"" and ""if (cond.i) v.i = a[j++]"" patterns, respectively. Note that when the mask starts with '1' bits followed by '0' bits, these operations are identical to :ref:`llvm.masked.store <int_mstore>` and :ref:`llvm.masked.load <int_mload>`. .. _int_expandload:. '``llvm.masked.expandload.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. Several values of integer, floating point or pointer data type are loaded from consecutive memory addresses and stored into the elements of a vector according to the mask. ::. declare <16 x float> @llvm.masked.expandload.v16f32 (ptr <ptr>, <16 x i1> <mask>, <16 x float> <passthru>); declare <2 x i64> @llvm.masked.expandload.v2i64 (ptr <ptr>, <2 x i1> <mask>, <2 x i64> <passthru>). Overview:; """""""""""""""""". Reads a number of scalar values sequentially from memory location provided in '``ptr``' and spreads them in a vector. The '``mask``' holds a bit for each vector lane. The number of elements read from memory is equal to the number of '1' bits in the mask. The loaded elements are positioned in the destination vector according to the sequence of '1' and '0' bits in the mask. E.g., if the mask vector is '10010001', ""expandload"" reads 3 values from memory addresses ptr, ptr+1, ptr+2 and places them in lanes 0, 3 and 7 accordingly. The masked-off lanes are filled by elements from the corresponding lanes of the '``passthru``' operand. Arguments:; """""""""""""""""""". The first operand i",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:855313,load,loaded,855313,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['loaded']
Performance,"tore"", and unmodeled side effects flags to; determine whether an instruction should be treated as a memory-barrier. This was; inaccurate in general and was changed so that now each instruction has an; IsAStoreBarrier and IsALoadBarrier flag. These flags are mca specific and; default to false for every instruction. If any instruction should have either of; these flags set, it should be done within the target's InstrPostProcess class.; For an example, look at the `X86InstrPostProcess::postProcessInstruction` method; within `llvm/lib/Target/X86/MCA/X86CustomBehaviour.cpp`. A load/store barrier consumes one entry of the load/store queue. A load/store; barrier enforces ordering of loads/stores. A younger load cannot pass a load; barrier. Also, a younger store cannot pass a store barrier. A younger load; has to wait for the memory/load barrier to execute. A load/store barrier is; ""executed"" when it becomes the oldest entry in the load/store queue(s). That; also means, by construction, all of the older loads/stores have been executed. In conclusion, the full set of load/store consistency rules are:. #. A store may not pass a previous store.; #. A store may not pass a previous load (regardless of ``-noalias``).; #. A store has to wait until an older store barrier is fully executed.; #. A load may pass a previous load.; #. A load may not pass a previous store unless ``-noalias`` is set.; #. A load has to wait until an older load barrier is fully executed. In-order Issue and Execute; """"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""; In-order processors are modelled as a single ``InOrderIssueStage`` stage. It; bypasses Dispatch, Scheduler and Load/Store unit. Instructions are issued as; soon as their operand registers are available and resource requirements are; met. Multiple instructions can be issued in one cycle according to the value of; the ``IssueWidth`` parameter in LLVM's scheduling model. Once issued, an instruction is moved to ``IssuedInst`` set until it is ready to; retire. :pro",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst:42664,load,loads,42664,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,1,['load'],['loads']
Performance,"tors are:. * ``Ninja`` --- for generating `Ninja <https://ninja-build.org>`_; build files. Most llvm developers use Ninja.; * ``Unix Makefiles`` --- for generating make-compatible parallel makefiles.; * ``Visual Studio`` --- for generating Visual Studio projects and; solutions.; * ``Xcode`` --- for generating Xcode projects. * See the `CMake docs; <https://cmake.org/cmake/help/latest/manual/cmake-generators.7.html>`_; for a more comprehensive list. Some common options:. * ``-DLLVM_ENABLE_PROJECTS='...'`` --- semicolon-separated list of the LLVM; subprojects you'd like to additionally build. Can include any of: clang,; clang-tools-extra, lldb, lld, polly, or cross-project-tests. For example, to build LLVM, Clang, and LLD, use; ``-DLLVM_ENABLE_PROJECTS=""clang;lld""``. * ``-DCMAKE_INSTALL_PREFIX=directory`` --- Specify for *directory* the full; pathname of where you want the LLVM tools and libraries to be installed; (default ``/usr/local``). * ``-DCMAKE_BUILD_TYPE=type`` --- Controls optimization level and debug; information of the build. Valid options for *type* are ``Debug``,; ``Release``, ``RelWithDebInfo``, and ``MinSizeRel``. For more detailed; information see :ref:`CMAKE_BUILD_TYPE <cmake_build_type>`. * ``-DLLVM_ENABLE_ASSERTIONS=ON`` --- Compile with assertion checks enabled; (default is ON for Debug builds, OFF for all other build types). * ``-DLLVM_USE_LINKER=lld`` --- Link with the `lld linker`_, assuming it; is installed on your system. This can dramatically speed up link times; if the default linker is slow. * ``-DLLVM_PARALLEL_{COMPILE,LINK}_JOBS=N`` --- Limit the number of; compile/link jobs running in parallel at the same time. This is; especially important for linking since linking can use lots of memory. If; you run into memory issues building LLVM, try setting this to limit the; maximum number of compile/link jobs running at the same time. * ``cmake --build build [--target <target>]`` or the build system specified; above directly. * The default target ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GettingStarted.rst:2938,optimiz,optimization,2938,interpreter/llvm-project/llvm/docs/GettingStarted.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GettingStarted.rst,1,['optimiz'],['optimization']
Performance,"tors for each pass to the containing upper; level pass manager. For example,. .. code-block:: c++. ModulePassManager MPM;; MPM.addPass(createModuleToFunctionPassAdaptor(FunctionPass1()));; MPM.addPass(createModuleToFunctionPassAdaptor(FunctionPass2()));; MPM.run();. will run ``FunctionPass1`` on each function in a module, then run; ``FunctionPass2`` on each function in the module. In contrast,. .. code-block:: c++. ModulePassManager MPM;. FunctionPassManager FPM;; FPM.addPass(FunctionPass1());; FPM.addPass(FunctionPass2());. MPM.addPass(createModuleToFunctionPassAdaptor(std::move(FPM)));. will run ``FunctionPass1`` and ``FunctionPass2`` on the first function in a; module, then run both passes on the second function in the module, and so on.; This is better for cache locality around LLVM data structures. This similarly; applies for the other IR types, and in some cases can even affect the quality; of optimization. For example, running all loop passes on a loop may cause a; later loop to be able to be optimized more than if each loop pass were run; separately. Inserting Passes into Default Pipelines; =======================================. Rather than manually adding passes to a pass manager, the typical way of; creating a pass manager is to use a ``PassBuilder`` and call something like; ``PassBuilder::buildPerModuleDefaultPipeline()`` which creates a typical; pipeline for a given optimization level. Sometimes either frontends or backends will want to inject passes into the; pipeline. For example, frontends may want to add instrumentation, and target; backends may want to add passes that lower custom intrinsics. For these; cases, ``PassBuilder`` exposes callbacks that allow injecting passes into; certain parts of the pipeline. For example,. .. code-block:: c++. PassBuilder PB;; PB.registerPipelineStartEPCallback([&](ModulePassManager &MPM,; PassBuilder::OptimizationLevel Level) {; MPM.addPass(FooPass());; };. will add ``FooPass`` near the very beginning of the pipeli",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:4884,optimiz,optimized,4884,interpreter/llvm-project/llvm/docs/NewPassManager.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst,1,['optimiz'],['optimized']
Performance,"tors; may be used to endorse or promote products derived from this software without; specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS; ""AS IS"" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED; TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR; PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS; BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR; CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE; GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION); HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT; LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY; OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF; SUCH DAMAGE. You are under no obligation whatsoever to provide any bug fixes,; patches, or upgrades to the features, functionality or performance of; the source code (""Enhancements"") to anyone; however, if you choose to; make your Enhancements available either publicly, or directly to; Lawrence Berkeley National Laboratory, without imposing a separate; written license agreement for such Enhancements, then you hereby grant; the following license: a non-exclusive, royalty-free perpetual license; to install, use, modify, prepare derivative works, incorporate into; other computer software, distribute, and sublicense such Enhancements; or derivative works thereof, in binary and source code form. Additional copyright holders; ----------------------------. In addition to LBNL/UC Berkeley, this package contains files copyrighted by; one or more of the following people and organizations, and licensed under; the same conditions (except for some compatible licenses as retained in the; source code):. CERN; Lucio Asnaghi; Torok Attila; Simone Bacchio; Niko Fink; Aaron Jomy; Mac Kolin; Baidyanath Kundu; Toby StClere-Smithe; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/CPyCppyy/LICENSE.txt:1855,perform,performance,1855,bindings/pyroot/cppyy/CPyCppyy/LICENSE.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/CPyCppyy/LICENSE.txt,1,['perform'],['performance']
Performance,"tps://github.com/brendangregg/FlameGraph>`_. To generate output for a flamegraph, a few more options are necessary. - ``--all-stacks`` - Emits all of the stacks.; - ``--stack-format`` - Choose the flamegraph output format 'flame'.; - ``--aggregation-type`` - Choose the metric to graph. You may pipe the command output directly to the flamegraph tool to obtain an; svg file. ::. $ llvm-xray stack xray-log.llc.5rqxkU --instr_map=./bin/llc --stack-format=flame --aggregation-type=time --all-stacks | \; /path/to/FlameGraph/flamegraph.pl > flamegraph.svg. If you open the svg in a browser, mouse events allow exploring the call stacks. Chrome Trace Viewer Visualization; ---------------------------------. We can also generate a trace which can be loaded by the Chrome Trace Viewer; from the same generated trace:. ::. $ llvm-xray convert --symbolize --instr_map=./bin/llc \; --output-format=trace_event xray-log.llc.5rqxkU \; | gzip > llc-trace.txt.gz. From a Chrome browser, navigating to ``chrome:///tracing`` allows us to load; the ``sample-trace.txt.gz`` file to visualize the execution trace. Further Exploration; -------------------. The ``llvm-xray`` tool has a few other subcommands that are in various stages; of being developed. One interesting subcommand that can highlight a few; interesting things is the ``graph`` subcommand. Given for example the following; toy program that we build with XRay instrumentation, we can see how the; generated graph may be a helpful indicator of where time is being spent for the; application. .. code-block:: c++. // sample.cc; #include <iostream>; #include <thread>. [[clang::xray_always_instrument]] void f() {; std::cerr << '.';; }. [[clang::xray_always_instrument]] void g() {; for (int i = 0; i < 1 << 10; ++i) {; std::cerr << '-';; }; }. int main(int argc, char* argv[]) {; std::thread t1([] {; for (int i = 0; i < 1 << 10; ++i); f();; });; std::thread t2([] {; g();; });; t1.join();; t2.join();; std::cerr << '\n';; }. We then build the above with",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/XRayExample.rst:13738,load,load,13738,interpreter/llvm-project/llvm/docs/XRayExample.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/XRayExample.rst,1,['load'],['load']
Performance,"tput and a chain and we; want to map it into one that just output a chain. The current trick is to select; it into a MERGE_VALUES with the first definition being an implicit_def. The; proper solution is to add new ISD opcodes for the no-output variant. DAG; combiner can then transform the node before it gets to target node selection. Problem #2 is we are adding a whole bunch of x86 atomic instructions when in; fact these instructions are identical to the non-lock versions. We need a way to; add target specific information to target nodes and have this information; carried over to machine instructions. Asm printer (or JIT) can use this; information to add the ""lock"" prefix. //===---------------------------------------------------------------------===//. struct B {; unsigned char y0 : 1;; };. int bar(struct B* a) { return a->y0; }. define i32 @bar(%struct.B* nocapture %a) nounwind readonly optsize {; %1 = getelementptr inbounds %struct.B* %a, i64 0, i32 0; %2 = load i8* %1, align 1; %3 = and i8 %2, 1; %4 = zext i8 %3 to i32; ret i32 %4; }. bar: # @bar; # %bb.0:; movb (%rdi), %al; andb $1, %al; movzbl %al, %eax; ret. Missed optimization: should be movl+andl. //===---------------------------------------------------------------------===//. The x86_64 abi says:. Booleans, when stored in a memory object, are stored as single byte objects the; value of which is always 0 (false) or 1 (true). We are not using this fact:. int bar(_Bool *a) { return *a; }. define i32 @bar(i8* nocapture %a) nounwind readonly optsize {; %1 = load i8* %a, align 1, !tbaa !0; %tmp = and i8 %1, 1; %2 = zext i8 %tmp to i32; ret i32 %2; }. bar:; movb (%rdi), %al; andb $1, %al; movzbl %al, %eax; ret. GCC produces. bar:; movzbl (%rdi), %eax; ret. //===---------------------------------------------------------------------===//. Take the following C code:; int f(int a, int b) { return (unsigned char)a == (unsigned char)b; }. We generate the following IR with clang:; define i32 @f(i32 %a, i32 %b) nounwind rea",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:35015,load,load,35015,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt,2,['load'],['load']
Performance,"tr %struct.ST, <4 x ptr> %s, <4 x i64> %ind1,; <4 x i32> <i32 2, i32 2, i32 2, i32 2>,; <4 x i32> <i32 1, i32 1, i32 1, i32 1>,; <4 x i32> %ind4,; <4 x i64> <i64 13, i64 13, i64 13, i64 13>. getelementptr %struct.ST, <4 x ptr> %s, <4 x i64> %ind1,; i32 2, i32 1, <4 x i32> %ind4, i64 13. Let's look at the C code, where the vector version of ``getelementptr``; makes sense:. .. code-block:: c. // Let's assume that we vectorize the following loop:; double *A, *B; int *C;; for (int i = 0; i < size; ++i) {; A[i] = B[C[i]];; }. .. code-block:: llvm. ; get pointers for 8 elements from array B; %ptrs = getelementptr double, ptr %B, <8 x i32> %C; ; load 8 elements from array B into A; %A = call <8 x double> @llvm.masked.gather.v8f64.v8p0f64(<8 x ptr> %ptrs,; i32 8, <8 x i1> %mask, <8 x double> %passthru). Conversion Operations; ---------------------. The instructions in this category are the conversion instructions; (casting) which all take a single operand and a type. They perform; various bit conversions on the operand. .. _i_trunc:. '``trunc .. to``' Instruction; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. <result> = trunc <ty> <value> to <ty2> ; yields ty2. Overview:; """""""""""""""""". The '``trunc``' instruction truncates its operand to the type ``ty2``. Arguments:; """""""""""""""""""". The '``trunc``' instruction takes a value to trunc, and a type to trunc; it to. Both types must be of :ref:`integer <t_integer>` types, or vectors; of the same number of integers. The bit size of the ``value`` must be; larger than the bit size of the destination type, ``ty2``. Equal sized; types are not allowed. Semantics:; """""""""""""""""""". The '``trunc``' instruction truncates the high order bits in ``value``; and converts the remaining bits to ``ty2``. Since the source size must; be larger than the destination size, ``trunc`` cannot be a *no-op cast*.; It will always truncate bits. Example:; """""""""""""""". .. code-block:: llvm. %X = trunc i32 257 to i8 ; yields i8:1; %Y = trunc i32 123 to i1 ; yields i1",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:441999,perform,perform,441999,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['perform']
Performance,"tr; %ptr = alloca i32, i32 4 ; yields ptr; %ptr = alloca i32, i32 4, align 1024 ; yields ptr; %ptr = alloca i32, align 1024 ; yields ptr. .. _i_load:. '``load``' Instruction; ^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. <result> = load [volatile] <ty>, ptr <pointer>[, align <alignment>][, !nontemporal !<nontemp_node>][, !invariant.load !<empty_node>][, !invariant.group !<empty_node>][, !nonnull !<empty_node>][, !dereferenceable !<deref_bytes_node>][, !dereferenceable_or_null !<deref_bytes_node>][, !align !<align_node>][, !noundef !<empty_node>]; <result> = load atomic [volatile] <ty>, ptr <pointer> [syncscope(""<target-scope>"")] <ordering>, align <alignment> [, !invariant.group !<empty_node>]; !<nontemp_node> = !{ i32 1 }; !<empty_node> = !{}; !<deref_bytes_node> = !{ i64 <dereferenceable_bytes> }; !<align_node> = !{ i64 <value_alignment> }. Overview:; """""""""""""""""". The '``load``' instruction is used to read from memory. Arguments:; """""""""""""""""""". The argument to the ``load`` instruction specifies the memory address from which; to load. The type specified must be a :ref:`first class <t_firstclass>` type of; known size (i.e. not containing an :ref:`opaque structural type <t_opaque>`). If; the ``load`` is marked as ``volatile``, then the optimizer is not allowed to; modify the number or order of execution of this ``load`` with other; :ref:`volatile operations <volatile>`. If the ``load`` is marked as ``atomic``, it takes an extra :ref:`ordering; <ordering>` and optional ``syncscope(""<target-scope>"")`` argument. The; ``release`` and ``acq_rel`` orderings are not valid on ``load`` instructions.; Atomic loads produce :ref:`defined <memmodel>` results when they may see; multiple atomic stores. The type of the pointee must be an integer, pointer, or; floating-point type whose bit width is a power of two greater than or equal to; eight and less than or equal to a target-specific size limit. ``align`` must be; explicitly specified on atomic loads. Note: if the alignment is not grea",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:412650,load,load,412650,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,2,['load'],['load']
Performance,"trace_LIBFILE}); set(system_libs ${system_libs} ${Backtrace_LIBFILE}); endif(); if( LLVM_ENABLE_TERMINFO ); set(imported_libs ${imported_libs} Terminfo::terminfo); endif(); set(system_libs ${system_libs} ${LLVM_ATOMIC_LIB}); set(system_libs ${system_libs} ${LLVM_PTHREAD_LIB}); if( UNIX AND NOT (BEOS OR HAIKU) ); set(system_libs ${system_libs} m); endif(); if( UNIX AND ${CMAKE_SYSTEM_NAME} MATCHES ""SunOS"" ); set(system_libs ${system_libs} kstat socket); endif(); if( FUCHSIA ); set(system_libs ${system_libs} zircon); endif(); if ( HAIKU ); add_compile_definitions(_BSD_SOURCE); set(system_libs ${system_libs} bsd network); endif(); endif( MSVC OR MINGW ). # Delay load shell32.dll if possible to speed up process startup.; set (delayload_flags); if (MSVC); # When linking with Swift, `swiftc.exe` is used as the linker drive rather; # than invoking `link.exe` directly. In such a case, the flags should be; # marked as `-Xlinker` to pass them directly to the linker. As a temporary; # workaround simply elide the delay loading.; set (delayload_flags $<$<NOT:$<LINK_LANGUAGE:Swift>>:delayimp -delayload:shell32.dll -delayload:ole32.dll>); endif(). # Link Z3 if the user wants to build it.; if(LLVM_WITH_Z3); set(system_libs ${system_libs} ${Z3_LIBRARIES}); endif(). # Override the C runtime allocator on Windows and embed it into LLVM tools & libraries; if(LLVM_INTEGRATED_CRT_ALLOC); if (NOT CMAKE_MSVC_RUNTIME_LIBRARY OR CMAKE_MSVC_RUNTIME_LIBRARY MATCHES ""DLL$""); message(FATAL_ERROR ""LLVM_INTEGRATED_CRT_ALLOC only works with CMAKE_MSVC_RUNTIME_LIBRARY set to MultiThreaded or MultiThreadedDebug.""); endif(). string(REGEX REPLACE ""(/|\\\\)$"" """" LLVM_INTEGRATED_CRT_ALLOC ""${LLVM_INTEGRATED_CRT_ALLOC}""). if(NOT EXISTS ""${LLVM_INTEGRATED_CRT_ALLOC}""); message(FATAL_ERROR ""Cannot find the path to `git clone` for the CRT allocator! (${LLVM_INTEGRATED_CRT_ALLOC}). Currently, rpmalloc, snmalloc and mimalloc are supported.""); endif(). if(LLVM_INTEGRATED_CRT_ALLOC MATCHES ""rpmalloc$""); add_compil",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/CMakeLists.txt:3269,load,loading,3269,interpreter/llvm-project/llvm/lib/Support/CMakeLists.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/CMakeLists.txt,2,['load'],['loading']
Performance,"tracking a declaration and a path; of fields and indices into that allocation.; * **TargetPointer**: represents a target address derived from a base address; through pointer arithmetic, such as ``((int *)0x100)[20]``. Null pointers are; target pointers with a zero offset.; * **TypeInfoPointer**: tracks information for the opaque type returned by; ``typeid``; * **InvalidPointer**: is dummy pointer created by an invalid operation which; allows the interpreter to continue execution. Does not allow pointer; arithmetic or dereferencing. Besides the previously mentioned union, a number of other pointer-like types; have their own type:. * **ObjCBlockPointer** tracks Objective-C blocks; * **FnPointer** tracks functions and lazily caches their compiled version; * **MemberPointer** tracks C++ object members. Void pointers, which can be built by casting any of the aforementioned; pointers, are implemented as a union of all pointer types. The ``BitCast``; opcode is responsible for performing all legal conversions between these; types and primitive integers. BlockPointer; ~~~~~~~~~~~~. Block pointers track a ``Pointee``, the block to which they point, along; with a ``Base`` and an ``Offset``. The base identifies the innermost field,; while the offset points to an array element relative to the base (including; one-past-end pointers). The offset identifies the array element or field; which is referenced, while the base points to the outer object or array which; contains the field. These two fields allow all pointers to be uniquely; identified, disambiguated and characterised. As an example, consider the following structure:. .. code-block:: c. struct A {; struct B {; int x;; int y;; } b;; struct C {; int a;; int b;; } c[2];; int z;; };; constexpr A a;. On the target, ``&a`` and ``&a.b.x`` are equal. So are ``&a.c[0]`` and; ``&a.c[0].a``. In the interpreter, all these pointers must be; distinguished since the are all allowed to address distinct range of; memory. In the interpreter, ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ConstantInterpreter.rst:10087,perform,performing,10087,interpreter/llvm-project/clang/docs/ConstantInterpreter.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ConstantInterpreter.rst,1,['perform'],['performing']
Performance,"tradicting assumptions, this can lead to arbitrary; miscompilations. See `issue #44218; <https://github.com/llvm/llvm-project/issues/44218>`_.; - x86-32 (even with SSE2 enabled) may implicitly perform such a conversion on; values returned from a function for some calling conventions. See `issue; #66803 <https://github.com/llvm/llvm-project/issues/66803>`_.; - Older MIPS versions use the opposite polarity for the quiet/signaling bit, and; LLVM does not correctly represent this. See `issue #60796; <https://github.com/llvm/llvm-project/issues/60796>`_. .. _fastmath:. Fast-Math Flags; ---------------. LLVM IR floating-point operations (:ref:`fneg <i_fneg>`, :ref:`fadd <i_fadd>`,; :ref:`fsub <i_fsub>`, :ref:`fmul <i_fmul>`, :ref:`fdiv <i_fdiv>`,; :ref:`frem <i_frem>`, :ref:`fcmp <i_fcmp>`), :ref:`phi <i_phi>`,; :ref:`select <i_select>` and :ref:`call <i_call>`; may use the following flags to enable otherwise unsafe; floating-point transformations. ``nnan``; No NaNs - Allow optimizations to assume the arguments and result are not; NaN. If an argument is a nan, or the result would be a nan, it produces; a :ref:`poison value <poisonvalues>` instead. ``ninf``; No Infs - Allow optimizations to assume the arguments and result are not; +/-Inf. If an argument is +/-Inf, or the result would be +/-Inf, it; produces a :ref:`poison value <poisonvalues>` instead. ``nsz``; No Signed Zeros - Allow optimizations to treat the sign of a zero; argument or zero result as insignificant. This does not imply that -0.0; is poison and/or guaranteed to not exist in the operation. ``arcp``; Allow Reciprocal - Allow optimizations to use the reciprocal of an; argument rather than perform division. ``contract``; Allow floating-point contraction (e.g. fusing a multiply followed by an; addition into a fused multiply-and-add). This does not enable reassociating; to form arbitrary contractions. For example, ``(a*b) + (c*d) + e`` can not; be transformed into ``(a*b) + ((c*d) + e)`` to create two fma operat",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:161848,optimiz,optimizations,161848,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['optimiz'],['optimizations']
Performance,"transformations. When creating; a matrix or a translation, this is by default owned by external objects.; The manager class becomes owner of all transformations used for; positioning volumes. In order to force the ownership for other; transformations, one can use TGeoMatrix::RegisterYourself() method. Do; not be therefore surprised that some transformations cannot be found by; name when creating a composite shape for instance if you did not; register them after creation. Logical nodes (positioned volumes) are created and destroyed by the; TGeoVolume class. Physical nodes and their global transformations; are subjected to a caching mechanism due to the sometimes very large; memory requirements of logical graph expansion. The total number of; physical instances of volumes triggers the caching mechanism and the; cache manager is a client of TGeoManager. The manager class also; controls the drawing/checking package (TGeoPainter client). This; is linked with %ROOT graphical libraries loaded on demand in order to; control visualization actions. \anchor GP02; ## Navigation and Tracking. Tracking is the feature allowing the transport of a given particle; knowing its kinematics. A state is determined by any combination of the; position \f$\vec{r}\f$ and direction \f$\vec{n}\f$ with respect to the world; reference frame. The direction \f$\vec{n}\f$ must be a unit vector having as; components the director cosines. The full classification of a given; state will provide the following information: the deepest physical node; containing the position vector, the distance to the closest boundary; along the direction vector, the next physical node after propagating the; current point with this distance and the safety distance to the nearest; boundary. This information allows the propagation of particles inside a; detector geometry by taking into account both geometrical and physical; constraints. We will hereby describe the user interface of `TGeo` to access; tracking functionality. T",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md:61156,load,loaded,61156,geom/geom/doc/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md,1,['load'],['loaded']
Performance,"trated by the fact that the instruction only; spent 1cy in the scheduler's queue. There is a gap of 5 cycles between the write-back stage and the retire event.; That is because instructions must retire in program order, so [1,0] has to wait; for [0,2] to be retired first (i.e., it has to wait until cycle 10). In the example, all instructions are in a RAW (Read After Write) dependency; chain. Register %xmm2 written by vmulps is immediately used by the first; vhaddps, and register %xmm3 written by the first vhaddps is used by the second; vhaddps. Long data dependencies negatively impact the ILP (Instruction Level; Parallelism). In the dot-product example, there are anti-dependencies introduced by; instructions from different iterations. However, those dependencies can be; removed at register renaming stage (at the cost of allocating register aliases,; and therefore consuming physical registers). Table *Average Wait times* helps diagnose performance issues that are caused by; the presence of long latency instructions and potentially long data dependencies; which may limit the ILP. Last row, ``<total>``, shows a global average over all; instructions measured. Note that :program:`llvm-mca`, by default, assumes at; least 1cy between the dispatch event and the issue event. When the performance is limited by data dependencies and/or long latency; instructions, the number of cycles spent while in the *ready* state is expected; to be very small when compared with the total number of cycles spent in the; scheduler's queue. The difference between the two counters is a good indicator; of how large of an impact data dependencies had on the execution of the; instructions. When performance is mostly limited by the lack of hardware; resources, the delta between the two counters is small. However, the number of; cycles spent in the queue tends to be larger (i.e., more than 1-3cy),; especially when compared to other low latency instructions. Bottleneck Analysis; ^^^^^^^^^^^^^^^^^^^; T",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst:25482,perform,performance,25482,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,2,"['latency', 'perform']","['latency', 'performance']"
Performance,"tremely high priority of LLVM debugging information is to make it interact; well with optimizations and analysis. In particular, the LLVM debug; information provides the following guarantees:. * LLVM debug information **always provides information to accurately read; the source-level state of the program**, regardless of which LLVM; optimizations have been run. :doc:`HowToUpdateDebugInfo` specifies how debug; info should be updated in various kinds of code transformations to avoid; breaking this guarantee, and how to preserve as much useful debug info as; possible. Note that some optimizations may impact the ability to modify the; current state of the program with a debugger, such as setting program; variables, or calling functions that have been deleted. * As desired, LLVM optimizations can be upgraded to be aware of debugging; information, allowing them to update the debugging information as they; perform aggressive optimizations. This means that, with effort, the LLVM; optimizers could optimize debug code just as well as non-debug code. * LLVM debug information does not prevent optimizations from; happening (for example inlining, basic block reordering/merging/cleanup,; tail duplication, etc). * LLVM debug information is automatically optimized along with the rest of; the program, using existing facilities. For example, duplicate; information is automatically merged by the linker, and unused information; is automatically removed. Basically, the debug information allows you to compile a program with; ""``-O0 -g``"" and get full debug information, allowing you to arbitrarily modify; the program as it executes from a debugger. Compiling a program with; ""``-O3 -g``"" gives you full debug information that is always available and; accurate for reading (e.g., you get accurate stack traces despite tail call; elimination and inlining), but you might lose the ability to modify the program; and call functions which were optimized out of the program, or inlined away; completely.",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst:4663,optimiz,optimizers,4663,interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,2,['optimiz'],"['optimize', 'optimizers']"
Performance,"tributions/4596763/) or consult the [RooBatchComupte README](https://github.com/root-project/root/tree/v6-26-00-patches/roofit/batchcompute).; The README also describes how to enable BatchMode support for your own PDFs. ### Parallel calculation of likelihood gradients during fitting; This release features two new optional RooFit libraries: `RooFit::MultiProcess` and `RooFit::TestStatistics`.; To activate both, build with `-Droofit_multiprocess=ON`. The `RooFit::TestStatistics` namespace contains a major refactoring of the `RooAbsTestStatistic`-`RooAbsOptTestStatistic`-`RooNLLVar` inheritance tree into:. 1. statistics-based classes on the one hand;; 2. calculation/evaluation/optimization based classes on the other hand. The main selling point of using `RooFit::TestStatistics` from a performance point of view is the implementation of the `RooFit::MultiProcess` based `LikelihoodGradientJob` calculator class.; To use it to perform a ""migrad"" fit (using Minuit2), one should create a `RooMinimizer` using a new constructor with a `RooAbsL` likelihood parameter as follows:. ```c++; using RooFit::TestStatistics::RooAbsL;; using RooFit::TestStatistics::buildLikelihood;. RooAbsPdf* pdf = ...; // build a pdf; RooAbsData* data = ...; // get some data. std::shared_ptr<RooAbsL> likelihood = buildLikelihood(pdf, data, [OPTIONAL ARGUMENTS]);. RooMinimizer m(likelihood);; m.migrad();; ```. The `RooMinimizer` object behaves as usual, except that behind the scenes it will now calculate each partial derivative on a separate process, ideally running on a separate CPU core.; This can be used to speed up fits with many parameters (at least as many as there are cores to parallelize over), since every parameter corresponds to a partial derivative.; The resulting fit parameters will be identical to those obtained with the non-parallelized gradients minimizer in most cases (see the usage notes linked below for exceptions). In upcoming releases, further developments are planned:. - Benchmark/pr",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v626/index.md:17149,perform,perform,17149,README/ReleaseNotes/v626/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v626/index.md,1,['perform'],['perform']
Performance,"trinsic. ::. ; Insert fixed type into scalable type; declare <vscale x 4 x float> @llvm.vector.insert.nxv4f32.v4f32(<vscale x 4 x float> %vec, <4 x float> %subvec, i64 <idx>); declare <vscale x 2 x double> @llvm.vector.insert.nxv2f64.v2f64(<vscale x 2 x double> %vec, <2 x double> %subvec, i64 <idx>). ; Insert scalable type into scalable type; declare <vscale x 4 x float> @llvm.vector.insert.nxv4f64.nxv2f64(<vscale x 4 x float> %vec, <vscale x 2 x float> %subvec, i64 <idx>). ; Insert fixed type into fixed type; declare <4 x double> @llvm.vector.insert.v4f64.v2f64(<4 x double> %vec, <2 x double> %subvec, i64 <idx>). Overview:; """""""""""""""""". The '``llvm.vector.insert.*``' intrinsics insert a vector into another vector; starting from a given index. The return type matches the type of the vector we; insert into. Conceptually, this can be used to build a scalable vector out of; non-scalable vectors, however this intrinsic can also be used on purely fixed; types. Scalable vectors can only be inserted into other scalable vectors. Arguments:; """""""""""""""""""". The ``vec`` is the vector which ``subvec`` will be inserted into.; The ``subvec`` is the vector that will be inserted. ``idx`` represents the starting element number at which ``subvec`` will be; inserted. ``idx`` must be a constant multiple of ``subvec``'s known minimum; vector length. If ``subvec`` is a scalable vector, ``idx`` is first scaled by; the runtime scaling factor of ``subvec``. The elements of ``vec`` starting at; ``idx`` are overwritten with ``subvec``. Elements ``idx`` through (``idx`` +; num_elements(``subvec``) - 1) must be valid ``vec`` indices. If this condition; cannot be determined statically but is false at runtime, then the result vector; is a :ref:`poison value <poisonvalues>`. '``llvm.vector.extract``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. ; Extract fixed type from scalable type; declare <4 x float> @llvm.vector.extract.v4f32.nxv4f32(<vscale",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:663382,scalab,scalable,663382,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['scalab'],['scalable']
Performance,"trinsics take both pointers as separate arguments for completeness. In; this snippet, ``%object`` is the object pointer, and ``%derived`` is the derived; pointer:. .. code-block:: llvm. ;; An array type.; %class.Array = type { %class.Object, i32, [0 x %class.Object*] }; ... ;; Load the object pointer from a gcroot.; %object = load %class.Array** %object_addr. ;; Compute the derived pointer.; %derived = getelementptr %object, i32 0, i32 2, i32 %n. LLVM does not enforce this relationship between the object and derived pointer; (although a particular :ref:`collector strategy <plugin>` might). However, it; would be an unusual collector that violated it. The use of these intrinsics is naturally optional if the target GC does not; require the corresponding barrier. The GC strategy used with such a collector; should replace the intrinsic calls with the corresponding ``load`` or; ``store`` instruction if they are used. One known deficiency with the current design is that the barrier intrinsics do; not include the size or alignment of the underlying operation performed. It is; currently assumed that the operation is of pointer size and the alignment is; assumed to be the target machine's default alignment. Write barrier: ``llvm.gcwrite``; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. .. code-block:: llvm. void @llvm.gcwrite(i8* %value, i8* %object, i8** %derived). For write barriers, LLVM provides the ``llvm.gcwrite`` intrinsic function. It; has exactly the same semantics as a non-volatile ``store`` to the derived; pointer (the third argument). The exact code generated is specified by the; Function's selected :ref:`GC strategy <plugin>`. Many important algorithms require write barriers, including generational and; concurrent collectors. Additionally, write barriers could be used to implement; reference counting. Read barrier: ``llvm.gcread``; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. .. code-block:: llvm. i8* @llvm.gcread(i8* %object, i8** %derived). For read barriers, LLVM provides the ``llvm.gcrea",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GarbageCollection.rst:14483,perform,performed,14483,interpreter/llvm-project/llvm/docs/GarbageCollection.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GarbageCollection.rst,1,['perform'],['performed']
Performance,"trivial to destroy exists at that location, the program has; undefined behavior. .. admonition:: Rationale. While these rules are far less fine-grained than C++, they are; nonetheless sufficient to express a wide spectrum of types.; Types that express some sort of ownership will generally be non-trivial; to both copy and destroy and either non-trivial or illegal to; default-initialize. Types that don't express ownership may still; be non-trivial to copy because of some sort of address sensitivity;; for example, a relative reference. Distinguishing default; initialization allows types to impose policies about how they are; created. These rules assume that assignment into an l-value is always a; modification of an existing object rather than an initialization.; Assignment is then a compound operation where the old value is; read and destroyed, if necessary, and the new value is put into; place. These are the natural semantics of value propagation, where; all basic operations on the type come down to copies and destroys,; and everything else is just an optimization on top of those. The most glaring weakness of programming with non-trivial types in C; is that there are no language mechanisms (akin to C++'s placement; ``new`` and explicit destructor calls) for explicitly creating and; destroying objects. Clang should consider adding builtins for this; purpose, as well as for common optimizations like destructive; relocation. Application of the formal C rules to nontrivial ownership qualifiers; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. Nontrivially ownership-qualified types are considered non-trivial; to copy, destroy, and default-initialize. A dynamic object of nontrivially ownership-qualified type contingently; exists at a location if the memory is filled with a zero pattern, e.g.; by ``calloc`` or ``bzero``. Such an object can be safely accessed in; all of the cases above, but its memory can also be safely repurposed.; Assigning a null point",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst:59290,optimiz,optimization,59290,interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,1,['optimiz'],['optimization']
Performance,"trix=gGeoIdentity); ```. The objects referencing a volume and a transformation are called `NODES`; and their creation is fully handled by the modeller. They represent the; link elements in the hierarchy of volumes. Nodes are unique and distinct; geometrical objects ONLY from their container point of view. Since; volumes can be replicated in the geometry, the same node may be found on; different branches. In order to provide navigation features, volumes have to be able to find; the proper container of any point defined in the local reference frame.; This can be the volume itself, one of its positioned daughter volumes or; none if the point is actually outside. On the other hand, volumes have; to provide also other navigation methods such as finding the distances; to its shape boundaries or which daughter will be crossed first. The; implementation of these features is done at shape level, but the local; mother-daughters management is handled by volumes. These build; additional optimization structures upon geometry closure. In order to; have navigation features properly working one has to follow some rules; for building a valid geometry. - The daughter volume(s) must not extrude the mother shape. They are; allowed however to have a common boundaries.; - The volumes positioned in the same container must not overlap with; each other. They may touch on one boundaries or shape vertex. The daughter nodes of a volume can be also removed or replaced with; other nodes:. ``` {.cpp}; void RemoveNode(TGeoNode* node); TGeoNode*ReplaceNode(TGeoNode* nodeorig, TGeoShape* newshape = 0,; TGeoMatrix* newpos = 0, TGeoMedium* newmed = 0); ```. The last method allows replacing an existing daughter of a volume with; another one. Providing only the node to be replaced will just create a; new volume for the node but having exactly the same parameters as the; old one. This helps in case of divisions for decoupling a node from the; logical hierarchy so getting new content/properties. For non-di",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md:68511,optimiz,optimization,68511,documentation/users-guide/Geometry.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md,1,['optimiz'],['optimization']
Performance,"trix=gGeoIdentity); ~~~. The objects referencing a volume and a transformation are called `NODES`; and their creation is fully handled by the modeller. They represent the; link elements in the hierarchy of volumes. Nodes are unique and distinct; geometrical objects ONLY from their container point of view. Since; volumes can be replicated in the geometry, the same node may be found on; different branches. In order to provide navigation features, volumes have to be able to find; the proper container of any point defined in the local reference frame.; This can be the volume itself, one of its positioned daughter volumes or; none if the point is actually outside. On the other hand, volumes have; to provide also other navigation methods such as finding the distances; to its shape boundaries or which daughter will be crossed first. The; implementation of these features is done at shape level, but the local; mother-daughters management is handled by volumes. These build; additional optimization structures upon geometry closure. In order to; have navigation features properly working one has to follow some rules; for building a valid geometry. - The daughter volume(s) must not extrude the mother shape. They are; allowed however to have a common boundaries.; - The volumes positioned in the same container must not overlap with; each other. They may touch on one boundaries or shape vertex. The daughter nodes of a volume can be also removed or replaced with; other nodes:. ~~~{.cpp}; void RemoveNode(TGeoNode* node); TGeoNode*ReplaceNode(TGeoNode* nodeorig, TGeoShape* newshape = 0,; TGeoMatrix* newpos = 0, TGeoMedium* newmed = 0); ~~~. The last method allows replacing an existing daughter of a volume with; another one. Providing only the node to be replaced will just create a; new volume for the node but having exactly the same parameters as the; old one. This helps in case of divisions for decoupling a node from the; logical hierarchy so getting new content/properties. For non-div",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md:28932,optimiz,optimization,28932,geom/geom/doc/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md,1,['optimiz'],['optimization']
Performance,"trols behavior of L2 cache. The default value is off (0). ======================================== ================================================; Syntax Description; ======================================== ================================================; slc Set slc bit to 1.; ======================================== ================================================. .. _amdgpu_synid_tfe:. tfe; ~~~. Controls access to partially resident textures. The default value is off (0). ======================================== ================================================; Syntax Description; ======================================== ================================================; tfe Set tfe bit to 1.; ======================================== ================================================. .. _amdgpu_synid_sc0:. sc0; ~~~. For atomic opcodes, this modifier indicates that the instruction returns the value from memory; before the operation. For other opcodes, it is used together with :ref:`sc1<amdgpu_synid_sc1>`; to specify cache policy. ======================================== ================================================; Syntax Description; ======================================== ================================================; sc0 Set sc0 bit to 1.; ======================================== ================================================. .. _amdgpu_synid_sc1:. sc1; ~~~. This modifier is used together with :ref:`sc0<amdgpu_synid_sc0>` to specify cache; policy. ======================================== ================================================; Syntax Description; ======================================== ================================================; sc1 Set sc1 bit to 1.; ======================================== ================================================. .. _amdgpu_synid_nt:. nt; ~~. Indicates an operation with non-temporal data. ======================================== ================================================; Syntax Description; ======",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUModifierSyntax.rst:20125,cache,cache,20125,interpreter/llvm-project/llvm/docs/AMDGPUModifierSyntax.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUModifierSyntax.rst,1,['cache'],['cache']
Performance,"tronger than; unordered (this is; termed the; release-fence-paired-atomic).; This satisfies the; requirements of; release. fence acq_rel - agent *none* 1. s_waitcnt lgkmcnt(0) &; - system vmcnt(0). - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate; (see comment for; previous fence).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; buffer_wbinvl1_vol.; - Ensures that the; preceding; global/local/generic; load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; acquire-fence-paired-atomic); has completed; before invalidating; the cache. This; satisfies the; requirements of; acquire.; - Ensures that all; previous memory; operations have; completed before a; following; global/local/generic; store; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; release-fence-paired-atomic).; This satisfies the; requirements of; release. 2. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. This; satisfies the; requirements of; acquire. **Sequential Consistent Atomic**; ------------------------------------------------------------------------------------; load atomic seq_cst - singlethread - global *Same as corresponding; - wavefront - local load atomic acquire,; - generic except must generate; all inst",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:229326,load,load,229326,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,"['cache', 'load']","['cache', 'load']"
Performance,"tructions are retired in program order. The register file is notified of the; retirement so that it can free the physical registers that were allocated for; the instruction during the register renaming stage. Load/Store Unit and Memory Consistency Model; """"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""; To simulate an out-of-order execution of memory operations, :program:`llvm-mca`; utilizes a simulated load/store unit (LSUnit) to simulate the speculative; execution of loads and stores. Each load (or store) consumes an entry in the load (or store) queue. Users can; specify flags ``-lqueue`` and ``-squeue`` to limit the number of entries in the; load and store queues respectively. The queues are unbounded by default. The LSUnit implements a relaxed consistency model for memory loads and stores.; The rules are:. 1. A younger load is allowed to pass an older load only if there are no; intervening stores or barriers between the two loads.; 2. A younger load is allowed to pass an older store provided that the load does; not alias with the store.; 3. A younger store is not allowed to pass an older store.; 4. A younger store is not allowed to pass an older load. By default, the LSUnit optimistically assumes that loads do not alias; (`-noalias=true`) store operations. Under this assumption, younger loads are; always allowed to pass older stores. Essentially, the LSUnit does not attempt; to run any alias analysis to predict when loads and stores do not alias with; each other. Note that, in the case of write-combining memory, rule 3 could be relaxed to; allow reordering of non-aliasing store operations. That being said, at the; moment, there is no way to further relax the memory model (``-noalias`` is the; only option). Essentially, there is no option to specify a different memory; type (e.g., write-back, write-combining, write-through; etc.) and consequently; to weaken, or strengthen, the memory model. Other limitations are:. * The LSUnit does not know when store-to-load forward",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst:39935,load,load,39935,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,2,['load'],['load']
Performance,"tructor; and destructor that initializes and destroys the global iostream objects; before they could possibly be used in the file. The code that you see in the; ``.ll`` file corresponds to the constructor and destructor registration code. If you would like to make it easier to *understand* the LLVM code generated; by the compiler in the demo page, consider using ``printf()`` instead of; ``iostream``\s to print values. Where did all of my code go??; -----------------------------; If you are using the LLVM demo page, you may often wonder what happened to; all of the code that you typed in. Remember that the demo script is running; the code through the LLVM optimizers, so if your code doesn't actually do; anything useful, it might all be deleted. To prevent this, make sure that the code is actually needed. For example, if; you are computing some expression, return the value from the function instead; of leaving it in a local variable. If you really want to constrain the; optimizer, you can read from and assign to ``volatile`` global variables. What is this ""``undef``"" thing that shows up in my code?; --------------------------------------------------------; ``undef`` is the LLVM way of representing a value that is not defined. You; can get these if you do not initialize a variable before you use it. For; example, the C function:. .. code-block:: c. int X() { int i; return i; }. Is compiled to ""``ret i32 undef``"" because ""``i``"" never has a value specified; for it. Why does instcombine + simplifycfg turn a call to a function with a mismatched calling convention into ""unreachable""? Why not make the verifier reject it?; ----------------------------------------------------------------------------------------------------------------------------------------------------------; This is a common problem run into by authors of front-ends that are using; custom calling conventions: you need to make sure to set the right calling; convention on both the function and on each call to ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/FAQ.rst:8520,optimiz,optimizer,8520,interpreter/llvm-project/llvm/docs/FAQ.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/FAQ.rst,1,['optimiz'],['optimizer']
Performance,"true is a required pass. For example:. .. code-block:: c++. class HelloWorldPass : public PassInfoMixin<HelloWorldPass> {; public:; PreservedAnalyses run(Function &F, FunctionAnalysisManager &AM);. static bool isRequired() { return true; }; };. A required pass is a pass that may not be skipped. An example of a required; pass is ``AlwaysInlinerPass``, which must always be run to preserve; ``alwaysinline`` semantics. Pass managers are required since they may contain; other required passes. An example of how a pass can be skipped is the ``optnone`` function; attribute, which specifies that optimizations should not be run on the; function. Required passes will still be run on ``optnone`` functions. For more implementation details, see; ``PassInstrumentation::runBeforePass()``. Registering passes as plugins; -----------------------------. LLVM provides a mechanism to register pass plugins within various tools like; ``clang`` or ``opt``. A pass plugin can add passes to default optimization; pipelines or to be manually run via tools like ``opt``. For more information,; see :doc:`NewPassManager`. Create a CMake project at the root of the repo alongside; other projects. This project must contain the following minimal; ``CMakeLists.txt``:. .. code-block:: cmake. add_llvm_pass_plugin(MyPassName source.cpp). See the definition of ``add_llvm_pass_plugin`` for more CMake details. The pass must provide at least one of two entry points for the new pass manager,; one for static registration and one for dynamically loaded plugins:. - ``llvm::PassPluginLibraryInfo get##Name##PluginInfo();``; - ``extern ""C"" ::llvm::PassPluginLibraryInfo llvmGetPassPluginInfo() LLVM_ATTRIBUTE_WEAK;``. Pass plugins are compiled and linked dynamically by default. Setting; ``LLVM_${NAME}_LINK_INTO_TOOLS`` to ``ON`` turns the project into a statically; linked extension. For an in-tree example, see ``llvm/examples/Bye/``. To make ``PassBuilder`` aware of statically linked pass plugins:. .. code-block:: c++. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMNewPMPass.rst:7505,optimiz,optimization,7505,interpreter/llvm-project/llvm/docs/WritingAnLLVMNewPMPass.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMNewPMPass.rst,1,['optimiz'],['optimization']
Performance,"trumenting code with sanitizers, it can be important to skip certain; functions to ensure no instrumentation is applied to them. This attribute is not always similar to absent ``sanitize_<name>``; attributes: depending on the specific sanitizer, code can be inserted into; functions regardless of the ``sanitize_<name>`` attribute to prevent false; positive reports. ``disable_sanitizer_instrumentation`` disables all kinds of instrumentation,; taking precedence over the ``sanitize_<name>`` attributes and other compiler; flags.; ``""dontcall-error""``; This attribute denotes that an error diagnostic should be emitted when a; call of a function with this attribute is not eliminated via optimization.; Front ends can provide optional ``srcloc`` metadata nodes on call sites of; such callees to attach information about where in the source language such a; call came from. A string value can be provided as a note.; ``""dontcall-warn""``; This attribute denotes that a warning diagnostic should be emitted when a; call of a function with this attribute is not eliminated via optimization.; Front ends can provide optional ``srcloc`` metadata nodes on call sites of; such callees to attach information about where in the source language such a; call came from. A string value can be provided as a note.; ``fn_ret_thunk_extern``; This attribute tells the code generator that returns from functions should; be replaced with jumps to externally-defined architecture-specific symbols.; For X86, this symbol's identifier is ``__x86_return_thunk``.; ``""frame-pointer""``; This attribute tells the code generator whether the function; should keep the frame pointer. The code generator may emit the frame pointer; even if this attribute says the frame pointer can be eliminated.; The allowed string values are:. * ``""none""`` (default) - the frame pointer can be eliminated.; * ``""non-leaf""`` - the frame pointer should be kept if the function calls; other functions.; * ``""all""`` - the frame pointer should be ke",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:82533,optimiz,optimization,82533,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['optimiz'],['optimization']
Performance,"try block guarantees that the alloca is only; executed once, which makes analysis simpler.; #. mem2reg only promotes allocas whose uses are direct loads and stores.; If the address of the stack object is passed to a function, or if any; funny pointer arithmetic is involved, the alloca will not be; promoted.; #. mem2reg only works on allocas of `first; class <../../LangRef.html#first-class-types>`_ values (such as pointers,; scalars and vectors), and only if the array size of the allocation is; 1 (or missing in the .ll file). mem2reg is not capable of promoting; structs or arrays to registers. Note that the ""sroa"" pass is; more powerful and can promote structs, ""unions"", and arrays in many; cases. All of these properties are easy to satisfy for most imperative; languages, and we'll illustrate it below with Kaleidoscope. The final; question you may be asking is: should I bother with this nonsense for my; front-end? Wouldn't it be better if I just did SSA construction; directly, avoiding use of the mem2reg optimization pass? In short, we; strongly recommend that you use this technique for building SSA form,; unless there is an extremely good reason not to. Using this technique; is:. - Proven and well tested: clang uses this technique; for local mutable variables. As such, the most common clients of LLVM; are using this to handle a bulk of their variables. You can be sure; that bugs are found fast and fixed early.; - Extremely Fast: mem2reg has a number of special cases that make it; fast in common cases as well as fully general. For example, it has; fast-paths for variables that are only used in a single block,; variables that only have one assignment point, good heuristics to; avoid insertion of unneeded phi nodes, etc.; - Needed for debug info generation: `Debug information in; LLVM <../../SourceLevelDebugging.html>`_ relies on having the address of; the variable exposed so that debug info can be attached to it. This; technique dovetails very naturally with this style",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:9028,optimiz,optimization,9028,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,1,['optimiz'],['optimization']
Performance,"ts and the result have the bitwidth specified; by the name of the builtin. These builtins can be used within constant; expressions. ``__builtin_unreachable``; -------------------------. ``__builtin_unreachable`` is used to indicate that a specific point in the; program cannot be reached, even if the compiler might otherwise think it can.; This is useful to improve optimization and eliminates certain warnings. For; example, without the ``__builtin_unreachable`` in the example below, the; compiler assumes that the inline asm can fall through and prints a ""function; declared '``noreturn``' should not return"" warning. **Syntax**:. .. code-block:: c++. __builtin_unreachable(). **Example of use**:. .. code-block:: c++. void myabort(void) __attribute__((noreturn));; void myabort(void) {; asm(""int3"");; __builtin_unreachable();; }. **Description**:. The ``__builtin_unreachable()`` builtin has completely undefined behavior.; Since it has undefined behavior, it is a statement that it is never reached and; the optimizer can take advantage of this to produce better code. This builtin; takes no arguments and produces a void result. Query for this feature with ``__has_builtin(__builtin_unreachable)``. ``__builtin_unpredictable``; ---------------------------. ``__builtin_unpredictable`` is used to indicate that a branch condition is; unpredictable by hardware mechanisms such as branch prediction logic. **Syntax**:. .. code-block:: c++. __builtin_unpredictable(long long). **Example of use**:. .. code-block:: c++. if (__builtin_unpredictable(x > 0)) {; foo();; }. **Description**:. The ``__builtin_unpredictable()`` builtin is expected to be used with control; flow conditions such as in ``if`` and ``switch`` statements. Query for this feature with ``__has_builtin(__builtin_unpredictable)``. ``__builtin_expect``; --------------------. ``__builtin_expect`` is used to indicate that the value of an expression is; anticipated to be the same as a statically known result. **Syntax**:. .. code-",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst:113056,optimiz,optimizer,113056,interpreter/llvm-project/clang/docs/LanguageExtensions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst,1,['optimiz'],['optimizer']
Performance,"ts any inline lock-free atomic operations of a given size, you should; support *ALL* operations of that size in a lock-free manner. When the target implements atomic ``cmpxchg`` or LL/SC instructions (as most do); this is trivial: all the other operations can be implemented on top of those; primitives. However, on many older CPUs (e.g. ARMv5, SparcV8, Intel 80386) there; are atomic load and store instructions, but no ``cmpxchg`` or LL/SC. As it is; invalid to implement ``atomic load`` using the native instruction, but; ``cmpxchg`` using a library call to a function that uses a mutex, ``atomic; load`` must *also* expand to a library call on such architectures, so that it; can remain atomic with regards to a simultaneous ``cmpxchg``, by using the same; mutex. AtomicExpandPass can help with that: it will expand all atomic operations to the; proper ``__atomic_*`` libcalls for any size above the maximum set by; ``setMaxAtomicSizeInBitsSupported`` (which defaults to 0). On x86, all atomic loads generate a ``MOV``. SequentiallyConsistent stores; generate an ``XCHG``, other stores generate a ``MOV``. SequentiallyConsistent; fences generate an ``MFENCE``, other fences do not cause any code to be; generated. ``cmpxchg`` uses the ``LOCK CMPXCHG`` instruction. ``atomicrmw xchg``; uses ``XCHG``, ``atomicrmw add`` and ``atomicrmw sub`` use ``XADD``, and all; other ``atomicrmw`` operations generate a loop with ``LOCK CMPXCHG``. Depending; on the users of the result, some ``atomicrmw`` operations can be translated into; operations like ``LOCK AND``, but that does not work in general. On ARM (before v8), MIPS, and many other RISC architectures, Acquire, Release,; and SequentiallyConsistent semantics require barrier instructions for every such; operation. Loads and stores generate normal instructions. ``cmpxchg`` and; ``atomicrmw`` can be represented using a loop with LL/SC-style instructions; which take some sort of exclusive lock on a cache line (``LDREX`` and ``STREX``; on ARM, etc",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst:19743,load,loads,19743,interpreter/llvm-project/llvm/docs/Atomics.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst,1,['load'],['loads']
Performance,"ts as a string, JITs and executes them.; Just like ``cppdef``, execution is in the global scope and all previously; loaded code is available.; If the statements are declarations, the effect is the same as ``cppdef``,; but ``cppexec`` also accepts executable lines.; Example::. >>> cppyy.cppexec(r""""""std::string hello = ""Hello, World!"";""""""); True; >>> cppyy.cppexec(""std::cout << hello << std::endl;""); Hello, World!; True; >>> . * ``include``: load declarations into the interpreter.; This function accepts C++ declarations from a file, typically a header.; Files are located through include paths given to the Cling.; Example::. >>> cppyy.include(""vector"") # equivalent to ""#include <vector>""; True; >>> . * ``c_include``: load declarations into the interpreter.; This function accepts C++ declarations from a file, typically a header.; Name mangling is an important difference between C and C++ code.; The use of ``c_include`` instead of ``include`` prevents mangling. * ``load_library``: load compiled C++ into the interpreter.; This function takes the name of a shared library and loads it into current; process, exposing all external symbols to Cling.; Libraries are located through load paths given to Cling, either through the; ""-L"" compiler flag or the dynamic search path environment variable (system; dependent).; Any method that brings symbols into the process (including normal linking,; e.g. when embedding Python in a C++ application) is suitable to expose; symbols.; An alternative for ``load_library`` is for example ``ctypes.CDLL``, but; that function does not respect dynamic load paths on all platforms. If a compilation error occurs during JITing of C++ code in any of the above; helpers, a Python ``SyntaxError`` exception is raised.; If a compilation warning occurs, a Python warning is issued. `Configuring Cling`; -------------------. It is often convenient to add additional search paths for Cling to find; headers and libraries when loading a module (Python does not have st",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/toplevel.rst:2588,load,load,2588,bindings/pyroot/cppyy/cppyy/doc/source/toplevel.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/toplevel.rst,1,['load'],['load']
Performance,"ts second and third; operands, if they agree in classification, or else the other if one is known; retain-agnostic. If the cast operand is known retained, the conversion is treated as a; ``__bridge_transfer`` cast. If the cast operand is known unretained or known; retain-agnostic, the conversion is treated as a ``__bridge`` cast. .. admonition:: Rationale. Bridging casts are annoying. Absent the ability to completely automate the; management of CF objects, however, we are left with relatively poor attempts; to reduce the need for a glut of explicit bridges. Hence these rules. We've so far consciously refrained from implicitly turning retained CF; results from function calls into ``__bridge_transfer`` casts. The worry is; that some code patterns --- for example, creating a CF value, assigning it; to an ObjC-typed local, and then calling ``CFRelease`` when done --- are a; bit too likely to be accidentally accepted, leading to mysterious behavior. For loads from ``const`` global variables of :ref:`C retainable pointer type; <arc.misc.c-retainable>`, it is reasonable to assume that global system; constants were initialized with true constants (e.g. string literals), but; user constants might have been initialized with something dynamically; allocated, using a global initializer. .. _arc.objects.restrictions.conversion-exception-contextual:. Conversion from retainable object pointer type in certain contexts; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. :when-revised:`[beginning Apple 4.0, LLVM 3.1]`. If an expression of retainable object pointer type is explicitly cast to a; :ref:`C retainable pointer type <arc.misc.c-retainable>`, the program is; ill-formed as discussed above unless the result is immediately used:. * to initialize a parameter in an Objective-C message send where the parameter; is not marked with the ``cf_consumed`` attribute, or; * to initialize a parameter in a direct call to an; :ref:`audited <arc.misc.c-retainable.audit>` funct",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst:28770,load,loads,28770,interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,1,['load'],['loads']
Performance,"ts that pre-populate the CMakeCache in; a build directory with commonly used settings. You can use the caches files with the following CMake invocation:. cmake -G <build system>; -C <path to cache file>; [additional CMake options (i.e. -DCMAKE_INSTALL_PREFIX=<install path>)]; <path to llvm>. Options specified on the command line will override options in the cache files. The following cache files exist. Apple-stage1; ------------. The Apple stage1 cache configures a two stage build similar to how Apple builds; the clang shipped with Xcode. The build files generated from this invocation has; a target named ""stage2"" which performs an LTO build of clang. The Apple-stage2 cache can be used directly to match the build settings Apple; uses in shipping builds without doing a full bootstrap build. PGO; ---. The PGO CMake cache can be used to generate a multi-stage instrumented compiler.; You can configure your build directory with the following invocation of CMake:. cmake -G <generator> -C <path_to_clang>/cmake/caches/PGO.cmake <source dir>. After configuration the following additional targets will be generated:. stage2-instrumented:; Builds a stage1 x86 compiler, runtime, and required tools (llvm-config,; llvm-profdata) then uses that compiler to build an instrumented stage2 compiler. stage2-instrumented-generate-profdata:; Depends on ""stage2-instrumented"" and will use the instrumented compiler to; generate profdata based on the training files in <clang>/utils/perf-training. stage2:; Depends on ""stage2-instrumented-generate-profdata"" and will use the stage1; compiler with the stage2 profdata to build a PGO-optimized compiler. stage2-check-llvm:; Depends on stage2 and runs check-llvm using the stage3 compiler. stage2-check-clang:; Depends on stage2 and runs check-clang using the stage3 compiler. stage2-check-all:; Depends on stage2 and runs check-all using the stage3 compiler. stage2-test-suite:; Depends on stage2 and runs the test-suite using the stage3 compiler (requires; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/cmake/caches/README.txt:1087,cache,caches,1087,interpreter/llvm-project/clang/cmake/caches/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/cmake/caches/README.txt,1,['cache'],['caches']
Performance,"ts the `code generator debugger`_. Finally, if the output of the selected code generator matches the reference; output, ``bugpoint`` runs the test program after all of the LLVM passes have; been applied to it. If its output differs from the reference output, it assumes; the difference resulted from a failure in one of the LLVM passes, and enters the; `miscompilation debugger`_. Otherwise, there is no problem ``bugpoint`` can; debug. .. _crash debugger:. Crash debugger; --------------. If an optimizer or code generator crashes, ``bugpoint`` will try as hard as it; can to reduce the list of passes (for optimizer crashes) and the size of the; test program. First, ``bugpoint`` figures out which combination of optimizer; passes triggers the bug. This is useful when debugging a problem exposed by; ``opt``, for example, because it runs over 38 passes. Next, ``bugpoint`` tries removing functions from the test program, to reduce its; size. Usually it is able to reduce a test program to a single function, when; debugging intraprocedural optimizations. Once the number of functions has been; reduced, it attempts to delete various edges in the control flow graph, to; reduce the size of the function as much as possible. Finally, ``bugpoint``; deletes any individual LLVM instructions whose absence does not eliminate the; failure. At the end, ``bugpoint`` should tell you what passes crash, give you a; bitcode file, and give you instructions on how to reproduce the failure with; ``opt`` or ``llc``. .. _code generator debugger:. Code generator debugger; -----------------------. The code generator debugger attempts to narrow down the amount of code that is; being miscompiled by the selected code generator. To do this, it takes the test; program and partitions it into two pieces: one piece which it compiles with the; ""safe"" backend (into a shared object), and one piece which it runs with either; the JIT or the static LLC compiler. It uses several techniques to reduce the; amount of code",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Bugpoint.rst:3618,optimiz,optimizations,3618,interpreter/llvm-project/llvm/docs/Bugpoint.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Bugpoint.rst,1,['optimiz'],['optimizations']
Performance,"ts, by creating a separate ""Printer"" pass to consume the analysis; result and print it on the standard output in a textual format suitable for; FileCheck.; See `llvm/test/Analysis/BranchProbabilityInfo/loop.ll <https://github.com/llvm/llvm-project/blob/main/llvm/test/Analysis/BranchProbabilityInfo/loop.ll>`_; for an example of such test. ``test-suite``; --------------. The test suite contains whole programs, which are pieces of code which; can be compiled and linked into a stand-alone program that can be; executed. These programs are generally written in high level languages; such as C or C++. These programs are compiled using a user specified compiler and set of; flags, and then executed to capture the program output and timing; information. The output of these programs is compared to a reference; output to ensure that the program is being compiled correctly. In addition to compiling and executing programs, whole program tests; serve as a way of benchmarking LLVM performance, both in terms of the; efficiency of the programs generated as well as the speed with which; LLVM compiles, optimizes, and generates code. The test-suite is located in the ``test-suite``; `repository on GitHub <https://github.com/llvm/llvm-test-suite.git>`_. See the :doc:`TestSuiteGuide` for details. Debugging Information tests; ---------------------------. The test suite contains tests to check quality of debugging information.; The test are written in C based languages or in LLVM assembly language. These tests are compiled and run under a debugger. The debugger output; is checked to validate of debugging information. See README.txt in the; test suite for more information. This test suite is located in the; ``cross-project-tests/debuginfo-tests`` directory. Quick start; ===========. The tests are located in two separate repositories. The unit and; regression tests are in the main ""llvm""/ directory under the directories; ``llvm/unittests`` and ``llvm/test`` (so you get these tests for free with ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TestingGuide.rst:3748,perform,performance,3748,interpreter/llvm-project/llvm/docs/TestingGuide.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TestingGuide.rst,2,"['optimiz', 'perform']","['optimizes', 'performance']"
Performance,"ts::; :local:. Introduction; ============. This document describes the CommandLine argument processing library. It will; show you how to use it, and what it can do. The CommandLine library uses a; declarative approach to specifying the command line options that your program; takes. By default, these options declarations implicitly hold the value parsed; for the option declared (of course this `can be changed`_). Although there are a **lot** of command line argument parsing libraries out; there in many different languages, none of them fit well with what I needed. By; looking at the features and problems of other libraries, I designed the; CommandLine library to have the following features:. #. Speed: The CommandLine library is very quick and uses little resources. The; parsing time of the library is directly proportional to the number of; arguments parsed, not the number of options recognized. Additionally,; command line argument values are captured transparently into user defined; global variables, which can be accessed like any other variable (and with the; same performance). #. Type Safe: As a user of CommandLine, you don't have to worry about; remembering the type of arguments that you want (is it an int? a string? a; bool? an enum?) and keep casting it around. Not only does this help prevent; error prone constructs, it also leads to dramatically cleaner source code. #. No subclasses required: To use CommandLine, you instantiate variables that; correspond to the arguments that you would like to capture, you don't; subclass a parser. This means that you don't have to write **any**; boilerplate code. #. Globally accessible: Libraries can specify command line arguments that are; automatically enabled in any tool that links to the library. This is; possible because the application doesn't have to keep a list of arguments to; pass to the parser. This also makes supporting `dynamically loaded options`_; trivial. #. Cleaner: CommandLine supports enum and other types di",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandLine.rst:1186,perform,performance,1186,interpreter/llvm-project/llvm/docs/CommandLine.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandLine.rst,1,['perform'],['performance']
Performance,"ts::; :local:; :depth: 2. Introduction; ============. GWP-ASan is a sampled allocator framework that assists in finding use-after-free; and heap-buffer-overflow bugs in production environments. It informally is a; recursive acronym, ""**G**\WP-ASan **W**\ill **P**\rovide **A**\llocation; **SAN**\ity"". GWP-ASan is based on the classic; `Electric Fence Malloc Debugger <https://linux.die.net/man/3/efence>`_, with a; key adaptation. Notably, we only choose a very small percentage of allocations; to sample, and apply guard pages to these sampled allocations only. The sampling; is small enough to allow us to have very low performance overhead. There is a small, tunable memory overhead that is fixed for the lifetime of the; process. This is approximately ~40KiB per process using the default settings,; depending on the average size of your allocations. GWP-ASan vs. ASan; =================. Unlike `AddressSanitizer <https://clang.llvm.org/docs/AddressSanitizer.html>`_,; GWP-ASan does not induce a significant performance overhead. ASan often requires; the use of dedicated canaries to be viable in production environments, and as; such is often impractical. GWP-ASan is only capable of finding a subset of the memory issues detected by; ASan. Furthermore, GWP-ASan's bug detection capabilities are only probabilistic.; As such, we recommend using ASan over GWP-ASan in testing, as well as anywhere; else that guaranteed error detection is more valuable than the 2x execution; slowdown/binary size bloat. For the majority of production environments, this; impact is too high, and GWP-ASan proves extremely useful. Design; ======. **Please note:** The implementation of GWP-ASan is largely in-flux, and these; details are subject to change. There are currently other implementations of; GWP-ASan, such as the implementation featured in; `Chromium <https://cs.chromium.org/chromium/src/components/gwp_asan/>`_. The; long-term support goal is to ensure feature-parity where reasonable, and to; suppo",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GwpAsan.rst:1053,perform,performance,1053,interpreter/llvm-project/llvm/docs/GwpAsan.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GwpAsan.rst,1,['perform'],['performance']
Performance,"ts; are reasonably small, a ``SmallSet<Type, N>`` is a good choice. This set has; space for N elements in place (thus, if the set is dynamically smaller than N,; no malloc traffic is required) and accesses them with a simple linear search.; When the set grows beyond N elements, it allocates a more expensive; representation that guarantees efficient access (for most types, it falls back; to :ref:`std::set <dss_set>`, but for pointers it uses something far better,; :ref:`SmallPtrSet <dss_smallptrset>`. The magic of this class is that it handles small sets extremely efficiently, but; gracefully handles extremely large sets without loss of efficiency. .. _dss_smallptrset:. llvm/ADT/SmallPtrSet.h; ^^^^^^^^^^^^^^^^^^^^^^. ``SmallPtrSet`` has all the advantages of ``SmallSet`` (and a ``SmallSet`` of; pointers is transparently implemented with a ``SmallPtrSet``). If more than N; insertions are performed, a single quadratically probed hash table is allocated; and grows as needed, providing extremely efficient access (constant time; insertion/deleting/queries with low constant factors) and is very stingy with; malloc traffic. Note that, unlike :ref:`std::set <dss_set>`, the iterators of ``SmallPtrSet``; are invalidated whenever an insertion occurs. Also, the values visited by the; iterators are not visited in sorted order. .. _dss_stringset:. llvm/ADT/StringSet.h; ^^^^^^^^^^^^^^^^^^^^. ``StringSet`` is a thin wrapper around :ref:`StringMap\<char\> <dss_stringmap>`,; and it allows efficient storage and retrieval of unique strings. Functionally analogous to ``SmallSet<StringRef>``, ``StringSet`` also supports; iteration. (The iterator dereferences to a ``StringMapEntry<char>``, so you; need to call ``i->getKey()`` to access the item of the StringSet.) On the; other hand, ``StringSet`` doesn't support range-insertion and; copy-construction, which :ref:`SmallSet <dss_smallset>` and :ref:`SmallPtrSet; <dss_smallptrset>` do support. .. _dss_denseset:. llvm/ADT/DenseSet.h; ^^^^^^^^^",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ProgrammersManual.rst:78911,perform,performed,78911,interpreter/llvm-project/llvm/docs/ProgrammersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ProgrammersManual.rst,1,['perform'],['performed']
Performance,"tsForGC_intrinsic_lowering>` for details on GC specific; lowering. Optimizer is allowed to inline memory copy when it's profitable to do so. '``llvm.memmove.element.unordered.atomic``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". This is an overloaded intrinsic. You can use; ``llvm.memmove.element.unordered.atomic`` on any integer bit width and for; different address spaces. Not all targets support all bit widths however. ::. declare void @llvm.memmove.element.unordered.atomic.p0.p0.i32(ptr <dest>,; ptr <src>,; i32 <len>,; i32 <element_size>); declare void @llvm.memmove.element.unordered.atomic.p0.p0.i64(ptr <dest>,; ptr <src>,; i64 <len>,; i32 <element_size>). Overview:; """""""""""""""""". The '``llvm.memmove.element.unordered.atomic.*``' intrinsic is a specialization; of the '``llvm.memmove.*``' intrinsic. It differs in that the ``dest`` and; ``src`` are treated as arrays with elements that are exactly ``element_size``; bytes, and the copy between buffers uses a sequence of; :ref:`unordered atomic <ordering>` load/store operations that are a positive; integer multiple of the ``element_size`` in size. Arguments:; """""""""""""""""""". The first three arguments are the same as they are in the; :ref:`@llvm.memmove <int_memmove>` intrinsic, with the added constraint that; ``len`` is required to be a positive integer multiple of the ``element_size``.; If ``len`` is not a positive integer multiple of ``element_size``, then the; behaviour of the intrinsic is undefined. ``element_size`` must be a compile-time constant positive power of two no; greater than a target-specific atomic access size limit. For each of the input pointers the ``align`` parameter attribute must be; specified. It must be a power of two no less than the ``element_size``. Caller; guarantees that both the source and destination pointers are aligned to that; boundary. Semantics:; """""""""""""""""""". The '``llvm.memmove.element.unordered.atomic.*``' intrinsic copies ``len`` bytes; of memory ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:961038,load,load,961038,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,"tself is only; concerned with computing addresses. Can array indices be negative?; ------------------------------. Yes. This is basically a special case of array indices being out of bounds. Can I compare two values computed with GEPs?; --------------------------------------------. Yes. If both addresses are within the same allocated object, or; one-past-the-end, you'll get the comparison result you expect. If either is; outside of it, integer arithmetic wrapping may occur, so the comparison may not; be meaningful. Can I do GEP with a different pointer type than the type of the underlying object?; ----------------------------------------------------------------------------------. Yes. There are no restrictions on bitcasting a pointer value to an arbitrary; pointer type. The types in a GEP serve only to define the parameters for the; underlying integer computation. They need not correspond with the actual type of; the underlying object. Furthermore, loads and stores don't have to use the same types as the type of; the underlying object. Types in this context serve only to specify memory size; and alignment. Beyond that there are merely a hint to the optimizer indicating; how the value will likely be used. Can I cast an object's address to integer and add it to null?; -------------------------------------------------------------. You can compute an address that way, but if you use GEP to do the add, you can't; use that pointer to actually access the object, unless the object is managed; outside of LLVM. The underlying integer computation is sufficiently defined; null has a defined; value --- zero --- and you can add whatever value you want to it. However, it's invalid to access (load from or store to) an LLVM-aware object; with such a pointer. This includes ``GlobalVariables``, ``Allocas``, and objects; pointed to by noalias pointers. If you really need this functionality, you can do the arithmetic with explicit; integer instructions, and use inttoptr to convert the r",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GetElementPtr.rst:15105,load,loads,15105,interpreter/llvm-project/llvm/docs/GetElementPtr.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GetElementPtr.rst,1,['load'],['loads']
Performance,"tted by your pass is still valid and well formed LLVM, which; hasn't been broken somehow. Now that you have seen the basics of the mechanics behind passes, we can talk; about some more details of how they work and how to use them. .. _writing-an-llvm-pass-pass-classes:. Pass classes and requirements; =============================. One of the first things that you should do when designing a new pass is to; decide what class you should subclass for your pass. The :ref:`Hello World; <writing-an-llvm-pass-basiccode>` example uses the :ref:`FunctionPass; <writing-an-llvm-pass-FunctionPass>` class for its implementation, but we did; not discuss why or when this should occur. Here we talk about the classes; available, from the most general to the most specific. When choosing a superclass for your ``Pass``, you should choose the **most; specific** class possible, while still being able to meet the requirements; listed. This gives the LLVM Pass Infrastructure information necessary to; optimize how passes are run, so that the resultant compiler isn't unnecessarily; slow. The ``ImmutablePass`` class; ---------------------------. The most plain and boring type of pass is the ""`ImmutablePass; <https://llvm.org/doxygen/classllvm_1_1ImmutablePass.html>`_"" class. This pass; type is used for passes that do not have to be run, do not change state, and; never need to be updated. This is not a normal type of transformation or; analysis, but can provide information about the current compiler configuration. Although this pass class is very infrequently used, it is important for; providing information about the current target machine being compiled for, and; other static information that can affect the various transformations. ``ImmutablePass``\ es never invalidate other transformations, are never; invalidated, and are never ""run"". .. _writing-an-llvm-pass-ModulePass:. The ``ModulePass`` class; ------------------------. The `ModulePass <https://llvm.org/doxygen/classllvm_1_1ModulePass.html",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst:11770,optimiz,optimize,11770,interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst,1,['optimiz'],['optimize']
Performance,"tting it into another is not necessarily easier than re-doing it.; > Optimization code is usually heavily tied in to the specific IR they use. Understood. The only reason that I brought this up is because SGI's IR is; more similar to LLVM than it is different in many respects (SSA based,; relatively low level, etc), and could be easily adapted. Also their; optimizations are written in C++ and are actually somewhat; structured... of course it would be no walk in the park, but it would be; much less time consuming to adapt, say, SSA-PRE than to rewrite it. > But your larger point is valid that adding SSA based optimizations is; > feasible and should be fun. (Again, link time cost is the issue.). Assuming linktime cost wasn't an issue, the question is: ; Does using GCC's backend buy us anything?. > It also occurs to me that GCC is probably doing quite a bit of back-end; > optimization (step 16 in your list). Do you have a breakdown of that?. Not really. The irritating part of GCC is that it mixes it all up and; doesn't have a clean separation of concerns. A lot of the ""back end; optimization"" happens right along with other data optimizations (ie, CSE; of machine specific things). As far as REAL back end optimizations go, it looks something like this:. 1. Instruction combination: try to make CISCy instructions, if available; 2. Register movement: try to get registers in the right places for the; architecture to avoid register to register moves. For example, try to get; the first argument of a function to naturally land in %o0 for sparc.; 3. Instruction scheduling: 'nuff said :); 4. Register class preferencing: ??; 5. Local register allocation; 6. global register allocation; 7. Spilling; 8. Local regalloc; 9. Jump optimization; 10. Delay slot scheduling; 11. Branch shorting for CISC machines; 12. Instruction selection & peephole optimization; 13. Debug info output. But none of this would be usable for LLVM anyways, unless we were using; GCC as a static compiler. -Chris. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt:2088,optimiz,optimization,2088,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt,5,['optimiz'],"['optimization', 'optimizations']"
Performance,"ttrDocs.td``, and is; used for documenting user-facing attributes. General BackEnds; ================. Print Records; -------------. The TableGen command option ``--print-records`` invokes a simple backend; that prints all the classes and records defined in the source files. This is; the default backend option. See the :doc:`TableGen Backend Developer's Guide; <./BackGuide>` for more information. Print Detailed Records; ----------------------. The TableGen command option ``--print-detailed-records`` invokes a backend; that prints all the global variables, classes, and records defined in the; source files, with more detail than the default record printer. See the; :doc:`TableGen Backend Developer's Guide <./BackGuide>` for more; information. JSON Reference; --------------. **Purpose**: Output all the values in every ``def``, as a JSON data; structure that can be easily parsed by a variety of languages. Useful; for writing custom backends without having to modify TableGen itself,; or for performing auxiliary analysis on the same TableGen data passed; to a built-in backend. **Output**:. The root of the output file is a JSON object (i.e. dictionary),; containing the following fixed keys:. * ``!tablegen_json_version``: a numeric version field that will; increase if an incompatible change is ever made to the structure of; this data. The format described here corresponds to version 1. * ``!instanceof``: a dictionary whose keys are the class names defined; in the TableGen input. For each key, the corresponding value is an; array of strings giving the names of ``def`` records that derive; from that class. So ``root[""!instanceof""][""Instruction""]``, for; example, would list the names of all the records deriving from the; class ``Instruction``. For each ``def`` record, the root object also has a key for the record; name. The corresponding value is a subsidiary object containing the; following fixed keys:. * ``!superclasses``: an array of strings giving the names of all the; clas",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst:15069,perform,performing,15069,interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst,1,['perform'],['performing']
Performance,"tual register to a; physical one, use ``VirtRegMap::assignVirt2Phys(vreg, preg)``. In order to map; a certain virtual register to memory, use; ``VirtRegMap::assignVirt2StackSlot(vreg)``. This method will return the stack; slot where ``vreg``'s value will be located. If it is necessary to map another; virtual register to the same stack slot, use; ``VirtRegMap::assignVirt2StackSlot(vreg, stack_location)``. One important point; to consider when using the indirect mapping, is that even if a virtual register; is mapped to memory, it still needs to be mapped to a physical register. This; physical register is the location where the virtual register is supposed to be; found before being stored or after being reloaded. If the indirect strategy is used, after all the virtual registers have been; mapped to physical registers or stack slots, it is necessary to use a spiller; object to place load and store instructions in the code. Every virtual that has; been mapped to a stack slot will be stored to memory after being defined and will; be loaded before being used. The implementation of the spiller tries to recycle; load/store instructions, avoiding unnecessary instructions. For an example of; how to invoke the spiller, see ``RegAllocLinearScan::runOnMachineFunction`` in; ``lib/CodeGen/RegAllocLinearScan.cpp``. Handling two address instructions; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. With very rare exceptions (e.g., function calls), the LLVM machine code; instructions are three address instructions. That is, each instruction is; expected to define at most one register, and to use at most two registers.; However, some architectures use two address instructions. In this case, the; defined register is also one of the used registers. For instance, an instruction; such as ``ADD %EAX, %EBX``, in X86 is actually equivalent to ``%EAX = %EAX +; %EBX``. In order to produce correct code, LLVM must convert three address instructions; that represent two address instructions into true two address",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst:66116,load,loaded,66116,interpreter/llvm-project/llvm/docs/CodeGenerator.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst,1,['load'],['loaded']
Performance,"tunable parameters are the ones related; to the operating and window system, to the fonts to be used, to the; location of start-up files. At start-up, ROOT looks for a `.rootrc` file; in the following order:. - `./.rootrc //local directory`. - `$HOME/.rootrc //user directory`. - `$ROOTSYS/etc/system.rootrc //global ROOT directory`. If more than one `.rootrc` files are found in the search paths above,; the options are merged, with precedence local, user, global. The parsing; and interpretation of this file is handled by the ROOT class `TEnv`.; Have a look to its documentation if you need such rather advanced; features. The file `.rootrc` defines the location of two rather; important files inspected at start-up: `rootalias.C` and `rootlogon.C`.; They can contain code that needs to be loaded and executed at ROOT; startup. `rootalias.C` is only loaded and best used to define some often; used functions. `rootlogon.C` contains code that will be executed at; startup: this file is extremely useful for example to pre-load a custom; style for the plots created with ROOT. This is done most easily by; creating a new `TStyle` object with your preferred settings, as; described in the class reference guide, and then use the command; `gROOT->SetStyle(""MyStyleName"");` to make this new style definition the; default one. As an example, have a look in the file `rootlogon.C` coming; with this tutorial. Another relevant file is `rootlogoff.C` that it; called when the session is finished. ### ROOT command history ###. Every command typed at the ROOT prompt is stored in a file `.root_hist`; in your home directory. ROOT uses this file to allow for navigation in; the command history with the up-arrow and down-arrow keys. It is also; convenient to extract successful ROOT commands with the help of a text; editor for use in your own macros. ### ROOT Global Pointers ###. All global pointers in ROOT begin with a small ""g"". Some of them were; already implicitly introduced (for example in the secti",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/primer/ROOT_as_calculator.md:19039,load,load,19039,documentation/primer/ROOT_as_calculator.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/primer/ROOT_as_calculator.md,1,['load'],['load']
Performance,"tures from OpenMP 5.0 and 5.1; see `OpenMP implementation details`_ and `OpenMP 51 implementation details`_. General improvements; ====================; - New collapse clause scheme to avoid expensive remainder operations.; Compute loop index variables after collapsing a loop nest via the; collapse clause by replacing the expensive remainder operation with; multiplications and additions. - When using the collapse clause on a loop nest the default behavior; is to automatically extend the representation of the loop counter to; 64 bits for the cases where the sizes of the collapsed loops are not; known at compile time. To prevent this conservative choice and use; at most 32 bits, compile your program with the; `-fopenmp-optimistic-collapse`. GPU devices support; ===================. Data-sharing modes; ------------------. Clang supports two data-sharing models for Cuda devices: `Generic` and `Cuda`; modes. The default mode is `Generic`. `Cuda` mode can give an additional; performance and can be activated using the `-fopenmp-cuda-mode` flag. In; `Generic` mode all local variables that can be shared in the parallel regions; are stored in the global memory. In `Cuda` mode local variables are not shared; between the threads and it is user responsibility to share the required data; between the threads in the parallel regions. Often, the optimizer is able to; reduce the cost of `Generic` mode to the level of `Cuda` mode, but the flag,; as well as other assumption flags, can be used for tuning. Features not supported or with limited support for Cuda devices; ---------------------------------------------------------------. - Cancellation constructs are not supported. - Doacross loop nest is not supported. - User-defined reductions are supported only for trivial types. - Nested parallelism: inner parallel regions are executed sequentially. - Debug information for OpenMP target regions is supported, but sometimes it may; be required to manually specify the address class of the i",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/OpenMPSupport.rst:1678,perform,performance,1678,interpreter/llvm-project/clang/docs/OpenMPSupport.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/OpenMPSupport.rst,1,['perform'],['performance']
Performance,"turing a send of; ``retain`` followed by a send of ``release`` to the same object, with no; intervening ``release`` on that object, is not equivalent under the high-level; semantics to a computation history in which these sends are removed. Note that; this implies that these methods may not raise exceptions. It is undefined behavior if a computation history features any use whatsoever; of an object following the completion of a send of ``release`` that is not; preceded by a send of ``retain`` to the same object. The behavior of ``autorelease`` must be equivalent to sending ``release`` when; one of the autorelease pools currently in scope is popped. It may not throw an; exception. When the semantics call for performing one of these operations on a retainable; object pointer, if that pointer is ``null`` then the effect is a no-op. All of the semantics described in this document are subject to additional; :ref:`optimization rules <arc.optimization>` which permit the removal or; optimization of operations based on local knowledge of data flow. The; semantics describe the high-level behaviors that the compiler implements, not; an exact sequence of operations that a program will be compiled into. .. _arc.objects.operands:. Retainable object pointers as operands and arguments; ----------------------------------------------------. In general, ARC does not perform retain or release operations when simply using; a retainable object pointer as an operand within an expression. This includes:. * loading a retainable pointer from an object with non-weak :ref:`ownership; <arc.ownership>`,; * passing a retainable pointer as an argument to a function or method, and; * receiving a retainable pointer as the result of a function or method call. .. admonition:: Rationale. While this might seem uncontroversial, it is actually unsafe when multiple; expressions are evaluated in ""parallel"", as with binary operators and calls,; because (for example) one expression might load from an object wh",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst:15090,optimiz,optimization,15090,interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,2,['optimiz'],['optimization']
Performance,"turn since the locations may be used for vector memory instructions by a; future wavefront that uses the same scratch area, or a function call that; creates a frame at the same address, respectively. There is no need for a; ``s_dcache_inv`` as all scalar writes are write-before-read in the same thread. For kernarg backing memory:. * CP invalidates the L1 cache at the start of each kernel dispatch.; * On dGPU the kernarg backing memory is allocated in host memory accessed as; MTYPE UC (uncached) to avoid needing to invalidate the L2 cache. This also; causes it to be treated as non-volatile and so is not invalidated by; ``*_vol``.; * On APU the kernarg backing memory it is accessed as MTYPE CC (cache coherent); and so the L2 cache will be coherent with the CPU and other agents. Scratch backing memory (which is used for the private address space) is accessed; with MTYPE NC_NV (non-coherent non-volatile). Since the private address space is; only accessed by a single thread, and is always write-before-read, there is; never a need to invalidate these entries from the L1 cache. Hence all cache; invalidates are done as ``*_vol`` to only invalidate the volatile cache lines. The code sequences used to implement the memory model for GFX6-GFX9 are defined; in table :ref:`amdgpu-amdhsa-memory-model-code-sequences-gfx6-gfx9-table`. .. table:: AMDHSA Memory Model Code Sequences GFX6-GFX9; :name: amdgpu-amdhsa-memory-model-code-sequences-gfx6-gfx9-table. ============ ============ ============== ========== ================================; LLVM Instr LLVM Memory LLVM Memory AMDGPU AMDGPU Machine Code; Ordering Sync Scope Address GFX6-GFX9; Space; ============ ============ ============== ========== ================================; **Non-Atomic**; ------------------------------------------------------------------------------------; load *none* *none* - global - !volatile & !nontemporal; - generic; - private 1. buffer/global/flat_load; - constant; - !volatile & nontemporal. 1. buffer/g",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:211253,cache,cache,211253,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['cache'],['cache']
Performance,"tween. .. code-block:: c. typedef struct {; int *__counted_by(count) buf; size_t count;; } sized_buf_t;. void alloc_buf(sized_buf_t *sbuf, sized_t nelems) {; sbuf->buf = (int *)malloc(sizeof(int) * nelems);; sbuf->count = nelems;; }. To implement this rule, the compiler requires a linear representation of; statements to understand the ordering and the adjacency between the two or more; assignments. The Clang CFG is used to implement this analysis as Clang CFG; provides a linear view of statements within each ``CFGBlock`` (Clang; ``CFGBlock`` represents a single basic block in a source-level CFG). Bounds check optimizations; ==========================. In ``-fbounds-safety``, the Clang frontend emits run-time checks for every; memory dereference if the type system or analyses in the frontend couldn’t; verify its bounds safety. The implementation relies on LLVM optimizations to; remove redundant run-time checks. Using this optimization strategy, if the; original source code already has bounds checks, the fewer additional checks; ``-fbounds-safety`` will introduce. The LLVM ``ConstraintElimination`` pass is; design to remove provable redundant checks (please check Florian Hahn’s; presentation in 2021 LLVM Dev Meeting and the implementation to learn more). In; the following example, ``-fbounds-safety`` implicitly adds the redundant bounds; checks that the optimizer can remove:. .. code-block:: c. void fill_array_with_indices(int *__counted_by(count) p, size_t count) {; for (size_t i = 0; i < count; ++i) {; // implicit bounds checks:; // if (p + i < p || p + i + 1 > p + count) trap();; p[i] = i;; }; }. ``ConstraintElimination`` collects the following facts and determines if the; bounds checks can be safely removed:. * Inside the for-loop, ``0 <= i < count``, hence ``1 <= i + 1 <= count``.; * Pointer arithmetic ``p + count`` in the if-condition doesn’t wrap.; * ``-fbounds-safety`` treats pointer arithmetic overflow as deterministically; two’s complement computation, not an",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/BoundsSafetyImplPlans.rst:6288,optimiz,optimization,6288,interpreter/llvm-project/clang/docs/BoundsSafetyImplPlans.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/BoundsSafetyImplPlans.rst,1,['optimiz'],['optimization']
Performance,"ty of reasons: changes to the code base invalidate them; silently, the developer mis-annotated them (e.g., using ``LIKELY`` instead of; ``UNLIKELY``), or perhaps they assumed something incorrectly when they wrote; the annotation. Regardless of why, it is useful to detect these situations so; that the optimizer can make more useful decisions about the code. MisExpect diagnostics are intended to help developers identify and address; these situations, by comparing the branch weights added by the ``llvm.expect``; intrinsic to those collected through profiling. Whenever these values are; mismatched, a diagnostic is surfaced to the user. Details on how the checks; operate in the LLVM backed can be found in LLVM's documentation. By default MisExpect checking is quite strict, because the use of the; ``llvm.expect`` intrinsic is designed for specialized cases, where the outcome; of a condition is severely skewed. As a result, the optimizer can be extremely; aggressive, which can result in performance degradation if the outcome is less; predictable than the annotation suggests. Even when the annotation is correct; 90% of the time, it may be beneficial to either remove the annotation or to use; a different intrinsic that can communicate the probability more directly. Because this may be too strict, MisExpect diagnostics are not enabled by; default, and support an additional flag to tolerate some deviation from the; exact thresholds. The ``-fdiagnostic-misexpect-tolerance=N`` accepts; deviations when comparing branch weights within ``N%`` of the expected values.; So passing ``-fdiagnostic-misexpect-tolerance=5`` will not report diagnostic messages; if the branch weight from the profile is within 5% of the weight added by; the ``llvm.expect`` intrinsic. MisExpect diagnostics are also available in the form of optimization remarks,; which can be serialized and processed through the ``opt-viewer.py``; scripts in LLVM. .. option:: -Rpass=misexpect. Enables optimization remarks for mi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/MisExpect.rst:1283,optimiz,optimizer,1283,interpreter/llvm-project/clang/docs/MisExpect.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/MisExpect.rst,2,"['optimiz', 'perform']","['optimizer', 'performance']"
Performance,"ty; information for the above hierarchy:. ::. @_ZTV1A = constant [...], !type !0; @_ZTV1B = constant [...], !type !0, !type !1; @_ZTV1C = constant [...], !type !2; @_ZTV1D = constant [...], !type !0, !type !3, !type !4. !0 = !{i64 16, !""_ZTS1A""}; !1 = !{i64 16, !""_ZTS1B""}; !2 = !{i64 16, !""_ZTS1C""}; !3 = !{i64 16, !""_ZTS1D""}; !4 = !{i64 48, !""_ZTS1C""}. With this type metadata, we can now use the ``llvm.type.test`` intrinsic to; test whether a given pointer is compatible with a type identifier. Working; backwards, if ``llvm.type.test`` returns true for a particular pointer,; we can also statically determine the identities of the virtual functions; that a particular virtual call may call. For example, if a program assumes; a pointer to be a member of ``!""_ZST1A""``, we know that the address can; be only be one of ``_ZTV1A+16``, ``_ZTV1B+16`` or ``_ZTV1D+16`` (i.e. the; address points of the vtables of A, B and D respectively). If we then load; an address from that pointer, we know that the address can only be one of; ``&A::f``, ``&B::f`` or ``&D::f``. .. _address point: https://itanium-cxx-abi.github.io/cxx-abi/abi.html#vtable-general. Testing Addresses For Type Membership; =====================================. If a program tests an address using ``llvm.type.test``, this will cause; a link-time optimization pass, ``LowerTypeTests``, to replace calls to this; intrinsic with efficient code to perform type member tests. At a high level,; the pass will lay out referenced globals in a consecutive memory region in; the object file, construct bit vectors that map onto that memory region,; and generate code at each of the ``llvm.type.test`` call sites to test; pointers against those bit vectors. Because of the layout manipulation, the; globals' definitions must be available at LTO time. For more information,; see the `control flow integrity design document`_. A type identifier that identifies functions is transformed into a jump table,; which is a block of code consisting of o",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TypeMetadata.rst:4708,load,load,4708,interpreter/llvm-project/llvm/docs/TypeMetadata.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TypeMetadata.rst,1,['load'],['load']
Performance,"u can use the `.x` command. This is the same as executing a; named script; you can also provide parameters. The only; difference is you need to append a + or a ++. ``` {.cpp}; root[] .x MyScript.C+(4000); Creating shared library /home/./MyScript_C.so; ```. You can select whether the script in compiled with debug symbol or; with optimization by appending the letter 'g' or 'O' after the '+' or; '++'. Without the specification, the script is compiled with the same; level of debugging symbol and optimization as the currently running; ROOT executable. For example:. ``` {.cpp}; root[] .L MyScript.C++g; ```. will compile `MyScript.C` with debug symbols; usually this means; giving the `-g` option to compiler. ``` {.cpp}; root[] .L MyScript.C++O; ```. will compile `MyScript.C` with optimizations; usually this means; giving the `-O` option to compiler. The syntax:. ``` {.cpp}; root[] .L MyScript.C++; ```. is using the default optimization level. The initial default is to; compile with the same level of optimization as the root executable; itself. The default can be changed by:. ``` {.cpp}; root[] gSystem->SetAclicMode(TSystem::kDebug);; root[] gSystem->SetAclicMode(TSystem::kOpt);; ```. Note that the commands:. ``` {.cpp}; root[] .L MyScript.C+g; root[] .L MyScript.C+O; ```. respectively compile `MyScript.C` with debug and optimization if the; library does not exist yet; they will not change the debug and the; optimization level if the library already exist and it is up to date.; To use ACLiC from compiled code or from inside another macro, we; recommend using `gROOT->ProcessLine()`. For; example, in one script you can use ACLiC to compile and load another; script. ``` {.cpp}; gROOT->ProcessLine("".L MyScript.C+""); gROOT->ProcessLine("".L MyScript.C++""); ```. ### Setting the Include Path. You can get the include path by typing:. ``` {.cpp}; root[] .include; ```. You can append to the include path by typing:. ``` {.cpp}; root[] .include $HOME/mypackage/include; ```. In a script y",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Cling.md:15611,optimiz,optimization,15611,documentation/users-guide/Cling.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Cling.md,1,['optimiz'],['optimization']
Performance,"u supplied the -M option); most filenames can be written to the file without any special formatting.; Different Make tools will treat different sets of characters as ""special""; and use different conventions for telling the Make tool that the character; is actually part of the filename. Normally Clang uses backslash to ""escape""; a special character, which is the convention used by GNU Make. The -MV; option tells Clang to put double-quotes around the entire filename, which; is the convention used by NMake and Jom. .. option:: -femit-dwarf-unwind=<value>. When to emit DWARF unwind (EH frame) info. This is a Mach-O-specific option. Valid values are:. * ``no-compact-unwind`` - Only emit DWARF unwind when compact unwind encodings; aren't available. This is the default for arm64.; * ``always`` - Always emit DWARF unwind regardless.; * ``default`` - Use the platform-specific default (``always`` for all; non-arm64-platforms). ``no-compact-unwind`` is a performance optimization -- Clang will emit smaller; object files that are more quickly processed by the linker. This may cause; binary compatibility issues on older x86_64 targets, however, so use it with; caution. .. _configuration-files:. Configuration files; -------------------. Configuration files group command-line options and allow all of them to be; specified just by referencing the configuration file. They may be used, for; example, to collect options required to tune compilation for particular; target, such as ``-L``, ``-I``, ``-l``, ``--sysroot``, codegen options, etc. Configuration files can be either specified on the command line or loaded; from default locations. If both variants are present, the default configuration; files are loaded first. The command line option ``--config=`` can be used to specify explicit; configuration files in a Clang invocation. If the option is used multiple times,; all specified files are loaded, in order. For example:. ::. clang --config=/home/user/cfgs/testing.txt; clang --config=deb",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:30269,perform,performance,30269,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,2,"['optimiz', 'perform']","['optimization', 'performance']"
Performance,"u>`_; adapted from libc++'s test suite. In nvcc ``math.h`` and ``cmath`` are mostly available. Versions of ``::foof``; in namespace std (e.g. ``std::sinf``) are not available, and where the standard; calls for overloads that take integral arguments, these are usually not; available. .. code-block:: c++. #include <math.h>; #include <cmath.h>. // clang is OK with everything in this function.; __device__ void test() {; std::sin(0.); // nvcc - ok; std::sin(0); // nvcc - error, because no std::sin(int) override is available.; sin(0); // nvcc - same as above. sinf(0.); // nvcc - ok; std::sinf(0.); // nvcc - no such function; }. ``<std::complex>``; ------------------. nvcc does not officially support ``std::complex``. It's an error to use; ``std::complex`` in ``__device__`` code, but it often works in ``__host__; __device__`` code due to nvcc's interpretation of the ""wrong-side rule"" (see; below). However, we have heard from implementers that it's possible to get; into situations where nvcc will omit a call to an ``std::complex`` function,; especially when compiling without optimizations. As of 2016-11-16, clang supports ``std::complex`` without these caveats. It is; tested with libstdc++ 4.8.5 and newer, but is known to work only with libc++; newer than 2016-11-16. ``<algorithm>``; ---------------. In C++14, many useful functions from ``<algorithm>`` (notably, ``std::min`` and; ``std::max``) become constexpr. You can therefore use these in device code,; when compiling with clang. Detecting clang vs NVCC from code; =================================. Although clang's CUDA implementation is largely compatible with NVCC's, you may; still want to detect when you're compiling CUDA code specifically with clang. This is tricky, because NVCC may invoke clang as part of its own compilation; process! For example, NVCC uses the host compiler's preprocessor when; compiling for device code, and that host compiler may in fact be clang. When clang is actually compiling CUDA code -- rather",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CompileCudaWithLLVM.rst:7079,optimiz,optimizations,7079,interpreter/llvm-project/llvm/docs/CompileCudaWithLLVM.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CompileCudaWithLLVM.rst,1,['optimiz'],['optimizations']
Performance,"ualifier on the second operand pointer type; always matches the address space qualifier on the result type. How is GEP different from ``ptrtoint``, arithmetic, and ``inttoptr``?; ---------------------------------------------------------------------. It's very similar; there are only subtle differences. With ptrtoint, you have to pick an integer type. One approach is to pick i64;; this is safe on everything LLVM supports (LLVM internally assumes pointers are; never wider than 64 bits in many places), and the optimizer will actually narrow; the i64 arithmetic down to the actual pointer size on targets which don't; support 64-bit arithmetic in most cases. However, there are some cases where it; doesn't do this. With GEP you can avoid this problem. Also, GEP carries additional pointer aliasing rules. It's invalid to take a GEP; from one object, address into a different separately allocated object, and; dereference it. IR producers (front-ends) must follow this rule, and consumers; (optimizers, specifically alias analysis) benefit from being able to rely on; it. See the `Rules`_ section for more information. And, GEP is more concise in common cases. However, for the underlying integer computation implied, there is no; difference. I'm writing a backend for a target which needs custom lowering for GEP. How do I do this?; -----------------------------------------------------------------------------------------. You don't. The integer computation implied by a GEP is target-independent.; Typically what you'll need to do is make your backend pattern-match expressions; trees involving ADD, MUL, etc., which are what GEP is lowered into. This has the; advantage of letting your code work correctly in more cases. GEP does use target-dependent parameters for the size and layout of data types,; which targets can customize. If you require support for addressing units which are not 8 bits, you'll need to; fix a lot of code in the backend, with GEP lowering being only a small piece of; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GetElementPtr.rst:10522,optimiz,optimizers,10522,interpreter/llvm-project/llvm/docs/GetElementPtr.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GetElementPtr.rst,1,['optimiz'],['optimizers']
Performance,"ualization. Turned off by default, because it is; still experimental. .. option:: -fwhole-program-vtables. Enable whole-program vtable optimizations, such as single-implementation; devirtualization and virtual constant propagation, for classes with; :doc:`hidden LTO visibility <LTOVisibility>`. Requires ``-flto``. .. option:: -f[no]split-lto-unit. Controls splitting the :doc:`LTO unit <LTOVisibility>` into regular LTO and; :doc:`ThinLTO` portions, when compiling with -flto=thin. Defaults to false; unless ``-fsanitize=cfi`` or ``-fwhole-program-vtables`` are specified, in; which case it defaults to true. Splitting is required with ``fsanitize=cfi``,; and it is an error to disable via ``-fno-split-lto-unit``. Splitting is; optional with ``-fwhole-program-vtables``, however, it enables more; aggressive whole program vtable optimizations (specifically virtual constant; propagation). When enabled, vtable definitions and select virtual functions are placed; in the split regular LTO module, enabling more aggressive whole program; vtable optimizations required for CFI and virtual constant propagation.; However, this can increase the LTO link time and memory requirements over; pure ThinLTO, as all split regular LTO modules are merged and LTO linked; with regular LTO. .. option:: -fforce-emit-vtables. In order to improve devirtualization, forces emitting of vtables even in; modules where it isn't necessary. It causes more inline virtual functions; to be emitted. .. option:: -fno-assume-sane-operator-new. Don't assume that the C++'s new operator is sane. This option tells the compiler to do not assume that C++'s global; new operator will always return a pointer that does not alias any; other pointer when the function returns. .. option:: -fassume-nothrow-exception-dtor. Assume that an exception object' destructor will not throw, and generate; less code for catch handlers. A throw expression of a type with a; potentially-throwing destructor will lead to an error. By default, Cla",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:81460,optimiz,optimizations,81460,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,1,['optimiz'],['optimizations']
Performance,"uation is complete. Otherwise, the DWARF expression is ill-formed if the updated operation; position is not in the range of the first to last operation inclusive, or; not at the start of an operation. 4. ``DW_OP_bra``. ``DW_OP_bra`` is a conditional branch. Its single operand is a 2-byte signed; integer constant. This operation pops the top of stack. If the value popped; is not the constant 0, the 2-byte constant operand is the number of bytes of; the DWARF operation expression to skip forward or backward from the current; operation, beginning after the 2-byte constant. If the updated position is at one past the end of the last operation, then; the operation expression evaluation is complete. Otherwise, the DWARF expression is ill-formed if the updated operation; position is not in the range of the first to last operation inclusive, or; not at the start of an operation. 5. ``DW_OP_call2, DW_OP_call4, DW_OP_call_ref``. ``DW_OP_call2``, ``DW_OP_call4``, and ``DW_OP_call_ref`` perform DWARF; procedure calls during evaluation of a DWARF operation expression. ``DW_OP_call2`` and ``DW_OP_call4``, have one operand that is, respectively,; a 2-byte or 4-byte unsigned offset DR that represents the byte offset of a; debugging information entry D relative to the beginning of the current; compilation unit. ``DW_OP_call_ref`` has one operand that is a 4-byte unsigned value in the; 32-bit DWARF format, or an 8-byte unsigned value in the 64-bit DWARF format,; that represents the byte offset DR of a debugging information entry D; relative to the beginning of the ``.debug_info`` section that contains the; current compilation unit. D may not be in the current compilation unit. .. note::. DWARF Version 5 states that DR can be an offset in a ``.debug_info``; section other than the one that contains the current compilation unit. It; states that relocation of references from one executable or shared object; file to another must be performed by the consumer. But given that DR is; defined as",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst:72319,perform,perform,72319,interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst,1,['perform'],['perform']
Performance,ub.com/root-project/root/issues/12783)] - [IO] Writing HistFactory model file twice gives strange results since ROOT 6.26.02; * [[#12770](https://github.com/root-project/root/issues/12770)] - tmva/sofie_parsers does not build with latest Protobuf (22.x); * [[#12744](https://github.com/root-project/root/issues/12744)] - wrong Python version found during build; * [[#12718](https://github.com/root-project/root/issues/12718)] - A crash when trying to initialise a vector from a >1D numpy array in PyROOT; * [[#12685](https://github.com/root-project/root/issues/12685)] - `TEnum::GetEnum` does NOT process typedefs; * [[#12644](https://github.com/root-project/root/issues/12644)] - Can't find cxxabi.h and build module 'ROOT_Foundation_Stage1_NoRTTI' when building from source on Macos; * [[#12631](https://github.com/root-project/root/issues/12631)] - Unable to build master with external XROOTD; * [[#12621](https://github.com/root-project/root/issues/12621)] - [I/O][RDF] Usage of xrootd from multi-thread event loops runs into severe bottlenecks; * [[#12592](https://github.com/root-project/root/issues/12592)] - [doc] TProfile bin error documentation is not correct; * [[#12591](https://github.com/root-project/root/issues/12591)] - Allow partial enablement of modules; * [[#12527](https://github.com/root-project/root/issues/12527)] - MacOS build fails if configured without cocoa; * [[#12492](https://github.com/root-project/root/issues/12492)] - The problem with building ROOT v6-26-10 in debug mode on ubuntu 20.04; * [[#12230](https://github.com/root-project/root/issues/12230)] - Wrong conversion from Numpy Array to `std.vector` when using the wrong type; * [[#12091](https://github.com/root-project/root/issues/12091)] - TSpline SaveAs not using equidistant and loss of precision; * [[#11924](https://github.com/root-project/root/issues/11924)] - PyROOT: wrong overload resolution for C++ functions from python ; * [[#11901](https://github.com/root-project/root/issues/11901)] - Binary dis,MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v632/index.md:41146,multi-thread,multi-thread,41146,README/ReleaseNotes/v632/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v632/index.md,2,"['bottleneck', 'multi-thread']","['bottlenecks', 'multi-thread']"
Performance,"ubtraction, and the second element of; which is a bit specifying if the unsigned subtraction resulted in an; overflow. Examples:; """""""""""""""""". .. code-block:: llvm. %res = call {i32, i1} @llvm.usub.with.overflow.i32(i32 %a, i32 %b); %sum = extractvalue {i32, i1} %res, 0; %obit = extractvalue {i32, i1} %res, 1; br i1 %obit, label %overflow, label %normal. '``llvm.smul.with.overflow.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". This is an overloaded intrinsic. You can use ``llvm.smul.with.overflow``; on any integer bit width or vectors of integers. ::. declare {i16, i1} @llvm.smul.with.overflow.i16(i16 %a, i16 %b); declare {i32, i1} @llvm.smul.with.overflow.i32(i32 %a, i32 %b); declare {i64, i1} @llvm.smul.with.overflow.i64(i64 %a, i64 %b); declare {<4 x i32>, <4 x i1>} @llvm.smul.with.overflow.v4i32(<4 x i32> %a, <4 x i32> %b). Overview:; """""""""""""""""". The '``llvm.smul.with.overflow``' family of intrinsic functions perform; a signed multiplication of the two arguments, and indicate whether an; overflow occurred during the signed multiplication. Arguments:; """""""""""""""""""". The arguments (%a and %b) and the first element of the result structure; may be of integer types of any bit width, but they must have the same; bit width. The second element of the result structure must be of type; ``i1``. ``%a`` and ``%b`` are the two values that will undergo signed; multiplication. Semantics:; """""""""""""""""""". The '``llvm.smul.with.overflow``' family of intrinsic functions perform; a signed multiplication of the two arguments. They return a structure ---; the first element of which is the multiplication, and the second element; of which is a bit specifying if the signed multiplication resulted in an; overflow. Examples:; """""""""""""""""". .. code-block:: llvm. %res = call {i32, i1} @llvm.smul.with.overflow.i32(i32 %a, i32 %b); %sum = extractvalue {i32, i1} %res, 0; %obit = extractvalue {i32, i1} %res, 1; br i1 %obit, label %overflow, label %normal. '``llvm.umul.with.ove",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:607906,perform,perform,607906,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['perform']
Performance,"uc.edu>; Subject: another thought. I have a budding idea about making LLVM a little more ambitious: a; customizable runtime system that can be used to implement language-specific; virtual machines for many different languages. E.g., a C vm, a C++ vm, a; Java vm, a Lisp vm, .. The idea would be that LLVM would provide a standard set of runtime features; (some low-level like standard assembly instructions with code generation and; static and runtime optimization; some higher-level like type-safety and; perhaps a garbage collection library). Each language vm would select the; runtime features needed for that language, extending or customizing them as; needed. Most of the machine-dependent code-generation and optimization; features as well as low-level machine-independent optimizations (like PRE); could be provided by LLVM and should be sufficient for any language,; simplifying the language compiler. (This would also help interoperability; between languages.) Also, some or most of the higher-level; machine-independent features like type-safety and access safety should be; reusable by different languages, with minor extensions. The language; compiler could then focus on language-specific analyses and optimizations. The risk is that this sounds like a universal IR -- something that the; compiler community has tried and failed to develop for decades, and is; universally skeptical about. No matter what we say, we won't be able to; convince anyone that we have a universal IR that will work. We need to; think about whether LLVM is different or if has something novel that might; convince people. E.g., the idea of providing a package of separable; features that different languages select from. Also, using SSA with or; without type-safety as the intermediate representation. One interesting starting point would be to discuss how a JVM would be; implemented on top of LLVM a bit more. That might give us clues on how to; structure LLVM to support one or more language VMs. --Vikram. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-01-31-UniversalIRIdea.txt:1328,optimiz,optimizations,1328,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-01-31-UniversalIRIdea.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-01-31-UniversalIRIdea.txt,1,['optimiz'],['optimizations']
Performance,"uch as; Intel's Sandylake), we do so now. To see which features and CPUs that LLVM knows about, we can use; ``llc``. For example, let's look at x86:. ::. $ llvm-as < /dev/null | llc -march=x86 -mattr=help; Available CPUs for this target:. amdfam10 - Select the amdfam10 processor.; athlon - Select the athlon processor.; athlon-4 - Select the athlon-4 processor.; ... Available features for this target:. 16bit-mode - 16-bit mode (i8086).; 32bit-mode - 32-bit mode (80386).; 3dnow - Enable 3DNow! instructions.; 3dnowa - Enable 3DNow! Athlon instructions.; ... For our example, we'll use the generic CPU without any additional feature or; target option. .. code-block:: c++. auto CPU = ""generic"";; auto Features = """";. TargetOptions opt;; auto TargetMachine = Target->createTargetMachine(TargetTriple, CPU, Features, opt, Reloc::PIC_);. Configuring the Module; ======================. We're now ready to configure our module, to specify the target and; data layout. This isn't strictly necessary, but the `frontend; performance guide <../../Frontend/PerformanceTips.html>`_ recommends; this. Optimizations benefit from knowing about the target and data; layout. .. code-block:: c++. TheModule->setDataLayout(TargetMachine->createDataLayout());; TheModule->setTargetTriple(TargetTriple);. Emit Object Code; ================. We're ready to emit object code! Let's define where we want to write; our file to:. .. code-block:: c++. auto Filename = ""output.o"";; std::error_code EC;; raw_fd_ostream dest(Filename, EC, sys::fs::OF_None);. if (EC) {; errs() << ""Could not open file: "" << EC.message();; return 1;; }. Finally, we define a pass that emits object code, then we run that; pass:. .. code-block:: c++. legacy::PassManager pass;; auto FileType = CodeGenFileType::ObjectFile;. if (TargetMachine->addPassesToEmitFile(pass, dest, nullptr, FileType)) {; errs() << ""TargetMachine can't emit a file of this type"";; return 1;; }. pass.run(*TheModule);; dest.flush();. Putting It All Together; ============",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl08.rst:3572,perform,performance,3572,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl08.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl08.rst,1,['perform'],['performance']
Performance,"uction set has support for a fused operation,; and (b) that the fused operation is more efficient than the equivalent,; separate pair of mul and add instructions. Arguments:; """""""""""""""""""". The first three arguments to the '``llvm.experimental.constrained.fmuladd``'; intrinsic must be floating-point or vector of floating-point values.; All three arguments must have identical types. The fourth and fifth arguments specify the rounding mode and exception behavior; as described above. Semantics:; """""""""""""""""""". The expression:. ::. %0 = call float @llvm.experimental.constrained.fmuladd.f32(%a, %b, %c,; metadata <rounding mode>,; metadata <exception behavior>). is equivalent to the expression:. ::. %0 = call float @llvm.experimental.constrained.fmul.f32(%a, %b,; metadata <rounding mode>,; metadata <exception behavior>); %1 = call float @llvm.experimental.constrained.fadd.f32(%0, %c,; metadata <rounding mode>,; metadata <exception behavior>). except that it is unspecified whether rounding will be performed between the; multiplication and addition steps. Fusion is not guaranteed, even if the target; platform supports it.; If a fused multiply-add is required, the corresponding; :ref:`llvm.experimental.constrained.fma <int_fma>` intrinsic function should be; used instead.; This never sets errno, just as '``llvm.experimental.constrained.fma.*``'. Constrained libm-equivalent Intrinsics; --------------------------------------. In addition to the basic floating-point operations for which constrained; intrinsics are described above, there are constrained versions of various; operations which provide equivalent behavior to a corresponding libm function.; These intrinsics allow the precise behavior of these operations with respect to; rounding mode and exception behavior to be controlled. As with the basic constrained floating-point intrinsics, the rounding mode; and exception behavior arguments only control the behavior of the optimizer.; They do not change the runtime floating-point env",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:888973,perform,performed,888973,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performed']
Performance,"uction; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. <result> = extractelement <n x <ty>> <val>, <ty2> <idx> ; yields <ty>; <result> = extractelement <vscale x n x <ty>> <val>, <ty2> <idx> ; yields <ty>. Overview:; """""""""""""""""". The '``extractelement``' instruction extracts a single scalar element; from a vector at a specified index. Arguments:; """""""""""""""""""". The first operand of an '``extractelement``' instruction is a value of; :ref:`vector <t_vector>` type. The second operand is an index indicating; the position from which to extract the element. The index may be a; variable of any integer type, and will be treated as an unsigned integer. Semantics:; """""""""""""""""""". The result is a scalar of the same type as the element type of ``val``.; Its value is the value at position ``idx`` of ``val``. If ``idx``; exceeds the length of ``val`` for a fixed-length vector, the result is a; :ref:`poison value <poisonvalues>`. For a scalable vector, if the value; of ``idx`` exceeds the runtime length of the vector, the result is a; :ref:`poison value <poisonvalues>`. Example:; """""""""""""""". .. code-block:: text. <result> = extractelement <4 x i32> %vec, i32 0 ; yields i32. .. _i_insertelement:. '``insertelement``' Instruction; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. <result> = insertelement <n x <ty>> <val>, <ty> <elt>, <ty2> <idx> ; yields <n x <ty>>; <result> = insertelement <vscale x n x <ty>> <val>, <ty> <elt>, <ty2> <idx> ; yields <vscale x n x <ty>>. Overview:; """""""""""""""""". The '``insertelement``' instruction inserts a scalar element into a; vector at a specified index. Arguments:; """""""""""""""""""". The first operand of an '``insertelement``' instruction is a value of; :ref:`vector <t_vector>` type. The second operand is a scalar value whose; type must equal the element type of the first operand. The third operand; is an index indicating the position at which to insert the value. The; index may be a variable of any integer type, and will be treated as an; unsigned inte",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:401847,scalab,scalable,401847,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['scalab'],['scalable']
Performance,"uctions occur. Similarly, a; ``regBankSelected`` function may not have virtual registers without a register; bank assigned. .. note::. For layering reasons, ``MachineVerifier`` isn't able to be the sole verifier; in GlobalISel. Currently some of the passes also perform verification while; we find a way to solve this problem. The main issue is that GlobalISel is a separate library, so we can't; directly reference it from CodeGen. Testing; -------. The ability to test GlobalISel is significantly improved over SelectionDAG.; SelectionDAG is something of a black box and there's a lot going on inside it.; This makes it difficult to write a test that reliably tests a particular aspect; of its behaviour. For comparison, see the following diagram:. .. image:: testing-pass-level.png. Each of the grey boxes indicates an opportunity to serialize the current state; and test the behaviour between two points in the pipeline. The current state; can be serialized using ``-stop-before`` or ``-stop-after`` and loaded using; ``-start-before``, ``-start-after``, and ``-run-pass``. We can also go further still, as many of GlobalISel's passes are readily unit; testable:. .. image:: testing-unit-level.png. It's possible to create an imaginary target such as in `LegalizerHelperTest.cpp <https://github.com/llvm/llvm-project/blob/93b29d3882baf7df42e4e9bc26b977b00373ef56/llvm/unittests/CodeGen/GlobalISel/LegalizerHelperTest.cpp#L28-L57>`_; and perform a single step of the algorithm and check the result. The MIR and; FileCheck directives can be embedded using strings so you still have access to; the convenience available in llvm-lit. Debugging; ---------. One debugging technique that's proven particularly valuable is to use the; BlockExtractor to extract basic blocks into new functions. This can be used; to track down correctness bugs and can also be used to track down performance; regressions. It can also be coupled with function attributes to disable; GlobalISel for one or more of the extract",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GlobalISel/Pipeline.rst:4459,load,loaded,4459,interpreter/llvm-project/llvm/docs/GlobalISel/Pipeline.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GlobalISel/Pipeline.rst,1,['load'],['loaded']
Performance,"ucture \n (a variable with \n an address)"", shape=""box""];; left3 [label="" 3. Built-In Type \n (int, float, etc.)"", shape=""box""];; output [label="" move to 'Assign' step "", shape=""box""];. synth -> mem;; mem -> withaloc [label=""Yes""];; mem -> noaloc [label=""No""];; withaloc -> right;; noaloc -> left2;; noaloc -> left3;; right -> output;; left2 -> output;; left3 -> output;; }; output -> assign; }. Where is the captured result stored?; ------------------------------------. ``LastValue`` holds the last result of the value printing. It is a class member; because it can be accessed even after subsequent inputs. **Note:** If no value printing happens, then it is in an invalid state. Improving Efficiency and User Experience; ----------------------------------------. The Value object is essentially used to create a mapping between an expression; 'type' and the allocated 'memory'. Built-in types (bool, char, int,; float, double, etc.) are copyable. Their memory allocation size is known; and the Value object can introduce a small-buffer optimization.; In case of objects, the ``Value`` class provides reference-counted memory; management. The implementation maps the type as written and the Clang Type to be able to use; the preprocessor to synthesize the relevant cast operations. For example,; ``X(char, Char_S)``, where ``char`` is the type from the language's type system; and ``Char_S`` is the Clang builtin type which represents it. This mapping helps; to import execution results from the interpreter in a compiled program and vice; versa. The ``Value.h`` header file can be included at runtime and this is why it; has a very low token count and was developed with strict constraints in mind. This also enables the user to receive the computed 'type' back in their code; and then transform the type into something else (e.g., re-cast a double into; a float). Normally, the compiler can handle these conversions transparently,; but in interpreter mode, the compiler cannot see all the 'from' a",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ClangRepl.rst:9051,optimiz,optimization,9051,interpreter/llvm-project/clang/docs/ClangRepl.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ClangRepl.rst,1,['optimiz'],['optimization']
Performance,"ucture members. Many variations are supported, but some that rely on undefined behaviour being; ignored (as other compilers do) are still being left un-vectorized. .. code-block:: c++. struct { int A[100], K, B[100]; } Foo;. void foo() {; for (int i = 0; i < 100; ++i); Foo.A[i] = Foo.B[i] + 100;; }. Vectorization of function calls; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. The Loop Vectorizer can vectorize intrinsic math functions.; See the table below for a list of these functions. +-----+-----+---------+; | pow | exp | exp2 |; +-----+-----+---------+; | sin | cos | sqrt |; +-----+-----+---------+; | log |log2 | log10 |; +-----+-----+---------+; |fabs |floor| ceil |; +-----+-----+---------+; |fma |trunc|nearbyint|; +-----+-----+---------+; | | | fmuladd |; +-----+-----+---------+. Note that the optimizer may not be able to vectorize math library functions; that correspond to these intrinsics if the library calls access external state; such as ""errno"". To allow better optimization of C/C++ math library functions,; use ""-fno-math-errno"". The loop vectorizer knows about special instructions on the target and will; vectorize a loop containing a function call that maps to the instructions. For; example, the loop below will be vectorized on Intel x86 if the SSE4.1 roundps; instruction is available. .. code-block:: c++. void foo(float *f) {; for (int i = 0; i != 1024; ++i); f[i] = floorf(f[i]);; }. Partial unrolling during vectorization; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Modern processors feature multiple execution units, and only programs that contain a; high degree of parallelism can fully utilize the entire width of the machine.; The Loop Vectorizer increases the instruction level parallelism (ILP) by; performing partial-unrolling of loops. In the example below the entire array is accumulated into the variable 'sum'.; This is inefficient because only a single execution port can be used by the processor.; By unrolling the code the Loop Vectorizer allows two or more executi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Vectorizers.rst:9840,optimiz,optimization,9840,interpreter/llvm-project/llvm/docs/Vectorizers.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Vectorizers.rst,1,['optimiz'],['optimization']
Performance,"uding the input ll/bc file), they are; given via the following flag:; `--test_args=<test_arguments>`; If unspecified, the test is run as given. It’s worth noting that the input file; would be passed as a parameter to the test, similar how `-compile-custom`; currently operates. ### Implementation; The tool would behave similar to CReduce’s functionality in that it would have a; list of passes that try to minimize the given test-case. We should be able to; modularize the tool’s behavior, as well as making it easier to maintain and; expand. The first version of this redesign would try to:. * Discard functions, instructions and metadata that don’t influence the; interesting-ness test; * Remove unused parameters from functions; * Eliminate unvisited conditional paths; * Rename variables to more regular ones (such as “a”, “b”, “c”, etc.). Once these passes are implemented, more meaningful reductions (such as type; reduction) would be added to the tool, to even further reduce IR. ## Background on historical bugpoint issues. ### Root Cause Analysis; Presently, bugpoint takes a long time to find the source problem in a given IR; file, mainly due to the fact that it tries to debug the input by running; various strategies to classify the bug, which in turn run multiple optimizer; and compilation passes over the input, taking up a lot of time. Furthermore,; when the IR crashes, it tries to reduce it by performing some sub-optimal; passes (e.g. a lot of unreachable blocks), and sometimes even fails to minimize; at all. ### ""Quirky"" Interface; Bugpoint’s current interface overwhelms and confuses the user, the help screen; alone ends up confusing rather providing guidance. And, not only are there; numerous features and options, but some of them also work in unexpected ways; and most of the time the user ends up using a custom script. Pruning and; simplifying the interface will be worth considering in order to make the tool; more useful in the general case and easier to maintain.; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BugpointRedesign.md:3276,optimiz,optimizer,3276,interpreter/llvm-project/llvm/docs/BugpointRedesign.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BugpointRedesign.md,2,"['optimiz', 'perform']","['optimizer', 'performing']"
Performance,"ue constraint information. ``lint``: Statically lint-checks LLVM IR; ----------------------------------------. This pass statically checks for common and easily-identified constructs which; produce undefined or likely unintended behavior in LLVM IR. It is not a guarantee of correctness, in two ways. First, it isn't; comprehensive. There are checks which could be done statically which are not; yet implemented. Some of these are indicated by TODO comments, but those; aren't comprehensive either. Second, many conditions cannot be checked; statically. This pass does no dynamic instrumentation, so it can't check for; all possible problems. Another limitation is that it assumes all code will be executed. A store; through a null pointer in a basic block which is never reached is harmless, but; this pass will warn about it anyway. Optimization passes may make conditions that this pass checks for more or less; obvious. If an optimization pass appears to be introducing a warning, it may; be that the optimization pass is merely exposing an existing condition in the; code. This code may be run before :ref:`instcombine <passes-instcombine>`. In many; cases, instcombine checks for the same kinds of things and turns instructions; with undefined behavior into unreachable (or equivalent). Because of this,; this pass makes some effort to look through bitcasts and so on. ``loops``: Natural Loop Information; -----------------------------------. This analysis is used to identify natural loops and determine the loop depth of; various nodes of the CFG. Note that the loops identified may actually be; several natural loops that share the same header node... not just a single; natural loop. ``memdep``: Memory Dependence Analysis; --------------------------------------. An analysis that determines, for a given memory operation, what preceding; memory operations it depends on. It builds on alias analysis information, and; tries to provide a lazy, caching interface to a common kind of alias; in",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:7237,optimiz,optimization,7237,interpreter/llvm-project/llvm/docs/Passes.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst,2,['optimiz'],['optimization']
Performance,"ue. (Note that; the function may also return true if the given pointer is not associated; with a type metadata identifier.) If the function's return value's second; element is true, the following rules apply to the first element:. - If the given pointer is associated with the given type metadata identifier,; it is the function pointer loaded from the given byte offset from the given; pointer. - If the given pointer is not associated with the given type metadata; identifier, it is one of the following (the choice of which is unspecified):. 1. The function pointer that would have been loaded from an arbitrarily chosen; (through an unspecified mechanism) pointer associated with the type; metadata. 2. If the function has a non-void return type, a pointer to a function that; returns an unspecified value without causing side effects. If the function's return value's second element is false, the value of the; first element is undefined. .. _type.checked.load.relative:. '``llvm.type.checked.load.relative``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare {ptr, i1} @llvm.type.checked.load.relative(ptr %ptr, i32 %offset, metadata %type) argmemonly nounwind readonly. Overview:; """""""""""""""""". The ``llvm.type.checked.load.relative`` intrinsic loads a relative pointer to a; function from a virtual table pointer using metadata. Otherwise, its semantic is; identical to the ``llvm.type.checked.load`` intrinsic. A relative pointer is a pointer to an offset to the pointed to value. The; address of the underlying pointer of the relative pointer is obtained by adding; the offset to the address of the offset value. '``llvm.arithmetic.fence``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare <type>; @llvm.arithmetic.fence(<type> <op>). Overview:; """""""""""""""""". The purpose of the ``llvm.arithmetic.fence`` intrinsic; is to prevent the optimizer from performing fast-math optimizations,; particularly reassociation,; between the",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:939805,load,load,939805,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,"ue/weight; 	 with optionally an error on the value or the coordinate and the `ROOT::Fit::UnBinData` for un-binned data sets,; 	 which consists only of a vector of coordinate values. The coordinate values can be; 	 one-dimensional (i.e. one entry per event) or multi-dimensional (N entries per event).; * Function classes defining the type of fit (the objective function used for fitting):; 	- `ROOT::Fit::Chi2FCN` for chi2 (least-square fits),; 	- `ROOT::Fit::PoissonLikelihoodFCN` for binned likelihood fits of histograms,; 	- `ROOT::Fit::LogLikelihoodFCN` for generic un-binned likelihood fits.; 	These classes are templated on the type of function interface they implement (see later). User convenient typedefs are also provided.; 	They derive from the common generic interface multi-dimensional for function evaluation, `ROOT::Math::IBaseFunctionMultiDim`. In addition the fitter classes make uses of the generic interfaces for parametric function evaluations, `ROOT::Math::IParametricFunctionMultiDim`; to define the fit model function and use the `ROOT::Math::Minimizer` interface to perform the minimization of the objective function.; More information about the function interface and the multi-dimensional minimization in ROOT is given in the Mathematical Library chapter. Here we present a detailed description of the `ROOT::Fit` classes and how to use them.; Using these classes instead of the interface provided directly in the ROOT data objects, like `TH1::Fit` allow are more fine control; to configure and customise the fits. For example, using these classes a combined fit of several histograms can be performed. To understand how these class work, let's go through a simple example, such as fitting an histogram. When fitting an histogram, instead of using `TH1::Fit` we will show in the following hot wo use the `ROOT::Fit` classes.; We will show how to perform the following different type of fits with the histogram data:; * a least square fit using the observed errors (Neyman chi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/FittingHistograms.md:28134,perform,perform,28134,documentation/users-guide/FittingHistograms.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/FittingHistograms.md,1,['perform'],['perform']
Performance,"uent for, while, do-while, or c++11 range-based for; loop. The directive allows vectorization and interleaving to be enabled or; disabled. Vector width as well as interleave count can also be manually; specified. The following example explicitly enables vectorization and; interleaving:. .. code-block:: c++. #pragma clang loop vectorize(enable) interleave(enable); while(...) {; ...; }. The following example implicitly enables vectorization and interleaving by; specifying a vector width and interleaving count:. .. code-block:: c++. #pragma clang loop vectorize_width(2) interleave_count(2); for(...) {; ...; }. See the Clang; `language extensions; <https://clang.llvm.org/docs/LanguageExtensions.html#extensions-for-loop-hint-optimizations>`_; for details. Diagnostics; -----------. Many loops cannot be vectorized including loops with complicated control flow,; unvectorizable types, and unvectorizable calls. The loop vectorizer generates; optimization remarks which can be queried using command line options to identify; and diagnose loops that are skipped by the loop-vectorizer. Optimization remarks are enabled using:. ``-Rpass=loop-vectorize`` identifies loops that were successfully vectorized. ``-Rpass-missed=loop-vectorize`` identifies loops that failed vectorization and; indicates if vectorization was specified. ``-Rpass-analysis=loop-vectorize`` identifies the statements that caused; vectorization to fail. If in addition ``-fsave-optimization-record`` is; provided, multiple causes of vectorization failure may be listed (this behavior; might change in the future). Consider the following loop:. .. code-block:: c++. #pragma clang loop vectorize(enable); for (int i = 0; i < Length; i++) {; switch(A[i]) {; case 0: A[i] = i*2; break;; case 1: A[i] = i; break;; default: A[i] = 0;; }; }. The command line ``-Rpass-missed=loop-vectorize`` prints the remark:. .. code-block:: console. no_switch.cpp:4:5: remark: loop not vectorized: vectorization is explicitly enabled [-Rpass-missed",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Vectorizers.rst:2668,optimiz,optimization,2668,interpreter/llvm-project/llvm/docs/Vectorizers.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Vectorizers.rst,1,['optimiz'],['optimization']
Performance,"uential Consistent Atomic**; ------------------------------------------------------------------------------------; load atomic seq_cst - singlethread - global *Same as corresponding; - wavefront - local load atomic acquire,; - generic except must generate; all instructions even; for OpenCL.*; load atomic seq_cst - workgroup - global 1. s_waitcnt lgkmcnt(0); - generic. - Must; happen after; preceding; local/generic load; atomic/store; atomic/atomicrmw; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; lgkmcnt(0) and so do; not need to be; considered.); - Ensures any; preceding; sequential; consistent local; memory instructions; have completed; before executing; this sequentially; consistent; instruction. This; prevents reordering; a seq_cst store; followed by a; seq_cst load. (Note; that seq_cst is; stronger than; acquire/release as; the reordering of; load acquire; followed by a store; release is; prevented by the; s_waitcnt of; the release, but; there is nothing; preventing a store; release followed by; load acquire from; completing out of; order. The s_waitcnt; could be placed after; seq_store or before; the seq_load. We; choose the load to; make the s_waitcnt be; as late as possible; so that the store; may have already; completed.). 2. *Following; instructions same as; corresponding load; atomic acquire,; except must generate; all instructions even; for OpenCL.*; load atomic seq_cst - workgroup - local *Same as corresponding; load atomic acquire,; except must generate; all instructions even; for OpenCL.*. load atomic seq_cst - agent - global 1. s_waitcnt lgkmcnt(0) &; - system - generic vmcnt(0). - Could be split into; separate s_waitcnt; vmcnt(0); and s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt lgkmcnt(0); must happen after; preceding; global/generic load; atomic/store; atomic/atomicrmw; with memory; ordering of seq_cst;",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:231076,load,load,231076,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"ues. The result of the instruction is a vector; whose length is the same as the shuffle mask and whose element type is the; same as the element type of the first two operands. Semantics:; """""""""""""""""""". The elements of the two input vectors are numbered from left to right; across both of the vectors. For each element of the result vector, the; shuffle mask selects an element from one of the input vectors to copy; to the result. Non-negative elements in the mask represent an index; into the concatenated pair of input vectors. A ``poison`` element in the mask vector specifies that the resulting element; is ``poison``.; For backwards-compatibility reasons, LLVM temporarily also accepts ``undef``; mask elements, which will be interpreted the same way as ``poison`` elements.; If the shuffle mask selects an ``undef`` element from one of the input; vectors, the resulting element is ``undef``. For scalable vectors, the only valid mask values at present are; ``zeroinitializer``, ``undef`` and ``poison``, since we cannot write all indices as; literals for a vector with a length unknown at compile time. Example:; """""""""""""""". .. code-block:: text. <result> = shufflevector <4 x i32> %v1, <4 x i32> %v2,; <4 x i32> <i32 0, i32 4, i32 1, i32 5> ; yields <4 x i32>; <result> = shufflevector <4 x i32> %v1, <4 x i32> poison,; <4 x i32> <i32 0, i32 1, i32 2, i32 3> ; yields <4 x i32> - Identity shuffle.; <result> = shufflevector <8 x i32> %v1, <8 x i32> poison,; <4 x i32> <i32 0, i32 1, i32 2, i32 3> ; yields <4 x i32>; <result> = shufflevector <4 x i32> %v1, <4 x i32> %v2,; <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7 > ; yields <8 x i32>. Aggregate Operations; --------------------. LLVM supports several instructions for working with; :ref:`aggregate <t_aggregate>` values. .. _i_extractvalue:. '``extractvalue``' Instruction; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. <result> = extractvalue <aggregate type> <val>, <idx>{, <idx>}*. Overview:; """""""""""""""""". The '``",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:405214,scalab,scalable,405214,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['scalab'],['scalable']
Performance,"ug is found by hitting; a node that satisfies some ""bug condition"" (basically a violation of a; checking invariant). The analyzer traces out multiple paths by reasoning about branches and; then bifurcating the state: on the true branch the conditions of the; branch are assumed to be true and on the false branch the conditions; of the branch are assumed to be false. Such ""assumptions"" create; constraints on the values of the program, and those constraints are; recorded in the ProgramState object (and are manipulated by the; ConstraintManager). If assuming the conditions of a branch would; cause the constraints to be unsatisfiable, the branch is considered; infeasible and that path is not taken. This is how we get; path-sensitivity. We reduce exponential blow-up by caching nodes. If; a new node with the same state and program point as an existing node; would get generated, the path ""caches out"" and we simply reuse the; existing node. Thus the ExplodedGraph is not a DAG; it can contain; cycles as paths loop back onto each other and cache out. ProgramState and ExplodedNodes are basically immutable once created. Once; one creates a ProgramState, you need to create a new one to get a new; ProgramState. This immutability is key since the ExplodedGraph represents; the behavior of the analyzed program from the entry point. To; represent these efficiently, we use functional data structures (e.g.,; ImmutableMaps) which share data between instances. Finally, individual Checkers work by also manipulating the analysis; state. The analyzer engine talks to them via a visitor interface.; For example, the PreVisitCallExpr() method is called by ExprEngine; to tell the Checker that we are about to analyze a CallExpr, and the; checker is asked to check for any preconditions that might not be; satisfied. The checker can do nothing, or it can generate a new; ProgramState and ExplodedNode which contains updated checker state. If it; finds a bug, it can tell the BugReporter object about the",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/StaticAnalyzer/README.txt:2705,cache,cache,2705,interpreter/llvm-project/clang/lib/StaticAnalyzer/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/StaticAnalyzer/README.txt,1,['cache'],['cache']
Performance,"ugh it's PC-relative.). XCore:. No additional modifiers. Inline Asm Metadata; ^^^^^^^^^^^^^^^^^^^. The call instructions that wrap inline asm nodes may have a; ""``!srcloc``"" MDNode attached to it that contains a list of constant; integers. If present, the code generator will use the integer as the; location cookie value when report errors through the ``LLVMContext``; error reporting mechanisms. This allows a front-end to correlate backend; errors that occur with inline asm back to the source code that produced; it. For example:. .. code-block:: llvm. call void asm sideeffect ""something bad"", """"(), !srcloc !42; ...; !42 = !{ i32 1234567 }. It is up to the front-end to make sense of the magic numbers it places; in the IR. If the MDNode contains multiple constants, the code generator; will use the one that corresponds to the line of the asm that the error; occurs on. .. _metadata:. Metadata; ========. LLVM IR allows metadata to be attached to instructions and global objects in the; program that can convey extra information about the code to the optimizers and; code generator. One example application of metadata is source-level; debug information. There are two metadata primitives: strings and nodes. Metadata does not have a type, and is not a value. If referenced from a; ``call`` instruction, it uses the ``metadata`` type. All metadata are identified in syntax by an exclamation point ('``!``'). .. _metadata-string:. Metadata Nodes and Metadata Strings; -----------------------------------. A metadata string is a string surrounded by double quotes. It can; contain any character by escaping non-printable characters with; ""``\xx``"" where ""``xx``"" is the two digit hex code. For example:; ""``!""test\00""``"". Metadata nodes are represented with notation similar to structure; constants (a comma separated list of elements, surrounded by braces and; preceded by an exclamation point). Metadata nodes can have any values as; their operand. For example:. .. code-block:: llvm. !{ !""tes",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:242671,optimiz,optimizers,242671,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['optimiz'],['optimizers']
Performance,"uild is a standard; build using the compiler installed on the host, and a stage2 build is built; using the stage1 compiler. This nomenclature holds up to more stages too. In; general a stage*n* build is built using the output from stage*n-1*. Apple Clang Builds (A More Complex Bootstrap); =============================================. Apple's Clang builds are a slightly more complicated example of the simple; bootstrapping scenario. Apple Clang is built using a 2-stage build. The stage1 compiler is a host-only compiler with some options set. The stage1; compiler is a balance of optimization vs build time because it is a throwaway.; The stage2 compiler is the fully optimized compiler intended to ship to users. Setting up these compilers requires a lot of options. To simplify the; configuration the Apple Clang build settings are contained in CMake Cache files.; You can build an Apple Clang compiler using the following commands:. .. code-block:: console. $ cmake -G Ninja -C <path to source>/clang/cmake/caches/Apple-stage1.cmake <path to source>/llvm; $ ninja stage2-distribution. This CMake invocation configures the stage1 host compiler, and sets; CLANG_BOOTSTRAP_CMAKE_ARGS to pass the Apple-stage2.cmake cache script to the; stage2 configuration step. When you build the stage2-distribution target it builds the minimal stage1; compiler and required tools, then configures and builds the stage2 compiler; based on the settings in Apple-stage2.cmake. This pattern of using cache scripts to set complex settings, and specifically to; make later stage builds include cache scripts is common in our more advanced; build configurations. Multi-stage PGO; ===============. Profile-Guided Optimizations (PGO) is a really great way to optimize the code; clang generates. Our multi-stage PGO builds are a workflow for generating PGO; profiles that can be used to optimize clang. At a high level, the way PGO works is that you build an instrumented compiler,; then you run the instrumented compi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AdvancedBuilds.rst:4071,cache,caches,4071,interpreter/llvm-project/llvm/docs/AdvancedBuilds.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AdvancedBuilds.rst,1,['cache'],['caches']
Performance,"uild; % cd gcc-${gcc_version}-build; % $PWD/../gcc-${gcc_version}/configure --prefix=$HOME/toolchains --enable-languages=c,c++; % make -j$(nproc); % make install. For more details, check out the excellent `GCC wiki entry`_, where I got most; of this information from. .. _GCC wiki entry:; https://gcc.gnu.org/wiki/InstallingGCC. Once you have a GCC toolchain, configure your build of LLVM to use the new; toolchain for your host compiler and C++ standard library. Because the new; version of libstdc++ is not on the system library search path, you need to pass; extra linker flags so that it can be found at link time (``-L``) and at runtime; (``-rpath``). If you are using CMake, this invocation should produce working; binaries:. .. code-block:: console. % mkdir build; % cd build; % CC=$HOME/toolchains/bin/gcc CXX=$HOME/toolchains/bin/g++ \; cmake .. -DCMAKE_CXX_LINK_FLAGS=""-Wl,-rpath,$HOME/toolchains/lib64 -L$HOME/toolchains/lib64"". If you fail to set rpath, most LLVM binaries will fail on startup with a message; from the loader similar to ``libstdc++.so.6: version `GLIBCXX_3.4.20' not; found``. This means you need to tweak the -rpath linker flag. This method will add an absolute path to the rpath of all executables. That's; fine for local development. If you want to distribute the binaries you build; so that they can run on older systems, copy ``libstdc++.so.6`` into the; ``lib/`` directory. All of LLVM's shipping binaries have an rpath pointing at; ``$ORIGIN/../lib``, so they will find ``libstdc++.so.6`` there. Non-distributed; binaries don't have an rpath set and won't find ``libstdc++.so.6``. Pass; ``-DLLVM_LOCAL_RPATH=""$HOME/toolchains/lib64""`` to cmake to add an absolute; path to ``libstdc++.so.6`` as above. Since these binaries are not distributed,; having an absolute local path is fine for them. When you build Clang, you will need to give *it* access to modern C++; standard library in order to use it as your new host in part of a bootstrap.; There are two easy ways",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GettingStarted.rst:18633,load,loader,18633,interpreter/llvm-project/llvm/docs/GettingStarted.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GettingStarted.rst,1,['load'],['loader']
Performance,"uilder->GetInsertBlock()->getParent();. // Create an alloca for the variable in the entry block.; AllocaInst *Alloca = CreateEntryBlockAlloca(TheFunction, VarName);. // Emit the start code first, without 'variable' in scope.; Value *StartVal = Start->codegen();; if (!StartVal); return nullptr;. // Store the value into the alloca.; Builder->CreateStore(StartVal, Alloca);; ... // Compute the end condition.; Value *EndCond = End->codegen();; if (!EndCond); return nullptr;. // Reload, increment, and restore the alloca. This handles the case where; // the body of the loop mutates the variable.; Value *CurVar = Builder->CreateLoad(Alloca->getAllocatedType(), Alloca,; VarName.c_str());; Value *NextVar = Builder->CreateFAdd(CurVar, StepVal, ""nextvar"");; Builder->CreateStore(NextVar, Alloca);; ... This code is virtually identical to the code `before we allowed mutable; variables <LangImpl05.html#code-generation-for-the-for-loop>`_. The big difference is that we; no longer have to construct a PHI node, and we use load/store to access; the variable as needed. To support mutable argument variables, we need to also make allocas for; them. The code for this is also pretty simple:. .. code-block:: c++. Function *FunctionAST::codegen() {; ...; Builder->SetInsertPoint(BB);. // Record the function arguments in the NamedValues map.; NamedValues.clear();; for (auto &Arg : TheFunction->args()) {; // Create an alloca for this variable.; AllocaInst *Alloca = CreateEntryBlockAlloca(TheFunction, Arg.getName());. // Store the initial value into the alloca.; Builder->CreateStore(&Arg, Alloca);. // Add arguments to variable symbol table.; NamedValues[std::string(Arg.getName())] = Alloca;; }. if (Value *RetVal = Body->codegen()) {; ... For each argument, we make an alloca, store the input value to the; function into the alloca, and register the alloca as the memory location; for the argument. This method gets invoked by ``FunctionAST::codegen()``; right after it sets up the entry block for the ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:15457,load,load,15457,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,1,['load'],['load']
Performance,"uire; followed by a store; release is; prevented by the; s_waitcnt of; the release, but; there is nothing; preventing a store; release followed by; load acquire from; completing out of; order. The s_waitcnt; could be placed after; seq_store or before; the seq_load. We; choose the load to; make the s_waitcnt be; as late as possible; so that the store; may have already; completed.). 2. *Following; instructions same as; corresponding load; atomic acquire,; except must generate; all instructions even; for OpenCL.*; load atomic seq_cst - workgroup - local *Same as corresponding; load atomic acquire,; except must generate; all instructions even; for OpenCL.*. load atomic seq_cst - agent - global 1. s_waitcnt lgkmcnt(0) &; - system - generic vmcnt(0). - Could be split into; separate s_waitcnt; vmcnt(0); and s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt lgkmcnt(0); must happen after; preceding; global/generic load; atomic/store; atomic/atomicrmw; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; lgkmcnt(0) and so do; not need to be; considered.); - s_waitcnt vmcnt(0); must happen after; preceding; global/generic load; atomic/store; atomic/atomicrmw; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; vmcnt(0) and so do; not need to be; considered.); - Ensures any; preceding; sequential; consistent global; memory instructions; have completed; before executing; this sequentially; consistent; instruction. This; prevents reordering; a seq_cst store; followed by a; seq_cst load. (Note; that seq_cst is; stronger than; acquire/release as; the reordering of; load acquire; followed by a store; release is; prevented by the; s_waitcnt of; the release, but; there is nothing; preventing a store; release followed by; load acquire from; completing out of; order. The s_waitcnt;",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:232066,load,load,232066,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"ul.with.overflow``; on any integer bit width or vectors of integers. ::. declare {i16, i1} @llvm.umul.with.overflow.i16(i16 %a, i16 %b); declare {i32, i1} @llvm.umul.with.overflow.i32(i32 %a, i32 %b); declare {i64, i1} @llvm.umul.with.overflow.i64(i64 %a, i64 %b); declare {<4 x i32>, <4 x i1>} @llvm.umul.with.overflow.v4i32(<4 x i32> %a, <4 x i32> %b). Overview:; """""""""""""""""". The '``llvm.umul.with.overflow``' family of intrinsic functions perform; a unsigned multiplication of the two arguments, and indicate whether an; overflow occurred during the unsigned multiplication. Arguments:; """""""""""""""""""". The arguments (%a and %b) and the first element of the result structure; may be of integer types of any bit width, but they must have the same; bit width. The second element of the result structure must be of type; ``i1``. ``%a`` and ``%b`` are the two values that will undergo unsigned; multiplication. Semantics:; """""""""""""""""""". The '``llvm.umul.with.overflow``' family of intrinsic functions perform; an unsigned multiplication of the two arguments. They return a structure ---; the first element of which is the multiplication, and the second; element of which is a bit specifying if the unsigned multiplication; resulted in an overflow. Examples:; """""""""""""""""". .. code-block:: llvm. %res = call {i32, i1} @llvm.umul.with.overflow.i32(i32 %a, i32 %b); %sum = extractvalue {i32, i1} %res, 0; %obit = extractvalue {i32, i1} %res, 1; br i1 %obit, label %overflow, label %normal. Saturation Arithmetic Intrinsics; ---------------------------------. Saturation arithmetic is a version of arithmetic in which operations are; limited to a fixed range between a minimum and maximum value. If the result of; an operation is greater than the maximum value, the result is set (or; ""clamped"") to this maximum. If it is below the minimum, it is clamped to this; minimum. '``llvm.sadd.sat.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax; """""""""""""". This is an overloaded intrinsic. You can use ``llvm.sadd.s",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:610083,perform,perform,610083,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['perform']
Performance,"ular symbol. This type; of rule is typically used to implement an interactive renaming action that; allows users to specify which occurrences should be renamed during the; refactoring. Subclasses that choose to implement this rule have to implement; the ``findSymbolOccurrences`` member function. The following set of quick checks might help if you are unsure about the type; of rule you should use:. #. If you would like to transform the source in one translation unit and if; you don't need any cross-TU information, then the; ``SourceChangeRefactoringRule`` should work for you. #. If you would like to implement a rename-like operation with potential; interactive components, then ``FindSymbolOccurrencesRefactoringRule`` might; work for you. How to Create a Rule; ^^^^^^^^^^^^^^^^^^^^. Once you determine which type of rule is suitable for your needs you can; implement the refactoring by subclassing the rule and implementing its; interface. The subclass should have a constructor that takes the inputs that; are needed to perform the refactoring. For example, if you want to implement a; rule that simply deletes a selection, you should create a subclass of; ``SourceChangeRefactoringRule`` with a constructor that accepts the selection; range:. .. code-block:: c++. class DeleteSelectedRange final : public SourceChangeRefactoringRule {; public:; DeleteSelection(SourceRange Selection) : Selection(Selection) {}. Expected<AtomicChanges>; createSourceReplacements(RefactoringRuleContext &Context) override {; AtomicChange Replacement(Context.getSources(), Selection.getBegin());; Replacement.replace(Context.getSource,; CharSourceRange::getCharRange(Selection), """");; return { Replacement };; }; private:; SourceRange Selection;; };. The rule's subclass can then be added to the list of refactoring action's; rules for a particular action using the ``createRefactoringActionRule``; function. For example, the class that's shown above can be added to the; list of action rules using the followi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/RefactoringEngine.rst:5240,perform,perform,5240,interpreter/llvm-project/clang/docs/RefactoringEngine.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/RefactoringEngine.rst,1,['perform'],['perform']
Performance,"ularity** --- SelectionDAG and FastISel are radically different and share; very little code. GlobalISel is built in a way that enables code reuse. For instance, both the; optimized and fast selectors share the :ref:`pipeline`, and targets can; configure that pipeline to better suit their needs. Design and Implementation Reference; ===================================. More information on the design and implementation of GlobalISel can be found in; the following sections. .. toctree::; :maxdepth: 1. GMIR; GenericOpcode; MIRPatterns; Pipeline; Porting; Resources. More information on specific passes can be found in the following sections:. .. toctree::; :maxdepth: 1. IRTranslator; Legalizer; RegBankSelect; InstructionSelect; KnownBits. .. _progress:. Progress and Future Work; ========================. The initial goal is to replace FastISel on AArch64. The next step will be to; replace SelectionDAG as the optimized ISel. ``NOTE``:; While we iterate on GlobalISel, we strive to avoid affecting the performance of; SelectionDAG, FastISel, or the other MIR passes. For instance, the types of; :ref:`gmir-gvregs` are stored in a separate table in ``MachineRegisterInfo``,; that is destroyed after :ref:`instructionselect`. .. _progress-fastisel:. FastISel Replacement; --------------------. For the initial FastISel replacement, we intend to fallback to SelectionDAG on; selection failures. Currently, compile-time of the fast pipeline is within 1.5x of FastISel.; We're optimistic we can get to within 1.1/1.2x, but beating FastISel will be; challenging given the multi-pass approach.; Still, supporting all IR (via a complete legalizer) and avoiding the fallback; to SelectionDAG in the worst case should enable better amortized performance; than SelectionDAG+FastISel. ``NOTE``:; We considered never having a fallback to SelectionDAG, instead deciding early; whether a given function is supported by GlobalISel or not. The decision would; be based on :ref:`milegalizer` queries.; We abandoned",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GlobalISel/index.rst:2102,perform,performance,2102,interpreter/llvm-project/llvm/docs/GlobalISel/index.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GlobalISel/index.rst,1,['perform'],['performance']
Performance,"ularly useful for the staging buildmaster which is silent; otherwise. #. Send the buildbot-worker access name and the access password directly to; `Galina Kistanova <mailto:gkistanova@gmail.com>`_, and wait until she; lets you know that your changes are applied and buildmaster is; reconfigured. #. Make sure you can start the buildbot-worker and successfully connect; to the silent buildmaster. Then set up your buildbot-worker to start; automatically at the start up time. See the buildbot documentation; for help. You may want to restart your computer to see if it works. #. Check the status of your buildbot-worker on the `Waterfall Display (Staging); <http://lab.llvm.org/staging/#/waterfall>`_ to make sure it is; connected, and the `Workers Display (Staging); <http://lab.llvm.org/staging/#/workers>`_ to see if administrator; contact and worker information are correct. #. At this point, you have a working builder connected to the staging; buildmaster. You can now make sure it is reliably green and keeps; up with the build queue. No notifications will be sent, so you can; keep an unstable builder connected to staging indefinitely. #. (Optional) Once the builder is stable on the staging buildmaster with; several days of green history, you can choose to move it to the production; buildmaster to enable developer notifications. Please email `Galina; Kistanova <mailto:gkistanova@gmail.com>`_ for review and approval. To move a worker to production (once approved), stop your worker, edit the; buildbot.tac file to change the port number from 9994 to 9990 and start it; again. Best Practices for Configuring a Fast Builder; =============================================. As mentioned above, we generally have a strong preference for; builders which can build every commit as they come in. This section; includes best practices and some recommendations as to how to achieve; that end. The goal; In 2020, the monorepo had just under 35 thousand commits. This works; out to an average of 4 co",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToAddABuilder.rst:7224,queue,queue,7224,interpreter/llvm-project/llvm/docs/HowToAddABuilder.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToAddABuilder.rst,1,['queue'],['queue']
Performance,"uld be undefined for pslld. //===---------------------------------------------------------------------===//. #include <math.h>; int t1(double d) { return signbit(d); }. This currently compiles to:; 	subl	$12, %esp; 	movsd	16(%esp), %xmm0; 	movsd	%xmm0, (%esp); 	movl	4(%esp), %eax; 	shrl	$31, %eax; 	addl	$12, %esp; 	ret. We should use movmskp{s|d} instead. //===---------------------------------------------------------------------===//. CodeGen/X86/vec_align.ll tests whether we can turn 4 scalar loads into a single; (aligned) vector load. This functionality has a couple of problems. 1. The code to infer alignment from loads of globals is in the X86 backend,; not the dag combiner. This is because dagcombine2 needs to be able to see; through the X86ISD::Wrapper node, which DAGCombine can't really do.; 2. The code for turning 4 x load into a single vector load is target ; independent and should be moved to the dag combiner.; 3. The code for turning 4 x load into a vector load can only handle a direct ; load from a global or a direct load from the stack. It should be generalized; to handle any load from P, P+4, P+8, P+12, where P can be anything.; 4. The alignment inference code cannot handle loads from globals in non-static; mode because it doesn't look through the extra dyld stub load. If you try; vec_align.ll without -relocation-model=static, you'll see what I mean. //===---------------------------------------------------------------------===//. We should lower store(fneg(load p), q) into an integer load+xor+store, which; eliminates a constant pool load. For example, consider:. define i64 @ccosf(float %z.0, float %z.1) nounwind readonly {; entry:; %tmp6 = fsub float -0.000000e+00, %z.1		; <float> [#uses=1]; %tmp20 = tail call i64 @ccoshf( float %tmp6, float %z.0 ) nounwind readonly; ret i64 %tmp20; }; declare i64 @ccoshf(float %z.0, float %z.1) nounwind readonly. This currently compiles to:. LCPI1_0:					# <4 x float>; 	.long	2147483648	# float -0; 	.long	2147483648	# ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:11906,load,load,11906,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,8,['load'],['load']
Performance,"uld look at the; root reference guide at: <http://root.cern.ch/root/Reference.html>. ## Overview of Matrix Classes. The figure below shows an overview of the classes available in the; linear algebra library,` libMatrix.so`. At the center is the base class; **`TMatrixDBase`** from which three different matrix classes,; **`TMatrixD`**, **`TMatrixDSym`** and **`TMatrixDFSparse`** derive. The; user can define customized matrix operations through the classes; **`TElementActionD`** and **`TElementsPosActionD`**. ![Overview of matrix classes](pictures/0300012D.png). Reference to different views of the matrix can be created through the; classes on the right-hand side, see ""Matrix Views"". These references; provide a natural connection to vectors. Matrix decompositions (used in equation solving and matrix inversion); are available through the classes on the left-hand side (see ""Matrix; Decompositions""). They inherit from the **`TDecompBase`** class. The; Eigen Analysis is performed through the classes at the top, see ""Matrix; Eigen Analysis"". In both cases, only some matrix types can be analyzed.; For instance, **`TDecompChol`** will only accept symmetric matrices as; defined **`TMatrixDSym`**. The assignment operator behaves somewhat; different than of most other classes. The following lines will result in; an error:. ``` {.cpp}; TMatrixD a(3,4);; TMatrixD b(5,6);; b = a;; ```. It required to first resize matrix b to the shape of `a`. ``` {.cpp}; TMatrixD a(3,4);; TMatrixD b(5,6);; b.ResizeTo(a);; b = a;; ```. ## Matrix Properties. A matrix has five properties, which are all set in the constructor:. - `precision` - float or double. In the first case you will use the; **`TMatrixF`** class family, in the latter case the **`TMatrixD`**; one;. - `type` - general (**`TMatrixD`**), symmetric (**`TMatrixDSym`**) or; sparse (**`TMatrixDSparse`**);. - `size` - number of rows and columns;. - `index` - range start of row and column index. By default these; start at zero;. - `sparse` `ma",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/LinearAlgebra.md:2337,perform,performed,2337,documentation/users-guide/LinearAlgebra.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/LinearAlgebra.md,1,['perform'],['performed']
Performance,"ule-path=<path/to/directory>``.; * (2) ``-fmodule-file=<path/to/BMI>`` (Deprecated).; * (3) ``-fmodule-file=<module-name>=<path/to/BMI>``. The option ``-fprebuilt-module-path`` tells the compiler the path where to search for dependent BMIs.; It may be used multiple times just like ``-I`` for specifying paths for header files. The look up rule here is:. * (1) When we import module M. The compiler would look up M.pcm in the directories specified; by ``-fprebuilt-module-path``.; * (2) When we import partition module unit M:P. The compiler would look up M-P.pcm in the; directories specified by ``-fprebuilt-module-path``. The option ``-fmodule-file=<path/to/BMI>`` tells the compiler to load the specified BMI directly.; The option ``-fmodule-file=<module-name>=<path/to/BMI>`` tells the compiler to load the specified BMI; for the module specified by ``<module-name>`` when necessary. The main difference is that; ``-fmodule-file=<path/to/BMI>`` will load the BMI eagerly, whereas; ``-fmodule-file=<module-name>=<path/to/BMI>`` will only load the BMI lazily, which is similar; with ``-fprebuilt-module-path``. The option ``-fmodule-file=<path/to/BMI>`` for named modules is deprecated; and is planning to be removed in future versions. In case all ``-fprebuilt-module-path=<path/to/directory>``, ``-fmodule-file=<path/to/BMI>`` and; ``-fmodule-file=<module-name>=<path/to/BMI>`` exist, the ``-fmodule-file=<path/to/BMI>`` option; takes highest precedence and ``-fmodule-file=<module-name>=<path/to/BMI>`` will take the second; highest precedence. We need to specify all the dependent (directly and indirectly) BMIs.; See https://github.com/llvm/llvm-project/issues/62707 for detail. When we compile a ``module implementation unit``, we must specify the BMI of the corresponding; ``primary module interface unit``.; Since the language specification says a module implementation unit implicitly imports; the primary module interface unit. [module.unit]p8. A module-declaration that contains neither ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/StandardCPlusPlusModules.rst:11788,load,load,11788,interpreter/llvm-project/clang/docs/StandardCPlusPlusModules.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/StandardCPlusPlusModules.rst,2,['load'],['load']
Performance,"ules. For; ROOT it is between 40-60 MB depending on the concrete configuration.; When the workload increases we notice that the overall memory performance; decreases in number of cases.; * Execution times -- likewise we have an execution overhead. For ; workflows which take ms the slowdown can be 2x. Increasing of the work; to seconds shows 50-60% slowdowns. The performance is dependent on many factors such as configuration of ROOT and; workflow. You can read more at our Intel IPCC-ROOT Showcase presentation; here (pp 25-33)[[8]]. #### Loading C++ Modules on Demand. In long term, we should optimize the preloading of modules to be a no-op and; avoid recursive behavior based on identifier lookup callbacks. Unfortunately,; at the moment the loading of C++ modules on demand shows significantly better; performance results. You can visit our continuous performance monitoring tool where we compare; the performance of ROOT against ROOT with a PCH [[9]].; *Note: if you get error 400, clean your cache or open a private browser session.*. ## How to use; C++ Modules in ROOT are default since v6.20 (Unix) and v6.22 (OSX). Enjoy. To disable C++ Modules in ROOT use `-Druntime_cxxmodules=Off`. ## Citing ROOT's C++ Modules; ```latex; % Peer-Reviewed Publication; %; % 22nd International Conference on Computing in High Energy and Nuclear Physics (CHEP); % 8-14 October, 2016, San Francisco, USA; %; @inproceedings{Vassilev_ROOTModules,; author = {Vassilev,V.},; title = {{Optimizing ROOT's Performance Using C++ Modules}},; journal = {Journal of Physics: Conference Series},; year = 2017,; month = {oct},; volume = {898},; number = {7},; pages = {072023},; doi = {10.1088/1742-6596/898/7/072023},; url = {https://iopscience.iop.org/article/10.1088/1742-6596/898/7/072023/pdf},; publisher = {{IOP} Publishing}; }; ```; ; # Acknowledgement. We would like to thank the ROOT team. We would like to thank Liz Sexton-Kennedy (FNAL) in particular for supporting; this project. We would like to thank Axel",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/README.CXXMODULES.md:19311,cache,cache,19311,README/README.CXXMODULES.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/README.CXXMODULES.md,1,['cache'],['cache']
Performance,"ull as a function parameter; which is annotated with ``_Nonnull``.; - ``-fsanitize=nullability-assign``: Assigning null to an lvalue which; is annotated with ``_Nonnull``.; - ``-fsanitize=nullability-return``: Returning null from a function with; a return type annotated with ``_Nonnull``.; - ``-fsanitize=objc-cast``: Invalid implicit cast of an ObjC object pointer; to an incompatible type. This is often unintentional, but is not undefined; behavior, therefore the check is not a part of the ``undefined`` group.; Currently only supported on Darwin.; - ``-fsanitize=object-size``: An attempt to potentially use bytes which; the optimizer can determine are not part of the object being accessed.; This will also detect some types of undefined behavior that may not; directly access memory, but are provably incorrect given the size of; the objects involved, such as invalid downcasts and calling methods on; invalid pointers. These checks are made in terms of; ``__builtin_object_size``, and consequently may be able to detect more; problems at higher optimization levels.; - ``-fsanitize=pointer-overflow``: Performing pointer arithmetic which; overflows, or where either the old or new pointer value is a null pointer; (or in C, when they both are).; - ``-fsanitize=return``: In C++, reaching the end of a; value-returning function without returning a value.; - ``-fsanitize=returns-nonnull-attribute``: Returning null pointer; from a function which is declared to never return null.; - ``-fsanitize=shift``: Shift operators where the amount shifted is; greater or equal to the promoted bit-width of the left hand side; or less than zero, or where the left hand side is negative. For a; signed left shift, also checks for signed overflow in C, and for; unsigned overflow in C++. You can use ``-fsanitize=shift-base`` or; ``-fsanitize=shift-exponent`` to check only left-hand side or; right-hand side of shift operation, respectively.; - ``-fsanitize=unsigned-shift-base``: check that an unsigned l",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UndefinedBehaviorSanitizer.rst:7538,optimiz,optimization,7538,interpreter/llvm-project/clang/docs/UndefinedBehaviorSanitizer.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UndefinedBehaviorSanitizer.rst,1,['optimiz'],['optimization']
Performance,"ult all such opcodes are filtered out.; This flag will instead show only such unstable opcodes. .. option:: --ignore-invalid-sched-class=false. If set, ignore instructions that do not have a sched class (class idx = 0). .. option:: --mtriple=<triple name>. Target triple. See `-version` for available targets. .. option:: --mcpu=<cpu name>. If set, measure the cpu characteristics using the counters for this CPU. This; is useful when creating new sched models (the host CPU is unknown to LLVM).; (`-mcpu=help` for details). .. option:: --analysis-override-benchmark-triple-and-cpu. By default, llvm-exegesis will analyze the benchmarks for the triple/CPU they; were measured for, but if you want to analyze them for some other combination; (specified via `-mtriple`/`-mcpu`), you can pass this flag. .. option:: --dump-object-to-disk=true. If set, llvm-exegesis will dump the generated code to a temporary file to; enable code inspection. Disabled by default. .. option:: --use-dummy-perf-counters. If set, llvm-exegesis will not read any real performance counters and; return a dummy value instead. This can be used to ensure a snippet doesn't; crash when hardware performance counters are unavailable and for; debugging :program:`llvm-exegesis` itself. .. option:: --execution-mode=[inprocess,subprocess]. This option specifies what execution mode to use. The `inprocess` execution; mode is the default. The `subprocess` execution mode allows for additional; features such as memory annotations but is currently restricted to X86-64; on Linux. .. option:: --benchmark-repeat-count=<repeat-count>. This option enables specifying the number of times to repeat the measurement; when performing latency measurements. By default, llvm-exegesis will repeat; a latency measurement enough times to balance run-time and noise reduction. EXIT STATUS; -----------. :program:`llvm-exegesis` returns 0 on success. Otherwise, an error message is; printed to standard error, and the tool returns a non 0 value.; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-exegesis.rst:17602,perform,performance,17602,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-exegesis.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-exegesis.rst,5,"['latency', 'perform']","['latency', 'performance', 'performing']"
Performance,"ult from 1.9 GBytes to 100 GBytes.; Add new special functions in TTreeFormula (and hence TTree::Draw and TTree::Scan) to calculate the minimun and maximum with an entry:; ; Min$(formula),Max$(formula):return the minimun/maximum (within one TTree entry) of the value of the; elements of the formula given as a parameter.; MinIf$(formula,condition),MaxIf$(formula,condition):return the minimum (maximum) (within one TTree entry); of the value of the elements of the formula given as a parameter; if they match the condition. If not element match the condition, the result is zero. To avoid the; the result is zero. To avoid the consequent peak a zero, use the; pattern:; tree->Draw(""MinIf$(formula,condition)"",""condition"");; which will avoid calculation MinIf$ for the entries that have no match; for the condition. Add support in TTreeFormula (and hence TTree::Draw and TTree::Scan) for the ternary condition operator ( cond ? if_expr : else_expr ).; Significantly (by 2 order of magnitude) improved the performance of TTree::Draw calling C++ functions.; Replace the function TSelectorDraw::MakeIndex and TSelectorDraw::GetNameByIndex; with the function TSelectorDraw::SplitNames. ; Add a return value to SetBranchAddress, a return value greater or equal to zero indicate success, a negative; value indicates failure (in both case, the address is still updated). Example:; if (tree->SetBranchAddress(mybranch,&myvar) < 0) {; cerr << ""Something went wrong\n"";; return;; }; The possible return values are:; kMissingBranch (-5) : Missing branch; kInternalError (-4) : Internal error (could not find the type corresponding to a data type number.; kMissingCompiledCollectionProxy (-3) : Missing compiled collection proxy for a compiled collection.; kMismatch (-2) : Non-Class Pointer type given does not match the type expected by the branch.; kClassMismatch (-1) : Class Pointer type given does not match the type expected by the branch.; kMatch (0) : perfect match.; kMatchConversion (1) : match with (I/",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/doc/v526/index.html:1039,perform,performance,1039,tree/doc/v526/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/doc/v526/index.html,2,['perform'],['performance']
Performance,"ult; environment. When code is compiled in this default mode, operations that depend; on the environment (such as floating-point arithmetic and `FLT_ROUNDS`) may have; undefined behavior if the dynamic environment is not the default environment; for; example, `FLT_ROUNDS` may or may not simply return its default value for the target; instead of reading the dynamic environment, and floating-point operations may be; optimized as if the dynamic environment were the default. Similarly, it is undefined; behavior to change the floating point environment in this default mode, for example; by calling the `fesetround` function.; C provides two pragmas to allow code to dynamically modify the floating point environment:. - ``#pragma STDC FENV_ACCESS ON`` allows dynamic changes to the entire floating; point environment. - ``#pragma STDC FENV_ROUND FE_DYNAMIC`` allows dynamic changes to just the floating; point rounding mode. This may be more optimizable than ``FENV_ACCESS ON`` because; the compiler can still ignore the possibility of floating-point exceptions by default. Both of these can be used either at the start of a block scope, in which case; they cover all code in that scope (unless they're turned off in a child scope),; or at the top level in a file, in which case they cover all subsequent function; bodies until they're turned off. Note that it is undefined behavior to enter; code that is *not* covered by one of these pragmas from code that *is* covered; by one of these pragmas unless the floating point environment has been restored; to its default state. See the C standard for more information about these pragmas. The command line option ``-frounding-math`` behaves as if the translation unit; began with ``#pragma STDC FENV_ROUND FE_DYNAMIC``. The command line option; ``-ffp-model=strict`` behaves as if the translation unit began with ``#pragma STDC FENV_ACCESS ON``. Code that just wants to use a specific rounding mode for specific floating point; operations can avoid mo",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:70383,optimiz,optimizable,70383,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,1,['optimiz'],['optimizable']
Performance,"ulting; ``llvm.module.flags`` metadata is the union of the modules' flags. That is, for; each unique metadata ID string, there will be exactly one entry in the merged; modules ``llvm.module.flags`` metadata table, and the value for that entry will; be determined by the merge behavior flag, as described below. The only exception; is that entries with the *Require* behavior are always preserved. The following behaviors are supported:. .. list-table::; :header-rows: 1; :widths: 10 90. * - Value; - Behavior. * - 1; - **Error**; Emits an error if two values disagree, otherwise the resulting value; is that of the operands. * - 2; - **Warning**; Emits a warning if two values disagree. The result value will be the; operand for the flag from the first module being linked, unless the; other module uses **Min** or **Max**, in which case the result will; be **Min** (with the min value) or **Max** (with the max value),; respectively. * - 3; - **Require**; Adds a requirement that another module flag be present and have a; specified value after linking is performed. The value must be a; metadata pair, where the first element of the pair is the ID of the; module flag to be restricted, and the second element of the pair is; the value the module flag should be restricted to. This behavior can; be used to restrict the allowable results (via triggering of an; error) of linking IDs with the **Override** behavior. * - 4; - **Override**; Uses the specified value, regardless of the behavior or value of the; other module. If both modules specify **Override**, but the values; differ, an error will be emitted. * - 5; - **Append**; Appends the two values, which are required to be metadata nodes. * - 6; - **AppendUnique**; Appends the two values, which are required to be metadata; nodes. However, duplicate entries in the second list are dropped; during the append operation. * - 7; - **Max**; Takes the max of the two values, which are required to be integers. * - 8; - **Min**; Takes the min of th",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:328841,perform,performed,328841,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performed']
Performance,"ultiple locations over part or all of its lifetime.*. If a location description has more than one single location description, the; DWARF expression is ill-formed if the object value held in each single location; description's position within the associated location storage is not the same; value, except for the parts of the value that are uninitialized. *A location description that has more than one single location description can; only be created by a location list expression that has overlapping program; location ranges, or certain expression operations that act on a location; description that has more than one single location description. There are no; operation expression operations that can directly create a location description; with more than one single location description.*. *A location description with more than one single location description can be; used to describe objects that reside in more than one piece of storage at the; same time. An object may have more than one location as a result of; optimization. For example, a value that is only read may be promoted from memory; to a register for some region of code, but later code may revert to reading the; value from memory as the register may be used for other purposes. For the code; region where the value is in a register, any change to the object value must be; made in both the register and the memory so both regions of code will read the; updated value.*. *A consumer of a location description with more than one single location; description can read the object's value from any of the single location; descriptions (since they all refer to location storage that has the same value),; but must write any changed value to all the single location descriptions.*. The evaluation of an expression may require context elements to create a; location description. If such a location description is accessed, the storage it; denotes is that associated with the context element values specified when the; location descrip",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst:60774,optimiz,optimization,60774,interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst,1,['optimiz'],['optimization']
Performance,"ultiple; sections. Supported flag names are `alloc`, `load`, `noload`, `readonly`, `exclude`,; `debug`, `code`, `data`, `rom`, `share`, `contents`, `merge`, `strings`, and; `large`. Not all flags are meaningful for all object file formats or target; architectures. For ELF objects, the flags have the following effects:. - `alloc` = add the `SHF_ALLOC` flag.; - `load` = if the section has `SHT_NOBITS` type, mark it as a `SHT_PROGBITS`; section.; - `readonly` = if this flag is not specified, add the `SHF_WRITE` flag.; - `exclude` = add the `SHF_EXCLUDE` flag.; - `code` = add the `SHF_EXECINSTR` flag.; - `merge` = add the `SHF_MERGE` flag.; - `strings` = add the `SHF_STRINGS` flag.; - `contents` = if the section has `SHT_NOBITS` type, mark it as a `SHT_PROGBITS`; section.; - `large` = add the `SHF_X86_64_LARGE` on x86_64; rejected if the target; architecture is not x86_64. For COFF objects, the flags have the following effects:. - `alloc` = add the `IMAGE_SCN_CNT_UNINITIALIZED_DATA` and `IMAGE_SCN_MEM_READ`; flags, unless the `load` flag is specified.; - `noload` = add the `IMAGE_SCN_LNK_REMOVE` and `IMAGE_SCN_MEM_READ` flags.; - `readonly` = if this flag is not specified, add the `IMAGE_SCN_MEM_WRITE`; flag.; - `exclude` = add the `IMAGE_SCN_LNK_REMOVE` and `IMAGE_SCN_MEM_READ` flags.; - `debug` = add the `IMAGE_SCN_CNT_INITIALIZED_DATA`,; `IMAGE_SCN_MEM_DISCARDABLE` and `IMAGE_SCN_MEM_READ` flags.; - `code` = add the `IMAGE_SCN_CNT_CODE`, `IMAGE_SCN_MEM_EXECUTE` and; `IMAGE_SCN_MEM_READ` flags.; - `data` = add the `IMAGE_SCN_CNT_INITIALIZED_DATA` and `IMAGE_SCN_MEM_READ`; flags.; - `share` = add the `IMAGE_SCN_MEM_SHARED` and `IMAGE_SCN_MEM_READ` flags. .. option:: --strip-all-gnu. Remove all symbols, debug sections and relocations from the output. This option; is equivalent to GNU :program:`objcopy`'s ``--strip-all`` switch. .. option:: --strip-all, -S. For ELF objects, remove from the output all symbols and non-alloc sections not; within segments, except for .gnu.war",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-objcopy.rst:5857,load,load,5857,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-objcopy.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-objcopy.rst,1,['load'],['load']
Performance,"ultiplication trees. First, the intrinsic needs to be extended to support integers, and second the; code generator needs to be enhanced to lower these to multiplication trees. //===---------------------------------------------------------------------===//. Interesting? testcase for add/shift/mul reassoc:. int bar(int x, int y) {; return x*x*x+y+x*x*x*x*x*y*y*y*y;; }; int foo(int z, int n) {; return bar(z, n) + bar(2*z, 2*n);; }. This is blocked on not handling X*X*X -> powi(X, 3) (see note above). The issue; is that we end up getting t = 2*X s = t*t and don't turn this into 4*X*X,; which is the same number of multiplies and is canonical, because the 2*X has; multiple uses. Here's a simple example:. define i32 @test15(i32 %X1) {; %B = mul i32 %X1, 47 ; X1*47; %C = mul i32 %B, %B; ret i32 %C; }. //===---------------------------------------------------------------------===//. Reassociate should handle the example in GCC PR16157:. extern int a0, a1, a2, a3, a4; extern int b0, b1, b2, b3, b4; ; void f () { /* this can be optimized to four additions... */ ; b4 = a4 + a3 + a2 + a1 + a0; ; b3 = a3 + a2 + a1 + a0; ; b2 = a2 + a1 + a0; ; b1 = a1 + a0; ; } . This requires reassociating to forms of expressions that are already available,; something that reassoc doesn't think about yet. //===---------------------------------------------------------------------===//. These two functions should generate the same code on big-endian systems:. int g(int *j,int *l) { return memcmp(j,l,4); }; int h(int *j, int *l) { return *j - *l; }. this could be done in SelectionDAGISel.cpp, along with other special cases,; for 1,2,4,8 bytes. //===---------------------------------------------------------------------===//. It would be nice to revert this patch:; http://lists.llvm.org/pipermail/llvm-commits/Week-of-Mon-20060213/031986.html. And teach the dag combiner enough to simplify the code expanded before ; legalize. It seems plausible that this knowledge would let it simplify other; stuff too. /",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:2622,optimiz,optimized,2622,interpreter/llvm-project/llvm/lib/Target/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt,2,['optimiz'],['optimized']
Performance,"um size of a dispatch group. Both IPC; and 'uOps Per Cycle' are limited by the amount of hardware parallelism. The; availability of hardware resources affects the resource pressure distribution,; and it limits the number of instructions that can be executed in parallel every; cycle. A delta between Dispatch Width and the theoretical maximum uOps per; Cycle (computed by dividing the number of uOps of a single iteration by the; `Block RThroughput`) is an indicator of a performance bottleneck caused by the; lack of hardware resources.; In general, the lower the Block RThroughput, the better. In this example, ``uOps per iteration/Block RThroughput`` is 1.50. Since there; are no loop-carried dependencies, the observed `uOps Per Cycle` is expected to; approach 1.50 when the number of iterations tends to infinity. The delta between; the Dispatch Width (2.00), and the theoretical maximum uOp throughput (1.50) is; an indicator of a performance bottleneck caused by the lack of hardware; resources, and the *Resource pressure view* can help to identify the problematic; resource usage. The second section of the report is the `instruction info view`. It shows the; latency and reciprocal throughput of every instruction in the sequence. It also; reports extra information related to the number of micro opcodes, and opcode; properties (i.e., 'MayLoad', 'MayStore', and 'HasSideEffects'). Field *RThroughput* is the reciprocal of the instruction throughput. Throughput; is computed as the maximum number of instructions of a same type that can be; executed per clock cycle in the absence of operand dependencies. In this; example, the reciprocal throughput of a vector float multiply is 1; cycles/instruction. That is because the FP multiplier JFPM is only available; from pipeline JFPU1. Instruction encodings are displayed within the instruction info view when flag; `-show-encoding` is specified. Below is an example of `-show-encoding` output for the dot-product kernel:. .. code-block:: none.",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst:18358,perform,performance,18358,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,2,"['bottleneck', 'perform']","['bottleneck', 'performance']"
Performance,"umber in the current; tree. Assuming that `fChain` is the pointer to the **`TChain`**; being processed, use. ``` {.cpp}; fChain->GetTree()->GetEntry(entry);; ```. To create a selector call:. ``` {.cpp}; root[] T->MakeSelector(""MySelector"");; ```. Where `T` is the **`TTree`** and `MySelector` is the name of created; class and the name of the `.h` and `.C` files. The resulting; **`TSelector`** is the argument to **`TTree::Process`**. The argument can; be the file name or a pointer to the selector object. ``` {.cpp}; root[] T->Process(""MySelector.C"","""",1000,100);; ```. This call will interpret the class defined in `MySelector.C` and process; 1000 entries beginning with entry 100. The file name can be appended; with a ""+"" or a ""++"" to use `ACLiC`. ``` {.cpp}; root[] T->Process(""MySelector.C++"","""",1000,100);; ```. When appending a ""++"", the class will be compiled and dynamically; loaded. ``` {.cpp}; root[] T->Process(""MySelector.C+"","""",1000,100);; ```. When appending a ""+"", the class will also be compiled and dynamically; loaded. When it is called again, it recompiles only if the macro; (`MySelector.C`) has changed since it was compiled last. If not, it; loads the existing library. The next example shows how to create a; selector with a pointer:. ``` {.cpp}; MySelector *selector = (MySelector *)TSelector::GetSelector(""MySelector.C+"");; T->Process(selector);; ```. `Using this form, you can do things like:`. ``` {.cpp}; selector->public_attribute1 = init_value;; for (int i=0; i<limit; i++) {; T->Process(selector);; selector->public_attribute1 =; function(selector->public_attribute2);; }; ```. `TTree::Process()` is aware of PROOF, ROOT parallel processing facility.; If PROOF is setup, it divides the processing amongst the slave CPUs. ### Performance Benchmarks; \index{benchmarks}. The program `$ROOTSYS/test/bench.cxx` compares the I/O performance of; STL vectors to the ROOT native **`TClonesArray`**`s` collection class.; It creates trees with and without compression for the ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Trees.md:134376,load,loaded,134376,documentation/users-guide/Trees.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Trees.md,1,['load'],['loaded']
Performance,"umber of alias queries. This can cause debugging techniques; involving pausing execution after a predetermined number of queries to be; unreliable. Many alias queries can be reformulated in terms of other alias queries. When; multiple ``AliasAnalysis`` queries are chained together, it would make sense to; start those queries from the beginning of the chain, with care taken to avoid; infinite looping, however currently an implementation which wants to do this can; only start such queries from itself. Using alias analysis results; ============================. There are several different ways to use alias analysis results. In order of; preference, these are:. Using the ``MemoryDependenceAnalysis`` Pass; -------------------------------------------. The ``memdep`` pass uses alias analysis to provide high-level dependence; information about memory-using instructions. This will tell you which store; feeds into a load, for example. It uses caching and other techniques to be; efficient, and is used by Dead Store Elimination, GVN, and memcpy optimizations. .. _AliasSetTracker:. Using the ``AliasSetTracker`` class; -----------------------------------. Many transformations need information about alias **sets** that are active in; some scope, rather than information about pairwise aliasing. The; `AliasSetTracker <https://llvm.org/doxygen/classllvm_1_1AliasSetTracker.html>`__; class is used to efficiently build these Alias Sets from the pairwise alias; analysis information provided by the ``AliasAnalysis`` interface. First you initialize the AliasSetTracker by using the ""``add``"" methods to add; information about various potentially aliasing instructions in the scope you are; interested in. Once all of the alias sets are completed, your pass should; simply iterate through the constructed alias sets, using the ``AliasSetTracker``; ``begin()``/``end()`` methods. The ``AliasSet``\s formed by the ``AliasSetTracker`` are guaranteed to be; disjoint, calculate mod/ref information and vo",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AliasAnalysis.rst:19802,optimiz,optimizations,19802,interpreter/llvm-project/llvm/docs/AliasAnalysis.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AliasAnalysis.rst,1,['optimiz'],['optimizations']
Performance,"ume objects inherit from TAttLine class so the line style or; width can also be changed:. ``` {.cpp}; myVolume->SetLineColor(kRed);; myVolume->SetLineWith(2);; myVolume->SetLineStyle(kDotted);; ```. When drawing in solid mode, the color of the drawn volume corresponds to; the line color. #### Visibility Settings. The way geometry is build forces the definition of several volumes that; does not represent real objects, but just virtual containers used for; grouping and positioning volumes together. One would not want to see; them in the picture. Since every volume is by default visible, one has; to do this sort of tuning by its own:. ``` {.cpp}; myVolumeContainer->SetVisibility(kFALSE);; ```. As described before, the drawing package supports two main global; options: 1 (default) - only final volume leaves; 0 - all volumes down; the drawn one appear on the screen. The global visible level put a; limitation on the maximum applied depth. Combined with visibility; settings per volume, these can tune quite well what should appear on the; screen. However, there are situations when users want to see a volume; branch displayed down to the maximum depth, keeping at the same time a; limitation or even suppressing others. In order to accomplish that, one; should use the volume attribute: `Visible daughters`. By default, all; daughters of all volumes are displayed if there is no limitation related; with their level depth with respect to the top drawn volume. ### Ray Tracing. Ray tracing is a quite known drawing technique based on tracking rays; from the eye position through all pixels of a view port device. The; pixel color is derived from the properties of the first crossed surface,; according some illumination model and material optical properties. While; there are currently existing quite sophisticated ray tracing models,; **`TGeo`** is currently using a very simple approach where the light; source is matching the eye position (no shadows or back-tracing of the; reflected ray)",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md:142619,tune,tune,142619,documentation/users-guide/Geometry.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md,1,['tune'],['tune']
Performance,"ume you have an X86-64 machine. To measure the latency of a single; instruction, run:. .. code-block:: bash. $ llvm-exegesis --mode=latency --opcode-name=ADD64rr. Measuring the uop decomposition or inverse throughput of an instruction works similarly:. .. code-block:: bash. $ llvm-exegesis --mode=uops --opcode-name=ADD64rr; $ llvm-exegesis --mode=inverse_throughput --opcode-name=ADD64rr. The output is a YAML document (the default is to write to stdout, but you can; redirect the output to a file using `--benchmarks-file`):. .. code-block:: none. ---; key:; opcode_name: ADD64rr; mode: latency; config: ''; cpu_name: haswell; llvm_triple: x86_64-unknown-linux-gnu; num_repetitions: 10000; measurements:; - { key: latency, value: 1.0058, debug_string: '' }; error: ''; info: 'explicit self cycles, selecting one aliasing configuration.; Snippet:; ADD64rr R8, R8, R10; '; ... To measure the latency of all instructions for the host architecture, run:. .. code-block:: bash. $ llvm-exegesis --mode=latency --opcode-index=-1. EXAMPLE 2: benchmarking a custom code snippet; ---------------------------------------------. To measure the latency/uops of a custom piece of code, you can specify the; `snippets-file` option (`-` reads from standard input). .. code-block:: bash. $ echo ""vzeroupper"" | llvm-exegesis --mode=uops --snippets-file=-. Real-life code snippets typically depend on registers or memory.; :program:`llvm-exegesis` checks the liveliness of registers (i.e. any register; use has a corresponding def or is a ""live in""). If your code depends on the; value of some registers, you need to use snippet annotations to ensure setup; is performed properly. For example, the following code snippet depends on the values of XMM1 (which; will be set by the tool) and the memory buffer passed in RDI (live in). .. code-block:: none. # LLVM-EXEGESIS-LIVEIN RDI; # LLVM-EXEGESIS-DEFREG XMM1 42; vmulps	(%rdi), %xmm1, %xmm2; vhaddps	%xmm2, %xmm2, %xmm3; addq $0x10, %rdi. Example 3: benchmarking wit",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-exegesis.rst:5793,latency,latency,5793,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-exegesis.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-exegesis.rst,1,['latency'],['latency']
Performance,"ument only accepts constants. Semantics:; """""""""""""""""""". Using this intrinsic on a coroutine that does not have a coroutine promise; leads to undefined behavior. It is possible to read and modify coroutine; promise of the coroutine which is currently executing. The coroutine author and; a coroutine user are responsible to makes sure there is no data races. Example:; """""""""""""""". .. code-block:: llvm. define ptr @f(i32 %n) {; entry:; %promise = alloca i32; ; the second argument to coro.id points to the coroutine promise.; %id = call token @llvm.coro.id(i32 0, ptr %promise, ptr null, ptr null); ...; %hdl = call noalias ptr @llvm.coro.begin(token %id, ptr %alloc); ...; store i32 42, ptr %promise ; store something into the promise; ...; ret ptr %hdl; }. define i32 @main() {; entry:; %hdl = call ptr @f(i32 4) ; starts the coroutine and returns its handle; %promise.addr = call ptr @llvm.coro.promise(ptr %hdl, i32 4, i1 false); %val = load i32, ptr %promise.addr ; load a value from the promise; call void @print(i32 %val); call void @llvm.coro.destroy(ptr %hdl); ret i32 0; }. .. _coroutine intrinsics:. Coroutine Structure Intrinsics; ------------------------------; Intrinsics described in this section are used within a coroutine to describe; the coroutine structure. They should not be used outside of a coroutine. .. _coro.size:. 'llvm.coro.size' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; ::. declare i32 @llvm.coro.size.i32(); declare i64 @llvm.coro.size.i64(). Overview:; """""""""""""""""". The '``llvm.coro.size``' intrinsic returns the number of bytes; required to store a `coroutine frame`_. This is only supported for; switched-resume coroutines. Arguments:; """""""""""""""""""". None. Semantics:; """""""""""""""""""". The `coro.size` intrinsic is lowered to a constant representing the size of; the coroutine frame. .. _coro.align:. 'llvm.coro.align' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; ::. declare i32 @llvm.coro.align.i32(); declare i64 @llvm.coro.align.i64(). Overview:; """""""""""""""""". T",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Coroutines.rst:30983,load,load,30983,interpreter/llvm-project/llvm/docs/Coroutines.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Coroutines.rst,1,['load'],['load']
Performance,"unction writes to a readonly pointer argument, the behavior is; undefined. ``writeonly``; This attribute indicates that the function may write to, but does not read; through this pointer argument (even though it may read from the memory that; the pointer points to). If a function reads from a writeonly pointer argument, the behavior is; undefined. ``writable``; This attribute is only meaningful in conjunction with ``dereferenceable(N)``; or another attribute that implies the first ``N`` bytes of the pointer; argument are dereferenceable. In that case, the attribute indicates that the first ``N`` bytes will be; (non-atomically) loaded and stored back on entry to the function. This implies that it's possible to introduce spurious stores on entry to; the function without introducing traps or data races. This does not; necessarily hold throughout the whole function, as the pointer may escape; to a different thread during the execution of the function. See also the; :ref:`atomic optimization guide <Optimization outside atomic>`. The ""other attributes"" that imply dereferenceability are; ``dereferenceable_or_null`` (if the pointer is non-null) and the; ``sret``, ``byval``, ``byref``, ``inalloca``, ``preallocated`` family of; attributes. Note that not all of these combinations are useful, e.g.; ``byval`` arguments are known to be writable even without this attribute. The ``writable`` attribute cannot be combined with ``readnone``,; ``readonly`` or a ``memory`` attribute that does not contain; ``argmem: write``. ``dead_on_unwind``; At a high level, this attribute indicates that the pointer argument is dead; if the call unwinds, in the sense that the caller will not depend on the; contents of the memory. Stores that would only be visible on the unwind; path can be elided. More precisely, the behavior is as-if any memory written through the; pointer during the execution of the function is overwritten with a poison; value on unwind. This includes memory written by the implicit ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:70503,optimiz,optimization,70503,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['optimiz'],['optimization']
Performance,"und are the same, and as this is the recommended setting, the ""bug"" ; had very litte impact. However in order to be a ""correct"" Fisher discriminant, ; the correct calculation has now been adopted. Fisher and LD are the same, ONCE; the events are weighted such that signal and background have the same weight. ; Hence, the LD classifier still gives exactly the same result as the ""old"" Fisher; implementation, while the corrected Fisher implementation allows to ""play"" with; different event weights to perhaps find better discrimination power in certain; regions of the ROC curve. ; 2) BDT. a) Changes to some tuning options . nEventsMin --> MinNodeSize; UseNTrainEvents --> BaggedSampleFraction. have been replaced by options that are now given in terms of the relative; size of the training sample rather than in absulut numbers of events. This; is in order to facilitate the parameter tuning on different sample sizes; (i.e when starting on a smaller data sample to tune the parameter in order; to speed up the training); Furthermore, this option here has been changed name. GradBaggingFraction --> BaggedSampleFraction. in an attempt to consolidate and avoid idential duplicate code; ; The option UseWeightedTrees has been removed and set to ""true"", as was default; anyway, as a measure of further consolidation. Removed the option NNodesMax --> This should be replaced by specifying MaxDepth; instead (limiting the maximum tree depth also limits the number of possible nodes!). b) Added a trial version of a new ""cost sensitive"" boosting algorithem according to; Wei Fan and Salvatore J. Stolfo, {\em AdaCost: misclassification cost-sensitive boosting}, Proceedings of the 16th International conference on machine learning (ICML 1999)}. With the currently; chosen DEFAULT settings (all costs equal and set to ""one""), it is equivalent to the ""real-AdaBoost"" (i.e. using the option !UseYesNoLeaf (which uses the leave node purity rather than a signal or background attribute in the leaf node of eac",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/tmva/UsersGuide/ReleaseNotes4.2.0.txt:1402,tune,tune,1402,documentation/tmva/UsersGuide/ReleaseNotes4.2.0.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/tmva/UsersGuide/ReleaseNotes4.2.0.txt,1,['tune'],['tune']
Performance,"und.dynamic"" because if the value of 'x' is +0 then; 'x-0' should evaluate to '-0' when rounding downward. However, this; transformation is legal for all other rounding modes. For values other than ""round.dynamic"" optimization passes may assume that the; actual runtime rounding mode (as defined in a target-specific manner) matches; the specified rounding mode, but this is not guaranteed. Using a specific; non-dynamic rounding mode which does not match the actual rounding mode at; runtime results in undefined behavior. The exception behavior argument is a metadata string describing the floating; point exception semantics that required for the intrinsic. This argument; must be one of the following strings:. ::. ""fpexcept.ignore""; ""fpexcept.maytrap""; ""fpexcept.strict"". If this argument is ""fpexcept.ignore"" optimization passes may assume that the; exception status flags will not be read and that floating-point exceptions will; be masked. This allows transformations to be performed that may change the; exception semantics of the original code. For example, FP operations may be; speculatively executed in this case whereas they must not be for either of the; other possible values of this argument. If the exception behavior argument is ""fpexcept.maytrap"" optimization passes; must avoid transformations that may raise exceptions that would not have been; raised by the original code (such as speculatively executing FP operations), but; passes are not required to preserve all exceptions that are implied by the; original code. For example, exceptions may be potentially hidden by constant; folding. If the exception behavior argument is ""fpexcept.strict"" all transformations must; strictly preserve the floating-point exception semantics of the original code.; Any FP exception that would have been raised by the original code must be raised; by the transformed code, and the transformed code must not raise any FP; exceptions that would not have been raised by the original code. This is",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:870241,perform,performed,870241,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performed']
Performance,"undamentals and; finally work up to building a complete, working program ? Let's skip all; that. In this guide, we will describe macros executed by the ROOT C++; interpreter Cling. It is relatively easy to compile a macro, either as a pre-compiled; library to load into ROOT, or as a stand-alone application, by adding; some include statements for header file or some ""dressing code"" to any; macro. ## General Remarks on ROOT macros ##. If you have a number of lines which you were able to execute at the ROOT; prompt, they can be turned into a ROOT macro by giving them a name which; corresponds to the file name without extension. The general structure; for a macro stored in file `MacroName.C` is. ``` {.cpp}; void MacroName() {; < ...; your lines of C++ code; ... >; }; ```. The macro is executed by typing. ``` {.cpp}; > root MacroName.C; ```. at the system prompt, or executed using `.x`. ``` {.cpp}; > root; root [0] .x MacroName.C; ```. at the ROOT prompt. or it can be loaded into a ROOT session and then; be executed by typing. ``` {.cpp}; root [0].L MacroName.C; root [1] MacroName();; ```. at the ROOT prompt. Note that more than one macro can be loaded this; way, as each macro has a unique name in the ROOT name space. A small set; of options can help making your plot nicer. ``` {.cpp}; gROOT->SetStyle(""Plain""); // set plain TStyle; gStyle->SetOptStat(111111); // draw statistics on plots,; // (0) for no output; gStyle->SetOptFit(1111); // draw fit results on plot,; // (0) for no ouput; gStyle->SetPalette(57); // set color map; gStyle->SetOptTitle(0); // suppress title box; ...; ```. Next, you should create a canvas for graphical output, with size,; subdivisions and format suitable to your needs, see documentation of; class `TCanvas`:. ``` {.cpp}; TCanvas c1(""c1"",""<Title>"",0,0,400,300); // create a canvas, specify position and size in pixels; c1.Divide(2,2); //set subdivisions, called pads; c1.cd(1); //change to pad 1 of canvas c1; ```. These parts of a well-written macro a",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/primer/your_first_ROOT_macro.md:1052,load,loaded,1052,documentation/primer/your_first_ROOT_macro.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/primer/your_first_ROOT_macro.md,1,['load'],['loaded']
Performance,"unded up to 1 or down to 0.5; %res = call i4 @llvm.sdiv.fix.sat.i4(i4 3, i4 4, i32 1) ; %res = 2 (or 1) (1.5 / 2 = 0.75). ; Saturation; %res = call i4 @llvm.sdiv.fix.sat.i4(i4 -8, i4 -1, i32 0) ; %res = 7 (-8 / -1 = 8 => 7); %res = call i4 @llvm.sdiv.fix.sat.i4(i4 4, i4 2, i32 2) ; %res = 7 (1 / 0.5 = 2 => 1.75); %res = call i4 @llvm.sdiv.fix.sat.i4(i4 -4, i4 1, i32 2) ; %res = -8 (-1 / 0.25 = -4 => -2). '``llvm.udiv.fix.sat.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax; """""""""""""". This is an overloaded intrinsic. You can use ``llvm.udiv.fix.sat``; on any integer bit width or vectors of integers. ::. declare i16 @llvm.udiv.fix.sat.i16(i16 %a, i16 %b, i32 %scale); declare i32 @llvm.udiv.fix.sat.i32(i32 %a, i32 %b, i32 %scale); declare i64 @llvm.udiv.fix.sat.i64(i64 %a, i64 %b, i32 %scale); declare <4 x i32> @llvm.udiv.fix.sat.v4i32(<4 x i32> %a, <4 x i32> %b, i32 %scale). Overview; """""""""""""""""". The '``llvm.udiv.fix.sat``' family of intrinsic functions perform unsigned; fixed point saturating division on 2 arguments of the same scale. Arguments; """""""""""""""""""". The arguments (%a and %b) and the result may be of integer types of any bit; width, but they must have the same bit width. ``%a`` and ``%b`` are the two; values that will undergo unsigned fixed point division. The argument; ``%scale`` represents the scale of both operands, and must be a constant; integer. Semantics:; """""""""""""""""""". This operation performs fixed point division on the 2 arguments of a; specified scale. The result will also be returned in the same scale specified; in the third argument. If the result value cannot be precisely represented in the given scale, the; value is rounded up or down to the closest representable value. The rounding; direction is unspecified. The maximum value this operation can clamp to is the largest unsigned value; representable by the bit width of the first 2 arguments. The minimum value is the; smallest unsigned value representable by this bit width (zero). It is un",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:636786,perform,perform,636786,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['perform']
Performance,"unning programs in; an isolated and reproducible environment, especially to maintain releases for; software deployed to large distributed fleets.; It uses linux kernel namespaces and cgroups to provide a lightweight isolation; inside currently running linux kernel.; A single active instance of dockerized environment is called a *docker; container*.; A snapshot of a docker container filesystem is called a *docker image*.; One can start a container from a prebuilt docker image. Docker images are built from a so-called *Dockerfile*, a source file written in; a specialized language that defines instructions to be used when build; the docker image (see `official; documentation <https://docs.docker.com/engine/reference/builder/>`_ for more; details). A minimal Dockerfile typically contains a base image and a number; of RUN commands that have to be executed to build the image. When building a new; image, docker will first download your base image, mount its filesystem as; read-only and then add a writable overlay on top of it to keep track of all; filesystem modifications, performed while building your image. When the build; process is finished, a diff between your image's final filesystem state and the; base image's filesystem is stored in the resulting image. Overview; ========; The ``llvm/utils/docker`` folder contains Dockerfiles and simple bash scripts to; serve as a basis for anyone who wants to create their own Docker image with; LLVM components, compiled from sources. The sources are checked out from the; upstream git repository when building the image. The resulting image contains only the requested LLVM components and a few extra; packages to make the image minimally useful for C++ development, e.g. libstdc++; and binutils. The interface to run the build is ``build_docker_image.sh`` script. It accepts a; list of LLVM repositories to checkout and arguments for CMake invocation. If you want to write your own docker image, start with an ``example/`` subfolder.; It pr",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Docker.rst:2212,perform,performed,2212,interpreter/llvm-project/llvm/docs/Docker.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Docker.rst,1,['perform'],['performed']
Performance,"unt can be specified explicitly with ``unroll_count(_value_)`` where; _value_ is a positive integer. If this value is greater than the trip count the; loop will be fully unrolled. Otherwise the loop is partially unrolled subject; to the same code size limit as with ``unroll(enable)``. .. code-block:: c++. #pragma clang loop unroll_count(8); for(...) {; ...; }. Unrolling of a loop can be prevented by specifying ``unroll(disable)``. Loop unroll parameters can be controlled by options; `-mllvm -unroll-count=n` and `-mllvm -pragma-unroll-threshold=n`. Loop Distribution; -----------------. Loop Distribution allows splitting a loop into multiple loops. This is; beneficial for example when the entire loop cannot be vectorized but some of the; resulting loops can. If ``distribute(enable))`` is specified and the loop has memory dependencies; that inhibit vectorization, the compiler will attempt to isolate the offending; operations into a new loop. This optimization is not enabled by default, only; loops marked with the pragma are considered. .. code-block:: c++. #pragma clang loop distribute(enable); for (i = 0; i < N; ++i) {; S1: A[i + 1] = A[i] + B[i];; S2: C[i] = D[i] * E[i];; }. This loop will be split into two loops between statements S1 and S2. The; second loop containing S2 will be vectorized. Loop Distribution is currently not enabled by default in the optimizer because; it can hurt performance in some cases. For example, instruction-level; parallelism could be reduced by sequentializing the execution of the; statements S1 and S2 above. If Loop Distribution is turned on globally with; ``-mllvm -enable-loop-distribution``, specifying ``distribute(disable)`` can; be used the disable it on a per-loop basis. Additional Information; ----------------------. For convenience multiple loop hints can be specified on a single line. .. code-block:: c++. #pragma clang loop vectorize_width(4) interleave_count(8); for(...) {; ...; }. If an optimization cannot be applied any hints t",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst:168216,optimiz,optimization,168216,interpreter/llvm-project/clang/docs/LanguageExtensions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst,1,['optimiz'],['optimization']
Performance,"unted_by`` in the example below) must be updated side by side within the; same basic block and without side effect in between. .. code-block:: c. typedef struct {; int *__counted_by(count) buf; size_t count;; } sized_buf_t;. void alloc_buf(sized_buf_t *sbuf, sized_t nelems) {; sbuf->buf = (int *)malloc(sizeof(int) * nelems);; sbuf->count = nelems;; }. To implement this rule, the compiler requires a linear representation of; statements to understand the ordering and the adjacency between the two or more; assignments. The Clang CFG is used to implement this analysis as Clang CFG; provides a linear view of statements within each ``CFGBlock`` (Clang; ``CFGBlock`` represents a single basic block in a source-level CFG). Bounds check optimizations; ==========================. In ``-fbounds-safety``, the Clang frontend emits run-time checks for every; memory dereference if the type system or analyses in the frontend couldn’t; verify its bounds safety. The implementation relies on LLVM optimizations to; remove redundant run-time checks. Using this optimization strategy, if the; original source code already has bounds checks, the fewer additional checks; ``-fbounds-safety`` will introduce. The LLVM ``ConstraintElimination`` pass is; design to remove provable redundant checks (please check Florian Hahn’s; presentation in 2021 LLVM Dev Meeting and the implementation to learn more). In; the following example, ``-fbounds-safety`` implicitly adds the redundant bounds; checks that the optimizer can remove:. .. code-block:: c. void fill_array_with_indices(int *__counted_by(count) p, size_t count) {; for (size_t i = 0; i < count; ++i) {; // implicit bounds checks:; // if (p + i < p || p + i + 1 > p + count) trap();; p[i] = i;; }; }. ``ConstraintElimination`` collects the following facts and determines if the; bounds checks can be safely removed:. * Inside the for-loop, ``0 <= i < count``, hence ``1 <= i + 1 <= count``.; * Pointer arithmetic ``p + count`` in the if-condition doesn’t w",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/BoundsSafetyImplPlans.rst:6225,optimiz,optimizations,6225,interpreter/llvm-project/clang/docs/BoundsSafetyImplPlans.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/BoundsSafetyImplPlans.rst,1,['optimiz'],['optimizations']
Performance,"untimes; (e.g. Objective C and Swift) and other JIT specific runtime code. This should; be built in a similar manner to compiler-rt (possibly even as part of it). 2. **Remote jit_dlopen / jit_dlclose**. To more fully mimic the environment that static programs operate in we would; like JIT'd code to be able to ""dlopen"" and ""dlclose"" JITDylibs, running all of; their initializers/deinitializers on the current thread. This would require; support from the runtime library described above. 3. **Debugging support**. ORC currently supports the GDBRegistrationListener API when using RuntimeDyld; as the underlying JIT linker. We will need a new solution for JITLink based; platforms. Further Future Work; -------------------. 1. **Speculative Compilation**. ORC's support for concurrent compilation allows us to easily enable; *speculative* JIT compilation: compilation of code that is not needed yet,; but which we have reason to believe will be needed in the future. This can be; used to hide compile latency and improve JIT throughput. A proof-of-concept; example of speculative compilation with ORC has already been developed (see; ``llvm/examples/SpeculativeJIT``). Future work on this is likely to focus on; re-using and improving existing profiling support (currently used by PGO) to; feed speculation decisions, as well as built-in tools to simplify use of; speculative compilation. .. [1] Formats/architectures vary in terms of supported features. MachO and; ELF tend to have better support than COFF. Patches very welcome!. .. [2] The ``LazyEmittingLayer``, ``RemoteObjectClientLayer`` and; ``RemoteObjectServerLayer`` do not have counterparts in the new; system. In the case of ``LazyEmittingLayer`` it was simply no longer; needed: in ORCv2, deferring compilation until symbols are looked up is; the default. The removal of ``RemoteObjectClientLayer`` and; ``RemoteObjectServerLayer`` means that JIT stacks can no longer be split; across processes, however this functionality appears not to ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:37270,latency,latency,37270,interpreter/llvm-project/llvm/docs/ORCv2.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst,2,"['latency', 'throughput']","['latency', 'throughput']"
Performance,"unwind readnone {; entry:; %sext = shl i32 %a, 24 ; <i32> [#uses=1]; %conv1 = ashr i32 %sext, 24 ; <i32> [#uses=1]; %sext6 = shl i32 %b, 24 ; <i32> [#uses=1]; %conv4 = ashr i32 %sext6, 24 ; <i32> [#uses=1]; %cmp = icmp eq i32 %conv1, %conv4 ; <i1> [#uses=1]; %conv5 = zext i1 %cmp to i32 ; <i32> [#uses=1]; ret i32 %conv5; }. And the following x86 code:; 	movsbl	%sil, %eax; 	movsbl	%dil, %ecx; 	cmpl	%eax, %ecx; 	sete	%al; 	movzbl	%al, %eax; 	ret. It should be possible to eliminate the sign extensions. //===---------------------------------------------------------------------===//. LLVM misses a load+store narrowing opportunity in this code:. %struct.bf = type { i64, i16, i16, i32 }. @bfi = external global %struct.bf* ; <%struct.bf**> [#uses=2]. define void @t1() nounwind ssp {; entry:; %0 = load %struct.bf** @bfi, align 8 ; <%struct.bf*> [#uses=1]; %1 = getelementptr %struct.bf* %0, i64 0, i32 1 ; <i16*> [#uses=1]; %2 = bitcast i16* %1 to i32* ; <i32*> [#uses=2]; %3 = load i32* %2, align 1 ; <i32> [#uses=1]; %4 = and i32 %3, -65537 ; <i32> [#uses=1]; store i32 %4, i32* %2, align 1; %5 = load %struct.bf** @bfi, align 8 ; <%struct.bf*> [#uses=1]; %6 = getelementptr %struct.bf* %5, i64 0, i32 1 ; <i16*> [#uses=1]; %7 = bitcast i16* %6 to i32* ; <i32*> [#uses=2]; %8 = load i32* %7, align 1 ; <i32> [#uses=1]; %9 = and i32 %8, -131073 ; <i32> [#uses=1]; store i32 %9, i32* %7, align 1; ret void; }. LLVM currently emits this:. movq bfi(%rip), %rax; andl $-65537, 8(%rax); movq bfi(%rip), %rax; andl $-131073, 8(%rax); ret. It could narrow the loads and stores to emit this:. movq bfi(%rip), %rax; andb $-2, 10(%rax); movq bfi(%rip), %rax; andb $-3, 10(%rax); ret. The trouble is that there is a TokenFactor between the store and the; load, making it non-trivial to determine if there's anything between; the load and the store which would prohibit narrowing. //===---------------------------------------------------------------------===//. This code:; void foo(unsigned x) {; if (x == 0)",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:37657,load,load,37657,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt,4,['load'],['load']
Performance,"up *none* 1. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0).; - However,; since LLVM; currently has no; address space on; the fence need to; conservatively; always generate; (see comment for; previous fence).; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/; load atomic/store atomic/; atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that all; memory operations; have; completed before; performing any; following global; memory operations.; - Ensures that the; preceding; local/generic load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; acquire-fence-paired-atomic); has completed; before following; global memory; operations. This; satisfies the; requirements of; acquire.; - Ensures that all; previous memory; operations have; completed before a; following; local/generic store; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; release-fence-paired-atomic).; This satisfies the; requirements of; release.; - Must happen before; the following; buffer_inv.; - Ensures that the; acquire-fence-paired; atomic has completed; before invalidating; the; cache. Therefore; any following; locations read must; be no older than; the value read by; the; acquire-fence-paired-atomic. 3. buffer_inv sc0=1. - If not TgSplit execution; mode, omit.; - Ensures that; following; loads will not see; stale data. fence acq_rel - agent *none* 1. buffer_wbl2 sc1=1. - If OpenCL and; address space is; local, omit.; - Must happen before; follo",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:325884,load,load,325884,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"up *none* 1. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0).; - However,; since LLVM; currently has no; address space on; the fence need to; conservatively; always generate; (see comment for; previous fence).; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/; load atomic/store atomic/; atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that all; memory operations; have; completed before; performing any; following global; memory operations.; - Ensures that the; preceding; local/generic load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; acquire-fence-paired-atomic); has completed; before following; global memory; operations. This; satisfies the; requirements of; acquire.; - Ensures that all; previous memory; operations have; completed before a; following; local/generic store; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; release-fence-paired-atomic).; This satisfies the; requirements of; release.; - Must happen before; the following; buffer_wbinvl1_vol.; - Ensures that the; acquire-fence-paired; atomic has completed; before invalidating; the; cache. Therefore; any following; locations read must; be no older than; the value read by; the; acquire-fence-paired-atomic. 2. buffer_wbinvl1_vol. - If not TgSplit execution; mode, omit.; - Ensures that; following; loads will not see; stale data. fence acq_rel - agent *none* 1. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - I",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:275174,load,load,275174,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"up - generic 1. flat_load glc=1. - If not TgSplit execution; mode, omit glc=1. 2. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL, omit lgkmcnt(0).; - Must happen before; the following; buffer_wbinvl1_vol and any; following global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local load; atomic value being; acquired. 3. buffer_wbinvl1_vol. - If not TgSplit execution; mode, omit.; - Ensures that; following; loads will not see; stale data. load atomic acquire - agent - global 1. buffer/global_load; glc=1; 2. s_waitcnt vmcnt(0). - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the load; has completed; before invalidating; the cache. 3. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale global data. load atomic acquire - system - global 1. buffer/global/flat_load; glc=1; 2. s_waitcnt vmcnt(0). - Must happen before; following buffer_invl2 and; buffer_wbinvl1_vol.; - Ensures the load; has completed; before invalidating; the cache. 3. buffer_invl2;; buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale L1 global data,; nor see stale L2 MTYPE; NC global data.; MTYPE RW and CC memory will; never be stale in L2 due to; the memory probes. load atomic acquire - agent - generic 1. flat_load glc=1; 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL omit; lgkmcnt(0).; - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the flat_load; has completed; before invalidating; the cache. 3. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. load ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:247142,load,load,247142,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"up to the; alignment value bytes can be safely loaded without trapping in the default; address space. Access of the high bytes can interfere with debugging tools, so; should not be accessed if the function has the ``sanitize_thread`` or; ``sanitize_address`` attributes. The alignment is only optional when parsing textual IR; for in-memory IR, it is; always present. An omitted ``align`` argument means that the operation has the; ABI alignment for the target. The optional ``!nontemporal`` metadata must reference a single; metadata name ``<nontemp_node>`` corresponding to a metadata node with one; ``i32`` entry of value 1. The existence of the ``!nontemporal``; metadata on the instruction tells the optimizer and code generator; that this load is not expected to be reused in the cache. The code; generator may select special instructions to save cache bandwidth, such; as the ``MOVNT`` instruction on x86. The optional ``!invariant.load`` metadata must reference a single; metadata name ``<empty_node>`` corresponding to a metadata node with no; entries. If a load instruction tagged with the ``!invariant.load``; metadata is executed, the memory location referenced by the load has; to contain the same value at all points in the program where the; memory location is dereferenceable; otherwise, the behavior is; undefined. The optional ``!invariant.group`` metadata must reference a single metadata name; ``<empty_node>`` corresponding to a metadata node with no entries.; See ``invariant.group`` metadata :ref:`invariant.group <md_invariant.group>`. The optional ``!nonnull`` metadata must reference a single; metadata name ``<empty_node>`` corresponding to a metadata node with no; entries. The existence of the ``!nonnull`` metadata on the; instruction tells the optimizer that the value loaded is known to; never be null. If the value is null at runtime, a poison value is returned; instead. This is analogous to the ``nonnull`` attribute on parameters and; return values. This metadata ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:415322,load,load,415322,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,"up/SPIRV-LLVM-Translator/#build-with-spirv-tools>`_. `The versioning; <https://github.com/KhronosGroup/SPIRV-LLVM-Translator/releases>`_ of; ``llvm-spirv`` is aligned with Clang major releases. The same applies to the; main development branch. It is therefore important to ensure the ``llvm-spirv``; version is in alignment with the Clang version. For troubleshooting purposes; ``llvm-spirv`` can be `tested in isolation; <https://github.com/KhronosGroup/SPIRV-LLVM-Translator#test-instructions>`_. Example usage for OpenCL kernel compilation:. .. code-block:: console. $ clang --target=spirv32 -c test.cl; $ clang --target=spirv64 -c test.cl. Both invocations of Clang will result in the generation of a SPIR-V binary file; `test.o` for 32 bit and 64 bit respectively. This file can be imported; by an OpenCL driver that support SPIR-V consumption or it can be compiled; further by offline SPIR-V consumer tools. Converting to SPIR-V produced with the optimization levels other than `-O0` is; currently available as an experimental feature and it is not guaranteed to work; in all cases. Clang also supports integrated generation of SPIR-V without use of ``llvm-spirv``; tool as an experimental feature when ``-fintegrated-objemitter`` flag is passed in; the command line. .. code-block:: console. $ clang --target=spirv32 -fintegrated-objemitter -c test.cl. Note that only very basic functionality is supported at this point and therefore; it is not suitable for arbitrary use cases. This feature is only enabled when clang; build is configured with ``-DLLVM_EXPERIMENTAL_TARGETS_TO_BUILD=SPIRV`` option. Linking is done using ``spirv-link`` from `the SPIRV-Tools project; <https://github.com/KhronosGroup/SPIRV-Tools#linker>`_. Similar to other external; linkers, Clang will expect ``spirv-link`` to be installed separately and to be; present in the ``PATH`` environment variable. Please refer to `the build and; installation instructions; <https://github.com/KhronosGroup/SPIRV-Tools#build>`_. .. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:167065,optimiz,optimization,167065,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,1,['optimiz'],['optimization']
Performance,"updated using the '``llvm.instrprof.mcdc.condbitmap.update``' intrinsic with; the true or false evaluation of each condition, uniquely identifies an executed; MC/DC test vector and is used as a bit index into the global test vector; bitmap. Arguments:; """""""""""""""""""". The first argument is a pointer to a global variable containing the; name of the entity being instrumented. This should generally be the; (mangled) function name for a set of counters. The second argument is a hash value that can be used by the consumer; of the profile data to detect changes to the instrumented source. The third argument is the number of bitmap bytes required by the function to; record the number of test vectors executed for each boolean expression. The fourth argument is the byte index into the global test vector bitmap; corresponding to the function. The fifth argument is the address of the condition bitmap, which contains a; value representing an executed MC/DC test vector. It is loaded and used as the; bit index of the test vector bitmap. Semantics:; """""""""""""""""""". This intrinsic represents the final operation of an MC/DC instrumentation; sequence and will cause the ``-instrprof`` pass to generate the code to; instrument an update of a function's global test vector bitmap to indicate that; a test vector has been executed. The global test vector bitmap can be consumed; by the ``llvm-profdata`` and ``llvm-cov`` tools. '``llvm.thread.pointer``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare ptr @llvm.thread.pointer(). Overview:; """""""""""""""""". The '``llvm.thread.pointer``' intrinsic returns the value of the thread; pointer. Semantics:; """""""""""""""""""". The '``llvm.thread.pointer``' intrinsic returns a pointer to the TLS area; for the current thread. The exact semantics of this value are target; specific: it may point to the start of TLS area, to the end, or somewhere; in the middle. Depending on the target, this intrinsic may read a register,; call a helper function, rea",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:536249,load,loaded,536249,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['loaded']
Performance,"uperimpose several objects; 9. Implement col2 and col3 draw options, using html5 canvas; 10. Support 'p' and 'p0' draw options for TH1 class. ## Development of version 3.0. ### November 2014; 1. Better font size and position in pave stats; 2. Resize/move of element only inside correspondent pad; 3. Adjust of frame size when Y-axis exceed pad limits; 4. Correct values in tooltip for THStack; 5. Exclude drawing of markers from TGraph outside visible range; 6. Drawing of canvas without TFrame object; 7. Many other small bug fixes and improvements, thanks to Maximilian Dietrich. ### October 2014; 1. Add ""shortcut icon""; 2. Add demo of online THttpServer - shell script copies data from; running httpserver.C macro on Apache webserver; 3. Evaluate 'monitoring' parameter for online server like:; <http://localhost:8080/?monitoring=1000>; Parameter defines how often displayed objects should be updated.; 4. Implement 'opt' and 'opts' URL parameters for main page.; 5. Show progress with scripts loading in the browser window; 6. When one appends ""+"" to the filename, its content read completely with first I/O operation.; 7. Implement JS custom streamer for TCanvas, restore aspect ratio when drawing; 8. Major redesign of drawing classes. Resize and update of TCanvas are implemented.; All major draw functions working with HTML element id as first argument.; 9. Extract 3D drawings into separate JSRoot3DPainter.js script; 10. Use newest three.min.js (r68) for 3D drawings, solves problem with Firefox.; 11. Introduce generic list of draw functions for all supported classes.; 12. Add possibility to 'expand' normal objects in the hierarchy browser.; For instance, this gives access to single elements of canvas,; when whole canvas cannot be drawn.; 13. Correct usage of colors map, provided with TCanvas.; 14. Introduce JSROOT.redraw() function which is capable to create or update object drawing.; 15. In main index.htm page browser can be disabled (nobrowser parameter) and; page can be used t",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/js/changes.md:70836,load,loading,70836,js/changes.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/js/changes.md,1,['load'],['loading']
Performance,"uplication. This will come at a; substantial performance penalty to the generated binary both because it reduces; optimization opportunity, and because dynamic linking requires resolving symbols; at process launch time, which can be very slow for C++ code. .. _shared_libs:. .. warning::; One very important note: Distributions should never be built using the; *BUILD_SHARED_LIBS* CMake option. That option exists for optimizing developer; workflow only. Due to design and implementation decisions, LLVM relies on; global data which can end up being duplicated across shared libraries; resulting in bugs. As such this is not a safe way to distribute LLVM or; LLVM-based tools. The simplest example of building a distribution with reasonable performance is; captured in the DistributionExample CMake cache file located at; clang/cmake/caches/DistributionExample.cmake. The following command will perform; and install the distribution build:. .. code-block:: console. $ cmake -G Ninja -C <path to clang>/cmake/caches/DistributionExample.cmake <path to LLVM source>; $ ninja stage2-distribution; $ ninja stage2-install-distribution. Difference between ``install`` and ``install-distribution``; -----------------------------------------------------------. One subtle but important thing to note is the difference between the ``install``; and ``install-distribution`` targets. The ``install`` target is expected to; install every part of LLVM that your build is configured to generate except the; LLVM testing tools. Alternatively the ``install-distribution`` target, which is; recommended for building distributions, only installs specific parts of LLVM as; specified at configuration time by *LLVM_DISTRIBUTION_COMPONENTS*. Additionally by default the ``install`` target will install the LLVM testing; tools as the public tools. This can be changed well by setting; *LLVM_INSTALL_TOOLCHAIN_ONLY* to ``On``. The LLVM tools are intended for; development and testing of LLVM, and should only be included in",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BuildingADistribution.rst:2821,cache,caches,2821,interpreter/llvm-project/llvm/docs/BuildingADistribution.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BuildingADistribution.rst,1,['cache'],['caches']
Performance,"upport atomic; scopes, then they will behave exactly as the standard GNU atomic builtins. Low-level ARM exclusive memory builtins; ---------------------------------------. Clang provides overloaded builtins giving direct access to the three key ARM; instructions for implementing atomic operations. .. code-block:: c. T __builtin_arm_ldrex(const volatile T *addr);; T __builtin_arm_ldaex(const volatile T *addr);; int __builtin_arm_strex(T val, volatile T *addr);; int __builtin_arm_stlex(T val, volatile T *addr);; void __builtin_arm_clrex(void);. The types ``T`` currently supported are:. * Integer types with width at most 64 bits (or 128 bits on AArch64).; * Floating-point types; * Pointer types. Note that the compiler does not guarantee it will not insert stores which clear; the exclusive monitor in between an ``ldrex`` type operation and its paired; ``strex``. In practice this is only usually a risk when the extra store is on; the same cache line as the variable being modified and Clang will only insert; stack stores on its own, so it is best not to use these operations on variables; with automatic storage duration. Also, loads and stores may be implicit in code written between the ``ldrex`` and; ``strex``. Clang will not necessarily mitigate the effects of these either, so; care should be exercised. For these reasons the higher level atomic primitives should be preferred where; possible. Non-temporal load/store builtins; --------------------------------. Clang provides overloaded builtins allowing generation of non-temporal memory; accesses. .. code-block:: c. T __builtin_nontemporal_load(T *addr);; void __builtin_nontemporal_store(T value, T *addr);. The types ``T`` currently supported are:. * Integer types.; * Floating-point types.; * Vector types. Note that the compiler does not guarantee that non-temporal loads or stores; will be used. C++ Coroutines support builtins; --------------------------------. .. warning::; This is a work in progress. Compatibility across ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst:144269,cache,cache,144269,interpreter/llvm-project/clang/docs/LanguageExtensions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst,1,['cache'],['cache']
Performance,"upported. Assembly Printer; ================. During the code emission stage, the code generator may utilize an LLVM pass to; produce assembly output. To do this, you want to implement the code for a; printer that converts LLVM IR to a GAS-format assembly language for your target; machine, using the following steps:. * Define all the assembly strings for your target, adding them to the; instructions defined in the ``XXXInstrInfo.td`` file. (See; :ref:`instruction-set`.) TableGen will produce an output file; (``XXXGenAsmWriter.inc``) with an implementation of the ``printInstruction``; method for the ``XXXAsmPrinter`` class. * Write ``XXXTargetAsmInfo.h``, which contains the bare-bones declaration of; the ``XXXTargetAsmInfo`` class (a subclass of ``TargetAsmInfo``). * Write ``XXXTargetAsmInfo.cpp``, which contains target-specific values for; ``TargetAsmInfo`` properties and sometimes new implementations for methods. * Write ``XXXAsmPrinter.cpp``, which implements the ``AsmPrinter`` class that; performs the LLVM-to-assembly conversion. The code in ``XXXTargetAsmInfo.h`` is usually a trivial declaration of the; ``XXXTargetAsmInfo`` class for use in ``XXXTargetAsmInfo.cpp``. Similarly,; ``XXXTargetAsmInfo.cpp`` usually has a few declarations of ``XXXTargetAsmInfo``; replacement values that override the default values in ``TargetAsmInfo.cpp``.; For example in ``SparcTargetAsmInfo.cpp``:. .. code-block:: c++. SparcTargetAsmInfo::SparcTargetAsmInfo(const SparcTargetMachine &TM) {; Data16bitsDirective = ""\t.half\t"";; Data32bitsDirective = ""\t.word\t"";; Data64bitsDirective = 0; // .xword is only supported by V9.; ZeroDirective = ""\t.skip\t"";; CommentString = ""!"";; ConstantPoolSection = ""\t.section \"".rodata\"",#alloc\n"";; }. The X86 assembly printer implementation (``X86TargetAsmInfo``) is an example; where the target specific ``TargetAsmInfo`` class uses an overridden methods:; ``ExpandInlineAsm``. A target-specific implementation of ``AsmPrinter`` is written in; ``XXXAsmPrin",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMBackend.rst:67519,perform,performs,67519,interpreter/llvm-project/llvm/docs/WritingAnLLVMBackend.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMBackend.rst,1,['perform'],['performs']
Performance,"ur expression's constructor properly computes the flags; for type dependence (i.e., the type your expression produces can change; from one instantiation to the next), value dependence (i.e., the constant; value your expression produces can change from one instantiation to the; next), instantiation dependence (i.e., a template parameter occurs; anywhere in your expression), and whether your expression contains a; parameter pack (for variadic templates). Often, computing these flags; just means combining the results from the various types and; subexpressions.; * Add ``TransformXXX`` and ``RebuildXXX`` functions to the ``TreeTransform``; class template in ``Sema``. ``TransformXXX`` should (recursively); transform all of the subexpressions and types within your expression,; using ``getDerived().TransformYYY``. If all of the subexpressions and; types transform without error, it will then call the ``RebuildXXX``; function, which will in turn call ``getSema().BuildXXX`` to perform; semantic analysis and build your expression.; * To test template instantiation, take those tests you wrote to make sure; that you were type checking with type-dependent expressions and dependent; types (from step #2) and instantiate those templates with various types,; some of which type-check and some that don't, and test the error messages; in each case. #. There are some ""extras"" that make other features work better. It's worth; handling these extras to give your expression complete integration into; Clang:. * Add code completion support for your expression in; ``SemaCodeComplete.cpp``.; * If your expression has types in it, or has any ""interesting"" features; other than subexpressions, extend libclang's ``CursorVisitor`` to provide; proper visitation for your expression, enabling various IDE features such; as syntax highlighting, cross-referencing, and so on. The; ``c-index-test`` helper program can be used to test these features. Testing; -------; All functional changes to Clang should come w",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/InternalsManual.rst:153422,perform,perform,153422,interpreter/llvm-project/clang/docs/InternalsManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/InternalsManual.rst,1,['perform'],['perform']
Performance,"urceTracker::remove``.; See the subsection `How to remove code`_. For code examples and suggestions of how to use the ORCv2 APIs, please see; the section `How-tos`_. How-tos; =======. How to manage symbol strings; ----------------------------. Symbol strings in ORC are uniqued to improve lookup performance, reduce memory; overhead, and allow symbol names to function as efficient keys. To get the; unique ``SymbolStringPtr`` for a string value, call the; ``ExecutionSession::intern`` method:. .. code-block:: c++. ExecutionSession ES;; /// ...; auto MainSymbolName = ES.intern(""main"");. If you wish to perform lookup using the C/IR name of a symbol you will also; need to apply the platform linker-mangling before interning the string. On; Linux this mangling is a no-op, but on other platforms it usually involves; adding a prefix to the string (e.g. '_' on Darwin). The mangling scheme is; based on the DataLayout for the target. Given a DataLayout and an; ExecutionSession, you can create a MangleAndInterner function object that; will perform both jobs for you:. .. code-block:: c++. ExecutionSession ES;; const DataLayout &DL = ...;; MangleAndInterner Mangle(ES, DL);. // ... // Portable IR-symbol-name lookup:; auto Sym = ES.lookup({&MainJD}, Mangle(""main""));. How to create JITDylibs and set up linkage relationships; --------------------------------------------------------. In ORC, all symbol definitions reside in JITDylibs. JITDylibs are created by; calling the ``ExecutionSession::createJITDylib`` method with a unique name:. .. code-block:: c++. ExecutionSession ES;; auto &JD = ES.createJITDylib(""libFoo.dylib"");. The JITDylib is owned by the ``ExecutionEngine`` instance and will be freed; when it is destroyed. How to remove code; ------------------. To remove an individual module from a JITDylib it must first be added using an; explicit ``ResourceTracker``. The module can then be removed by calling; ``ResourceTracker::remove``:. .. code-block:: c++. auto &JD = ... ;; auto M = .",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:22831,perform,perform,22831,interpreter/llvm-project/llvm/docs/ORCv2.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst,1,['perform'],['perform']
Performance,"ure for caching p.d.f and functions. The infrastructure that exists for caching p.d.f.s, i.e. p.d.f that precalculate their value; for all observable values at one and cache those in a histogram that is returned as p.d.f shape; (with optional interpolation), has been expanded. This infrastructure comprises RooAbsCached; the base class for all caching p.d.fs, RooAbsSelfCachedPdf a base class for end-user; caching p.d.f implementations that simply cache the result of evaluate() and RooCachedPdf; that can wrap and cache any input p.d.f specified in its constructor. . By default a p.d.f is sampled and cached in all observables in any; given use context, with no need to specify what those are in advance.; The internal code has also been changed such that all cache; histograms now store pre-normalized p.d.f, which is more efficient; than 'raw' p.d.f histograms that are explicitly post-normalized; through integration. Multiple different use cases (e.g. definitions; of what are observables vs parameters) can be cached; simultaneously. Now it is also possible to specify that p.d.f.s; should be sampled and cached in one or more parameter dimensions; in addition to the automatically determined set of observables.; as well. Also a complete new line of classes with similar functionality has been added inheriting from RooAbsReal.; These are RooAbsCachedReal,RooAbsSelfCachedReal and RooCachedReal. A newly; added class RooHistFunc presents these shapes and is capable of handling negative entries. New PDF error handling structure. New infrastructure has been put into place to propagate and process p.d.f evaluation errors during fitting.; Previously evaluation errors were marked with a zero p.d.f value and propagated as a special condition; in RooAddPdf, RooProdPdf etc to result in a zero top-level p.d.f value that was caught by the RooFit minuit; interface as a special condition. Summary information on the value of the parameters and the observables; was printed for the first 10 occu",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v520/index.html:14493,cache,cached,14493,roofit/doc/v520/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v520/index.html,2,['cache'],['cached']
Performance,"ure.png; :align: center; :alt: Driver Architecture Diagram. Driver Stages; -------------. The driver functionality is conceptually divided into five stages:. #. **Parse: Option Parsing**. The command line argument strings are decomposed into arguments; (``Arg`` instances). The driver expects to understand all available; options, although there is some facility for just passing certain; classes of options through (like ``-Wl,``). Each argument corresponds to exactly one abstract ``Option``; definition, which describes how the option is parsed along with some; additional metadata. The Arg instances themselves are lightweight and; merely contain enough information for clients to determine which; option they correspond to and their values (if they have additional; parameters). For example, a command line like ""-Ifoo -I foo"" would parse to two; Arg instances (a JoinedArg and a SeparateArg instance), but each; would refer to the same Option. Options are lazily created in order to avoid populating all Option; classes when the driver is loaded. Most of the driver code only needs; to deal with options by their unique ID (e.g., ``options::OPT_I``),. Arg instances themselves do not generally store the values of; parameters. In many cases, this would simply result in creating; unnecessary string copies. Instead, Arg instances are always embedded; inside an ArgList structure, which contains the original vector of; argument strings. Each Arg itself only needs to contain an index into; this vector instead of storing its values directly. The clang driver can dump the results of this stage using the; ``-###`` flag (which must precede any actual command; line arguments). For example:. .. code-block:: console. $ clang -### -Xarch_i386 -fomit-frame-pointer -Wa,-fast -Ifoo -I foo t.c; Option 0 - Name: ""-Xarch_"", Values: {""i386"", ""-fomit-frame-pointer""}; Option 1 - Name: ""-Wa,"", Values: {""-fast""}; Option 2 - Name: ""-I"", Values: {""foo""}; Option 3 - Name: ""-I"", Values: {""foo""}; Option 4 - N",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/DriverInternals.rst:4766,load,loaded,4766,interpreter/llvm-project/clang/docs/DriverInternals.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/DriverInternals.rst,1,['load'],['loaded']
Performance,"ures that all; previous memory; operations have; completed before a; following; global/local/generic; store; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; release-fence-paired-atomic).; This satisfies the; requirements of; release. 2. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. This; satisfies the; requirements of; acquire. **Sequential Consistent Atomic**; ------------------------------------------------------------------------------------; load atomic seq_cst - singlethread - global *Same as corresponding; - wavefront - local load atomic acquire,; - generic except must generate; all instructions even; for OpenCL.*; load atomic seq_cst - workgroup - global 1. s_waitcnt lgkmcnt(0); - generic. - Must; happen after; preceding; local/generic load; atomic/store; atomic/atomicrmw; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; lgkmcnt(0) and so do; not need to be; considered.); - Ensures any; preceding; sequential; consistent local; memory instructions; have completed; before executing; this sequentially; consistent; instruction. This; prevents reordering; a seq_cst store; followed by a; seq_cst load. (Note; that seq_cst is; stronger than; acquire/release as; the reordering of; load acquire; followed by a store; release is; prevented by the; s_waitcnt of; the release, but; there is nothing; preventing a store; release followed by; load acquire from; completing out of; order. The s_waitcnt; could be placed after; seq_store or before; the seq_load. We; choose the load to; make the s_waitcnt be; as late as possible; so that the store; may have already; completed.). 2. *Following; instructions same as; corresponding load; atomic acquire,; except must generate; all instructions eve",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:230554,load,load,230554,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"ures that the; acquire-fence-paired; atomic has completed; before invalidating; the; cache. Therefore; any following; locations read must; be no older than; the value read by; the; acquire-fence-paired-atomic. 2. buffer_wbinvl1_vol. - If not TgSplit execution; mode, omit.; - Ensures that; following; loads will not see; stale data. fence acq_rel - agent *none* 1. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate; (see comment for; previous fence).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; buffer_wbinvl1_vol.; - Ensures that the; preceding; global/local/generic; load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; acquire-fence-paired-atomic); has completed; before invalidating; the cache. This; satisfies the; requirements of; acquire.; - Ensures that all; previous memory; operations have; completed before a; following; global/local/generic; store; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; release-fence-paired-atomic).; This satisfies the; requirements of; release. 2. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. This; satisfies the; requirements of; acquire. fence acq_rel - syste",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:276823,load,load,276823,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"ures that the; preceding; global/local/generic; load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; acquire-fence-paired-atomic); has completed; before invalidating; the cache. This; satisfies the; requirements of; acquire.; - Ensures that all; previous memory; operations have; completed before a; following; global/local/generic; store; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; release-fence-paired-atomic).; This satisfies the; requirements of; release. 2. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. This; satisfies the; requirements of; acquire. **Sequential Consistent Atomic**; ------------------------------------------------------------------------------------; load atomic seq_cst - singlethread - global *Same as corresponding; - wavefront - local load atomic acquire,; - generic except must generate; all instructions even; for OpenCL.*; load atomic seq_cst - workgroup - global 1. s_waitcnt lgkmcnt(0); - generic. - Must; happen after; preceding; local/generic load; atomic/store; atomic/atomicrmw; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; lgkmcnt(0) and so do; not need to be; considered.); - Ensures any; preceding; sequential; consistent local; memory instructions; have completed; before executing; this sequentially; consistent; instruction. This; prevents reordering; a seq_cst store; followed by a; seq_cst load. (Note; that seq_cst is; stronger than; acquire/release as; the reordering of; load acquire; followed by a store; release is; prevented by the; s_waitcnt of; the release, but; there is nothing; preventing a store; release followed by; load acquire from; completing out of; order. Th",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:230251,load,load,230251,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"ures`. The code sequences used to implement the memory model for GFX10-GFX11 are defined in; table :ref:`amdgpu-amdhsa-memory-model-code-sequences-gfx10-gfx11-table`. .. table:: AMDHSA Memory Model Code Sequences GFX10-GFX11; :name: amdgpu-amdhsa-memory-model-code-sequences-gfx10-gfx11-table. ============ ============ ============== ========== ================================; LLVM Instr LLVM Memory LLVM Memory AMDGPU AMDGPU Machine Code; Ordering Sync Scope Address GFX10-GFX11; Space; ============ ============ ============== ========== ================================; **Non-Atomic**; ------------------------------------------------------------------------------------; load *none* *none* - global - !volatile & !nontemporal; - generic; - private 1. buffer/global/flat_load; - constant; - !volatile & nontemporal. 1. buffer/global/flat_load; slc=1 dlc=1. - If GFX10, omit dlc=1. - volatile. 1. buffer/global/flat_load; glc=1 dlc=1. 2. s_waitcnt vmcnt(0). - Must happen before; any following volatile; global/generic; load/store.; - Ensures that; volatile; operations to; different; addresses will not; be reordered by; hardware. load *none* *none* - local 1. ds_load; store *none* *none* - global - !volatile & !nontemporal; - generic; - private 1. buffer/global/flat_store; - constant; - !volatile & nontemporal. 1. buffer/global/flat_store; glc=1 slc=1 dlc=1. - If GFX10, omit dlc=1. - volatile. 1. buffer/global/flat_store; dlc=1. - If GFX10, omit dlc=1. 2. s_waitcnt vscnt(0). - Must happen before; any following volatile; global/generic; load/store.; - Ensures that; volatile; operations to; different; addresses will not; be reordered by; hardware. store *none* *none* - local 1. ds_store; **Unordered Atomic**; ------------------------------------------------------------------------------------; load atomic unordered *any* *any* *Same as non-atomic*.; store atomic unordered *any* *any* *Same as non-atomic*.; atomicrmw unordered *any* *any* *Same as monotonic atomic*.; **Monotonic",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:344061,load,load,344061,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"urity protocols in the list of protocols returned by the serverMake the readahead strategy more conservativeFix a rare race condition happening when destroying instances with outstanding open requestsEnforce cache coherency in the case of reads+writes in the same fileCorrectly guess the filesize of a file opened for writing in sync modeMake server host name check more flexible for GSI authenticationFix some relevant issues with cache handling on the client, including a rare but fatal bug in; determining the cache holes list and the end of a cache lookupMore complete detection of async read errorsGeneralFix problem in handling the return code; of X509_REQ_verify; in XrdCryptosslX509Req.ccAvoid SEGV when doing an lsd admin command with; authenticated xrootd clientsClose race conditions that allowed a supervisor/manager; to subscribe without declaring a data port. Initialize nostage state in; XrdCmsState to prevent erroneous state declaration during; initialization.Fix a problem with the subject name of proxies of level; > 1; this was creating a failure when a Globus application was; trying to use the proxy certificateFix a problem with cache refreshing in XrdSutCache; affecting automatic reloading of password filesFor now, turn off IPV6 processing as it seems to create; several problems.Fix a few issues with the available releases of gcc 4.4Fix a few issues with the 'icc' compilerFix several issues in GSI and PWD authentication modulesNew featuresNew File Residency Manager (frm), replacement for the MPS scriptsScripts are now provided toautomatically donwload a CRL certificate; (utils/getCRLcert)install the recommended verion of OpenSSL and build it; with the options optimal for usage in XROOTD/SCALLA; (utils/installOpenSSL.sh)install the recommended verion of OpenAFS and build it; with the options optimal for usage in; XROOTD/SCALLA (utils/installOpenAFS.sh)MiscellaneaTokenAuthz and CS2 modules are no longer part of the main; built; they have to be built externally. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/net/doc/v524/index.html:3197,cache,cache,3197,net/doc/v524/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/net/doc/v524/index.html,2,['cache'],['cache']
Performance,"urn (a&&b) || (a&&!b);}; Should fold to ""a"". Currently not optimized with ""clang -emit-llvm-bc; | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b, int c) {return (a&&b) || (!a&&c);}; Should fold to ""a ? b : c"", or at least something sane. Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b, int c) {return (a&&b) || (a&&c) || (a&&b&&c);}; Should fold to a && (b || c). Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int x) {return x | ((x & 8) ^ 8);}; Should combine to x | 8. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int x) {return x ^ ((x & 8) ^ 8);}; Should also combine to x | 8. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int x) {return ((x | -9) ^ 8) & x;}; Should combine to x & -9. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned a(unsigned a) {return a * 0x11111111 >> 28 & 1;}; Should combine to ""a * 0x88888888 >> 31"". Currently not optimized; with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned a(char* x) {if ((*x & 32) == 0) return b();}; There's an unnecessary zext in the generated code with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned a(unsigned long long x) {return 40 * (x >> 1);}; Should combine to ""20 * (((unsigned)x) & -2)"". Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===-------------------------------",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:25594,optimiz,optimized,25594,interpreter/llvm-project/llvm/lib/Target/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt,2,['optimiz'],['optimized']
Performance,"urn S->getKind() == SK_Square;; + }; };. class Circle : public Shape {; double Radius;; public:; Circle(double R) : Shape(SK_Circle), Radius(R) {}; double computeArea() override;; +; + static bool classof(const Shape *S) {; + return S->getKind() == SK_Circle;; + }; };. The job of ``classof`` is to dynamically determine whether an object of; a base class is in fact of a particular derived class. In order to; downcast a type ``Base`` to a type ``Derived``, there needs to be a; ``classof`` in ``Derived`` which will accept an object of type ``Base``. To be concrete, consider the following code:. .. code-block:: c++. Shape *S = ...;; if (isa<Circle>(S)) {; /* do something ... */; }. The code of the ``isa<>`` test in this code will eventually boil; down---after template instantiation and some other machinery---to a; check roughly like ``Circle::classof(S)``. For more information, see; :ref:`classof-contract`. The argument to ``classof`` should always be an *ancestor* class because; the implementation has logic to allow and optimize away; upcasts/up-``isa<>``'s automatically. It is as though every class; ``Foo`` automatically has a ``classof`` like:. .. code-block:: c++. class Foo {; [...]; template <class T>; static bool classof(const T *,; ::std::enable_if<; ::std::is_base_of<Foo, T>::value; >::type* = 0) { return true; }; [...]; };. Note that this is the reason that we did not need to introduce a; ``classof`` into ``Shape``: all relevant classes derive from ``Shape``,; and ``Shape`` itself is abstract (has no entry in the ``Kind`` enum),; so this notional inferred ``classof`` is all we need. See `Concrete; Bases and Deeper Hierarchies`_ for more information about how to extend; this example to more general hierarchies. Although for this small example setting up LLVM-style RTTI seems like a lot; of ""boilerplate"", if your classes are doing anything interesting then this; will end up being a tiny fraction of the code. Concrete Bases and Deeper Hierarchies; =================",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToSetUpLLVMStyleRTTI.rst:6282,optimiz,optimize,6282,interpreter/llvm-project/llvm/docs/HowToSetUpLLVMStyleRTTI.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToSetUpLLVMStyleRTTI.rst,1,['optimiz'],['optimize']
Performance,"urrent module.; #) Embed the pre-link bitcode in a special ``.llvm.lto`` section.; #) Finish optimizing the module using the ModuleOptimization pipeline.; #) Emit the object file, including the new ``.llvm.lto`` section. .. NOTE. Previously, we conservatively ran independent pipelines on separate copies; of the LLVM module to generate the bitcode section and the object code,; which happen to be identical to those used outside of FatLTO. While that; resulted in compiled artifacts that were identical to those produced by the; default and (Thin)LTO pipelines, module cloning led to some cases of; miscompilation, and we have moved away from trying to keep bitcode; generation and optimization completely disjoint. Bit-for-bit compatibility is not (and never was) a guarantee, and we reserve; the right to change this at any time. Explicitly, users should not rely on; the produced bitcode or object code to match their non-LTO counterparts; precisely. They will exhibit similar performance characteristics, but may; not be bit-for-bit the same. Internally, the ``.llvm.lto`` section is created by running the; ``EmbedBitcodePass`` after the ``ThinLTOPreLinkDefaultPipeline``. This pass is; responsible for emitting the ``.llvm.lto`` section. Afterwards, the; ``ThinLTODefaultPipeline`` runs and the compiler can emit the fat object file. Limitations; ===========. Linkers; -------. Currently, using LTO with LLVM fat lto objects is supported by LLD and by the; GNU linkers via :doc:`GoldPlugin`. This may change in the future, but; extending support to other linkers isn't planned for now. .. NOTE; For standard linking the fat object files should be usable by any; linker capable of using ELF objects, since the ``.llvm.lto`` section is; marked ``SHF_EXCLUDE``. Supported File Formats; ----------------------. The current implementation only supports ELF files. At time of writing, it is; unclear if it will be useful to support other object file formats like ``COFF``; or ``Mach-O``. Usage; =====",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/FatLTO.rst:1917,perform,performance,1917,interpreter/llvm-project/llvm/docs/FatLTO.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/FatLTO.rst,1,['perform'],['performance']
Performance,"use ICM, STCM, or CLM. --. We don't use ADD (LOGICAL) HIGH, SUBTRACT (LOGICAL) HIGH,; or COMPARE (LOGICAL) HIGH yet. --. DAGCombiner doesn't yet fold truncations of extended loads. Functions like:. unsigned long f (unsigned long x, unsigned short *y); {; return (x << 32) | *y;; }. therefore end up as:. sllg %r2, %r2, 32; llgh %r0, 0(%r3); lr %r2, %r0; br %r14. but truncating the load would give:. sllg %r2, %r2, 32; lh %r2, 0(%r3); br %r14. --. Functions like:. define i64 @f1(i64 %a) {; %and = and i64 %a, 1; ret i64 %and; }. ought to be implemented as:. lhi %r0, 1; ngr %r2, %r0; br %r14. but two-address optimizations reverse the order of the AND and force:. lhi %r0, 1; ngr %r0, %r2; lgr %r2, %r0; br %r14. CodeGen/SystemZ/and-04.ll has several examples of this. --. Out-of-range displacements are usually handled by loading the full; address into a register. In many cases it would be better to create; an anchor point instead. E.g. for:. define void @f4a(i128 *%aptr, i64 %base) {; %addr = add i64 %base, 524288; %bptr = inttoptr i64 %addr to i128 *; %a = load volatile i128 *%aptr; %b = load i128 *%bptr; %add = add i128 %a, %b; store i128 %add, i128 *%aptr; ret void; }. (from CodeGen/SystemZ/int-add-08.ll) we load %base+524288 and %base+524296; into separate registers, rather than using %base+524288 as a base for both. --. Dynamic stack allocations round the size to 8 bytes and then allocate; that rounded amount. It would be simpler to subtract the unrounded; size from the copy of the stack pointer and then align the result.; See CodeGen/SystemZ/alloca-01.ll for an example. --. If needed, we can support 16-byte atomics using LPQ, STPQ and CSDG. --. We might want to model all access registers and use them to spill; 32-bit values. --. We might want to use the 'overflow' condition of eg. AR to support; llvm.sadd.with.overflow.i32 and related instructions - the generated code; for signed overflow check is currently quite bad. This would improve; the results of using -ftrapv.; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt:2920,load,load,2920,interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt,6,['load'],['load']
Performance,"use complexities in SSA; construction, consider this extremely simple C example:. .. code-block:: c. int G, H;; int test(_Bool Condition) {; int X;; if (Condition); X = G;; else; X = H;; return X;; }. In this case, we have the variable ""X"", whose value depends on the path; executed in the program. Because there are two different possible values; for X before the return instruction, a PHI node is inserted to merge the; two values. The LLVM IR that we want for this example looks like this:. .. code-block:: llvm. @G = weak global i32 0 ; type of @G is i32*; @H = weak global i32 0 ; type of @H is i32*. define i32 @test(i1 %Condition) {; entry:; br i1 %Condition, label %cond_true, label %cond_false. cond_true:; %X.0 = load i32, i32* @G; br label %cond_next. cond_false:; %X.1 = load i32, i32* @H; br label %cond_next. cond_next:; %X.2 = phi i32 [ %X.1, %cond_false ], [ %X.0, %cond_true ]; ret i32 %X.2; }. In this example, the loads from the G and H global variables are; explicit in the LLVM IR, and they live in the then/else branches of the; if statement (cond\_true/cond\_false). In order to merge the incoming; values, the X.2 phi node in the cond\_next block selects the right value; to use based on where control flow is coming from: if control flow comes; from the cond\_false block, X.2 gets the value of X.1. Alternatively, if; control flow comes from cond\_true, it gets the value of X.0. The intent; of this chapter is not to explain the details of SSA form. For more; information, see one of the many `online; references <http://en.wikipedia.org/wiki/Static_single_assignment_form>`_. The question for this article is ""who places the phi nodes when lowering; assignments to mutable variables?"". The issue here is that LLVM; *requires* that its IR be in SSA form: there is no ""non-ssa"" mode for; it. However, SSA construction requires non-trivial algorithms and data; structures, so it is inconvenient and wasteful for every front-end to; have to reproduce this logic. Memory in LLV",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:2431,load,loads,2431,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,1,['load'],['loads']
Performance,"use the `httpRequest` method.; For instance to receive object from a THttpServer server one could do:. ```javascript; import { httpRequest } from 'https://root.cern/js/latest/modules/main.mjs';; let obj = await httpRequest(""http://your_root_server:8080/Canvases/c1/root.json"", ""object""); console.log('Read object of type', obj._typename);; ```. Function returns Promise, which provides parsed object (or Error in case of failure). If JSON string was obtained by different method, it could be parsed with `parse` function:. ```javascript; import { parse } from 'https://root.cern/js/latest/modules/main.mjs';; let obj = parse(json_string);; ```. ### Objects drawing. After an object has been created, one can directly draw it. If HTML page has `<div>` element:. ```html; <div id=""drawing""></div>; ```. One could use the `draw` function:. ```javascript; import { draw } from 'https://root.cern/js/latest/modules/main.mjs';; draw(""drawing"", obj, ""colz"");; ```. The first argument is the id of the HTML div element, where drawing will be performed. The second argument is the object to draw and the third one is the drawing option. Here is complete [running example](https://root.cern/js/latest/api.htm#custom_html_read_json) ans [source code](https://github.com/root-project/jsroot/blob/master/demo/read_json.htm):. ```javascript; import { httpRequest, draw, redraw, resize, cleanup } from 'https://root.cern/js/latest/modules/main.mjs';; let filename = ""https://root.cern/js/files/th2ul.json.gz"";; let obj = await httpRequest(filename, 'object');; draw(""drawing"", obj, ""lego"");; ```. In very seldom cases one need to access painter object, created in `draw()` function. This can be done via; handling Promise results like:. ```javascript; let painter = await draw(""drawing"", obj, ""colz"");; console.log('Object type in painter', painter.getClassName());; ```. One is also able to update the drawing with a new version of the object:. ```javascript; // after some interval request object again; redraw(""dr",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/JSROOT/JSROOT.md:38163,perform,performed,38163,documentation/JSROOT/JSROOT.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/JSROOT/JSROOT.md,1,['perform'],['performed']
Performance,"used to; get the address of the most recent dynamic alloca, allocated by :ref:`alloca <i_alloca>`; on the caller's stack. In particular, for targets where stack grows downwards,; adding this offset to the native stack pointer would get the address of the most; recent dynamic alloca. For targets where stack grows upwards, the situation is a bit more; complicated, because subtracting this value from stack pointer would get the address; one past the end of the most recent dynamic alloca. Although for most targets `llvm.get.dynamic.area.offset <int_get_dynamic_area_offset>`; returns just a zero, for others, such as PowerPC and PowerPC64, it returns a; compile-time-known constant value. The return value type of :ref:`llvm.get.dynamic.area.offset <int_get_dynamic_area_offset>`; must match the target's default address space's (address space 0) pointer type. '``llvm.prefetch``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare void @llvm.prefetch(ptr <address>, i32 <rw>, i32 <locality>, i32 <cache type>). Overview:; """""""""""""""""". The '``llvm.prefetch``' intrinsic is a hint to the code generator to; insert a prefetch instruction if supported; otherwise, it is a noop.; Prefetches have no effect on the behavior of the program but can change; its performance characteristics. Arguments:; """""""""""""""""""". ``address`` is the address to be prefetched, ``rw`` is the specifier; determining if the fetch should be for a read (0) or write (1), and; ``locality`` is a temporal locality specifier ranging from (0) - no; locality, to (3) - extremely local keep in cache. The ``cache type``; specifies whether the prefetch is performed on the data (1) or; instruction (0) cache. The ``rw``, ``locality`` and ``cache type``; arguments must be constant integers. Semantics:; """""""""""""""""""". This intrinsic does not modify the behavior of the program. In; particular, prefetches cannot trap and do not produce a value. On; targets that support this intrinsic, the prefetch can provide hints to;",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:523018,cache,cache,523018,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['cache'],['cache']
Performance,"uses a *destructive* or; *non-reversible* modification of the address to prevent an attacker from; reversing the check using attacker-controlled inputs.; * It does not completely block speculative execution, and merely prevents; *mis*-speculated paths from leaking secrets from memory (and stalls; speculation until this can be determined).; * It is completely general and makes no fundamental assumptions about the; underlying architecture other than the ability to do branchless conditional; data updates and a lack of value prediction.; * It does not require programmers to identify all possible secret data using; static source code annotations or code vulnerable to a variant #1 style; attack. Limitations of this approach:; * It requires re-compiling source code to insert hardening instruction; sequences. Only software compiled in this mode is protected.; * The performance is heavily dependent on a particular architecture's; implementation strategy. We outline a potential x86 implementation below and; characterize its performance.; * It does not defend against secret data already loaded from memory and; residing in registers or leaked through other side-channels in; non-speculative execution. Code dealing with this, e.g cryptographic; routines, already uses constant-time algorithms and code to prevent; side-channels. Such code should also scrub registers of secret data following; [these; guidelines](https://github.com/HACS-workshop/spectre-mitigations/blob/master/crypto_guidelines.md).; * To achieve reasonable performance, many loads may not be checked, such as; those with compile-time fixed addresses. This primarily consists of accesses; at compile-time constant offsets of global and local variables. Code which; needs this protection and intentionally stores secret data must ensure the; memory regions used for secret data are necessarily dynamic mappings or heap; allocations. This is an area which can be tuned to provide more comprehensive; protection at the cost of per",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md:6697,perform,performance,6697,interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,1,['perform'],['performance']
Performance,"uses that compiler to build an instrumented stage2 compiler. **stage2-instrumented-generate-profdata**; Depends on stage2-instrumented and will use the instrumented compiler to; generate profdata based on the training files in clang/utils/perf-training. **stage2**; Depends on stage2-instrumented-generate-profdata and will use the stage1; compiler with the stage2 profdata to build a PGO-optimized compiler. **stage2-check-llvm**; Depends on stage2 and runs check-llvm using the stage2 compiler. **stage2-check-clang**; Depends on stage2 and runs check-clang using the stage2 compiler. **stage2-check-all**; Depends on stage2 and runs check-all using the stage2 compiler. **stage2-test-suite**; Depends on stage2 and runs the test-suite using the stage2 compiler (requires; in-tree test-suite). BOLT; ====. `BOLT <https://github.com/llvm/llvm-project/blob/main/bolt/README.md>`_; (Binary Optimization and Layout Tool) is a tool that optimizes binaries; post-link by profiling them at runtime and then using that information to; optimize the layout of the final binary among other optimizations performed; at the binary level. There are also CMake caches available to build; LLVM/Clang with BOLT. To configure a single-stage build that builds LLVM/Clang and then optimizes; it with BOLT, use the following CMake configuration:. .. code-block:: console. $ cmake <path to source>/llvm -C <path to source>/clang/cmake/caches/BOLT.cmake. Then, build the BOLT-optimized binary by running the following ninja command:. .. code-block:: console. $ ninja clang-bolt. If you're seeing errors in the build process, try building with a recent; version of Clang/LLVM by setting the CMAKE_C_COMPILER and; CMAKE_CXX_COMPILER flags to the appropriate values. It is also possible to use BOLT on top of PGO and (Thin)LTO for an even more; significant runtime speedup. To configure a three stage PGO build with ThinLTO; that optimizes the resulting binary with BOLT, use the following CMake; configuration command:. .. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AdvancedBuilds.rst:9949,optimiz,optimizes,9949,interpreter/llvm-project/llvm/docs/AdvancedBuilds.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AdvancedBuilds.rst,4,"['optimiz', 'perform']","['optimizations', 'optimize', 'optimizes', 'performed']"
